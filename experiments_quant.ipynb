{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "1BSghSI2PI-B",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.quantization\n",
        "from torch.quantization import FakeQuantize, QConfig\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "gW11Upt5SLCs"
      },
      "outputs": [],
      "source": [
        "def get_activation_function(activation: str, quantize=False):\n",
        "    if activation == 'relu6':\n",
        "        # ReLU6 is already quantizable\n",
        "        return nn.ReLU6(inplace=True)\n",
        "    if activation == 'relu':\n",
        "        act = nn.ReLU(inplace=True)\n",
        "    elif activation == 'hardtanh':\n",
        "        act = nn.Hardtanh(inplace=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation: %s\" % activation)\n",
        "    \n",
        "    if not quantize:\n",
        "        return act\n",
        "    else:\n",
        "        # Put QuantStub after activation, to force quantization of it's output\n",
        "        # (By default, Pytorch framework supposes that user fuses ReLU with previous layers\n",
        "        # but you can't fuse with Hardtanh anyway)\n",
        "        return nn.Sequential(act, torch.quantization.QuantStub())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "JiqEVDVbPI-C"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation=\"relu\", quantize=False):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.quantize = quantize\n",
        "        if quantize:\n",
        "            self.quant = torch.quantization.QuantStub()\n",
        "            self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=5),\n",
        "            # Never quantize first activation func\n",
        "            get_activation_function(activation, quantize=False),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            get_activation_function(activation, quantize=quantize),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(400, 120), get_activation_function(activation, quantize=quantize)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(120, 84), get_activation_function(activation, quantize=quantize)\n",
        "        )\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        if self.quantize:\n",
        "            x = self.quant(x)\n",
        "            \n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        if self.quantize:\n",
        "            x = self.dequant(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "MbSCoDqF9J9j"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_channels: int, out_channels: int, stride: int = 1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_channels: int, out_channels: int, stride: int = 1) -> nn.Conv2d:\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, stride=1, downsample=None, activation=\"relu\", quantize=False\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.act_fn1 = get_activation_function(activation, quantize=quantize)\n",
        "\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "        # Remember to use two independent ReLU for layer fusion.\n",
        "        self.act_fn2 = get_activation_function(activation, quantize=quantize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # Use FloatFunctional for addition for quantization compatibility\n",
        "        out = self.skip_add.add(residual, out)\n",
        "        out = self.act_fn2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "A7hEgTOZ4W6B"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        num_classes=10,\n",
        "        activation=\"relu\",\n",
        "        initial_channels=16,\n",
        "        quantize=False,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.quantize = quantize\n",
        "        if quantize:\n",
        "            self.quant = torch.quantization.QuantStub()\n",
        "            self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        self.in_channels = initial_channels\n",
        "        self.initial_layer = nn.Sequential(\n",
        "            conv3x3(3, initial_channels),\n",
        "            nn.BatchNorm2d(initial_channels),\n",
        "            # Never quantize first activation func\n",
        "            get_activation_function(activation, quantize=False)\n",
        "        )\n",
        "\n",
        "        self.layer1 = self.make_layer(\n",
        "            block, initial_channels, layers[0], activation=activation\n",
        "        )\n",
        "        self.layer2 = self.make_layer(\n",
        "            block, initial_channels * 2, layers[1], stride=2, activation=activation, \n",
        "        )\n",
        "        self.layer3 = self.make_layer(\n",
        "            block, initial_channels * 4, layers[2], stride=2, activation=activation\n",
        "        )\n",
        "        if len(layers) == 4:\n",
        "            self.layer4 = self.make_layer(\n",
        "                block, initial_channels * 8, layers[3], stride=2, activation=activation\n",
        "            )\n",
        "        else:\n",
        "            self.layer4 = None\n",
        "\n",
        "        pool_size = 8 if initial_channels == 16 else 4\n",
        "        fc_in_features = (\n",
        "            initial_channels * 4 if len(layers) == 3 else initial_channels * 8\n",
        "        )\n",
        "        self.avg_pool = nn.AvgPool2d(pool_size)\n",
        "        self.fc = nn.Linear(fc_in_features, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1, activation=\"relu\"):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.in_channels, out_channels, stride, downsample, activation, quantize=self.quantize)\n",
        "        )\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels, activation=activation, quantize=self.quantize))\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.initial_layer(x)\n",
        "        # Start quantizing AFTER first layer\n",
        "        if self.quantize:\n",
        "            out = self.quant(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        if self.layer4:\n",
        "            out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        \n",
        "        if self.quantize:\n",
        "            out = self.dequant(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "E71eqKTH93QE"
      },
      "outputs": [],
      "source": [
        "def ResNet20(num_classes=10, activation='relu', quantize=False):\n",
        "    return ResNet(ResidualBlock, [3, 3, 3], num_classes, activation, initial_channels=16, quantize=quantize)\n",
        "\n",
        "def ResNet18(num_classes=10, activation='relu', quantize=False):\n",
        "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes, activation, initial_channels=64, quantize=quantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaT5BVpE3OD"
      },
      "source": [
        "**Обучение моделей**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF-xsDMjPI-E"
      },
      "outputs": [],
      "source": [
        "cuda_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Quantize-converted models can be run only on CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "NUM_EPOCHS = 120\n",
        "BATCH_SIZE = 128\n",
        "LR = 1e-3\n",
        "NUM_CLASSES = 10\n",
        "QAT_EPOCHS = 50\n",
        "QAT_LR = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiWTiFwfPI-E",
        "outputId": "39bd1ebc-fccb-49e2-e1f6-4ff16bbf1fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data',\n",
        "                                 train=True,\n",
        "                                 transform=train_transform,\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='data',\n",
        "                                train=False,\n",
        "                                transform=test_transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=8,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         num_workers=8,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KO7_sP5cPI-E"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, device=cuda_device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    # We don't want Observers to change quantization params\n",
        "    model.apply(torch.ao.quantization.disable_observer)\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    model.apply(torch.ao.quantization.enable_observer)\n",
        "    return eval_loss, eval_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gA3OchQPI-E"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    learning_rate,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    writer=None,\n",
        "    model_name=\"model\",\n",
        "    device=cuda_device,\n",
        "):\n",
        "    patience = 15\n",
        "    cur_idle = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    if \"quantized\" in model_name:\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[30, 40], gamma=0.25, last_epoch=-1\n",
        "        )\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[70, 100, 130], gamma=0.1, last_epoch=-1\n",
        "        )\n",
        "\n",
        "    best_eval_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        eval_loss, eval_accuracy = evaluate_model(\n",
        "            model=model, device=device, criterion=criterion\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "        if writer:\n",
        "            writer.add_scalar(f\"TrainLoss/{model_name}\", train_loss, epoch)\n",
        "            writer.add_scalar(f\"TrainAcc/{model_name}\", train_accuracy, epoch)\n",
        "            writer.add_scalar(f\"EvalLoss/{model_name}\", eval_loss, epoch)\n",
        "            writer.add_scalar(f\"EvalAcc/{model_name}\", eval_accuracy, epoch)\n",
        "\n",
        "        print(\n",
        "            \"[{}] Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n",
        "                model_name,\n",
        "                epoch + 1,\n",
        "                train_loss,\n",
        "                train_accuracy,\n",
        "                eval_loss,\n",
        "                eval_accuracy,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        cur_idle += 1\n",
        "        if eval_accuracy > best_eval_acc:\n",
        "            best_eval_acc = eval_accuracy\n",
        "            torch.save(model.state_dict(), f\"checkpoint/best_{model_name}.ckpt\")\n",
        "            cur_idle = 0\n",
        "        elif cur_idle >= patience:\n",
        "            print(\"Early stopping was triggered!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Sm62KPslHekH"
      },
      "outputs": [],
      "source": [
        "def train_orig_model(model_class, activation):\n",
        "    model_name = f\"{model_class.__name__}_{activation}\"\n",
        "    model = model_class(activation=activation).to(cuda_device)\n",
        "    writer = SummaryWriter(f\"runs/{model_name}\")\n",
        "\n",
        "    train_model(\n",
        "        model,\n",
        "        learning_rate=LR,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        writer=writer,\n",
        "        model_name=model_name,\n",
        "        device=cuda_device,\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    torch.save(model.state_dict(), f\"checkpoint/{model_name}.ckpt\")\n",
        "    writer.close()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configure_qat(model, activation_bitwidth=4, weight_bitwidth=4):\n",
        "    # Fake quantizer for activations\n",
        "    fq_activation = FakeQuantize.with_args(\n",
        "        observer=torch.quantization.MovingAverageMinMaxObserver.with_args(\n",
        "            quant_min=0,\n",
        "            quant_max=2**activation_bitwidth - 1,\n",
        "            dtype=torch.quint8,\n",
        "            qscheme=torch.per_tensor_affine,\n",
        "            reduce_range=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Fake quantizer for weights\n",
        "    fq_weights = FakeQuantize.with_args(\n",
        "        observer=torch.quantization.MovingAveragePerChannelMinMaxObserver.with_args(\n",
        "            quant_min=-(2**weight_bitwidth) // 2,\n",
        "            quant_max=(2**weight_bitwidth) // 2 - 1,\n",
        "            dtype=torch.qint8,\n",
        "            qscheme=torch.per_channel_symmetric,\n",
        "            reduce_range=False,\n",
        "            ch_axis=0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # We don't want non-activation layers to quantize it's output\n",
        "    # (for example, Conv2d)\n",
        "    # Because it will harm subsequent Activation performance\n",
        "    # (other solution will be to fuse Activation with previous layer\n",
        "    # but pytorch can't fuse with anything, except for nn.ReLU)\n",
        "    weight_only_qconfig = QConfig(\n",
        "        activation=torch.quantization.NoopObserver.with_args(dtype=torch.float32),\n",
        "        weight=fq_weights,\n",
        "    )\n",
        "    \n",
        "    activation_qconfig = QConfig(activation=fq_activation, weight=fq_weights)\n",
        "    \n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Hardtanh, nn.ReLU6, nn.ReLU, torch.quantization.QuantStub, torch.quantization.DeQuantStub)):\n",
        "            module.qconfig = activation_qconfig\n",
        "        else:\n",
        "            module.qconfig = weight_only_qconfig\n",
        "\n",
        "     # Avoid quantizing first and last layers\n",
        "    if isinstance(model, LeNet5):\n",
        "        # First Layer\n",
        "        model.conv1.qconfig = None\n",
        "        for name, module in model.conv1.named_modules():\n",
        "            module.qconfig = None\n",
        "        model.fc3.qconfig = None # Last Linear\n",
        "\n",
        "    elif isinstance(model, ResNet):\n",
        "        model.initial_layer.qconfig = None\n",
        "        for name, module in model.initial_layer.named_modules():\n",
        "            module.qconfig = None\n",
        "        model.fc.qconfig = None # Last Linear\n",
        "\n",
        "    torch.quantization.prepare_qat(model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8pwe7fgIroo"
      },
      "outputs": [],
      "source": [
        "def train_quantized_model(model_class, activation, ckpt_path):\n",
        "    device=cuda_device\n",
        "    \n",
        "    model_name = f\"{model_class.__name__}_{activation}_quantized\"\n",
        "    model = model_class(activation=activation, quantize=True).to(device)\n",
        "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "\n",
        "    model.train()\n",
        "    configure_qat(model)\n",
        "    \n",
        "    print(f\"[{model_name}] after configure_qat:\", model)\n",
        "    \n",
        "    # fine-tune via QAT\n",
        "    writer = SummaryWriter(f\"runs/{model_name}\")\n",
        "    train_model(model, learning_rate=QAT_LR, num_epochs=QAT_EPOCHS, writer=writer, model_name=model_name, device=device)\n",
        "\n",
        "    # Save quantized model\n",
        "    model.eval()\n",
        "    torch.save(model.state_dict(), f\"checkpoint/{model_name}.ckpt\")\n",
        "    writer.close()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All possible architectures and activations\n",
        "models = [LeNet5, ResNet20, ResNet18]\n",
        "activations = ['relu', 'hardtanh', 'relu6']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "collapsed": true,
        "id": "-wKYDsXyHiHM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training LeNet5 with relu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LeNet5_relu] Epoch: 001 Train Loss: 2.301 Train Acc: 0.110 Eval Loss: 2.295 Eval Acc: 0.124\n",
            "[LeNet5_relu] Epoch: 002 Train Loss: 2.270 Train Acc: 0.153 Eval Loss: 2.206 Eval Acc: 0.193\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[83], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m activation \u001b[38;5;129;01min\u001b[39;00m activations:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     trained_models[(model_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activation)] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_orig_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[79], line 6\u001b[0m, in \u001b[0;36mtrain_orig_model\u001b[0;34m(model_class, activation)\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(activation\u001b[38;5;241m=\u001b[39mactivation)\u001b[38;5;241m.\u001b[39mto(cuda_device)\n\u001b[1;32m      4\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[78], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, num_epochs, writer, model_name, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     31\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:112\u001b[0m, in \u001b[0;36mrebuild_tensor\u001b[0;34m(cls, storage, metadata)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_tensor\u001b[39m(\u001b[38;5;28mcls\u001b[39m, storage, metadata):\n\u001b[1;32m    111\u001b[0m     storage_offset, size, stride, requires_grad \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 112\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rebuild_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mParameter:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# we have to pass requires_grad into constructor, rather than set it as an\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# attribute later, because it's an important check for Integer Tensors to\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# have requires_grad=False (or else they raise an error)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mParameter(t, requires_grad\u001b[38;5;241m=\u001b[39mrequires_grad)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:174\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[0;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mset_(storage\u001b[38;5;241m.\u001b[39m_untyped_storage, storage_offset, size, stride)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train all combinations (without quantization)\n",
        "trained_models = {}\n",
        "for model_class in models:\n",
        "    for activation in activations:\n",
        "        print(f\"\\nTraining {model_class.__name__} with {activation}\")\n",
        "        trained_models[(model_class.__name__, activation)] = train_orig_model(model_class, activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_GxJiVZPI-F",
        "outputId": "8b523ab4-9a87-4a99-bb52-6e1fda59ec5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Quantizing LeNet5 with relu\n",
            "[LeNet5_relu_quantized] initial state: LeNet5(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): QuantStub()\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): QuantStub()\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): QuantStub()\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_relu_quantized] after configure_qat: LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): QuantStub(\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): QuantStub(\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): QuantStub(\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2662/1806173639.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LeNet5_relu_quantized] Epoch: 001 Train Loss: 1.176 Train Acc: 0.584 Eval Loss: 1.060 Eval Acc: 0.622\n",
            "\n",
            "Quantizing LeNet5 with hardtanh\n",
            "[LeNet5_hardtanh_quantized] initial state: LeNet5(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Sequential(\n",
            "      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "      (1): QuantStub()\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): Sequential(\n",
            "      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "      (1): QuantStub()\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): Sequential(\n",
            "      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "      (1): QuantStub()\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_hardtanh_quantized] after configure_qat: LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "      (1): QuantStub(\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "      (1): QuantStub(\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "      (1): QuantStub(\n",
            "        (activation_post_process): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_hardtanh_quantized] Epoch: 001 Train Loss: 1.170 Train Acc: 0.582 Eval Loss: 1.060 Eval Acc: 0.628\n",
            "\n",
            "Quantizing LeNet5 with relu6\n",
            "[LeNet5_relu6_quantized] initial state: LeNet5(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU6(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU6(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU6(inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): ReLU6(inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_relu6_quantized] after configure_qat: LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU6(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_relu6_quantized] Epoch: 001 Train Loss: 1.121 Train Acc: 0.602 Eval Loss: 1.044 Eval Acc: 0.629\n",
            "\n",
            "Quantizing ResNet20 with relu\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"initial_layer.0.weight\", \"initial_layer.1.weight\", \"initial_layer.1.bias\", \"initial_layer.1.running_mean\", \"initial_layer.1.running_var\". \n\tUnexpected key(s) in state_dict: \"conv.weight\", \"bn.weight\", \"bn.bias\", \"bn.running_mean\", \"bn.running_var\", \"bn.num_batches_tracked\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[84], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuantizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m quantized_models[(model_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activation)] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_quantized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[81], line 6\u001b[0m, in \u001b[0;36mtrain_quantized_model\u001b[0;34m(model_class, activation, ckpt_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(activation\u001b[38;5;241m=\u001b[39mactivation, quantize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] initial state:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"initial_layer.0.weight\", \"initial_layer.1.weight\", \"initial_layer.1.bias\", \"initial_layer.1.running_mean\", \"initial_layer.1.running_var\". \n\tUnexpected key(s) in state_dict: \"conv.weight\", \"bn.weight\", \"bn.bias\", \"bn.running_mean\", \"bn.running_var\", \"bn.num_batches_tracked\". "
          ]
        }
      ],
      "source": [
        "# Train quantized versions\n",
        "\n",
        "quantized_models = {}\n",
        "for model_class in models:\n",
        "    for activation in activations:\n",
        "        ckpt_path = f\"checkpoint/{model_class.__name__}_{activation}.ckpt\"\n",
        "        print(f\"\\nQuantizing {model_class.__name__} with {activation}\")\n",
        "        quantized_models[(model_class.__name__, activation)] = train_quantized_model(model_class, activation, ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX5ZzmKoJrRA"
      },
      "source": [
        "**Сравнение полученных моделей**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhAONBNliNq9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0826, 0.0611, 0.0373, 0.0597, 0.1061, 0.0572], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.5735, -0.4584, -0.2217, -0.4479, -0.7958, -0.4291], device='cuda:0'), max_val=tensor([0.6197, 0.3445, 0.2797, 0.3303, 0.5830, 0.3701], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0508, 0.0392, 0.0345, 0.0425, 0.0345, 0.0519, 0.0346, 0.0358, 0.0486,\n",
            "                0.0362, 0.0369, 0.0399, 0.0325, 0.0490, 0.0325, 0.0406],\n",
            "               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.3814, -0.1795, -0.2342, -0.3190, -0.1841, -0.3889, -0.2438, -0.2120,\n",
            "                  -0.3646, -0.2713, -0.2164, -0.2384, -0.1856, -0.3112, -0.1681, -0.1561],\n",
            "                 device='cuda:0'), max_val=tensor([0.3306, 0.2942, 0.2591, 0.3025, 0.2584, 0.3877, 0.2597, 0.2684, 0.2356,\n",
            "                  0.1995, 0.2764, 0.2995, 0.2436, 0.3673, 0.2440, 0.3046],\n",
            "                 device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0145, 0.0141, 0.0164, 0.0095, 0.0242, 0.0157, 0.0065, 0.0138, 0.0099,\n",
            "                0.0159, 0.0158, 0.0167, 0.0142, 0.0154, 0.0219, 0.0176, 0.0145, 0.0177,\n",
            "                0.0156, 0.0141, 0.0135, 0.0066, 0.0146, 0.0154, 0.0167, 0.0159, 0.0163,\n",
            "                0.0126, 0.0102, 0.0157, 0.0174, 0.0151, 0.0211, 0.0163, 0.0134, 0.0227,\n",
            "                0.0255, 0.0162, 0.0192, 0.0171, 0.0070, 0.0216, 0.0204, 0.0173, 0.0194,\n",
            "                0.0159, 0.0074, 0.0134, 0.0137, 0.0228, 0.0120, 0.0066, 0.0167, 0.0169,\n",
            "                0.0222, 0.0154, 0.0150, 0.0130, 0.0147, 0.0117, 0.0188, 0.0160, 0.0194,\n",
            "                0.0166, 0.0179, 0.0104, 0.0095, 0.0113, 0.0124, 0.0155, 0.0165, 0.0133,\n",
            "                0.0134, 0.0101, 0.0178, 0.0129, 0.0177, 0.0185, 0.0132, 0.0118, 0.0147,\n",
            "                0.0144, 0.0184, 0.0146, 0.0158, 0.0227, 0.0158, 0.0180, 0.0129, 0.0139,\n",
            "                0.0145, 0.0163, 0.0155, 0.0162, 0.0169, 0.0228, 0.0070, 0.0121, 0.0156,\n",
            "                0.0136, 0.0134, 0.0148, 0.0146, 0.0214, 0.0095, 0.0179, 0.0205, 0.0184,\n",
            "                0.0067, 0.0123, 0.0203, 0.0149, 0.0240, 0.0103, 0.0091, 0.0068, 0.0099,\n",
            "                0.0110, 0.0139, 0.0072], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "               device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1042, -0.1049, -0.1227, -0.0711, -0.1818, -0.1174, -0.0487, -0.0882,\n",
            "                  -0.0741, -0.1193, -0.1082, -0.1256, -0.1023, -0.1155, -0.1222, -0.1093,\n",
            "                  -0.1086, -0.1324, -0.1170, -0.0888, -0.1015, -0.0499, -0.1051, -0.1154,\n",
            "                  -0.1208, -0.1134, -0.1222, -0.0908, -0.0762, -0.1177, -0.0872, -0.1130,\n",
            "                  -0.1583, -0.1224, -0.0910, -0.1704, -0.1912, -0.1123, -0.1141, -0.1286,\n",
            "                  -0.0527, -0.1597, -0.1234, -0.0917, -0.1363, -0.1195, -0.0555, -0.1002,\n",
            "                  -0.1026, -0.1709, -0.0892, -0.0484, -0.1254, -0.1009, -0.1667, -0.1134,\n",
            "                  -0.0984, -0.0976, -0.1102, -0.0879, -0.1411, -0.1202, -0.0928, -0.1213,\n",
            "                  -0.0895, -0.0731, -0.0710, -0.0847, -0.0889, -0.0993, -0.1239, -0.0999,\n",
            "                  -0.1007, -0.0761, -0.1334, -0.0845, -0.1328, -0.1389, -0.0897, -0.0886,\n",
            "                  -0.0873, -0.0804, -0.1384, -0.0972, -0.1140, -0.1702, -0.1148, -0.1350,\n",
            "                  -0.0968, -0.0750, -0.1088, -0.1225, -0.1164, -0.1219, -0.1131, -0.1399,\n",
            "                  -0.0529, -0.0905, -0.1172, -0.1020, -0.0856, -0.0992, -0.0942, -0.1124,\n",
            "                  -0.0605, -0.1346, -0.1240, -0.1377, -0.0504, -0.0919, -0.1485, -0.1116,\n",
            "                  -0.1803, -0.0743, -0.0583, -0.0507, -0.0742, -0.0718, -0.0724, -0.0538],\n",
            "                 device='cuda:0'), max_val=tensor([0.1088, 0.1054, 0.0869, 0.0700, 0.1116, 0.0842, 0.0485, 0.1038, 0.0725,\n",
            "                  0.0851, 0.1185, 0.1016, 0.1062, 0.0904, 0.1645, 0.1321, 0.1089, 0.1325,\n",
            "                  0.1080, 0.1054, 0.0940, 0.0468, 0.1098, 0.0819, 0.1254, 0.1191, 0.1067,\n",
            "                  0.0944, 0.0712, 0.0895, 0.1307, 0.0979, 0.1162, 0.1097, 0.1008, 0.1042,\n",
            "                  0.1317, 0.1214, 0.1439, 0.1234, 0.0504, 0.1619, 0.1533, 0.1298, 0.1456,\n",
            "                  0.1188, 0.0498, 0.0965, 0.1022, 0.1173, 0.0902, 0.0497, 0.0938, 0.1269,\n",
            "                  0.0951, 0.1156, 0.1128, 0.0936, 0.1006, 0.0820, 0.1107, 0.1132, 0.1454,\n",
            "                  0.1243, 0.1343, 0.0782, 0.0652, 0.0831, 0.0928, 0.1165, 0.0906, 0.0876,\n",
            "                  0.0830, 0.0758, 0.1259, 0.0971, 0.1123, 0.1127, 0.0987, 0.0778, 0.1105,\n",
            "                  0.1080, 0.1060, 0.1098, 0.1187, 0.0982, 0.1185, 0.1023, 0.0929, 0.1044,\n",
            "                  0.0926, 0.1076, 0.1061, 0.1060, 0.1265, 0.1709, 0.0468, 0.0817, 0.1147,\n",
            "                  0.0979, 0.1002, 0.1110, 0.1095, 0.1607, 0.0714, 0.1142, 0.1534, 0.1321,\n",
            "                  0.0495, 0.0859, 0.1522, 0.1112, 0.1082, 0.0772, 0.0686, 0.0471, 0.0739,\n",
            "                  0.0824, 0.1040, 0.0483], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0224, 0.0224, 0.0221, 0.0194, 0.0230, 0.0267, 0.0183, 0.0202, 0.0158,\n",
            "                0.0260, 0.0191, 0.0276, 0.0216, 0.0137, 0.0166, 0.0195, 0.0166, 0.0261,\n",
            "                0.0235, 0.0214, 0.0153, 0.0181, 0.0218, 0.0182, 0.0262, 0.0172, 0.0203,\n",
            "                0.0279, 0.0263, 0.0173, 0.0167, 0.0232, 0.0155, 0.0213, 0.0228, 0.0195,\n",
            "                0.0190, 0.0237, 0.0247, 0.0190, 0.0285, 0.0221, 0.0209, 0.0207, 0.0236,\n",
            "                0.0215, 0.0230, 0.0188, 0.0281, 0.0223, 0.0117, 0.0121, 0.0156, 0.0186,\n",
            "                0.0218, 0.0246, 0.0117, 0.0220, 0.0191, 0.0316, 0.0196, 0.0184, 0.0178,\n",
            "                0.0234, 0.0257, 0.0194, 0.0214, 0.0125, 0.0236, 0.0157, 0.0209, 0.0199,\n",
            "                0.0312, 0.0124, 0.0211, 0.0248, 0.0232, 0.0210, 0.0244, 0.0207, 0.0199,\n",
            "                0.0225, 0.0201, 0.0269], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1683, -0.1677, -0.1579, -0.1452, -0.1723, -0.2000, -0.1373, -0.1239,\n",
            "                  -0.1053, -0.1950, -0.1216, -0.2073, -0.1205, -0.1026, -0.1239, -0.1055,\n",
            "                  -0.1074, -0.1958, -0.1614, -0.1605, -0.0994, -0.1320, -0.1234, -0.1353,\n",
            "                  -0.1603, -0.1290, -0.1479, -0.1452, -0.1588, -0.1000, -0.1190, -0.1738,\n",
            "                  -0.0911, -0.1424, -0.1706, -0.1370, -0.1177, -0.1706, -0.1851, -0.1343,\n",
            "                  -0.2134, -0.1634, -0.1352, -0.1529, -0.1772, -0.1276, -0.1721, -0.1304,\n",
            "                  -0.2106, -0.1507, -0.0881, -0.0894, -0.1171, -0.1394, -0.1319, -0.1841,\n",
            "                  -0.0881, -0.1534, -0.1222, -0.1985, -0.1403, -0.1159, -0.1281, -0.1392,\n",
            "                  -0.1320, -0.1452, -0.1339, -0.0936, -0.1442, -0.1170, -0.1566, -0.1492,\n",
            "                  -0.1796, -0.0931, -0.1464, -0.1624, -0.1623, -0.1205, -0.1487, -0.1482,\n",
            "                  -0.1494, -0.1552, -0.1452, -0.1543], device='cuda:0'), max_val=tensor([0.1359, 0.1439, 0.1660, 0.1385, 0.1303, 0.1728, 0.1284, 0.1519, 0.1186,\n",
            "                  0.1528, 0.1434, 0.1648, 0.1617, 0.0798, 0.1248, 0.1466, 0.1247, 0.1790,\n",
            "                  0.1759, 0.1399, 0.1149, 0.1356, 0.1636, 0.1369, 0.1962, 0.1113, 0.1526,\n",
            "                  0.2090, 0.1970, 0.1294, 0.1255, 0.1484, 0.1161, 0.1594, 0.1678, 0.1461,\n",
            "                  0.1426, 0.1777, 0.1644, 0.1428, 0.1406, 0.1658, 0.1571, 0.1553, 0.1665,\n",
            "                  0.1610, 0.1474, 0.1408, 0.1456, 0.1674, 0.0881, 0.0907, 0.1047, 0.1057,\n",
            "                  0.1634, 0.1493, 0.0866, 0.1648, 0.1430, 0.2369, 0.1474, 0.1383, 0.1335,\n",
            "                  0.1752, 0.1930, 0.1450, 0.1603, 0.0929, 0.1770, 0.1179, 0.1563, 0.1454,\n",
            "                  0.2341, 0.0888, 0.1582, 0.1858, 0.1741, 0.1572, 0.1833, 0.1555, 0.1216,\n",
            "                  0.1685, 0.1509, 0.2015], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0403, 0.0491, 0.0593, 0.0375, 0.0471, 0.0360, 0.0459, 0.0450, 0.0515,\n",
            "              0.0463], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.2619, -0.3683, -0.2507, -0.2542, -0.2832, -0.2363, -0.3440, -0.3286,\n",
            "                -0.3862, -0.3009], device='cuda:0'), max_val=tensor([0.3020, 0.3610, 0.4447, 0.2809, 0.3536, 0.2700, 0.3273, 0.3374, 0.3314,\n",
            "                0.3473], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1174/819746684.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  orig_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}.ckpt\"))\n",
            "/tmp/ipykernel_1174/819746684.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  quant_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}_quantized.ckpt\"), strict=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0643, 0.0567, 0.0619, 0.0634, 0.0559, 0.0742], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.4361, -0.4252, -0.4645, -0.4022, -0.3557, -0.5566], device='cuda:0'), max_val=tensor([0.4819, 0.2786, 0.3365, 0.4756, 0.4195, 0.5182], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0238, 0.0479, 0.0238, 0.0349, 0.0385, 0.0268, 0.0283, 0.0306, 0.0376,\n",
            "                0.0348, 0.0229, 0.0544, 0.0264, 0.0257, 0.0309, 0.0318],\n",
            "               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1785, -0.3592, -0.1783, -0.1313, -0.2890, -0.1874, -0.2124, -0.2256,\n",
            "                  -0.2237, -0.2607, -0.1719, -0.4079, -0.1961, -0.1926, -0.2320, -0.1407],\n",
            "                 device='cuda:0'), max_val=tensor([0.1650, 0.1684, 0.1558, 0.2621, 0.2807, 0.2011, 0.1596, 0.2296, 0.2819,\n",
            "                  0.2062, 0.1705, 0.1885, 0.1983, 0.1412, 0.1458, 0.2383],\n",
            "                 device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0147, 0.0124, 0.0110, 0.0137, 0.0122, 0.0167, 0.0151, 0.0134, 0.0187,\n",
            "                0.0119, 0.0129, 0.0139, 0.0134, 0.0156, 0.0125, 0.0164, 0.0167, 0.0117,\n",
            "                0.0145, 0.0177, 0.0111, 0.0122, 0.0158, 0.0132, 0.0168, 0.0125, 0.0131,\n",
            "                0.0115, 0.0166, 0.0156, 0.0178, 0.0140, 0.0135, 0.0094, 0.0214, 0.0159,\n",
            "                0.0175, 0.0164, 0.0144, 0.0133, 0.0122, 0.0109, 0.0155, 0.0168, 0.0170,\n",
            "                0.0110, 0.0162, 0.0117, 0.0177, 0.0117, 0.0186, 0.0182, 0.0144, 0.0102,\n",
            "                0.0111, 0.0114, 0.0139, 0.0129, 0.0130, 0.0110, 0.0113, 0.0101, 0.0106,\n",
            "                0.0083, 0.0180, 0.0137, 0.0090, 0.0107, 0.0216, 0.0155, 0.0136, 0.0145,\n",
            "                0.0120, 0.0130, 0.0119, 0.0128, 0.0096, 0.0100, 0.0189, 0.0113, 0.0121,\n",
            "                0.0099, 0.0154, 0.0104, 0.0104, 0.0093, 0.0118, 0.0140, 0.0215, 0.0113,\n",
            "                0.0139, 0.0124, 0.0134, 0.0180, 0.0116, 0.0162, 0.0114, 0.0154, 0.0203,\n",
            "                0.0111, 0.0127, 0.0141, 0.0137, 0.0157, 0.0099, 0.0135, 0.0184, 0.0157,\n",
            "                0.0131, 0.0130, 0.0119, 0.0092, 0.0171, 0.0141, 0.0139, 0.0174, 0.0129,\n",
            "                0.0134, 0.0120, 0.0145], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "               device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1105, -0.0743, -0.0822, -0.1026, -0.0710, -0.1080, -0.0696, -0.0972,\n",
            "                  -0.0962, -0.0844, -0.0967, -0.1040, -0.1004, -0.1173, -0.0790, -0.1214,\n",
            "                  -0.1249, -0.0858, -0.1079, -0.1330, -0.0831, -0.0918, -0.1186, -0.0989,\n",
            "                  -0.0879, -0.0941, -0.0795, -0.0827, -0.1248, -0.0873, -0.1332, -0.1052,\n",
            "                  -0.1012, -0.0682, -0.1609, -0.1193, -0.1314, -0.0878, -0.1080, -0.0917,\n",
            "                  -0.0835, -0.0817, -0.0987, -0.1262, -0.1271, -0.0823, -0.1217, -0.0879,\n",
            "                  -0.0870, -0.0877, -0.1397, -0.1365, -0.1071, -0.0763, -0.0713, -0.0851,\n",
            "                  -0.1042, -0.0964, -0.0976, -0.0711, -0.0801, -0.0719, -0.0786, -0.0549,\n",
            "                  -0.1135, -0.0713, -0.0677, -0.0760, -0.1619, -0.1093, -0.1017, -0.1089,\n",
            "                  -0.0662, -0.0977, -0.0894, -0.0857, -0.0718, -0.0750, -0.1419, -0.0743,\n",
            "                  -0.0794, -0.0739, -0.1152, -0.0713, -0.0690, -0.0694, -0.0886, -0.1049,\n",
            "                  -0.1609, -0.0732, -0.0851, -0.0790, -0.0981, -0.1350, -0.0792, -0.1215,\n",
            "                  -0.0726, -0.1152, -0.1523, -0.0832, -0.0820, -0.1054, -0.0716, -0.1176,\n",
            "                  -0.0743, -0.0791, -0.1382, -0.1177, -0.0980, -0.0957, -0.0894, -0.0692,\n",
            "                  -0.1283, -0.1061, -0.0915, -0.1303, -0.0964, -0.0852, -0.0893, -0.1087],\n",
            "                 device='cuda:0'), max_val=tensor([0.1025, 0.0933, 0.0822, 0.0856, 0.0914, 0.1251, 0.1129, 0.1005, 0.1403,\n",
            "                  0.0893, 0.0842, 0.1043, 0.0773, 0.1058, 0.0940, 0.1230, 0.1248, 0.0878,\n",
            "                  0.1087, 0.1049, 0.0790, 0.0854, 0.1048, 0.0983, 0.1263, 0.0830, 0.0985,\n",
            "                  0.0864, 0.1118, 0.1173, 0.1011, 0.1013, 0.0899, 0.0708, 0.1262, 0.0956,\n",
            "                  0.1257, 0.1230, 0.0838, 0.0994, 0.0918, 0.0767, 0.1165, 0.0796, 0.1066,\n",
            "                  0.0753, 0.1090, 0.0743, 0.1330, 0.0851, 0.0946, 0.0878, 0.1077, 0.0765,\n",
            "                  0.0832, 0.0857, 0.0788, 0.0909, 0.0777, 0.0822, 0.0844, 0.0755, 0.0795,\n",
            "                  0.0625, 0.1350, 0.1028, 0.0662, 0.0805, 0.0933, 0.1162, 0.0843, 0.0913,\n",
            "                  0.0898, 0.0813, 0.0742, 0.0959, 0.0719, 0.0639, 0.1168, 0.0846, 0.0910,\n",
            "                  0.0742, 0.0930, 0.0781, 0.0776, 0.0643, 0.0755, 0.0983, 0.1116, 0.0847,\n",
            "                  0.1041, 0.0929, 0.1006, 0.1211, 0.0871, 0.0874, 0.0855, 0.1036, 0.1351,\n",
            "                  0.0768, 0.0951, 0.0799, 0.1024, 0.1052, 0.0660, 0.1015, 0.1206, 0.1176,\n",
            "                  0.0981, 0.0973, 0.0884, 0.0690, 0.1101, 0.0889, 0.1041, 0.0970, 0.0928,\n",
            "                  0.1009, 0.0904, 0.0880], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0332, 0.0149, 0.0201, 0.0225, 0.0264, 0.0201, 0.0206, 0.0295, 0.0217,\n",
            "                0.0243, 0.0229, 0.0290, 0.0159, 0.0281, 0.0209, 0.0272, 0.0272, 0.0203,\n",
            "                0.0221, 0.0241, 0.0219, 0.0189, 0.0207, 0.0250, 0.0197, 0.0278, 0.0206,\n",
            "                0.0210, 0.0191, 0.0199, 0.0210, 0.0244, 0.0233, 0.0219, 0.0198, 0.0234,\n",
            "                0.0235, 0.0201, 0.0183, 0.0141, 0.0235, 0.0190, 0.0221, 0.0182, 0.0233,\n",
            "                0.0266, 0.0206, 0.0258, 0.0269, 0.0184, 0.0195, 0.0212, 0.0213, 0.0184,\n",
            "                0.0221, 0.0183, 0.0160, 0.0210, 0.0245, 0.0163, 0.0214, 0.0221, 0.0180,\n",
            "                0.0199, 0.0198, 0.0190, 0.0198, 0.0162, 0.0183, 0.0211, 0.0177, 0.0191,\n",
            "                0.0225, 0.0191, 0.0231, 0.0220, 0.0227, 0.0227, 0.0260, 0.0212, 0.0256,\n",
            "                0.0211, 0.0202, 0.0209], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.2487, -0.0993, -0.1511, -0.1685, -0.1928, -0.1509, -0.1546, -0.2061,\n",
            "                  -0.1567, -0.1824, -0.1716, -0.1493, -0.1193, -0.2107, -0.1217, -0.1926,\n",
            "                  -0.2038, -0.1525, -0.1559, -0.1298, -0.1260, -0.1418, -0.1243, -0.1625,\n",
            "                  -0.1247, -0.1726, -0.1544, -0.1158, -0.1415, -0.1494, -0.1573, -0.1832,\n",
            "                  -0.1746, -0.1335, -0.1352, -0.1751, -0.1760, -0.1255, -0.1375, -0.0965,\n",
            "                  -0.1760, -0.1083, -0.1659, -0.1366, -0.1595, -0.1993, -0.1333, -0.1933,\n",
            "                  -0.2019, -0.1378, -0.1347, -0.1590, -0.1597, -0.1375, -0.1658, -0.1344,\n",
            "                  -0.1161, -0.1578, -0.1841, -0.1224, -0.1448, -0.1658, -0.1349, -0.1491,\n",
            "                  -0.1484, -0.1424, -0.1482, -0.1160, -0.1218, -0.1586, -0.1017, -0.1431,\n",
            "                  -0.1684, -0.1177, -0.1652, -0.1380, -0.1699, -0.1699, -0.1574, -0.1581,\n",
            "                  -0.1923, -0.1585, -0.1330, -0.1569], device='cuda:0'), max_val=tensor([0.1573, 0.1115, 0.1479, 0.1372, 0.1981, 0.1395, 0.1379, 0.2212, 0.1629,\n",
            "                  0.1524, 0.1533, 0.2178, 0.1194, 0.0967, 0.1566, 0.2042, 0.1286, 0.1520,\n",
            "                  0.1655, 0.1804, 0.1640, 0.1143, 0.1551, 0.1874, 0.1480, 0.2085, 0.1548,\n",
            "                  0.1571, 0.1431, 0.1316, 0.1507, 0.1421, 0.1481, 0.1645, 0.1484, 0.1681,\n",
            "                  0.1310, 0.1511, 0.1360, 0.1055, 0.1184, 0.1426, 0.1446, 0.1323, 0.1745,\n",
            "                  0.1641, 0.1543, 0.1383, 0.1764, 0.1381, 0.1464, 0.1251, 0.1432, 0.1382,\n",
            "                  0.1376, 0.1374, 0.1199, 0.1302, 0.1371, 0.1207, 0.1608, 0.1591, 0.1158,\n",
            "                  0.1196, 0.1249, 0.1422, 0.1461, 0.1218, 0.1371, 0.1394, 0.1326, 0.1310,\n",
            "                  0.1683, 0.1432, 0.1734, 0.1653, 0.1270, 0.1680, 0.1953, 0.1593, 0.1700,\n",
            "                  0.1293, 0.1512, 0.1312], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0598, 0.0540, 0.0568, 0.0390, 0.0463, 0.0488, 0.0435, 0.0640, 0.0558,\n",
            "              0.0500], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.4486, -0.3670, -0.2943, -0.2551, -0.3470, -0.3656, -0.3259, -0.4801,\n",
            "                -0.3797, -0.3250], device='cuda:0'), max_val=tensor([0.3378, 0.4050, 0.4257, 0.2925, 0.3381, 0.3166, 0.3180, 0.4089, 0.4184,\n",
            "                0.3748], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0458, 0.1047, 0.0456, 0.0705, 0.0992, 0.0493], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2614, -0.7853, -0.3418, -0.3896, -0.4943, -0.3698], device='cuda:0'), max_val=tensor([0.3435, 0.4272, 0.3417, 0.5288, 0.7444, 0.3535], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0421, 0.0330, 0.0275, 0.0314, 0.0360, 0.0350, 0.0333, 0.0396, 0.0189,\n",
            "                0.0365, 0.0443, 0.0305, 0.0388, 0.0263, 0.0436, 0.0494],\n",
            "               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.3159, -0.2108, -0.2064, -0.1831, -0.2002, -0.2378, -0.1647, -0.2788,\n",
            "                  -0.1420, -0.2738, -0.1564, -0.2278, -0.2306, -0.1975, -0.1971, -0.3706],\n",
            "                 device='cuda:0'), max_val=tensor([0.2412, 0.2474, 0.1773, 0.2359, 0.2701, 0.2625, 0.2499, 0.2971, 0.1386,\n",
            "                  0.2437, 0.3322, 0.2288, 0.2912, 0.1962, 0.3268, 0.2726],\n",
            "                 device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0132, 0.0171, 0.0164, 0.0186, 0.0130, 0.0144, 0.0157, 0.0148, 0.0065,\n",
            "                0.0171, 0.0135, 0.0233, 0.0148, 0.0188, 0.0204, 0.0167, 0.0185, 0.0157,\n",
            "                0.0128, 0.0144, 0.0146, 0.0192, 0.0140, 0.0147, 0.0175, 0.0066, 0.0155,\n",
            "                0.0249, 0.0161, 0.0185, 0.0091, 0.0091, 0.0120, 0.0161, 0.0082, 0.0146,\n",
            "                0.0164, 0.0086, 0.0227, 0.0123, 0.0209, 0.0147, 0.0212, 0.0195, 0.0156,\n",
            "                0.0131, 0.0169, 0.0167, 0.0125, 0.0130, 0.0178, 0.0149, 0.0095, 0.0154,\n",
            "                0.0154, 0.0162, 0.0175, 0.0206, 0.0165, 0.0101, 0.0124, 0.0131, 0.0066,\n",
            "                0.0143, 0.0140, 0.0132, 0.0066, 0.0164, 0.0095, 0.0135, 0.0152, 0.0185,\n",
            "                0.0199, 0.0065, 0.0145, 0.0167, 0.0158, 0.0138, 0.0162, 0.0068, 0.0139,\n",
            "                0.0065, 0.0192, 0.0146, 0.0173, 0.0170, 0.0240, 0.0165, 0.0159, 0.0158,\n",
            "                0.0131, 0.0167, 0.0193, 0.0159, 0.0161, 0.0189, 0.0253, 0.0155, 0.0152,\n",
            "                0.0123, 0.0077, 0.0116, 0.0171, 0.0156, 0.0128, 0.0134, 0.0150, 0.0140,\n",
            "                0.0247, 0.0180, 0.0229, 0.0151, 0.0152, 0.0126, 0.0155, 0.0247, 0.0138,\n",
            "                0.0149, 0.0191, 0.0071], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "               device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.0862, -0.1281, -0.1221, -0.1243, -0.0941, -0.1080, -0.0925, -0.1102,\n",
            "                  -0.0479, -0.1282, -0.0967, -0.1746, -0.0909, -0.1409, -0.0797, -0.1250,\n",
            "                  -0.1367, -0.1105, -0.0959, -0.1009, -0.1022, -0.1290, -0.0837, -0.1106,\n",
            "                  -0.1312, -0.0495, -0.1163, -0.1865, -0.1208, -0.1388, -0.0683, -0.0682,\n",
            "                  -0.0897, -0.1211, -0.0587, -0.1093, -0.1230, -0.0647, -0.1242, -0.0806,\n",
            "                  -0.1247, -0.1103, -0.1588, -0.1125, -0.0933, -0.0763, -0.1270, -0.0947,\n",
            "                  -0.0915, -0.0929, -0.1068, -0.0965, -0.0714, -0.0844, -0.1153, -0.1216,\n",
            "                  -0.1309, -0.1544, -0.0844, -0.0756, -0.0926, -0.0979, -0.0497, -0.0887,\n",
            "                  -0.1047, -0.0860, -0.0494, -0.1227, -0.0716, -0.0965, -0.1128, -0.1333,\n",
            "                  -0.1016, -0.0488, -0.0900, -0.1250, -0.1185, -0.1034, -0.1212, -0.0511,\n",
            "                  -0.1037, -0.0491, -0.1438, -0.1001, -0.1020, -0.1273, -0.1171, -0.1238,\n",
            "                  -0.1132, -0.1034, -0.0965, -0.1173, -0.1449, -0.1170, -0.1126, -0.1104,\n",
            "                  -0.1896, -0.1151, -0.0873, -0.0402, -0.0572, -0.0873, -0.1281, -0.0961,\n",
            "                  -0.0947, -0.1004, -0.0996, -0.1053, -0.0846, -0.1347, -0.0869, -0.1133,\n",
            "                  -0.0890, -0.0921, -0.1161, -0.1126, -0.0841, -0.0894, -0.1430, -0.0532],\n",
            "                 device='cuda:0'), max_val=tensor([0.0987, 0.1237, 0.1229, 0.1394, 0.0978, 0.1038, 0.1181, 0.1109, 0.0485,\n",
            "                  0.1121, 0.1014, 0.0963, 0.1109, 0.1294, 0.1531, 0.1059, 0.1386, 0.1178,\n",
            "                  0.0875, 0.1077, 0.1098, 0.1437, 0.1048, 0.0934, 0.1274, 0.0478, 0.1041,\n",
            "                  0.1449, 0.0893, 0.1060, 0.0678, 0.0683, 0.0662, 0.1053, 0.0615, 0.0934,\n",
            "                  0.1226, 0.0640, 0.1702, 0.0926, 0.1568, 0.1106, 0.1106, 0.1465, 0.1171,\n",
            "                  0.0984, 0.0996, 0.1250, 0.0939, 0.0975, 0.1334, 0.1115, 0.0624, 0.1156,\n",
            "                  0.0998, 0.1001, 0.0944, 0.1053, 0.1239, 0.0751, 0.0829, 0.0872, 0.0493,\n",
            "                  0.1073, 0.0866, 0.0994, 0.0478, 0.0891, 0.0638, 0.1011, 0.1142, 0.1386,\n",
            "                  0.1490, 0.0484, 0.1087, 0.0997, 0.0900, 0.0856, 0.0991, 0.0494, 0.1045,\n",
            "                  0.0483, 0.1162, 0.1094, 0.1298, 0.1033, 0.1803, 0.1132, 0.1192, 0.1186,\n",
            "                  0.0981, 0.1250, 0.1260, 0.1189, 0.1210, 0.1419, 0.1222, 0.1166, 0.1139,\n",
            "                  0.0921, 0.0576, 0.0873, 0.1135, 0.1173, 0.0958, 0.0881, 0.1123, 0.0883,\n",
            "                  0.1853, 0.1353, 0.1720, 0.0962, 0.1139, 0.0943, 0.0958, 0.1855, 0.1032,\n",
            "                  0.1119, 0.1144, 0.0476], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0127, 0.0201, 0.0196, 0.0207, 0.0125, 0.0270, 0.0270, 0.0116, 0.0283,\n",
            "                0.0241, 0.0306, 0.0213, 0.0214, 0.0215, 0.0254, 0.0127, 0.0246, 0.0176,\n",
            "                0.0184, 0.0209, 0.0206, 0.0273, 0.0224, 0.0315, 0.0320, 0.0244, 0.0118,\n",
            "                0.0118, 0.0272, 0.0203, 0.0254, 0.0257, 0.0207, 0.0123, 0.0188, 0.0321,\n",
            "                0.0239, 0.0258, 0.0209, 0.0227, 0.0253, 0.0165, 0.0203, 0.0312, 0.0115,\n",
            "                0.0218, 0.0181, 0.0198, 0.0249, 0.0117, 0.0215, 0.0199, 0.0317, 0.0182,\n",
            "                0.0315, 0.0241, 0.0261, 0.0277, 0.0269, 0.0189, 0.0192, 0.0187, 0.0160,\n",
            "                0.0211, 0.0237, 0.0230, 0.0359, 0.0203, 0.0118, 0.0196, 0.0199, 0.0277,\n",
            "                0.0246, 0.0173, 0.0205, 0.0248, 0.0226, 0.0242, 0.0176, 0.0117, 0.0297,\n",
            "                0.0224, 0.0192, 0.0116], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.0928, -0.1509, -0.1327, -0.1380, -0.0920, -0.1495, -0.1503, -0.0869,\n",
            "                  -0.1532, -0.1259, -0.1609, -0.1397, -0.1602, -0.1294, -0.1906, -0.0932,\n",
            "                  -0.1121, -0.1322, -0.1379, -0.1564, -0.1340, -0.2050, -0.1612, -0.1543,\n",
            "                  -0.1469, -0.1832, -0.0884, -0.0882, -0.1312, -0.1520, -0.1902, -0.1929,\n",
            "                  -0.1159, -0.0925, -0.1411, -0.1979, -0.1796, -0.1938, -0.1571, -0.1518,\n",
            "                  -0.1652, -0.1167, -0.1522, -0.1749, -0.0862, -0.1635, -0.1274, -0.1489,\n",
            "                  -0.1860, -0.0858, -0.1338, -0.1418, -0.2378, -0.1216, -0.1213, -0.1482,\n",
            "                  -0.1614, -0.1515, -0.2021, -0.1417, -0.1202, -0.0995, -0.1160, -0.1583,\n",
            "                  -0.1773, -0.1574, -0.1765, -0.1323, -0.0882, -0.1470, -0.1348, -0.2076,\n",
            "                  -0.1846, -0.1220, -0.1535, -0.1859, -0.1698, -0.1818, -0.1094, -0.0873,\n",
            "                  -0.1766, -0.1683, -0.1402, -0.0873], device='cuda:0'), max_val=tensor([0.0955, 0.1174, 0.1467, 0.1550, 0.0935, 0.2024, 0.2026, 0.0843, 0.2124,\n",
            "                  0.1807, 0.2294, 0.1597, 0.1191, 0.1611, 0.1739, 0.0954, 0.1843, 0.1215,\n",
            "                  0.1259, 0.1566, 0.1548, 0.1874, 0.1679, 0.2359, 0.2401, 0.1678, 0.0860,\n",
            "                  0.0876, 0.2037, 0.1420, 0.1693, 0.1661, 0.1550, 0.0818, 0.1328, 0.2411,\n",
            "                  0.1140, 0.1575, 0.1324, 0.1702, 0.1901, 0.1236, 0.1378, 0.2341, 0.0834,\n",
            "                  0.1639, 0.1361, 0.1308, 0.1870, 0.0874, 0.1613, 0.1495, 0.2053, 0.1364,\n",
            "                  0.2363, 0.1806, 0.1959, 0.2080, 0.1633, 0.1259, 0.1442, 0.1404, 0.1203,\n",
            "                  0.1531, 0.1779, 0.1728, 0.2694, 0.1524, 0.0884, 0.1469, 0.1492, 0.1544,\n",
            "                  0.1721, 0.1300, 0.1531, 0.1696, 0.1672, 0.1602, 0.1319, 0.0876, 0.2229,\n",
            "                  0.1358, 0.1441, 0.0870], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0440, 0.0507, 0.0429, 0.0338, 0.0401, 0.0411, 0.0507, 0.0472, 0.0516,\n",
            "              0.0531], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.3298, -0.3152, -0.2296, -0.1821, -0.3007, -0.2579, -0.2816, -0.2879,\n",
            "                -0.3867, -0.2342], device='cuda:0'), max_val=tensor([0.2808, 0.3800, 0.3218, 0.2534, 0.2974, 0.3079, 0.3801, 0.3537, 0.3355,\n",
            "                0.3979], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "ResNet(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv): Conv2d(\n",
            "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0447, 0.0591, 0.0670, 0.0496, 0.0486, 0.0402, 0.0533, 0.0364, 0.0359,\n",
            "              0.0388, 0.0672, 0.0275, 0.0638, 0.0689, 0.0523, 0.0712],\n",
            "             device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "             dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.3352, -0.4435, -0.5027, -0.3326, -0.2866, -0.3006, -0.2863, -0.2730,\n",
            "                -0.2694, -0.2120, -0.5040, -0.1931, -0.4786, -0.3322, -0.3919, -0.3639],\n",
            "               device='cuda:0'), max_val=tensor([0.2882, 0.2248, 0.4188, 0.3724, 0.3647, 0.3015, 0.3997, 0.2692, 0.2336,\n",
            "                0.2910, 0.3670, 0.2059, 0.3323, 0.5171, 0.3322, 0.5343],\n",
            "               device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            "  (bn): BatchNorm2d(\n",
            "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            "  (act_fn): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0212, 0.0238, 0.0266, 0.0270, 0.0275, 0.0179, 0.0326, 0.0252, 0.0225,\n",
            "                  0.0584, 0.0249, 0.0371, 0.0252, 0.0226, 0.0225, 0.0275],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1591, -0.1785, -0.1997, -0.2022, -0.2061, -0.1295, -0.2444, -0.1892,\n",
            "                    -0.1686, -0.4378, -0.1866, -0.2553, -0.1892, -0.1698, -0.1691, -0.1553],\n",
            "                   device='cuda:0'), max_val=tensor([0.1404, 0.1654, 0.1484, 0.1948, 0.1642, 0.1342, 0.1756, 0.1856, 0.1405,\n",
            "                    0.3023, 0.1867, 0.2779, 0.1654, 0.1334, 0.1633, 0.2062],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0231, 0.0377, 0.0249, 0.0280, 0.0173, 0.0213, 0.0256, 0.0269, 0.0240,\n",
            "                  0.0295, 0.0201, 0.0308, 0.0295, 0.0244, 0.0200, 0.0292],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1733, -0.2829, -0.1646, -0.1565, -0.1136, -0.1595, -0.1922, -0.1863,\n",
            "                    -0.1697, -0.1124, -0.1298, -0.2310, -0.2214, -0.1826, -0.1411, -0.2192],\n",
            "                   device='cuda:0'), max_val=tensor([0.1735, 0.1764, 0.1865, 0.2097, 0.1296, 0.1251, 0.1700, 0.2020, 0.1799,\n",
            "                    0.2211, 0.1506, 0.1662, 0.1639, 0.1678, 0.1500, 0.1673],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0253, 0.0314, 0.0323, 0.0232, 0.0205, 0.0326, 0.0315, 0.0277, 0.0296,\n",
            "                  0.0204, 0.0236, 0.0198, 0.0197, 0.0266, 0.0186, 0.0245],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1901, -0.2204, -0.2157, -0.1741, -0.1536, -0.2446, -0.2049, -0.2075,\n",
            "                    -0.2219, -0.1532, -0.1769, -0.1472, -0.1475, -0.1988, -0.1295, -0.1820],\n",
            "                   device='cuda:0'), max_val=tensor([0.1798, 0.2358, 0.2425, 0.1529, 0.1381, 0.1474, 0.2360, 0.1985, 0.1475,\n",
            "                    0.1439, 0.1560, 0.1487, 0.1277, 0.1991, 0.1394, 0.1840],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0268, 0.0208, 0.0224, 0.0263, 0.0201, 0.0203, 0.0228, 0.0221, 0.0209,\n",
            "                  0.0184, 0.0200, 0.0266, 0.0225, 0.0211, 0.0255, 0.0181],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.2009, -0.1560, -0.1679, -0.1972, -0.1506, -0.1526, -0.1711, -0.1660,\n",
            "                    -0.1517, -0.1320, -0.1309, -0.1993, -0.1689, -0.1584, -0.1915, -0.1218],\n",
            "                   device='cuda:0'), max_val=tensor([0.1415, 0.1264, 0.1584, 0.1648, 0.1119, 0.1376, 0.1167, 0.1306, 0.1569,\n",
            "                    0.1381, 0.1498, 0.0888, 0.1151, 0.1503, 0.1394, 0.1356],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0181, 0.0258, 0.0213, 0.0168, 0.0193, 0.0170, 0.0177, 0.0215, 0.0191,\n",
            "                  0.0216, 0.0185, 0.0175, 0.0187, 0.0235, 0.0237, 0.0170],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1341, -0.1937, -0.1599, -0.1176, -0.1367, -0.1260, -0.1325, -0.1612,\n",
            "                    -0.1430, -0.1478, -0.1389, -0.1307, -0.1192, -0.1765, -0.1736, -0.1211],\n",
            "                   device='cuda:0'), max_val=tensor([0.1357, 0.1676, 0.1597, 0.1260, 0.1446, 0.1275, 0.1328, 0.1194, 0.1432,\n",
            "                    0.1622, 0.1194, 0.1312, 0.1404, 0.1515, 0.1780, 0.1272],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0211, 0.0188, 0.0199, 0.0188, 0.0168, 0.0157, 0.0200, 0.0157, 0.0168,\n",
            "                  0.0221, 0.0156, 0.0193, 0.0182, 0.0207, 0.0222, 0.0191],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1564, -0.1178, -0.1024, -0.1389, -0.1205, -0.1138, -0.1499, -0.1165,\n",
            "                    -0.1135, -0.1247, -0.1168, -0.1253, -0.1362, -0.1555, -0.1667, -0.1025],\n",
            "                   device='cuda:0'), max_val=tensor([0.1583, 0.1409, 0.1492, 0.1411, 0.1259, 0.1179, 0.1348, 0.1176, 0.1258,\n",
            "                    0.1654, 0.1174, 0.1444, 0.1181, 0.1384, 0.1221, 0.1429],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0166, 0.0170, 0.0156, 0.0190, 0.0173, 0.0167, 0.0198, 0.0168, 0.0200,\n",
            "                  0.0159, 0.0192, 0.0181, 0.0181, 0.0171, 0.0173, 0.0174, 0.0174, 0.0187,\n",
            "                  0.0208, 0.0183, 0.0168, 0.0163, 0.0174, 0.0193, 0.0210, 0.0145, 0.0154,\n",
            "                  0.0162, 0.0213, 0.0159, 0.0198, 0.0163], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1247, -0.1211, -0.1171, -0.1305, -0.1299, -0.1250, -0.1217, -0.1256,\n",
            "                    -0.0777, -0.1192, -0.1438, -0.1274, -0.1155, -0.1079, -0.1295, -0.1304,\n",
            "                    -0.1306, -0.1039, -0.1560, -0.1370, -0.1260, -0.1224, -0.1263, -0.1130,\n",
            "                    -0.1576, -0.1085, -0.1128, -0.1216, -0.1601, -0.0988, -0.1486, -0.1222],\n",
            "                   device='cuda:0'), max_val=tensor([0.1174, 0.1272, 0.1099, 0.1428, 0.1108, 0.0921, 0.1484, 0.1254, 0.1496,\n",
            "                    0.1044, 0.1399, 0.1359, 0.1360, 0.1282, 0.1217, 0.0856, 0.1117, 0.1403,\n",
            "                    0.0901, 0.1098, 0.1082, 0.1077, 0.1301, 0.1451, 0.1418, 0.1024, 0.1157,\n",
            "                    0.1154, 0.0990, 0.1195, 0.1021, 0.1099], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0141, 0.0133, 0.0136, 0.0160, 0.0147, 0.0172, 0.0135, 0.0134, 0.0152,\n",
            "                  0.0134, 0.0144, 0.0175, 0.0126, 0.0152, 0.0143, 0.0163, 0.0184, 0.0159,\n",
            "                  0.0161, 0.0140, 0.0197, 0.0186, 0.0164, 0.0147, 0.0169, 0.0166, 0.0138,\n",
            "                  0.0144, 0.0160, 0.0136, 0.0145, 0.0153], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1055, -0.0996, -0.1023, -0.1034, -0.1103, -0.1291, -0.1014, -0.1009,\n",
            "                    -0.1136, -0.0986, -0.1076, -0.1222, -0.0830, -0.1058, -0.1074, -0.1223,\n",
            "                    -0.1378, -0.1120, -0.1038, -0.0939, -0.1141, -0.1398, -0.0967, -0.1099,\n",
            "                    -0.1269, -0.1225, -0.1011, -0.1044, -0.1149, -0.1022, -0.1087, -0.1149],\n",
            "                   device='cuda:0'), max_val=tensor([0.0981, 0.0883, 0.0827, 0.1200, 0.0957, 0.1060, 0.0946, 0.0884, 0.1140,\n",
            "                    0.1008, 0.1008, 0.1314, 0.0945, 0.1141, 0.1066, 0.1112, 0.1339, 0.1195,\n",
            "                    0.1206, 0.1047, 0.1479, 0.1015, 0.1232, 0.1101, 0.1271, 0.1248, 0.1034,\n",
            "                    0.1080, 0.1203, 0.1020, 0.1045, 0.0878], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(\n",
            "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0280, 0.0313, 0.0352, 0.0305, 0.0357, 0.0333, 0.0326, 0.0371, 0.0369,\n",
            "                    0.0266, 0.0340, 0.0290, 0.0301, 0.0346, 0.0313, 0.0331, 0.0317, 0.0308,\n",
            "                    0.0316, 0.0322, 0.0370, 0.0299, 0.0419, 0.0394, 0.0347, 0.0350, 0.0253,\n",
            "                    0.0364, 0.0366, 0.0378, 0.0356, 0.0334], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                    0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "              min_val=tensor([-0.1752, -0.1228, -0.2640, -0.2019, -0.2680, -0.2496, -0.2290, -0.2780,\n",
            "                      -0.2764, -0.1996, -0.2243, -0.1865, -0.2016, -0.2594, -0.2344, -0.2482,\n",
            "                      -0.2143, -0.2281, -0.2114, -0.2413, -0.2777, -0.2168, -0.3144, -0.2958,\n",
            "                      -0.2601, -0.2048, -0.0786, -0.2732, -0.2747, -0.2836, -0.2671, -0.2506],\n",
            "                     device='cuda:0'), max_val=tensor([0.2102, 0.2344, 0.2288, 0.2289, 0.2065, 0.2150, 0.2444, 0.1935, 0.2384,\n",
            "                      0.1692, 0.2550, 0.2176, 0.2255, 0.2399, 0.2350, 0.1476, 0.2376, 0.2312,\n",
            "                      0.2373, 0.1936, 0.2326, 0.2244, 0.1474, 0.2090, 0.1787, 0.2628, 0.1898,\n",
            "                      0.1530, 0.1990, 0.2366, 0.2310, 0.2157], device='cuda:0')\n",
            "            )\n",
            "          )\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "        (1): BatchNorm2d(\n",
            "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0146, 0.0148, 0.0150, 0.0143, 0.0160, 0.0165, 0.0123, 0.0155, 0.0197,\n",
            "                  0.0168, 0.0168, 0.0134, 0.0123, 0.0163, 0.0118, 0.0157, 0.0164, 0.0171,\n",
            "                  0.0177, 0.0166, 0.0183, 0.0149, 0.0153, 0.0159, 0.0167, 0.0135, 0.0142,\n",
            "                  0.0148, 0.0144, 0.0151, 0.0176, 0.0153], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0980, -0.1109, -0.1052, -0.1051, -0.1202, -0.1041, -0.0924, -0.1152,\n",
            "                    -0.1475, -0.1262, -0.1260, -0.0946, -0.0912, -0.1091, -0.0889, -0.0929,\n",
            "                    -0.1227, -0.1196, -0.1178, -0.1182, -0.1101, -0.1103, -0.1145, -0.0855,\n",
            "                    -0.1023, -0.0952, -0.1061, -0.1109, -0.1079, -0.1000, -0.1320, -0.1145],\n",
            "                   device='cuda:0'), max_val=tensor([0.1098, 0.0979, 0.1124, 0.1070, 0.1189, 0.1238, 0.0921, 0.1161, 0.1130,\n",
            "                    0.1071, 0.0912, 0.1002, 0.0924, 0.1223, 0.0885, 0.1177, 0.1103, 0.1281,\n",
            "                    0.1331, 0.1246, 0.1374, 0.1117, 0.0970, 0.1189, 0.1251, 0.1014, 0.1066,\n",
            "                    0.1088, 0.0957, 0.1132, 0.1173, 0.1007], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0161, 0.0132, 0.0124, 0.0146, 0.0126, 0.0140, 0.0144, 0.0129, 0.0136,\n",
            "                  0.0183, 0.0125, 0.0127, 0.0133, 0.0150, 0.0138, 0.0151, 0.0162, 0.0133,\n",
            "                  0.0147, 0.0153, 0.0145, 0.0134, 0.0156, 0.0164, 0.0151, 0.0157, 0.0169,\n",
            "                  0.0138, 0.0141, 0.0136, 0.0165, 0.0142], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0886, -0.0988, -0.0912, -0.1055, -0.0927, -0.1052, -0.1077, -0.0806,\n",
            "                    -0.1013, -0.1369, -0.0884, -0.0912, -0.0895, -0.1082, -0.1032, -0.0874,\n",
            "                    -0.1112, -0.0909, -0.1105, -0.1147, -0.1087, -0.0865, -0.1167, -0.1229,\n",
            "                    -0.1130, -0.1026, -0.1266, -0.0931, -0.0956, -0.0985, -0.1059, -0.1069],\n",
            "                   device='cuda:0'), max_val=tensor([0.1211, 0.0891, 0.0932, 0.1095, 0.0944, 0.1017, 0.1042, 0.0965, 0.1023,\n",
            "                    0.0975, 0.0937, 0.0951, 0.0999, 0.1128, 0.1037, 0.1131, 0.1217, 0.1001,\n",
            "                    0.1100, 0.1004, 0.0988, 0.1007, 0.0996, 0.1225, 0.1104, 0.1176, 0.0894,\n",
            "                    0.1032, 0.1059, 0.1021, 0.1235, 0.0913], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0185, 0.0182, 0.0143, 0.0140, 0.0132, 0.0151, 0.0146, 0.0177, 0.0151,\n",
            "                  0.0172, 0.0132, 0.0144, 0.0147, 0.0122, 0.0138, 0.0135, 0.0167, 0.0144,\n",
            "                  0.0146, 0.0163, 0.0128, 0.0148, 0.0162, 0.0115, 0.0147, 0.0163, 0.0167,\n",
            "                  0.0118, 0.0132, 0.0141, 0.0153, 0.0163], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1173, -0.1115, -0.1075, -0.0988, -0.0988, -0.1131, -0.0909, -0.1326,\n",
            "                    -0.1133, -0.1054, -0.0978, -0.0974, -0.1002, -0.0890, -0.0974, -0.1010,\n",
            "                    -0.1218, -0.1080, -0.1030, -0.0987, -0.0813, -0.0845, -0.1212, -0.0819,\n",
            "                    -0.1104, -0.1220, -0.1252, -0.0884, -0.0943, -0.0864, -0.1075, -0.1024],\n",
            "                   device='cuda:0'), max_val=tensor([0.1387, 0.1366, 0.0981, 0.1047, 0.0992, 0.1027, 0.1097, 0.1013, 0.1130,\n",
            "                    0.1293, 0.0993, 0.1081, 0.1102, 0.0915, 0.1036, 0.1009, 0.1254, 0.0937,\n",
            "                    0.1092, 0.1225, 0.0963, 0.1107, 0.1083, 0.0863, 0.0905, 0.1018, 0.1080,\n",
            "                    0.0736, 0.0988, 0.1054, 0.1144, 0.1226], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0148, 0.0178, 0.0135, 0.0137, 0.0127, 0.0154, 0.0137, 0.0118, 0.0151,\n",
            "                  0.0120, 0.0136, 0.0138, 0.0131, 0.0139, 0.0128, 0.0166, 0.0122, 0.0149,\n",
            "                  0.0144, 0.0145, 0.0132, 0.0138, 0.0147, 0.0147, 0.0151, 0.0123, 0.0129,\n",
            "                  0.0153, 0.0131, 0.0125, 0.0133, 0.0138], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1110, -0.1335, -0.0951, -0.0947, -0.0953, -0.1142, -0.1000, -0.0834,\n",
            "                    -0.1134, -0.0876, -0.1016, -0.1032, -0.0859, -0.1046, -0.0813, -0.0760,\n",
            "                    -0.0888, -0.1116, -0.0967, -0.0915, -0.0991, -0.1037, -0.1096, -0.1013,\n",
            "                    -0.0940, -0.0909, -0.0876, -0.1029, -0.0983, -0.0744, -0.0904, -0.1004],\n",
            "                   device='cuda:0'), max_val=tensor([0.0908, 0.1025, 0.1015, 0.1030, 0.0852, 0.1153, 0.1025, 0.0885, 0.1021,\n",
            "                    0.0897, 0.0868, 0.0937, 0.0984, 0.0894, 0.0961, 0.1242, 0.0915, 0.1118,\n",
            "                    0.1079, 0.1084, 0.0986, 0.0863, 0.1104, 0.1100, 0.1129, 0.0925, 0.0966,\n",
            "                    0.1149, 0.0869, 0.0939, 0.1001, 0.1035], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0120, 0.0111, 0.0139, 0.0120, 0.0117, 0.0149, 0.0120, 0.0140, 0.0134,\n",
            "                  0.0112, 0.0126, 0.0124, 0.0115, 0.0119, 0.0134, 0.0116, 0.0131, 0.0117,\n",
            "                  0.0117, 0.0125, 0.0130, 0.0117, 0.0133, 0.0131, 0.0117, 0.0123, 0.0134,\n",
            "                  0.0127, 0.0123, 0.0127, 0.0128, 0.0135, 0.0107, 0.0126, 0.0126, 0.0113,\n",
            "                  0.0167, 0.0133, 0.0127, 0.0118, 0.0118, 0.0141, 0.0145, 0.0128, 0.0108,\n",
            "                  0.0131, 0.0113, 0.0118, 0.0144, 0.0119, 0.0115, 0.0109, 0.0138, 0.0121,\n",
            "                  0.0121, 0.0125, 0.0116, 0.0118, 0.0128, 0.0132, 0.0126, 0.0141, 0.0122,\n",
            "                  0.0140], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0853, -0.0833, -0.1042, -0.0901, -0.0847, -0.1010, -0.0858, -0.0949,\n",
            "                    -0.1006, -0.0797, -0.0947, -0.0927, -0.0837, -0.0891, -0.1008, -0.0853,\n",
            "                    -0.0836, -0.0877, -0.0875, -0.0936, -0.0913, -0.0846, -0.1000, -0.0980,\n",
            "                    -0.0876, -0.0842, -0.1005, -0.0953, -0.0736, -0.0949, -0.0963, -0.1016,\n",
            "                    -0.0714, -0.0942, -0.0739, -0.0850, -0.1254, -0.0905, -0.0952, -0.0821,\n",
            "                    -0.0813, -0.0851, -0.1050, -0.0963, -0.0805, -0.0904, -0.0784, -0.0883,\n",
            "                    -0.1078, -0.0861, -0.0807, -0.0821, -0.1036, -0.0909, -0.0882, -0.0771,\n",
            "                    -0.0859, -0.0817, -0.0807, -0.0933, -0.0927, -0.0832, -0.0912, -0.1047],\n",
            "                   device='cuda:0'), max_val=tensor([0.0899, 0.0835, 0.0904, 0.0816, 0.0878, 0.1119, 0.0903, 0.1049, 0.0861,\n",
            "                    0.0839, 0.0919, 0.0927, 0.0864, 0.0826, 0.0803, 0.0870, 0.0981, 0.0829,\n",
            "                    0.0764, 0.0939, 0.0974, 0.0875, 0.0923, 0.0789, 0.0878, 0.0924, 0.0939,\n",
            "                    0.0873, 0.0925, 0.0955, 0.0907, 0.0929, 0.0800, 0.0897, 0.0944, 0.0769,\n",
            "                    0.0858, 0.0997, 0.0937, 0.0888, 0.0886, 0.1058, 0.1088, 0.0951, 0.0813,\n",
            "                    0.0980, 0.0851, 0.0767, 0.0908, 0.0891, 0.0864, 0.0760, 0.1026, 0.0904,\n",
            "                    0.0911, 0.0939, 0.0868, 0.0883, 0.0959, 0.0993, 0.0946, 0.1056, 0.0855,\n",
            "                    0.0820], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0107, 0.0086, 0.0103, 0.0101, 0.0116, 0.0104, 0.0111, 0.0094, 0.0122,\n",
            "                  0.0102, 0.0103, 0.0107, 0.0126, 0.0098, 0.0125, 0.0116, 0.0107, 0.0096,\n",
            "                  0.0117, 0.0109, 0.0118, 0.0139, 0.0093, 0.0104, 0.0105, 0.0097, 0.0106,\n",
            "                  0.0103, 0.0124, 0.0106, 0.0094, 0.0110, 0.0106, 0.0100, 0.0095, 0.0107,\n",
            "                  0.0102, 0.0098, 0.0113, 0.0107, 0.0105, 0.0102, 0.0092, 0.0110, 0.0104,\n",
            "                  0.0090, 0.0107, 0.0105, 0.0102, 0.0104, 0.0106, 0.0135, 0.0096, 0.0116,\n",
            "                  0.0118, 0.0134, 0.0119, 0.0145, 0.0117, 0.0100, 0.0131, 0.0109, 0.0097,\n",
            "                  0.0103], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0788, -0.0612, -0.0770, -0.0757, -0.0868, -0.0778, -0.0694, -0.0666,\n",
            "                    -0.0919, -0.0737, -0.0761, -0.0767, -0.0942, -0.0666, -0.0937, -0.0852,\n",
            "                    -0.0800, -0.0721, -0.0874, -0.0817, -0.0799, -0.0834, -0.0699, -0.0779,\n",
            "                    -0.0789, -0.0689, -0.0766, -0.0730, -0.0927, -0.0795, -0.0666, -0.0812,\n",
            "                    -0.0741, -0.0712, -0.0695, -0.0804, -0.0743, -0.0629, -0.0845, -0.0698,\n",
            "                    -0.0788, -0.0748, -0.0678, -0.0828, -0.0777, -0.0570, -0.0801, -0.0667,\n",
            "                    -0.0710, -0.0680, -0.0785, -0.1009, -0.0720, -0.0868, -0.0769, -0.0728,\n",
            "                    -0.0702, -0.1088, -0.0764, -0.0695, -0.0980, -0.0740, -0.0718, -0.0772],\n",
            "                   device='cuda:0'), max_val=tensor([0.0803, 0.0642, 0.0743, 0.0727, 0.0739, 0.0690, 0.0832, 0.0704, 0.0908,\n",
            "                    0.0764, 0.0769, 0.0803, 0.0875, 0.0735, 0.0748, 0.0868, 0.0705, 0.0722,\n",
            "                    0.0777, 0.0711, 0.0882, 0.1041, 0.0646, 0.0745, 0.0653, 0.0724, 0.0798,\n",
            "                    0.0770, 0.0718, 0.0782, 0.0709, 0.0826, 0.0797, 0.0748, 0.0715, 0.0671,\n",
            "                    0.0764, 0.0734, 0.0779, 0.0805, 0.0757, 0.0767, 0.0692, 0.0754, 0.0727,\n",
            "                    0.0678, 0.0586, 0.0786, 0.0763, 0.0782, 0.0792, 0.0826, 0.0684, 0.0762,\n",
            "                    0.0883, 0.1002, 0.0894, 0.0885, 0.0875, 0.0751, 0.0868, 0.0817, 0.0730,\n",
            "                    0.0726], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(\n",
            "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0267, 0.0252, 0.0246, 0.0255, 0.0225, 0.0224, 0.0284, 0.0263, 0.0235,\n",
            "                    0.0267, 0.0233, 0.0240, 0.0247, 0.0210, 0.0254, 0.0239, 0.0234, 0.0235,\n",
            "                    0.0220, 0.0248, 0.0238, 0.0257, 0.0232, 0.0235, 0.0278, 0.0230, 0.0225,\n",
            "                    0.0239, 0.0257, 0.0236, 0.0234, 0.0227, 0.0297, 0.0239, 0.0236, 0.0251,\n",
            "                    0.0255, 0.0258, 0.0284, 0.0254, 0.0254, 0.0246, 0.0224, 0.0236, 0.0259,\n",
            "                    0.0227, 0.0243, 0.0216, 0.0276, 0.0250, 0.0276, 0.0294, 0.0245, 0.0220,\n",
            "                    0.0257, 0.0259, 0.0240, 0.0246, 0.0252, 0.0239, 0.0261, 0.0239, 0.0239,\n",
            "                    0.0264], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                   dtype=torch.int32)\n",
            "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "              min_val=tensor([-0.2001, -0.1894, -0.1487, -0.1550, -0.1690, -0.1664, -0.1726, -0.1973,\n",
            "                      -0.1741, -0.1643, -0.1708, -0.1796, -0.1735, -0.1572, -0.1692, -0.1769,\n",
            "                      -0.1400, -0.1537, -0.1249, -0.1857, -0.1788, -0.1927, -0.1741, -0.1540,\n",
            "                      -0.1436, -0.1468, -0.1689, -0.1735, -0.1931, -0.1628, -0.1645, -0.1662,\n",
            "                      -0.1828, -0.1666, -0.1773, -0.1660, -0.1380, -0.1933, -0.2127, -0.1903,\n",
            "                      -0.1909, -0.1807, -0.1680, -0.1770, -0.1361, -0.1702, -0.1819, -0.1622,\n",
            "                      -0.1888, -0.1578, -0.1943, -0.2067, -0.1841, -0.1653, -0.1681, -0.1942,\n",
            "                      -0.1690, -0.1846, -0.1891, -0.1772, -0.1762, -0.1751, -0.1795, -0.1618],\n",
            "                     device='cuda:0'), max_val=tensor([0.1416, 0.1781, 0.1841, 0.1913, 0.1638, 0.1683, 0.2132, 0.1743, 0.1761,\n",
            "                      0.1999, 0.1745, 0.1614, 0.1853, 0.1244, 0.1907, 0.1792, 0.1753, 0.1759,\n",
            "                      0.1650, 0.1646, 0.1747, 0.1681, 0.1716, 0.1761, 0.2085, 0.1726, 0.1635,\n",
            "                      0.1791, 0.1718, 0.1769, 0.1757, 0.1705, 0.2230, 0.1790, 0.1771, 0.1882,\n",
            "                      0.1910, 0.1872, 0.1572, 0.1394, 0.1565, 0.1843, 0.1585, 0.1419, 0.1942,\n",
            "                      0.1633, 0.1714, 0.1596, 0.2068, 0.1876, 0.2067, 0.2202, 0.1463, 0.1400,\n",
            "                      0.1930, 0.1634, 0.1803, 0.1590, 0.1733, 0.1795, 0.1955, 0.1794, 0.1786,\n",
            "                      0.1984], device='cuda:0')\n",
            "            )\n",
            "          )\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "        (1): BatchNorm2d(\n",
            "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0123, 0.0095, 0.0093, 0.0121, 0.0110, 0.0102, 0.0125, 0.0110, 0.0091,\n",
            "                  0.0109, 0.0115, 0.0112, 0.0101, 0.0128, 0.0129, 0.0093, 0.0112, 0.0107,\n",
            "                  0.0100, 0.0126, 0.0128, 0.0111, 0.0120, 0.0105, 0.0101, 0.0115, 0.0108,\n",
            "                  0.0122, 0.0113, 0.0106, 0.0117, 0.0110, 0.0120, 0.0103, 0.0128, 0.0098,\n",
            "                  0.0106, 0.0105, 0.0099, 0.0095, 0.0100, 0.0116, 0.0095, 0.0103, 0.0110,\n",
            "                  0.0089, 0.0112, 0.0097, 0.0111, 0.0106, 0.0094, 0.0119, 0.0131, 0.0110,\n",
            "                  0.0132, 0.0105, 0.0098, 0.0097, 0.0122, 0.0099, 0.0118, 0.0105, 0.0101,\n",
            "                  0.0120], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0925, -0.0653, -0.0697, -0.0910, -0.0823, -0.0759, -0.0925, -0.0825,\n",
            "                    -0.0650, -0.0820, -0.0779, -0.0730, -0.0761, -0.0964, -0.0970, -0.0699,\n",
            "                    -0.0837, -0.0795, -0.0747, -0.0733, -0.0961, -0.0834, -0.0901, -0.0784,\n",
            "                    -0.0741, -0.0863, -0.0808, -0.0915, -0.0845, -0.0795, -0.0866, -0.0829,\n",
            "                    -0.0756, -0.0773, -0.0959, -0.0733, -0.0774, -0.0703, -0.0670, -0.0657,\n",
            "                    -0.0744, -0.0781, -0.0693, -0.0773, -0.0821, -0.0610, -0.0672, -0.0670,\n",
            "                    -0.0809, -0.0737, -0.0703, -0.0876, -0.0985, -0.0697, -0.0990, -0.0785,\n",
            "                    -0.0735, -0.0636, -0.0916, -0.0746, -0.0883, -0.0784, -0.0711, -0.0902],\n",
            "                   device='cuda:0'), max_val=tensor([0.0709, 0.0715, 0.0656, 0.0838, 0.0726, 0.0762, 0.0935, 0.0687, 0.0680,\n",
            "                    0.0681, 0.0864, 0.0843, 0.0666, 0.0854, 0.0774, 0.0672, 0.0670, 0.0802,\n",
            "                    0.0744, 0.0946, 0.0871, 0.0832, 0.0797, 0.0710, 0.0759, 0.0808, 0.0796,\n",
            "                    0.0744, 0.0828, 0.0633, 0.0875, 0.0759, 0.0904, 0.0652, 0.0727, 0.0603,\n",
            "                    0.0794, 0.0786, 0.0740, 0.0715, 0.0748, 0.0870, 0.0711, 0.0761, 0.0819,\n",
            "                    0.0665, 0.0838, 0.0730, 0.0832, 0.0793, 0.0635, 0.0893, 0.0759, 0.0828,\n",
            "                    0.0764, 0.0730, 0.0663, 0.0728, 0.0908, 0.0670, 0.0843, 0.0693, 0.0760,\n",
            "                    0.0783], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0096, 0.0100, 0.0130, 0.0083, 0.0093, 0.0080, 0.0081, 0.0085, 0.0079,\n",
            "                  0.0098, 0.0100, 0.0084, 0.0090, 0.0087, 0.0083, 0.0084, 0.0095, 0.0090,\n",
            "                  0.0092, 0.0085, 0.0076, 0.0089, 0.0104, 0.0087, 0.0075, 0.0084, 0.0086,\n",
            "                  0.0076, 0.0103, 0.0096, 0.0083, 0.0090, 0.0092, 0.0084, 0.0089, 0.0078,\n",
            "                  0.0084, 0.0093, 0.0085, 0.0090, 0.0106, 0.0094, 0.0077, 0.0091, 0.0095,\n",
            "                  0.0085, 0.0090, 0.0091, 0.0117, 0.0092, 0.0084, 0.0106, 0.0087, 0.0085,\n",
            "                  0.0094, 0.0084, 0.0098, 0.0102, 0.0093, 0.0089, 0.0082, 0.0099, 0.0087,\n",
            "                  0.0079], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0716, -0.0645, -0.0971, -0.0579, -0.0677, -0.0569, -0.0571, -0.0638,\n",
            "                    -0.0595, -0.0732, -0.0680, -0.0629, -0.0676, -0.0646, -0.0621, -0.0591,\n",
            "                    -0.0640, -0.0569, -0.0589, -0.0628, -0.0560, -0.0564, -0.0652, -0.0652,\n",
            "                    -0.0553, -0.0629, -0.0645, -0.0558, -0.0771, -0.0716, -0.0543, -0.0662,\n",
            "                    -0.0566, -0.0612, -0.0600, -0.0583, -0.0630, -0.0649, -0.0595, -0.0676,\n",
            "                    -0.0747, -0.0702, -0.0533, -0.0643, -0.0640, -0.0606, -0.0527, -0.0682,\n",
            "                    -0.0876, -0.0693, -0.0609, -0.0736, -0.0554, -0.0568, -0.0624, -0.0501,\n",
            "                    -0.0666, -0.0756, -0.0571, -0.0668, -0.0582, -0.0745, -0.0650, -0.0590],\n",
            "                   device='cuda:0'), max_val=tensor([0.0722, 0.0747, 0.0655, 0.0619, 0.0701, 0.0602, 0.0609, 0.0628, 0.0567,\n",
            "                    0.0677, 0.0753, 0.0562, 0.0574, 0.0651, 0.0587, 0.0629, 0.0711, 0.0672,\n",
            "                    0.0689, 0.0634, 0.0570, 0.0665, 0.0776, 0.0607, 0.0560, 0.0568, 0.0618,\n",
            "                    0.0572, 0.0773, 0.0717, 0.0624, 0.0672, 0.0687, 0.0628, 0.0671, 0.0568,\n",
            "                    0.0592, 0.0698, 0.0640, 0.0607, 0.0794, 0.0706, 0.0577, 0.0681, 0.0716,\n",
            "                    0.0641, 0.0672, 0.0627, 0.0663, 0.0624, 0.0631, 0.0793, 0.0649, 0.0637,\n",
            "                    0.0702, 0.0632, 0.0731, 0.0766, 0.0695, 0.0671, 0.0618, 0.0736, 0.0646,\n",
            "                    0.0578], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0090, 0.0080, 0.0081, 0.0069, 0.0085, 0.0080, 0.0074, 0.0077, 0.0082,\n",
            "                  0.0086, 0.0102, 0.0093, 0.0077, 0.0095, 0.0074, 0.0080, 0.0090, 0.0105,\n",
            "                  0.0100, 0.0072, 0.0084, 0.0078, 0.0084, 0.0089, 0.0083, 0.0077, 0.0108,\n",
            "                  0.0084, 0.0082, 0.0078, 0.0094, 0.0086, 0.0091, 0.0088, 0.0071, 0.0075,\n",
            "                  0.0080, 0.0092, 0.0071, 0.0078, 0.0079, 0.0092, 0.0079, 0.0071, 0.0089,\n",
            "                  0.0088, 0.0075, 0.0083, 0.0068, 0.0082, 0.0067, 0.0083, 0.0077, 0.0072,\n",
            "                  0.0083, 0.0081, 0.0084, 0.0067, 0.0086, 0.0080, 0.0086, 0.0076, 0.0076,\n",
            "                  0.0094], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0641, -0.0465, -0.0607, -0.0516, -0.0640, -0.0572, -0.0552, -0.0549,\n",
            "                    -0.0543, -0.0575, -0.0670, -0.0625, -0.0575, -0.0709, -0.0550, -0.0602,\n",
            "                    -0.0679, -0.0758, -0.0641, -0.0540, -0.0615, -0.0583, -0.0624, -0.0666,\n",
            "                    -0.0584, -0.0578, -0.0811, -0.0631, -0.0613, -0.0561, -0.0703, -0.0644,\n",
            "                    -0.0606, -0.0609, -0.0471, -0.0562, -0.0601, -0.0612, -0.0533, -0.0587,\n",
            "                    -0.0591, -0.0691, -0.0538, -0.0517, -0.0670, -0.0658, -0.0514, -0.0598,\n",
            "                    -0.0513, -0.0613, -0.0501, -0.0625, -0.0514, -0.0485, -0.0578, -0.0609,\n",
            "                    -0.0578, -0.0503, -0.0585, -0.0572, -0.0642, -0.0563, -0.0570, -0.0708],\n",
            "                   device='cuda:0'), max_val=tensor([0.0674, 0.0604, 0.0476, 0.0489, 0.0541, 0.0603, 0.0537, 0.0575, 0.0615,\n",
            "                    0.0646, 0.0766, 0.0701, 0.0508, 0.0546, 0.0554, 0.0533, 0.0645, 0.0791,\n",
            "                    0.0753, 0.0509, 0.0629, 0.0558, 0.0627, 0.0616, 0.0620, 0.0540, 0.0665,\n",
            "                    0.0615, 0.0618, 0.0587, 0.0674, 0.0579, 0.0685, 0.0658, 0.0534, 0.0538,\n",
            "                    0.0558, 0.0690, 0.0458, 0.0527, 0.0563, 0.0572, 0.0595, 0.0531, 0.0668,\n",
            "                    0.0602, 0.0565, 0.0625, 0.0456, 0.0616, 0.0505, 0.0594, 0.0581, 0.0538,\n",
            "                    0.0621, 0.0546, 0.0633, 0.0499, 0.0645, 0.0599, 0.0637, 0.0571, 0.0543,\n",
            "                    0.0619], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0082, 0.0083, 0.0077, 0.0075, 0.0078, 0.0072, 0.0073, 0.0082, 0.0090,\n",
            "                  0.0078, 0.0085, 0.0080, 0.0075, 0.0099, 0.0081, 0.0080, 0.0084, 0.0086,\n",
            "                  0.0079, 0.0076, 0.0074, 0.0072, 0.0083, 0.0073, 0.0083, 0.0079, 0.0078,\n",
            "                  0.0079, 0.0086, 0.0081, 0.0080, 0.0083, 0.0083, 0.0080, 0.0086, 0.0078,\n",
            "                  0.0084, 0.0085, 0.0084, 0.0078, 0.0072, 0.0079, 0.0080, 0.0083, 0.0085,\n",
            "                  0.0072, 0.0076, 0.0082, 0.0073, 0.0083, 0.0074, 0.0078, 0.0080, 0.0073,\n",
            "                  0.0075, 0.0076, 0.0083, 0.0075, 0.0077, 0.0080, 0.0081, 0.0091, 0.0088,\n",
            "                  0.0072], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0556, -0.0623, -0.0525, -0.0560, -0.0589, -0.0540, -0.0538, -0.0508,\n",
            "                    -0.0506, -0.0510, -0.0530, -0.0536, -0.0490, -0.0633, -0.0555, -0.0535,\n",
            "                    -0.0571, -0.0565, -0.0521, -0.0501, -0.0507, -0.0529, -0.0508, -0.0543,\n",
            "                    -0.0451, -0.0584, -0.0573, -0.0516, -0.0564, -0.0535, -0.0597, -0.0483,\n",
            "                    -0.0484, -0.0583, -0.0489, -0.0510, -0.0519, -0.0516, -0.0506, -0.0582,\n",
            "                    -0.0504, -0.0535, -0.0568, -0.0502, -0.0466, -0.0480, -0.0463, -0.0531,\n",
            "                    -0.0546, -0.0616, -0.0545, -0.0504, -0.0602, -0.0497, -0.0520, -0.0567,\n",
            "                    -0.0549, -0.0556, -0.0575, -0.0510, -0.0502, -0.0593, -0.0521, -0.0533],\n",
            "                   device='cuda:0'), max_val=tensor([0.0618, 0.0610, 0.0580, 0.0509, 0.0568, 0.0532, 0.0550, 0.0618, 0.0677,\n",
            "                    0.0585, 0.0636, 0.0597, 0.0565, 0.0739, 0.0609, 0.0598, 0.0632, 0.0645,\n",
            "                    0.0590, 0.0570, 0.0553, 0.0538, 0.0623, 0.0549, 0.0621, 0.0595, 0.0584,\n",
            "                    0.0591, 0.0646, 0.0608, 0.0549, 0.0622, 0.0620, 0.0598, 0.0646, 0.0585,\n",
            "                    0.0627, 0.0634, 0.0633, 0.0570, 0.0536, 0.0596, 0.0599, 0.0626, 0.0639,\n",
            "                    0.0539, 0.0569, 0.0613, 0.0527, 0.0624, 0.0554, 0.0586, 0.0603, 0.0549,\n",
            "                    0.0563, 0.0529, 0.0622, 0.0562, 0.0548, 0.0600, 0.0611, 0.0682, 0.0660,\n",
            "                    0.0540], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
            "  (fc): Linear(\n",
            "    in_features=64, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0537, 0.0592, 0.0575, 0.0562, 0.0563, 0.0639, 0.0699, 0.0684, 0.0637,\n",
            "              0.0632], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.3420, -0.2935, -0.2855, -0.2702, -0.3181, -0.3129, -0.3404, -0.3181,\n",
            "                -0.3342, -0.3082], device='cuda:0'), max_val=tensor([0.4027, 0.4438, 0.4310, 0.4217, 0.4219, 0.4792, 0.5242, 0.5130, 0.4778,\n",
            "                0.4737], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "\n",
            "Model: LeNet5 with relu\n",
            "Original Accuracy: 0.6583%\n",
            "Quantized Accuracy: 0.6619%\n",
            "\n",
            "Model: LeNet5 with hardtanh\n",
            "Original Accuracy: 0.6388%\n",
            "Quantized Accuracy: 0.6391%\n",
            "\n",
            "Model: LeNet5 with relu6\n",
            "Original Accuracy: 0.6435%\n",
            "Quantized Accuracy: 0.6421%\n",
            "\n",
            "Model: ResNet20 with relu\n",
            "Original Accuracy: 0.8663%\n",
            "Quantized Accuracy: 0.8509%\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for model_class in models:\n",
        "    for activation in activations:\n",
        "        if model_class != LeNet5 and (model_class == ResNet18 or activation != \"relu\"):\n",
        "            continue\n",
        "\n",
        "        key = (model_class.__name__, activation)\n",
        "        orig_model = model_class(activation=activation).to(cuda_device)\n",
        "        quant_model = model_class(activation=activation, quantize=True).to(cuda_device)\n",
        "        configure_qat(quant_model)\n",
        "        \n",
        "        orig_model.eval()\n",
        "        quant_model.eval()\n",
        "        orig_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}.ckpt\"))\n",
        "        quant_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}_quantized.ckpt\"), strict=True)\n",
        "\n",
        "        print(quant_model)\n",
        "        _, orig_acc = evaluate_model(orig_model)\n",
        "        _, quant_acc = evaluate_model(quant_model)\n",
        "\n",
        "        results[key] = {\n",
        "            \"orig_acc\": orig_acc,\n",
        "            \"quant_acc\": quant_acc,\n",
        "        }\n",
        "\n",
        "\n",
        "for key, result in results.items():\n",
        "    print(f\"\\nModel: {key[0]} with {key[1]}\")\n",
        "    print(f\"Original Accuracy: {result['orig_acc']:.4f}%\")\n",
        "    print(f\"Quantized Accuracy: {result['quant_acc']:.4f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
