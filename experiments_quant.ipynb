{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "1BSghSI2PI-B",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.quantization\n",
        "from torch.quantization import FakeQuantize, QConfig\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "gW11Upt5SLCs"
      },
      "outputs": [],
      "source": [
        "def get_activation_function(activation: str):\n",
        "    if activation == 'relu':\n",
        "        return nn.ReLU(inplace=True)\n",
        "    elif activation == 'hardtanh':\n",
        "        return nn.Hardtanh(inplace=True)\n",
        "    elif activation == 'relu6':\n",
        "        return nn.ReLU6(inplace=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported activation: %s\" % activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "JiqEVDVbPI-C"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation=\"relu\", quantize=False):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.quantize = quantize\n",
        "        if quantize:\n",
        "            self.quant = torch.quantization.QuantStub()\n",
        "            self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=5),\n",
        "            get_activation_function(activation),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            get_activation_function(activation),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(400, 120), get_activation_function(activation)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(120, 84), get_activation_function(activation)\n",
        "        )\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.quantize:\n",
        "            x = self.quant(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        if self.quantize:\n",
        "            x = self.dequant(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "MbSCoDqF9J9j"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_channels: int, out_channels: int, stride: int = 1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_channels: int, out_channels: int, stride: int = 1) -> nn.Conv2d:\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, stride=1, downsample=None, activation=\"relu\"\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.act_fn1 = get_activation_function(activation)\n",
        "\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "        # Remember to use two independent ReLU for layer fusion.\n",
        "        self.act_fn2 = get_activation_function(activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # Use FloatFunctional for addition for quantization compatibility\n",
        "        out = self.skip_add.add(residual, out)\n",
        "        out = self.act_fn2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "A7hEgTOZ4W6B"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        num_classes=10,\n",
        "        activation=\"relu\",\n",
        "        initial_channels=16,\n",
        "        quantize=False,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.quantize = quantize\n",
        "        if quantize:\n",
        "            self.quant = torch.quantization.QuantStub()\n",
        "            self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        self.in_channels = initial_channels\n",
        "\n",
        "        self.conv = conv3x3(3, initial_channels)\n",
        "        self.bn = nn.BatchNorm2d(initial_channels)\n",
        "        self.act_fn = get_activation_function(activation)\n",
        "\n",
        "        self.layer1 = self.make_layer(\n",
        "            block, initial_channels, layers[0], activation=activation\n",
        "        )\n",
        "        self.layer2 = self.make_layer(\n",
        "            block, initial_channels * 2, layers[1], stride=2, activation=activation\n",
        "        )\n",
        "        self.layer3 = self.make_layer(\n",
        "            block, initial_channels * 4, layers[2], stride=2, activation=activation\n",
        "        )\n",
        "        if len(layers) == 4:\n",
        "            self.layer4 = self.make_layer(\n",
        "                block, initial_channels * 8, layers[3], stride=2, activation=activation\n",
        "            )\n",
        "        else:\n",
        "            self.layer4 = None\n",
        "\n",
        "        pool_size = 8 if initial_channels == 16 else 4\n",
        "        fc_in_features = (\n",
        "            initial_channels * 4 if len(layers) == 3 else initial_channels * 8\n",
        "        )\n",
        "        self.avg_pool = nn.AvgPool2d(pool_size)\n",
        "        self.fc = nn.Linear(fc_in_features, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1, activation=\"relu\"):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.in_channels, out_channels, stride, downsample, activation)\n",
        "        )\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels, activation=activation))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def create_fused_model(self):\n",
        "        fused_model = copy.deepcopy(self)\n",
        "        fused_model.train()\n",
        "        fused_model = torch.quantization.fuse_modules(\n",
        "            fused_model, [[\"conv\", \"bn\", \"act_fn\"]], inplace=True\n",
        "        )\n",
        "\n",
        "        for module_name, module in fused_model.named_children():\n",
        "            if \"layer\" in module_name:\n",
        "                for basic_block_name, basic_block in module.named_children():\n",
        "                    torch.quantization.fuse_modules(\n",
        "                        basic_block,\n",
        "                        [[\"conv1\", \"bn1\", \"act_fn1\"], [\"conv2\", \"bn2\"], [\"skip_add\", \"act_fn2\"]],\n",
        "                        inplace=True,\n",
        "                    )\n",
        "                    for sub_block_name, sub_block in basic_block.named_children():\n",
        "                        if sub_block_name == \"downsample\":\n",
        "                            torch.quantization.fuse_modules(\n",
        "                                sub_block, [[\"0\", \"1\"]], inplace=True\n",
        "                            )\n",
        "        \n",
        "        return fused_model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.quantize:\n",
        "            x = self.quant(x)\n",
        "\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.act_fn(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        if self.layer4:\n",
        "            out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        if self.quantize:\n",
        "            x = self.dequant(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "E71eqKTH93QE"
      },
      "outputs": [],
      "source": [
        "def ResNet20(num_classes=10, activation='relu', quantize=False):\n",
        "    return ResNet(ResidualBlock, [3, 3, 3], num_classes, activation, initial_channels=16, quantize=quantize)\n",
        "\n",
        "def ResNet18(num_classes=10, activation='relu', quantize=False):\n",
        "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes, activation, initial_channels=64, quantize=quantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaT5BVpE3OD"
      },
      "source": [
        "**Обучение моделей**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF-xsDMjPI-E"
      },
      "outputs": [],
      "source": [
        "cuda_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Quantize-converted models can be run only on CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "NUM_EPOCHS = 120\n",
        "BATCH_SIZE = 128\n",
        "LR = 1e-3\n",
        "NUM_CLASSES = 10\n",
        "QAT_EPOCHS = 30\n",
        "QAT_LR = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiWTiFwfPI-E",
        "outputId": "39bd1ebc-fccb-49e2-e1f6-4ff16bbf1fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data',\n",
        "                                 train=True,\n",
        "                                 transform=train_transform,\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='data',\n",
        "                                train=False,\n",
        "                                transform=test_transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=8,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         num_workers=8,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "KO7_sP5cPI-E"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, device=cuda_device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gA3OchQPI-E"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    learning_rate,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    writer=None,\n",
        "    model_name=\"model\",\n",
        "    device=cuda_device,\n",
        "):\n",
        "    patience = 15\n",
        "    cur_idle = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=[70, 100, 130], gamma=0.1, last_epoch=-1\n",
        "    )\n",
        "\n",
        "    eval_loss, eval_accuracy = evaluate_model(\n",
        "        model=model, device=device, criterion=criterion\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"[{}] Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n",
        "            model_name, 0, eval_loss, eval_accuracy\n",
        "        )\n",
        "    )\n",
        "    best_eval_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        eval_loss, eval_accuracy = evaluate_model(\n",
        "            model=model, device=device, criterion=criterion\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "        if writer:\n",
        "            writer.add_scalar(f\"TrainLoss/{model_name}\", train_loss, epoch)\n",
        "            writer.add_scalar(f\"TrainAcc/{model_name}\", train_accuracy, epoch)\n",
        "            writer.add_scalar(f\"EvalLoss/{model_name}\", eval_loss, epoch)\n",
        "            writer.add_scalar(f\"EvalAcc/{model_name}\", eval_accuracy, epoch)\n",
        "\n",
        "        print(\n",
        "            \"[{}] Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n",
        "                model_name,\n",
        "                epoch + 1,\n",
        "                train_loss,\n",
        "                train_accuracy,\n",
        "                eval_loss,\n",
        "                eval_accuracy,\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        cur_idle += 1\n",
        "        if eval_accuracy > best_eval_acc:\n",
        "            best_eval_acc = eval_accuracy\n",
        "            torch.save(model.state_dict(), f\"checkpoint/best_{model_name}.ckpt\")\n",
        "            cur_idle = 0\n",
        "        elif cur_idle >= patience:\n",
        "            print(\"Early stopping was triggered!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "Sm62KPslHekH"
      },
      "outputs": [],
      "source": [
        "def train_orig_model(model_class, activation):\n",
        "    model_name = f\"{model_class.__name__}_{activation}\"\n",
        "    model = model_class(activation=activation).to(cuda_device)\n",
        "    writer = SummaryWriter(f\"runs/{model_name}\")\n",
        "\n",
        "    train_model(\n",
        "        model,\n",
        "        learning_rate=LR,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        writer=writer,\n",
        "        model_name=model_name,\n",
        "        device=cuda_device,\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    torch.save(model.state_dict(), f\"checkpoint/{model_name}.ckpt\")\n",
        "    writer.close()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configure_qat(model, activation_bitwidth=4, weight_bitwidth=4):\n",
        "    # Fake quantizer for activations\n",
        "    fq_activation = FakeQuantize.with_args(\n",
        "        observer=torch.quantization.MovingAverageMinMaxObserver.with_args(\n",
        "            quant_min=0,\n",
        "            quant_max=2**activation_bitwidth - 1,\n",
        "            dtype=torch.quint8,\n",
        "            qscheme=torch.per_tensor_affine,\n",
        "            reduce_range=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Fake quantizer for weights\n",
        "    fq_weights = FakeQuantize.with_args(\n",
        "        observer=torch.quantization.MovingAveragePerChannelMinMaxObserver.with_args(\n",
        "            quant_min=-(2**weight_bitwidth) // 2,\n",
        "            quant_max=(2**weight_bitwidth) // 2 - 1,\n",
        "            dtype=torch.qint8,\n",
        "            qscheme=torch.per_channel_symmetric,\n",
        "            reduce_range=False,\n",
        "            ch_axis=0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # We don't want non-activation layers to quantize it's output\n",
        "    # (for example, Conv2d)\n",
        "    # Because it will harm subsequent Activation performance\n",
        "    # (other solution will be to fuse Activation with previous layer\n",
        "    # but pytorch can't fuse with anything, except for nn.ReLU)\n",
        "    weight_only_qconfig = QConfig(\n",
        "        activation=torch.quantization.NoopObserver.with_args(dtype=torch.float32),\n",
        "        weight=fq_weights,\n",
        "    )\n",
        "    \n",
        "    activation_qconfig = QConfig(activation=fq_activation, weight=fq_weights)\n",
        "    \n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Hardtanh, nn.ReLU6, nn.ReLU, torch.quantization.QuantStub, torch.quantization.DeQuantStub)):\n",
        "            module.qconfig = activation_qconfig\n",
        "        else:\n",
        "            module.qconfig = weight_only_qconfig\n",
        "\n",
        "    torch.quantization.prepare_qat(model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "T8pwe7fgIroo"
      },
      "outputs": [],
      "source": [
        "def train_quantized_model(model_class, activation, ckpt_path):\n",
        "    device=cuda_device\n",
        "    \n",
        "    model_name = f\"{model_class.__name__}_{activation}_quantized\"\n",
        "    model = model_class(activation=activation, quantize=True).to(device)\n",
        "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "\n",
        "    model.train()\n",
        "    print(f\"[{model_name}] initial state:\", model)\n",
        "    \n",
        "    configure_qat(model)\n",
        "    \n",
        "    print(f\"[{model_name}] after configure_qat:\", model)\n",
        "    \n",
        "    # fine-tune via QAT\n",
        "    writer = SummaryWriter(f\"runs/{model_name}\")\n",
        "    train_model(model, learning_rate=QAT_LR, num_epochs=QAT_EPOCHS, writer=writer, model_name=model_name, device=device)\n",
        "\n",
        "    # Convert into quantized model\n",
        "    model.eval()\n",
        "    torch.save(model.state_dict(), f\"checkpoint/{model_name}.ckpt\")\n",
        "    writer.close()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All possible architectures and activations\n",
        "models = [LeNet5, ResNet20, ResNet18]\n",
        "activations = ['relu', 'hardtanh', 'relu6']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "collapsed": true,
        "id": "-wKYDsXyHiHM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training LeNet5 with relu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LeNet5_relu] Epoch: 000 Eval Loss: 2.304 Eval Acc: 0.095\n",
            "[LeNet5_relu] Epoch: 001 Train Loss: 2.298 Train Acc: 0.123 Eval Loss: 2.287 Eval Acc: 0.180\n",
            "[LeNet5_relu] Epoch: 002 Train Loss: 2.217 Train Acc: 0.203 Eval Loss: 2.069 Eval Acc: 0.253\n",
            "[LeNet5_relu] Epoch: 003 Train Loss: 2.008 Train Acc: 0.262 Eval Loss: 1.924 Eval Acc: 0.306\n",
            "[LeNet5_relu] Epoch: 004 Train Loss: 1.908 Train Acc: 0.302 Eval Loss: 1.795 Eval Acc: 0.353\n",
            "[LeNet5_relu] Epoch: 005 Train Loss: 1.812 Train Acc: 0.334 Eval Loss: 1.705 Eval Acc: 0.377\n",
            "[LeNet5_relu] Epoch: 006 Train Loss: 1.745 Train Acc: 0.353 Eval Loss: 1.650 Eval Acc: 0.397\n",
            "[LeNet5_relu] Epoch: 007 Train Loss: 1.703 Train Acc: 0.372 Eval Loss: 1.592 Eval Acc: 0.416\n",
            "[LeNet5_relu] Epoch: 008 Train Loss: 1.666 Train Acc: 0.382 Eval Loss: 1.557 Eval Acc: 0.434\n",
            "[LeNet5_relu] Epoch: 009 Train Loss: 1.634 Train Acc: 0.398 Eval Loss: 1.537 Eval Acc: 0.441\n",
            "[LeNet5_relu] Epoch: 010 Train Loss: 1.607 Train Acc: 0.405 Eval Loss: 1.488 Eval Acc: 0.459\n",
            "[LeNet5_relu] Epoch: 011 Train Loss: 1.586 Train Acc: 0.419 Eval Loss: 1.469 Eval Acc: 0.468\n",
            "[LeNet5_relu] Epoch: 012 Train Loss: 1.561 Train Acc: 0.430 Eval Loss: 1.455 Eval Acc: 0.466\n",
            "[LeNet5_relu] Epoch: 013 Train Loss: 1.534 Train Acc: 0.439 Eval Loss: 1.434 Eval Acc: 0.479\n",
            "[LeNet5_relu] Epoch: 014 Train Loss: 1.513 Train Acc: 0.446 Eval Loss: 1.405 Eval Acc: 0.494\n",
            "[LeNet5_relu] Epoch: 015 Train Loss: 1.500 Train Acc: 0.455 Eval Loss: 1.389 Eval Acc: 0.502\n",
            "[LeNet5_relu] Epoch: 016 Train Loss: 1.476 Train Acc: 0.464 Eval Loss: 1.357 Eval Acc: 0.512\n",
            "[LeNet5_relu] Epoch: 017 Train Loss: 1.459 Train Acc: 0.472 Eval Loss: 1.355 Eval Acc: 0.513\n",
            "[LeNet5_relu] Epoch: 018 Train Loss: 1.443 Train Acc: 0.478 Eval Loss: 1.347 Eval Acc: 0.523\n",
            "[LeNet5_relu] Epoch: 019 Train Loss: 1.427 Train Acc: 0.486 Eval Loss: 1.308 Eval Acc: 0.532\n",
            "[LeNet5_relu] Epoch: 020 Train Loss: 1.415 Train Acc: 0.489 Eval Loss: 1.297 Eval Acc: 0.540\n",
            "[LeNet5_relu] Epoch: 021 Train Loss: 1.395 Train Acc: 0.498 Eval Loss: 1.287 Eval Acc: 0.543\n",
            "[LeNet5_relu] Epoch: 022 Train Loss: 1.387 Train Acc: 0.500 Eval Loss: 1.294 Eval Acc: 0.539\n",
            "[LeNet5_relu] Epoch: 023 Train Loss: 1.368 Train Acc: 0.508 Eval Loss: 1.281 Eval Acc: 0.543\n",
            "[LeNet5_relu] Epoch: 024 Train Loss: 1.363 Train Acc: 0.510 Eval Loss: 1.246 Eval Acc: 0.562\n",
            "[LeNet5_relu] Epoch: 025 Train Loss: 1.346 Train Acc: 0.517 Eval Loss: 1.253 Eval Acc: 0.558\n",
            "[LeNet5_relu] Epoch: 026 Train Loss: 1.334 Train Acc: 0.521 Eval Loss: 1.236 Eval Acc: 0.562\n",
            "[LeNet5_relu] Epoch: 027 Train Loss: 1.325 Train Acc: 0.527 Eval Loss: 1.216 Eval Acc: 0.569\n",
            "[LeNet5_relu] Epoch: 028 Train Loss: 1.309 Train Acc: 0.532 Eval Loss: 1.215 Eval Acc: 0.571\n",
            "[LeNet5_relu] Epoch: 029 Train Loss: 1.306 Train Acc: 0.530 Eval Loss: 1.208 Eval Acc: 0.574\n",
            "[LeNet5_relu] Epoch: 030 Train Loss: 1.299 Train Acc: 0.537 Eval Loss: 1.187 Eval Acc: 0.582\n",
            "[LeNet5_relu] Epoch: 031 Train Loss: 1.285 Train Acc: 0.542 Eval Loss: 1.205 Eval Acc: 0.576\n",
            "[LeNet5_relu] Epoch: 032 Train Loss: 1.275 Train Acc: 0.545 Eval Loss: 1.175 Eval Acc: 0.584\n",
            "[LeNet5_relu] Epoch: 033 Train Loss: 1.266 Train Acc: 0.548 Eval Loss: 1.159 Eval Acc: 0.592\n",
            "[LeNet5_relu] Epoch: 034 Train Loss: 1.257 Train Acc: 0.551 Eval Loss: 1.155 Eval Acc: 0.591\n",
            "[LeNet5_relu] Epoch: 035 Train Loss: 1.248 Train Acc: 0.554 Eval Loss: 1.151 Eval Acc: 0.593\n",
            "[LeNet5_relu] Epoch: 036 Train Loss: 1.249 Train Acc: 0.554 Eval Loss: 1.162 Eval Acc: 0.588\n",
            "[LeNet5_relu] Epoch: 037 Train Loss: 1.239 Train Acc: 0.558 Eval Loss: 1.150 Eval Acc: 0.597\n",
            "[LeNet5_relu] Epoch: 038 Train Loss: 1.232 Train Acc: 0.561 Eval Loss: 1.110 Eval Acc: 0.610\n",
            "[LeNet5_relu] Epoch: 039 Train Loss: 1.224 Train Acc: 0.565 Eval Loss: 1.134 Eval Acc: 0.600\n",
            "[LeNet5_relu] Epoch: 040 Train Loss: 1.221 Train Acc: 0.564 Eval Loss: 1.104 Eval Acc: 0.614\n",
            "[LeNet5_relu] Epoch: 041 Train Loss: 1.209 Train Acc: 0.571 Eval Loss: 1.109 Eval Acc: 0.606\n",
            "[LeNet5_relu] Epoch: 042 Train Loss: 1.205 Train Acc: 0.572 Eval Loss: 1.114 Eval Acc: 0.608\n",
            "[LeNet5_relu] Epoch: 043 Train Loss: 1.202 Train Acc: 0.572 Eval Loss: 1.100 Eval Acc: 0.610\n",
            "[LeNet5_relu] Epoch: 044 Train Loss: 1.188 Train Acc: 0.581 Eval Loss: 1.081 Eval Acc: 0.616\n",
            "[LeNet5_relu] Epoch: 045 Train Loss: 1.188 Train Acc: 0.579 Eval Loss: 1.096 Eval Acc: 0.611\n",
            "[LeNet5_relu] Epoch: 046 Train Loss: 1.189 Train Acc: 0.576 Eval Loss: 1.097 Eval Acc: 0.609\n",
            "[LeNet5_relu] Epoch: 047 Train Loss: 1.177 Train Acc: 0.583 Eval Loss: 1.088 Eval Acc: 0.617\n",
            "[LeNet5_relu] Epoch: 048 Train Loss: 1.176 Train Acc: 0.585 Eval Loss: 1.076 Eval Acc: 0.624\n",
            "[LeNet5_relu] Epoch: 049 Train Loss: 1.170 Train Acc: 0.584 Eval Loss: 1.056 Eval Acc: 0.628\n",
            "[LeNet5_relu] Epoch: 050 Train Loss: 1.163 Train Acc: 0.588 Eval Loss: 1.063 Eval Acc: 0.625\n",
            "[LeNet5_relu] Epoch: 051 Train Loss: 1.164 Train Acc: 0.588 Eval Loss: 1.059 Eval Acc: 0.625\n",
            "[LeNet5_relu] Epoch: 052 Train Loss: 1.155 Train Acc: 0.591 Eval Loss: 1.058 Eval Acc: 0.628\n",
            "[LeNet5_relu] Epoch: 053 Train Loss: 1.151 Train Acc: 0.592 Eval Loss: 1.078 Eval Acc: 0.618\n",
            "[LeNet5_relu] Epoch: 054 Train Loss: 1.148 Train Acc: 0.593 Eval Loss: 1.048 Eval Acc: 0.630\n",
            "[LeNet5_relu] Epoch: 055 Train Loss: 1.143 Train Acc: 0.598 Eval Loss: 1.043 Eval Acc: 0.632\n",
            "[LeNet5_relu] Epoch: 056 Train Loss: 1.141 Train Acc: 0.597 Eval Loss: 1.044 Eval Acc: 0.633\n",
            "[LeNet5_relu] Epoch: 057 Train Loss: 1.133 Train Acc: 0.600 Eval Loss: 1.056 Eval Acc: 0.626\n",
            "[LeNet5_relu] Epoch: 058 Train Loss: 1.132 Train Acc: 0.595 Eval Loss: 1.032 Eval Acc: 0.636\n",
            "[LeNet5_relu] Epoch: 059 Train Loss: 1.126 Train Acc: 0.602 Eval Loss: 1.029 Eval Acc: 0.638\n",
            "[LeNet5_relu] Epoch: 060 Train Loss: 1.124 Train Acc: 0.603 Eval Loss: 1.012 Eval Acc: 0.647\n",
            "[LeNet5_relu] Epoch: 061 Train Loss: 1.094 Train Acc: 0.611 Eval Loss: 1.005 Eval Acc: 0.648\n",
            "[LeNet5_relu] Epoch: 062 Train Loss: 1.089 Train Acc: 0.615 Eval Loss: 1.004 Eval Acc: 0.648\n",
            "[LeNet5_relu] Epoch: 063 Train Loss: 1.088 Train Acc: 0.616 Eval Loss: 0.998 Eval Acc: 0.650\n",
            "[LeNet5_relu] Epoch: 064 Train Loss: 1.079 Train Acc: 0.618 Eval Loss: 0.999 Eval Acc: 0.649\n",
            "[LeNet5_relu] Epoch: 065 Train Loss: 1.089 Train Acc: 0.614 Eval Loss: 1.001 Eval Acc: 0.648\n",
            "[LeNet5_relu] Epoch: 066 Train Loss: 1.084 Train Acc: 0.616 Eval Loss: 0.996 Eval Acc: 0.652\n",
            "[LeNet5_relu] Epoch: 067 Train Loss: 1.083 Train Acc: 0.619 Eval Loss: 0.998 Eval Acc: 0.651\n",
            "[LeNet5_relu] Epoch: 068 Train Loss: 1.083 Train Acc: 0.618 Eval Loss: 0.997 Eval Acc: 0.651\n",
            "[LeNet5_relu] Epoch: 069 Train Loss: 1.087 Train Acc: 0.616 Eval Loss: 0.999 Eval Acc: 0.653\n",
            "[LeNet5_relu] Epoch: 070 Train Loss: 1.081 Train Acc: 0.618 Eval Loss: 0.998 Eval Acc: 0.651\n",
            "[LeNet5_relu] Epoch: 071 Train Loss: 1.087 Train Acc: 0.617 Eval Loss: 0.998 Eval Acc: 0.651\n",
            "[LeNet5_relu] Epoch: 072 Train Loss: 1.086 Train Acc: 0.616 Eval Loss: 0.993 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 073 Train Loss: 1.083 Train Acc: 0.618 Eval Loss: 1.001 Eval Acc: 0.650\n",
            "[LeNet5_relu] Epoch: 074 Train Loss: 1.083 Train Acc: 0.619 Eval Loss: 0.995 Eval Acc: 0.654\n",
            "[LeNet5_relu] Epoch: 075 Train Loss: 1.085 Train Acc: 0.618 Eval Loss: 0.990 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 076 Train Loss: 1.082 Train Acc: 0.616 Eval Loss: 0.992 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 077 Train Loss: 1.078 Train Acc: 0.620 Eval Loss: 0.996 Eval Acc: 0.655\n",
            "[LeNet5_relu] Epoch: 078 Train Loss: 1.080 Train Acc: 0.619 Eval Loss: 0.993 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 079 Train Loss: 1.080 Train Acc: 0.618 Eval Loss: 0.993 Eval Acc: 0.654\n",
            "[LeNet5_relu] Epoch: 080 Train Loss: 1.075 Train Acc: 0.623 Eval Loss: 0.993 Eval Acc: 0.653\n",
            "[LeNet5_relu] Epoch: 081 Train Loss: 1.073 Train Acc: 0.624 Eval Loss: 0.993 Eval Acc: 0.652\n",
            "[LeNet5_relu] Epoch: 082 Train Loss: 1.071 Train Acc: 0.623 Eval Loss: 0.994 Eval Acc: 0.654\n",
            "[LeNet5_relu] Epoch: 083 Train Loss: 1.074 Train Acc: 0.620 Eval Loss: 0.989 Eval Acc: 0.655\n",
            "[LeNet5_relu] Epoch: 084 Train Loss: 1.073 Train Acc: 0.623 Eval Loss: 0.990 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 085 Train Loss: 1.074 Train Acc: 0.622 Eval Loss: 0.994 Eval Acc: 0.654\n",
            "[LeNet5_relu] Epoch: 086 Train Loss: 1.070 Train Acc: 0.622 Eval Loss: 0.992 Eval Acc: 0.653\n",
            "[LeNet5_relu] Epoch: 087 Train Loss: 1.074 Train Acc: 0.621 Eval Loss: 0.989 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 088 Train Loss: 1.077 Train Acc: 0.618 Eval Loss: 0.989 Eval Acc: 0.655\n",
            "[LeNet5_relu] Epoch: 089 Train Loss: 1.071 Train Acc: 0.622 Eval Loss: 0.991 Eval Acc: 0.658\n",
            "[LeNet5_relu] Epoch: 090 Train Loss: 1.073 Train Acc: 0.622 Eval Loss: 0.990 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 091 Train Loss: 1.068 Train Acc: 0.622 Eval Loss: 0.987 Eval Acc: 0.656\n",
            "[LeNet5_relu] Epoch: 092 Train Loss: 1.066 Train Acc: 0.624 Eval Loss: 0.987 Eval Acc: 0.658\n",
            "[LeNet5_relu] Epoch: 093 Train Loss: 1.069 Train Acc: 0.624 Eval Loss: 0.987 Eval Acc: 0.658\n",
            "[LeNet5_relu] Epoch: 094 Train Loss: 1.070 Train Acc: 0.624 Eval Loss: 0.987 Eval Acc: 0.657\n",
            "[LeNet5_relu] Epoch: 095 Train Loss: 1.069 Train Acc: 0.623 Eval Loss: 0.986 Eval Acc: 0.657\n",
            "[LeNet5_relu] Epoch: 096 Train Loss: 1.073 Train Acc: 0.622 Eval Loss: 0.986 Eval Acc: 0.657\n",
            "[LeNet5_relu] Epoch: 097 Train Loss: 1.067 Train Acc: 0.623 Eval Loss: 0.987 Eval Acc: 0.657\n",
            "[LeNet5_relu] Epoch: 098 Train Loss: 1.068 Train Acc: 0.623 Eval Loss: 0.987 Eval Acc: 0.657\n",
            "[LeNet5_relu] Epoch: 099 Train Loss: 1.067 Train Acc: 0.623 Eval Loss: 0.987 Eval Acc: 0.657\n",
            "Early stopping was triggered!\n",
            "\n",
            "Training LeNet5 with hardtanh\n",
            "[LeNet5_hardtanh] Epoch: 000 Eval Loss: 2.302 Eval Acc: 0.119\n",
            "[LeNet5_hardtanh] Epoch: 001 Train Loss: 2.166 Train Acc: 0.206 Eval Loss: 2.002 Eval Acc: 0.273\n",
            "[LeNet5_hardtanh] Epoch: 002 Train Loss: 1.970 Train Acc: 0.284 Eval Loss: 1.869 Eval Acc: 0.330\n",
            "[LeNet5_hardtanh] Epoch: 003 Train Loss: 1.856 Train Acc: 0.326 Eval Loss: 1.737 Eval Acc: 0.372\n",
            "[LeNet5_hardtanh] Epoch: 004 Train Loss: 1.758 Train Acc: 0.354 Eval Loss: 1.667 Eval Acc: 0.390\n",
            "[LeNet5_hardtanh] Epoch: 005 Train Loss: 1.707 Train Acc: 0.372 Eval Loss: 1.619 Eval Acc: 0.407\n",
            "[LeNet5_hardtanh] Epoch: 006 Train Loss: 1.666 Train Acc: 0.390 Eval Loss: 1.563 Eval Acc: 0.432\n",
            "[LeNet5_hardtanh] Epoch: 007 Train Loss: 1.628 Train Acc: 0.403 Eval Loss: 1.528 Eval Acc: 0.444\n",
            "[LeNet5_hardtanh] Epoch: 008 Train Loss: 1.593 Train Acc: 0.419 Eval Loss: 1.478 Eval Acc: 0.463\n",
            "[LeNet5_hardtanh] Epoch: 009 Train Loss: 1.562 Train Acc: 0.428 Eval Loss: 1.451 Eval Acc: 0.472\n",
            "[LeNet5_hardtanh] Epoch: 010 Train Loss: 1.538 Train Acc: 0.438 Eval Loss: 1.435 Eval Acc: 0.480\n",
            "[LeNet5_hardtanh] Epoch: 011 Train Loss: 1.513 Train Acc: 0.446 Eval Loss: 1.404 Eval Acc: 0.491\n",
            "[LeNet5_hardtanh] Epoch: 012 Train Loss: 1.495 Train Acc: 0.455 Eval Loss: 1.390 Eval Acc: 0.498\n",
            "[LeNet5_hardtanh] Epoch: 013 Train Loss: 1.486 Train Acc: 0.457 Eval Loss: 1.378 Eval Acc: 0.500\n",
            "[LeNet5_hardtanh] Epoch: 014 Train Loss: 1.469 Train Acc: 0.462 Eval Loss: 1.375 Eval Acc: 0.504\n",
            "[LeNet5_hardtanh] Epoch: 015 Train Loss: 1.458 Train Acc: 0.466 Eval Loss: 1.346 Eval Acc: 0.514\n",
            "[LeNet5_hardtanh] Epoch: 016 Train Loss: 1.439 Train Acc: 0.476 Eval Loss: 1.333 Eval Acc: 0.517\n",
            "[LeNet5_hardtanh] Epoch: 017 Train Loss: 1.432 Train Acc: 0.476 Eval Loss: 1.323 Eval Acc: 0.519\n",
            "[LeNet5_hardtanh] Epoch: 018 Train Loss: 1.419 Train Acc: 0.482 Eval Loss: 1.329 Eval Acc: 0.519\n",
            "[LeNet5_hardtanh] Epoch: 019 Train Loss: 1.413 Train Acc: 0.488 Eval Loss: 1.303 Eval Acc: 0.534\n",
            "[LeNet5_hardtanh] Epoch: 020 Train Loss: 1.402 Train Acc: 0.490 Eval Loss: 1.280 Eval Acc: 0.537\n",
            "[LeNet5_hardtanh] Epoch: 021 Train Loss: 1.391 Train Acc: 0.494 Eval Loss: 1.296 Eval Acc: 0.530\n",
            "[LeNet5_hardtanh] Epoch: 022 Train Loss: 1.384 Train Acc: 0.499 Eval Loss: 1.270 Eval Acc: 0.542\n",
            "[LeNet5_hardtanh] Epoch: 023 Train Loss: 1.374 Train Acc: 0.502 Eval Loss: 1.264 Eval Acc: 0.545\n",
            "[LeNet5_hardtanh] Epoch: 024 Train Loss: 1.363 Train Acc: 0.507 Eval Loss: 1.259 Eval Acc: 0.548\n",
            "[LeNet5_hardtanh] Epoch: 025 Train Loss: 1.351 Train Acc: 0.510 Eval Loss: 1.230 Eval Acc: 0.559\n",
            "[LeNet5_hardtanh] Epoch: 026 Train Loss: 1.347 Train Acc: 0.513 Eval Loss: 1.231 Eval Acc: 0.559\n",
            "[LeNet5_hardtanh] Epoch: 027 Train Loss: 1.339 Train Acc: 0.515 Eval Loss: 1.216 Eval Acc: 0.560\n",
            "[LeNet5_hardtanh] Epoch: 028 Train Loss: 1.328 Train Acc: 0.522 Eval Loss: 1.227 Eval Acc: 0.555\n",
            "[LeNet5_hardtanh] Epoch: 029 Train Loss: 1.320 Train Acc: 0.526 Eval Loss: 1.202 Eval Acc: 0.568\n",
            "[LeNet5_hardtanh] Epoch: 030 Train Loss: 1.314 Train Acc: 0.527 Eval Loss: 1.190 Eval Acc: 0.569\n",
            "[LeNet5_hardtanh] Epoch: 031 Train Loss: 1.307 Train Acc: 0.532 Eval Loss: 1.203 Eval Acc: 0.568\n",
            "[LeNet5_hardtanh] Epoch: 032 Train Loss: 1.299 Train Acc: 0.534 Eval Loss: 1.179 Eval Acc: 0.577\n",
            "[LeNet5_hardtanh] Epoch: 033 Train Loss: 1.293 Train Acc: 0.538 Eval Loss: 1.163 Eval Acc: 0.581\n",
            "[LeNet5_hardtanh] Epoch: 034 Train Loss: 1.281 Train Acc: 0.538 Eval Loss: 1.168 Eval Acc: 0.582\n",
            "[LeNet5_hardtanh] Epoch: 035 Train Loss: 1.275 Train Acc: 0.544 Eval Loss: 1.156 Eval Acc: 0.586\n",
            "[LeNet5_hardtanh] Epoch: 036 Train Loss: 1.278 Train Acc: 0.540 Eval Loss: 1.174 Eval Acc: 0.579\n",
            "[LeNet5_hardtanh] Epoch: 037 Train Loss: 1.265 Train Acc: 0.547 Eval Loss: 1.136 Eval Acc: 0.594\n",
            "[LeNet5_hardtanh] Epoch: 038 Train Loss: 1.262 Train Acc: 0.547 Eval Loss: 1.137 Eval Acc: 0.595\n",
            "[LeNet5_hardtanh] Epoch: 039 Train Loss: 1.256 Train Acc: 0.549 Eval Loss: 1.159 Eval Acc: 0.582\n",
            "[LeNet5_hardtanh] Epoch: 040 Train Loss: 1.254 Train Acc: 0.552 Eval Loss: 1.141 Eval Acc: 0.592\n",
            "[LeNet5_hardtanh] Epoch: 041 Train Loss: 1.245 Train Acc: 0.555 Eval Loss: 1.145 Eval Acc: 0.590\n",
            "[LeNet5_hardtanh] Epoch: 042 Train Loss: 1.237 Train Acc: 0.557 Eval Loss: 1.119 Eval Acc: 0.601\n",
            "[LeNet5_hardtanh] Epoch: 043 Train Loss: 1.229 Train Acc: 0.563 Eval Loss: 1.117 Eval Acc: 0.599\n",
            "[LeNet5_hardtanh] Epoch: 044 Train Loss: 1.227 Train Acc: 0.562 Eval Loss: 1.107 Eval Acc: 0.604\n",
            "[LeNet5_hardtanh] Epoch: 045 Train Loss: 1.228 Train Acc: 0.561 Eval Loss: 1.096 Eval Acc: 0.610\n",
            "[LeNet5_hardtanh] Epoch: 046 Train Loss: 1.219 Train Acc: 0.564 Eval Loss: 1.119 Eval Acc: 0.602\n",
            "[LeNet5_hardtanh] Epoch: 047 Train Loss: 1.216 Train Acc: 0.566 Eval Loss: 1.103 Eval Acc: 0.610\n",
            "[LeNet5_hardtanh] Epoch: 048 Train Loss: 1.208 Train Acc: 0.568 Eval Loss: 1.081 Eval Acc: 0.614\n",
            "[LeNet5_hardtanh] Epoch: 049 Train Loss: 1.204 Train Acc: 0.571 Eval Loss: 1.098 Eval Acc: 0.605\n",
            "[LeNet5_hardtanh] Epoch: 050 Train Loss: 1.205 Train Acc: 0.571 Eval Loss: 1.087 Eval Acc: 0.614\n",
            "[LeNet5_hardtanh] Epoch: 051 Train Loss: 1.197 Train Acc: 0.571 Eval Loss: 1.105 Eval Acc: 0.608\n",
            "[LeNet5_hardtanh] Epoch: 052 Train Loss: 1.194 Train Acc: 0.574 Eval Loss: 1.079 Eval Acc: 0.618\n",
            "[LeNet5_hardtanh] Epoch: 053 Train Loss: 1.188 Train Acc: 0.575 Eval Loss: 1.070 Eval Acc: 0.622\n",
            "[LeNet5_hardtanh] Epoch: 054 Train Loss: 1.186 Train Acc: 0.578 Eval Loss: 1.063 Eval Acc: 0.622\n",
            "[LeNet5_hardtanh] Epoch: 055 Train Loss: 1.177 Train Acc: 0.580 Eval Loss: 1.078 Eval Acc: 0.616\n",
            "[LeNet5_hardtanh] Epoch: 056 Train Loss: 1.178 Train Acc: 0.581 Eval Loss: 1.053 Eval Acc: 0.626\n",
            "[LeNet5_hardtanh] Epoch: 057 Train Loss: 1.180 Train Acc: 0.580 Eval Loss: 1.060 Eval Acc: 0.622\n",
            "[LeNet5_hardtanh] Epoch: 058 Train Loss: 1.174 Train Acc: 0.581 Eval Loss: 1.052 Eval Acc: 0.627\n",
            "[LeNet5_hardtanh] Epoch: 059 Train Loss: 1.170 Train Acc: 0.585 Eval Loss: 1.071 Eval Acc: 0.617\n",
            "[LeNet5_hardtanh] Epoch: 060 Train Loss: 1.163 Train Acc: 0.587 Eval Loss: 1.040 Eval Acc: 0.630\n",
            "[LeNet5_hardtanh] Epoch: 061 Train Loss: 1.141 Train Acc: 0.594 Eval Loss: 1.031 Eval Acc: 0.632\n",
            "[LeNet5_hardtanh] Epoch: 062 Train Loss: 1.133 Train Acc: 0.597 Eval Loss: 1.032 Eval Acc: 0.636\n",
            "[LeNet5_hardtanh] Epoch: 063 Train Loss: 1.137 Train Acc: 0.597 Eval Loss: 1.030 Eval Acc: 0.634\n",
            "[LeNet5_hardtanh] Epoch: 064 Train Loss: 1.134 Train Acc: 0.596 Eval Loss: 1.029 Eval Acc: 0.637\n",
            "[LeNet5_hardtanh] Epoch: 065 Train Loss: 1.137 Train Acc: 0.595 Eval Loss: 1.028 Eval Acc: 0.637\n",
            "[LeNet5_hardtanh] Epoch: 066 Train Loss: 1.134 Train Acc: 0.600 Eval Loss: 1.024 Eval Acc: 0.638\n",
            "[LeNet5_hardtanh] Epoch: 067 Train Loss: 1.132 Train Acc: 0.598 Eval Loss: 1.023 Eval Acc: 0.639\n",
            "[LeNet5_hardtanh] Epoch: 068 Train Loss: 1.130 Train Acc: 0.599 Eval Loss: 1.026 Eval Acc: 0.636\n",
            "[LeNet5_hardtanh] Epoch: 069 Train Loss: 1.131 Train Acc: 0.600 Eval Loss: 1.026 Eval Acc: 0.634\n",
            "[LeNet5_hardtanh] Epoch: 070 Train Loss: 1.131 Train Acc: 0.597 Eval Loss: 1.026 Eval Acc: 0.635\n",
            "[LeNet5_hardtanh] Epoch: 071 Train Loss: 1.129 Train Acc: 0.598 Eval Loss: 1.027 Eval Acc: 0.636\n",
            "[LeNet5_hardtanh] Epoch: 072 Train Loss: 1.128 Train Acc: 0.597 Eval Loss: 1.022 Eval Acc: 0.638\n",
            "[LeNet5_hardtanh] Epoch: 073 Train Loss: 1.127 Train Acc: 0.597 Eval Loss: 1.026 Eval Acc: 0.635\n",
            "[LeNet5_hardtanh] Epoch: 074 Train Loss: 1.127 Train Acc: 0.599 Eval Loss: 1.024 Eval Acc: 0.637\n",
            "[LeNet5_hardtanh] Epoch: 075 Train Loss: 1.129 Train Acc: 0.599 Eval Loss: 1.025 Eval Acc: 0.636\n",
            "[LeNet5_hardtanh] Epoch: 076 Train Loss: 1.131 Train Acc: 0.598 Eval Loss: 1.021 Eval Acc: 0.638\n",
            "[LeNet5_hardtanh] Epoch: 077 Train Loss: 1.126 Train Acc: 0.599 Eval Loss: 1.022 Eval Acc: 0.637\n",
            "Early stopping was triggered!\n",
            "\n",
            "Training LeNet5 with relu6\n",
            "[LeNet5_relu6] Epoch: 000 Eval Loss: 2.306 Eval Acc: 0.099\n",
            "[LeNet5_relu6] Epoch: 001 Train Loss: 2.292 Train Acc: 0.144 Eval Loss: 2.259 Eval Acc: 0.159\n",
            "[LeNet5_relu6] Epoch: 002 Train Loss: 2.173 Train Acc: 0.201 Eval Loss: 2.057 Eval Acc: 0.254\n",
            "[LeNet5_relu6] Epoch: 003 Train Loss: 2.023 Train Acc: 0.252 Eval Loss: 1.935 Eval Acc: 0.294\n",
            "[LeNet5_relu6] Epoch: 004 Train Loss: 1.926 Train Acc: 0.294 Eval Loss: 1.813 Eval Acc: 0.343\n",
            "[LeNet5_relu6] Epoch: 005 Train Loss: 1.807 Train Acc: 0.335 Eval Loss: 1.664 Eval Acc: 0.391\n",
            "[LeNet5_relu6] Epoch: 006 Train Loss: 1.708 Train Acc: 0.368 Eval Loss: 1.597 Eval Acc: 0.416\n",
            "[LeNet5_relu6] Epoch: 007 Train Loss: 1.654 Train Acc: 0.387 Eval Loss: 1.527 Eval Acc: 0.441\n",
            "[LeNet5_relu6] Epoch: 008 Train Loss: 1.604 Train Acc: 0.406 Eval Loss: 1.486 Eval Acc: 0.463\n",
            "[LeNet5_relu6] Epoch: 009 Train Loss: 1.561 Train Acc: 0.425 Eval Loss: 1.456 Eval Acc: 0.477\n",
            "[LeNet5_relu6] Epoch: 010 Train Loss: 1.529 Train Acc: 0.439 Eval Loss: 1.447 Eval Acc: 0.479\n",
            "[LeNet5_relu6] Epoch: 011 Train Loss: 1.502 Train Acc: 0.447 Eval Loss: 1.397 Eval Acc: 0.499\n",
            "[LeNet5_relu6] Epoch: 012 Train Loss: 1.488 Train Acc: 0.455 Eval Loss: 1.371 Eval Acc: 0.510\n",
            "[LeNet5_relu6] Epoch: 013 Train Loss: 1.466 Train Acc: 0.462 Eval Loss: 1.367 Eval Acc: 0.508\n",
            "[LeNet5_relu6] Epoch: 014 Train Loss: 1.452 Train Acc: 0.469 Eval Loss: 1.339 Eval Acc: 0.515\n",
            "[LeNet5_relu6] Epoch: 015 Train Loss: 1.435 Train Acc: 0.479 Eval Loss: 1.342 Eval Acc: 0.523\n",
            "[LeNet5_relu6] Epoch: 016 Train Loss: 1.429 Train Acc: 0.478 Eval Loss: 1.313 Eval Acc: 0.527\n",
            "[LeNet5_relu6] Epoch: 017 Train Loss: 1.405 Train Acc: 0.489 Eval Loss: 1.300 Eval Acc: 0.534\n",
            "[LeNet5_relu6] Epoch: 018 Train Loss: 1.395 Train Acc: 0.495 Eval Loss: 1.268 Eval Acc: 0.548\n",
            "[LeNet5_relu6] Epoch: 019 Train Loss: 1.381 Train Acc: 0.500 Eval Loss: 1.259 Eval Acc: 0.550\n",
            "[LeNet5_relu6] Epoch: 020 Train Loss: 1.366 Train Acc: 0.505 Eval Loss: 1.248 Eval Acc: 0.554\n",
            "[LeNet5_relu6] Epoch: 021 Train Loss: 1.355 Train Acc: 0.510 Eval Loss: 1.282 Eval Acc: 0.539\n",
            "[LeNet5_relu6] Epoch: 022 Train Loss: 1.343 Train Acc: 0.516 Eval Loss: 1.248 Eval Acc: 0.554\n",
            "[LeNet5_relu6] Epoch: 023 Train Loss: 1.334 Train Acc: 0.519 Eval Loss: 1.220 Eval Acc: 0.564\n",
            "[LeNet5_relu6] Epoch: 024 Train Loss: 1.328 Train Acc: 0.523 Eval Loss: 1.208 Eval Acc: 0.569\n",
            "[LeNet5_relu6] Epoch: 025 Train Loss: 1.310 Train Acc: 0.530 Eval Loss: 1.213 Eval Acc: 0.567\n",
            "[LeNet5_relu6] Epoch: 026 Train Loss: 1.297 Train Acc: 0.531 Eval Loss: 1.217 Eval Acc: 0.560\n",
            "[LeNet5_relu6] Epoch: 027 Train Loss: 1.294 Train Acc: 0.535 Eval Loss: 1.192 Eval Acc: 0.573\n",
            "[LeNet5_relu6] Epoch: 028 Train Loss: 1.284 Train Acc: 0.539 Eval Loss: 1.209 Eval Acc: 0.568\n",
            "[LeNet5_relu6] Epoch: 029 Train Loss: 1.277 Train Acc: 0.542 Eval Loss: 1.178 Eval Acc: 0.580\n",
            "[LeNet5_relu6] Epoch: 030 Train Loss: 1.264 Train Acc: 0.545 Eval Loss: 1.159 Eval Acc: 0.586\n",
            "[LeNet5_relu6] Epoch: 031 Train Loss: 1.255 Train Acc: 0.552 Eval Loss: 1.169 Eval Acc: 0.580\n",
            "[LeNet5_relu6] Epoch: 032 Train Loss: 1.249 Train Acc: 0.551 Eval Loss: 1.168 Eval Acc: 0.584\n",
            "[LeNet5_relu6] Epoch: 033 Train Loss: 1.242 Train Acc: 0.554 Eval Loss: 1.153 Eval Acc: 0.585\n",
            "[LeNet5_relu6] Epoch: 034 Train Loss: 1.236 Train Acc: 0.558 Eval Loss: 1.140 Eval Acc: 0.594\n",
            "[LeNet5_relu6] Epoch: 035 Train Loss: 1.228 Train Acc: 0.562 Eval Loss: 1.152 Eval Acc: 0.586\n",
            "[LeNet5_relu6] Epoch: 036 Train Loss: 1.220 Train Acc: 0.562 Eval Loss: 1.146 Eval Acc: 0.591\n",
            "[LeNet5_relu6] Epoch: 037 Train Loss: 1.217 Train Acc: 0.566 Eval Loss: 1.131 Eval Acc: 0.594\n",
            "[LeNet5_relu6] Epoch: 038 Train Loss: 1.209 Train Acc: 0.567 Eval Loss: 1.138 Eval Acc: 0.597\n",
            "[LeNet5_relu6] Epoch: 039 Train Loss: 1.203 Train Acc: 0.571 Eval Loss: 1.127 Eval Acc: 0.595\n",
            "[LeNet5_relu6] Epoch: 040 Train Loss: 1.202 Train Acc: 0.571 Eval Loss: 1.103 Eval Acc: 0.604\n",
            "[LeNet5_relu6] Epoch: 041 Train Loss: 1.195 Train Acc: 0.572 Eval Loss: 1.098 Eval Acc: 0.606\n",
            "[LeNet5_relu6] Epoch: 042 Train Loss: 1.187 Train Acc: 0.576 Eval Loss: 1.106 Eval Acc: 0.605\n",
            "[LeNet5_relu6] Epoch: 043 Train Loss: 1.179 Train Acc: 0.579 Eval Loss: 1.103 Eval Acc: 0.604\n",
            "[LeNet5_relu6] Epoch: 044 Train Loss: 1.180 Train Acc: 0.578 Eval Loss: 1.092 Eval Acc: 0.609\n",
            "[LeNet5_relu6] Epoch: 045 Train Loss: 1.174 Train Acc: 0.580 Eval Loss: 1.096 Eval Acc: 0.607\n",
            "[LeNet5_relu6] Epoch: 046 Train Loss: 1.166 Train Acc: 0.586 Eval Loss: 1.111 Eval Acc: 0.607\n",
            "[LeNet5_relu6] Epoch: 047 Train Loss: 1.162 Train Acc: 0.586 Eval Loss: 1.066 Eval Acc: 0.618\n",
            "[LeNet5_relu6] Epoch: 048 Train Loss: 1.154 Train Acc: 0.589 Eval Loss: 1.075 Eval Acc: 0.612\n",
            "[LeNet5_relu6] Epoch: 049 Train Loss: 1.153 Train Acc: 0.590 Eval Loss: 1.117 Eval Acc: 0.600\n",
            "[LeNet5_relu6] Epoch: 050 Train Loss: 1.151 Train Acc: 0.589 Eval Loss: 1.113 Eval Acc: 0.604\n",
            "[LeNet5_relu6] Epoch: 051 Train Loss: 1.140 Train Acc: 0.595 Eval Loss: 1.074 Eval Acc: 0.620\n",
            "[LeNet5_relu6] Epoch: 052 Train Loss: 1.140 Train Acc: 0.593 Eval Loss: 1.081 Eval Acc: 0.611\n",
            "[LeNet5_relu6] Epoch: 053 Train Loss: 1.139 Train Acc: 0.594 Eval Loss: 1.062 Eval Acc: 0.620\n",
            "[LeNet5_relu6] Epoch: 054 Train Loss: 1.133 Train Acc: 0.599 Eval Loss: 1.047 Eval Acc: 0.626\n",
            "[LeNet5_relu6] Epoch: 055 Train Loss: 1.125 Train Acc: 0.601 Eval Loss: 1.074 Eval Acc: 0.614\n",
            "[LeNet5_relu6] Epoch: 056 Train Loss: 1.120 Train Acc: 0.600 Eval Loss: 1.066 Eval Acc: 0.617\n",
            "[LeNet5_relu6] Epoch: 057 Train Loss: 1.119 Train Acc: 0.603 Eval Loss: 1.055 Eval Acc: 0.622\n",
            "[LeNet5_relu6] Epoch: 058 Train Loss: 1.118 Train Acc: 0.601 Eval Loss: 1.050 Eval Acc: 0.622\n",
            "[LeNet5_relu6] Epoch: 059 Train Loss: 1.116 Train Acc: 0.604 Eval Loss: 1.047 Eval Acc: 0.626\n",
            "[LeNet5_relu6] Epoch: 060 Train Loss: 1.111 Train Acc: 0.604 Eval Loss: 1.020 Eval Acc: 0.635\n",
            "[LeNet5_relu6] Epoch: 061 Train Loss: 1.083 Train Acc: 0.615 Eval Loss: 1.021 Eval Acc: 0.634\n",
            "[LeNet5_relu6] Epoch: 062 Train Loss: 1.082 Train Acc: 0.617 Eval Loss: 1.017 Eval Acc: 0.637\n",
            "[LeNet5_relu6] Epoch: 063 Train Loss: 1.078 Train Acc: 0.618 Eval Loss: 1.022 Eval Acc: 0.635\n",
            "[LeNet5_relu6] Epoch: 064 Train Loss: 1.076 Train Acc: 0.618 Eval Loss: 1.010 Eval Acc: 0.639\n",
            "[LeNet5_relu6] Epoch: 065 Train Loss: 1.074 Train Acc: 0.617 Eval Loss: 1.014 Eval Acc: 0.637\n",
            "[LeNet5_relu6] Epoch: 066 Train Loss: 1.078 Train Acc: 0.617 Eval Loss: 1.011 Eval Acc: 0.640\n",
            "[LeNet5_relu6] Epoch: 067 Train Loss: 1.073 Train Acc: 0.618 Eval Loss: 1.013 Eval Acc: 0.636\n",
            "[LeNet5_relu6] Epoch: 068 Train Loss: 1.073 Train Acc: 0.620 Eval Loss: 1.012 Eval Acc: 0.637\n",
            "[LeNet5_relu6] Epoch: 069 Train Loss: 1.074 Train Acc: 0.619 Eval Loss: 1.007 Eval Acc: 0.640\n",
            "[LeNet5_relu6] Epoch: 070 Train Loss: 1.072 Train Acc: 0.619 Eval Loss: 1.010 Eval Acc: 0.639\n",
            "[LeNet5_relu6] Epoch: 071 Train Loss: 1.076 Train Acc: 0.620 Eval Loss: 1.009 Eval Acc: 0.638\n",
            "[LeNet5_relu6] Epoch: 072 Train Loss: 1.071 Train Acc: 0.620 Eval Loss: 1.008 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 073 Train Loss: 1.067 Train Acc: 0.620 Eval Loss: 1.015 Eval Acc: 0.637\n",
            "[LeNet5_relu6] Epoch: 074 Train Loss: 1.070 Train Acc: 0.620 Eval Loss: 1.009 Eval Acc: 0.639\n",
            "[LeNet5_relu6] Epoch: 075 Train Loss: 1.071 Train Acc: 0.620 Eval Loss: 1.007 Eval Acc: 0.640\n",
            "[LeNet5_relu6] Epoch: 076 Train Loss: 1.070 Train Acc: 0.619 Eval Loss: 1.011 Eval Acc: 0.637\n",
            "[LeNet5_relu6] Epoch: 077 Train Loss: 1.066 Train Acc: 0.621 Eval Loss: 1.004 Eval Acc: 0.639\n",
            "[LeNet5_relu6] Epoch: 078 Train Loss: 1.064 Train Acc: 0.623 Eval Loss: 1.008 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 079 Train Loss: 1.062 Train Acc: 0.622 Eval Loss: 1.008 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 080 Train Loss: 1.066 Train Acc: 0.623 Eval Loss: 1.010 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 081 Train Loss: 1.065 Train Acc: 0.620 Eval Loss: 1.005 Eval Acc: 0.639\n",
            "[LeNet5_relu6] Epoch: 082 Train Loss: 1.066 Train Acc: 0.621 Eval Loss: 1.005 Eval Acc: 0.640\n",
            "[LeNet5_relu6] Epoch: 083 Train Loss: 1.068 Train Acc: 0.623 Eval Loss: 1.004 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 084 Train Loss: 1.065 Train Acc: 0.620 Eval Loss: 1.011 Eval Acc: 0.638\n",
            "[LeNet5_relu6] Epoch: 085 Train Loss: 1.063 Train Acc: 0.622 Eval Loss: 1.004 Eval Acc: 0.637\n",
            "[LeNet5_relu6] Epoch: 086 Train Loss: 1.065 Train Acc: 0.622 Eval Loss: 1.006 Eval Acc: 0.638\n",
            "[LeNet5_relu6] Epoch: 087 Train Loss: 1.066 Train Acc: 0.620 Eval Loss: 1.005 Eval Acc: 0.639\n",
            "[LeNet5_relu6] Epoch: 088 Train Loss: 1.063 Train Acc: 0.625 Eval Loss: 1.006 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 089 Train Loss: 1.063 Train Acc: 0.623 Eval Loss: 1.001 Eval Acc: 0.642\n",
            "[LeNet5_relu6] Epoch: 090 Train Loss: 1.062 Train Acc: 0.623 Eval Loss: 0.998 Eval Acc: 0.642\n",
            "[LeNet5_relu6] Epoch: 091 Train Loss: 1.058 Train Acc: 0.624 Eval Loss: 1.004 Eval Acc: 0.641\n",
            "[LeNet5_relu6] Epoch: 092 Train Loss: 1.058 Train Acc: 0.624 Eval Loss: 1.001 Eval Acc: 0.642\n",
            "[LeNet5_relu6] Epoch: 093 Train Loss: 1.060 Train Acc: 0.624 Eval Loss: 1.002 Eval Acc: 0.643\n",
            "[LeNet5_relu6] Epoch: 094 Train Loss: 1.056 Train Acc: 0.627 Eval Loss: 1.002 Eval Acc: 0.642\n",
            "[LeNet5_relu6] Epoch: 095 Train Loss: 1.057 Train Acc: 0.627 Eval Loss: 1.001 Eval Acc: 0.643\n",
            "[LeNet5_relu6] Epoch: 096 Train Loss: 1.056 Train Acc: 0.625 Eval Loss: 1.002 Eval Acc: 0.643\n",
            "[LeNet5_relu6] Epoch: 097 Train Loss: 1.055 Train Acc: 0.626 Eval Loss: 1.001 Eval Acc: 0.643\n",
            "[LeNet5_relu6] Epoch: 098 Train Loss: 1.060 Train Acc: 0.622 Eval Loss: 1.001 Eval Acc: 0.643\n",
            "[LeNet5_relu6] Epoch: 099 Train Loss: 1.059 Train Acc: 0.624 Eval Loss: 1.001 Eval Acc: 0.643\n",
            "[LeNet5_relu6] Epoch: 100 Train Loss: 1.057 Train Acc: 0.625 Eval Loss: 1.002 Eval Acc: 0.643\n",
            "\n",
            "Training ResNet20 with relu\n",
            "[ResNet20_relu] Epoch: 000 Eval Loss: 2.307 Eval Acc: 0.100\n",
            "[ResNet20_relu] Epoch: 001 Train Loss: 1.856 Train Acc: 0.301 Eval Loss: 1.626 Eval Acc: 0.392\n",
            "[ResNet20_relu] Epoch: 002 Train Loss: 1.535 Train Acc: 0.427 Eval Loss: 1.475 Eval Acc: 0.463\n",
            "[ResNet20_relu] Epoch: 003 Train Loss: 1.385 Train Acc: 0.495 Eval Loss: 1.311 Eval Acc: 0.523\n",
            "[ResNet20_relu] Epoch: 004 Train Loss: 1.267 Train Acc: 0.539 Eval Loss: 1.289 Eval Acc: 0.533\n",
            "[ResNet20_relu] Epoch: 005 Train Loss: 1.186 Train Acc: 0.573 Eval Loss: 1.145 Eval Acc: 0.586\n",
            "[ResNet20_relu] Epoch: 006 Train Loss: 1.108 Train Acc: 0.601 Eval Loss: 1.074 Eval Acc: 0.615\n",
            "[ResNet20_relu] Epoch: 007 Train Loss: 1.043 Train Acc: 0.626 Eval Loss: 1.009 Eval Acc: 0.644\n",
            "[ResNet20_relu] Epoch: 008 Train Loss: 0.990 Train Acc: 0.647 Eval Loss: 0.986 Eval Acc: 0.651\n",
            "[ResNet20_relu] Epoch: 009 Train Loss: 0.945 Train Acc: 0.662 Eval Loss: 0.914 Eval Acc: 0.681\n",
            "[ResNet20_relu] Epoch: 010 Train Loss: 0.905 Train Acc: 0.679 Eval Loss: 1.023 Eval Acc: 0.657\n",
            "[ResNet20_relu] Epoch: 011 Train Loss: 0.869 Train Acc: 0.691 Eval Loss: 0.856 Eval Acc: 0.702\n",
            "[ResNet20_relu] Epoch: 012 Train Loss: 0.845 Train Acc: 0.699 Eval Loss: 0.843 Eval Acc: 0.705\n",
            "[ResNet20_relu] Epoch: 013 Train Loss: 0.815 Train Acc: 0.711 Eval Loss: 0.806 Eval Acc: 0.721\n",
            "[ResNet20_relu] Epoch: 014 Train Loss: 0.785 Train Acc: 0.723 Eval Loss: 0.787 Eval Acc: 0.731\n",
            "[ResNet20_relu] Epoch: 015 Train Loss: 0.764 Train Acc: 0.733 Eval Loss: 0.785 Eval Acc: 0.731\n",
            "[ResNet20_relu] Epoch: 016 Train Loss: 0.747 Train Acc: 0.737 Eval Loss: 0.784 Eval Acc: 0.731\n",
            "[ResNet20_relu] Epoch: 017 Train Loss: 0.720 Train Acc: 0.748 Eval Loss: 0.796 Eval Acc: 0.729\n",
            "[ResNet20_relu] Epoch: 018 Train Loss: 0.701 Train Acc: 0.755 Eval Loss: 0.713 Eval Acc: 0.755\n",
            "[ResNet20_relu] Epoch: 019 Train Loss: 0.685 Train Acc: 0.760 Eval Loss: 0.715 Eval Acc: 0.754\n",
            "[ResNet20_relu] Epoch: 020 Train Loss: 0.669 Train Acc: 0.767 Eval Loss: 0.672 Eval Acc: 0.771\n",
            "[ResNet20_relu] Epoch: 021 Train Loss: 0.650 Train Acc: 0.774 Eval Loss: 0.640 Eval Acc: 0.784\n",
            "[ResNet20_relu] Epoch: 022 Train Loss: 0.634 Train Acc: 0.776 Eval Loss: 0.654 Eval Acc: 0.776\n",
            "[ResNet20_relu] Epoch: 023 Train Loss: 0.621 Train Acc: 0.784 Eval Loss: 0.717 Eval Acc: 0.754\n",
            "[ResNet20_relu] Epoch: 024 Train Loss: 0.608 Train Acc: 0.788 Eval Loss: 0.618 Eval Acc: 0.790\n",
            "[ResNet20_relu] Epoch: 025 Train Loss: 0.603 Train Acc: 0.790 Eval Loss: 0.636 Eval Acc: 0.784\n",
            "[ResNet20_relu] Epoch: 026 Train Loss: 0.584 Train Acc: 0.797 Eval Loss: 0.680 Eval Acc: 0.774\n",
            "[ResNet20_relu] Epoch: 027 Train Loss: 0.574 Train Acc: 0.798 Eval Loss: 0.613 Eval Acc: 0.795\n",
            "[ResNet20_relu] Epoch: 028 Train Loss: 0.565 Train Acc: 0.803 Eval Loss: 0.614 Eval Acc: 0.791\n",
            "[ResNet20_relu] Epoch: 029 Train Loss: 0.551 Train Acc: 0.807 Eval Loss: 0.585 Eval Acc: 0.808\n",
            "[ResNet20_relu] Epoch: 030 Train Loss: 0.547 Train Acc: 0.809 Eval Loss: 0.578 Eval Acc: 0.805\n",
            "[ResNet20_relu] Epoch: 031 Train Loss: 0.536 Train Acc: 0.814 Eval Loss: 0.564 Eval Acc: 0.812\n",
            "[ResNet20_relu] Epoch: 032 Train Loss: 0.526 Train Acc: 0.817 Eval Loss: 0.609 Eval Acc: 0.801\n",
            "[ResNet20_relu] Epoch: 033 Train Loss: 0.514 Train Acc: 0.820 Eval Loss: 0.582 Eval Acc: 0.805\n",
            "[ResNet20_relu] Epoch: 034 Train Loss: 0.513 Train Acc: 0.821 Eval Loss: 0.576 Eval Acc: 0.808\n",
            "[ResNet20_relu] Epoch: 035 Train Loss: 0.503 Train Acc: 0.824 Eval Loss: 0.565 Eval Acc: 0.812\n",
            "[ResNet20_relu] Epoch: 036 Train Loss: 0.502 Train Acc: 0.824 Eval Loss: 0.612 Eval Acc: 0.800\n",
            "[ResNet20_relu] Epoch: 037 Train Loss: 0.492 Train Acc: 0.830 Eval Loss: 0.574 Eval Acc: 0.808\n",
            "[ResNet20_relu] Epoch: 038 Train Loss: 0.484 Train Acc: 0.831 Eval Loss: 0.546 Eval Acc: 0.818\n",
            "[ResNet20_relu] Epoch: 039 Train Loss: 0.479 Train Acc: 0.834 Eval Loss: 0.558 Eval Acc: 0.816\n",
            "[ResNet20_relu] Epoch: 040 Train Loss: 0.469 Train Acc: 0.839 Eval Loss: 0.564 Eval Acc: 0.812\n",
            "[ResNet20_relu] Epoch: 041 Train Loss: 0.465 Train Acc: 0.838 Eval Loss: 0.536 Eval Acc: 0.823\n",
            "[ResNet20_relu] Epoch: 042 Train Loss: 0.454 Train Acc: 0.843 Eval Loss: 0.583 Eval Acc: 0.810\n",
            "[ResNet20_relu] Epoch: 043 Train Loss: 0.453 Train Acc: 0.841 Eval Loss: 0.506 Eval Acc: 0.835\n",
            "[ResNet20_relu] Epoch: 044 Train Loss: 0.448 Train Acc: 0.845 Eval Loss: 0.513 Eval Acc: 0.829\n",
            "[ResNet20_relu] Epoch: 045 Train Loss: 0.444 Train Acc: 0.846 Eval Loss: 0.510 Eval Acc: 0.829\n",
            "[ResNet20_relu] Epoch: 046 Train Loss: 0.436 Train Acc: 0.848 Eval Loss: 0.571 Eval Acc: 0.811\n",
            "[ResNet20_relu] Epoch: 047 Train Loss: 0.432 Train Acc: 0.849 Eval Loss: 0.512 Eval Acc: 0.832\n",
            "[ResNet20_relu] Epoch: 048 Train Loss: 0.426 Train Acc: 0.853 Eval Loss: 0.494 Eval Acc: 0.834\n",
            "[ResNet20_relu] Epoch: 049 Train Loss: 0.423 Train Acc: 0.853 Eval Loss: 0.482 Eval Acc: 0.840\n",
            "[ResNet20_relu] Epoch: 050 Train Loss: 0.417 Train Acc: 0.855 Eval Loss: 0.538 Eval Acc: 0.828\n",
            "[ResNet20_relu] Epoch: 051 Train Loss: 0.411 Train Acc: 0.858 Eval Loss: 0.496 Eval Acc: 0.840\n",
            "[ResNet20_relu] Epoch: 052 Train Loss: 0.408 Train Acc: 0.859 Eval Loss: 0.493 Eval Acc: 0.838\n",
            "[ResNet20_relu] Epoch: 053 Train Loss: 0.398 Train Acc: 0.861 Eval Loss: 0.523 Eval Acc: 0.827\n",
            "[ResNet20_relu] Epoch: 054 Train Loss: 0.401 Train Acc: 0.858 Eval Loss: 0.540 Eval Acc: 0.827\n",
            "[ResNet20_relu] Epoch: 055 Train Loss: 0.397 Train Acc: 0.862 Eval Loss: 0.494 Eval Acc: 0.835\n",
            "[ResNet20_relu] Epoch: 056 Train Loss: 0.389 Train Acc: 0.864 Eval Loss: 0.462 Eval Acc: 0.850\n",
            "[ResNet20_relu] Epoch: 057 Train Loss: 0.385 Train Acc: 0.867 Eval Loss: 0.478 Eval Acc: 0.844\n",
            "[ResNet20_relu] Epoch: 058 Train Loss: 0.384 Train Acc: 0.866 Eval Loss: 0.482 Eval Acc: 0.840\n",
            "[ResNet20_relu] Epoch: 059 Train Loss: 0.379 Train Acc: 0.868 Eval Loss: 0.491 Eval Acc: 0.839\n",
            "[ResNet20_relu] Epoch: 060 Train Loss: 0.379 Train Acc: 0.868 Eval Loss: 0.499 Eval Acc: 0.840\n",
            "[ResNet20_relu] Epoch: 061 Train Loss: 0.337 Train Acc: 0.883 Eval Loss: 0.408 Eval Acc: 0.861\n",
            "[ResNet20_relu] Epoch: 062 Train Loss: 0.327 Train Acc: 0.886 Eval Loss: 0.407 Eval Acc: 0.861\n",
            "[ResNet20_relu] Epoch: 063 Train Loss: 0.324 Train Acc: 0.888 Eval Loss: 0.404 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 064 Train Loss: 0.323 Train Acc: 0.887 Eval Loss: 0.405 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 065 Train Loss: 0.320 Train Acc: 0.889 Eval Loss: 0.403 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 066 Train Loss: 0.320 Train Acc: 0.889 Eval Loss: 0.409 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 067 Train Loss: 0.316 Train Acc: 0.891 Eval Loss: 0.407 Eval Acc: 0.862\n",
            "[ResNet20_relu] Epoch: 068 Train Loss: 0.320 Train Acc: 0.891 Eval Loss: 0.409 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 069 Train Loss: 0.318 Train Acc: 0.891 Eval Loss: 0.405 Eval Acc: 0.865\n",
            "[ResNet20_relu] Epoch: 070 Train Loss: 0.316 Train Acc: 0.889 Eval Loss: 0.407 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 071 Train Loss: 0.312 Train Acc: 0.892 Eval Loss: 0.405 Eval Acc: 0.862\n",
            "[ResNet20_relu] Epoch: 072 Train Loss: 0.315 Train Acc: 0.892 Eval Loss: 0.407 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 073 Train Loss: 0.311 Train Acc: 0.892 Eval Loss: 0.405 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 074 Train Loss: 0.315 Train Acc: 0.890 Eval Loss: 0.405 Eval Acc: 0.865\n",
            "[ResNet20_relu] Epoch: 075 Train Loss: 0.310 Train Acc: 0.894 Eval Loss: 0.406 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 076 Train Loss: 0.310 Train Acc: 0.892 Eval Loss: 0.405 Eval Acc: 0.865\n",
            "[ResNet20_relu] Epoch: 077 Train Loss: 0.310 Train Acc: 0.893 Eval Loss: 0.403 Eval Acc: 0.866\n",
            "[ResNet20_relu] Epoch: 078 Train Loss: 0.306 Train Acc: 0.894 Eval Loss: 0.411 Eval Acc: 0.863\n",
            "[ResNet20_relu] Epoch: 079 Train Loss: 0.309 Train Acc: 0.892 Eval Loss: 0.405 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 080 Train Loss: 0.309 Train Acc: 0.894 Eval Loss: 0.405 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 081 Train Loss: 0.309 Train Acc: 0.893 Eval Loss: 0.403 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 082 Train Loss: 0.306 Train Acc: 0.894 Eval Loss: 0.403 Eval Acc: 0.866\n",
            "[ResNet20_relu] Epoch: 083 Train Loss: 0.304 Train Acc: 0.894 Eval Loss: 0.411 Eval Acc: 0.862\n",
            "[ResNet20_relu] Epoch: 084 Train Loss: 0.305 Train Acc: 0.893 Eval Loss: 0.407 Eval Acc: 0.865\n",
            "[ResNet20_relu] Epoch: 085 Train Loss: 0.300 Train Acc: 0.897 Eval Loss: 0.408 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 086 Train Loss: 0.304 Train Acc: 0.894 Eval Loss: 0.408 Eval Acc: 0.864\n",
            "[ResNet20_relu] Epoch: 087 Train Loss: 0.301 Train Acc: 0.895 Eval Loss: 0.406 Eval Acc: 0.865\n",
            "Early stopping was triggered!\n",
            "\n",
            "Training ResNet20 with hardtanh\n",
            "[ResNet20_hardtanh] Epoch: 000 Eval Loss: 2.309 Eval Acc: 0.147\n",
            "[ResNet20_hardtanh] Epoch: 001 Train Loss: 2.035 Train Acc: 0.233 Eval Loss: 1.873 Eval Acc: 0.301\n",
            "[ResNet20_hardtanh] Epoch: 002 Train Loss: 1.796 Train Acc: 0.333 Eval Loss: 1.680 Eval Acc: 0.374\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[160], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m activation \u001b[38;5;129;01min\u001b[39;00m activations:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     trained_models[(model_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activation)] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_orig_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[156], line 6\u001b[0m, in \u001b[0;36mtrain_orig_model\u001b[0;34m(model_class, activation)\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(activation\u001b[38;5;241m=\u001b[39mactivation)\u001b[38;5;241m.\u001b[39mto(cuda_device)\n\u001b[1;32m      4\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[155], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, num_epochs, writer, model_name, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 49\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     50\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     52\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train all combinations (without quantization)\n",
        "trained_models = {}\n",
        "for model_class in models:\n",
        "    for activation in activations:\n",
        "        print(f\"\\nTraining {model_class.__name__} with {activation}\")\n",
        "        trained_models[(model_class.__name__, activation)] = train_orig_model(model_class, activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_GxJiVZPI-F",
        "outputId": "8b523ab4-9a87-4a99-bb52-6e1fda59ec5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Quantizing LeNet5 with relu\n",
            "[LeNet5_relu_quantized] initial state: LeNet5(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_relu_quantized] after configure_qat: LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1174/1108861780.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LeNet5_relu_quantized] Epoch: 000 Eval Loss: 1.124 Eval Acc: 0.610\n",
            "[LeNet5_relu_quantized] Epoch: 001 Train Loss: 1.143 Train Acc: 0.595 Eval Loss: 1.027 Eval Acc: 0.642\n",
            "[LeNet5_relu_quantized] Epoch: 002 Train Loss: 1.150 Train Acc: 0.593 Eval Loss: 1.041 Eval Acc: 0.633\n",
            "[LeNet5_relu_quantized] Epoch: 003 Train Loss: 1.144 Train Acc: 0.594 Eval Loss: 1.055 Eval Acc: 0.630\n",
            "[LeNet5_relu_quantized] Epoch: 004 Train Loss: 1.148 Train Acc: 0.594 Eval Loss: 1.051 Eval Acc: 0.634\n",
            "[LeNet5_relu_quantized] Epoch: 005 Train Loss: 1.144 Train Acc: 0.595 Eval Loss: 1.039 Eval Acc: 0.633\n",
            "[LeNet5_relu_quantized] Epoch: 006 Train Loss: 1.147 Train Acc: 0.595 Eval Loss: 1.084 Eval Acc: 0.622\n",
            "[LeNet5_relu_quantized] Epoch: 007 Train Loss: 1.137 Train Acc: 0.597 Eval Loss: 1.028 Eval Acc: 0.644\n",
            "[LeNet5_relu_quantized] Epoch: 008 Train Loss: 1.135 Train Acc: 0.599 Eval Loss: 1.021 Eval Acc: 0.639\n",
            "[LeNet5_relu_quantized] Epoch: 009 Train Loss: 1.141 Train Acc: 0.597 Eval Loss: 1.057 Eval Acc: 0.632\n",
            "[LeNet5_relu_quantized] Epoch: 010 Train Loss: 1.135 Train Acc: 0.598 Eval Loss: 1.005 Eval Acc: 0.650\n",
            "[LeNet5_relu_quantized] Epoch: 011 Train Loss: 1.120 Train Acc: 0.605 Eval Loss: 1.056 Eval Acc: 0.628\n",
            "[LeNet5_relu_quantized] Epoch: 012 Train Loss: 1.121 Train Acc: 0.603 Eval Loss: 1.059 Eval Acc: 0.630\n",
            "[LeNet5_relu_quantized] Epoch: 013 Train Loss: 1.126 Train Acc: 0.604 Eval Loss: 1.023 Eval Acc: 0.644\n",
            "[LeNet5_relu_quantized] Epoch: 014 Train Loss: 1.124 Train Acc: 0.601 Eval Loss: 1.077 Eval Acc: 0.627\n",
            "[LeNet5_relu_quantized] Epoch: 015 Train Loss: 1.120 Train Acc: 0.607 Eval Loss: 1.029 Eval Acc: 0.647\n",
            "[LeNet5_relu_quantized] Epoch: 016 Train Loss: 1.113 Train Acc: 0.608 Eval Loss: 0.997 Eval Acc: 0.653\n",
            "[LeNet5_relu_quantized] Epoch: 017 Train Loss: 1.111 Train Acc: 0.609 Eval Loss: 0.993 Eval Acc: 0.655\n",
            "[LeNet5_relu_quantized] Epoch: 018 Train Loss: 1.110 Train Acc: 0.608 Eval Loss: 1.020 Eval Acc: 0.644\n",
            "[LeNet5_relu_quantized] Epoch: 019 Train Loss: 1.105 Train Acc: 0.611 Eval Loss: 1.064 Eval Acc: 0.627\n",
            "[LeNet5_relu_quantized] Epoch: 020 Train Loss: 1.105 Train Acc: 0.610 Eval Loss: 1.006 Eval Acc: 0.650\n",
            "[LeNet5_relu_quantized] Epoch: 021 Train Loss: 1.097 Train Acc: 0.613 Eval Loss: 0.997 Eval Acc: 0.647\n",
            "[LeNet5_relu_quantized] Epoch: 022 Train Loss: 1.096 Train Acc: 0.614 Eval Loss: 1.014 Eval Acc: 0.644\n",
            "[LeNet5_relu_quantized] Epoch: 023 Train Loss: 1.093 Train Acc: 0.614 Eval Loss: 0.981 Eval Acc: 0.657\n",
            "[LeNet5_relu_quantized] Epoch: 024 Train Loss: 1.090 Train Acc: 0.615 Eval Loss: 0.986 Eval Acc: 0.657\n",
            "[LeNet5_relu_quantized] Epoch: 025 Train Loss: 1.090 Train Acc: 0.617 Eval Loss: 0.970 Eval Acc: 0.663\n",
            "\n",
            "Quantizing LeNet5 with hardtanh\n",
            "[LeNet5_hardtanh_quantized] initial state: LeNet5(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_hardtanh_quantized] after configure_qat: LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "[LeNet5_hardtanh_quantized] Epoch: 000 Eval Loss: 1.075 Eval Acc: 0.615\n",
            "[LeNet5_hardtanh_quantized] Epoch: 001 Train Loss: 1.197 Train Acc: 0.571 Eval Loss: 1.062 Eval Acc: 0.622\n",
            "[LeNet5_hardtanh_quantized] Epoch: 002 Train Loss: 1.200 Train Acc: 0.573 Eval Loss: 1.068 Eval Acc: 0.622\n",
            "[LeNet5_hardtanh_quantized] Epoch: 003 Train Loss: 1.198 Train Acc: 0.573 Eval Loss: 1.102 Eval Acc: 0.608\n",
            "[LeNet5_hardtanh_quantized] Epoch: 004 Train Loss: 1.200 Train Acc: 0.571 Eval Loss: 1.098 Eval Acc: 0.611\n",
            "[LeNet5_hardtanh_quantized] Epoch: 005 Train Loss: 1.200 Train Acc: 0.572 Eval Loss: 1.137 Eval Acc: 0.595\n",
            "[LeNet5_hardtanh_quantized] Epoch: 006 Train Loss: 1.196 Train Acc: 0.572 Eval Loss: 1.078 Eval Acc: 0.617\n",
            "[LeNet5_hardtanh_quantized] Epoch: 007 Train Loss: 1.189 Train Acc: 0.575 Eval Loss: 1.084 Eval Acc: 0.612\n",
            "[LeNet5_hardtanh_quantized] Epoch: 008 Train Loss: 1.193 Train Acc: 0.573 Eval Loss: 1.076 Eval Acc: 0.620\n",
            "[LeNet5_hardtanh_quantized] Epoch: 009 Train Loss: 1.187 Train Acc: 0.576 Eval Loss: 1.081 Eval Acc: 0.617\n",
            "[LeNet5_hardtanh_quantized] Epoch: 010 Train Loss: 1.181 Train Acc: 0.580 Eval Loss: 1.175 Eval Acc: 0.583\n",
            "[LeNet5_hardtanh_quantized] Epoch: 011 Train Loss: 1.203 Train Acc: 0.573 Eval Loss: 1.114 Eval Acc: 0.605\n",
            "[LeNet5_hardtanh_quantized] Epoch: 012 Train Loss: 1.194 Train Acc: 0.573 Eval Loss: 1.074 Eval Acc: 0.622\n",
            "[LeNet5_hardtanh_quantized] Epoch: 013 Train Loss: 1.184 Train Acc: 0.577 Eval Loss: 1.087 Eval Acc: 0.608\n",
            "[LeNet5_hardtanh_quantized] Epoch: 014 Train Loss: 1.180 Train Acc: 0.576 Eval Loss: 1.061 Eval Acc: 0.621\n",
            "[LeNet5_hardtanh_quantized] Epoch: 015 Train Loss: 1.178 Train Acc: 0.580 Eval Loss: 1.057 Eval Acc: 0.624\n",
            "[LeNet5_hardtanh_quantized] Epoch: 016 Train Loss: 1.177 Train Acc: 0.581 Eval Loss: 1.132 Eval Acc: 0.595\n",
            "[LeNet5_hardtanh_quantized] Epoch: 017 Train Loss: 1.172 Train Acc: 0.581 Eval Loss: 1.063 Eval Acc: 0.620\n",
            "[LeNet5_hardtanh_quantized] Epoch: 018 Train Loss: 1.166 Train Acc: 0.585 Eval Loss: 1.061 Eval Acc: 0.624\n",
            "[LeNet5_hardtanh_quantized] Epoch: 019 Train Loss: 1.165 Train Acc: 0.585 Eval Loss: 1.050 Eval Acc: 0.627\n",
            "[LeNet5_hardtanh_quantized] Epoch: 020 Train Loss: 1.163 Train Acc: 0.586 Eval Loss: 1.027 Eval Acc: 0.633\n",
            "[LeNet5_hardtanh_quantized] Epoch: 021 Train Loss: 1.160 Train Acc: 0.589 Eval Loss: 1.056 Eval Acc: 0.625\n",
            "[LeNet5_hardtanh_quantized] Epoch: 022 Train Loss: 1.155 Train Acc: 0.588 Eval Loss: 1.048 Eval Acc: 0.627\n",
            "[LeNet5_hardtanh_quantized] Epoch: 023 Train Loss: 1.159 Train Acc: 0.590 Eval Loss: 1.023 Eval Acc: 0.638\n",
            "[LeNet5_hardtanh_quantized] Epoch: 024 Train Loss: 1.150 Train Acc: 0.591 Eval Loss: 1.075 Eval Acc: 0.616\n",
            "[LeNet5_hardtanh_quantized] Epoch: 025 Train Loss: 1.151 Train Acc: 0.591 Eval Loss: 1.029 Eval Acc: 0.634\n",
            "\n",
            "Quantizing LeNet5 with relu6\n",
            "[LeNet5_relu6_quantized] initial state: LeNet5(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU6(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU6(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU6(inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): ReLU6(inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[LeNet5_relu6_quantized] after configure_qat: LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "[LeNet5_relu6_quantized] Epoch: 000 Eval Loss: 1.078 Eval Acc: 0.615\n",
            "[LeNet5_relu6_quantized] Epoch: 001 Train Loss: 1.136 Train Acc: 0.594 Eval Loss: 1.044 Eval Acc: 0.625\n",
            "[LeNet5_relu6_quantized] Epoch: 002 Train Loss: 1.145 Train Acc: 0.593 Eval Loss: 1.089 Eval Acc: 0.610\n",
            "[LeNet5_relu6_quantized] Epoch: 003 Train Loss: 1.143 Train Acc: 0.591 Eval Loss: 1.072 Eval Acc: 0.613\n",
            "[LeNet5_relu6_quantized] Epoch: 004 Train Loss: 1.144 Train Acc: 0.593 Eval Loss: 1.053 Eval Acc: 0.628\n",
            "[LeNet5_relu6_quantized] Epoch: 005 Train Loss: 1.145 Train Acc: 0.592 Eval Loss: 1.056 Eval Acc: 0.621\n",
            "[LeNet5_relu6_quantized] Epoch: 006 Train Loss: 1.136 Train Acc: 0.593 Eval Loss: 1.043 Eval Acc: 0.625\n",
            "[LeNet5_relu6_quantized] Epoch: 007 Train Loss: 1.139 Train Acc: 0.595 Eval Loss: 1.037 Eval Acc: 0.632\n",
            "[LeNet5_relu6_quantized] Epoch: 008 Train Loss: 1.126 Train Acc: 0.601 Eval Loss: 1.058 Eval Acc: 0.627\n",
            "[LeNet5_relu6_quantized] Epoch: 009 Train Loss: 1.121 Train Acc: 0.600 Eval Loss: 1.033 Eval Acc: 0.632\n",
            "[LeNet5_relu6_quantized] Epoch: 010 Train Loss: 1.120 Train Acc: 0.600 Eval Loss: 1.071 Eval Acc: 0.618\n",
            "[LeNet5_relu6_quantized] Epoch: 011 Train Loss: 1.121 Train Acc: 0.601 Eval Loss: 1.100 Eval Acc: 0.604\n",
            "[LeNet5_relu6_quantized] Epoch: 012 Train Loss: 1.117 Train Acc: 0.602 Eval Loss: 1.061 Eval Acc: 0.619\n",
            "[LeNet5_relu6_quantized] Epoch: 013 Train Loss: 1.116 Train Acc: 0.604 Eval Loss: 1.064 Eval Acc: 0.622\n",
            "[LeNet5_relu6_quantized] Epoch: 014 Train Loss: 1.123 Train Acc: 0.598 Eval Loss: 1.060 Eval Acc: 0.627\n",
            "[LeNet5_relu6_quantized] Epoch: 015 Train Loss: 1.123 Train Acc: 0.601 Eval Loss: 1.060 Eval Acc: 0.621\n",
            "[LeNet5_relu6_quantized] Epoch: 016 Train Loss: 1.112 Train Acc: 0.604 Eval Loss: 1.033 Eval Acc: 0.633\n",
            "[LeNet5_relu6_quantized] Epoch: 017 Train Loss: 1.109 Train Acc: 0.605 Eval Loss: 1.051 Eval Acc: 0.624\n",
            "[LeNet5_relu6_quantized] Epoch: 018 Train Loss: 1.118 Train Acc: 0.602 Eval Loss: 1.020 Eval Acc: 0.640\n",
            "[LeNet5_relu6_quantized] Epoch: 019 Train Loss: 1.103 Train Acc: 0.608 Eval Loss: 1.011 Eval Acc: 0.646\n",
            "[LeNet5_relu6_quantized] Epoch: 020 Train Loss: 1.111 Train Acc: 0.603 Eval Loss: 1.002 Eval Acc: 0.643\n",
            "[LeNet5_relu6_quantized] Epoch: 021 Train Loss: 1.097 Train Acc: 0.610 Eval Loss: 1.009 Eval Acc: 0.640\n",
            "[LeNet5_relu6_quantized] Epoch: 022 Train Loss: 1.105 Train Acc: 0.606 Eval Loss: 1.049 Eval Acc: 0.629\n",
            "[LeNet5_relu6_quantized] Epoch: 023 Train Loss: 1.100 Train Acc: 0.609 Eval Loss: 1.017 Eval Acc: 0.635\n",
            "[LeNet5_relu6_quantized] Epoch: 024 Train Loss: 1.094 Train Acc: 0.613 Eval Loss: 1.046 Eval Acc: 0.630\n",
            "[LeNet5_relu6_quantized] Epoch: 025 Train Loss: 1.098 Train Acc: 0.610 Eval Loss: 1.024 Eval Acc: 0.639\n",
            "\n",
            "Quantizing ResNet20 with relu\n",
            "[ResNet20_relu_quantized] initial state: ResNet(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act_fn): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "[ResNet20_relu_quantized] after configure_qat: ResNet(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv): Conv2d(\n",
            "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            "  (bn): BatchNorm2d(\n",
            "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            "  (act_fn): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(\n",
            "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "          )\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "        (1): BatchNorm2d(\n",
            "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(\n",
            "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "          )\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "        (1): BatchNorm2d(\n",
            "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
            "  (fc): Linear(\n",
            "    in_features=64, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "[ResNet20_relu_quantized] Epoch: 000 Eval Loss: 0.548 Eval Acc: 0.818\n",
            "[ResNet20_relu_quantized] Epoch: 001 Train Loss: 0.426 Train Acc: 0.852 Eval Loss: 0.540 Eval Acc: 0.821\n",
            "[ResNet20_relu_quantized] Epoch: 002 Train Loss: 0.422 Train Acc: 0.850 Eval Loss: 0.539 Eval Acc: 0.818\n",
            "[ResNet20_relu_quantized] Epoch: 003 Train Loss: 0.417 Train Acc: 0.854 Eval Loss: 0.562 Eval Acc: 0.815\n",
            "[ResNet20_relu_quantized] Epoch: 004 Train Loss: 0.411 Train Acc: 0.854 Eval Loss: 0.566 Eval Acc: 0.819\n",
            "[ResNet20_relu_quantized] Epoch: 005 Train Loss: 0.413 Train Acc: 0.856 Eval Loss: 0.499 Eval Acc: 0.833\n",
            "[ResNet20_relu_quantized] Epoch: 006 Train Loss: 0.404 Train Acc: 0.858 Eval Loss: 0.505 Eval Acc: 0.834\n",
            "[ResNet20_relu_quantized] Epoch: 007 Train Loss: 0.400 Train Acc: 0.859 Eval Loss: 0.536 Eval Acc: 0.824\n",
            "[ResNet20_relu_quantized] Epoch: 008 Train Loss: 0.401 Train Acc: 0.859 Eval Loss: 0.530 Eval Acc: 0.826\n",
            "[ResNet20_relu_quantized] Epoch: 009 Train Loss: 0.401 Train Acc: 0.859 Eval Loss: 0.500 Eval Acc: 0.833\n",
            "[ResNet20_relu_quantized] Epoch: 010 Train Loss: 0.393 Train Acc: 0.863 Eval Loss: 0.500 Eval Acc: 0.839\n",
            "[ResNet20_relu_quantized] Epoch: 011 Train Loss: 0.392 Train Acc: 0.862 Eval Loss: 0.509 Eval Acc: 0.833\n",
            "[ResNet20_relu_quantized] Epoch: 012 Train Loss: 0.387 Train Acc: 0.865 Eval Loss: 0.533 Eval Acc: 0.830\n",
            "[ResNet20_relu_quantized] Epoch: 013 Train Loss: 0.388 Train Acc: 0.863 Eval Loss: 0.492 Eval Acc: 0.838\n",
            "[ResNet20_relu_quantized] Epoch: 014 Train Loss: 0.388 Train Acc: 0.863 Eval Loss: 0.526 Eval Acc: 0.828\n",
            "[ResNet20_relu_quantized] Epoch: 015 Train Loss: 0.376 Train Acc: 0.867 Eval Loss: 0.524 Eval Acc: 0.830\n",
            "[ResNet20_relu_quantized] Epoch: 016 Train Loss: 0.377 Train Acc: 0.867 Eval Loss: 0.524 Eval Acc: 0.831\n",
            "[ResNet20_relu_quantized] Epoch: 017 Train Loss: 0.373 Train Acc: 0.869 Eval Loss: 0.495 Eval Acc: 0.840\n",
            "[ResNet20_relu_quantized] Epoch: 018 Train Loss: 0.375 Train Acc: 0.868 Eval Loss: 0.516 Eval Acc: 0.830\n",
            "[ResNet20_relu_quantized] Epoch: 019 Train Loss: 0.373 Train Acc: 0.870 Eval Loss: 0.524 Eval Acc: 0.825\n",
            "[ResNet20_relu_quantized] Epoch: 020 Train Loss: 0.370 Train Acc: 0.872 Eval Loss: 0.499 Eval Acc: 0.838\n",
            "[ResNet20_relu_quantized] Epoch: 021 Train Loss: 0.361 Train Acc: 0.875 Eval Loss: 0.470 Eval Acc: 0.843\n",
            "[ResNet20_relu_quantized] Epoch: 022 Train Loss: 0.370 Train Acc: 0.870 Eval Loss: 0.518 Eval Acc: 0.833\n",
            "[ResNet20_relu_quantized] Epoch: 023 Train Loss: 0.357 Train Acc: 0.875 Eval Loss: 0.496 Eval Acc: 0.839\n",
            "[ResNet20_relu_quantized] Epoch: 024 Train Loss: 0.359 Train Acc: 0.875 Eval Loss: 0.487 Eval Acc: 0.839\n",
            "[ResNet20_relu_quantized] Epoch: 025 Train Loss: 0.356 Train Acc: 0.875 Eval Loss: 0.457 Eval Acc: 0.849\n",
            "\n",
            "Quantizing ResNet20 with hardtanh\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoint/ResNet20_hardtanh.ckpt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[170], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuantizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m quantized_models[(model_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activation)] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_quantized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[169], line 6\u001b[0m, in \u001b[0;36mtrain_quantized_model\u001b[0;34m(model_class, activation, ckpt_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(activation\u001b[38;5;241m=\u001b[39mactivation, quantize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] initial state:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint/ResNet20_hardtanh.ckpt'"
          ]
        }
      ],
      "source": [
        "# Train quantized versions\n",
        "\n",
        "quantized_models = {}\n",
        "for model_class in models:\n",
        "    for activation in activations:\n",
        "        ckpt_path = f\"checkpoint/{model_class.__name__}_{activation}.ckpt\"\n",
        "        print(f\"\\nQuantizing {model_class.__name__} with {activation}\")\n",
        "        quantized_models[(model_class.__name__, activation)] = train_quantized_model(model_class, activation, ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX5ZzmKoJrRA"
      },
      "source": [
        "**Сравнение полученных моделей**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "XhAONBNliNq9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0826, 0.0611, 0.0373, 0.0597, 0.1061, 0.0572], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.5735, -0.4584, -0.2217, -0.4479, -0.7958, -0.4291], device='cuda:0'), max_val=tensor([0.6197, 0.3445, 0.2797, 0.3303, 0.5830, 0.3701], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0508, 0.0392, 0.0345, 0.0425, 0.0345, 0.0519, 0.0346, 0.0358, 0.0486,\n",
            "                0.0362, 0.0369, 0.0399, 0.0325, 0.0490, 0.0325, 0.0406],\n",
            "               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.3814, -0.1795, -0.2342, -0.3190, -0.1841, -0.3889, -0.2438, -0.2120,\n",
            "                  -0.3646, -0.2713, -0.2164, -0.2384, -0.1856, -0.3112, -0.1681, -0.1561],\n",
            "                 device='cuda:0'), max_val=tensor([0.3306, 0.2942, 0.2591, 0.3025, 0.2584, 0.3877, 0.2597, 0.2684, 0.2356,\n",
            "                  0.1995, 0.2764, 0.2995, 0.2436, 0.3673, 0.2440, 0.3046],\n",
            "                 device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0145, 0.0141, 0.0164, 0.0095, 0.0242, 0.0157, 0.0065, 0.0138, 0.0099,\n",
            "                0.0159, 0.0158, 0.0167, 0.0142, 0.0154, 0.0219, 0.0176, 0.0145, 0.0177,\n",
            "                0.0156, 0.0141, 0.0135, 0.0066, 0.0146, 0.0154, 0.0167, 0.0159, 0.0163,\n",
            "                0.0126, 0.0102, 0.0157, 0.0174, 0.0151, 0.0211, 0.0163, 0.0134, 0.0227,\n",
            "                0.0255, 0.0162, 0.0192, 0.0171, 0.0070, 0.0216, 0.0204, 0.0173, 0.0194,\n",
            "                0.0159, 0.0074, 0.0134, 0.0137, 0.0228, 0.0120, 0.0066, 0.0167, 0.0169,\n",
            "                0.0222, 0.0154, 0.0150, 0.0130, 0.0147, 0.0117, 0.0188, 0.0160, 0.0194,\n",
            "                0.0166, 0.0179, 0.0104, 0.0095, 0.0113, 0.0124, 0.0155, 0.0165, 0.0133,\n",
            "                0.0134, 0.0101, 0.0178, 0.0129, 0.0177, 0.0185, 0.0132, 0.0118, 0.0147,\n",
            "                0.0144, 0.0184, 0.0146, 0.0158, 0.0227, 0.0158, 0.0180, 0.0129, 0.0139,\n",
            "                0.0145, 0.0163, 0.0155, 0.0162, 0.0169, 0.0228, 0.0070, 0.0121, 0.0156,\n",
            "                0.0136, 0.0134, 0.0148, 0.0146, 0.0214, 0.0095, 0.0179, 0.0205, 0.0184,\n",
            "                0.0067, 0.0123, 0.0203, 0.0149, 0.0240, 0.0103, 0.0091, 0.0068, 0.0099,\n",
            "                0.0110, 0.0139, 0.0072], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "               device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1042, -0.1049, -0.1227, -0.0711, -0.1818, -0.1174, -0.0487, -0.0882,\n",
            "                  -0.0741, -0.1193, -0.1082, -0.1256, -0.1023, -0.1155, -0.1222, -0.1093,\n",
            "                  -0.1086, -0.1324, -0.1170, -0.0888, -0.1015, -0.0499, -0.1051, -0.1154,\n",
            "                  -0.1208, -0.1134, -0.1222, -0.0908, -0.0762, -0.1177, -0.0872, -0.1130,\n",
            "                  -0.1583, -0.1224, -0.0910, -0.1704, -0.1912, -0.1123, -0.1141, -0.1286,\n",
            "                  -0.0527, -0.1597, -0.1234, -0.0917, -0.1363, -0.1195, -0.0555, -0.1002,\n",
            "                  -0.1026, -0.1709, -0.0892, -0.0484, -0.1254, -0.1009, -0.1667, -0.1134,\n",
            "                  -0.0984, -0.0976, -0.1102, -0.0879, -0.1411, -0.1202, -0.0928, -0.1213,\n",
            "                  -0.0895, -0.0731, -0.0710, -0.0847, -0.0889, -0.0993, -0.1239, -0.0999,\n",
            "                  -0.1007, -0.0761, -0.1334, -0.0845, -0.1328, -0.1389, -0.0897, -0.0886,\n",
            "                  -0.0873, -0.0804, -0.1384, -0.0972, -0.1140, -0.1702, -0.1148, -0.1350,\n",
            "                  -0.0968, -0.0750, -0.1088, -0.1225, -0.1164, -0.1219, -0.1131, -0.1399,\n",
            "                  -0.0529, -0.0905, -0.1172, -0.1020, -0.0856, -0.0992, -0.0942, -0.1124,\n",
            "                  -0.0605, -0.1346, -0.1240, -0.1377, -0.0504, -0.0919, -0.1485, -0.1116,\n",
            "                  -0.1803, -0.0743, -0.0583, -0.0507, -0.0742, -0.0718, -0.0724, -0.0538],\n",
            "                 device='cuda:0'), max_val=tensor([0.1088, 0.1054, 0.0869, 0.0700, 0.1116, 0.0842, 0.0485, 0.1038, 0.0725,\n",
            "                  0.0851, 0.1185, 0.1016, 0.1062, 0.0904, 0.1645, 0.1321, 0.1089, 0.1325,\n",
            "                  0.1080, 0.1054, 0.0940, 0.0468, 0.1098, 0.0819, 0.1254, 0.1191, 0.1067,\n",
            "                  0.0944, 0.0712, 0.0895, 0.1307, 0.0979, 0.1162, 0.1097, 0.1008, 0.1042,\n",
            "                  0.1317, 0.1214, 0.1439, 0.1234, 0.0504, 0.1619, 0.1533, 0.1298, 0.1456,\n",
            "                  0.1188, 0.0498, 0.0965, 0.1022, 0.1173, 0.0902, 0.0497, 0.0938, 0.1269,\n",
            "                  0.0951, 0.1156, 0.1128, 0.0936, 0.1006, 0.0820, 0.1107, 0.1132, 0.1454,\n",
            "                  0.1243, 0.1343, 0.0782, 0.0652, 0.0831, 0.0928, 0.1165, 0.0906, 0.0876,\n",
            "                  0.0830, 0.0758, 0.1259, 0.0971, 0.1123, 0.1127, 0.0987, 0.0778, 0.1105,\n",
            "                  0.1080, 0.1060, 0.1098, 0.1187, 0.0982, 0.1185, 0.1023, 0.0929, 0.1044,\n",
            "                  0.0926, 0.1076, 0.1061, 0.1060, 0.1265, 0.1709, 0.0468, 0.0817, 0.1147,\n",
            "                  0.0979, 0.1002, 0.1110, 0.1095, 0.1607, 0.0714, 0.1142, 0.1534, 0.1321,\n",
            "                  0.0495, 0.0859, 0.1522, 0.1112, 0.1082, 0.0772, 0.0686, 0.0471, 0.0739,\n",
            "                  0.0824, 0.1040, 0.0483], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0224, 0.0224, 0.0221, 0.0194, 0.0230, 0.0267, 0.0183, 0.0202, 0.0158,\n",
            "                0.0260, 0.0191, 0.0276, 0.0216, 0.0137, 0.0166, 0.0195, 0.0166, 0.0261,\n",
            "                0.0235, 0.0214, 0.0153, 0.0181, 0.0218, 0.0182, 0.0262, 0.0172, 0.0203,\n",
            "                0.0279, 0.0263, 0.0173, 0.0167, 0.0232, 0.0155, 0.0213, 0.0228, 0.0195,\n",
            "                0.0190, 0.0237, 0.0247, 0.0190, 0.0285, 0.0221, 0.0209, 0.0207, 0.0236,\n",
            "                0.0215, 0.0230, 0.0188, 0.0281, 0.0223, 0.0117, 0.0121, 0.0156, 0.0186,\n",
            "                0.0218, 0.0246, 0.0117, 0.0220, 0.0191, 0.0316, 0.0196, 0.0184, 0.0178,\n",
            "                0.0234, 0.0257, 0.0194, 0.0214, 0.0125, 0.0236, 0.0157, 0.0209, 0.0199,\n",
            "                0.0312, 0.0124, 0.0211, 0.0248, 0.0232, 0.0210, 0.0244, 0.0207, 0.0199,\n",
            "                0.0225, 0.0201, 0.0269], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1683, -0.1677, -0.1579, -0.1452, -0.1723, -0.2000, -0.1373, -0.1239,\n",
            "                  -0.1053, -0.1950, -0.1216, -0.2073, -0.1205, -0.1026, -0.1239, -0.1055,\n",
            "                  -0.1074, -0.1958, -0.1614, -0.1605, -0.0994, -0.1320, -0.1234, -0.1353,\n",
            "                  -0.1603, -0.1290, -0.1479, -0.1452, -0.1588, -0.1000, -0.1190, -0.1738,\n",
            "                  -0.0911, -0.1424, -0.1706, -0.1370, -0.1177, -0.1706, -0.1851, -0.1343,\n",
            "                  -0.2134, -0.1634, -0.1352, -0.1529, -0.1772, -0.1276, -0.1721, -0.1304,\n",
            "                  -0.2106, -0.1507, -0.0881, -0.0894, -0.1171, -0.1394, -0.1319, -0.1841,\n",
            "                  -0.0881, -0.1534, -0.1222, -0.1985, -0.1403, -0.1159, -0.1281, -0.1392,\n",
            "                  -0.1320, -0.1452, -0.1339, -0.0936, -0.1442, -0.1170, -0.1566, -0.1492,\n",
            "                  -0.1796, -0.0931, -0.1464, -0.1624, -0.1623, -0.1205, -0.1487, -0.1482,\n",
            "                  -0.1494, -0.1552, -0.1452, -0.1543], device='cuda:0'), max_val=tensor([0.1359, 0.1439, 0.1660, 0.1385, 0.1303, 0.1728, 0.1284, 0.1519, 0.1186,\n",
            "                  0.1528, 0.1434, 0.1648, 0.1617, 0.0798, 0.1248, 0.1466, 0.1247, 0.1790,\n",
            "                  0.1759, 0.1399, 0.1149, 0.1356, 0.1636, 0.1369, 0.1962, 0.1113, 0.1526,\n",
            "                  0.2090, 0.1970, 0.1294, 0.1255, 0.1484, 0.1161, 0.1594, 0.1678, 0.1461,\n",
            "                  0.1426, 0.1777, 0.1644, 0.1428, 0.1406, 0.1658, 0.1571, 0.1553, 0.1665,\n",
            "                  0.1610, 0.1474, 0.1408, 0.1456, 0.1674, 0.0881, 0.0907, 0.1047, 0.1057,\n",
            "                  0.1634, 0.1493, 0.0866, 0.1648, 0.1430, 0.2369, 0.1474, 0.1383, 0.1335,\n",
            "                  0.1752, 0.1930, 0.1450, 0.1603, 0.0929, 0.1770, 0.1179, 0.1563, 0.1454,\n",
            "                  0.2341, 0.0888, 0.1582, 0.1858, 0.1741, 0.1572, 0.1833, 0.1555, 0.1216,\n",
            "                  0.1685, 0.1509, 0.2015], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0403, 0.0491, 0.0593, 0.0375, 0.0471, 0.0360, 0.0459, 0.0450, 0.0515,\n",
            "              0.0463], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.2619, -0.3683, -0.2507, -0.2542, -0.2832, -0.2363, -0.3440, -0.3286,\n",
            "                -0.3862, -0.3009], device='cuda:0'), max_val=tensor([0.3020, 0.3610, 0.4447, 0.2809, 0.3536, 0.2700, 0.3273, 0.3374, 0.3314,\n",
            "                0.3473], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1174/819746684.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  orig_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}.ckpt\"))\n",
            "/tmp/ipykernel_1174/819746684.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  quant_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}_quantized.ckpt\"), strict=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0643, 0.0567, 0.0619, 0.0634, 0.0559, 0.0742], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.4361, -0.4252, -0.4645, -0.4022, -0.3557, -0.5566], device='cuda:0'), max_val=tensor([0.4819, 0.2786, 0.3365, 0.4756, 0.4195, 0.5182], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0238, 0.0479, 0.0238, 0.0349, 0.0385, 0.0268, 0.0283, 0.0306, 0.0376,\n",
            "                0.0348, 0.0229, 0.0544, 0.0264, 0.0257, 0.0309, 0.0318],\n",
            "               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1785, -0.3592, -0.1783, -0.1313, -0.2890, -0.1874, -0.2124, -0.2256,\n",
            "                  -0.2237, -0.2607, -0.1719, -0.4079, -0.1961, -0.1926, -0.2320, -0.1407],\n",
            "                 device='cuda:0'), max_val=tensor([0.1650, 0.1684, 0.1558, 0.2621, 0.2807, 0.2011, 0.1596, 0.2296, 0.2819,\n",
            "                  0.2062, 0.1705, 0.1885, 0.1983, 0.1412, 0.1458, 0.2383],\n",
            "                 device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0147, 0.0124, 0.0110, 0.0137, 0.0122, 0.0167, 0.0151, 0.0134, 0.0187,\n",
            "                0.0119, 0.0129, 0.0139, 0.0134, 0.0156, 0.0125, 0.0164, 0.0167, 0.0117,\n",
            "                0.0145, 0.0177, 0.0111, 0.0122, 0.0158, 0.0132, 0.0168, 0.0125, 0.0131,\n",
            "                0.0115, 0.0166, 0.0156, 0.0178, 0.0140, 0.0135, 0.0094, 0.0214, 0.0159,\n",
            "                0.0175, 0.0164, 0.0144, 0.0133, 0.0122, 0.0109, 0.0155, 0.0168, 0.0170,\n",
            "                0.0110, 0.0162, 0.0117, 0.0177, 0.0117, 0.0186, 0.0182, 0.0144, 0.0102,\n",
            "                0.0111, 0.0114, 0.0139, 0.0129, 0.0130, 0.0110, 0.0113, 0.0101, 0.0106,\n",
            "                0.0083, 0.0180, 0.0137, 0.0090, 0.0107, 0.0216, 0.0155, 0.0136, 0.0145,\n",
            "                0.0120, 0.0130, 0.0119, 0.0128, 0.0096, 0.0100, 0.0189, 0.0113, 0.0121,\n",
            "                0.0099, 0.0154, 0.0104, 0.0104, 0.0093, 0.0118, 0.0140, 0.0215, 0.0113,\n",
            "                0.0139, 0.0124, 0.0134, 0.0180, 0.0116, 0.0162, 0.0114, 0.0154, 0.0203,\n",
            "                0.0111, 0.0127, 0.0141, 0.0137, 0.0157, 0.0099, 0.0135, 0.0184, 0.0157,\n",
            "                0.0131, 0.0130, 0.0119, 0.0092, 0.0171, 0.0141, 0.0139, 0.0174, 0.0129,\n",
            "                0.0134, 0.0120, 0.0145], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "               device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.1105, -0.0743, -0.0822, -0.1026, -0.0710, -0.1080, -0.0696, -0.0972,\n",
            "                  -0.0962, -0.0844, -0.0967, -0.1040, -0.1004, -0.1173, -0.0790, -0.1214,\n",
            "                  -0.1249, -0.0858, -0.1079, -0.1330, -0.0831, -0.0918, -0.1186, -0.0989,\n",
            "                  -0.0879, -0.0941, -0.0795, -0.0827, -0.1248, -0.0873, -0.1332, -0.1052,\n",
            "                  -0.1012, -0.0682, -0.1609, -0.1193, -0.1314, -0.0878, -0.1080, -0.0917,\n",
            "                  -0.0835, -0.0817, -0.0987, -0.1262, -0.1271, -0.0823, -0.1217, -0.0879,\n",
            "                  -0.0870, -0.0877, -0.1397, -0.1365, -0.1071, -0.0763, -0.0713, -0.0851,\n",
            "                  -0.1042, -0.0964, -0.0976, -0.0711, -0.0801, -0.0719, -0.0786, -0.0549,\n",
            "                  -0.1135, -0.0713, -0.0677, -0.0760, -0.1619, -0.1093, -0.1017, -0.1089,\n",
            "                  -0.0662, -0.0977, -0.0894, -0.0857, -0.0718, -0.0750, -0.1419, -0.0743,\n",
            "                  -0.0794, -0.0739, -0.1152, -0.0713, -0.0690, -0.0694, -0.0886, -0.1049,\n",
            "                  -0.1609, -0.0732, -0.0851, -0.0790, -0.0981, -0.1350, -0.0792, -0.1215,\n",
            "                  -0.0726, -0.1152, -0.1523, -0.0832, -0.0820, -0.1054, -0.0716, -0.1176,\n",
            "                  -0.0743, -0.0791, -0.1382, -0.1177, -0.0980, -0.0957, -0.0894, -0.0692,\n",
            "                  -0.1283, -0.1061, -0.0915, -0.1303, -0.0964, -0.0852, -0.0893, -0.1087],\n",
            "                 device='cuda:0'), max_val=tensor([0.1025, 0.0933, 0.0822, 0.0856, 0.0914, 0.1251, 0.1129, 0.1005, 0.1403,\n",
            "                  0.0893, 0.0842, 0.1043, 0.0773, 0.1058, 0.0940, 0.1230, 0.1248, 0.0878,\n",
            "                  0.1087, 0.1049, 0.0790, 0.0854, 0.1048, 0.0983, 0.1263, 0.0830, 0.0985,\n",
            "                  0.0864, 0.1118, 0.1173, 0.1011, 0.1013, 0.0899, 0.0708, 0.1262, 0.0956,\n",
            "                  0.1257, 0.1230, 0.0838, 0.0994, 0.0918, 0.0767, 0.1165, 0.0796, 0.1066,\n",
            "                  0.0753, 0.1090, 0.0743, 0.1330, 0.0851, 0.0946, 0.0878, 0.1077, 0.0765,\n",
            "                  0.0832, 0.0857, 0.0788, 0.0909, 0.0777, 0.0822, 0.0844, 0.0755, 0.0795,\n",
            "                  0.0625, 0.1350, 0.1028, 0.0662, 0.0805, 0.0933, 0.1162, 0.0843, 0.0913,\n",
            "                  0.0898, 0.0813, 0.0742, 0.0959, 0.0719, 0.0639, 0.1168, 0.0846, 0.0910,\n",
            "                  0.0742, 0.0930, 0.0781, 0.0776, 0.0643, 0.0755, 0.0983, 0.1116, 0.0847,\n",
            "                  0.1041, 0.0929, 0.1006, 0.1211, 0.0871, 0.0874, 0.0855, 0.1036, 0.1351,\n",
            "                  0.0768, 0.0951, 0.0799, 0.1024, 0.1052, 0.0660, 0.1015, 0.1206, 0.1176,\n",
            "                  0.0981, 0.0973, 0.0884, 0.0690, 0.1101, 0.0889, 0.1041, 0.0970, 0.0928,\n",
            "                  0.1009, 0.0904, 0.0880], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0332, 0.0149, 0.0201, 0.0225, 0.0264, 0.0201, 0.0206, 0.0295, 0.0217,\n",
            "                0.0243, 0.0229, 0.0290, 0.0159, 0.0281, 0.0209, 0.0272, 0.0272, 0.0203,\n",
            "                0.0221, 0.0241, 0.0219, 0.0189, 0.0207, 0.0250, 0.0197, 0.0278, 0.0206,\n",
            "                0.0210, 0.0191, 0.0199, 0.0210, 0.0244, 0.0233, 0.0219, 0.0198, 0.0234,\n",
            "                0.0235, 0.0201, 0.0183, 0.0141, 0.0235, 0.0190, 0.0221, 0.0182, 0.0233,\n",
            "                0.0266, 0.0206, 0.0258, 0.0269, 0.0184, 0.0195, 0.0212, 0.0213, 0.0184,\n",
            "                0.0221, 0.0183, 0.0160, 0.0210, 0.0245, 0.0163, 0.0214, 0.0221, 0.0180,\n",
            "                0.0199, 0.0198, 0.0190, 0.0198, 0.0162, 0.0183, 0.0211, 0.0177, 0.0191,\n",
            "                0.0225, 0.0191, 0.0231, 0.0220, 0.0227, 0.0227, 0.0260, 0.0212, 0.0256,\n",
            "                0.0211, 0.0202, 0.0209], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.2487, -0.0993, -0.1511, -0.1685, -0.1928, -0.1509, -0.1546, -0.2061,\n",
            "                  -0.1567, -0.1824, -0.1716, -0.1493, -0.1193, -0.2107, -0.1217, -0.1926,\n",
            "                  -0.2038, -0.1525, -0.1559, -0.1298, -0.1260, -0.1418, -0.1243, -0.1625,\n",
            "                  -0.1247, -0.1726, -0.1544, -0.1158, -0.1415, -0.1494, -0.1573, -0.1832,\n",
            "                  -0.1746, -0.1335, -0.1352, -0.1751, -0.1760, -0.1255, -0.1375, -0.0965,\n",
            "                  -0.1760, -0.1083, -0.1659, -0.1366, -0.1595, -0.1993, -0.1333, -0.1933,\n",
            "                  -0.2019, -0.1378, -0.1347, -0.1590, -0.1597, -0.1375, -0.1658, -0.1344,\n",
            "                  -0.1161, -0.1578, -0.1841, -0.1224, -0.1448, -0.1658, -0.1349, -0.1491,\n",
            "                  -0.1484, -0.1424, -0.1482, -0.1160, -0.1218, -0.1586, -0.1017, -0.1431,\n",
            "                  -0.1684, -0.1177, -0.1652, -0.1380, -0.1699, -0.1699, -0.1574, -0.1581,\n",
            "                  -0.1923, -0.1585, -0.1330, -0.1569], device='cuda:0'), max_val=tensor([0.1573, 0.1115, 0.1479, 0.1372, 0.1981, 0.1395, 0.1379, 0.2212, 0.1629,\n",
            "                  0.1524, 0.1533, 0.2178, 0.1194, 0.0967, 0.1566, 0.2042, 0.1286, 0.1520,\n",
            "                  0.1655, 0.1804, 0.1640, 0.1143, 0.1551, 0.1874, 0.1480, 0.2085, 0.1548,\n",
            "                  0.1571, 0.1431, 0.1316, 0.1507, 0.1421, 0.1481, 0.1645, 0.1484, 0.1681,\n",
            "                  0.1310, 0.1511, 0.1360, 0.1055, 0.1184, 0.1426, 0.1446, 0.1323, 0.1745,\n",
            "                  0.1641, 0.1543, 0.1383, 0.1764, 0.1381, 0.1464, 0.1251, 0.1432, 0.1382,\n",
            "                  0.1376, 0.1374, 0.1199, 0.1302, 0.1371, 0.1207, 0.1608, 0.1591, 0.1158,\n",
            "                  0.1196, 0.1249, 0.1422, 0.1461, 0.1218, 0.1371, 0.1394, 0.1326, 0.1310,\n",
            "                  0.1683, 0.1432, 0.1734, 0.1653, 0.1270, 0.1680, 0.1953, 0.1593, 0.1700,\n",
            "                  0.1293, 0.1512, 0.1312], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0598, 0.0540, 0.0568, 0.0390, 0.0463, 0.0488, 0.0435, 0.0640, 0.0558,\n",
            "              0.0500], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.4486, -0.3670, -0.2943, -0.2551, -0.3470, -0.3656, -0.3259, -0.4801,\n",
            "                -0.3797, -0.3250], device='cuda:0'), max_val=tensor([0.3378, 0.4050, 0.4257, 0.2925, 0.3381, 0.3166, 0.3180, 0.4089, 0.4184,\n",
            "                0.3748], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "LeNet5(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(\n",
            "      3, 6, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0458, 0.1047, 0.0456, 0.0705, 0.0992, 0.0493], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2614, -0.7853, -0.3418, -0.3896, -0.4943, -0.3698], device='cuda:0'), max_val=tensor([0.3435, 0.4272, 0.3417, 0.5288, 0.7444, 0.3535], device='cuda:0'))\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(\n",
            "      6, 16, kernel_size=(5, 5), stride=(1, 1)\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0421, 0.0330, 0.0275, 0.0314, 0.0360, 0.0350, 0.0333, 0.0396, 0.0189,\n",
            "                0.0365, 0.0443, 0.0305, 0.0388, 0.0263, 0.0436, 0.0494],\n",
            "               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.3159, -0.2108, -0.2064, -0.1831, -0.2002, -0.2378, -0.1647, -0.2788,\n",
            "                  -0.1420, -0.2738, -0.1564, -0.2278, -0.2306, -0.1975, -0.1971, -0.3706],\n",
            "                 device='cuda:0'), max_val=tensor([0.2412, 0.2474, 0.1773, 0.2359, 0.2701, 0.2625, 0.2499, 0.2971, 0.1386,\n",
            "                  0.2437, 0.3322, 0.2288, 0.2912, 0.1962, 0.3268, 0.2726],\n",
            "                 device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=400, out_features=120, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0132, 0.0171, 0.0164, 0.0186, 0.0130, 0.0144, 0.0157, 0.0148, 0.0065,\n",
            "                0.0171, 0.0135, 0.0233, 0.0148, 0.0188, 0.0204, 0.0167, 0.0185, 0.0157,\n",
            "                0.0128, 0.0144, 0.0146, 0.0192, 0.0140, 0.0147, 0.0175, 0.0066, 0.0155,\n",
            "                0.0249, 0.0161, 0.0185, 0.0091, 0.0091, 0.0120, 0.0161, 0.0082, 0.0146,\n",
            "                0.0164, 0.0086, 0.0227, 0.0123, 0.0209, 0.0147, 0.0212, 0.0195, 0.0156,\n",
            "                0.0131, 0.0169, 0.0167, 0.0125, 0.0130, 0.0178, 0.0149, 0.0095, 0.0154,\n",
            "                0.0154, 0.0162, 0.0175, 0.0206, 0.0165, 0.0101, 0.0124, 0.0131, 0.0066,\n",
            "                0.0143, 0.0140, 0.0132, 0.0066, 0.0164, 0.0095, 0.0135, 0.0152, 0.0185,\n",
            "                0.0199, 0.0065, 0.0145, 0.0167, 0.0158, 0.0138, 0.0162, 0.0068, 0.0139,\n",
            "                0.0065, 0.0192, 0.0146, 0.0173, 0.0170, 0.0240, 0.0165, 0.0159, 0.0158,\n",
            "                0.0131, 0.0167, 0.0193, 0.0159, 0.0161, 0.0189, 0.0253, 0.0155, 0.0152,\n",
            "                0.0123, 0.0077, 0.0116, 0.0171, 0.0156, 0.0128, 0.0134, 0.0150, 0.0140,\n",
            "                0.0247, 0.0180, 0.0229, 0.0151, 0.0152, 0.0126, 0.0155, 0.0247, 0.0138,\n",
            "                0.0149, 0.0191, 0.0071], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "               device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.0862, -0.1281, -0.1221, -0.1243, -0.0941, -0.1080, -0.0925, -0.1102,\n",
            "                  -0.0479, -0.1282, -0.0967, -0.1746, -0.0909, -0.1409, -0.0797, -0.1250,\n",
            "                  -0.1367, -0.1105, -0.0959, -0.1009, -0.1022, -0.1290, -0.0837, -0.1106,\n",
            "                  -0.1312, -0.0495, -0.1163, -0.1865, -0.1208, -0.1388, -0.0683, -0.0682,\n",
            "                  -0.0897, -0.1211, -0.0587, -0.1093, -0.1230, -0.0647, -0.1242, -0.0806,\n",
            "                  -0.1247, -0.1103, -0.1588, -0.1125, -0.0933, -0.0763, -0.1270, -0.0947,\n",
            "                  -0.0915, -0.0929, -0.1068, -0.0965, -0.0714, -0.0844, -0.1153, -0.1216,\n",
            "                  -0.1309, -0.1544, -0.0844, -0.0756, -0.0926, -0.0979, -0.0497, -0.0887,\n",
            "                  -0.1047, -0.0860, -0.0494, -0.1227, -0.0716, -0.0965, -0.1128, -0.1333,\n",
            "                  -0.1016, -0.0488, -0.0900, -0.1250, -0.1185, -0.1034, -0.1212, -0.0511,\n",
            "                  -0.1037, -0.0491, -0.1438, -0.1001, -0.1020, -0.1273, -0.1171, -0.1238,\n",
            "                  -0.1132, -0.1034, -0.0965, -0.1173, -0.1449, -0.1170, -0.1126, -0.1104,\n",
            "                  -0.1896, -0.1151, -0.0873, -0.0402, -0.0572, -0.0873, -0.1281, -0.0961,\n",
            "                  -0.0947, -0.1004, -0.0996, -0.1053, -0.0846, -0.1347, -0.0869, -0.1133,\n",
            "                  -0.0890, -0.0921, -0.1161, -0.1126, -0.0841, -0.0894, -0.1430, -0.0532],\n",
            "                 device='cuda:0'), max_val=tensor([0.0987, 0.1237, 0.1229, 0.1394, 0.0978, 0.1038, 0.1181, 0.1109, 0.0485,\n",
            "                  0.1121, 0.1014, 0.0963, 0.1109, 0.1294, 0.1531, 0.1059, 0.1386, 0.1178,\n",
            "                  0.0875, 0.1077, 0.1098, 0.1437, 0.1048, 0.0934, 0.1274, 0.0478, 0.1041,\n",
            "                  0.1449, 0.0893, 0.1060, 0.0678, 0.0683, 0.0662, 0.1053, 0.0615, 0.0934,\n",
            "                  0.1226, 0.0640, 0.1702, 0.0926, 0.1568, 0.1106, 0.1106, 0.1465, 0.1171,\n",
            "                  0.0984, 0.0996, 0.1250, 0.0939, 0.0975, 0.1334, 0.1115, 0.0624, 0.1156,\n",
            "                  0.0998, 0.1001, 0.0944, 0.1053, 0.1239, 0.0751, 0.0829, 0.0872, 0.0493,\n",
            "                  0.1073, 0.0866, 0.0994, 0.0478, 0.0891, 0.0638, 0.1011, 0.1142, 0.1386,\n",
            "                  0.1490, 0.0484, 0.1087, 0.0997, 0.0900, 0.0856, 0.0991, 0.0494, 0.1045,\n",
            "                  0.0483, 0.1162, 0.1094, 0.1298, 0.1033, 0.1803, 0.1132, 0.1192, 0.1186,\n",
            "                  0.0981, 0.1250, 0.1260, 0.1189, 0.1210, 0.1419, 0.1222, 0.1166, 0.1139,\n",
            "                  0.0921, 0.0576, 0.0873, 0.1135, 0.1173, 0.0958, 0.0881, 0.1123, 0.0883,\n",
            "                  0.1853, 0.1353, 0.1720, 0.0962, 0.1139, 0.0943, 0.0958, 0.1855, 0.1032,\n",
            "                  0.1119, 0.1144, 0.0476], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(\n",
            "      in_features=120, out_features=84, bias=True\n",
            "      (weight_fake_quant): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0127, 0.0201, 0.0196, 0.0207, 0.0125, 0.0270, 0.0270, 0.0116, 0.0283,\n",
            "                0.0241, 0.0306, 0.0213, 0.0214, 0.0215, 0.0254, 0.0127, 0.0246, 0.0176,\n",
            "                0.0184, 0.0209, 0.0206, 0.0273, 0.0224, 0.0315, 0.0320, 0.0244, 0.0118,\n",
            "                0.0118, 0.0272, 0.0203, 0.0254, 0.0257, 0.0207, 0.0123, 0.0188, 0.0321,\n",
            "                0.0239, 0.0258, 0.0209, 0.0227, 0.0253, 0.0165, 0.0203, 0.0312, 0.0115,\n",
            "                0.0218, 0.0181, 0.0198, 0.0249, 0.0117, 0.0215, 0.0199, 0.0317, 0.0182,\n",
            "                0.0315, 0.0241, 0.0261, 0.0277, 0.0269, 0.0189, 0.0192, 0.0187, 0.0160,\n",
            "                0.0211, 0.0237, 0.0230, 0.0359, 0.0203, 0.0118, 0.0196, 0.0199, 0.0277,\n",
            "                0.0246, 0.0173, 0.0205, 0.0248, 0.0226, 0.0242, 0.0176, 0.0117, 0.0297,\n",
            "                0.0224, 0.0192, 0.0116], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "               dtype=torch.int32)\n",
            "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "          min_val=tensor([-0.0928, -0.1509, -0.1327, -0.1380, -0.0920, -0.1495, -0.1503, -0.0869,\n",
            "                  -0.1532, -0.1259, -0.1609, -0.1397, -0.1602, -0.1294, -0.1906, -0.0932,\n",
            "                  -0.1121, -0.1322, -0.1379, -0.1564, -0.1340, -0.2050, -0.1612, -0.1543,\n",
            "                  -0.1469, -0.1832, -0.0884, -0.0882, -0.1312, -0.1520, -0.1902, -0.1929,\n",
            "                  -0.1159, -0.0925, -0.1411, -0.1979, -0.1796, -0.1938, -0.1571, -0.1518,\n",
            "                  -0.1652, -0.1167, -0.1522, -0.1749, -0.0862, -0.1635, -0.1274, -0.1489,\n",
            "                  -0.1860, -0.0858, -0.1338, -0.1418, -0.2378, -0.1216, -0.1213, -0.1482,\n",
            "                  -0.1614, -0.1515, -0.2021, -0.1417, -0.1202, -0.0995, -0.1160, -0.1583,\n",
            "                  -0.1773, -0.1574, -0.1765, -0.1323, -0.0882, -0.1470, -0.1348, -0.2076,\n",
            "                  -0.1846, -0.1220, -0.1535, -0.1859, -0.1698, -0.1818, -0.1094, -0.0873,\n",
            "                  -0.1766, -0.1683, -0.1402, -0.0873], device='cuda:0'), max_val=tensor([0.0955, 0.1174, 0.1467, 0.1550, 0.0935, 0.2024, 0.2026, 0.0843, 0.2124,\n",
            "                  0.1807, 0.2294, 0.1597, 0.1191, 0.1611, 0.1739, 0.0954, 0.1843, 0.1215,\n",
            "                  0.1259, 0.1566, 0.1548, 0.1874, 0.1679, 0.2359, 0.2401, 0.1678, 0.0860,\n",
            "                  0.0876, 0.2037, 0.1420, 0.1693, 0.1661, 0.1550, 0.0818, 0.1328, 0.2411,\n",
            "                  0.1140, 0.1575, 0.1324, 0.1702, 0.1901, 0.1236, 0.1378, 0.2341, 0.0834,\n",
            "                  0.1639, 0.1361, 0.1308, 0.1870, 0.0874, 0.1613, 0.1495, 0.2053, 0.1364,\n",
            "                  0.2363, 0.1806, 0.1959, 0.2080, 0.1633, 0.1259, 0.1442, 0.1404, 0.1203,\n",
            "                  0.1531, 0.1779, 0.1728, 0.2694, 0.1524, 0.0884, 0.1469, 0.1492, 0.1544,\n",
            "                  0.1721, 0.1300, 0.1531, 0.1696, 0.1672, 0.1602, 0.1319, 0.0876, 0.2229,\n",
            "                  0.1358, 0.1441, 0.0870], device='cuda:0')\n",
            "        )\n",
            "      )\n",
            "      (activation_post_process): NoopObserver()\n",
            "    )\n",
            "    (1): ReLU6(\n",
            "      inplace=True\n",
            "      (activation_post_process): FakeQuantize(\n",
            "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
            "        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(\n",
            "    in_features=84, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0440, 0.0507, 0.0429, 0.0338, 0.0401, 0.0411, 0.0507, 0.0472, 0.0516,\n",
            "              0.0531], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.3298, -0.3152, -0.2296, -0.1821, -0.3007, -0.2579, -0.2816, -0.2879,\n",
            "                -0.3867, -0.2342], device='cuda:0'), max_val=tensor([0.2808, 0.3800, 0.3218, 0.2534, 0.2974, 0.3079, 0.3801, 0.3537, 0.3355,\n",
            "                0.3979], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "ResNet(\n",
            "  (quant): QuantStub(\n",
            "    (activation_post_process): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            "  (conv): Conv2d(\n",
            "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0447, 0.0591, 0.0670, 0.0496, 0.0486, 0.0402, 0.0533, 0.0364, 0.0359,\n",
            "              0.0388, 0.0672, 0.0275, 0.0638, 0.0689, 0.0523, 0.0712],\n",
            "             device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "             dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.3352, -0.4435, -0.5027, -0.3326, -0.2866, -0.3006, -0.2863, -0.2730,\n",
            "                -0.2694, -0.2120, -0.5040, -0.1931, -0.4786, -0.3322, -0.3919, -0.3639],\n",
            "               device='cuda:0'), max_val=tensor([0.2882, 0.2248, 0.4188, 0.3724, 0.3647, 0.3015, 0.3997, 0.2692, 0.2336,\n",
            "                0.2910, 0.3670, 0.2059, 0.3323, 0.5171, 0.3322, 0.5343],\n",
            "               device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            "  (bn): BatchNorm2d(\n",
            "    16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            "  (act_fn): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0212, 0.0238, 0.0266, 0.0270, 0.0275, 0.0179, 0.0326, 0.0252, 0.0225,\n",
            "                  0.0584, 0.0249, 0.0371, 0.0252, 0.0226, 0.0225, 0.0275],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1591, -0.1785, -0.1997, -0.2022, -0.2061, -0.1295, -0.2444, -0.1892,\n",
            "                    -0.1686, -0.4378, -0.1866, -0.2553, -0.1892, -0.1698, -0.1691, -0.1553],\n",
            "                   device='cuda:0'), max_val=tensor([0.1404, 0.1654, 0.1484, 0.1948, 0.1642, 0.1342, 0.1756, 0.1856, 0.1405,\n",
            "                    0.3023, 0.1867, 0.2779, 0.1654, 0.1334, 0.1633, 0.2062],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0231, 0.0377, 0.0249, 0.0280, 0.0173, 0.0213, 0.0256, 0.0269, 0.0240,\n",
            "                  0.0295, 0.0201, 0.0308, 0.0295, 0.0244, 0.0200, 0.0292],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1733, -0.2829, -0.1646, -0.1565, -0.1136, -0.1595, -0.1922, -0.1863,\n",
            "                    -0.1697, -0.1124, -0.1298, -0.2310, -0.2214, -0.1826, -0.1411, -0.2192],\n",
            "                   device='cuda:0'), max_val=tensor([0.1735, 0.1764, 0.1865, 0.2097, 0.1296, 0.1251, 0.1700, 0.2020, 0.1799,\n",
            "                    0.2211, 0.1506, 0.1662, 0.1639, 0.1678, 0.1500, 0.1673],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0253, 0.0314, 0.0323, 0.0232, 0.0205, 0.0326, 0.0315, 0.0277, 0.0296,\n",
            "                  0.0204, 0.0236, 0.0198, 0.0197, 0.0266, 0.0186, 0.0245],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1901, -0.2204, -0.2157, -0.1741, -0.1536, -0.2446, -0.2049, -0.2075,\n",
            "                    -0.2219, -0.1532, -0.1769, -0.1472, -0.1475, -0.1988, -0.1295, -0.1820],\n",
            "                   device='cuda:0'), max_val=tensor([0.1798, 0.2358, 0.2425, 0.1529, 0.1381, 0.1474, 0.2360, 0.1985, 0.1475,\n",
            "                    0.1439, 0.1560, 0.1487, 0.1277, 0.1991, 0.1394, 0.1840],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0268, 0.0208, 0.0224, 0.0263, 0.0201, 0.0203, 0.0228, 0.0221, 0.0209,\n",
            "                  0.0184, 0.0200, 0.0266, 0.0225, 0.0211, 0.0255, 0.0181],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.2009, -0.1560, -0.1679, -0.1972, -0.1506, -0.1526, -0.1711, -0.1660,\n",
            "                    -0.1517, -0.1320, -0.1309, -0.1993, -0.1689, -0.1584, -0.1915, -0.1218],\n",
            "                   device='cuda:0'), max_val=tensor([0.1415, 0.1264, 0.1584, 0.1648, 0.1119, 0.1376, 0.1167, 0.1306, 0.1569,\n",
            "                    0.1381, 0.1498, 0.0888, 0.1151, 0.1503, 0.1394, 0.1356],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0181, 0.0258, 0.0213, 0.0168, 0.0193, 0.0170, 0.0177, 0.0215, 0.0191,\n",
            "                  0.0216, 0.0185, 0.0175, 0.0187, 0.0235, 0.0237, 0.0170],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1341, -0.1937, -0.1599, -0.1176, -0.1367, -0.1260, -0.1325, -0.1612,\n",
            "                    -0.1430, -0.1478, -0.1389, -0.1307, -0.1192, -0.1765, -0.1736, -0.1211],\n",
            "                   device='cuda:0'), max_val=tensor([0.1357, 0.1676, 0.1597, 0.1260, 0.1446, 0.1275, 0.1328, 0.1194, 0.1432,\n",
            "                    0.1622, 0.1194, 0.1312, 0.1404, 0.1515, 0.1780, 0.1272],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0211, 0.0188, 0.0199, 0.0188, 0.0168, 0.0157, 0.0200, 0.0157, 0.0168,\n",
            "                  0.0221, 0.0156, 0.0193, 0.0182, 0.0207, 0.0222, 0.0191],\n",
            "                 device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1564, -0.1178, -0.1024, -0.1389, -0.1205, -0.1138, -0.1499, -0.1165,\n",
            "                    -0.1135, -0.1247, -0.1168, -0.1253, -0.1362, -0.1555, -0.1667, -0.1025],\n",
            "                   device='cuda:0'), max_val=tensor([0.1583, 0.1409, 0.1492, 0.1411, 0.1259, 0.1179, 0.1348, 0.1176, 0.1258,\n",
            "                    0.1654, 0.1174, 0.1444, 0.1181, 0.1384, 0.1221, 0.1429],\n",
            "                   device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0166, 0.0170, 0.0156, 0.0190, 0.0173, 0.0167, 0.0198, 0.0168, 0.0200,\n",
            "                  0.0159, 0.0192, 0.0181, 0.0181, 0.0171, 0.0173, 0.0174, 0.0174, 0.0187,\n",
            "                  0.0208, 0.0183, 0.0168, 0.0163, 0.0174, 0.0193, 0.0210, 0.0145, 0.0154,\n",
            "                  0.0162, 0.0213, 0.0159, 0.0198, 0.0163], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1247, -0.1211, -0.1171, -0.1305, -0.1299, -0.1250, -0.1217, -0.1256,\n",
            "                    -0.0777, -0.1192, -0.1438, -0.1274, -0.1155, -0.1079, -0.1295, -0.1304,\n",
            "                    -0.1306, -0.1039, -0.1560, -0.1370, -0.1260, -0.1224, -0.1263, -0.1130,\n",
            "                    -0.1576, -0.1085, -0.1128, -0.1216, -0.1601, -0.0988, -0.1486, -0.1222],\n",
            "                   device='cuda:0'), max_val=tensor([0.1174, 0.1272, 0.1099, 0.1428, 0.1108, 0.0921, 0.1484, 0.1254, 0.1496,\n",
            "                    0.1044, 0.1399, 0.1359, 0.1360, 0.1282, 0.1217, 0.0856, 0.1117, 0.1403,\n",
            "                    0.0901, 0.1098, 0.1082, 0.1077, 0.1301, 0.1451, 0.1418, 0.1024, 0.1157,\n",
            "                    0.1154, 0.0990, 0.1195, 0.1021, 0.1099], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0141, 0.0133, 0.0136, 0.0160, 0.0147, 0.0172, 0.0135, 0.0134, 0.0152,\n",
            "                  0.0134, 0.0144, 0.0175, 0.0126, 0.0152, 0.0143, 0.0163, 0.0184, 0.0159,\n",
            "                  0.0161, 0.0140, 0.0197, 0.0186, 0.0164, 0.0147, 0.0169, 0.0166, 0.0138,\n",
            "                  0.0144, 0.0160, 0.0136, 0.0145, 0.0153], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1055, -0.0996, -0.1023, -0.1034, -0.1103, -0.1291, -0.1014, -0.1009,\n",
            "                    -0.1136, -0.0986, -0.1076, -0.1222, -0.0830, -0.1058, -0.1074, -0.1223,\n",
            "                    -0.1378, -0.1120, -0.1038, -0.0939, -0.1141, -0.1398, -0.0967, -0.1099,\n",
            "                    -0.1269, -0.1225, -0.1011, -0.1044, -0.1149, -0.1022, -0.1087, -0.1149],\n",
            "                   device='cuda:0'), max_val=tensor([0.0981, 0.0883, 0.0827, 0.1200, 0.0957, 0.1060, 0.0946, 0.0884, 0.1140,\n",
            "                    0.1008, 0.1008, 0.1314, 0.0945, 0.1141, 0.1066, 0.1112, 0.1339, 0.1195,\n",
            "                    0.1206, 0.1047, 0.1479, 0.1015, 0.1232, 0.1101, 0.1271, 0.1248, 0.1034,\n",
            "                    0.1080, 0.1203, 0.1020, 0.1045, 0.0878], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(\n",
            "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0280, 0.0313, 0.0352, 0.0305, 0.0357, 0.0333, 0.0326, 0.0371, 0.0369,\n",
            "                    0.0266, 0.0340, 0.0290, 0.0301, 0.0346, 0.0313, 0.0331, 0.0317, 0.0308,\n",
            "                    0.0316, 0.0322, 0.0370, 0.0299, 0.0419, 0.0394, 0.0347, 0.0350, 0.0253,\n",
            "                    0.0364, 0.0366, 0.0378, 0.0356, 0.0334], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                    0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "              min_val=tensor([-0.1752, -0.1228, -0.2640, -0.2019, -0.2680, -0.2496, -0.2290, -0.2780,\n",
            "                      -0.2764, -0.1996, -0.2243, -0.1865, -0.2016, -0.2594, -0.2344, -0.2482,\n",
            "                      -0.2143, -0.2281, -0.2114, -0.2413, -0.2777, -0.2168, -0.3144, -0.2958,\n",
            "                      -0.2601, -0.2048, -0.0786, -0.2732, -0.2747, -0.2836, -0.2671, -0.2506],\n",
            "                     device='cuda:0'), max_val=tensor([0.2102, 0.2344, 0.2288, 0.2289, 0.2065, 0.2150, 0.2444, 0.1935, 0.2384,\n",
            "                      0.1692, 0.2550, 0.2176, 0.2255, 0.2399, 0.2350, 0.1476, 0.2376, 0.2312,\n",
            "                      0.2373, 0.1936, 0.2326, 0.2244, 0.1474, 0.2090, 0.1787, 0.2628, 0.1898,\n",
            "                      0.1530, 0.1990, 0.2366, 0.2310, 0.2157], device='cuda:0')\n",
            "            )\n",
            "          )\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "        (1): BatchNorm2d(\n",
            "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0146, 0.0148, 0.0150, 0.0143, 0.0160, 0.0165, 0.0123, 0.0155, 0.0197,\n",
            "                  0.0168, 0.0168, 0.0134, 0.0123, 0.0163, 0.0118, 0.0157, 0.0164, 0.0171,\n",
            "                  0.0177, 0.0166, 0.0183, 0.0149, 0.0153, 0.0159, 0.0167, 0.0135, 0.0142,\n",
            "                  0.0148, 0.0144, 0.0151, 0.0176, 0.0153], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0980, -0.1109, -0.1052, -0.1051, -0.1202, -0.1041, -0.0924, -0.1152,\n",
            "                    -0.1475, -0.1262, -0.1260, -0.0946, -0.0912, -0.1091, -0.0889, -0.0929,\n",
            "                    -0.1227, -0.1196, -0.1178, -0.1182, -0.1101, -0.1103, -0.1145, -0.0855,\n",
            "                    -0.1023, -0.0952, -0.1061, -0.1109, -0.1079, -0.1000, -0.1320, -0.1145],\n",
            "                   device='cuda:0'), max_val=tensor([0.1098, 0.0979, 0.1124, 0.1070, 0.1189, 0.1238, 0.0921, 0.1161, 0.1130,\n",
            "                    0.1071, 0.0912, 0.1002, 0.0924, 0.1223, 0.0885, 0.1177, 0.1103, 0.1281,\n",
            "                    0.1331, 0.1246, 0.1374, 0.1117, 0.0970, 0.1189, 0.1251, 0.1014, 0.1066,\n",
            "                    0.1088, 0.0957, 0.1132, 0.1173, 0.1007], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0161, 0.0132, 0.0124, 0.0146, 0.0126, 0.0140, 0.0144, 0.0129, 0.0136,\n",
            "                  0.0183, 0.0125, 0.0127, 0.0133, 0.0150, 0.0138, 0.0151, 0.0162, 0.0133,\n",
            "                  0.0147, 0.0153, 0.0145, 0.0134, 0.0156, 0.0164, 0.0151, 0.0157, 0.0169,\n",
            "                  0.0138, 0.0141, 0.0136, 0.0165, 0.0142], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0886, -0.0988, -0.0912, -0.1055, -0.0927, -0.1052, -0.1077, -0.0806,\n",
            "                    -0.1013, -0.1369, -0.0884, -0.0912, -0.0895, -0.1082, -0.1032, -0.0874,\n",
            "                    -0.1112, -0.0909, -0.1105, -0.1147, -0.1087, -0.0865, -0.1167, -0.1229,\n",
            "                    -0.1130, -0.1026, -0.1266, -0.0931, -0.0956, -0.0985, -0.1059, -0.1069],\n",
            "                   device='cuda:0'), max_val=tensor([0.1211, 0.0891, 0.0932, 0.1095, 0.0944, 0.1017, 0.1042, 0.0965, 0.1023,\n",
            "                    0.0975, 0.0937, 0.0951, 0.0999, 0.1128, 0.1037, 0.1131, 0.1217, 0.1001,\n",
            "                    0.1100, 0.1004, 0.0988, 0.1007, 0.0996, 0.1225, 0.1104, 0.1176, 0.0894,\n",
            "                    0.1032, 0.1059, 0.1021, 0.1235, 0.0913], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0185, 0.0182, 0.0143, 0.0140, 0.0132, 0.0151, 0.0146, 0.0177, 0.0151,\n",
            "                  0.0172, 0.0132, 0.0144, 0.0147, 0.0122, 0.0138, 0.0135, 0.0167, 0.0144,\n",
            "                  0.0146, 0.0163, 0.0128, 0.0148, 0.0162, 0.0115, 0.0147, 0.0163, 0.0167,\n",
            "                  0.0118, 0.0132, 0.0141, 0.0153, 0.0163], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1173, -0.1115, -0.1075, -0.0988, -0.0988, -0.1131, -0.0909, -0.1326,\n",
            "                    -0.1133, -0.1054, -0.0978, -0.0974, -0.1002, -0.0890, -0.0974, -0.1010,\n",
            "                    -0.1218, -0.1080, -0.1030, -0.0987, -0.0813, -0.0845, -0.1212, -0.0819,\n",
            "                    -0.1104, -0.1220, -0.1252, -0.0884, -0.0943, -0.0864, -0.1075, -0.1024],\n",
            "                   device='cuda:0'), max_val=tensor([0.1387, 0.1366, 0.0981, 0.1047, 0.0992, 0.1027, 0.1097, 0.1013, 0.1130,\n",
            "                    0.1293, 0.0993, 0.1081, 0.1102, 0.0915, 0.1036, 0.1009, 0.1254, 0.0937,\n",
            "                    0.1092, 0.1225, 0.0963, 0.1107, 0.1083, 0.0863, 0.0905, 0.1018, 0.1080,\n",
            "                    0.0736, 0.0988, 0.1054, 0.1144, 0.1226], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0148, 0.0178, 0.0135, 0.0137, 0.0127, 0.0154, 0.0137, 0.0118, 0.0151,\n",
            "                  0.0120, 0.0136, 0.0138, 0.0131, 0.0139, 0.0128, 0.0166, 0.0122, 0.0149,\n",
            "                  0.0144, 0.0145, 0.0132, 0.0138, 0.0147, 0.0147, 0.0151, 0.0123, 0.0129,\n",
            "                  0.0153, 0.0131, 0.0125, 0.0133, 0.0138], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.1110, -0.1335, -0.0951, -0.0947, -0.0953, -0.1142, -0.1000, -0.0834,\n",
            "                    -0.1134, -0.0876, -0.1016, -0.1032, -0.0859, -0.1046, -0.0813, -0.0760,\n",
            "                    -0.0888, -0.1116, -0.0967, -0.0915, -0.0991, -0.1037, -0.1096, -0.1013,\n",
            "                    -0.0940, -0.0909, -0.0876, -0.1029, -0.0983, -0.0744, -0.0904, -0.1004],\n",
            "                   device='cuda:0'), max_val=tensor([0.0908, 0.1025, 0.1015, 0.1030, 0.0852, 0.1153, 0.1025, 0.0885, 0.1021,\n",
            "                    0.0897, 0.0868, 0.0937, 0.0984, 0.0894, 0.0961, 0.1242, 0.0915, 0.1118,\n",
            "                    0.1079, 0.1084, 0.0986, 0.0863, 0.1104, 0.1100, 0.1129, 0.0925, 0.0966,\n",
            "                    0.1149, 0.0869, 0.0939, 0.1001, 0.1035], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0120, 0.0111, 0.0139, 0.0120, 0.0117, 0.0149, 0.0120, 0.0140, 0.0134,\n",
            "                  0.0112, 0.0126, 0.0124, 0.0115, 0.0119, 0.0134, 0.0116, 0.0131, 0.0117,\n",
            "                  0.0117, 0.0125, 0.0130, 0.0117, 0.0133, 0.0131, 0.0117, 0.0123, 0.0134,\n",
            "                  0.0127, 0.0123, 0.0127, 0.0128, 0.0135, 0.0107, 0.0126, 0.0126, 0.0113,\n",
            "                  0.0167, 0.0133, 0.0127, 0.0118, 0.0118, 0.0141, 0.0145, 0.0128, 0.0108,\n",
            "                  0.0131, 0.0113, 0.0118, 0.0144, 0.0119, 0.0115, 0.0109, 0.0138, 0.0121,\n",
            "                  0.0121, 0.0125, 0.0116, 0.0118, 0.0128, 0.0132, 0.0126, 0.0141, 0.0122,\n",
            "                  0.0140], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0853, -0.0833, -0.1042, -0.0901, -0.0847, -0.1010, -0.0858, -0.0949,\n",
            "                    -0.1006, -0.0797, -0.0947, -0.0927, -0.0837, -0.0891, -0.1008, -0.0853,\n",
            "                    -0.0836, -0.0877, -0.0875, -0.0936, -0.0913, -0.0846, -0.1000, -0.0980,\n",
            "                    -0.0876, -0.0842, -0.1005, -0.0953, -0.0736, -0.0949, -0.0963, -0.1016,\n",
            "                    -0.0714, -0.0942, -0.0739, -0.0850, -0.1254, -0.0905, -0.0952, -0.0821,\n",
            "                    -0.0813, -0.0851, -0.1050, -0.0963, -0.0805, -0.0904, -0.0784, -0.0883,\n",
            "                    -0.1078, -0.0861, -0.0807, -0.0821, -0.1036, -0.0909, -0.0882, -0.0771,\n",
            "                    -0.0859, -0.0817, -0.0807, -0.0933, -0.0927, -0.0832, -0.0912, -0.1047],\n",
            "                   device='cuda:0'), max_val=tensor([0.0899, 0.0835, 0.0904, 0.0816, 0.0878, 0.1119, 0.0903, 0.1049, 0.0861,\n",
            "                    0.0839, 0.0919, 0.0927, 0.0864, 0.0826, 0.0803, 0.0870, 0.0981, 0.0829,\n",
            "                    0.0764, 0.0939, 0.0974, 0.0875, 0.0923, 0.0789, 0.0878, 0.0924, 0.0939,\n",
            "                    0.0873, 0.0925, 0.0955, 0.0907, 0.0929, 0.0800, 0.0897, 0.0944, 0.0769,\n",
            "                    0.0858, 0.0997, 0.0937, 0.0888, 0.0886, 0.1058, 0.1088, 0.0951, 0.0813,\n",
            "                    0.0980, 0.0851, 0.0767, 0.0908, 0.0891, 0.0864, 0.0760, 0.1026, 0.0904,\n",
            "                    0.0911, 0.0939, 0.0868, 0.0883, 0.0959, 0.0993, 0.0946, 0.1056, 0.0855,\n",
            "                    0.0820], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0107, 0.0086, 0.0103, 0.0101, 0.0116, 0.0104, 0.0111, 0.0094, 0.0122,\n",
            "                  0.0102, 0.0103, 0.0107, 0.0126, 0.0098, 0.0125, 0.0116, 0.0107, 0.0096,\n",
            "                  0.0117, 0.0109, 0.0118, 0.0139, 0.0093, 0.0104, 0.0105, 0.0097, 0.0106,\n",
            "                  0.0103, 0.0124, 0.0106, 0.0094, 0.0110, 0.0106, 0.0100, 0.0095, 0.0107,\n",
            "                  0.0102, 0.0098, 0.0113, 0.0107, 0.0105, 0.0102, 0.0092, 0.0110, 0.0104,\n",
            "                  0.0090, 0.0107, 0.0105, 0.0102, 0.0104, 0.0106, 0.0135, 0.0096, 0.0116,\n",
            "                  0.0118, 0.0134, 0.0119, 0.0145, 0.0117, 0.0100, 0.0131, 0.0109, 0.0097,\n",
            "                  0.0103], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0788, -0.0612, -0.0770, -0.0757, -0.0868, -0.0778, -0.0694, -0.0666,\n",
            "                    -0.0919, -0.0737, -0.0761, -0.0767, -0.0942, -0.0666, -0.0937, -0.0852,\n",
            "                    -0.0800, -0.0721, -0.0874, -0.0817, -0.0799, -0.0834, -0.0699, -0.0779,\n",
            "                    -0.0789, -0.0689, -0.0766, -0.0730, -0.0927, -0.0795, -0.0666, -0.0812,\n",
            "                    -0.0741, -0.0712, -0.0695, -0.0804, -0.0743, -0.0629, -0.0845, -0.0698,\n",
            "                    -0.0788, -0.0748, -0.0678, -0.0828, -0.0777, -0.0570, -0.0801, -0.0667,\n",
            "                    -0.0710, -0.0680, -0.0785, -0.1009, -0.0720, -0.0868, -0.0769, -0.0728,\n",
            "                    -0.0702, -0.1088, -0.0764, -0.0695, -0.0980, -0.0740, -0.0718, -0.0772],\n",
            "                   device='cuda:0'), max_val=tensor([0.0803, 0.0642, 0.0743, 0.0727, 0.0739, 0.0690, 0.0832, 0.0704, 0.0908,\n",
            "                    0.0764, 0.0769, 0.0803, 0.0875, 0.0735, 0.0748, 0.0868, 0.0705, 0.0722,\n",
            "                    0.0777, 0.0711, 0.0882, 0.1041, 0.0646, 0.0745, 0.0653, 0.0724, 0.0798,\n",
            "                    0.0770, 0.0718, 0.0782, 0.0709, 0.0826, 0.0797, 0.0748, 0.0715, 0.0671,\n",
            "                    0.0764, 0.0734, 0.0779, 0.0805, 0.0757, 0.0767, 0.0692, 0.0754, 0.0727,\n",
            "                    0.0678, 0.0586, 0.0786, 0.0763, 0.0782, 0.0792, 0.0826, 0.0684, 0.0762,\n",
            "                    0.0883, 0.1002, 0.0894, 0.0885, 0.0875, 0.0751, 0.0868, 0.0817, 0.0730,\n",
            "                    0.0726], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(\n",
            "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (weight_fake_quant): FakeQuantize(\n",
            "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0267, 0.0252, 0.0246, 0.0255, 0.0225, 0.0224, 0.0284, 0.0263, 0.0235,\n",
            "                    0.0267, 0.0233, 0.0240, 0.0247, 0.0210, 0.0254, 0.0239, 0.0234, 0.0235,\n",
            "                    0.0220, 0.0248, 0.0238, 0.0257, 0.0232, 0.0235, 0.0278, 0.0230, 0.0225,\n",
            "                    0.0239, 0.0257, 0.0236, 0.0234, 0.0227, 0.0297, 0.0239, 0.0236, 0.0251,\n",
            "                    0.0255, 0.0258, 0.0284, 0.0254, 0.0254, 0.0246, 0.0224, 0.0236, 0.0259,\n",
            "                    0.0227, 0.0243, 0.0216, 0.0276, 0.0250, 0.0276, 0.0294, 0.0245, 0.0220,\n",
            "                    0.0257, 0.0259, 0.0240, 0.0246, 0.0252, 0.0239, 0.0261, 0.0239, 0.0239,\n",
            "                    0.0264], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                   dtype=torch.int32)\n",
            "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "              min_val=tensor([-0.2001, -0.1894, -0.1487, -0.1550, -0.1690, -0.1664, -0.1726, -0.1973,\n",
            "                      -0.1741, -0.1643, -0.1708, -0.1796, -0.1735, -0.1572, -0.1692, -0.1769,\n",
            "                      -0.1400, -0.1537, -0.1249, -0.1857, -0.1788, -0.1927, -0.1741, -0.1540,\n",
            "                      -0.1436, -0.1468, -0.1689, -0.1735, -0.1931, -0.1628, -0.1645, -0.1662,\n",
            "                      -0.1828, -0.1666, -0.1773, -0.1660, -0.1380, -0.1933, -0.2127, -0.1903,\n",
            "                      -0.1909, -0.1807, -0.1680, -0.1770, -0.1361, -0.1702, -0.1819, -0.1622,\n",
            "                      -0.1888, -0.1578, -0.1943, -0.2067, -0.1841, -0.1653, -0.1681, -0.1942,\n",
            "                      -0.1690, -0.1846, -0.1891, -0.1772, -0.1762, -0.1751, -0.1795, -0.1618],\n",
            "                     device='cuda:0'), max_val=tensor([0.1416, 0.1781, 0.1841, 0.1913, 0.1638, 0.1683, 0.2132, 0.1743, 0.1761,\n",
            "                      0.1999, 0.1745, 0.1614, 0.1853, 0.1244, 0.1907, 0.1792, 0.1753, 0.1759,\n",
            "                      0.1650, 0.1646, 0.1747, 0.1681, 0.1716, 0.1761, 0.2085, 0.1726, 0.1635,\n",
            "                      0.1791, 0.1718, 0.1769, 0.1757, 0.1705, 0.2230, 0.1790, 0.1771, 0.1882,\n",
            "                      0.1910, 0.1872, 0.1572, 0.1394, 0.1565, 0.1843, 0.1585, 0.1419, 0.1942,\n",
            "                      0.1633, 0.1714, 0.1596, 0.2068, 0.1876, 0.2067, 0.2202, 0.1463, 0.1400,\n",
            "                      0.1930, 0.1634, 0.1803, 0.1590, 0.1733, 0.1795, 0.1955, 0.1794, 0.1786,\n",
            "                      0.1984], device='cuda:0')\n",
            "            )\n",
            "          )\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "        (1): BatchNorm2d(\n",
            "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "          (activation_post_process): NoopObserver()\n",
            "        )\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0123, 0.0095, 0.0093, 0.0121, 0.0110, 0.0102, 0.0125, 0.0110, 0.0091,\n",
            "                  0.0109, 0.0115, 0.0112, 0.0101, 0.0128, 0.0129, 0.0093, 0.0112, 0.0107,\n",
            "                  0.0100, 0.0126, 0.0128, 0.0111, 0.0120, 0.0105, 0.0101, 0.0115, 0.0108,\n",
            "                  0.0122, 0.0113, 0.0106, 0.0117, 0.0110, 0.0120, 0.0103, 0.0128, 0.0098,\n",
            "                  0.0106, 0.0105, 0.0099, 0.0095, 0.0100, 0.0116, 0.0095, 0.0103, 0.0110,\n",
            "                  0.0089, 0.0112, 0.0097, 0.0111, 0.0106, 0.0094, 0.0119, 0.0131, 0.0110,\n",
            "                  0.0132, 0.0105, 0.0098, 0.0097, 0.0122, 0.0099, 0.0118, 0.0105, 0.0101,\n",
            "                  0.0120], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0925, -0.0653, -0.0697, -0.0910, -0.0823, -0.0759, -0.0925, -0.0825,\n",
            "                    -0.0650, -0.0820, -0.0779, -0.0730, -0.0761, -0.0964, -0.0970, -0.0699,\n",
            "                    -0.0837, -0.0795, -0.0747, -0.0733, -0.0961, -0.0834, -0.0901, -0.0784,\n",
            "                    -0.0741, -0.0863, -0.0808, -0.0915, -0.0845, -0.0795, -0.0866, -0.0829,\n",
            "                    -0.0756, -0.0773, -0.0959, -0.0733, -0.0774, -0.0703, -0.0670, -0.0657,\n",
            "                    -0.0744, -0.0781, -0.0693, -0.0773, -0.0821, -0.0610, -0.0672, -0.0670,\n",
            "                    -0.0809, -0.0737, -0.0703, -0.0876, -0.0985, -0.0697, -0.0990, -0.0785,\n",
            "                    -0.0735, -0.0636, -0.0916, -0.0746, -0.0883, -0.0784, -0.0711, -0.0902],\n",
            "                   device='cuda:0'), max_val=tensor([0.0709, 0.0715, 0.0656, 0.0838, 0.0726, 0.0762, 0.0935, 0.0687, 0.0680,\n",
            "                    0.0681, 0.0864, 0.0843, 0.0666, 0.0854, 0.0774, 0.0672, 0.0670, 0.0802,\n",
            "                    0.0744, 0.0946, 0.0871, 0.0832, 0.0797, 0.0710, 0.0759, 0.0808, 0.0796,\n",
            "                    0.0744, 0.0828, 0.0633, 0.0875, 0.0759, 0.0904, 0.0652, 0.0727, 0.0603,\n",
            "                    0.0794, 0.0786, 0.0740, 0.0715, 0.0748, 0.0870, 0.0711, 0.0761, 0.0819,\n",
            "                    0.0665, 0.0838, 0.0730, 0.0832, 0.0793, 0.0635, 0.0893, 0.0759, 0.0828,\n",
            "                    0.0764, 0.0730, 0.0663, 0.0728, 0.0908, 0.0670, 0.0843, 0.0693, 0.0760,\n",
            "                    0.0783], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0096, 0.0100, 0.0130, 0.0083, 0.0093, 0.0080, 0.0081, 0.0085, 0.0079,\n",
            "                  0.0098, 0.0100, 0.0084, 0.0090, 0.0087, 0.0083, 0.0084, 0.0095, 0.0090,\n",
            "                  0.0092, 0.0085, 0.0076, 0.0089, 0.0104, 0.0087, 0.0075, 0.0084, 0.0086,\n",
            "                  0.0076, 0.0103, 0.0096, 0.0083, 0.0090, 0.0092, 0.0084, 0.0089, 0.0078,\n",
            "                  0.0084, 0.0093, 0.0085, 0.0090, 0.0106, 0.0094, 0.0077, 0.0091, 0.0095,\n",
            "                  0.0085, 0.0090, 0.0091, 0.0117, 0.0092, 0.0084, 0.0106, 0.0087, 0.0085,\n",
            "                  0.0094, 0.0084, 0.0098, 0.0102, 0.0093, 0.0089, 0.0082, 0.0099, 0.0087,\n",
            "                  0.0079], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0716, -0.0645, -0.0971, -0.0579, -0.0677, -0.0569, -0.0571, -0.0638,\n",
            "                    -0.0595, -0.0732, -0.0680, -0.0629, -0.0676, -0.0646, -0.0621, -0.0591,\n",
            "                    -0.0640, -0.0569, -0.0589, -0.0628, -0.0560, -0.0564, -0.0652, -0.0652,\n",
            "                    -0.0553, -0.0629, -0.0645, -0.0558, -0.0771, -0.0716, -0.0543, -0.0662,\n",
            "                    -0.0566, -0.0612, -0.0600, -0.0583, -0.0630, -0.0649, -0.0595, -0.0676,\n",
            "                    -0.0747, -0.0702, -0.0533, -0.0643, -0.0640, -0.0606, -0.0527, -0.0682,\n",
            "                    -0.0876, -0.0693, -0.0609, -0.0736, -0.0554, -0.0568, -0.0624, -0.0501,\n",
            "                    -0.0666, -0.0756, -0.0571, -0.0668, -0.0582, -0.0745, -0.0650, -0.0590],\n",
            "                   device='cuda:0'), max_val=tensor([0.0722, 0.0747, 0.0655, 0.0619, 0.0701, 0.0602, 0.0609, 0.0628, 0.0567,\n",
            "                    0.0677, 0.0753, 0.0562, 0.0574, 0.0651, 0.0587, 0.0629, 0.0711, 0.0672,\n",
            "                    0.0689, 0.0634, 0.0570, 0.0665, 0.0776, 0.0607, 0.0560, 0.0568, 0.0618,\n",
            "                    0.0572, 0.0773, 0.0717, 0.0624, 0.0672, 0.0687, 0.0628, 0.0671, 0.0568,\n",
            "                    0.0592, 0.0698, 0.0640, 0.0607, 0.0794, 0.0706, 0.0577, 0.0681, 0.0716,\n",
            "                    0.0641, 0.0672, 0.0627, 0.0663, 0.0624, 0.0631, 0.0793, 0.0649, 0.0637,\n",
            "                    0.0702, 0.0632, 0.0731, 0.0766, 0.0695, 0.0671, 0.0618, 0.0736, 0.0646,\n",
            "                    0.0578], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0090, 0.0080, 0.0081, 0.0069, 0.0085, 0.0080, 0.0074, 0.0077, 0.0082,\n",
            "                  0.0086, 0.0102, 0.0093, 0.0077, 0.0095, 0.0074, 0.0080, 0.0090, 0.0105,\n",
            "                  0.0100, 0.0072, 0.0084, 0.0078, 0.0084, 0.0089, 0.0083, 0.0077, 0.0108,\n",
            "                  0.0084, 0.0082, 0.0078, 0.0094, 0.0086, 0.0091, 0.0088, 0.0071, 0.0075,\n",
            "                  0.0080, 0.0092, 0.0071, 0.0078, 0.0079, 0.0092, 0.0079, 0.0071, 0.0089,\n",
            "                  0.0088, 0.0075, 0.0083, 0.0068, 0.0082, 0.0067, 0.0083, 0.0077, 0.0072,\n",
            "                  0.0083, 0.0081, 0.0084, 0.0067, 0.0086, 0.0080, 0.0086, 0.0076, 0.0076,\n",
            "                  0.0094], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0641, -0.0465, -0.0607, -0.0516, -0.0640, -0.0572, -0.0552, -0.0549,\n",
            "                    -0.0543, -0.0575, -0.0670, -0.0625, -0.0575, -0.0709, -0.0550, -0.0602,\n",
            "                    -0.0679, -0.0758, -0.0641, -0.0540, -0.0615, -0.0583, -0.0624, -0.0666,\n",
            "                    -0.0584, -0.0578, -0.0811, -0.0631, -0.0613, -0.0561, -0.0703, -0.0644,\n",
            "                    -0.0606, -0.0609, -0.0471, -0.0562, -0.0601, -0.0612, -0.0533, -0.0587,\n",
            "                    -0.0591, -0.0691, -0.0538, -0.0517, -0.0670, -0.0658, -0.0514, -0.0598,\n",
            "                    -0.0513, -0.0613, -0.0501, -0.0625, -0.0514, -0.0485, -0.0578, -0.0609,\n",
            "                    -0.0578, -0.0503, -0.0585, -0.0572, -0.0642, -0.0563, -0.0570, -0.0708],\n",
            "                   device='cuda:0'), max_val=tensor([0.0674, 0.0604, 0.0476, 0.0489, 0.0541, 0.0603, 0.0537, 0.0575, 0.0615,\n",
            "                    0.0646, 0.0766, 0.0701, 0.0508, 0.0546, 0.0554, 0.0533, 0.0645, 0.0791,\n",
            "                    0.0753, 0.0509, 0.0629, 0.0558, 0.0627, 0.0616, 0.0620, 0.0540, 0.0665,\n",
            "                    0.0615, 0.0618, 0.0587, 0.0674, 0.0579, 0.0685, 0.0658, 0.0534, 0.0538,\n",
            "                    0.0558, 0.0690, 0.0458, 0.0527, 0.0563, 0.0572, 0.0595, 0.0531, 0.0668,\n",
            "                    0.0602, 0.0565, 0.0625, 0.0456, 0.0616, 0.0505, 0.0594, 0.0581, 0.0538,\n",
            "                    0.0621, 0.0546, 0.0633, 0.0499, 0.0645, 0.0599, 0.0637, 0.0571, 0.0543,\n",
            "                    0.0619], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn1): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(\n",
            "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (weight_fake_quant): FakeQuantize(\n",
            "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0082, 0.0083, 0.0077, 0.0075, 0.0078, 0.0072, 0.0073, 0.0082, 0.0090,\n",
            "                  0.0078, 0.0085, 0.0080, 0.0075, 0.0099, 0.0081, 0.0080, 0.0084, 0.0086,\n",
            "                  0.0079, 0.0076, 0.0074, 0.0072, 0.0083, 0.0073, 0.0083, 0.0079, 0.0078,\n",
            "                  0.0079, 0.0086, 0.0081, 0.0080, 0.0083, 0.0083, 0.0080, 0.0086, 0.0078,\n",
            "                  0.0084, 0.0085, 0.0084, 0.0078, 0.0072, 0.0079, 0.0080, 0.0083, 0.0085,\n",
            "                  0.0072, 0.0076, 0.0082, 0.0073, 0.0083, 0.0074, 0.0078, 0.0080, 0.0073,\n",
            "                  0.0075, 0.0076, 0.0083, 0.0075, 0.0077, 0.0080, 0.0081, 0.0091, 0.0088,\n",
            "                  0.0072], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
            "                 dtype=torch.int32)\n",
            "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "            min_val=tensor([-0.0556, -0.0623, -0.0525, -0.0560, -0.0589, -0.0540, -0.0538, -0.0508,\n",
            "                    -0.0506, -0.0510, -0.0530, -0.0536, -0.0490, -0.0633, -0.0555, -0.0535,\n",
            "                    -0.0571, -0.0565, -0.0521, -0.0501, -0.0507, -0.0529, -0.0508, -0.0543,\n",
            "                    -0.0451, -0.0584, -0.0573, -0.0516, -0.0564, -0.0535, -0.0597, -0.0483,\n",
            "                    -0.0484, -0.0583, -0.0489, -0.0510, -0.0519, -0.0516, -0.0506, -0.0582,\n",
            "                    -0.0504, -0.0535, -0.0568, -0.0502, -0.0466, -0.0480, -0.0463, -0.0531,\n",
            "                    -0.0546, -0.0616, -0.0545, -0.0504, -0.0602, -0.0497, -0.0520, -0.0567,\n",
            "                    -0.0549, -0.0556, -0.0575, -0.0510, -0.0502, -0.0593, -0.0521, -0.0533],\n",
            "                   device='cuda:0'), max_val=tensor([0.0618, 0.0610, 0.0580, 0.0509, 0.0568, 0.0532, 0.0550, 0.0618, 0.0677,\n",
            "                    0.0585, 0.0636, 0.0597, 0.0565, 0.0739, 0.0609, 0.0598, 0.0632, 0.0645,\n",
            "                    0.0590, 0.0570, 0.0553, 0.0538, 0.0623, 0.0549, 0.0621, 0.0595, 0.0584,\n",
            "                    0.0591, 0.0646, 0.0608, 0.0549, 0.0622, 0.0620, 0.0598, 0.0646, 0.0585,\n",
            "                    0.0627, 0.0634, 0.0633, 0.0570, 0.0536, 0.0596, 0.0599, 0.0626, 0.0639,\n",
            "                    0.0539, 0.0569, 0.0613, 0.0527, 0.0624, 0.0554, 0.0586, 0.0603, 0.0549,\n",
            "                    0.0563, 0.0529, 0.0622, 0.0562, 0.0548, 0.0600, 0.0611, 0.0682, 0.0660,\n",
            "                    0.0540], device='cuda:0')\n",
            "          )\n",
            "        )\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (bn2): BatchNorm2d(\n",
            "        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): NoopObserver()\n",
            "      )\n",
            "      (act_fn2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
            "  (fc): Linear(\n",
            "    in_features=64, out_features=10, bias=True\n",
            "    (weight_fake_quant): FakeQuantize(\n",
            "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0537, 0.0592, 0.0575, 0.0562, 0.0563, 0.0639, 0.0699, 0.0684, 0.0637,\n",
            "              0.0632], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\n",
            "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
            "        min_val=tensor([-0.3420, -0.2935, -0.2855, -0.2702, -0.3181, -0.3129, -0.3404, -0.3181,\n",
            "                -0.3342, -0.3082], device='cuda:0'), max_val=tensor([0.4027, 0.4438, 0.4310, 0.4217, 0.4219, 0.4792, 0.5242, 0.5130, 0.4778,\n",
            "                0.4737], device='cuda:0')\n",
            "      )\n",
            "    )\n",
            "    (activation_post_process): NoopObserver()\n",
            "  )\n",
            ")\n",
            "\n",
            "Model: LeNet5 with relu\n",
            "Original Accuracy: 0.6583%\n",
            "Quantized Accuracy: 0.6619%\n",
            "\n",
            "Model: LeNet5 with hardtanh\n",
            "Original Accuracy: 0.6388%\n",
            "Quantized Accuracy: 0.6391%\n",
            "\n",
            "Model: LeNet5 with relu6\n",
            "Original Accuracy: 0.6435%\n",
            "Quantized Accuracy: 0.6421%\n",
            "\n",
            "Model: ResNet20 with relu\n",
            "Original Accuracy: 0.8663%\n",
            "Quantized Accuracy: 0.8509%\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for model_class in models:\n",
        "    for activation in activations:\n",
        "        if model_class != LeNet5 and (model_class == ResNet18 or activation != \"relu\"):\n",
        "            continue\n",
        "\n",
        "        key = (model_class.__name__, activation)\n",
        "        orig_model = model_class(activation=activation).to(cuda_device)\n",
        "        quant_model = model_class(activation=activation, quantize=True).to(cuda_device)\n",
        "        configure_qat(quant_model)\n",
        "        \n",
        "        orig_model.eval()\n",
        "        quant_model.eval()\n",
        "        orig_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}.ckpt\"))\n",
        "        quant_model.load_state_dict(torch.load(f\"checkpoint/best_{model_class.__name__}_{activation}_quantized.ckpt\"), strict=True)\n",
        "\n",
        "        print(quant_model)\n",
        "        _, orig_acc = evaluate_model(orig_model)\n",
        "        _, quant_acc = evaluate_model(quant_model)\n",
        "\n",
        "        results[key] = {\n",
        "            \"orig_acc\": orig_acc,\n",
        "            \"quant_acc\": quant_acc,\n",
        "        }\n",
        "\n",
        "\n",
        "for key, result in results.items():\n",
        "    print(f\"\\nModel: {key[0]} with {key[1]}\")\n",
        "    print(f\"Original Accuracy: {result['orig_acc']:.4f}%\")\n",
        "    print(f\"Quantized Accuracy: {result['quant_acc']:.4f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
