[2025-05-12 08:36:47,214]: 
Training ResNet20 with relu6
[2025-05-12 08:38:21,813]: [ResNet20_relu6] Epoch: 001 Train Loss: 1.8592 Train Acc: 0.3039 Eval Loss: 1.5754 Eval Acc: 0.4121 (LR: 0.001000)
[2025-05-12 08:39:54,267]: [ResNet20_relu6] Epoch: 002 Train Loss: 1.5055 Train Acc: 0.4431 Eval Loss: 1.4011 Eval Acc: 0.4871 (LR: 0.001000)
[2025-05-12 08:41:17,939]: [ResNet20_relu6] Epoch: 003 Train Loss: 1.3514 Train Acc: 0.5052 Eval Loss: 1.2702 Eval Acc: 0.5300 (LR: 0.001000)
[2025-05-12 08:42:33,320]: [ResNet20_relu6] Epoch: 004 Train Loss: 1.2408 Train Acc: 0.5489 Eval Loss: 1.1572 Eval Acc: 0.5854 (LR: 0.001000)
[2025-05-12 08:44:01,131]: [ResNet20_relu6] Epoch: 005 Train Loss: 1.1436 Train Acc: 0.5889 Eval Loss: 1.1243 Eval Acc: 0.6017 (LR: 0.001000)
[2025-05-12 08:45:24,737]: [ResNet20_relu6] Epoch: 006 Train Loss: 1.0775 Train Acc: 0.6139 Eval Loss: 1.0328 Eval Acc: 0.6285 (LR: 0.001000)
[2025-05-12 08:46:49,042]: [ResNet20_relu6] Epoch: 007 Train Loss: 1.0172 Train Acc: 0.6326 Eval Loss: 0.9729 Eval Acc: 0.6531 (LR: 0.001000)
[2025-05-12 08:48:16,440]: [ResNet20_relu6] Epoch: 008 Train Loss: 0.9747 Train Acc: 0.6505 Eval Loss: 0.9273 Eval Acc: 0.6683 (LR: 0.001000)
[2025-05-12 08:49:47,273]: [ResNet20_relu6] Epoch: 009 Train Loss: 0.9336 Train Acc: 0.6667 Eval Loss: 0.9874 Eval Acc: 0.6618 (LR: 0.001000)
[2025-05-12 08:51:18,487]: [ResNet20_relu6] Epoch: 010 Train Loss: 0.9045 Train Acc: 0.6791 Eval Loss: 0.8797 Eval Acc: 0.6875 (LR: 0.001000)
[2025-05-12 08:52:48,225]: [ResNet20_relu6] Epoch: 011 Train Loss: 0.8616 Train Acc: 0.6946 Eval Loss: 0.8911 Eval Acc: 0.6920 (LR: 0.001000)
[2025-05-12 08:54:15,013]: [ResNet20_relu6] Epoch: 012 Train Loss: 0.8344 Train Acc: 0.7040 Eval Loss: 0.8689 Eval Acc: 0.7006 (LR: 0.001000)
[2025-05-12 08:55:32,307]: [ResNet20_relu6] Epoch: 013 Train Loss: 0.8020 Train Acc: 0.7156 Eval Loss: 0.8148 Eval Acc: 0.7153 (LR: 0.001000)
[2025-05-12 08:56:48,828]: [ResNet20_relu6] Epoch: 014 Train Loss: 0.7742 Train Acc: 0.7264 Eval Loss: 0.7832 Eval Acc: 0.7290 (LR: 0.001000)
[2025-05-12 08:58:27,637]: [ResNet20_relu6] Epoch: 015 Train Loss: 0.7521 Train Acc: 0.7345 Eval Loss: 0.7971 Eval Acc: 0.7225 (LR: 0.001000)
[2025-05-12 08:59:58,421]: [ResNet20_relu6] Epoch: 016 Train Loss: 0.7305 Train Acc: 0.7426 Eval Loss: 0.7252 Eval Acc: 0.7438 (LR: 0.001000)
[2025-05-12 09:01:31,155]: [ResNet20_relu6] Epoch: 017 Train Loss: 0.7073 Train Acc: 0.7516 Eval Loss: 0.7508 Eval Acc: 0.7411 (LR: 0.001000)
[2025-05-12 09:02:59,913]: [ResNet20_relu6] Epoch: 018 Train Loss: 0.6879 Train Acc: 0.7594 Eval Loss: 0.7013 Eval Acc: 0.7583 (LR: 0.001000)
[2025-05-12 09:04:33,130]: [ResNet20_relu6] Epoch: 019 Train Loss: 0.6745 Train Acc: 0.7646 Eval Loss: 0.6572 Eval Acc: 0.7699 (LR: 0.001000)
[2025-05-12 09:06:08,034]: [ResNet20_relu6] Epoch: 020 Train Loss: 0.6519 Train Acc: 0.7744 Eval Loss: 0.6698 Eval Acc: 0.7706 (LR: 0.001000)
[2025-05-12 09:07:36,864]: [ResNet20_relu6] Epoch: 021 Train Loss: 0.6379 Train Acc: 0.7769 Eval Loss: 0.6710 Eval Acc: 0.7717 (LR: 0.001000)
[2025-05-12 09:09:08,461]: [ResNet20_relu6] Epoch: 022 Train Loss: 0.6248 Train Acc: 0.7800 Eval Loss: 0.6406 Eval Acc: 0.7826 (LR: 0.001000)
[2025-05-12 09:10:40,958]: [ResNet20_relu6] Epoch: 023 Train Loss: 0.6125 Train Acc: 0.7868 Eval Loss: 0.6271 Eval Acc: 0.7832 (LR: 0.001000)
[2025-05-12 09:12:06,811]: [ResNet20_relu6] Epoch: 024 Train Loss: 0.6040 Train Acc: 0.7892 Eval Loss: 0.7449 Eval Acc: 0.7519 (LR: 0.001000)
[2025-05-12 09:13:41,102]: [ResNet20_relu6] Epoch: 025 Train Loss: 0.5926 Train Acc: 0.7943 Eval Loss: 0.6174 Eval Acc: 0.7881 (LR: 0.001000)
[2025-05-12 09:15:08,043]: [ResNet20_relu6] Epoch: 026 Train Loss: 0.5756 Train Acc: 0.8002 Eval Loss: 0.6259 Eval Acc: 0.7862 (LR: 0.001000)
[2025-05-12 09:16:32,468]: [ResNet20_relu6] Epoch: 027 Train Loss: 0.5664 Train Acc: 0.8031 Eval Loss: 0.5648 Eval Acc: 0.8093 (LR: 0.001000)
[2025-05-12 09:18:03,836]: [ResNet20_relu6] Epoch: 028 Train Loss: 0.5594 Train Acc: 0.8061 Eval Loss: 0.6563 Eval Acc: 0.7830 (LR: 0.001000)
[2025-05-12 09:19:29,838]: [ResNet20_relu6] Epoch: 029 Train Loss: 0.5480 Train Acc: 0.8106 Eval Loss: 0.6414 Eval Acc: 0.7811 (LR: 0.001000)
[2025-05-12 09:20:51,197]: [ResNet20_relu6] Epoch: 030 Train Loss: 0.5427 Train Acc: 0.8107 Eval Loss: 0.5760 Eval Acc: 0.8037 (LR: 0.001000)
[2025-05-12 09:22:06,729]: [ResNet20_relu6] Epoch: 031 Train Loss: 0.5339 Train Acc: 0.8147 Eval Loss: 0.5612 Eval Acc: 0.8074 (LR: 0.001000)
[2025-05-12 09:23:13,440]: [ResNet20_relu6] Epoch: 032 Train Loss: 0.5236 Train Acc: 0.8192 Eval Loss: 0.5541 Eval Acc: 0.8104 (LR: 0.001000)
[2025-05-12 09:24:30,997]: [ResNet20_relu6] Epoch: 033 Train Loss: 0.5197 Train Acc: 0.8198 Eval Loss: 0.5594 Eval Acc: 0.8118 (LR: 0.001000)
[2025-05-12 09:25:49,608]: [ResNet20_relu6] Epoch: 034 Train Loss: 0.5086 Train Acc: 0.8229 Eval Loss: 0.5200 Eval Acc: 0.8247 (LR: 0.001000)
[2025-05-12 09:27:21,659]: [ResNet20_relu6] Epoch: 035 Train Loss: 0.4994 Train Acc: 0.8261 Eval Loss: 0.5652 Eval Acc: 0.8086 (LR: 0.001000)
[2025-05-12 09:28:50,596]: [ResNet20_relu6] Epoch: 036 Train Loss: 0.4994 Train Acc: 0.8263 Eval Loss: 0.5684 Eval Acc: 0.8084 (LR: 0.001000)
[2025-05-12 09:30:21,113]: [ResNet20_relu6] Epoch: 037 Train Loss: 0.4849 Train Acc: 0.8311 Eval Loss: 0.5517 Eval Acc: 0.8130 (LR: 0.001000)
[2025-05-12 09:31:57,213]: [ResNet20_relu6] Epoch: 038 Train Loss: 0.4791 Train Acc: 0.8337 Eval Loss: 0.6070 Eval Acc: 0.7978 (LR: 0.001000)
[2025-05-12 09:33:27,144]: [ResNet20_relu6] Epoch: 039 Train Loss: 0.4724 Train Acc: 0.8366 Eval Loss: 0.5562 Eval Acc: 0.8128 (LR: 0.001000)
[2025-05-12 09:35:02,193]: [ResNet20_relu6] Epoch: 040 Train Loss: 0.4675 Train Acc: 0.8363 Eval Loss: 0.5496 Eval Acc: 0.8131 (LR: 0.001000)
[2025-05-12 09:36:36,475]: [ResNet20_relu6] Epoch: 041 Train Loss: 0.4614 Train Acc: 0.8409 Eval Loss: 0.5192 Eval Acc: 0.8281 (LR: 0.001000)
[2025-05-12 09:38:08,877]: [ResNet20_relu6] Epoch: 042 Train Loss: 0.4536 Train Acc: 0.8416 Eval Loss: 0.4964 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-12 09:39:45,563]: [ResNet20_relu6] Epoch: 043 Train Loss: 0.4497 Train Acc: 0.8445 Eval Loss: 0.4994 Eval Acc: 0.8324 (LR: 0.001000)
[2025-05-12 09:41:26,308]: [ResNet20_relu6] Epoch: 044 Train Loss: 0.4429 Train Acc: 0.8457 Eval Loss: 0.5128 Eval Acc: 0.8281 (LR: 0.001000)
[2025-05-12 09:43:08,148]: [ResNet20_relu6] Epoch: 045 Train Loss: 0.4462 Train Acc: 0.8421 Eval Loss: 0.6063 Eval Acc: 0.8023 (LR: 0.001000)
[2025-05-12 09:44:36,326]: [ResNet20_relu6] Epoch: 046 Train Loss: 0.4388 Train Acc: 0.8470 Eval Loss: 0.5522 Eval Acc: 0.8162 (LR: 0.001000)
[2025-05-12 09:45:55,254]: [ResNet20_relu6] Epoch: 047 Train Loss: 0.4293 Train Acc: 0.8511 Eval Loss: 0.4985 Eval Acc: 0.8349 (LR: 0.001000)
[2025-05-12 09:47:11,216]: [ResNet20_relu6] Epoch: 048 Train Loss: 0.4286 Train Acc: 0.8507 Eval Loss: 0.4911 Eval Acc: 0.8385 (LR: 0.001000)
[2025-05-12 09:48:28,042]: [ResNet20_relu6] Epoch: 049 Train Loss: 0.4171 Train Acc: 0.8525 Eval Loss: 0.5037 Eval Acc: 0.8304 (LR: 0.001000)
[2025-05-12 09:49:47,204]: [ResNet20_relu6] Epoch: 050 Train Loss: 0.4173 Train Acc: 0.8555 Eval Loss: 0.4805 Eval Acc: 0.8450 (LR: 0.001000)
[2025-05-12 09:51:03,020]: [ResNet20_relu6] Epoch: 051 Train Loss: 0.4075 Train Acc: 0.8580 Eval Loss: 0.4733 Eval Acc: 0.8443 (LR: 0.001000)
[2025-05-12 09:52:20,033]: [ResNet20_relu6] Epoch: 052 Train Loss: 0.4086 Train Acc: 0.8576 Eval Loss: 0.4872 Eval Acc: 0.8377 (LR: 0.001000)
[2025-05-12 09:53:39,593]: [ResNet20_relu6] Epoch: 053 Train Loss: 0.4053 Train Acc: 0.8582 Eval Loss: 0.4803 Eval Acc: 0.8394 (LR: 0.001000)
[2025-05-12 09:54:59,441]: [ResNet20_relu6] Epoch: 054 Train Loss: 0.4037 Train Acc: 0.8603 Eval Loss: 0.4917 Eval Acc: 0.8397 (LR: 0.001000)
[2025-05-12 09:56:16,982]: [ResNet20_relu6] Epoch: 055 Train Loss: 0.3995 Train Acc: 0.8612 Eval Loss: 0.4977 Eval Acc: 0.8364 (LR: 0.001000)
[2025-05-12 09:57:29,604]: [ResNet20_relu6] Epoch: 056 Train Loss: 0.3943 Train Acc: 0.8620 Eval Loss: 0.4892 Eval Acc: 0.8371 (LR: 0.001000)
[2025-05-12 09:58:45,594]: [ResNet20_relu6] Epoch: 057 Train Loss: 0.3880 Train Acc: 0.8641 Eval Loss: 0.4681 Eval Acc: 0.8491 (LR: 0.001000)
[2025-05-12 10:00:00,109]: [ResNet20_relu6] Epoch: 058 Train Loss: 0.3858 Train Acc: 0.8644 Eval Loss: 0.4720 Eval Acc: 0.8449 (LR: 0.001000)
[2025-05-12 10:01:08,020]: [ResNet20_relu6] Epoch: 059 Train Loss: 0.3827 Train Acc: 0.8660 Eval Loss: 0.5058 Eval Acc: 0.8380 (LR: 0.001000)
[2025-05-12 10:02:20,720]: [ResNet20_relu6] Epoch: 060 Train Loss: 0.3837 Train Acc: 0.8658 Eval Loss: 0.4655 Eval Acc: 0.8442 (LR: 0.001000)
[2025-05-12 10:03:25,954]: [ResNet20_relu6] Epoch: 061 Train Loss: 0.3747 Train Acc: 0.8707 Eval Loss: 0.4945 Eval Acc: 0.8380 (LR: 0.001000)
[2025-05-12 10:04:35,931]: [ResNet20_relu6] Epoch: 062 Train Loss: 0.3711 Train Acc: 0.8704 Eval Loss: 0.4677 Eval Acc: 0.8446 (LR: 0.001000)
[2025-05-12 10:05:32,210]: [ResNet20_relu6] Epoch: 063 Train Loss: 0.3673 Train Acc: 0.8735 Eval Loss: 0.4475 Eval Acc: 0.8533 (LR: 0.001000)
[2025-05-12 10:06:33,791]: [ResNet20_relu6] Epoch: 064 Train Loss: 0.3652 Train Acc: 0.8728 Eval Loss: 0.5001 Eval Acc: 0.8372 (LR: 0.001000)
[2025-05-12 10:07:25,534]: [ResNet20_relu6] Epoch: 065 Train Loss: 0.3623 Train Acc: 0.8729 Eval Loss: 0.4637 Eval Acc: 0.8470 (LR: 0.001000)
[2025-05-12 10:08:29,428]: [ResNet20_relu6] Epoch: 066 Train Loss: 0.3611 Train Acc: 0.8739 Eval Loss: 0.4902 Eval Acc: 0.8434 (LR: 0.001000)
[2025-05-12 10:09:32,931]: [ResNet20_relu6] Epoch: 067 Train Loss: 0.3567 Train Acc: 0.8749 Eval Loss: 0.4756 Eval Acc: 0.8442 (LR: 0.001000)
[2025-05-12 10:10:37,803]: [ResNet20_relu6] Epoch: 068 Train Loss: 0.3557 Train Acc: 0.8759 Eval Loss: 0.5140 Eval Acc: 0.8299 (LR: 0.001000)
[2025-05-12 10:11:45,197]: [ResNet20_relu6] Epoch: 069 Train Loss: 0.3500 Train Acc: 0.8775 Eval Loss: 0.4765 Eval Acc: 0.8487 (LR: 0.001000)
[2025-05-12 10:12:50,406]: [ResNet20_relu6] Epoch: 070 Train Loss: 0.3520 Train Acc: 0.8763 Eval Loss: 0.4699 Eval Acc: 0.8462 (LR: 0.000100)
[2025-05-12 10:14:00,864]: [ResNet20_relu6] Epoch: 071 Train Loss: 0.3067 Train Acc: 0.8930 Eval Loss: 0.3939 Eval Acc: 0.8705 (LR: 0.000100)
[2025-05-12 10:15:06,147]: [ResNet20_relu6] Epoch: 072 Train Loss: 0.3005 Train Acc: 0.8963 Eval Loss: 0.3915 Eval Acc: 0.8713 (LR: 0.000100)
[2025-05-12 10:16:16,497]: [ResNet20_relu6] Epoch: 073 Train Loss: 0.2955 Train Acc: 0.8988 Eval Loss: 0.3967 Eval Acc: 0.8694 (LR: 0.000100)
[2025-05-12 10:17:18,761]: [ResNet20_relu6] Epoch: 074 Train Loss: 0.2921 Train Acc: 0.9002 Eval Loss: 0.3950 Eval Acc: 0.8694 (LR: 0.000100)
[2025-05-12 10:18:26,017]: [ResNet20_relu6] Epoch: 075 Train Loss: 0.2882 Train Acc: 0.9004 Eval Loss: 0.3924 Eval Acc: 0.8711 (LR: 0.000100)
[2025-05-12 10:19:14,728]: [ResNet20_relu6] Epoch: 076 Train Loss: 0.2905 Train Acc: 0.9001 Eval Loss: 0.3938 Eval Acc: 0.8729 (LR: 0.000100)
[2025-05-12 10:20:14,672]: [ResNet20_relu6] Epoch: 077 Train Loss: 0.2888 Train Acc: 0.8995 Eval Loss: 0.3932 Eval Acc: 0.8717 (LR: 0.000100)
[2025-05-12 10:21:12,754]: [ResNet20_relu6] Epoch: 078 Train Loss: 0.2850 Train Acc: 0.9009 Eval Loss: 0.3939 Eval Acc: 0.8708 (LR: 0.000100)
[2025-05-12 10:22:22,335]: [ResNet20_relu6] Epoch: 079 Train Loss: 0.2892 Train Acc: 0.9001 Eval Loss: 0.3910 Eval Acc: 0.8733 (LR: 0.000100)
[2025-05-12 10:23:30,889]: [ResNet20_relu6] Epoch: 080 Train Loss: 0.2865 Train Acc: 0.9002 Eval Loss: 0.3892 Eval Acc: 0.8724 (LR: 0.000100)
[2025-05-12 10:24:37,891]: [ResNet20_relu6] Epoch: 081 Train Loss: 0.2829 Train Acc: 0.9023 Eval Loss: 0.3936 Eval Acc: 0.8713 (LR: 0.000100)
[2025-05-12 10:25:49,145]: [ResNet20_relu6] Epoch: 082 Train Loss: 0.2828 Train Acc: 0.9010 Eval Loss: 0.3981 Eval Acc: 0.8721 (LR: 0.000100)
[2025-05-12 10:27:05,387]: [ResNet20_relu6] Epoch: 083 Train Loss: 0.2838 Train Acc: 0.9006 Eval Loss: 0.3956 Eval Acc: 0.8715 (LR: 0.000100)
[2025-05-12 10:28:20,490]: [ResNet20_relu6] Epoch: 084 Train Loss: 0.2800 Train Acc: 0.9043 Eval Loss: 0.3935 Eval Acc: 0.8727 (LR: 0.000100)
[2025-05-12 10:29:31,836]: [ResNet20_relu6] Epoch: 085 Train Loss: 0.2791 Train Acc: 0.9033 Eval Loss: 0.3924 Eval Acc: 0.8737 (LR: 0.000100)
[2025-05-12 10:30:41,728]: [ResNet20_relu6] Epoch: 086 Train Loss: 0.2799 Train Acc: 0.9033 Eval Loss: 0.3954 Eval Acc: 0.8732 (LR: 0.000100)
[2025-05-12 10:31:56,024]: [ResNet20_relu6] Epoch: 087 Train Loss: 0.2815 Train Acc: 0.9027 Eval Loss: 0.3955 Eval Acc: 0.8741 (LR: 0.000100)
[2025-05-12 10:33:06,389]: [ResNet20_relu6] Epoch: 088 Train Loss: 0.2783 Train Acc: 0.9036 Eval Loss: 0.3925 Eval Acc: 0.8740 (LR: 0.000100)
[2025-05-12 10:34:19,385]: [ResNet20_relu6] Epoch: 089 Train Loss: 0.2787 Train Acc: 0.9039 Eval Loss: 0.3961 Eval Acc: 0.8721 (LR: 0.000100)
[2025-05-12 10:35:33,571]: [ResNet20_relu6] Epoch: 090 Train Loss: 0.2780 Train Acc: 0.9043 Eval Loss: 0.3937 Eval Acc: 0.8738 (LR: 0.000100)
[2025-05-12 10:36:48,980]: [ResNet20_relu6] Epoch: 091 Train Loss: 0.2773 Train Acc: 0.9035 Eval Loss: 0.3950 Eval Acc: 0.8724 (LR: 0.000100)
[2025-05-12 10:38:01,310]: [ResNet20_relu6] Epoch: 092 Train Loss: 0.2772 Train Acc: 0.9039 Eval Loss: 0.3933 Eval Acc: 0.8735 (LR: 0.000100)
[2025-05-12 10:39:10,939]: [ResNet20_relu6] Epoch: 093 Train Loss: 0.2783 Train Acc: 0.9034 Eval Loss: 0.3964 Eval Acc: 0.8730 (LR: 0.000100)
[2025-05-12 10:40:20,982]: [ResNet20_relu6] Epoch: 094 Train Loss: 0.2776 Train Acc: 0.9041 Eval Loss: 0.3933 Eval Acc: 0.8741 (LR: 0.000100)
[2025-05-12 10:41:35,523]: [ResNet20_relu6] Epoch: 095 Train Loss: 0.2755 Train Acc: 0.9050 Eval Loss: 0.3947 Eval Acc: 0.8723 (LR: 0.000100)
[2025-05-12 10:42:43,461]: [ResNet20_relu6] Epoch: 096 Train Loss: 0.2758 Train Acc: 0.9026 Eval Loss: 0.3948 Eval Acc: 0.8741 (LR: 0.000100)
[2025-05-12 10:43:50,268]: [ResNet20_relu6] Epoch: 097 Train Loss: 0.2761 Train Acc: 0.9027 Eval Loss: 0.3950 Eval Acc: 0.8758 (LR: 0.000100)
[2025-05-12 10:45:02,019]: [ResNet20_relu6] Epoch: 098 Train Loss: 0.2754 Train Acc: 0.9062 Eval Loss: 0.3912 Eval Acc: 0.8750 (LR: 0.000100)
[2025-05-12 10:46:08,229]: [ResNet20_relu6] Epoch: 099 Train Loss: 0.2703 Train Acc: 0.9060 Eval Loss: 0.3934 Eval Acc: 0.8740 (LR: 0.000100)
[2025-05-12 10:47:21,593]: [ResNet20_relu6] Epoch: 100 Train Loss: 0.2735 Train Acc: 0.9043 Eval Loss: 0.3916 Eval Acc: 0.8737 (LR: 0.000010)
[2025-05-12 10:48:29,928]: [ResNet20_relu6] Epoch: 101 Train Loss: 0.2708 Train Acc: 0.9065 Eval Loss: 0.3915 Eval Acc: 0.8741 (LR: 0.000010)
[2025-05-12 10:49:39,297]: [ResNet20_relu6] Epoch: 102 Train Loss: 0.2672 Train Acc: 0.9088 Eval Loss: 0.3920 Eval Acc: 0.8737 (LR: 0.000010)
[2025-05-12 10:50:52,189]: [ResNet20_relu6] Epoch: 103 Train Loss: 0.2683 Train Acc: 0.9074 Eval Loss: 0.3908 Eval Acc: 0.8749 (LR: 0.000010)
[2025-05-12 10:51:57,505]: [ResNet20_relu6] Epoch: 104 Train Loss: 0.2664 Train Acc: 0.9080 Eval Loss: 0.3895 Eval Acc: 0.8757 (LR: 0.000010)
[2025-05-12 10:53:06,255]: [ResNet20_relu6] Epoch: 105 Train Loss: 0.2686 Train Acc: 0.9072 Eval Loss: 0.3922 Eval Acc: 0.8735 (LR: 0.000010)
[2025-05-12 10:54:12,266]: [ResNet20_relu6] Epoch: 106 Train Loss: 0.2664 Train Acc: 0.9076 Eval Loss: 0.3893 Eval Acc: 0.8748 (LR: 0.000010)
[2025-05-12 10:55:08,394]: [ResNet20_relu6] Epoch: 107 Train Loss: 0.2670 Train Acc: 0.9070 Eval Loss: 0.3906 Eval Acc: 0.8735 (LR: 0.000010)
[2025-05-12 10:56:06,776]: [ResNet20_relu6] Epoch: 108 Train Loss: 0.2658 Train Acc: 0.9068 Eval Loss: 0.3913 Eval Acc: 0.8746 (LR: 0.000010)
[2025-05-12 10:57:03,921]: [ResNet20_relu6] Epoch: 109 Train Loss: 0.2680 Train Acc: 0.9060 Eval Loss: 0.3906 Eval Acc: 0.8751 (LR: 0.000010)
[2025-05-12 10:58:10,174]: [ResNet20_relu6] Epoch: 110 Train Loss: 0.2641 Train Acc: 0.9076 Eval Loss: 0.3899 Eval Acc: 0.8753 (LR: 0.000010)
[2025-05-12 10:59:16,058]: [ResNet20_relu6] Epoch: 111 Train Loss: 0.2671 Train Acc: 0.9070 Eval Loss: 0.3884 Eval Acc: 0.8749 (LR: 0.000010)
[2025-05-12 11:00:26,318]: [ResNet20_relu6] Epoch: 112 Train Loss: 0.2657 Train Acc: 0.9094 Eval Loss: 0.3900 Eval Acc: 0.8752 (LR: 0.000010)
[2025-05-12 11:01:29,583]: [ResNet20_relu6] Epoch: 113 Train Loss: 0.2651 Train Acc: 0.9092 Eval Loss: 0.3882 Eval Acc: 0.8758 (LR: 0.000010)
[2025-05-12 11:02:40,097]: [ResNet20_relu6] Epoch: 114 Train Loss: 0.2673 Train Acc: 0.9082 Eval Loss: 0.3955 Eval Acc: 0.8724 (LR: 0.000010)
[2025-05-12 11:03:44,748]: [ResNet20_relu6] Epoch: 115 Train Loss: 0.2716 Train Acc: 0.9063 Eval Loss: 0.3913 Eval Acc: 0.8753 (LR: 0.000010)
[2025-05-12 11:04:37,253]: [ResNet20_relu6] Epoch: 116 Train Loss: 0.2644 Train Acc: 0.9083 Eval Loss: 0.3915 Eval Acc: 0.8742 (LR: 0.000010)
[2025-05-12 11:05:27,548]: [ResNet20_relu6] Epoch: 117 Train Loss: 0.2658 Train Acc: 0.9090 Eval Loss: 0.3908 Eval Acc: 0.8746 (LR: 0.000010)
[2025-05-12 11:05:27,549]: Early stopping was triggered!
[2025-05-12 11:05:27,549]: [ResNet20_relu6] Best Eval Accuracy: 0.8758
[2025-05-12 11:05:27,576]: 
Training of full-precision model finished!
[2025-05-12 11:05:27,576]: Model Architecture:
[2025-05-12 11:05:27,577]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:05:27,577]: 
Model Weights:
[2025-05-12 11:05:27,577]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 11:05:27,599]: Sample Values (25 elements): [-0.015555236488580704, 0.06234687566757202, 0.027820948511362076, -0.05442791432142258, 0.12172363698482513, 0.024032289162278175, 0.2665603458881378, 0.11344902217388153, 0.3067314624786377, -0.1413663774728775, 0.4645552933216095, -0.21488028764724731, -0.01743548922240734, -0.2889649569988251, 0.07539592683315277, -0.07545514404773712, -0.10834477096796036, 0.2655116319656372, -0.1376667022705078, -0.04934567213058472, 0.0250389501452446, -0.22773854434490204, 0.18364422023296356, -0.09092982113361359, 0.2524643540382385]
[2025-05-12 11:05:27,607]: Mean: -0.00217092
[2025-05-12 11:05:27,618]: Min: -0.41431385
[2025-05-12 11:05:27,632]: Max: 0.46455529
[2025-05-12 11:05:27,632]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,641]: Sample Values (16 elements): [0.82960444688797, 0.9712873697280884, 0.9403159618377686, 0.8697679042816162, 0.9691023230552673, 0.9435600638389587, 1.0196959972381592, 0.9791669249534607, 0.9517862796783447, 0.9732092618942261, 0.949558675289154, 0.8799673914909363, 0.8111761808395386, 0.9714764952659607, 0.9871712923049927, 1.0321210622787476]
[2025-05-12 11:05:27,642]: Mean: 0.94243544
[2025-05-12 11:05:27,643]: Min: 0.81117618
[2025-05-12 11:05:27,644]: Max: 1.03212106
[2025-05-12 11:05:27,644]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:05:27,649]: Sample Values (25 elements): [0.0037897168658673763, -0.041815586388111115, 0.016533322632312775, 0.01725056953728199, 0.030239524319767952, 0.0005712536512874067, 0.054383501410484314, -0.06192365288734436, 0.12192200124263763, 0.03938714787364006, 0.05855463445186615, -0.011695882305502892, 0.0009607158717699349, 0.1103932112455368, -0.006148953922092915, -0.07132422924041748, -0.059574078768491745, -0.09946009516716003, 0.047337986528873444, -0.11700615286827087, 0.013101855292916298, -0.044313427060842514, 0.01400131918489933, 0.15988363325595856, 0.045509446412324905]
[2025-05-12 11:05:27,652]: Mean: -0.00508357
[2025-05-12 11:05:27,654]: Min: -0.26241294
[2025-05-12 11:05:27,654]: Max: 0.25463867
[2025-05-12 11:05:27,654]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,657]: Sample Values (16 elements): [1.0610839128494263, 0.9462055563926697, 0.9583539962768555, 0.9001021385192871, 0.9875110387802124, 0.9154037237167358, 0.9281938076019287, 0.9540413618087769, 0.9019508957862854, 1.0099748373031616, 0.98753821849823, 0.9056928157806396, 0.8400694131851196, 0.9313960075378418, 1.0088317394256592, 1.2134125232696533]
[2025-05-12 11:05:27,661]: Mean: 0.96561015
[2025-05-12 11:05:27,662]: Min: 0.84006941
[2025-05-12 11:05:27,664]: Max: 1.21341252
[2025-05-12 11:05:27,664]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:05:27,666]: Sample Values (25 elements): [-0.00032676628325134516, 0.1537810117006302, -0.06349415332078934, 0.05238575115799904, -0.11252971738576889, 0.09021150320768356, 0.0695563331246376, 0.033220238983631134, -0.06395295262336731, -0.006954542826861143, -0.10725608468055725, 0.01481647975742817, 0.02516188472509384, -0.04430917650461197, 0.10802258551120758, -0.0010241230484098196, -0.041764385998249054, 0.05888867750763893, -0.016400886699557304, -0.08399241417646408, -0.024153053760528564, -0.06871511787176132, -0.02892698533833027, -0.04711855575442314, -0.013674496673047543]
[2025-05-12 11:05:27,669]: Mean: -0.00661969
[2025-05-12 11:05:27,672]: Min: -0.25465593
[2025-05-12 11:05:27,675]: Max: 0.19005676
[2025-05-12 11:05:27,675]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,685]: Sample Values (16 elements): [0.9219076037406921, 0.9915598630905151, 0.9545282125473022, 0.9340626001358032, 0.9388591647148132, 1.002206802368164, 0.9625418186187744, 0.9392557740211487, 0.8988445997238159, 1.0111628770828247, 0.9178022146224976, 1.0546424388885498, 1.0322123765945435, 0.9231488108634949, 0.8910037875175476, 0.9376549124717712]
[2025-05-12 11:05:27,692]: Mean: 0.95696211
[2025-05-12 11:05:27,706]: Min: 0.89100379
[2025-05-12 11:05:27,713]: Max: 1.05464244
[2025-05-12 11:05:27,714]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:05:27,718]: Sample Values (25 elements): [0.14006102085113525, -0.04716665670275688, -0.03336421400308609, -0.05050061270594597, 0.07592713087797165, -0.0394553616642952, 0.017742324620485306, 0.04674533009529114, 0.13041523098945618, 0.1012897938489914, 0.0042011248879134655, -0.09630125761032104, -0.005435570143163204, -0.03229512646794319, 0.11471741646528244, -0.02927585318684578, -0.041601985692977905, 0.008504923433065414, 0.025300871580839157, -0.02388743683695793, 0.02722693420946598, -0.010162947699427605, -0.07009895145893097, -0.024250458925962448, -0.05740988999605179]
[2025-05-12 11:05:27,719]: Mean: -0.00026892
[2025-05-12 11:05:27,728]: Min: -0.36582538
[2025-05-12 11:05:27,728]: Max: 0.23175400
[2025-05-12 11:05:27,729]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,729]: Sample Values (16 elements): [0.8526464104652405, 0.9335021376609802, 0.9458043575286865, 0.9697980284690857, 1.0798888206481934, 0.9526911973953247, 1.0101356506347656, 0.9659003019332886, 1.092736840248108, 0.9781055450439453, 1.0410571098327637, 0.9662887454032898, 0.8699436187744141, 0.9904306530952454, 0.9255362749099731, 0.9299032092094421]
[2025-05-12 11:05:27,729]: Mean: 0.96902311
[2025-05-12 11:05:27,730]: Min: 0.85264641
[2025-05-12 11:05:27,733]: Max: 1.09273684
[2025-05-12 11:05:27,733]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:05:27,739]: Sample Values (25 elements): [-0.055183157324790955, -0.044260796159505844, -0.09594942629337311, 0.04934335872530937, 0.08973278105258942, 0.024458665400743484, -0.012057986110448837, 0.03498726710677147, 0.0908617377281189, -0.03572901338338852, 0.026674002408981323, 0.05956985801458359, 0.006342059001326561, 0.03717454522848129, 0.10016022622585297, -0.019698387011885643, 0.036162037402391434, 0.0025109192356467247, -0.061959169805049896, -0.029501572251319885, 0.06890927255153656, 0.0031452930998057127, 0.032484039664268494, -0.05446776747703552, -0.04499302804470062]
[2025-05-12 11:05:27,740]: Mean: -0.00178671
[2025-05-12 11:05:27,742]: Min: -0.24341005
[2025-05-12 11:05:27,745]: Max: 0.20913196
[2025-05-12 11:05:27,745]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,758]: Sample Values (16 elements): [0.9672292470932007, 1.0130265951156616, 0.9617103338241577, 0.9612610340118408, 1.0164015293121338, 1.0661182403564453, 0.8801037669181824, 0.985175609588623, 0.8958598971366882, 1.0011826753616333, 1.0157978534698486, 1.0044224262237549, 0.9257702827453613, 1.1177725791931152, 0.9081193208694458, 0.9175053834915161]
[2025-05-12 11:05:27,765]: Mean: 0.97734106
[2025-05-12 11:05:27,773]: Min: 0.88010377
[2025-05-12 11:05:27,774]: Max: 1.11777258
[2025-05-12 11:05:27,774]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:05:27,784]: Sample Values (25 elements): [-0.026981761679053307, -0.019087396562099457, 0.06327753514051437, 0.023376919329166412, -0.036441002041101456, 0.09168540686368942, -0.04794396832585335, -0.08971038460731506, -0.10239437222480774, -0.025302618741989136, 0.018175913020968437, -0.03753145784139633, -0.06743230670690536, -0.012754496186971664, 0.04288662597537041, -0.05172010883688927, 0.061600763350725174, 0.03382612019777298, 0.03527965024113655, 0.05293210595846176, -0.040241796523332596, -0.05874548852443695, 0.06199709326028824, -0.027900878340005875, 0.06693880259990692]
[2025-05-12 11:05:27,784]: Mean: -0.00127750
[2025-05-12 11:05:27,785]: Min: -0.17408736
[2025-05-12 11:05:27,785]: Max: 0.15955572
[2025-05-12 11:05:27,785]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,788]: Sample Values (16 elements): [1.0182095766067505, 0.9709465503692627, 1.0028682947158813, 0.9462302327156067, 0.9309574365615845, 0.9561683535575867, 0.937445342540741, 0.991246223449707, 0.9460048079490662, 1.0000760555267334, 0.9821298718452454, 0.9731598496437073, 0.9536235332489014, 0.9767462015151978, 0.9929454922676086, 0.9676724672317505]
[2025-05-12 11:05:27,791]: Mean: 0.97165191
[2025-05-12 11:05:27,791]: Min: 0.93095744
[2025-05-12 11:05:27,797]: Max: 1.01820958
[2025-05-12 11:05:27,797]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:05:27,802]: Sample Values (25 elements): [-0.05293181911110878, 0.07458753138780594, -0.000986987492069602, -0.04789161682128906, 0.09552936255931854, 0.04488613083958626, -0.08495687693357468, -0.00337800825946033, 0.048350196331739426, -0.04614303633570671, 0.0271061472594738, -0.007102333474904299, 0.030099069699645042, -0.10711563378572464, 0.005026310682296753, 0.025155214592814445, 0.03007633052766323, 0.10248975455760956, -0.02374722622334957, 0.050781525671482086, -0.008933867327868938, 0.0003363694413565099, 0.05970286577939987, -0.0386740081012249, 0.02397901751101017]
[2025-05-12 11:05:27,803]: Mean: 0.00446166
[2025-05-12 11:05:27,804]: Min: -0.15161772
[2025-05-12 11:05:27,805]: Max: 0.17536297
[2025-05-12 11:05:27,805]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:05:27,809]: Sample Values (16 elements): [1.0223084688186646, 0.9175274968147278, 0.9351231455802917, 1.050735592842102, 0.9445242285728455, 0.9715240597724915, 0.9850732684135437, 0.9347430467605591, 1.0218021869659424, 0.9254943132400513, 0.9578601121902466, 1.0026854276657104, 0.9496945142745972, 0.9729701280593872, 0.9281061291694641, 0.9491670727729797]
[2025-05-12 11:05:27,811]: Mean: 0.96683371
[2025-05-12 11:05:27,812]: Min: 0.91752750
[2025-05-12 11:05:27,817]: Max: 1.05073559
[2025-05-12 11:05:27,817]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 11:05:27,825]: Sample Values (25 elements): [-0.11063548177480698, 0.07388502359390259, 0.0014258107403293252, -0.07632151991128922, 0.04148542135953903, 0.04735851660370827, 0.0054748631082475185, -0.02165985479950905, -0.07817739993333817, -0.02474191039800644, -0.09539373219013214, -0.018031252548098564, -0.10676328092813492, 0.020861370489001274, 0.018713930621743202, -0.056778863072395325, -0.008262810297310352, -0.03405103459954262, -0.005621456075459719, 0.006625521928071976, -0.08175735175609589, -0.07038091123104095, 0.046275049448013306, -0.07578425109386444, 0.009829501621425152]
[2025-05-12 11:05:27,828]: Mean: -0.00255837
[2025-05-12 11:05:27,830]: Min: -0.16188423
[2025-05-12 11:05:27,834]: Max: 0.14978592
[2025-05-12 11:05:27,834]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:05:27,860]: Sample Values (25 elements): [0.9463622570037842, 0.9405783414840698, 0.9541155099868774, 0.9681991934776306, 0.94450443983078, 1.0133870840072632, 0.9464497566223145, 0.9390439391136169, 1.0066962242126465, 0.9877651333808899, 1.0316990613937378, 0.9646947979927063, 0.9742883443832397, 0.9733706712722778, 0.9757831692695618, 0.94688880443573, 1.008462905883789, 0.9826675057411194, 0.9738548398017883, 0.97147136926651, 0.9801679253578186, 0.9238371253013611, 0.9677572846412659, 0.9287264347076416, 1.0065712928771973]
[2025-05-12 11:05:27,865]: Mean: 0.97142744
[2025-05-12 11:05:27,876]: Min: 0.92383713
[2025-05-12 11:05:27,885]: Max: 1.03169906
[2025-05-12 11:05:27,885]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:05:27,897]: Sample Values (25 elements): [-0.0007242296705953777, 0.02534933015704155, 0.06233860179781914, 0.028301632031798363, 0.02884156070649624, -0.07879669219255447, 0.053461797535419464, -0.00716973515227437, 0.024614788591861725, 0.004193459637463093, -0.02066730335354805, -0.05967553332448006, -0.0037177996709942818, 0.062027450650930405, -0.015742434188723564, 0.05407183617353439, -0.014341284520924091, -0.04684757813811302, -0.005543336737900972, 0.06916780769824982, 0.0883949026465416, 0.05977778136730194, -0.04101185128092766, 0.051341794431209564, -0.01925915852189064]
[2025-05-12 11:05:27,901]: Mean: -0.00243687
[2025-05-12 11:05:27,905]: Min: -0.13292612
[2025-05-12 11:05:27,909]: Max: 0.16702336
[2025-05-12 11:05:27,909]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:05:27,914]: Sample Values (25 elements): [1.0132508277893066, 1.0235660076141357, 0.9989749789237976, 0.994866132736206, 0.9791312217712402, 0.974797248840332, 0.9948697686195374, 0.9917595386505127, 0.9543544054031372, 0.9591935276985168, 1.0434627532958984, 1.0087130069732666, 0.927151620388031, 0.9796141386032104, 1.0556784868240356, 1.010019063949585, 0.9788212180137634, 0.9655796885490417, 0.9756991267204285, 0.980673611164093, 1.0015673637390137, 0.9258459806442261, 0.989778995513916, 1.022400975227356, 0.9688878059387207]
[2025-05-12 11:05:27,914]: Mean: 0.98876429
[2025-05-12 11:05:27,916]: Min: 0.92584598
[2025-05-12 11:05:27,918]: Max: 1.05567849
[2025-05-12 11:05:27,918]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 11:05:27,920]: Sample Values (25 elements): [0.17637605965137482, -0.18396347761154175, -0.24736157059669495, -0.1714932769536972, -0.0588412843644619, 0.18701934814453125, 0.0653034895658493, 0.1022832989692688, -0.13146601617336273, 0.1488720029592514, -0.12180493026971817, 0.0668211504817009, 0.13419586420059204, -0.018938658758997917, 0.18402236700057983, 0.22945913672447205, 0.1290024369955063, 0.12288687378168106, 0.0015498045831918716, 0.11355314403772354, -0.16381895542144775, -0.011710693128407001, 0.04532143846154213, -0.008313512429594994, 0.08140644431114197]
[2025-05-12 11:05:27,921]: Mean: -0.01202782
[2025-05-12 11:05:27,922]: Min: -0.33272597
[2025-05-12 11:05:27,924]: Max: 0.29549730
[2025-05-12 11:05:27,924]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 11:05:27,934]: Sample Values (25 elements): [0.8941407799720764, 0.9154193997383118, 0.8846413493156433, 0.8977342844009399, 0.8895688056945801, 0.9466601610183716, 0.9082387089729309, 0.8953590989112854, 0.8784403204917908, 0.8716742992401123, 0.911105751991272, 0.9258013367652893, 0.8864918947219849, 0.9359943866729736, 0.9002286195755005, 0.9180118441581726, 0.9165241122245789, 0.8750239610671997, 0.8572744727134705, 0.8849126696586609, 0.9071003198623657, 0.885761559009552, 0.9059128165245056, 0.9074681401252747, 0.9043740630149841]
[2025-05-12 11:05:27,940]: Mean: 0.90024674
[2025-05-12 11:05:27,949]: Min: 0.85435158
[2025-05-12 11:05:27,959]: Max: 0.94666016
[2025-05-12 11:05:27,959]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:05:27,960]: Sample Values (25 elements): [-0.03389040753245354, 0.04302819445729256, -0.021209940314292908, 0.012002114206552505, 0.006587722338736057, 0.022515492513775826, -0.010975728742778301, -0.025302177295088768, 0.045397721230983734, 0.012217581272125244, 0.05949446186423302, -0.0063191549852490425, -0.0007915954920463264, -0.005412380211055279, -0.0630253329873085, 0.02828208915889263, -0.023192068561911583, -0.0044778622686862946, 0.018483426421880722, 0.05147945508360863, -0.008258038200438023, -0.0174437053501606, 0.08222650736570358, -0.024141892790794373, 0.002898400416597724]
[2025-05-12 11:05:27,961]: Mean: -0.00065846
[2025-05-12 11:05:27,962]: Min: -0.13904835
[2025-05-12 11:05:27,967]: Max: 0.16238055
[2025-05-12 11:05:27,967]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:05:27,971]: Sample Values (25 elements): [0.9570817947387695, 0.9645057916641235, 0.9828509092330933, 0.9483150243759155, 0.9779830574989319, 0.9660223722457886, 0.9639737010002136, 0.969115138053894, 0.9888737201690674, 0.9572615623474121, 0.9461897611618042, 0.9960842728614807, 0.935609757900238, 0.9840537905693054, 0.921978235244751, 0.9705373644828796, 0.967630922794342, 0.9793229103088379, 0.9809253811836243, 0.9309807419776917, 0.9997647404670715, 1.0043078660964966, 0.9467435479164124, 0.9975130558013916, 0.9938456416130066]
[2025-05-12 11:05:27,972]: Mean: 0.97146326
[2025-05-12 11:05:27,974]: Min: 0.92197824
[2025-05-12 11:05:27,975]: Max: 1.03591883
[2025-05-12 11:05:27,975]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:05:27,978]: Sample Values (25 elements): [0.042005155235528946, 0.012622392736375332, -0.004337322898209095, 0.08407822996377945, -0.03748469799757004, 0.0018131033284589648, 0.00620998116210103, -0.06792522221803665, -0.024641983211040497, 0.0415988489985466, -0.0009241301449947059, -0.011519786901772022, 0.05007486045360565, -0.021111559122800827, -0.03742014616727829, 0.07240502536296844, -0.02203109674155712, -0.017158882692456245, -0.04732843115925789, -0.06640259921550751, 0.01959097385406494, 0.01805744878947735, 0.052465613931417465, 0.037707965821027756, -0.04527319595217705]
[2025-05-12 11:05:27,980]: Mean: -0.00099471
[2025-05-12 11:05:27,985]: Min: -0.13249341
[2025-05-12 11:05:27,985]: Max: 0.14188699
[2025-05-12 11:05:27,986]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:05:27,989]: Sample Values (25 elements): [1.0092278718948364, 1.0182865858078003, 0.9972540140151978, 0.983253538608551, 1.0230156183242798, 0.9865583181381226, 0.9471346139907837, 0.964625895023346, 0.9648975133895874, 0.9017069935798645, 0.9920593500137329, 1.0102581977844238, 1.0244685411453247, 0.9442722797393799, 0.9953306317329407, 0.9495878219604492, 1.0094780921936035, 0.9780310392379761, 0.9618037939071655, 0.9479343295097351, 1.0194306373596191, 0.993669867515564, 1.0262678861618042, 0.9994835257530212, 0.9995121359825134]
[2025-05-12 11:05:28,005]: Mean: 0.98006397
[2025-05-12 11:05:28,011]: Min: 0.89896679
[2025-05-12 11:05:28,011]: Max: 1.02626789
[2025-05-12 11:05:28,012]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:05:28,012]: Sample Values (25 elements): [0.03427380323410034, -0.027514588087797165, 0.036934398114681244, -0.015283928252756596, -0.02239157073199749, -0.0473908931016922, 0.02876918762922287, 0.059846051037311554, -0.01205936074256897, -0.03284650668501854, 0.03603099286556244, 0.0630868673324585, 0.06683216989040375, 0.03612404316663742, 0.05255495384335518, 0.03084094449877739, 0.018132418394088745, 0.014903624542057514, 0.035862233489751816, 0.009736067615449429, -0.03435050696134567, -0.04929183050990105, -0.06250791996717453, -0.033391714096069336, -0.07422827184200287]
[2025-05-12 11:05:28,013]: Mean: -0.00164544
[2025-05-12 11:05:28,013]: Min: -0.12818266
[2025-05-12 11:05:28,013]: Max: 0.11665729
[2025-05-12 11:05:28,013]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:05:28,023]: Sample Values (25 elements): [0.9855061173439026, 0.9736809730529785, 0.9756616950035095, 0.9833142161369324, 0.9440253973007202, 0.9695380330085754, 0.9825120568275452, 0.9472293853759766, 0.9540855884552002, 0.9553167223930359, 1.0007740259170532, 0.9695165157318115, 0.9774529337882996, 0.949130117893219, 0.9564632177352905, 0.9860637784004211, 0.9679075479507446, 0.9640095829963684, 0.9647329449653625, 0.9807966947555542, 0.9929925203323364, 0.9656550288200378, 0.9853280782699585, 0.9571728706359863, 0.97173011302948]
[2025-05-12 11:05:28,034]: Mean: 0.97169065
[2025-05-12 11:05:28,036]: Min: 0.94402540
[2025-05-12 11:05:28,038]: Max: 1.00077403
[2025-05-12 11:05:28,038]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:05:28,041]: Sample Values (25 elements): [0.0036984996404498816, -0.022047152742743492, -0.04818189516663551, 0.061755117028951645, 0.04377082735300064, 0.030479537323117256, 0.025651635602116585, -0.020549152046442032, 0.011650769039988518, 0.0286406222730875, -0.008187741972506046, -0.0161136407405138, -0.00594928115606308, -0.02247699163854122, -0.04870763048529625, 0.04118472337722778, -0.02153453789651394, 0.01297343336045742, 0.005625379737466574, -0.022133396938443184, -0.007591424509882927, 0.0068350802175700665, -0.08628307282924652, 0.03450881317257881, -0.005691751837730408]
[2025-05-12 11:05:28,044]: Mean: 0.00057949
[2025-05-12 11:05:28,048]: Min: -0.11535227
[2025-05-12 11:05:28,049]: Max: 0.11690252
[2025-05-12 11:05:28,049]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:05:28,062]: Sample Values (25 elements): [0.9933385848999023, 1.0055458545684814, 1.028727412223816, 1.0036625862121582, 0.9971937537193298, 0.985233724117279, 1.0174572467803955, 0.989132821559906, 0.9966281056404114, 1.026810646057129, 0.9665327072143555, 0.975551426410675, 0.9940811395645142, 0.991356372833252, 0.9628710746765137, 1.002806544303894, 0.9585106372833252, 1.0139003992080688, 1.023871898651123, 0.9779974818229675, 0.976018488407135, 0.988950788974762, 0.9880562424659729, 0.9658844470977783, 0.9880427718162537]
[2025-05-12 11:05:28,070]: Mean: 0.99397254
[2025-05-12 11:05:28,078]: Min: 0.95851064
[2025-05-12 11:05:28,094]: Max: 1.03126013
[2025-05-12 11:05:28,094]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 11:05:28,100]: Sample Values (25 elements): [0.02532297931611538, 0.0033426957670599222, 0.05019958317279816, 0.03229409456253052, 0.03575257584452629, -0.005542153026908636, 0.04984119534492493, -0.004806444980204105, -0.07075818628072739, -0.051652733236551285, -0.01996743306517601, -0.015215348452329636, -0.0035037428606301546, -0.021273881196975708, 0.02535795234143734, -0.02109989896416664, -0.061364661902189255, -0.034998297691345215, -0.03176721930503845, -0.053362611681222916, -0.01648065634071827, 0.05649988725781441, -0.05157090350985527, -0.03183677792549133, 0.06939973682165146]
[2025-05-12 11:05:28,100]: Mean: 0.00032836
[2025-05-12 11:05:28,101]: Min: -0.11161965
[2025-05-12 11:05:28,101]: Max: 0.11574600
[2025-05-12 11:05:28,101]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,101]: Sample Values (25 elements): [0.9597347378730774, 0.9642203450202942, 1.0078129768371582, 0.9513673782348633, 0.9698841571807861, 0.9577082395553589, 0.984537661075592, 0.9856100678443909, 0.9650125503540039, 0.976840615272522, 0.9926304817199707, 0.9596152901649475, 0.9599635004997253, 0.9640181660652161, 1.0012125968933105, 0.9474722743034363, 0.9869967699050903, 0.9659631252288818, 0.9804617762565613, 0.9606460332870483, 0.9519876837730408, 0.9984710216522217, 0.9654675722122192, 0.9610590934753418, 0.9723965525627136]
[2025-05-12 11:05:28,102]: Mean: 0.97157156
[2025-05-12 11:05:28,102]: Min: 0.94747227
[2025-05-12 11:05:28,103]: Max: 1.00781298
[2025-05-12 11:05:28,103]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:05:28,110]: Sample Values (25 elements): [0.0019828088115900755, -0.03455779701471329, -0.02349817380309105, -0.0016477886820212007, 0.047868307679891586, -0.021921668201684952, 0.01778106391429901, -0.046292103826999664, 0.007926367223262787, -0.019651660695672035, -0.041263796389102936, -0.007198335137218237, 0.0011281594634056091, 0.02160697430372238, 0.02428443357348442, -0.01233404316008091, -0.019429607316851616, 0.003953178878873587, -0.009792309254407883, 0.011088701896369457, 0.018107471987605095, -0.015352501533925533, -0.009834576398134232, -0.007324547506868839, 0.020969465374946594]
[2025-05-12 11:05:28,112]: Mean: -0.00072650
[2025-05-12 11:05:28,115]: Min: -0.09486002
[2025-05-12 11:05:28,116]: Max: 0.10117839
[2025-05-12 11:05:28,117]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,119]: Sample Values (25 elements): [1.0115333795547485, 0.973920464515686, 0.9901326298713684, 1.0282328128814697, 1.0096231698989868, 1.0074149370193481, 1.0081911087036133, 0.9823876023292542, 1.0581283569335938, 0.9848083257675171, 1.0069539546966553, 1.0034414529800415, 1.0070371627807617, 0.9988895058631897, 1.0069596767425537, 1.0042455196380615, 0.9850828647613525, 1.012048363685608, 1.0039687156677246, 1.0010021924972534, 1.006117820739746, 1.0067403316497803, 0.9975500106811523, 1.017491102218628, 1.0155029296875]
[2025-05-12 11:05:28,120]: Mean: 1.00531816
[2025-05-12 11:05:28,122]: Min: 0.96408635
[2025-05-12 11:05:28,124]: Max: 1.05812836
[2025-05-12 11:05:28,124]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 11:05:28,130]: Sample Values (25 elements): [-0.0640578493475914, 0.07479462772607803, -0.12536028027534485, 0.04025912284851074, 0.036851875483989716, 0.044575802981853485, 0.05354731157422066, -0.0262207742780447, 0.10831476747989655, 0.04589148238301277, 0.19857138395309448, 0.06801474094390869, -0.03635713458061218, 0.13107623159885406, -0.038514889776706696, -0.0637565478682518, 0.0339944027364254, 0.12377004325389862, -0.06288601458072662, 0.12662069499492645, 0.03191496804356575, 0.05311998352408409, -0.14475759863853455, 0.14157630503177643, 0.15142442286014557]
[2025-05-12 11:05:28,138]: Mean: 0.00155670
[2025-05-12 11:05:28,138]: Min: -0.20240621
[2025-05-12 11:05:28,150]: Max: 0.22171742
[2025-05-12 11:05:28,150]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,157]: Sample Values (25 elements): [0.9010459184646606, 0.9465607404708862, 0.9383257031440735, 0.9272127747535706, 0.9370661377906799, 0.940202534198761, 0.9521276950836182, 0.9319652915000916, 0.9465441107749939, 0.9294629693031311, 0.9484407901763916, 0.9359354376792908, 0.9627918004989624, 0.9366710782051086, 0.9380616545677185, 0.9301420450210571, 0.9528124928474426, 0.9270758628845215, 0.9569934606552124, 0.9236426949501038, 0.9438292980194092, 0.938404381275177, 0.9146831035614014, 0.935524582862854, 0.9428192377090454]
[2025-05-12 11:05:28,157]: Mean: 0.93502891
[2025-05-12 11:05:28,158]: Min: 0.89137560
[2025-05-12 11:05:28,158]: Max: 0.97278446
[2025-05-12 11:05:28,158]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:05:28,161]: Sample Values (25 elements): [0.024505769833922386, -0.004763205535709858, -0.021168828010559082, -0.001880971365608275, -0.03933294117450714, -0.0316617526113987, -0.016736062243580818, -0.03296303004026413, -0.02516315132379532, -0.0477360300719738, -0.03712627664208412, 0.0016496774042025208, 0.017215847969055176, 0.02545415423810482, 0.0011669483501464128, 0.04933636635541916, 0.00658121332526207, 0.012527508661150932, -0.016872873529791832, -0.028186948969960213, 0.015835560858249664, 0.017438089475035667, -0.015722520649433136, -0.004705824889242649, 0.004362219944596291]
[2025-05-12 11:05:28,166]: Mean: -0.00073494
[2025-05-12 11:05:28,167]: Min: -0.09670923
[2025-05-12 11:05:28,168]: Max: 0.09786256
[2025-05-12 11:05:28,168]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,173]: Sample Values (25 elements): [0.9781590104103088, 0.9611482620239258, 0.9662331938743591, 0.975788414478302, 0.9624235033988953, 0.9610795378684998, 0.9660535454750061, 0.9733690023422241, 0.9868221879005432, 0.9868056774139404, 0.9738014340400696, 0.9724204540252686, 0.9574354290962219, 0.9546166658401489, 0.962989330291748, 0.9718424677848816, 0.9834166765213013, 0.9598636031150818, 0.9967861175537109, 0.95997554063797, 0.9704294204711914, 0.9785062670707703, 0.9926931262016296, 0.9680833220481873, 0.973103940486908]
[2025-05-12 11:05:28,174]: Mean: 0.97133887
[2025-05-12 11:05:28,176]: Min: 0.94696856
[2025-05-12 11:05:28,177]: Max: 0.99804890
[2025-05-12 11:05:28,177]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:05:28,183]: Sample Values (25 elements): [0.042397454380989075, -0.03127002716064453, -0.02348131127655506, -0.022331714630126953, -0.02865372784435749, -0.034021198749542236, 0.012073135003447533, 0.011201226152479649, 0.04552425071597099, 0.014626722782850266, -0.0052363211289048195, 0.04660078510642052, -0.012293430045247078, -0.03390960395336151, 0.005311840679496527, -0.021904412657022476, -0.013944066129624844, -0.012713541276752949, 0.00458928570151329, 0.013280455954372883, -0.047566045075654984, -0.027170591056346893, -0.01882258802652359, 0.013832947239279747, 0.032731182873249054]
[2025-05-12 11:05:28,184]: Mean: 0.00035254
[2025-05-12 11:05:28,185]: Min: -0.08299042
[2025-05-12 11:05:28,186]: Max: 0.08793408
[2025-05-12 11:05:28,186]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,190]: Sample Values (25 elements): [1.0057803392410278, 1.0177010297775269, 1.031974196434021, 1.0112884044647217, 1.0314199924468994, 1.0381699800491333, 1.033387303352356, 1.0353105068206787, 1.0252760648727417, 1.0251731872558594, 1.0259861946105957, 1.0073037147521973, 1.0070288181304932, 1.0210371017456055, 1.0202192068099976, 1.0278652906417847, 1.0316822528839111, 1.0246309041976929, 1.0178791284561157, 1.0307247638702393, 0.9985826015472412, 1.0043271780014038, 1.0220612287521362, 1.024420976638794, 1.0488669872283936]
[2025-05-12 11:05:28,197]: Mean: 1.02227366
[2025-05-12 11:05:28,201]: Min: 0.98997474
[2025-05-12 11:05:28,203]: Max: 1.06698811
[2025-05-12 11:05:28,203]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:05:28,225]: Sample Values (25 elements): [-0.03007294237613678, -0.0011791997822001576, 0.022673772647976875, -0.03833690658211708, 0.009952280670404434, 0.016420915722846985, -0.0020867343991994858, -0.018343212082982063, -0.0015960108721628785, 0.032745759934186935, -0.00856739841401577, 0.03039039857685566, 0.003801099956035614, 0.03709308058023453, 0.048636097460985184, 0.0007211757474578917, -0.002353146905079484, 0.013276096433401108, -0.004267450422048569, 0.012883090414106846, 0.013170391321182251, -0.014390305615961552, -0.03994578495621681, -0.013917304575443268, -0.0163728054612875]
[2025-05-12 11:05:28,226]: Mean: -0.00075867
[2025-05-12 11:05:28,230]: Min: -0.08220493
[2025-05-12 11:05:28,245]: Max: 0.08083215
[2025-05-12 11:05:28,245]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,248]: Sample Values (25 elements): [0.9797641038894653, 0.9615703821182251, 0.9656795263290405, 0.9745582938194275, 0.9522634744644165, 0.9759594202041626, 0.9609909057617188, 0.9730408787727356, 0.96310955286026, 0.9920079112052917, 0.9654351472854614, 0.9688403010368347, 0.9701508283615112, 0.9866403341293335, 0.974037766456604, 0.9789637923240662, 0.9793083667755127, 0.9639291167259216, 0.9665294289588928, 0.9671114087104797, 0.9697061777114868, 0.9770506620407104, 0.9990110397338867, 0.9591658115386963, 0.9627606272697449]
[2025-05-12 11:05:28,249]: Mean: 0.97159123
[2025-05-12 11:05:28,250]: Min: 0.95226347
[2025-05-12 11:05:28,251]: Max: 0.99901104
[2025-05-12 11:05:28,251]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:05:28,260]: Sample Values (25 elements): [-0.018984388560056686, 0.014322969131171703, -0.004850935656577349, 0.0011145095340907574, -0.04306865856051445, -0.02859971858561039, -0.029617425054311752, 0.0435318797826767, 0.010717806406319141, -0.02604193054139614, -0.025780657306313515, 0.023083548992872238, -0.03011132776737213, 0.023380031809210777, 0.0007746414048597217, -0.04246973246335983, -0.00551673024892807, -0.006858163513243198, -0.008117049932479858, -0.04075324907898903, 0.0018747575813904405, -0.0014740675687789917, -0.025962332263588905, -0.012489354237914085, -0.00717938132584095]
[2025-05-12 11:05:28,264]: Mean: -0.00002776
[2025-05-12 11:05:28,270]: Min: -0.06680338
[2025-05-12 11:05:28,271]: Max: 0.07352764
[2025-05-12 11:05:28,271]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:05:28,292]: Sample Values (25 elements): [1.0269274711608887, 1.0642296075820923, 1.0464656352996826, 1.0893974304199219, 1.059775948524475, 1.045703649520874, 1.0393967628479004, 1.0472241640090942, 1.0425554513931274, 1.0487176179885864, 1.0786168575286865, 1.0554991960525513, 1.0406925678253174, 1.042291522026062, 1.048006534576416, 1.0493290424346924, 1.051931619644165, 1.0892608165740967, 1.069117546081543, 1.0679047107696533, 1.0472532510757446, 1.1033453941345215, 1.0834708213806152, 1.0902531147003174, 1.0689830780029297]
[2025-05-12 11:05:28,293]: Mean: 1.05767834
[2025-05-12 11:05:28,293]: Min: 1.01734889
[2025-05-12 11:05:28,294]: Max: 1.10334539
[2025-05-12 11:05:28,295]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 11:05:28,303]: Sample Values (25 elements): [0.12727656960487366, -0.3814055323600769, 0.1723077893257141, -0.16251637041568756, -0.05527644604444504, -0.21799950301647186, 0.17024299502372742, 0.2326502650976181, -0.12404131889343262, -0.03539256751537323, -0.16263431310653687, -0.09353596717119217, -0.053518638014793396, -0.28159335255622864, -0.2446291744709015, -0.19765082001686096, -0.10388380289077759, -0.12397821247577667, -0.13875724375247955, -0.272879421710968, 0.1558971107006073, 0.2806193232536316, -0.15041427314281464, -0.1343243271112442, 0.2226959764957428]
[2025-05-12 11:05:28,305]: Mean: -0.00364517
[2025-05-12 11:05:28,307]: Min: -0.38140553
[2025-05-12 11:05:28,308]: Max: 0.55274087
[2025-05-12 11:05:28,308]: 


QAT of ResNet20 with relu6 down to 4 bits...
[2025-05-12 11:05:29,718]: [ResNet20_relu6_quantized_4_bits] after configure_qat:
[2025-05-12 11:05:30,304]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:08:10,338]: [ResNet20_relu6_quantized_4_bits] Epoch: 001 Train Loss: 0.4071 Train Acc: 0.8572 Eval Loss: 0.5948 Eval Acc: 0.8155 (LR: 0.001000)
[2025-05-12 11:10:15,683]: [ResNet20_relu6_quantized_4_bits] Epoch: 002 Train Loss: 0.3997 Train Acc: 0.8582 Eval Loss: 0.5002 Eval Acc: 0.8351 (LR: 0.001000)
[2025-05-12 11:12:27,508]: [ResNet20_relu6_quantized_4_bits] Epoch: 003 Train Loss: 0.4034 Train Acc: 0.8593 Eval Loss: 0.5438 Eval Acc: 0.8190 (LR: 0.001000)
[2025-05-12 11:14:45,353]: [ResNet20_relu6_quantized_4_bits] Epoch: 004 Train Loss: 0.3944 Train Acc: 0.8610 Eval Loss: 0.5430 Eval Acc: 0.8248 (LR: 0.001000)
[2025-05-12 11:17:02,094]: [ResNet20_relu6_quantized_4_bits] Epoch: 005 Train Loss: 0.3913 Train Acc: 0.8622 Eval Loss: 0.5168 Eval Acc: 0.8314 (LR: 0.001000)
[2025-05-12 11:19:17,315]: [ResNet20_relu6_quantized_4_bits] Epoch: 006 Train Loss: 0.3880 Train Acc: 0.8649 Eval Loss: 0.4994 Eval Acc: 0.8412 (LR: 0.001000)
[2025-05-12 11:21:34,431]: [ResNet20_relu6_quantized_4_bits] Epoch: 007 Train Loss: 0.3898 Train Acc: 0.8637 Eval Loss: 0.6284 Eval Acc: 0.8051 (LR: 0.001000)
[2025-05-12 11:23:44,919]: [ResNet20_relu6_quantized_4_bits] Epoch: 008 Train Loss: 0.3863 Train Acc: 0.8638 Eval Loss: 0.5045 Eval Acc: 0.8335 (LR: 0.001000)
[2025-05-12 11:26:02,633]: [ResNet20_relu6_quantized_4_bits] Epoch: 009 Train Loss: 0.3855 Train Acc: 0.8641 Eval Loss: 0.4875 Eval Acc: 0.8401 (LR: 0.001000)
[2025-05-12 11:28:18,779]: [ResNet20_relu6_quantized_4_bits] Epoch: 010 Train Loss: 0.3805 Train Acc: 0.8669 Eval Loss: 0.4850 Eval Acc: 0.8452 (LR: 0.001000)
[2025-05-12 11:30:32,757]: [ResNet20_relu6_quantized_4_bits] Epoch: 011 Train Loss: 0.3764 Train Acc: 0.8678 Eval Loss: 0.4548 Eval Acc: 0.8525 (LR: 0.001000)
[2025-05-12 11:32:48,339]: [ResNet20_relu6_quantized_4_bits] Epoch: 012 Train Loss: 0.3731 Train Acc: 0.8682 Eval Loss: 0.5893 Eval Acc: 0.8096 (LR: 0.001000)
[2025-05-12 11:35:00,995]: [ResNet20_relu6_quantized_4_bits] Epoch: 013 Train Loss: 0.3727 Train Acc: 0.8695 Eval Loss: 0.5386 Eval Acc: 0.8266 (LR: 0.001000)
[2025-05-12 11:37:19,160]: [ResNet20_relu6_quantized_4_bits] Epoch: 014 Train Loss: 0.3748 Train Acc: 0.8676 Eval Loss: 0.4476 Eval Acc: 0.8520 (LR: 0.001000)
[2025-05-12 11:39:36,605]: [ResNet20_relu6_quantized_4_bits] Epoch: 015 Train Loss: 0.3676 Train Acc: 0.8696 Eval Loss: 0.6114 Eval Acc: 0.8141 (LR: 0.001000)
[2025-05-12 11:41:52,526]: [ResNet20_relu6_quantized_4_bits] Epoch: 016 Train Loss: 0.3675 Train Acc: 0.8703 Eval Loss: 0.4880 Eval Acc: 0.8406 (LR: 0.001000)
[2025-05-12 11:43:44,280]: [ResNet20_relu6_quantized_4_bits] Epoch: 017 Train Loss: 0.3648 Train Acc: 0.8723 Eval Loss: 0.5146 Eval Acc: 0.8310 (LR: 0.001000)
[2025-05-12 11:45:36,157]: [ResNet20_relu6_quantized_4_bits] Epoch: 018 Train Loss: 0.3605 Train Acc: 0.8741 Eval Loss: 0.5404 Eval Acc: 0.8244 (LR: 0.001000)
[2025-05-12 11:47:26,510]: [ResNet20_relu6_quantized_4_bits] Epoch: 019 Train Loss: 0.3626 Train Acc: 0.8713 Eval Loss: 0.4665 Eval Acc: 0.8515 (LR: 0.001000)
[2025-05-12 11:49:14,848]: [ResNet20_relu6_quantized_4_bits] Epoch: 020 Train Loss: 0.3546 Train Acc: 0.8766 Eval Loss: 0.4698 Eval Acc: 0.8478 (LR: 0.001000)
[2025-05-12 11:51:01,150]: [ResNet20_relu6_quantized_4_bits] Epoch: 021 Train Loss: 0.3536 Train Acc: 0.8766 Eval Loss: 0.4508 Eval Acc: 0.8521 (LR: 0.001000)
[2025-05-12 11:52:46,246]: [ResNet20_relu6_quantized_4_bits] Epoch: 022 Train Loss: 0.3562 Train Acc: 0.8755 Eval Loss: 0.4380 Eval Acc: 0.8494 (LR: 0.001000)
[2025-05-12 11:54:30,564]: [ResNet20_relu6_quantized_4_bits] Epoch: 023 Train Loss: 0.3550 Train Acc: 0.8755 Eval Loss: 0.4910 Eval Acc: 0.8411 (LR: 0.001000)
[2025-05-12 11:56:15,646]: [ResNet20_relu6_quantized_4_bits] Epoch: 024 Train Loss: 0.3523 Train Acc: 0.8762 Eval Loss: 0.4732 Eval Acc: 0.8474 (LR: 0.001000)
[2025-05-12 11:58:00,416]: [ResNet20_relu6_quantized_4_bits] Epoch: 025 Train Loss: 0.3479 Train Acc: 0.8768 Eval Loss: 0.5391 Eval Acc: 0.8298 (LR: 0.001000)
[2025-05-12 11:59:48,483]: [ResNet20_relu6_quantized_4_bits] Epoch: 026 Train Loss: 0.3490 Train Acc: 0.8764 Eval Loss: 0.4837 Eval Acc: 0.8449 (LR: 0.001000)
[2025-05-12 12:01:37,312]: [ResNet20_relu6_quantized_4_bits] Epoch: 027 Train Loss: 0.3415 Train Acc: 0.8812 Eval Loss: 0.4914 Eval Acc: 0.8461 (LR: 0.001000)
[2025-05-12 12:03:28,848]: [ResNet20_relu6_quantized_4_bits] Epoch: 028 Train Loss: 0.3431 Train Acc: 0.8807 Eval Loss: 0.4925 Eval Acc: 0.8432 (LR: 0.001000)
[2025-05-12 12:05:19,764]: [ResNet20_relu6_quantized_4_bits] Epoch: 029 Train Loss: 0.3408 Train Acc: 0.8809 Eval Loss: 0.4497 Eval Acc: 0.8526 (LR: 0.001000)
[2025-05-12 12:07:10,033]: [ResNet20_relu6_quantized_4_bits] Epoch: 030 Train Loss: 0.3438 Train Acc: 0.8792 Eval Loss: 0.4806 Eval Acc: 0.8439 (LR: 0.000250)
[2025-05-12 12:08:51,176]: [ResNet20_relu6_quantized_4_bits] Epoch: 031 Train Loss: 0.3009 Train Acc: 0.8959 Eval Loss: 0.4140 Eval Acc: 0.8660 (LR: 0.000250)
[2025-05-12 12:10:30,530]: [ResNet20_relu6_quantized_4_bits] Epoch: 032 Train Loss: 0.2893 Train Acc: 0.9005 Eval Loss: 0.3973 Eval Acc: 0.8661 (LR: 0.000250)
[2025-05-12 12:12:10,077]: [ResNet20_relu6_quantized_4_bits] Epoch: 033 Train Loss: 0.2892 Train Acc: 0.8993 Eval Loss: 0.3927 Eval Acc: 0.8706 (LR: 0.000250)
[2025-05-12 12:13:52,533]: [ResNet20_relu6_quantized_4_bits] Epoch: 034 Train Loss: 0.2856 Train Acc: 0.8990 Eval Loss: 0.4007 Eval Acc: 0.8687 (LR: 0.000250)
[2025-05-12 12:15:37,896]: [ResNet20_relu6_quantized_4_bits] Epoch: 035 Train Loss: 0.2893 Train Acc: 0.8977 Eval Loss: 0.3887 Eval Acc: 0.8736 (LR: 0.000250)
[2025-05-12 12:17:24,005]: [ResNet20_relu6_quantized_4_bits] Epoch: 036 Train Loss: 0.2850 Train Acc: 0.9009 Eval Loss: 0.4127 Eval Acc: 0.8691 (LR: 0.000250)
[2025-05-12 12:19:12,661]: [ResNet20_relu6_quantized_4_bits] Epoch: 037 Train Loss: 0.2847 Train Acc: 0.9009 Eval Loss: 0.3969 Eval Acc: 0.8716 (LR: 0.000250)
[2025-05-12 12:21:00,805]: [ResNet20_relu6_quantized_4_bits] Epoch: 038 Train Loss: 0.2868 Train Acc: 0.8986 Eval Loss: 0.4087 Eval Acc: 0.8684 (LR: 0.000250)
[2025-05-12 12:22:48,528]: [ResNet20_relu6_quantized_4_bits] Epoch: 039 Train Loss: 0.2832 Train Acc: 0.9005 Eval Loss: 0.4015 Eval Acc: 0.8724 (LR: 0.000250)
[2025-05-12 12:24:35,402]: [ResNet20_relu6_quantized_4_bits] Epoch: 040 Train Loss: 0.2865 Train Acc: 0.8998 Eval Loss: 0.4060 Eval Acc: 0.8695 (LR: 0.000250)
[2025-05-12 12:26:19,429]: [ResNet20_relu6_quantized_4_bits] Epoch: 041 Train Loss: 0.2834 Train Acc: 0.9014 Eval Loss: 0.4060 Eval Acc: 0.8694 (LR: 0.000250)
[2025-05-12 12:28:03,995]: [ResNet20_relu6_quantized_4_bits] Epoch: 042 Train Loss: 0.2811 Train Acc: 0.9014 Eval Loss: 0.4129 Eval Acc: 0.8647 (LR: 0.000250)
[2025-05-12 12:29:47,700]: [ResNet20_relu6_quantized_4_bits] Epoch: 043 Train Loss: 0.2798 Train Acc: 0.9020 Eval Loss: 0.4043 Eval Acc: 0.8704 (LR: 0.000250)
[2025-05-12 12:31:28,839]: [ResNet20_relu6_quantized_4_bits] Epoch: 044 Train Loss: 0.2844 Train Acc: 0.9004 Eval Loss: 0.4001 Eval Acc: 0.8702 (LR: 0.000250)
[2025-05-12 12:33:12,474]: [ResNet20_relu6_quantized_4_bits] Epoch: 045 Train Loss: 0.2813 Train Acc: 0.9020 Eval Loss: 0.3992 Eval Acc: 0.8732 (LR: 0.000063)
[2025-05-12 12:34:57,477]: [ResNet20_relu6_quantized_4_bits] Epoch: 046 Train Loss: 0.2659 Train Acc: 0.9073 Eval Loss: 0.3821 Eval Acc: 0.8761 (LR: 0.000063)
[2025-05-12 12:36:48,785]: [ResNet20_relu6_quantized_4_bits] Epoch: 047 Train Loss: 0.2674 Train Acc: 0.9070 Eval Loss: 0.3792 Eval Acc: 0.8786 (LR: 0.000063)
[2025-05-12 12:38:42,822]: [ResNet20_relu6_quantized_4_bits] Epoch: 048 Train Loss: 0.2623 Train Acc: 0.9091 Eval Loss: 0.3897 Eval Acc: 0.8770 (LR: 0.000063)
[2025-05-12 12:40:37,813]: [ResNet20_relu6_quantized_4_bits] Epoch: 049 Train Loss: 0.2591 Train Acc: 0.9108 Eval Loss: 0.3858 Eval Acc: 0.8758 (LR: 0.000063)
[2025-05-12 12:42:32,548]: [ResNet20_relu6_quantized_4_bits] Epoch: 050 Train Loss: 0.2642 Train Acc: 0.9087 Eval Loss: 0.3942 Eval Acc: 0.8758 (LR: 0.000063)
[2025-05-12 12:44:27,788]: [ResNet20_relu6_quantized_4_bits] Epoch: 051 Train Loss: 0.2631 Train Acc: 0.9077 Eval Loss: 0.3891 Eval Acc: 0.8738 (LR: 0.000063)
[2025-05-12 12:46:22,696]: [ResNet20_relu6_quantized_4_bits] Epoch: 052 Train Loss: 0.2650 Train Acc: 0.9078 Eval Loss: 0.4010 Eval Acc: 0.8724 (LR: 0.000063)
[2025-05-12 12:48:18,833]: [ResNet20_relu6_quantized_4_bits] Epoch: 053 Train Loss: 0.2622 Train Acc: 0.9082 Eval Loss: 0.3897 Eval Acc: 0.8746 (LR: 0.000063)
[2025-05-12 12:50:15,434]: [ResNet20_relu6_quantized_4_bits] Epoch: 054 Train Loss: 0.2629 Train Acc: 0.9091 Eval Loss: 0.3938 Eval Acc: 0.8750 (LR: 0.000063)
[2025-05-12 12:52:10,898]: [ResNet20_relu6_quantized_4_bits] Epoch: 055 Train Loss: 0.2601 Train Acc: 0.9098 Eval Loss: 0.3876 Eval Acc: 0.8777 (LR: 0.000063)
[2025-05-12 12:54:04,146]: [ResNet20_relu6_quantized_4_bits] Epoch: 056 Train Loss: 0.2620 Train Acc: 0.9075 Eval Loss: 0.3913 Eval Acc: 0.8754 (LR: 0.000063)
[2025-05-12 12:55:53,574]: [ResNet20_relu6_quantized_4_bits] Epoch: 057 Train Loss: 0.2631 Train Acc: 0.9082 Eval Loss: 0.3889 Eval Acc: 0.8767 (LR: 0.000063)
[2025-05-12 12:57:37,114]: [ResNet20_relu6_quantized_4_bits] Epoch: 058 Train Loss: 0.2624 Train Acc: 0.9089 Eval Loss: 0.3930 Eval Acc: 0.8757 (LR: 0.000063)
[2025-05-12 12:59:14,652]: [ResNet20_relu6_quantized_4_bits] Epoch: 059 Train Loss: 0.2620 Train Acc: 0.9090 Eval Loss: 0.3878 Eval Acc: 0.8765 (LR: 0.000063)
[2025-05-12 13:00:53,803]: [ResNet20_relu6_quantized_4_bits] Epoch: 060 Train Loss: 0.2627 Train Acc: 0.9077 Eval Loss: 0.3970 Eval Acc: 0.8717 (LR: 0.000063)
[2025-05-12 13:00:53,804]: [ResNet20_relu6_quantized_4_bits] Best Eval Accuracy: 0.8786
[2025-05-12 13:00:53,863]: 


Quantization of model down to 4 bits finished
[2025-05-12 13:00:53,863]: Model Architecture:
[2025-05-12 13:00:54,091]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0372], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27895140647888184, max_val=0.27891626954078674)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0335], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28549763560295105, max_val=0.216904878616333)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0502], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4726775586605072, max_val=0.28069764375686646)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0336], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2778078615665436, max_val=0.22681847214698792)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0247], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18496717512607574, max_val=0.18494126200675964)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3897], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.845380783081055)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0239], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15612703561782837, max_val=0.20228701829910278)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0218], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1741221696138382, max_val=0.1526859700679779)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3517], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.275839328765869)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0227], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1494555026292801, max_val=0.19063220918178558)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3592624366283417, max_val=0.30832070112228394)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0240], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16198736429214478, max_val=0.19778409600257874)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3932], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.89858341217041)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0200], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15030546486377716, max_val=0.15010666847229004)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0182], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14545902609825134, max_val=0.12773026525974274)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3867], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.800175189971924)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0169], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12686340510845184, max_val=0.12709516286849976)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0164], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12259180098772049, max_val=0.12280401587486267)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3819], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.728611469268799)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0151], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10756010562181473, max_val=0.11823613941669464)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0294], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21025286614894867, max_val=0.23054425418376923)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0152], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11690761893987656, max_val=0.1104024201631546)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3993], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.989267349243164)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09332960098981857, max_val=0.09364531934261322)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0130], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09618478268384933, max_val=0.09831391274929047)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3946], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.918704032897949)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06793742626905441, max_val=0.07872914522886276)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:00:54,091]: 
Model Weights:
[2025-05-12 13:00:54,092]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 13:00:54,096]: Sample Values (25 elements): [0.04088549315929413, -0.01442924328148365, -0.1056126058101654, -0.13179151713848114, -0.16493535041809082, 0.02308029867708683, 0.08155068755149841, -0.06961026787757874, 0.10052051395177841, 0.07112478464841843, -0.12284471839666367, 0.2646043002605438, -0.24394109845161438, -0.09263881295919418, 0.17958509922027588, 0.044982049614191055, -0.17630693316459656, 0.23873864114284515, 0.01357194222509861, -0.07098665088415146, -0.39761918783187866, 0.20858979225158691, 0.05016305297613144, -0.1741938591003418, -0.2595709562301636]
[2025-05-12 13:00:54,101]: Mean: -0.00182563
[2025-05-12 13:00:54,109]: Min: -0.45357603
[2025-05-12 13:00:54,113]: Max: 0.53834188
[2025-05-12 13:00:54,113]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,119]: Sample Values (16 elements): [1.0570497512817383, 1.019884705543518, 1.03936767578125, 1.0063968896865845, 0.8898484110832214, 1.051098108291626, 0.9055558443069458, 1.0416146516799927, 1.167688250541687, 1.0283491611480713, 0.9045971035957336, 1.111700415611267, 1.0418524742126465, 1.0074244737625122, 0.8371406197547913, 1.0780904293060303]
[2025-05-12 13:00:54,120]: Mean: 1.01172876
[2025-05-12 13:00:54,120]: Min: 0.83714062
[2025-05-12 13:00:54,120]: Max: 1.16768825
[2025-05-12 13:00:54,122]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:00:54,124]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.11157360672950745, -0.18595600128173828, -0.11157360672950745, 0.0, 0.037191201001405716, -0.037191201001405716, 0.07438240200281143, 0.11157360672950745, -0.11157360672950745, 0.0, -0.037191201001405716, 0.11157360672950745, 0.0, -0.037191201001405716, -0.07438240200281143, 0.037191201001405716, -0.037191201001405716, 0.07438240200281143, 0.037191201001405716, 0.0, 0.037191201001405716, -0.037191201001405716]
[2025-05-12 13:00:54,124]: Mean: -0.00734462
[2025-05-12 13:00:54,125]: Min: -0.29752961
[2025-05-12 13:00:54,125]: Max: 0.26033840
[2025-05-12 13:00:54,125]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,126]: Sample Values (16 elements): [0.8842479586601257, 1.047021508216858, 0.9154263734817505, 1.122248888015747, 0.9643096923828125, 1.3375539779663086, 0.9394497871398926, 0.9787446856498718, 0.9946354031562805, 0.834222674369812, 1.043212652206421, 1.0275665521621704, 1.0151692628860474, 0.934905469417572, 0.9274269342422485, 0.9543848037719727]
[2025-05-12 13:00:54,126]: Mean: 0.99503291
[2025-05-12 13:00:54,127]: Min: 0.83422267
[2025-05-12 13:00:54,127]: Max: 1.33755398
[2025-05-12 13:00:54,131]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:00:54,132]: Sample Values (25 elements): [0.033493489027023315, 0.06698697805404663, -0.10048046708106995, -0.10048046708106995, 0.0, -0.13397395610809326, -0.06698697805404663, -0.06698697805404663, -0.033493489027023315, -0.10048046708106995, 0.0, -0.06698697805404663, 0.033493489027023315, -0.16746744513511658, 0.033493489027023315, 0.0, 0.10048046708106995, -0.06698697805404663, -0.033493489027023315, -0.033493489027023315, 0.10048046708106995, -0.06698697805404663, -0.06698697805404663, 0.0, 0.06698697805404663]
[2025-05-12 13:00:54,132]: Mean: -0.00732670
[2025-05-12 13:00:54,132]: Min: -0.30144140
[2025-05-12 13:00:54,133]: Max: 0.20096093
[2025-05-12 13:00:54,133]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,135]: Sample Values (16 elements): [0.9710783362388611, 0.8853752017021179, 0.9970101118087769, 1.0895291566848755, 1.0611529350280762, 0.9504202604293823, 0.8527683615684509, 0.925471305847168, 0.9202157258987427, 0.9175456166267395, 1.0216747522354126, 0.9387441873550415, 0.9995964169502258, 1.004970669746399, 0.9530822038650513, 0.9527075290679932]
[2025-05-12 13:00:54,136]: Mean: 0.96508396
[2025-05-12 13:00:54,136]: Min: 0.85276836
[2025-05-12 13:00:54,137]: Max: 1.08952916
[2025-05-12 13:00:54,141]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:00:54,142]: Sample Values (25 elements): [-0.050225045531988144, -0.050225045531988144, -0.050225045531988144, 0.0, 0.15067513287067413, 0.050225045531988144, 0.10045009106397629, 0.0, 0.050225045531988144, 0.0, 0.15067513287067413, -0.050225045531988144, 0.050225045531988144, -0.050225045531988144, -0.050225045531988144, -0.050225045531988144, -0.10045009106397629, 0.050225045531988144, 0.10045009106397629, 0.0, -0.050225045531988144, 0.10045009106397629, 0.050225045531988144, 0.10045009106397629, 0.0]
[2025-05-12 13:00:54,142]: Mean: -0.00041418
[2025-05-12 13:00:54,143]: Min: -0.45202541
[2025-05-12 13:00:54,143]: Max: 0.30135027
[2025-05-12 13:00:54,143]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,144]: Sample Values (16 elements): [1.1932947635650635, 0.9610497951507568, 0.9508655071258545, 0.8586592078208923, 0.9292698502540588, 0.924623966217041, 0.9800894260406494, 1.0998772382736206, 0.9756388664245605, 0.9580352306365967, 0.9371753931045532, 1.1668939590454102, 1.0250775814056396, 1.0636550188064575, 0.9577053785324097, 0.8186426162719727]
[2025-05-12 13:00:54,145]: Mean: 0.98753458
[2025-05-12 13:00:54,146]: Min: 0.81864262
[2025-05-12 13:00:54,146]: Max: 1.19329476
[2025-05-12 13:00:54,149]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:00:54,149]: Sample Values (25 elements): [0.0, -0.06728348881006241, 0.033641744405031204, 0.033641744405031204, -0.10092523694038391, 0.0, -0.06728348881006241, 0.033641744405031204, 0.033641744405031204, 0.033641744405031204, -0.06728348881006241, 0.06728348881006241, 0.033641744405031204, -0.033641744405031204, 0.033641744405031204, -0.06728348881006241, 0.0, -0.10092523694038391, -0.06728348881006241, -0.033641744405031204, 0.033641744405031204, -0.06728348881006241, 0.0, 0.06728348881006241, 0.0]
[2025-05-12 13:00:54,150]: Mean: -0.00164996
[2025-05-12 13:00:54,150]: Min: -0.26913396
[2025-05-12 13:00:54,152]: Max: 0.23549221
[2025-05-12 13:00:54,152]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,152]: Sample Values (16 elements): [0.9631301760673523, 0.9124680757522583, 1.0247567892074585, 0.9513124823570251, 0.9751116633415222, 1.1810215711593628, 1.0038623809814453, 1.1151410341262817, 1.0143406391143799, 0.855210542678833, 0.9870769381523132, 1.0195515155792236, 0.8612892031669617, 0.9249100089073181, 0.9868640899658203, 0.8810811638832092]
[2025-05-12 13:00:54,153]: Mean: 0.97857046
[2025-05-12 13:00:54,153]: Min: 0.85521054
[2025-05-12 13:00:54,153]: Max: 1.18102157
[2025-05-12 13:00:54,155]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:00:54,157]: Sample Values (25 elements): [-0.09864235669374466, 0.04932117834687233, -0.024660589173436165, -0.024660589173436165, 0.0, -0.024660589173436165, 0.024660589173436165, 0.024660589173436165, -0.07398176938295364, -0.07398176938295364, 0.12330294400453568, 0.04932117834687233, 0.09864235669374466, -0.07398176938295364, -0.024660589173436165, 0.024660589173436165, 0.024660589173436165, 0.0, -0.024660589173436165, -0.024660589173436165, 0.0, -0.04932117834687233, 0.12330294400453568, 0.04932117834687233, -0.024660589173436165]
[2025-05-12 13:00:54,157]: Mean: -0.00238685
[2025-05-12 13:00:54,158]: Min: -0.17262413
[2025-05-12 13:00:54,158]: Max: 0.17262413
[2025-05-12 13:00:54,158]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,158]: Sample Values (16 elements): [0.9172365665435791, 0.9602555632591248, 0.9881430864334106, 0.9709815979003906, 0.991369366645813, 0.9226382970809937, 0.9945582151412964, 0.9809808731079102, 0.9598231911659241, 1.0403965711593628, 1.0181801319122314, 0.9328599572181702, 0.982390820980072, 0.9483793377876282, 0.9863232970237732, 0.9702467322349548]
[2025-05-12 13:00:54,159]: Mean: 0.97279775
[2025-05-12 13:00:54,159]: Min: 0.91723657
[2025-05-12 13:00:54,159]: Max: 1.04039657
[2025-05-12 13:00:54,161]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:00:54,162]: Sample Values (25 elements): [0.02389427088201046, 0.02389427088201046, -0.02389427088201046, 0.07168281078338623, 0.07168281078338623, 0.02389427088201046, -0.04778854176402092, 0.02389427088201046, -0.02389427088201046, 0.14336562156677246, 0.07168281078338623, -0.04778854176402092, -0.07168281078338623, -0.07168281078338623, 0.02389427088201046, 0.0, 0.11947135627269745, 0.0, -0.07168281078338623, 0.02389427088201046, 0.02389427088201046, 0.0, 0.02389427088201046, 0.0, 0.02389427088201046]
[2025-05-12 13:00:54,162]: Mean: 0.00538243
[2025-05-12 13:00:54,162]: Min: -0.16725990
[2025-05-12 13:00:54,163]: Max: 0.19115417
[2025-05-12 13:00:54,163]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:00:54,163]: Sample Values (16 elements): [0.9677526354789734, 0.9475710391998291, 0.9142130613327026, 0.8907267451286316, 0.947265625, 0.9509687423706055, 1.0385653972625732, 0.9771559834480286, 0.9658469557762146, 0.9910203218460083, 0.9283455610275269, 0.9375495910644531, 1.0056214332580566, 1.0673731565475464, 0.9134561419487, 1.0211445093154907]
[2025-05-12 13:00:54,163]: Mean: 0.96653605
[2025-05-12 13:00:54,164]: Min: 0.89072675
[2025-05-12 13:00:54,164]: Max: 1.06737316
[2025-05-12 13:00:54,166]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 13:00:54,167]: Sample Values (25 elements): [0.0, -0.021787215024232864, -0.021787215024232864, 0.0, -0.04357443004846573, 0.0, -0.04357443004846573, 0.021787215024232864, 0.021787215024232864, -0.021787215024232864, -0.021787215024232864, 0.021787215024232864, -0.021787215024232864, -0.08714886009693146, 0.04357443004846573, 0.0, 0.0, 0.021787215024232864, -0.08714886009693146, 0.0, -0.021787215024232864, 0.021787215024232864, -0.08714886009693146, -0.06536164879798889, -0.04357443004846573]
[2025-05-12 13:00:54,167]: Mean: -0.00242553
[2025-05-12 13:00:54,167]: Min: -0.17429772
[2025-05-12 13:00:54,168]: Max: 0.15251051
[2025-05-12 13:00:54,168]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,169]: Sample Values (25 elements): [1.0267798900604248, 1.0280488729476929, 0.9366285800933838, 0.9914907217025757, 0.9826371669769287, 0.956648051738739, 0.9777425527572632, 0.9496114253997803, 0.9709635972976685, 0.9263452887535095, 0.9686731100082397, 0.9817497730255127, 0.9785801768302917, 0.9162456393241882, 0.9785372018814087, 1.015799880027771, 0.9820184111595154, 0.9420315027236938, 0.9999504685401917, 0.9704055786132812, 0.9778214693069458, 0.919251561164856, 0.9384272694587708, 0.9998922348022461, 0.9759573340415955]
[2025-05-12 13:00:54,169]: Mean: 0.97129667
[2025-05-12 13:00:54,169]: Min: 0.91624564
[2025-05-12 13:00:54,170]: Max: 1.02804887
[2025-05-12 13:00:54,171]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:00:54,172]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.04534506797790527, -0.06801760196685791, -0.06801760196685791, -0.06801760196685791, -0.04534506797790527, 0.06801760196685791, -0.022672533988952637, -0.06801760196685791, -0.04534506797790527, 0.022672533988952637, 0.022672533988952637, 0.0, 0.09069013595581055, -0.06801760196685791, -0.04534506797790527, -0.04534506797790527, -0.022672533988952637, -0.022672533988952637, 0.022672533988952637, 0.0, 0.06801760196685791, -0.022672533988952637]
[2025-05-12 13:00:54,172]: Mean: -0.00295707
[2025-05-12 13:00:54,176]: Min: -0.15870774
[2025-05-12 13:00:54,176]: Max: 0.18138027
[2025-05-12 13:00:54,177]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,181]: Sample Values (25 elements): [0.9068658947944641, 0.9885222911834717, 1.033186435699463, 1.0049744844436646, 1.0104615688323975, 0.9756042957305908, 0.9705324769020081, 0.9915122389793396, 0.9677673578262329, 1.0188071727752686, 1.060752511024475, 1.0001381635665894, 0.9992142915725708, 0.9796184301376343, 1.0118236541748047, 1.0808441638946533, 1.0035037994384766, 1.0033645629882812, 0.9395884871482849, 0.9273203015327454, 1.0263378620147705, 1.0415846109390259, 0.984595000743866, 0.9734027981758118, 0.9902805685997009]
[2025-05-12 13:00:54,184]: Mean: 0.99199700
[2025-05-12 13:00:54,186]: Min: 0.90686589
[2025-05-12 13:00:54,191]: Max: 1.08084416
[2025-05-12 13:00:54,212]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 13:00:54,213]: Sample Values (25 elements): [0.04450550675392151, 0.08901101350784302, -0.13351652026176453, 0.17802202701568604, 0.08901101350784302, -0.13351652026176453, -0.04450550675392151, 0.0, 0.17802202701568604, -0.08901101350784302, 0.08901101350784302, 0.0, -0.17802202701568604, -0.22252753376960754, 0.04450550675392151, -0.08901101350784302, -0.13351652026176453, 0.0, -0.04450550675392151, -0.13351652026176453, 0.13351652026176453, -0.04450550675392151, 0.26703304052352905, 0.13351652026176453, -0.26703304052352905]
[2025-05-12 13:00:54,213]: Mean: -0.01416874
[2025-05-12 13:00:54,214]: Min: -0.35604405
[2025-05-12 13:00:54,214]: Max: 0.31153855
[2025-05-12 13:00:54,214]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,215]: Sample Values (25 elements): [0.8958911895751953, 0.8802450299263, 0.8651137351989746, 0.8618553876876831, 0.8295344114303589, 0.8353143334388733, 0.8864137530326843, 0.8374344110488892, 0.8735526204109192, 0.8518285155296326, 0.8823228478431702, 0.83820641040802, 0.8458941578865051, 0.8484677076339722, 0.8410190939903259, 0.836276650428772, 0.904964029788971, 0.8035774827003479, 0.8265559673309326, 0.8081953525543213, 0.8843557834625244, 0.8576364517211914, 0.9011310935020447, 0.8549928069114685, 0.8563186526298523]
[2025-05-12 13:00:54,215]: Mean: 0.86035478
[2025-05-12 13:00:54,216]: Min: 0.80357748
[2025-05-12 13:00:54,216]: Max: 0.93098849
[2025-05-12 13:00:54,219]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:00:54,220]: Sample Values (25 elements): [-0.07195430994033813, 0.07195430994033813, -0.023984769359230995, 0.0, 0.07195430994033813, -0.04796953871846199, 0.023984769359230995, -0.023984769359230995, 0.0, 0.023984769359230995, 0.04796953871846199, -0.023984769359230995, 0.04796953871846199, -0.07195430994033813, -0.07195430994033813, 0.0, 0.04796953871846199, -0.023984769359230995, -0.07195430994033813, -0.04796953871846199, 0.04796953871846199, -0.04796953871846199, -0.023984769359230995, 0.04796953871846199, 0.023984769359230995]
[2025-05-12 13:00:54,220]: Mean: -0.00084321
[2025-05-12 13:00:54,221]: Min: -0.16789338
[2025-05-12 13:00:54,221]: Max: 0.19187815
[2025-05-12 13:00:54,221]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,222]: Sample Values (25 elements): [0.9094295501708984, 0.9995350241661072, 0.9439218640327454, 0.9900813102722168, 0.9953656792640686, 0.9839931726455688, 0.9910321235656738, 0.9423935413360596, 0.9199636578559875, 0.9964113235473633, 0.9654090404510498, 0.9848842620849609, 0.9534611105918884, 0.9658421277999878, 0.946657657623291, 0.9692692756652832, 1.0340604782104492, 0.9477737545967102, 1.0488653182983398, 0.9739930629730225, 0.9490668773651123, 0.9919276833534241, 0.9173035621643066, 0.9732635021209717, 0.9353693127632141]
[2025-05-12 13:00:54,223]: Mean: 0.96932447
[2025-05-12 13:00:54,223]: Min: 0.90942955
[2025-05-12 13:00:54,224]: Max: 1.04886532
[2025-05-12 13:00:54,226]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:00:54,226]: Sample Values (25 elements): [0.0, 0.020027553662657738, -0.040055107325315475, 0.060082659125328064, -0.040055107325315475, 0.0, 0.0, 0.0, -0.020027553662657738, -0.020027553662657738, -0.08011021465063095, 0.020027553662657738, -0.040055107325315475, 0.0, 0.0, 0.040055107325315475, -0.020027553662657738, 0.0, 0.0, -0.020027553662657738, 0.060082659125328064, 0.020027553662657738, -0.040055107325315475, 0.08011021465063095, -0.020027553662657738]
[2025-05-12 13:00:54,227]: Mean: -0.00094314
[2025-05-12 13:00:54,227]: Min: -0.16022043
[2025-05-12 13:00:54,228]: Max: 0.14019288
[2025-05-12 13:00:54,228]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,228]: Sample Values (25 elements): [0.9572377800941467, 1.0254673957824707, 0.9694998264312744, 0.9338878393173218, 1.010335922241211, 0.914536714553833, 0.9735823273658752, 1.0097702741622925, 0.9532933235168457, 0.9674794673919678, 0.9800503253936768, 0.9316535592079163, 0.998448371887207, 0.9969490170478821, 0.9861047267913818, 1.0117244720458984, 1.044395923614502, 1.0188732147216797, 0.9249531626701355, 0.9644887447357178, 1.02884840965271, 0.8662447333335876, 0.9766860604286194, 1.0404913425445557, 0.8734341263771057]
[2025-05-12 13:00:54,229]: Mean: 0.97638673
[2025-05-12 13:00:54,229]: Min: 0.86624473
[2025-05-12 13:00:54,229]: Max: 1.04439592
[2025-05-12 13:00:54,232]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:00:54,233]: Sample Values (25 elements): [0.0, -0.07285059988498688, -0.01821264997124672, 0.07285059988498688, 0.0, 0.03642529994249344, 0.0, 0.05463794991374016, 0.0, 0.0, 0.01821264997124672, -0.03642529994249344, 0.01821264997124672, 0.01821264997124672, -0.01821264997124672, -0.05463794991374016, 0.0, 0.01821264997124672, -0.05463794991374016, 0.05463794991374016, -0.01821264997124672, 0.0, 0.0, 0.07285059988498688, -0.01821264997124672]
[2025-05-12 13:00:54,233]: Mean: -0.00196829
[2025-05-12 13:00:54,234]: Min: -0.14570120
[2025-05-12 13:00:54,234]: Max: 0.12748855
[2025-05-12 13:00:54,234]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,235]: Sample Values (25 elements): [0.9669763445854187, 0.9711620807647705, 0.9832178950309753, 0.9720526337623596, 0.976224958896637, 0.9687613248825073, 0.9731088280677795, 1.0104856491088867, 0.9786184430122375, 0.9586946368217468, 0.958469569683075, 0.9810466170310974, 0.9315559267997742, 0.924250602722168, 0.9512320756912231, 0.9785857796669006, 0.9529790282249451, 0.9348497986793518, 0.9983158111572266, 0.971961259841919, 0.9426930546760559, 0.9728795289993286, 0.9723559021949768, 0.9608873128890991, 0.9763681292533875]
[2025-05-12 13:00:54,236]: Mean: 0.96554464
[2025-05-12 13:00:54,236]: Min: 0.92425060
[2025-05-12 13:00:54,237]: Max: 1.01048565
[2025-05-12 13:00:54,239]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:00:54,240]: Sample Values (25 elements): [0.03386116027832031, 0.06772232055664062, -0.03386116027832031, 0.0, 0.0, 0.0, -0.016930580139160156, -0.08465290069580078, -0.03386116027832031, -0.016930580139160156, 0.05079174041748047, -0.016930580139160156, -0.06772232055664062, -0.05079174041748047, 0.0, -0.06772232055664062, 0.016930580139160156, -0.05079174041748047, 0.0, 0.0, 0.0, 0.016930580139160156, 0.06772232055664062, -0.06772232055664062, -0.016930580139160156]
[2025-05-12 13:00:54,241]: Mean: 0.00046662
[2025-05-12 13:00:54,241]: Min: -0.11851406
[2025-05-12 13:00:54,241]: Max: 0.13544464
[2025-05-12 13:00:54,241]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:00:54,242]: Sample Values (25 elements): [0.9712216258049011, 1.0300865173339844, 0.9704349040985107, 1.0091863870620728, 0.95489501953125, 0.9687743186950684, 0.9976840615272522, 0.9960455894470215, 0.9738443493843079, 0.9813030362129211, 1.008898138999939, 1.0432332754135132, 0.9793940782546997, 1.020691156387329, 0.9704514741897583, 0.940322995185852, 1.0001164674758911, 1.0417464971542358, 0.9808019399642944, 1.0098109245300293, 0.9708379507064819, 0.9723650217056274, 0.9909218549728394, 0.9760910868644714, 0.9643464684486389]
[2025-05-12 13:00:54,243]: Mean: 0.99310064
[2025-05-12 13:00:54,243]: Min: 0.94032300
[2025-05-12 13:00:54,243]: Max: 1.04323328
[2025-05-12 13:00:54,245]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 13:00:54,246]: Sample Values (25 elements): [0.0163597259670496, -0.049079179763793945, -0.0163597259670496, 0.0163597259670496, 0.0, -0.0163597259670496, 0.08179862797260284, 0.0, -0.0654389038681984, -0.0654389038681984, 0.0163597259670496, 0.049079179763793945, -0.0654389038681984, -0.0163597259670496, 0.0327194519340992, 0.0, 0.0, 0.0654389038681984, 0.0654389038681984, 0.0327194519340992, -0.0163597259670496, -0.0327194519340992, 0.0, -0.049079179763793945, 0.0163597259670496]
[2025-05-12 13:00:54,247]: Mean: 0.00042781
[2025-05-12 13:00:54,247]: Min: -0.11451808
[2025-05-12 13:00:54,247]: Max: 0.13087781
[2025-05-12 13:00:54,247]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,248]: Sample Values (25 elements): [0.9527546763420105, 0.964635968208313, 0.9772931337356567, 0.9514182209968567, 0.9712616801261902, 0.9630380868911743, 0.9818469285964966, 0.9782689809799194, 0.9816664457321167, 0.9703646898269653, 0.9474320411682129, 0.943890392780304, 1.0061614513397217, 0.9828550815582275, 0.9510724544525146, 0.9776514172554016, 0.9519118666648865, 0.9571226835250854, 0.9472497701644897, 1.005326509475708, 0.9862437844276428, 0.9697995781898499, 0.9484721422195435, 0.9460750818252563, 0.9967000484466553]
[2025-05-12 13:00:54,248]: Mean: 0.96599728
[2025-05-12 13:00:54,248]: Min: 0.93625689
[2025-05-12 13:00:54,249]: Max: 1.00944352
[2025-05-12 13:00:54,251]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:00:54,252]: Sample Values (25 elements): [0.015053101815283298, 0.030106203630566597, 0.030106203630566597, -0.030106203630566597, 0.015053101815283298, 0.0, 0.015053101815283298, -0.04515930637717247, -0.030106203630566597, 0.030106203630566597, -0.015053101815283298, -0.030106203630566597, -0.015053101815283298, 0.015053101815283298, -0.07526551187038422, 0.015053101815283298, -0.015053101815283298, 0.060212407261133194, -0.030106203630566597, -0.015053101815283298, -0.04515930637717247, -0.030106203630566597, 0.0, -0.015053101815283298, 0.0]
[2025-05-12 13:00:54,252]: Mean: -0.00088447
[2025-05-12 13:00:54,253]: Min: -0.10537171
[2025-05-12 13:00:54,254]: Max: 0.12042481
[2025-05-12 13:00:54,254]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,257]: Sample Values (25 elements): [1.0079079866409302, 0.9944782853126526, 1.0408616065979004, 1.020592451095581, 1.0066620111465454, 1.012670636177063, 1.0005462169647217, 1.023850440979004, 1.0174187421798706, 1.0204463005065918, 0.9980422854423523, 0.9842469692230225, 1.0500602722167969, 1.020761489868164, 1.0052233934402466, 1.0368491411209106, 0.986622154712677, 1.0111205577850342, 0.9605637788772583, 0.9734671115875244, 0.9975196123123169, 1.0379602909088135, 1.0002473592758179, 1.0073034763336182, 1.0119633674621582]
[2025-05-12 13:00:54,258]: Mean: 1.00952625
[2025-05-12 13:00:54,259]: Min: 0.94965613
[2025-05-12 13:00:54,259]: Max: 1.08405638
[2025-05-12 13:00:54,273]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 13:00:54,278]: Sample Values (25 elements): [-0.05877300351858139, 0.0, 0.0, 0.11754600703716278, 0.029386501759290695, -0.14693251252174377, -0.11754600703716278, -0.08815950155258179, 0.20570550858974457, -0.11754600703716278, -0.14693251252174377, 0.05877300351858139, 0.0, -0.08815950155258179, -0.05877300351858139, 0.029386501759290695, -0.05877300351858139, -0.08815950155258179, 0.05877300351858139, 0.029386501759290695, 0.029386501759290695, 0.05877300351858139, -0.029386501759290695, -0.17631900310516357, 0.14693251252174377]
[2025-05-12 13:00:54,291]: Mean: 0.00213798
[2025-05-12 13:00:54,293]: Min: -0.20570551
[2025-05-12 13:00:54,295]: Max: 0.23509201
[2025-05-12 13:00:54,295]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,307]: Sample Values (25 elements): [0.9123100638389587, 0.9044098854064941, 0.9158017635345459, 0.8779011964797974, 0.8845278024673462, 0.8980752229690552, 0.9414744973182678, 0.8453235030174255, 0.9162480235099792, 0.8974711298942566, 0.8826369047164917, 0.9237198829650879, 0.9264765977859497, 0.9124232530593872, 0.9000782370567322, 0.917779266834259, 0.9021041393280029, 0.8867555856704712, 0.9001495838165283, 0.8927851915359497, 0.920369029045105, 0.8838760256767273, 0.9275790452957153, 0.9041344523429871, 0.8976009488105774]
[2025-05-12 13:00:54,318]: Mean: 0.90387535
[2025-05-12 13:00:54,322]: Min: 0.84532350
[2025-05-12 13:00:54,328]: Max: 0.94543862
[2025-05-12 13:00:54,334]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:00:54,336]: Sample Values (25 elements): [-0.030308013781905174, -0.030308013781905174, -0.06061602756381035, -0.04546201974153519, 0.030308013781905174, -0.015154006890952587, 0.04546201974153519, 0.015154006890952587, -0.015154006890952587, 0.0, 0.0, 0.0, -0.04546201974153519, -0.015154006890952587, 0.0, -0.04546201974153519, -0.015154006890952587, 0.0, -0.04546201974153519, -0.015154006890952587, 0.030308013781905174, -0.030308013781905174, 0.030308013781905174, 0.0, 0.015154006890952587]
[2025-05-12 13:00:54,336]: Mean: -0.00086943
[2025-05-12 13:00:54,336]: Min: -0.12123206
[2025-05-12 13:00:54,337]: Max: 0.10607805
[2025-05-12 13:00:54,337]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,338]: Sample Values (25 elements): [0.9645771980285645, 0.9606097340583801, 0.9723269939422607, 0.9632185101509094, 0.9791727066040039, 0.9759155511856079, 0.9779687523841858, 0.9802834391593933, 0.9724780321121216, 0.9476982355117798, 0.9615568518638611, 0.9714999794960022, 0.9650794863700867, 0.9362208843231201, 0.9636701345443726, 0.9678630232810974, 0.9679595828056335, 0.984077513217926, 0.9526686072349548, 0.991988480091095, 0.9622235894203186, 0.9655115604400635, 0.9908392429351807, 0.9588895440101624, 0.9535089731216431]
[2025-05-12 13:00:54,339]: Mean: 0.96508002
[2025-05-12 13:00:54,339]: Min: 0.92700857
[2025-05-12 13:00:54,339]: Max: 1.00325799
[2025-05-12 13:00:54,343]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:00:54,345]: Sample Values (25 elements): [0.03739486634731293, 0.012464955449104309, -0.024929910898208618, -0.012464955449104309, -0.012464955449104309, -0.012464955449104309, -0.024929910898208618, 0.024929910898208618, -0.024929910898208618, 0.0, 0.0, 0.049859821796417236, -0.012464955449104309, 0.0, 0.024929910898208618, -0.062324777245521545, 0.012464955449104309, 0.012464955449104309, 0.0, 0.03739486634731293, 0.024929910898208618, 0.03739486634731293, -0.024929910898208618, 0.024929910898208618, -0.012464955449104309]
[2025-05-12 13:00:54,345]: Mean: 0.00031751
[2025-05-12 13:00:54,346]: Min: -0.08725469
[2025-05-12 13:00:54,346]: Max: 0.09971964
[2025-05-12 13:00:54,346]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,347]: Sample Values (25 elements): [1.029229998588562, 1.0117932558059692, 1.0230470895767212, 1.0253198146820068, 0.9950724244117737, 1.0444679260253906, 1.0213713645935059, 1.0409480333328247, 1.041466236114502, 1.0156052112579346, 1.0133118629455566, 1.0016428232192993, 1.0509517192840576, 0.997576892375946, 1.034638524055481, 1.00365149974823, 1.031278371810913, 1.0374184846878052, 0.9956920742988586, 1.0170130729675293, 1.0087834596633911, 1.0237109661102295, 0.9982233643531799, 1.004945158958435, 1.0357918739318848]
[2025-05-12 13:00:54,348]: Mean: 1.02375007
[2025-05-12 13:00:54,349]: Min: 0.98561114
[2025-05-12 13:00:54,350]: Max: 1.07690609
[2025-05-12 13:00:54,353]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:00:54,356]: Sample Values (25 elements): [0.012966553680598736, 0.02593310736119747, 0.02593310736119747, 0.02593310736119747, -0.02593310736119747, -0.012966553680598736, 0.0, -0.03889966011047363, -0.012966553680598736, 0.02593310736119747, -0.03889966011047363, -0.03889966011047363, 0.012966553680598736, -0.012966553680598736, 0.02593310736119747, 0.05186621472239494, 0.012966553680598736, -0.012966553680598736, -0.012966553680598736, -0.03889966011047363, -0.012966553680598736, 0.05186621472239494, -0.02593310736119747, -0.03889966011047363, 0.012966553680598736]
[2025-05-12 13:00:54,356]: Mean: -0.00070207
[2025-05-12 13:00:54,357]: Min: -0.09076588
[2025-05-12 13:00:54,357]: Max: 0.10373243
[2025-05-12 13:00:54,358]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,358]: Sample Values (25 elements): [0.9566429257392883, 0.9570094347000122, 0.9441940188407898, 0.9584333896636963, 0.9738815426826477, 0.9428071975708008, 0.9492695927619934, 0.9590198993682861, 0.9680513739585876, 0.9668115973472595, 0.9816850423812866, 0.9557154178619385, 0.9543887376785278, 0.9672888517379761, 0.9507825970649719, 0.9506701231002808, 0.95259028673172, 0.9412253499031067, 0.9625840783119202, 0.9598233103752136, 0.9588444232940674, 0.9647828340530396, 0.9544295072555542, 0.9538800716400146, 0.948668897151947]
[2025-05-12 13:00:54,358]: Mean: 0.95967674
[2025-05-12 13:00:54,359]: Min: 0.93936664
[2025-05-12 13:00:54,359]: Max: 0.99095589
[2025-05-12 13:00:54,361]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:00:54,363]: Sample Values (25 elements): [-0.00977777224034071, -0.029333315789699554, 0.0, -0.01955554448068142, -0.01955554448068142, -0.01955554448068142, 0.0, -0.00977777224034071, 0.029333315789699554, 0.029333315789699554, 0.029333315789699554, 0.0, 0.03911108896136284, -0.01955554448068142, -0.029333315789699554, 0.0, -0.03911108896136284, 0.00977777224034071, -0.01955554448068142, 0.04888886213302612, 0.0, -0.01955554448068142, 0.01955554448068142, -0.03911108896136284, -0.01955554448068142]
[2025-05-12 13:00:54,363]: Mean: 0.00000133
[2025-05-12 13:00:54,363]: Min: -0.06844441
[2025-05-12 13:00:54,363]: Max: 0.07822218
[2025-05-12 13:00:54,363]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:00:54,364]: Sample Values (25 elements): [1.0775113105773926, 1.0650008916854858, 1.0765019655227661, 1.0491489171981812, 1.0396355390548706, 1.0480602979660034, 1.0943609476089478, 1.0512136220932007, 1.089874029159546, 1.0640851259231567, 1.047638177871704, 1.1060017347335815, 1.0325413942337036, 1.0732265710830688, 1.0526427030563354, 1.050476312637329, 1.0612589120864868, 1.0949647426605225, 1.0827769041061401, 1.058501958847046, 1.0885756015777588, 1.0668514966964722, 1.0595135688781738, 1.1015812158584595, 1.0534820556640625]
[2025-05-12 13:00:54,365]: Mean: 1.07007563
[2025-05-12 13:00:54,365]: Min: 1.01990557
[2025-05-12 13:00:54,365]: Max: 1.12343776
[2025-05-12 13:00:54,365]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 13:00:54,366]: Sample Values (25 elements): [-0.17286896705627441, 0.09451933205127716, 0.42785921692848206, -0.042399268597364426, -0.21201466023921967, 0.18912523984909058, -0.16862042248249054, -0.26446932554244995, -0.36463308334350586, -0.38557207584381104, 0.3963588774204254, 0.3572433888912201, -0.3062298893928528, 0.45555734634399414, 0.5384999513626099, 0.020732609555125237, -0.25706160068511963, 0.1278071105480194, -0.32722797989845276, -0.05328568443655968, -0.026678189635276794, 0.23181800544261932, -0.0585576556622982, 0.05282008275389671, -0.0367303229868412]
[2025-05-12 13:00:54,366]: Mean: -0.00359617
[2025-05-12 13:00:54,367]: Min: -0.41896382
[2025-05-12 13:00:54,368]: Max: 0.60666347
[2025-05-12 13:00:54,368]: 


QAT of ResNet20 with relu6 down to 3 bits...
[2025-05-12 13:00:54,802]: [ResNet20_relu6_quantized_3_bits] after configure_qat:
[2025-05-12 13:00:54,998]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:02:36,232]: [ResNet20_relu6_quantized_3_bits] Epoch: 001 Train Loss: 0.6096 Train Acc: 0.7868 Eval Loss: 0.6979 Eval Acc: 0.7750 (LR: 0.001000)
[2025-05-12 13:04:19,520]: [ResNet20_relu6_quantized_3_bits] Epoch: 002 Train Loss: 0.5554 Train Acc: 0.8058 Eval Loss: 0.6788 Eval Acc: 0.7747 (LR: 0.001000)
[2025-05-12 13:06:05,013]: [ResNet20_relu6_quantized_3_bits] Epoch: 003 Train Loss: 0.5467 Train Acc: 0.8084 Eval Loss: 0.6264 Eval Acc: 0.7944 (LR: 0.001000)
[2025-05-12 13:07:50,620]: [ResNet20_relu6_quantized_3_bits] Epoch: 004 Train Loss: 0.5318 Train Acc: 0.8131 Eval Loss: 0.6393 Eval Acc: 0.7880 (LR: 0.001000)
[2025-05-12 13:09:37,282]: [ResNet20_relu6_quantized_3_bits] Epoch: 005 Train Loss: 0.5320 Train Acc: 0.8119 Eval Loss: 0.6282 Eval Acc: 0.7952 (LR: 0.001000)
[2025-05-12 13:11:24,409]: [ResNet20_relu6_quantized_3_bits] Epoch: 006 Train Loss: 0.5256 Train Acc: 0.8154 Eval Loss: 0.6420 Eval Acc: 0.7832 (LR: 0.001000)
[2025-05-12 13:13:09,892]: [ResNet20_relu6_quantized_3_bits] Epoch: 007 Train Loss: 0.5151 Train Acc: 0.8192 Eval Loss: 0.6165 Eval Acc: 0.7934 (LR: 0.001000)
[2025-05-12 13:14:54,522]: [ResNet20_relu6_quantized_3_bits] Epoch: 008 Train Loss: 0.5133 Train Acc: 0.8198 Eval Loss: 0.6138 Eval Acc: 0.7984 (LR: 0.001000)
[2025-05-12 13:16:41,677]: [ResNet20_relu6_quantized_3_bits] Epoch: 009 Train Loss: 0.5113 Train Acc: 0.8201 Eval Loss: 0.6045 Eval Acc: 0.7981 (LR: 0.001000)
[2025-05-12 13:18:29,713]: [ResNet20_relu6_quantized_3_bits] Epoch: 010 Train Loss: 0.5024 Train Acc: 0.8236 Eval Loss: 0.6548 Eval Acc: 0.7885 (LR: 0.001000)
[2025-05-12 13:20:18,332]: [ResNet20_relu6_quantized_3_bits] Epoch: 011 Train Loss: 0.5065 Train Acc: 0.8229 Eval Loss: 0.6337 Eval Acc: 0.7958 (LR: 0.001000)
[2025-05-12 13:22:06,532]: [ResNet20_relu6_quantized_3_bits] Epoch: 012 Train Loss: 0.5013 Train Acc: 0.8247 Eval Loss: 0.6647 Eval Acc: 0.7783 (LR: 0.001000)
[2025-05-12 13:23:54,619]: [ResNet20_relu6_quantized_3_bits] Epoch: 013 Train Loss: 0.5017 Train Acc: 0.8250 Eval Loss: 0.5427 Eval Acc: 0.8204 (LR: 0.001000)
[2025-05-12 13:25:43,216]: [ResNet20_relu6_quantized_3_bits] Epoch: 014 Train Loss: 0.5034 Train Acc: 0.8229 Eval Loss: 0.5964 Eval Acc: 0.7980 (LR: 0.001000)
[2025-05-12 13:27:32,357]: [ResNet20_relu6_quantized_3_bits] Epoch: 015 Train Loss: 0.4924 Train Acc: 0.8265 Eval Loss: 0.6046 Eval Acc: 0.7970 (LR: 0.001000)
[2025-05-12 13:29:21,331]: [ResNet20_relu6_quantized_3_bits] Epoch: 016 Train Loss: 0.4874 Train Acc: 0.8288 Eval Loss: 0.6790 Eval Acc: 0.7864 (LR: 0.001000)
[2025-05-12 13:31:10,712]: [ResNet20_relu6_quantized_3_bits] Epoch: 017 Train Loss: 0.4867 Train Acc: 0.8286 Eval Loss: 0.6367 Eval Acc: 0.7928 (LR: 0.001000)
[2025-05-12 13:33:00,521]: [ResNet20_relu6_quantized_3_bits] Epoch: 018 Train Loss: 0.4890 Train Acc: 0.8271 Eval Loss: 0.6871 Eval Acc: 0.7774 (LR: 0.001000)
[2025-05-12 13:34:50,376]: [ResNet20_relu6_quantized_3_bits] Epoch: 019 Train Loss: 0.4834 Train Acc: 0.8303 Eval Loss: 0.5434 Eval Acc: 0.8169 (LR: 0.001000)
[2025-05-12 13:36:39,251]: [ResNet20_relu6_quantized_3_bits] Epoch: 020 Train Loss: 0.4852 Train Acc: 0.8313 Eval Loss: 0.5880 Eval Acc: 0.8090 (LR: 0.001000)
[2025-05-12 13:38:28,270]: [ResNet20_relu6_quantized_3_bits] Epoch: 021 Train Loss: 0.4782 Train Acc: 0.8324 Eval Loss: 0.5715 Eval Acc: 0.8146 (LR: 0.001000)
[2025-05-12 13:40:15,048]: [ResNet20_relu6_quantized_3_bits] Epoch: 022 Train Loss: 0.4794 Train Acc: 0.8333 Eval Loss: 0.7025 Eval Acc: 0.7815 (LR: 0.001000)
[2025-05-12 13:42:03,773]: [ResNet20_relu6_quantized_3_bits] Epoch: 023 Train Loss: 0.4760 Train Acc: 0.8348 Eval Loss: 0.5853 Eval Acc: 0.8035 (LR: 0.001000)
[2025-05-12 13:43:53,806]: [ResNet20_relu6_quantized_3_bits] Epoch: 024 Train Loss: 0.4691 Train Acc: 0.8365 Eval Loss: 0.6692 Eval Acc: 0.7887 (LR: 0.001000)
[2025-05-12 13:45:44,685]: [ResNet20_relu6_quantized_3_bits] Epoch: 025 Train Loss: 0.4672 Train Acc: 0.8376 Eval Loss: 0.6723 Eval Acc: 0.7863 (LR: 0.001000)
[2025-05-12 13:47:35,979]: [ResNet20_relu6_quantized_3_bits] Epoch: 026 Train Loss: 0.4642 Train Acc: 0.8377 Eval Loss: 0.5884 Eval Acc: 0.8043 (LR: 0.001000)
[2025-05-12 13:49:30,219]: [ResNet20_relu6_quantized_3_bits] Epoch: 027 Train Loss: 0.4599 Train Acc: 0.8377 Eval Loss: 0.6284 Eval Acc: 0.7976 (LR: 0.001000)
[2025-05-12 13:51:23,811]: [ResNet20_relu6_quantized_3_bits] Epoch: 028 Train Loss: 0.4649 Train Acc: 0.8370 Eval Loss: 0.6521 Eval Acc: 0.7912 (LR: 0.001000)
[2025-05-12 13:53:17,117]: [ResNet20_relu6_quantized_3_bits] Epoch: 029 Train Loss: 0.4596 Train Acc: 0.8383 Eval Loss: 0.5697 Eval Acc: 0.8090 (LR: 0.001000)
[2025-05-12 13:55:08,549]: [ResNet20_relu6_quantized_3_bits] Epoch: 030 Train Loss: 0.4589 Train Acc: 0.8385 Eval Loss: 0.6490 Eval Acc: 0.7896 (LR: 0.000250)
[2025-05-12 13:56:55,585]: [ResNet20_relu6_quantized_3_bits] Epoch: 031 Train Loss: 0.4145 Train Acc: 0.8546 Eval Loss: 0.4602 Eval Acc: 0.8433 (LR: 0.000250)
[2025-05-12 13:58:42,218]: [ResNet20_relu6_quantized_3_bits] Epoch: 032 Train Loss: 0.4083 Train Acc: 0.8584 Eval Loss: 0.4574 Eval Acc: 0.8481 (LR: 0.000250)
[2025-05-12 14:00:28,483]: [ResNet20_relu6_quantized_3_bits] Epoch: 033 Train Loss: 0.4042 Train Acc: 0.8596 Eval Loss: 0.5405 Eval Acc: 0.8211 (LR: 0.000250)
[2025-05-12 14:02:15,825]: [ResNet20_relu6_quantized_3_bits] Epoch: 034 Train Loss: 0.4037 Train Acc: 0.8609 Eval Loss: 0.4750 Eval Acc: 0.8426 (LR: 0.000250)
[2025-05-12 14:04:02,088]: [ResNet20_relu6_quantized_3_bits] Epoch: 035 Train Loss: 0.4084 Train Acc: 0.8573 Eval Loss: 0.5221 Eval Acc: 0.8261 (LR: 0.000250)
[2025-05-12 14:05:47,937]: [ResNet20_relu6_quantized_3_bits] Epoch: 036 Train Loss: 0.4066 Train Acc: 0.8582 Eval Loss: 0.5113 Eval Acc: 0.8326 (LR: 0.000250)
[2025-05-12 14:07:32,644]: [ResNet20_relu6_quantized_3_bits] Epoch: 037 Train Loss: 0.4030 Train Acc: 0.8585 Eval Loss: 0.5085 Eval Acc: 0.8293 (LR: 0.000250)
[2025-05-12 14:09:17,396]: [ResNet20_relu6_quantized_3_bits] Epoch: 038 Train Loss: 0.4033 Train Acc: 0.8591 Eval Loss: 0.4698 Eval Acc: 0.8451 (LR: 0.000250)
[2025-05-12 14:11:06,081]: [ResNet20_relu6_quantized_3_bits] Epoch: 039 Train Loss: 0.4026 Train Acc: 0.8610 Eval Loss: 0.4680 Eval Acc: 0.8437 (LR: 0.000250)
[2025-05-12 14:12:56,410]: [ResNet20_relu6_quantized_3_bits] Epoch: 040 Train Loss: 0.4047 Train Acc: 0.8579 Eval Loss: 0.5053 Eval Acc: 0.8344 (LR: 0.000250)
[2025-05-12 14:14:45,658]: [ResNet20_relu6_quantized_3_bits] Epoch: 041 Train Loss: 0.4034 Train Acc: 0.8599 Eval Loss: 0.5232 Eval Acc: 0.8299 (LR: 0.000250)
[2025-05-12 14:16:33,526]: [ResNet20_relu6_quantized_3_bits] Epoch: 042 Train Loss: 0.4053 Train Acc: 0.8578 Eval Loss: 0.7953 Eval Acc: 0.7607 (LR: 0.000250)
[2025-05-12 14:18:19,789]: [ResNet20_relu6_quantized_3_bits] Epoch: 043 Train Loss: 0.4019 Train Acc: 0.8583 Eval Loss: 0.4872 Eval Acc: 0.8382 (LR: 0.000250)
[2025-05-12 14:20:01,223]: [ResNet20_relu6_quantized_3_bits] Epoch: 044 Train Loss: 0.4041 Train Acc: 0.8588 Eval Loss: 0.5084 Eval Acc: 0.8313 (LR: 0.000250)
[2025-05-12 14:21:38,029]: [ResNet20_relu6_quantized_3_bits] Epoch: 045 Train Loss: 0.4082 Train Acc: 0.8557 Eval Loss: 0.4872 Eval Acc: 0.8396 (LR: 0.000063)
[2025-05-12 14:23:19,321]: [ResNet20_relu6_quantized_3_bits] Epoch: 046 Train Loss: 0.3826 Train Acc: 0.8686 Eval Loss: 0.4497 Eval Acc: 0.8485 (LR: 0.000063)
[2025-05-12 14:25:11,857]: [ResNet20_relu6_quantized_3_bits] Epoch: 047 Train Loss: 0.3810 Train Acc: 0.8680 Eval Loss: 0.4691 Eval Acc: 0.8468 (LR: 0.000063)
[2025-05-12 14:26:58,744]: [ResNet20_relu6_quantized_3_bits] Epoch: 048 Train Loss: 0.3779 Train Acc: 0.8690 Eval Loss: 0.4462 Eval Acc: 0.8524 (LR: 0.000063)
[2025-05-12 14:28:53,193]: [ResNet20_relu6_quantized_3_bits] Epoch: 049 Train Loss: 0.3770 Train Acc: 0.8674 Eval Loss: 0.4499 Eval Acc: 0.8513 (LR: 0.000063)
[2025-05-12 14:30:54,913]: [ResNet20_relu6_quantized_3_bits] Epoch: 050 Train Loss: 0.3806 Train Acc: 0.8671 Eval Loss: 0.4561 Eval Acc: 0.8455 (LR: 0.000063)
[2025-05-12 14:32:55,626]: [ResNet20_relu6_quantized_3_bits] Epoch: 051 Train Loss: 0.3816 Train Acc: 0.8653 Eval Loss: 0.4481 Eval Acc: 0.8511 (LR: 0.000063)
[2025-05-12 14:34:51,869]: [ResNet20_relu6_quantized_3_bits] Epoch: 052 Train Loss: 0.3756 Train Acc: 0.8685 Eval Loss: 0.4421 Eval Acc: 0.8541 (LR: 0.000063)
[2025-05-12 14:36:49,764]: [ResNet20_relu6_quantized_3_bits] Epoch: 053 Train Loss: 0.3760 Train Acc: 0.8682 Eval Loss: 0.4739 Eval Acc: 0.8467 (LR: 0.000063)
[2025-05-12 14:39:02,924]: [ResNet20_relu6_quantized_3_bits] Epoch: 054 Train Loss: 0.3793 Train Acc: 0.8677 Eval Loss: 0.4662 Eval Acc: 0.8473 (LR: 0.000063)
[2025-05-12 14:41:28,205]: [ResNet20_relu6_quantized_3_bits] Epoch: 055 Train Loss: 0.3781 Train Acc: 0.8682 Eval Loss: 0.4535 Eval Acc: 0.8478 (LR: 0.000063)
[2025-05-12 14:43:52,049]: [ResNet20_relu6_quantized_3_bits] Epoch: 056 Train Loss: 0.3821 Train Acc: 0.8651 Eval Loss: 0.4421 Eval Acc: 0.8541 (LR: 0.000063)
[2025-05-12 14:46:14,761]: [ResNet20_relu6_quantized_3_bits] Epoch: 057 Train Loss: 0.3818 Train Acc: 0.8683 Eval Loss: 0.4561 Eval Acc: 0.8499 (LR: 0.000063)
[2025-05-12 14:48:37,024]: [ResNet20_relu6_quantized_3_bits] Epoch: 058 Train Loss: 0.3800 Train Acc: 0.8666 Eval Loss: 0.4663 Eval Acc: 0.8471 (LR: 0.000063)
[2025-05-12 14:50:57,857]: [ResNet20_relu6_quantized_3_bits] Epoch: 059 Train Loss: 0.3777 Train Acc: 0.8676 Eval Loss: 0.4474 Eval Acc: 0.8488 (LR: 0.000063)
[2025-05-12 14:53:17,562]: [ResNet20_relu6_quantized_3_bits] Epoch: 060 Train Loss: 0.3817 Train Acc: 0.8664 Eval Loss: 0.4525 Eval Acc: 0.8489 (LR: 0.000063)
[2025-05-12 14:53:17,562]: [ResNet20_relu6_quantized_3_bits] Best Eval Accuracy: 0.8541
[2025-05-12 14:53:17,614]: 


Quantization of model down to 3 bits finished
[2025-05-12 14:53:17,615]: Model Architecture:
[2025-05-12 14:53:17,840]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0854], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2939392030239105, max_val=0.3035891652107239)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0729], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27999213337898254, max_val=0.23025104403495789)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1137], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47285738587379456, max_val=0.3231408894062042)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0765], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2801933288574219, max_val=0.2554768919944763)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0566], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19390444457530975, max_val=0.20242726802825928)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8553], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.986942291259766)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0549], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16003111004829407, max_val=0.22440657019615173)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17637142539024353, max_val=0.1767280250787735)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7832], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.482743263244629)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0493], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15158145129680634, max_val=0.19330793619155884)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0990], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3712000548839569, max_val=0.3215451240539551)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0516], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16427427530288696, max_val=0.1967090666294098)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8507], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.9550933837890625)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14707718789577484, max_val=0.15266038477420807)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0403], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14438626170158386, max_val=0.13750886917114258)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8449], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.914095878601074)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0361], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1264198124408722, max_val=0.12660983204841614)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12692351639270782, max_val=0.12962587177753448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8314], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.819631576538086)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0334], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1130579337477684, max_val=0.12095164507627487)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0656], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2169441133737564, max_val=0.24204418063163757)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0331], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1159886047244072, max_val=0.11591992527246475)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8569], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99852991104126)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0267], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09040188789367676, max_val=0.09626708179712296)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09358134120702744, max_val=0.0986889973282814)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8545], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.981377124786377)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07136264443397522, max_val=0.07456298917531967)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 14:53:17,840]: 
Model Weights:
[2025-05-12 14:53:17,840]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 14:53:17,841]: Sample Values (25 elements): [-0.21359370648860931, 0.021588914096355438, -0.12177074700593948, -0.08114878088235855, 0.08307382464408875, 0.02153964899480343, 0.3727555274963379, 0.3439517915248871, 0.04937972128391266, -0.08708912134170532, 0.04764935374259949, -0.2864506244659424, 0.11786340922117233, -0.06276518851518631, -0.25281575322151184, -0.04472237080335617, 0.08002829551696777, -0.053173359483480453, -0.2069108933210373, -0.05079331248998642, -0.14588946104049683, 0.008839421905577183, -0.10913275182247162, -0.17507150769233704, -0.17363615334033966]
[2025-05-12 14:53:17,843]: Mean: -0.00166134
[2025-05-12 14:53:17,844]: Min: -0.46099982
[2025-05-12 14:53:17,844]: Max: 0.52992141
[2025-05-12 14:53:17,844]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,845]: Sample Values (16 elements): [1.2639323472976685, 1.1491352319717407, 0.9094492197036743, 1.136247992515564, 1.3110285997390747, 0.9560832381248474, 1.1831201314926147, 1.0177574157714844, 1.3244867324829102, 1.137855887413025, 1.1751444339752197, 1.1668838262557983, 1.182255744934082, 1.1228039264678955, 1.0989841222763062, 1.109363079071045]
[2025-05-12 14:53:17,846]: Mean: 1.14028323
[2025-05-12 14:53:17,846]: Min: 0.90944922
[2025-05-12 14:53:17,846]: Max: 1.32448673
[2025-05-12 14:53:17,852]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:53:17,854]: Sample Values (25 elements): [0.0, 0.0, -0.08536122739315033, -0.17072245478630066, -0.08536122739315033, 0.0, 0.08536122739315033, 0.0, 0.0, -0.08536122739315033, 0.0, 0.08536122739315033, 0.0, 0.0, -0.08536122739315033, 0.0, 0.08536122739315033, 0.0, 0.08536122739315033, 0.0, 0.0, 0.08536122739315033, 0.0, 0.0, -0.08536122739315033]
[2025-05-12 14:53:17,855]: Mean: -0.00829901
[2025-05-12 14:53:17,856]: Min: -0.25608367
[2025-05-12 14:53:17,857]: Max: 0.34144491
[2025-05-12 14:53:17,857]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,863]: Sample Values (16 elements): [1.4838213920593262, 1.0546915531158447, 0.985224723815918, 0.9839359521865845, 0.9779173731803894, 1.0750327110290527, 1.0237480401992798, 0.8632263541221619, 1.0058010816574097, 0.9609101414680481, 1.1853623390197754, 1.034105896949768, 0.9317341446876526, 1.0824512243270874, 1.1031608581542969, 1.117212176322937]
[2025-05-12 14:53:17,863]: Mean: 1.05427098
[2025-05-12 14:53:17,864]: Min: 0.86322635
[2025-05-12 14:53:17,866]: Max: 1.48382139
[2025-05-12 14:53:17,875]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:53:17,878]: Sample Values (25 elements): [0.07289190590381622, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07289190590381622, -0.07289190590381622, 0.14578381180763245, -0.07289190590381622, 0.14578381180763245, 0.07289190590381622, 0.07289190590381622, -0.07289190590381622, -0.14578381180763245, 0.0, -0.07289190590381622, 0.0, 0.07289190590381622, 0.0, -0.14578381180763245, 0.0, 0.0]
[2025-05-12 14:53:17,879]: Mean: -0.00787764
[2025-05-12 14:53:17,881]: Min: -0.29156762
[2025-05-12 14:53:17,892]: Max: 0.21867572
[2025-05-12 14:53:17,892]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,904]: Sample Values (16 elements): [1.0537153482437134, 1.0297011137008667, 0.9427868723869324, 1.0268748998641968, 0.8871685862541199, 1.093826174736023, 1.0146393775939941, 1.0750019550323486, 1.0465776920318604, 0.9188769459724426, 0.969627320766449, 1.1232151985168457, 0.9891757369041443, 0.9588667750358582, 0.9644280672073364, 0.9635118246078491]
[2025-05-12 14:53:17,906]: Mean: 1.00362468
[2025-05-12 14:53:17,912]: Min: 0.88716859
[2025-05-12 14:53:17,919]: Max: 1.12321520
[2025-05-12 14:53:17,924]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:53:17,925]: Sample Values (25 elements): [0.2274283468723297, 0.0, -0.2274283468723297, 0.0, -0.11371417343616486, -0.11371417343616486, -0.11371417343616486, 0.0, 0.2274283468723297, 0.0, 0.11371417343616486, 0.0, 0.0, 0.0, 0.0, -0.11371417343616486, 0.0, 0.11371417343616486, 0.0, 0.0, 0.11371417343616486, -0.11371417343616486, 0.0, 0.0, 0.0]
[2025-05-12 14:53:17,925]: Mean: 0.00024678
[2025-05-12 14:53:17,926]: Min: -0.45485669
[2025-05-12 14:53:17,926]: Max: 0.34114254
[2025-05-12 14:53:17,926]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,927]: Sample Values (16 elements): [1.0619207620620728, 0.995430052280426, 1.0193153619766235, 1.2727030515670776, 1.1236562728881836, 0.9986065030097961, 1.2715950012207031, 0.984165608882904, 0.9105302691459656, 1.173250675201416, 0.9807538390159607, 1.000357747077942, 0.8638812899589539, 0.9525425434112549, 1.0011606216430664, 1.000473141670227]
[2025-05-12 14:53:17,928]: Mean: 1.03814638
[2025-05-12 14:53:17,929]: Min: 0.86388129
[2025-05-12 14:53:17,929]: Max: 1.27270305
[2025-05-12 14:53:17,933]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:53:17,934]: Sample Values (25 elements): [0.07652433216571808, 0.07652433216571808, 0.0, 0.0, 0.0, 0.0, -0.07652433216571808, 0.07652433216571808, -0.07652433216571808, 0.0, 0.0, 0.0, 0.07652433216571808, 0.07652433216571808, 0.0, 0.07652433216571808, 0.0, -0.07652433216571808, -0.07652433216571808, 0.07652433216571808, -0.15304866433143616, 0.0, 0.07652433216571808, -0.07652433216571808, -0.07652433216571808]
[2025-05-12 14:53:17,935]: Mean: -0.00126212
[2025-05-12 14:53:17,935]: Min: -0.30609733
[2025-05-12 14:53:17,936]: Max: 0.22957300
[2025-05-12 14:53:17,936]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,937]: Sample Values (16 elements): [1.04055655002594, 1.2207239866256714, 0.9522465467453003, 1.0686967372894287, 0.9955199360847473, 1.0218292474746704, 0.9172008037567139, 0.8819327354431152, 1.0216175317764282, 1.151898741722107, 1.055145502090454, 1.0125902891159058, 0.9231604933738708, 1.0152937173843384, 0.9612649083137512, 0.8716976046562195]
[2025-05-12 14:53:17,938]: Mean: 1.00696087
[2025-05-12 14:53:17,939]: Min: 0.87169760
[2025-05-12 14:53:17,940]: Max: 1.22072399
[2025-05-12 14:53:17,943]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:53:17,944]: Sample Values (25 elements): [0.0, -0.05661880224943161, -0.11323760449886322, 0.05661880224943161, 0.0, -0.11323760449886322, 0.0, -0.11323760449886322, -0.05661880224943161, 0.05661880224943161, 0.05661880224943161, -0.05661880224943161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05661880224943161, 0.0, 0.05661880224943161, -0.11323760449886322, 0.05661880224943161, -0.05661880224943161, 0.11323760449886322, 0.05661880224943161]
[2025-05-12 14:53:17,945]: Mean: -0.00265401
[2025-05-12 14:53:17,945]: Min: -0.16985640
[2025-05-12 14:53:17,946]: Max: 0.22647521
[2025-05-12 14:53:17,946]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,947]: Sample Values (16 elements): [1.0163849592208862, 0.9564830660820007, 1.038833737373352, 0.9997349381446838, 0.9797919392585754, 0.988405168056488, 1.02427077293396, 1.0359556674957275, 1.0229408740997314, 0.9334792494773865, 1.0222734212875366, 1.0409388542175293, 1.0916999578475952, 0.9786134362220764, 1.0077639818191528, 0.940920352935791]
[2025-05-12 14:53:17,948]: Mean: 1.00490570
[2025-05-12 14:53:17,948]: Min: 0.93347925
[2025-05-12 14:53:17,948]: Max: 1.09169996
[2025-05-12 14:53:17,952]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:53:17,953]: Sample Values (25 elements): [0.0, 0.0, -0.05491967126727104, 0.05491967126727104, -0.05491967126727104, 0.05491967126727104, -0.05491967126727104, 0.0, 0.05491967126727104, -0.05491967126727104, 0.05491967126727104, 0.05491967126727104, 0.0, -0.05491967126727104, -0.05491967126727104, -0.05491967126727104, 0.0, -0.05491967126727104, 0.10983934253454208, 0.05491967126727104, 0.10983934253454208, 0.10983934253454208, -0.05491967126727104, 0.0, -0.05491967126727104]
[2025-05-12 14:53:17,954]: Mean: 0.00526790
[2025-05-12 14:53:17,957]: Min: -0.16475901
[2025-05-12 14:53:17,961]: Max: 0.21967869
[2025-05-12 14:53:17,961]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:53:17,967]: Sample Values (16 elements): [0.9388880133628845, 0.904891848564148, 0.9237231016159058, 0.9895312190055847, 0.9535816311836243, 1.0330159664154053, 1.0738450288772583, 0.9575663805007935, 1.0075958967208862, 0.9898341298103333, 1.0598843097686768, 0.9558866620063782, 0.973923921585083, 0.9565507173538208, 0.9877477884292603, 1.1184964179992676]
[2025-05-12 14:53:17,978]: Mean: 0.98906016
[2025-05-12 14:53:17,989]: Min: 0.90489185
[2025-05-12 14:53:18,000]: Max: 1.11849642
[2025-05-12 14:53:18,009]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 14:53:18,010]: Sample Values (25 elements): [-0.05044274032115936, 0.0, -0.05044274032115936, -0.05044274032115936, 0.05044274032115936, 0.05044274032115936, 0.0, 0.0, -0.05044274032115936, 0.0, 0.05044274032115936, 0.0, 0.05044274032115936, 0.05044274032115936, 0.0, -0.05044274032115936, 0.0, -0.05044274032115936, 0.0, 0.05044274032115936, 0.1513282209634781, 0.05044274032115936, -0.05044274032115936, 0.0, 0.0]
[2025-05-12 14:53:18,010]: Mean: -0.00307604
[2025-05-12 14:53:18,011]: Min: -0.15132822
[2025-05-12 14:53:18,013]: Max: 0.20177096
[2025-05-12 14:53:18,013]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,014]: Sample Values (25 elements): [0.9778879284858704, 1.0110912322998047, 0.9624303579330444, 0.9484829306602478, 1.01369047164917, 1.0474086999893188, 1.0038303136825562, 0.9815667271614075, 0.9473562836647034, 1.0509754419326782, 0.9997819066047668, 0.9869550466537476, 1.0414444208145142, 0.9633216261863708, 1.0175094604492188, 0.9524005651473999, 0.9710218906402588, 1.0438997745513916, 0.9988850951194763, 1.0054810047149658, 0.9599947333335876, 1.0101797580718994, 1.0559669733047485, 0.9519690275192261, 1.008662223815918]
[2025-05-12 14:53:18,014]: Mean: 1.00130188
[2025-05-12 14:53:18,014]: Min: 0.94735628
[2025-05-12 14:53:18,015]: Max: 1.07509923
[2025-05-12 14:53:18,020]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:53:18,022]: Sample Values (25 elements): [-0.09853971749544144, 0.0, -0.09853971749544144, -0.04926985874772072, 0.0, 0.0, 0.04926985874772072, -0.04926985874772072, 0.04926985874772072, -0.04926985874772072, 0.04926985874772072, -0.04926985874772072, 0.04926985874772072, 0.04926985874772072, -0.04926985874772072, -0.04926985874772072, 0.04926985874772072, 0.04926985874772072, 0.09853971749544144, 0.09853971749544144, 0.04926985874772072, -0.04926985874772072, -0.04926985874772072, 0.0, 0.0]
[2025-05-12 14:53:18,022]: Mean: -0.00322371
[2025-05-12 14:53:18,023]: Min: -0.14780958
[2025-05-12 14:53:18,023]: Max: 0.19707943
[2025-05-12 14:53:18,023]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,025]: Sample Values (25 elements): [1.04777991771698, 1.0341181755065918, 1.0532383918762207, 1.0178765058517456, 0.9134408235549927, 0.9741343259811401, 1.036808967590332, 0.9835601449012756, 1.020220398902893, 1.0401123762130737, 1.0313596725463867, 1.0164607763290405, 0.9929941892623901, 0.9862895011901855, 1.007029414176941, 0.9915401339530945, 1.0354669094085693, 0.9986844658851624, 0.9893894195556641, 1.048134446144104, 1.0497726202011108, 1.0391980409622192, 1.0984008312225342, 0.9909025430679321, 1.0412148237228394]
[2025-05-12 14:53:18,026]: Mean: 1.01864517
[2025-05-12 14:53:18,026]: Min: 0.91344082
[2025-05-12 14:53:18,028]: Max: 1.10121763
[2025-05-12 14:53:18,031]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 14:53:18,032]: Sample Values (25 elements): [0.0, -0.09896362572908401, 0.09896362572908401, -0.29689088463783264, 0.09896362572908401, 0.29689088463783264, 0.0, 0.0, 0.29689088463783264, 0.19792725145816803, -0.09896362572908401, 0.09896362572908401, -0.19792725145816803, 0.09896362572908401, -0.19792725145816803, 0.0, -0.19792725145816803, 0.09896362572908401, -0.29689088463783264, -0.09896362572908401, 0.09896362572908401, 0.0, -0.09896362572908401, 0.0, -0.09896362572908401]
[2025-05-12 14:53:18,032]: Mean: -0.01488320
[2025-05-12 14:53:18,033]: Min: -0.39585450
[2025-05-12 14:53:18,033]: Max: 0.29689088
[2025-05-12 14:53:18,033]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,035]: Sample Values (25 elements): [0.8825494050979614, 0.895866334438324, 0.8486202359199524, 0.8683123588562012, 0.8346980214118958, 0.8272212743759155, 0.8730164170265198, 0.8316762447357178, 0.7961823344230652, 0.8493020534515381, 0.8951516151428223, 0.8444095849990845, 0.8592580556869507, 0.8461549282073975, 0.8671610355377197, 0.85161292552948, 0.9128395318984985, 0.9386457800865173, 0.902634859085083, 0.8907738924026489, 0.8792405724525452, 0.9060519933700562, 0.8237140774726868, 0.8596014380455017, 0.8701552748680115]
[2025-05-12 14:53:18,036]: Mean: 0.86273235
[2025-05-12 14:53:18,036]: Min: 0.79618233
[2025-05-12 14:53:18,036]: Max: 0.93864578
[2025-05-12 14:53:18,039]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:53:18,040]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.05156911537051201, -0.05156911537051201, 0.05156911537051201, 0.05156911537051201, 0.0, -0.05156911537051201, 0.0, -0.05156911537051201, 0.0, -0.05156911537051201, 0.0, 0.05156911537051201, 0.10313823074102402, 0.0, 0.05156911537051201, 0.05156911537051201, 0.0, -0.05156911537051201, 0.0, 0.0, 0.05156911537051201, 0.05156911537051201]
[2025-05-12 14:53:18,041]: Mean: -0.00059873
[2025-05-12 14:53:18,042]: Min: -0.15470734
[2025-05-12 14:53:18,043]: Max: 0.20627646
[2025-05-12 14:53:18,043]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,044]: Sample Values (25 elements): [1.0456255674362183, 0.9796817302703857, 1.0848603248596191, 0.9801980257034302, 1.0412518978118896, 0.9607826471328735, 0.9896742105484009, 0.981148898601532, 1.0146548748016357, 0.9557228088378906, 1.0120549201965332, 0.9918088316917419, 1.0271875858306885, 0.9556828141212463, 0.9987059831619263, 0.9639654159545898, 1.0005394220352173, 1.0019824504852295, 0.9880393147468567, 1.0200557708740234, 0.9445012807846069, 1.0231618881225586, 1.073660135269165, 0.9791254997253418, 1.0282464027404785]
[2025-05-12 14:53:18,045]: Mean: 0.99452269
[2025-05-12 14:53:18,045]: Min: 0.93283534
[2025-05-12 14:53:18,046]: Max: 1.08486032
[2025-05-12 14:53:18,048]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:53:18,050]: Sample Values (25 elements): [0.04281962290406227, 0.0, 0.0, 0.08563924580812454, 0.04281962290406227, 0.0, 0.0, 0.0, -0.08563924580812454, -0.04281962290406227, 0.0, 0.04281962290406227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04281962290406227, -0.04281962290406227, -0.04281962290406227, 0.04281962290406227, 0.0, 0.04281962290406227, -0.08563924580812454]
[2025-05-12 14:53:18,051]: Mean: -0.00091531
[2025-05-12 14:53:18,051]: Min: -0.12845887
[2025-05-12 14:53:18,052]: Max: 0.17127849
[2025-05-12 14:53:18,052]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,052]: Sample Values (25 elements): [1.0072017908096313, 0.9778803586959839, 0.9684874415397644, 0.9814996719360352, 1.0619503259658813, 0.9585742950439453, 1.0202465057373047, 1.0460463762283325, 0.9783187508583069, 0.9975976347923279, 1.036604881286621, 1.0421326160430908, 0.9004992246627808, 0.9989382028579712, 0.9532812237739563, 1.0441067218780518, 1.042839527130127, 1.0086708068847656, 0.9556575417518616, 1.0196627378463745, 1.0055238008499146, 0.929466724395752, 1.0310243368148804, 1.0447585582733154, 1.0224812030792236]
[2025-05-12 14:53:18,053]: Mean: 0.99133968
[2025-05-12 14:53:18,053]: Min: 0.86544865
[2025-05-12 14:53:18,054]: Max: 1.06195033
[2025-05-12 14:53:18,056]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:53:18,057]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.04027063772082329, 0.04027063772082329, -0.04027063772082329, -0.04027063772082329, -0.04027063772082329, 0.04027063772082329, -0.04027063772082329, 0.04027063772082329, 0.0, 0.0, 0.0, -0.04027063772082329, -0.04027063772082329, -0.04027063772082329, 0.0, 0.0, -0.04027063772082329, -0.04027063772082329, 0.0, 0.0, -0.04027063772082329, -0.04027063772082329]
[2025-05-12 14:53:18,059]: Mean: -0.00199256
[2025-05-12 14:53:18,059]: Min: -0.16108255
[2025-05-12 14:53:18,059]: Max: 0.12081191
[2025-05-12 14:53:18,059]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,061]: Sample Values (25 elements): [0.9394577145576477, 0.9920392632484436, 1.0081498622894287, 1.000331997871399, 0.979107677936554, 1.0082632303237915, 1.0170518159866333, 0.9731370806694031, 0.9516386389732361, 0.992178201675415, 0.9905458688735962, 0.979345440864563, 0.9952585101127625, 0.9741932153701782, 0.9459105730056763, 0.9848493933677673, 0.9725881814956665, 0.9901921153068542, 0.9782610535621643, 0.9944092631340027, 1.000812292098999, 0.9988241195678711, 0.983144223690033, 0.9856414198875427, 0.9576972723007202]
[2025-05-12 14:53:18,061]: Mean: 0.98213124
[2025-05-12 14:53:18,061]: Min: 0.93945771
[2025-05-12 14:53:18,062]: Max: 1.02687681
[2025-05-12 14:53:18,066]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:53:18,067]: Sample Values (25 elements): [0.07229425758123398, 0.03614712879061699, 0.0, -0.03614712879061699, -0.03614712879061699, 0.0, 0.0, 0.03614712879061699, -0.03614712879061699, 0.07229425758123398, 0.03614712879061699, 0.0, -0.03614712879061699, -0.03614712879061699, -0.07229425758123398, 0.0, 0.03614712879061699, 0.0, 0.07229425758123398, 0.0, -0.03614712879061699, -0.07229425758123398, 0.07229425758123398, 0.03614712879061699, 0.03614712879061699]
[2025-05-12 14:53:18,068]: Mean: 0.00039614
[2025-05-12 14:53:18,068]: Min: -0.10844138
[2025-05-12 14:53:18,068]: Max: 0.14458852
[2025-05-12 14:53:18,068]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:53:18,070]: Sample Values (25 elements): [1.0246198177337646, 0.9872629642486572, 0.994543194770813, 1.0019420385360718, 0.9775868058204651, 1.0199216604232788, 0.9746779799461365, 0.9825707674026489, 1.0196022987365723, 1.0232200622558594, 0.9794915318489075, 1.0199902057647705, 0.97981196641922, 0.9650270342826843, 1.022665023803711, 1.0074459314346313, 0.9781489968299866, 0.9505744576454163, 1.0625375509262085, 1.013308048248291, 0.9668375253677368, 1.0398913621902466, 1.0197222232818604, 1.0552664995193481, 1.0353055000305176]
[2025-05-12 14:53:18,071]: Mean: 1.00205827
[2025-05-12 14:53:18,071]: Min: 0.95057446
[2025-05-12 14:53:18,071]: Max: 1.06253755
[2025-05-12 14:53:18,075]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 14:53:18,077]: Sample Values (25 elements): [-0.036649905145168304, -0.07329981029033661, -0.07329981029033661, 0.036649905145168304, -0.036649905145168304, 0.0, 0.0, -0.07329981029033661, 0.0, 0.036649905145168304, 0.0, 0.0, 0.0, -0.07329981029033661, -0.036649905145168304, -0.036649905145168304, 0.0, -0.036649905145168304, 0.0, -0.07329981029033661, 0.07329981029033661, 0.0, 0.036649905145168304, 0.036649905145168304, 0.0]
[2025-05-12 14:53:18,077]: Mean: 0.00062634
[2025-05-12 14:53:18,078]: Min: -0.10994972
[2025-05-12 14:53:18,078]: Max: 0.14659962
[2025-05-12 14:53:18,078]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,080]: Sample Values (25 elements): [1.0122398138046265, 0.9728679656982422, 0.9652711153030396, 1.0138243436813354, 0.964238703250885, 0.971069872379303, 0.956790566444397, 1.0021129846572876, 0.9979647397994995, 0.9731936454772949, 0.9651492834091187, 0.9739338159561157, 0.9847961664199829, 1.01920747756958, 0.9966592788696289, 0.9541454315185547, 0.9924471974372864, 0.979360044002533, 0.9680015444755554, 1.0266640186309814, 0.9581031799316406, 1.006399154663086, 0.9745288491249084, 0.9582245349884033, 1.0330108404159546]
[2025-05-12 14:53:18,081]: Mean: 0.98381972
[2025-05-12 14:53:18,081]: Min: 0.95414543
[2025-05-12 14:53:18,082]: Max: 1.03973866
[2025-05-12 14:53:18,086]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:53:18,088]: Sample Values (25 elements): [0.0, 0.03342997282743454, 0.03342997282743454, -0.03342997282743454, -0.03342997282743454, -0.03342997282743454, 0.06685994565486908, 0.03342997282743454, 0.0, 0.03342997282743454, 0.06685994565486908, 0.03342997282743454, -0.06685994565486908, 0.0, -0.03342997282743454, -0.06685994565486908, 0.0, -0.06685994565486908, 0.0, -0.03342997282743454, 0.0, -0.03342997282743454, 0.03342997282743454, 0.0, 0.0]
[2025-05-12 14:53:18,089]: Mean: -0.00079712
[2025-05-12 14:53:18,089]: Min: -0.10028992
[2025-05-12 14:53:18,090]: Max: 0.13371989
[2025-05-12 14:53:18,090]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,091]: Sample Values (25 elements): [1.0580010414123535, 1.0018242597579956, 1.0082823038101196, 1.0177385807037354, 0.9826022982597351, 1.0417693853378296, 1.0082942247390747, 1.0025482177734375, 1.0194326639175415, 1.0423409938812256, 0.9783185124397278, 1.040783405303955, 1.0325459241867065, 0.9909399747848511, 1.0457472801208496, 1.0026415586471558, 1.0472759008407593, 1.00465726852417, 0.9957122206687927, 1.0111130475997925, 1.0216937065124512, 1.0423833131790161, 1.033196210861206, 1.0117518901824951, 1.066286325454712]
[2025-05-12 14:53:18,091]: Mean: 1.01951981
[2025-05-12 14:53:18,092]: Min: 0.95696509
[2025-05-12 14:53:18,092]: Max: 1.10101473
[2025-05-12 14:53:18,095]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 14:53:18,096]: Sample Values (25 elements): [0.0, 0.06556975096464157, 0.06556975096464157, 0.13113950192928314, 0.06556975096464157, 0.06556975096464157, 0.0, -0.06556975096464157, 0.19670924544334412, 0.06556975096464157, 0.0, 0.06556975096464157, -0.13113950192928314, 0.06556975096464157, -0.06556975096464157, 0.13113950192928314, -0.06556975096464157, 0.13113950192928314, 0.06556975096464157, -0.13113950192928314, 0.06556975096464157, 0.0, 0.19670924544334412, 0.0, 0.06556975096464157]
[2025-05-12 14:53:18,097]: Mean: 0.00262535
[2025-05-12 14:53:18,097]: Min: -0.19670925
[2025-05-12 14:53:18,098]: Max: 0.26227900
[2025-05-12 14:53:18,098]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,099]: Sample Values (25 elements): [0.908464252948761, 0.909159779548645, 0.9091575145721436, 0.9503518342971802, 0.8918862342834473, 0.9031605124473572, 0.9257065057754517, 0.954658567905426, 0.8643421530723572, 0.9025639295578003, 0.9007779359817505, 0.9225142598152161, 0.8608292937278748, 0.8902866840362549, 0.8898741602897644, 0.9133361577987671, 0.8760427236557007, 0.9246438145637512, 0.9177196621894836, 0.9167799949645996, 0.9251987338066101, 0.9253562092781067, 0.9094716310501099, 0.8826942443847656, 0.9415330290794373]
[2025-05-12 14:53:18,099]: Mean: 0.90456700
[2025-05-12 14:53:18,100]: Min: 0.84382212
[2025-05-12 14:53:18,100]: Max: 0.95465857
[2025-05-12 14:53:18,103]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:53:18,104]: Sample Values (25 elements): [0.033129751682281494, 0.0, 0.0, 0.033129751682281494, 0.033129751682281494, -0.033129751682281494, -0.033129751682281494, -0.033129751682281494, 0.0, -0.033129751682281494, 0.0, 0.0, 0.0, 0.0, -0.033129751682281494, -0.033129751682281494, 0.0, 0.033129751682281494, 0.033129751682281494, -0.033129751682281494, -0.033129751682281494, -0.033129751682281494, 0.0, 0.033129751682281494, 0.033129751682281494]
[2025-05-12 14:53:18,105]: Mean: -0.00072166
[2025-05-12 14:53:18,105]: Min: -0.13251901
[2025-05-12 14:53:18,106]: Max: 0.09938926
[2025-05-12 14:53:18,106]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,107]: Sample Values (25 elements): [0.9987591505050659, 0.9850743412971497, 1.036448359489441, 0.948227047920227, 0.9725509881973267, 0.9730870127677917, 0.9870123267173767, 0.9725750088691711, 0.9997884631156921, 0.9791327714920044, 0.9736990928649902, 1.0027490854263306, 0.9975684285163879, 0.9713963866233826, 0.9668532609939575, 0.9749521017074585, 0.9898029565811157, 0.9826947450637817, 0.9742663502693176, 0.9712624549865723, 0.9839882850646973, 0.9614800214767456, 1.0156069993972778, 1.0165470838546753, 0.9700797200202942]
[2025-05-12 14:53:18,107]: Mean: 0.98146349
[2025-05-12 14:53:18,108]: Min: 0.94087917
[2025-05-12 14:53:18,109]: Max: 1.03644836
[2025-05-12 14:53:18,111]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:53:18,113]: Sample Values (25 elements): [0.05333392694592476, -0.02666696347296238, -0.02666696347296238, -0.02666696347296238, 0.0, 0.02666696347296238, -0.02666696347296238, -0.02666696347296238, 0.0, 0.02666696347296238, -0.05333392694592476, -0.02666696347296238, 0.05333392694592476, 0.02666696347296238, 0.0, 0.0, 0.05333392694592476, 0.02666696347296238, 0.0, 0.0, -0.02666696347296238, 0.05333392694592476, -0.02666696347296238, 0.0, 0.02666696347296238]
[2025-05-12 14:53:18,113]: Mean: 0.00044416
[2025-05-12 14:53:18,114]: Min: -0.08000089
[2025-05-12 14:53:18,114]: Max: 0.10666785
[2025-05-12 14:53:18,114]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,115]: Sample Values (25 elements): [1.0202592611312866, 1.0074912309646606, 0.9870486259460449, 1.0376664400100708, 1.0329035520553589, 1.0197532176971436, 0.9921393990516663, 1.0026745796203613, 1.0104964971542358, 1.0472278594970703, 1.0217044353485107, 1.0220445394515991, 1.0233227014541626, 1.0695557594299316, 1.040857195854187, 1.0048953294754028, 0.9924272894859314, 1.0160822868347168, 1.0523529052734375, 1.0284024477005005, 0.994670033454895, 1.0236294269561768, 1.0556179285049438, 1.0300025939941406, 1.0309386253356934]
[2025-05-12 14:53:18,116]: Mean: 1.02195799
[2025-05-12 14:53:18,117]: Min: 0.98704863
[2025-05-12 14:53:18,117]: Max: 1.06955576
[2025-05-12 14:53:18,122]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:53:18,123]: Sample Values (25 elements): [0.054934460669755936, 0.0, 0.054934460669755936, 0.027467230334877968, 0.027467230334877968, 0.027467230334877968, 0.027467230334877968, 0.0, -0.027467230334877968, -0.027467230334877968, -0.027467230334877968, 0.0, 0.0, 0.027467230334877968, -0.027467230334877968, 0.027467230334877968, 0.027467230334877968, 0.027467230334877968, 0.0, -0.027467230334877968, 0.0, 0.0, 0.054934460669755936, 0.027467230334877968, 0.054934460669755936]
[2025-05-12 14:53:18,125]: Mean: -0.00054392
[2025-05-12 14:53:18,126]: Min: -0.08240169
[2025-05-12 14:53:18,126]: Max: 0.10986892
[2025-05-12 14:53:18,126]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,127]: Sample Values (25 elements): [0.9748319387435913, 0.9607973694801331, 0.9692527055740356, 0.9687698483467102, 0.9769835472106934, 0.9566389322280884, 0.9455040097236633, 0.9654173851013184, 0.9708905220031738, 0.9581708908081055, 0.9568382501602173, 0.9704991579055786, 0.9521850943565369, 0.9503915905952454, 0.9576160311698914, 0.9485697746276855, 0.9594161510467529, 0.9592615962028503, 0.9440404772758484, 0.977116048336029, 0.9745449423789978, 0.9604358077049255, 0.951076865196228, 0.9828062653541565, 0.9676712155342102]
[2025-05-12 14:53:18,128]: Mean: 0.96363109
[2025-05-12 14:53:18,129]: Min: 0.94404048
[2025-05-12 14:53:18,129]: Max: 0.99163711
[2025-05-12 14:53:18,137]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:53:18,139]: Sample Values (25 elements): [-0.041692983359098434, 0.020846491679549217, -0.020846491679549217, 0.0, 0.0, 0.0, 0.0, 0.0, -0.041692983359098434, 0.020846491679549217, 0.0, -0.020846491679549217, -0.020846491679549217, -0.041692983359098434, -0.020846491679549217, -0.020846491679549217, -0.041692983359098434, 0.0, 0.0, 0.0, 0.020846491679549217, 0.0, 0.0, 0.0, -0.0625394731760025]
[2025-05-12 14:53:18,141]: Mean: 0.00023525
[2025-05-12 14:53:18,142]: Min: -0.06253947
[2025-05-12 14:53:18,144]: Max: 0.08338597
[2025-05-12 14:53:18,144]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:53:18,147]: Sample Values (25 elements): [1.0418469905853271, 1.0436440706253052, 1.0550607442855835, 1.0506184101104736, 1.0811102390289307, 1.0324233770370483, 1.083439588546753, 1.0712387561798096, 1.0412845611572266, 1.0501410961151123, 1.073890209197998, 1.1068902015686035, 1.0379393100738525, 1.0355100631713867, 1.0917271375656128, 1.0444177389144897, 1.079872727394104, 1.0901505947113037, 1.0706366300582886, 1.0767134428024292, 1.0809375047683716, 1.0497652292251587, 1.0555546283721924, 1.0712757110595703, 1.0499669313430786]
[2025-05-12 14:53:18,150]: Mean: 1.05780387
[2025-05-12 14:53:18,153]: Min: 1.01184213
[2025-05-12 14:53:18,153]: Max: 1.10689020
[2025-05-12 14:53:18,153]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 14:53:18,157]: Sample Values (25 elements): [0.35262176394462585, 0.3285525441169739, 0.07802924513816833, 0.15625998377799988, -0.09411084651947021, -0.27022239565849304, 0.25505390763282776, -0.188774973154068, -0.08980642259120941, 0.5200018882751465, -0.12346072494983673, 0.36045029759407043, -0.010871537029743195, -0.08538085222244263, -0.22408892214298248, -0.1331373155117035, 0.004767156206071377, 0.28585800528526306, -0.21336877346038818, -0.16240954399108887, -0.15925483405590057, 0.3117145597934723, -0.2454535812139511, 0.2752213478088379, 0.1863192915916443]
[2025-05-12 14:53:18,161]: Mean: -0.00359616
[2025-05-12 14:53:18,172]: Min: -0.39474228
[2025-05-12 14:53:18,182]: Max: 0.58537847
[2025-05-12 14:53:18,182]: 


QAT of ResNet20 with relu6 down to 2 bits...
[2025-05-12 14:53:18,700]: [ResNet20_relu6_quantized_2_bits] after configure_qat:
[2025-05-12 14:53:18,852]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 14:55:20,564]: [ResNet20_relu6_quantized_2_bits] Epoch: 001 Train Loss: 1.6173 Train Acc: 0.4245 Eval Loss: 1.3740 Eval Acc: 0.5119 (LR: 0.001000)
[2025-05-12 14:57:14,516]: [ResNet20_relu6_quantized_2_bits] Epoch: 002 Train Loss: 1.2743 Train Acc: 0.5420 Eval Loss: 1.4783 Eval Acc: 0.4960 (LR: 0.001000)
[2025-05-12 14:59:09,956]: [ResNet20_relu6_quantized_2_bits] Epoch: 003 Train Loss: 1.1661 Train Acc: 0.5836 Eval Loss: 1.2172 Eval Acc: 0.5840 (LR: 0.001000)
[2025-05-12 15:01:06,212]: [ResNet20_relu6_quantized_2_bits] Epoch: 004 Train Loss: 1.0914 Train Acc: 0.6118 Eval Loss: 1.4571 Eval Acc: 0.5155 (LR: 0.001000)
[2025-05-12 15:03:01,774]: [ResNet20_relu6_quantized_2_bits] Epoch: 005 Train Loss: 1.0382 Train Acc: 0.6319 Eval Loss: 1.1566 Eval Acc: 0.5984 (LR: 0.001000)
[2025-05-12 15:04:56,603]: [ResNet20_relu6_quantized_2_bits] Epoch: 006 Train Loss: 1.0064 Train Acc: 0.6430 Eval Loss: 1.0820 Eval Acc: 0.6306 (LR: 0.001000)
[2025-05-12 15:06:50,539]: [ResNet20_relu6_quantized_2_bits] Epoch: 007 Train Loss: 0.9839 Train Acc: 0.6512 Eval Loss: 1.0866 Eval Acc: 0.6132 (LR: 0.001000)
[2025-05-12 15:08:44,647]: [ResNet20_relu6_quantized_2_bits] Epoch: 008 Train Loss: 0.9555 Train Acc: 0.6614 Eval Loss: 1.0309 Eval Acc: 0.6389 (LR: 0.001000)
[2025-05-12 15:10:39,784]: [ResNet20_relu6_quantized_2_bits] Epoch: 009 Train Loss: 0.9388 Train Acc: 0.6681 Eval Loss: 0.9434 Eval Acc: 0.6669 (LR: 0.001000)
[2025-05-12 15:12:35,619]: [ResNet20_relu6_quantized_2_bits] Epoch: 010 Train Loss: 0.9206 Train Acc: 0.6748 Eval Loss: 0.9668 Eval Acc: 0.6651 (LR: 0.001000)
[2025-05-12 15:14:28,379]: [ResNet20_relu6_quantized_2_bits] Epoch: 011 Train Loss: 0.9067 Train Acc: 0.6817 Eval Loss: 1.2528 Eval Acc: 0.5986 (LR: 0.001000)
[2025-05-12 15:16:21,548]: [ResNet20_relu6_quantized_2_bits] Epoch: 012 Train Loss: 0.8921 Train Acc: 0.6859 Eval Loss: 1.0332 Eval Acc: 0.6416 (LR: 0.001000)
[2025-05-12 15:18:15,116]: [ResNet20_relu6_quantized_2_bits] Epoch: 013 Train Loss: 0.8769 Train Acc: 0.6918 Eval Loss: 1.0509 Eval Acc: 0.6504 (LR: 0.001000)
[2025-05-12 15:20:02,869]: [ResNet20_relu6_quantized_2_bits] Epoch: 014 Train Loss: 0.8786 Train Acc: 0.6903 Eval Loss: 1.0700 Eval Acc: 0.6414 (LR: 0.001000)
[2025-05-12 15:21:43,311]: [ResNet20_relu6_quantized_2_bits] Epoch: 015 Train Loss: 0.8605 Train Acc: 0.6983 Eval Loss: 1.0443 Eval Acc: 0.6432 (LR: 0.001000)
[2025-05-12 15:23:22,572]: [ResNet20_relu6_quantized_2_bits] Epoch: 016 Train Loss: 0.8509 Train Acc: 0.7012 Eval Loss: 1.1692 Eval Acc: 0.6204 (LR: 0.001000)
[2025-05-12 15:25:00,930]: [ResNet20_relu6_quantized_2_bits] Epoch: 017 Train Loss: 0.8472 Train Acc: 0.7043 Eval Loss: 1.1378 Eval Acc: 0.6376 (LR: 0.001000)
[2025-05-12 15:26:43,236]: [ResNet20_relu6_quantized_2_bits] Epoch: 018 Train Loss: 0.8385 Train Acc: 0.7056 Eval Loss: 0.9416 Eval Acc: 0.6770 (LR: 0.001000)
[2025-05-12 15:28:28,961]: [ResNet20_relu6_quantized_2_bits] Epoch: 019 Train Loss: 0.8282 Train Acc: 0.7091 Eval Loss: 0.8537 Eval Acc: 0.7094 (LR: 0.001000)
[2025-05-12 15:30:14,107]: [ResNet20_relu6_quantized_2_bits] Epoch: 020 Train Loss: 0.8247 Train Acc: 0.7098 Eval Loss: 0.8274 Eval Acc: 0.7109 (LR: 0.001000)
[2025-05-12 15:32:01,468]: [ResNet20_relu6_quantized_2_bits] Epoch: 021 Train Loss: 0.8251 Train Acc: 0.7108 Eval Loss: 0.9166 Eval Acc: 0.6834 (LR: 0.001000)
[2025-05-12 15:33:48,453]: [ResNet20_relu6_quantized_2_bits] Epoch: 022 Train Loss: 0.8156 Train Acc: 0.7152 Eval Loss: 0.9801 Eval Acc: 0.6635 (LR: 0.001000)
[2025-05-12 15:35:35,565]: [ResNet20_relu6_quantized_2_bits] Epoch: 023 Train Loss: 0.8135 Train Acc: 0.7153 Eval Loss: 0.9734 Eval Acc: 0.6700 (LR: 0.001000)
[2025-05-12 15:37:21,602]: [ResNet20_relu6_quantized_2_bits] Epoch: 024 Train Loss: 0.8066 Train Acc: 0.7179 Eval Loss: 0.9848 Eval Acc: 0.6585 (LR: 0.001000)
[2025-05-12 15:39:05,482]: [ResNet20_relu6_quantized_2_bits] Epoch: 025 Train Loss: 0.7987 Train Acc: 0.7212 Eval Loss: 0.9404 Eval Acc: 0.6828 (LR: 0.001000)
[2025-05-12 15:40:50,765]: [ResNet20_relu6_quantized_2_bits] Epoch: 026 Train Loss: 0.7950 Train Acc: 0.7210 Eval Loss: 0.7895 Eval Acc: 0.7257 (LR: 0.001000)
[2025-05-12 15:42:37,278]: [ResNet20_relu6_quantized_2_bits] Epoch: 027 Train Loss: 0.7925 Train Acc: 0.7229 Eval Loss: 0.9447 Eval Acc: 0.6829 (LR: 0.001000)
[2025-05-12 15:44:25,085]: [ResNet20_relu6_quantized_2_bits] Epoch: 028 Train Loss: 0.7898 Train Acc: 0.7239 Eval Loss: 0.8551 Eval Acc: 0.7152 (LR: 0.001000)
[2025-05-12 15:46:12,483]: [ResNet20_relu6_quantized_2_bits] Epoch: 029 Train Loss: 0.7812 Train Acc: 0.7270 Eval Loss: 0.8329 Eval Acc: 0.7143 (LR: 0.001000)
[2025-05-12 15:47:58,463]: [ResNet20_relu6_quantized_2_bits] Epoch: 030 Train Loss: 0.7729 Train Acc: 0.7310 Eval Loss: 0.8593 Eval Acc: 0.7025 (LR: 0.000250)
[2025-05-12 15:49:50,368]: [ResNet20_relu6_quantized_2_bits] Epoch: 031 Train Loss: 0.7401 Train Acc: 0.7412 Eval Loss: 0.8278 Eval Acc: 0.7243 (LR: 0.000250)
[2025-05-12 15:51:42,933]: [ResNet20_relu6_quantized_2_bits] Epoch: 032 Train Loss: 0.7371 Train Acc: 0.7417 Eval Loss: 0.7898 Eval Acc: 0.7247 (LR: 0.000250)
[2025-05-12 15:53:40,807]: [ResNet20_relu6_quantized_2_bits] Epoch: 033 Train Loss: 0.7352 Train Acc: 0.7402 Eval Loss: 0.8136 Eval Acc: 0.7235 (LR: 0.000250)
[2025-05-12 15:55:40,622]: [ResNet20_relu6_quantized_2_bits] Epoch: 034 Train Loss: 0.7340 Train Acc: 0.7417 Eval Loss: 0.7386 Eval Acc: 0.7452 (LR: 0.000250)
[2025-05-12 15:57:39,934]: [ResNet20_relu6_quantized_2_bits] Epoch: 035 Train Loss: 0.7351 Train Acc: 0.7433 Eval Loss: 0.7673 Eval Acc: 0.7353 (LR: 0.000250)
[2025-05-12 15:59:41,387]: [ResNet20_relu6_quantized_2_bits] Epoch: 036 Train Loss: 0.7321 Train Acc: 0.7428 Eval Loss: 0.8896 Eval Acc: 0.6987 (LR: 0.000250)
[2025-05-12 16:01:41,121]: [ResNet20_relu6_quantized_2_bits] Epoch: 037 Train Loss: 0.7345 Train Acc: 0.7423 Eval Loss: 0.7604 Eval Acc: 0.7375 (LR: 0.000250)
[2025-05-12 16:03:39,927]: [ResNet20_relu6_quantized_2_bits] Epoch: 038 Train Loss: 0.7396 Train Acc: 0.7406 Eval Loss: 0.7753 Eval Acc: 0.7334 (LR: 0.000250)
[2025-05-12 16:05:36,076]: [ResNet20_relu6_quantized_2_bits] Epoch: 039 Train Loss: 0.7314 Train Acc: 0.7458 Eval Loss: 0.7541 Eval Acc: 0.7459 (LR: 0.000250)
[2025-05-12 16:07:29,054]: [ResNet20_relu6_quantized_2_bits] Epoch: 040 Train Loss: 0.7380 Train Acc: 0.7405 Eval Loss: 0.7915 Eval Acc: 0.7275 (LR: 0.000250)
[2025-05-12 16:09:21,295]: [ResNet20_relu6_quantized_2_bits] Epoch: 041 Train Loss: 0.7279 Train Acc: 0.7462 Eval Loss: 0.7580 Eval Acc: 0.7395 (LR: 0.000250)
[2025-05-12 16:11:13,885]: [ResNet20_relu6_quantized_2_bits] Epoch: 042 Train Loss: 0.7331 Train Acc: 0.7422 Eval Loss: 0.9675 Eval Acc: 0.6791 (LR: 0.000250)
[2025-05-12 16:13:07,232]: [ResNet20_relu6_quantized_2_bits] Epoch: 043 Train Loss: 0.7349 Train Acc: 0.7451 Eval Loss: 0.7805 Eval Acc: 0.7329 (LR: 0.000250)
[2025-05-12 16:15:01,144]: [ResNet20_relu6_quantized_2_bits] Epoch: 044 Train Loss: 0.7307 Train Acc: 0.7458 Eval Loss: 0.8302 Eval Acc: 0.7168 (LR: 0.000250)
[2025-05-12 16:16:53,841]: [ResNet20_relu6_quantized_2_bits] Epoch: 045 Train Loss: 0.7341 Train Acc: 0.7434 Eval Loss: 0.8137 Eval Acc: 0.7240 (LR: 0.000063)
[2025-05-12 16:18:41,677]: [ResNet20_relu6_quantized_2_bits] Epoch: 046 Train Loss: 0.7078 Train Acc: 0.7530 Eval Loss: 0.7697 Eval Acc: 0.7329 (LR: 0.000063)
[2025-05-12 16:20:29,030]: [ResNet20_relu6_quantized_2_bits] Epoch: 047 Train Loss: 0.7095 Train Acc: 0.7510 Eval Loss: 0.7144 Eval Acc: 0.7508 (LR: 0.000063)
[2025-05-12 16:22:17,477]: [ResNet20_relu6_quantized_2_bits] Epoch: 048 Train Loss: 0.7111 Train Acc: 0.7525 Eval Loss: 0.7963 Eval Acc: 0.7299 (LR: 0.000063)
[2025-05-12 16:24:05,593]: [ResNet20_relu6_quantized_2_bits] Epoch: 049 Train Loss: 0.7100 Train Acc: 0.7514 Eval Loss: 0.6987 Eval Acc: 0.7625 (LR: 0.000063)
[2025-05-12 16:25:55,243]: [ResNet20_relu6_quantized_2_bits] Epoch: 050 Train Loss: 0.7107 Train Acc: 0.7518 Eval Loss: 0.7078 Eval Acc: 0.7555 (LR: 0.000063)
[2025-05-12 16:27:48,489]: [ResNet20_relu6_quantized_2_bits] Epoch: 051 Train Loss: 0.7077 Train Acc: 0.7518 Eval Loss: 0.7687 Eval Acc: 0.7401 (LR: 0.000063)
[2025-05-12 16:29:42,742]: [ResNet20_relu6_quantized_2_bits] Epoch: 052 Train Loss: 0.7191 Train Acc: 0.7481 Eval Loss: 0.8078 Eval Acc: 0.7298 (LR: 0.000063)
[2025-05-12 16:31:34,350]: [ResNet20_relu6_quantized_2_bits] Epoch: 053 Train Loss: 0.7135 Train Acc: 0.7511 Eval Loss: 0.7373 Eval Acc: 0.7493 (LR: 0.000063)
[2025-05-12 16:33:20,078]: [ResNet20_relu6_quantized_2_bits] Epoch: 054 Train Loss: 0.7134 Train Acc: 0.7507 Eval Loss: 0.7337 Eval Acc: 0.7461 (LR: 0.000063)
[2025-05-12 16:34:58,166]: [ResNet20_relu6_quantized_2_bits] Epoch: 055 Train Loss: 0.7201 Train Acc: 0.7466 Eval Loss: 0.8253 Eval Acc: 0.7226 (LR: 0.000063)
[2025-05-12 16:36:30,378]: [ResNet20_relu6_quantized_2_bits] Epoch: 056 Train Loss: 0.7090 Train Acc: 0.7493 Eval Loss: 0.7692 Eval Acc: 0.7340 (LR: 0.000063)
[2025-05-12 16:38:07,645]: [ResNet20_relu6_quantized_2_bits] Epoch: 057 Train Loss: 0.7106 Train Acc: 0.7538 Eval Loss: 0.7465 Eval Acc: 0.7474 (LR: 0.000063)
[2025-05-12 16:40:04,688]: [ResNet20_relu6_quantized_2_bits] Epoch: 058 Train Loss: 0.7111 Train Acc: 0.7491 Eval Loss: 0.7867 Eval Acc: 0.7370 (LR: 0.000063)
[2025-05-12 16:42:38,158]: [ResNet20_relu6_quantized_2_bits] Epoch: 059 Train Loss: 0.7167 Train Acc: 0.7494 Eval Loss: 0.7629 Eval Acc: 0.7408 (LR: 0.000063)
[2025-05-12 16:45:08,377]: [ResNet20_relu6_quantized_2_bits] Epoch: 060 Train Loss: 0.7202 Train Acc: 0.7479 Eval Loss: 0.7717 Eval Acc: 0.7347 (LR: 0.000063)
[2025-05-12 16:45:08,378]: [ResNet20_relu6_quantized_2_bits] Best Eval Accuracy: 0.7625
[2025-05-12 16:45:08,427]: 


Quantization of model down to 2 bits finished
[2025-05-12 16:45:08,428]: Model Architecture:
[2025-05-12 16:45:08,810]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2129], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2952387034893036, max_val=0.34341704845428467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1627], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24641959369182587, max_val=0.24167224764823914)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3126], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47874072194099426, max_val=0.4591763913631439)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1747], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2635575830936432, max_val=0.260592520236969)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1351], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.200885608792305, max_val=0.20426712930202484)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9900], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.970036506652832)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1381], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18069137632846832, max_val=0.23359090089797974)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1151], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17271995544433594, max_val=0.17271621525287628)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8440], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.531877040863037)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1065], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15647223591804504, max_val=0.16311907768249512)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2407], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3611006736755371, max_val=0.36106353998184204)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1247], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16326531767845154, max_val=0.21075311303138733)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1064], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15668712556362152, max_val=0.1626596748828888)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0948], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14196641743183136, max_val=0.14243341982364655)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9970], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.990982532501221)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0921], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13816134631633759, max_val=0.1380503624677658)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0891], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12767066061496735, max_val=0.1397775411605835)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9837], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.951117038726807)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0773], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10826247185468674, max_val=0.12357985228300095)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1615], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24209590256214142, max_val=0.24225428700447083)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0768], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11378073692321777, max_val=0.11656013131141663)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.999856472015381)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0633], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09114820510149002, max_val=0.09888886660337448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0626], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09394970536231995, max_val=0.09397129714488983)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9969], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.990776538848877)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0486], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07287346571683884, max_val=0.07292710989713669)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 16:45:08,811]: 
Model Weights:
[2025-05-12 16:45:08,811]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 16:45:08,812]: Sample Values (25 elements): [-0.0037931802216917276, -0.19893108308315277, 0.03942067548632622, 0.1729782074689865, 0.2602582573890686, -0.08584137260913849, 0.03313884884119034, 0.10912736505270004, 0.1314515769481659, 0.16019746661186218, -0.12188305705785751, 0.07147259265184402, -0.24607804417610168, 0.21168695390224457, 0.03759453445672989, 0.2749893069267273, 0.05230821669101715, 0.1017395555973053, -0.24579595029354095, 0.010921599343419075, -0.17835351824760437, 0.15145035088062286, 0.12436280399560928, -0.011202686466276646, -0.16676852107048035]
[2025-05-12 16:45:08,813]: Mean: -0.00160703
[2025-05-12 16:45:08,814]: Min: -0.47652882
[2025-05-12 16:45:08,815]: Max: 0.50309169
[2025-05-12 16:45:08,815]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 16:45:08,819]: Sample Values (16 elements): [0.9565341472625732, 1.2911386489868164, 1.4359551668167114, 1.2218666076660156, 1.137398600578308, 1.0306063890457153, 1.220329999923706, 1.184169054031372, 1.1424373388290405, 1.2967712879180908, 1.082444429397583, 1.496923804283142, 1.4390298128128052, 1.3026126623153687, 1.2395360469818115, 1.330877661705017]
[2025-05-12 16:45:08,822]: Mean: 1.23803949
[2025-05-12 16:45:08,823]: Min: 0.95653415
[2025-05-12 16:45:08,823]: Max: 1.49692380
[2025-05-12 16:45:08,837]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:45:08,842]: Sample Values (25 elements): [0.0, 0.0, -0.21288537979125977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.21288537979125977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:45:08,846]: Mean: -0.00526670
[2025-05-12 16:45:08,850]: Min: -0.21288538
[2025-05-12 16:45:08,851]: Max: 0.42577076
[2025-05-12 16:45:08,852]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:45:08,857]: Sample Values (16 elements): [0.9877581596374512, 1.110322117805481, 1.6420117616653442, 1.014811396598816, 1.0143648386001587, 1.1783554553985596, 0.9673365354537964, 1.0560612678527832, 1.1893826723098755, 1.0945411920547485, 0.9856811761856079, 1.1106230020523071, 1.0816417932510376, 1.0498687028884888, 0.8776038885116577, 0.946729302406311]
[2025-05-12 16:45:08,858]: Mean: 1.08169341
[2025-05-12 16:45:08,858]: Min: 0.87760389
[2025-05-12 16:45:08,858]: Max: 1.64201176
[2025-05-12 16:45:08,874]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:45:08,881]: Sample Values (25 elements): [0.0, 0.0, -0.16269735991954803, 0.0, 0.0, 0.16269735991954803, 0.0, 0.0, -0.16269735991954803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.16269735991954803, 0.0, 0.0, 0.0, -0.16269735991954803, 0.16269735991954803, 0.0, 0.0, 0.0]
[2025-05-12 16:45:08,882]: Mean: -0.00550798
[2025-05-12 16:45:08,884]: Min: -0.32539472
[2025-05-12 16:45:08,884]: Max: 0.16269736
[2025-05-12 16:45:08,884]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:45:08,886]: Sample Values (16 elements): [1.014898419380188, 1.1482853889465332, 0.9913643598556519, 1.088176965713501, 0.9022670388221741, 1.0527275800704956, 1.0222220420837402, 1.0440027713775635, 1.0072981119155884, 1.1516586542129517, 1.096558690071106, 1.1255813837051392, 1.0672708749771118, 1.0618703365325928, 0.9273782968521118, 0.9291595816612244]
[2025-05-12 16:45:08,887]: Mean: 1.03941989
[2025-05-12 16:45:08,901]: Min: 0.90226704
[2025-05-12 16:45:08,901]: Max: 1.15165865
[2025-05-12 16:45:08,923]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:45:08,926]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31263917684555054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.31263917684555054, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:45:08,932]: Mean: 0.00067847
[2025-05-12 16:45:08,937]: Min: -0.62527835
[2025-05-12 16:45:08,938]: Max: 0.31263918
[2025-05-12 16:45:08,939]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:45:08,946]: Sample Values (16 elements): [1.0267082452774048, 0.8803896307945251, 1.1950695514678955, 1.1665054559707642, 1.0145448446273804, 0.9615642428398132, 1.0053521394729614, 0.9856341481208801, 0.9660167694091797, 0.9889816045761108, 1.023589015007019, 0.985907256603241, 1.3413641452789307, 1.2672473192214966, 1.0204373598098755, 0.9647179245948792]
[2025-05-12 16:45:08,947]: Mean: 1.04962683
[2025-05-12 16:45:08,951]: Min: 0.88038963
[2025-05-12 16:45:08,955]: Max: 1.34136415
[2025-05-12 16:45:08,960]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:45:08,961]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.1747168004512787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1747168004512787, 0.0, -0.1747168004512787, 0.0, -0.1747168004512787]
[2025-05-12 16:45:08,961]: Mean: -0.00030333
[2025-05-12 16:45:08,963]: Min: -0.34943360
[2025-05-12 16:45:08,968]: Max: 0.17471680
[2025-05-12 16:45:08,968]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:45:08,970]: Sample Values (16 elements): [0.9443220496177673, 0.9062734842300415, 1.160239815711975, 0.9621696472167969, 0.8668181896209717, 1.0130233764648438, 1.270143985748291, 0.9179564714431763, 1.0047568082809448, 0.888271152973175, 1.1104251146316528, 0.9694963693618774, 1.0664571523666382, 0.8779752254486084, 1.027474284172058, 0.9413263201713562]
[2025-05-12 16:45:08,971]: Mean: 0.99544555
[2025-05-12 16:45:08,972]: Min: 0.86681819
[2025-05-12 16:45:08,973]: Max: 1.27014399
[2025-05-12 16:45:08,979]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:45:08,980]: Sample Values (25 elements): [0.0, 0.0, -0.13505099713802338, 0.0, 0.0, 0.13505099713802338, 0.0, -0.13505099713802338, 0.0, 0.0, 0.13505099713802338, -0.13505099713802338, 0.0, 0.0, 0.13505099713802338, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13505099713802338, 0.0, -0.13505099713802338, 0.0, 0.13505099713802338]
[2025-05-12 16:45:08,982]: Mean: -0.00345834
[2025-05-12 16:45:08,984]: Min: -0.13505100
[2025-05-12 16:45:08,986]: Max: 0.27010199
[2025-05-12 16:45:08,986]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:45:08,989]: Sample Values (16 elements): [1.0480926036834717, 1.033898949623108, 1.0548518896102905, 1.0407665967941284, 1.0804479122161865, 0.9812271595001221, 0.9764309525489807, 1.1549755334854126, 1.0408211946487427, 1.0897780656814575, 0.953108549118042, 1.0552396774291992, 1.1159178018569946, 0.9855024814605713, 0.9599658846855164, 1.0212488174438477]
[2025-05-12 16:45:08,990]: Mean: 1.03701711
[2025-05-12 16:45:08,993]: Min: 0.95310855
[2025-05-12 16:45:08,995]: Max: 1.15497553
[2025-05-12 16:45:09,013]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:45:09,024]: Sample Values (25 elements): [0.13809415698051453, 0.0, 0.0, 0.0, -0.13809415698051453, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13809415698051453, 0.0, 0.13809415698051453, 0.0, 0.0, 0.13809415698051453, 0.0, 0.0, 0.0, 0.0, 0.13809415698051453, 0.0, 0.13809415698051453, -0.13809415698051453, 0.0]
[2025-05-12 16:45:09,029]: Mean: 0.00431544
[2025-05-12 16:45:09,030]: Min: -0.13809416
[2025-05-12 16:45:09,031]: Max: 0.27618831
[2025-05-12 16:45:09,031]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:45:09,031]: Sample Values (16 elements): [1.0291757583618164, 0.9955873489379883, 1.0558242797851562, 1.0272111892700195, 0.9813655018806458, 0.9952994585037231, 0.9981411099433899, 1.0308679342269897, 0.9871813654899597, 0.8942387700080872, 1.2117557525634766, 0.9974634647369385, 1.1816203594207764, 1.0984467267990112, 1.0042908191680908, 1.0054140090942383]
[2025-05-12 16:45:09,032]: Mean: 1.03086770
[2025-05-12 16:45:09,032]: Min: 0.89423877
[2025-05-12 16:45:09,033]: Max: 1.21175575
[2025-05-12 16:45:09,046]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 16:45:09,047]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11514543741941452, 0.11514543741941452, 0.0, 0.11514543741941452, 0.11514543741941452, 0.0, 0.11514543741941452, 0.0, 0.0, 0.11514543741941452, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11514543741941452, 0.0]
[2025-05-12 16:45:09,049]: Mean: -0.00247383
[2025-05-12 16:45:09,049]: Min: -0.23029087
[2025-05-12 16:45:09,050]: Max: 0.11514544
[2025-05-12 16:45:09,050]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,053]: Sample Values (25 elements): [1.0766278505325317, 1.0483332872390747, 1.0503969192504883, 1.087180495262146, 1.0832310914993286, 1.0270580053329468, 1.0621074438095093, 1.0330127477645874, 1.0961966514587402, 1.0661816596984863, 1.0113040208816528, 0.9585283398628235, 1.0421425104141235, 1.0859224796295166, 0.9868680834770203, 1.070101261138916, 1.031363606452942, 1.034974217414856, 1.1783915758132935, 1.0183753967285156, 1.0047835111618042, 1.0182998180389404, 0.9999086856842041, 1.0507529973983765, 1.07072913646698]
[2025-05-12 16:45:09,055]: Mean: 1.04657376
[2025-05-12 16:45:09,059]: Min: 0.95852834
[2025-05-12 16:45:09,062]: Max: 1.17839158
[2025-05-12 16:45:09,072]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:45:09,078]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.10653048753738403, 0.0, 0.0, -0.10653048753738403, 0.0, 0.10653048753738403, 0.10653048753738403, 0.10653048753738403, 0.0, -0.10653048753738403, 0.0, 0.0, -0.10653048753738403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10653048753738403, -0.10653048753738403]
[2025-05-12 16:45:09,093]: Mean: -0.00394172
[2025-05-12 16:45:09,097]: Min: -0.10653049
[2025-05-12 16:45:09,103]: Max: 0.21306098
[2025-05-12 16:45:09,103]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,104]: Sample Values (25 elements): [1.0075771808624268, 1.0418295860290527, 1.0283087491989136, 1.1013199090957642, 1.1663556098937988, 1.1207165718078613, 1.0898131132125854, 1.0517334938049316, 1.1274492740631104, 1.0573668479919434, 1.1018459796905518, 1.114644169807434, 1.0816893577575684, 1.1329525709152222, 1.09989595413208, 1.0473883152008057, 1.0900120735168457, 1.1017144918441772, 1.0398963689804077, 1.0511646270751953, 1.0413819551467896, 1.0678216218948364, 1.0463634729385376, 0.9993473291397095, 1.022811770439148]
[2025-05-12 16:45:09,105]: Mean: 1.07083702
[2025-05-12 16:45:09,105]: Min: 0.99934733
[2025-05-12 16:45:09,106]: Max: 1.16635561
[2025-05-12 16:45:09,109]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 16:45:09,114]: Sample Values (25 elements): [0.0, -0.24072150886058807, 0.0, 0.24072150886058807, -0.24072150886058807, 0.0, -0.24072150886058807, -0.24072150886058807, 0.24072150886058807, 0.0, 0.0, 0.0, 0.0, -0.24072150886058807, 0.0, -0.24072150886058807, -0.24072150886058807, 0.0, 0.24072150886058807, 0.24072150886058807, -0.24072150886058807, 0.0, 0.24072150886058807, -0.24072150886058807, 0.24072150886058807]
[2025-05-12 16:45:09,116]: Mean: -0.00940318
[2025-05-12 16:45:09,117]: Min: -0.48144302
[2025-05-12 16:45:09,118]: Max: 0.24072151
[2025-05-12 16:45:09,118]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,120]: Sample Values (25 elements): [0.833446741104126, 0.8646638989448547, 0.8724222779273987, 0.8730672001838684, 0.9010785818099976, 0.8609628677368164, 0.8129477500915527, 0.9008219838142395, 0.9082598090171814, 0.9173672795295715, 0.9550800323486328, 0.9021409749984741, 0.8888470530509949, 0.8819054961204529, 0.8226290941238403, 0.8599249124526978, 0.847085177898407, 0.859326958656311, 0.8789162635803223, 0.8407779932022095, 0.9184654951095581, 0.8668172359466553, 0.9054423570632935, 0.8666273355484009, 0.9483705163002014]
[2025-05-12 16:45:09,121]: Mean: 0.88218391
[2025-05-12 16:45:09,122]: Min: 0.81294775
[2025-05-12 16:45:09,123]: Max: 0.95508003
[2025-05-12 16:45:09,130]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:45:09,134]: Sample Values (25 elements): [0.0, -0.12467287480831146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.12467287480831146, 0.12467287480831146, -0.12467287480831146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:45:09,134]: Mean: 0.00037878
[2025-05-12 16:45:09,136]: Min: -0.12467287
[2025-05-12 16:45:09,140]: Max: 0.24934575
[2025-05-12 16:45:09,140]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,145]: Sample Values (25 elements): [1.1298946142196655, 0.9888182878494263, 1.025680422782898, 1.0108200311660767, 1.086745262145996, 1.0297569036483765, 1.093223214149475, 1.073165774345398, 1.0292214155197144, 1.0181881189346313, 1.0269358158111572, 0.9777887463569641, 1.0553770065307617, 1.0558730363845825, 0.9752920866012573, 1.0287740230560303, 1.1052606105804443, 1.0390561819076538, 1.0493069887161255, 1.0373812913894653, 1.0211741924285889, 1.0730260610580444, 0.9871664643287659, 1.029499888420105, 1.0390150547027588]
[2025-05-12 16:45:09,150]: Mean: 1.03025007
[2025-05-12 16:45:09,155]: Min: 0.96772015
[2025-05-12 16:45:09,159]: Max: 1.12989461
[2025-05-12 16:45:09,171]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:45:09,172]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.1064489334821701, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1064489334821701, 0.0, 0.0, 0.0, 0.0, -0.1064489334821701, 0.1064489334821701, 0.0, 0.1064489334821701, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:45:09,172]: Mean: -0.00093559
[2025-05-12 16:45:09,174]: Min: -0.10644893
[2025-05-12 16:45:09,178]: Max: 0.21289787
[2025-05-12 16:45:09,178]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,181]: Sample Values (25 elements): [1.0223666429519653, 0.9659878611564636, 0.9995310306549072, 0.9747588634490967, 1.0601513385772705, 1.0619990825653076, 1.0550209283828735, 0.9965754151344299, 0.9596443772315979, 0.9929977655410767, 1.0239160060882568, 1.0421226024627686, 0.8884453177452087, 1.024307131767273, 1.0459297895431519, 0.9865801930427551, 0.9805808663368225, 1.0037078857421875, 0.9867547154426575, 0.9970277547836304, 1.0492316484451294, 1.0054491758346558, 1.0465977191925049, 0.9711998701095581, 1.0126628875732422]
[2025-05-12 16:45:09,183]: Mean: 1.01575017
[2025-05-12 16:45:09,183]: Min: 0.88844532
[2025-05-12 16:45:09,184]: Max: 1.07931137
[2025-05-12 16:45:09,190]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:45:09,193]: Sample Values (25 elements): [0.0, -0.09480001777410507, 0.0, 0.0, 0.0, -0.09480001777410507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09480001777410507, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09480001777410507, 0.0, 0.09480001777410507, -0.09480001777410507, 0.0, 0.0, 0.0]
[2025-05-12 16:45:09,194]: Mean: -0.00102865
[2025-05-12 16:45:09,196]: Min: -0.09480002
[2025-05-12 16:45:09,199]: Max: 0.18960004
[2025-05-12 16:45:09,199]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,203]: Sample Values (25 elements): [0.9908815026283264, 0.9739117622375488, 1.0124143362045288, 1.0186750888824463, 0.9934526681900024, 1.0051591396331787, 1.0166800022125244, 1.0060274600982666, 1.0143917798995972, 1.0326690673828125, 1.0233687162399292, 0.9877023696899414, 1.0453689098358154, 1.000022292137146, 1.0046969652175903, 0.9534905552864075, 0.9936555624008179, 0.9954532384872437, 1.0036673545837402, 1.0008528232574463, 1.0539690256118774, 1.061367154121399, 1.021044373512268, 1.0053678750991821, 1.0470091104507446]
[2025-05-12 16:45:09,206]: Mean: 1.00872958
[2025-05-12 16:45:09,210]: Min: 0.95349056
[2025-05-12 16:45:09,215]: Max: 1.06136715
[2025-05-12 16:45:09,233]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:45:09,234]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09207047522068024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09207047522068024, 0.0, -0.09207047522068024, 0.0, 0.0, 0.0, 0.09207047522068024]
[2025-05-12 16:45:09,234]: Mean: -0.00047953
[2025-05-12 16:45:09,235]: Min: -0.18414095
[2025-05-12 16:45:09,235]: Max: 0.09207048
[2025-05-12 16:45:09,235]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:45:09,236]: Sample Values (25 elements): [1.0052825212478638, 1.0789730548858643, 0.9906655550003052, 1.0061640739440918, 0.9801554679870605, 1.0161164999008179, 1.0429809093475342, 1.009178638458252, 1.003670573234558, 1.0270119905471802, 0.9834297299385071, 1.0735803842544556, 1.0752800703048706, 1.0352460145950317, 1.0278174877166748, 1.0123592615127563, 1.023881435394287, 1.0149726867675781, 1.005810260772705, 1.0199346542358398, 1.0295380353927612, 1.0017188787460327, 1.0040483474731445, 1.0683594942092896, 1.0852006673812866]
[2025-05-12 16:45:09,237]: Mean: 1.02746224
[2025-05-12 16:45:09,245]: Min: 0.98015547
[2025-05-12 16:45:09,246]: Max: 1.08617055
[2025-05-12 16:45:09,251]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 16:45:09,252]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.08914946764707565, 0.0, 0.0, 0.0, -0.08914946764707565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08914946764707565, 0.08914946764707565]
[2025-05-12 16:45:09,253]: Mean: 0.00127204
[2025-05-12 16:45:09,254]: Min: -0.08914947
[2025-05-12 16:45:09,254]: Max: 0.17829894
[2025-05-12 16:45:09,254]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,259]: Sample Values (25 elements): [1.017754316329956, 1.027249813079834, 1.006581425666809, 1.0201867818832397, 0.999481737613678, 0.9975717663764954, 0.9898574352264404, 1.0293983221054077, 0.9834290742874146, 1.0007327795028687, 1.0166399478912354, 1.031194806098938, 0.9819378852844238, 1.022433876991272, 1.0264242887496948, 1.003382682800293, 0.9850295782089233, 0.9856049418449402, 1.0255229473114014, 0.997417151927948, 1.053997278213501, 1.0087841749191284, 1.0448552370071411, 0.9979386925697327, 0.9904093146324158]
[2025-05-12 16:45:09,261]: Mean: 1.01366687
[2025-05-12 16:45:09,262]: Min: 0.96653098
[2025-05-12 16:45:09,263]: Max: 1.09116328
[2025-05-12 16:45:09,273]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:45:09,278]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0772809237241745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0772809237241745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0772809237241745]
[2025-05-12 16:45:09,279]: Mean: -0.00018448
[2025-05-12 16:45:09,280]: Min: -0.07728092
[2025-05-12 16:45:09,284]: Max: 0.15456185
[2025-05-12 16:45:09,284]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,289]: Sample Values (25 elements): [1.0393434762954712, 1.0365041494369507, 1.021368145942688, 1.0537290573120117, 1.082994818687439, 1.0143990516662598, 1.026796579360962, 1.0492278337478638, 1.0176582336425781, 1.1042219400405884, 1.0570180416107178, 1.0334773063659668, 1.0363606214523315, 0.9909301996231079, 1.0400892496109009, 1.028817892074585, 1.0775038003921509, 1.0654670000076294, 1.0681620836257935, 1.0461868047714233, 1.0465583801269531, 1.0236196517944336, 1.025612473487854, 1.0138232707977295, 1.027207612991333]
[2025-05-12 16:45:09,297]: Mean: 1.03937507
[2025-05-12 16:45:09,298]: Min: 0.96861762
[2025-05-12 16:45:09,298]: Max: 1.10422194
[2025-05-12 16:45:09,302]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 16:45:09,303]: Sample Values (25 elements): [0.16145014762878418, 0.16145014762878418, 0.0, 0.0, 0.0, 0.0, -0.16145014762878418, 0.0, 0.16145014762878418, 0.0, 0.16145014762878418, -0.16145014762878418, 0.0, -0.16145014762878418, 0.16145014762878418, 0.0, 0.0, 0.0, -0.16145014762878418, 0.0, 0.16145014762878418, 0.0, 0.0, -0.16145014762878418, 0.0]
[2025-05-12 16:45:09,305]: Mean: 0.00370515
[2025-05-12 16:45:09,309]: Min: -0.16145015
[2025-05-12 16:45:09,311]: Max: 0.32290030
[2025-05-12 16:45:09,311]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,314]: Sample Values (25 elements): [0.9337616562843323, 0.8843966722488403, 0.896274209022522, 0.9133546948432922, 0.8765588998794556, 0.9188148975372314, 0.9078503251075745, 0.9082254767417908, 0.9388782978057861, 0.9081641435623169, 0.9332431554794312, 0.9355423450469971, 0.9207608103752136, 0.8770381212234497, 0.9264240264892578, 0.90301114320755, 0.9218115210533142, 0.916512668132782, 0.9400284290313721, 0.9371864795684814, 0.9292612671852112, 0.9239060878753662, 0.914686918258667, 0.9046978950500488, 0.9189122319221497]
[2025-05-12 16:45:09,314]: Mean: 0.91305268
[2025-05-12 16:45:09,315]: Min: 0.85528761
[2025-05-12 16:45:09,317]: Max: 0.97208673
[2025-05-12 16:45:09,330]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:45:09,333]: Sample Values (25 elements): [-0.07678034156560898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07678034156560898, -0.07678034156560898, 0.0, -0.07678034156560898, 0.0, -0.07678034156560898, 0.0, 0.0, 0.0, -0.07678034156560898]
[2025-05-12 16:45:09,334]: Mean: -0.00004999
[2025-05-12 16:45:09,336]: Min: -0.07678034
[2025-05-12 16:45:09,340]: Max: 0.15356068
[2025-05-12 16:45:09,340]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,353]: Sample Values (25 elements): [0.9602135419845581, 0.9677592515945435, 1.0090703964233398, 0.9928398728370667, 0.967042088508606, 1.05129873752594, 1.0162440538406372, 1.0054309368133545, 0.9941884875297546, 0.9938719272613525, 1.0099519491195679, 1.0544772148132324, 1.00078547000885, 0.9736582636833191, 1.0151257514953613, 1.014220118522644, 1.007236361503601, 1.0076617002487183, 1.0159130096435547, 0.9874124526977539, 1.0088812112808228, 1.0355104207992554, 0.9863104224205017, 1.0448024272918701, 1.0377750396728516]
[2025-05-12 16:45:09,355]: Mean: 1.00299466
[2025-05-12 16:45:09,357]: Min: 0.94527030
[2025-05-12 16:45:09,360]: Max: 1.05864108
[2025-05-12 16:45:09,383]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:45:09,392]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.06334559619426727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06334559619426727, 0.0, 0.06334559619426727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06334559619426727, 0.0, 0.0, 0.0]
[2025-05-12 16:45:09,400]: Mean: 0.00044506
[2025-05-12 16:45:09,400]: Min: -0.06334560
[2025-05-12 16:45:09,401]: Max: 0.12669119
[2025-05-12 16:45:09,402]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,403]: Sample Values (25 elements): [1.0364562273025513, 1.0322121381759644, 0.9965933561325073, 0.9971400499343872, 1.014628529548645, 1.003982663154602, 1.0093333721160889, 1.0719445943832397, 1.0362677574157715, 1.0137012004852295, 1.0253973007202148, 1.0169949531555176, 1.0338881015777588, 1.0135524272918701, 1.0164430141448975, 1.0410072803497314, 1.0025714635849, 1.0295510292053223, 1.0112091302871704, 1.052648663520813, 1.0498884916305542, 1.0299897193908691, 1.0406116247177124, 1.007586121559143, 1.009270429611206]
[2025-05-12 16:45:09,403]: Mean: 1.02106118
[2025-05-12 16:45:09,404]: Min: 0.98478752
[2025-05-12 16:45:09,404]: Max: 1.07194459
[2025-05-12 16:45:09,413]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:45:09,415]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06264041364192963, -0.06264041364192963, 0.06264041364192963, 0.0, 0.0, -0.06264041364192963, 0.0, -0.06264041364192963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06264041364192963, 0.0]
[2025-05-12 16:45:09,417]: Mean: -0.00040272
[2025-05-12 16:45:09,417]: Min: -0.06264041
[2025-05-12 16:45:09,419]: Max: 0.12528083
[2025-05-12 16:45:09,419]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,421]: Sample Values (25 elements): [0.9633864760398865, 0.9786434769630432, 0.986572802066803, 0.958044171333313, 0.97770094871521, 0.9850669503211975, 0.9823167324066162, 0.9678393006324768, 0.9668050408363342, 0.9791452884674072, 0.9812741279602051, 0.9536323547363281, 0.9627823829650879, 0.9706341624259949, 0.9826980233192444, 0.9988681077957153, 0.9710703492164612, 0.957417905330658, 0.9712812900543213, 0.9756208062171936, 0.9828292727470398, 0.9850156307220459, 0.9840849041938782, 0.966373085975647, 0.9602221846580505]
[2025-05-12 16:45:09,421]: Mean: 0.97458994
[2025-05-12 16:45:09,422]: Min: 0.95363235
[2025-05-12 16:45:09,423]: Max: 0.99886811
[2025-05-12 16:45:09,429]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:45:09,431]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.048600196838378906, -0.048600196838378906, 0.0, 0.0, 0.0, 0.0, 0.0, -0.048600196838378906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.048600196838378906, 0.0, 0.0, 0.0]
[2025-05-12 16:45:09,433]: Mean: 0.00001450
[2025-05-12 16:45:09,436]: Min: -0.04860020
[2025-05-12 16:45:09,438]: Max: 0.09720039
[2025-05-12 16:45:09,438]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:45:09,441]: Sample Values (25 elements): [1.0458561182022095, 1.0471946001052856, 1.0681982040405273, 1.026863694190979, 1.0264835357666016, 1.0226026773452759, 1.0307142734527588, 1.0567036867141724, 1.0348683595657349, 1.0426311492919922, 1.0541644096374512, 1.0368226766586304, 1.0320576429367065, 1.0121382474899292, 1.0357646942138672, 1.0300155878067017, 1.0491548776626587, 1.058546781539917, 1.0666017532348633, 1.0186818838119507, 1.0273431539535522, 1.0263960361480713, 1.0406336784362793, 1.0277646780014038, 1.0303362607955933]
[2025-05-12 16:45:09,445]: Mean: 1.03882504
[2025-05-12 16:45:09,446]: Min: 0.99887437
[2025-05-12 16:45:09,450]: Max: 1.07779944
[2025-05-12 16:45:09,450]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 16:45:09,459]: Sample Values (25 elements): [0.23281703889369965, -0.07745718955993652, -0.33950990438461304, -0.2748620808124542, -0.047119591385126114, 0.020943550392985344, -0.11595495790243149, 0.4249556064605713, -0.12917417287826538, -0.22421228885650635, -0.06164269894361496, -0.24115203320980072, 0.013791567645967007, 0.013324223458766937, -0.12204442173242569, 0.2892569899559021, -0.07648592442274094, -0.013330264016985893, 0.21479323506355286, 0.15014313161373138, 0.14753660559654236, 0.3322148025035858, 0.02711552381515503, -0.21032866835594177, 0.14339368045330048]
[2025-05-12 16:45:09,463]: Mean: -0.00359619
[2025-05-12 16:45:09,467]: Min: -0.36468619
[2025-05-12 16:45:09,469]: Max: 0.53602034
