[2025-05-26 16:53:13,800]: 
Training ResNet20 with relu6
[2025-05-26 16:53:56,616]: [ResNet20_relu6] Epoch: 001 Train Loss: 1.4935 Train Acc: 0.4466 Eval Loss: 1.3035 Eval Acc: 0.5433 (LR: 0.00100000)
[2025-05-26 16:55:01,664]: [ResNet20_relu6] Epoch: 002 Train Loss: 1.0919 Train Acc: 0.6075 Eval Loss: 1.0348 Eval Acc: 0.6259 (LR: 0.00100000)
[2025-05-26 16:55:51,304]: [ResNet20_relu6] Epoch: 003 Train Loss: 0.9311 Train Acc: 0.6690 Eval Loss: 1.1205 Eval Acc: 0.6381 (LR: 0.00100000)
[2025-05-26 16:56:53,963]: [ResNet20_relu6] Epoch: 004 Train Loss: 0.8252 Train Acc: 0.7091 Eval Loss: 0.9071 Eval Acc: 0.7014 (LR: 0.00100000)
[2025-05-26 16:57:47,560]: [ResNet20_relu6] Epoch: 005 Train Loss: 0.7516 Train Acc: 0.7364 Eval Loss: 0.7622 Eval Acc: 0.7353 (LR: 0.00100000)
[2025-05-26 16:58:47,122]: [ResNet20_relu6] Epoch: 006 Train Loss: 0.6924 Train Acc: 0.7561 Eval Loss: 0.7181 Eval Acc: 0.7593 (LR: 0.00100000)
[2025-05-26 16:59:52,216]: [ResNet20_relu6] Epoch: 007 Train Loss: 0.6513 Train Acc: 0.7739 Eval Loss: 0.7843 Eval Acc: 0.7372 (LR: 0.00100000)
[2025-05-26 17:00:42,116]: [ResNet20_relu6] Epoch: 008 Train Loss: 0.6209 Train Acc: 0.7835 Eval Loss: 0.6501 Eval Acc: 0.7853 (LR: 0.00100000)
[2025-05-26 17:01:47,006]: [ResNet20_relu6] Epoch: 009 Train Loss: 0.5951 Train Acc: 0.7917 Eval Loss: 0.6022 Eval Acc: 0.7944 (LR: 0.00100000)
[2025-05-26 17:02:32,208]: [ResNet20_relu6] Epoch: 010 Train Loss: 0.5686 Train Acc: 0.8020 Eval Loss: 0.6237 Eval Acc: 0.7906 (LR: 0.00100000)
[2025-05-26 17:03:37,107]: [ResNet20_relu6] Epoch: 011 Train Loss: 0.5461 Train Acc: 0.8116 Eval Loss: 0.7032 Eval Acc: 0.7756 (LR: 0.00100000)
[2025-05-26 17:04:17,267]: [ResNet20_relu6] Epoch: 012 Train Loss: 0.5258 Train Acc: 0.8177 Eval Loss: 0.5678 Eval Acc: 0.8132 (LR: 0.00100000)
[2025-05-26 17:05:21,943]: [ResNet20_relu6] Epoch: 013 Train Loss: 0.5087 Train Acc: 0.8242 Eval Loss: 0.5712 Eval Acc: 0.8105 (LR: 0.00100000)
[2025-05-26 17:06:08,297]: [ResNet20_relu6] Epoch: 014 Train Loss: 0.4969 Train Acc: 0.8271 Eval Loss: 0.5568 Eval Acc: 0.8119 (LR: 0.00100000)
[2025-05-26 17:07:10,748]: [ResNet20_relu6] Epoch: 015 Train Loss: 0.4850 Train Acc: 0.8335 Eval Loss: 0.5885 Eval Acc: 0.8086 (LR: 0.00100000)
[2025-05-26 17:08:01,171]: [ResNet20_relu6] Epoch: 016 Train Loss: 0.4716 Train Acc: 0.8375 Eval Loss: 0.5357 Eval Acc: 0.8225 (LR: 0.00100000)
[2025-05-26 17:09:05,565]: [ResNet20_relu6] Epoch: 017 Train Loss: 0.4574 Train Acc: 0.8392 Eval Loss: 0.4975 Eval Acc: 0.8385 (LR: 0.00100000)
[2025-05-26 17:10:08,102]: [ResNet20_relu6] Epoch: 018 Train Loss: 0.4431 Train Acc: 0.8456 Eval Loss: 0.4957 Eval Acc: 0.8342 (LR: 0.00100000)
[2025-05-26 17:10:58,619]: [ResNet20_relu6] Epoch: 019 Train Loss: 0.4323 Train Acc: 0.8503 Eval Loss: 0.5410 Eval Acc: 0.8215 (LR: 0.00100000)
[2025-05-26 17:12:03,591]: [ResNet20_relu6] Epoch: 020 Train Loss: 0.4268 Train Acc: 0.8501 Eval Loss: 0.4793 Eval Acc: 0.8425 (LR: 0.00100000)
[2025-05-26 17:12:50,884]: [ResNet20_relu6] Epoch: 021 Train Loss: 0.4151 Train Acc: 0.8550 Eval Loss: 0.5707 Eval Acc: 0.8198 (LR: 0.00100000)
[2025-05-26 17:13:55,985]: [ResNet20_relu6] Epoch: 022 Train Loss: 0.4094 Train Acc: 0.8576 Eval Loss: 0.5318 Eval Acc: 0.8273 (LR: 0.00100000)
[2025-05-26 17:14:36,337]: [ResNet20_relu6] Epoch: 023 Train Loss: 0.4013 Train Acc: 0.8598 Eval Loss: 0.4803 Eval Acc: 0.8474 (LR: 0.00100000)
[2025-05-26 17:15:41,283]: [ResNet20_relu6] Epoch: 024 Train Loss: 0.3922 Train Acc: 0.8631 Eval Loss: 0.4979 Eval Acc: 0.8396 (LR: 0.00100000)
[2025-05-26 17:16:25,729]: [ResNet20_relu6] Epoch: 025 Train Loss: 0.3865 Train Acc: 0.8656 Eval Loss: 0.4875 Eval Acc: 0.8414 (LR: 0.00100000)
[2025-05-26 17:17:29,587]: [ResNet20_relu6] Epoch: 026 Train Loss: 0.3815 Train Acc: 0.8678 Eval Loss: 0.4334 Eval Acc: 0.8587 (LR: 0.00100000)
[2025-05-26 17:18:19,633]: [ResNet20_relu6] Epoch: 027 Train Loss: 0.3741 Train Acc: 0.8696 Eval Loss: 0.5002 Eval Acc: 0.8375 (LR: 0.00100000)
[2025-05-26 17:19:22,212]: [ResNet20_relu6] Epoch: 028 Train Loss: 0.3682 Train Acc: 0.8699 Eval Loss: 0.4315 Eval Acc: 0.8578 (LR: 0.00100000)
[2025-05-26 17:20:20,716]: [ResNet20_relu6] Epoch: 029 Train Loss: 0.3596 Train Acc: 0.8761 Eval Loss: 0.4754 Eval Acc: 0.8448 (LR: 0.00100000)
[2025-05-26 17:21:13,608]: [ResNet20_relu6] Epoch: 030 Train Loss: 0.3571 Train Acc: 0.8764 Eval Loss: 0.4604 Eval Acc: 0.8473 (LR: 0.00100000)
[2025-05-26 17:22:18,619]: [ResNet20_relu6] Epoch: 031 Train Loss: 0.3491 Train Acc: 0.8782 Eval Loss: 0.4918 Eval Acc: 0.8376 (LR: 0.00100000)
[2025-05-26 17:23:09,043]: [ResNet20_relu6] Epoch: 032 Train Loss: 0.3452 Train Acc: 0.8798 Eval Loss: 0.4451 Eval Acc: 0.8539 (LR: 0.00100000)
[2025-05-26 17:24:13,997]: [ResNet20_relu6] Epoch: 033 Train Loss: 0.3468 Train Acc: 0.8780 Eval Loss: 0.4478 Eval Acc: 0.8548 (LR: 0.00100000)
[2025-05-26 17:24:55,878]: [ResNet20_relu6] Epoch: 034 Train Loss: 0.3354 Train Acc: 0.8834 Eval Loss: 0.4981 Eval Acc: 0.8468 (LR: 0.00010000)
[2025-05-26 17:26:00,547]: [ResNet20_relu6] Epoch: 035 Train Loss: 0.2741 Train Acc: 0.9049 Eval Loss: 0.3418 Eval Acc: 0.8857 (LR: 0.00010000)
[2025-05-26 17:26:43,292]: [ResNet20_relu6] Epoch: 036 Train Loss: 0.2522 Train Acc: 0.9128 Eval Loss: 0.3373 Eval Acc: 0.8873 (LR: 0.00010000)
[2025-05-26 17:27:48,206]: [ResNet20_relu6] Epoch: 037 Train Loss: 0.2444 Train Acc: 0.9156 Eval Loss: 0.3365 Eval Acc: 0.8893 (LR: 0.00010000)
[2025-05-26 17:28:38,251]: [ResNet20_relu6] Epoch: 038 Train Loss: 0.2404 Train Acc: 0.9161 Eval Loss: 0.3347 Eval Acc: 0.8903 (LR: 0.00010000)
[2025-05-26 17:29:40,949]: [ResNet20_relu6] Epoch: 039 Train Loss: 0.2312 Train Acc: 0.9201 Eval Loss: 0.3368 Eval Acc: 0.8906 (LR: 0.00010000)
[2025-05-26 17:30:38,209]: [ResNet20_relu6] Epoch: 040 Train Loss: 0.2295 Train Acc: 0.9193 Eval Loss: 0.3329 Eval Acc: 0.8927 (LR: 0.00010000)
[2025-05-26 17:31:32,605]: [ResNet20_relu6] Epoch: 041 Train Loss: 0.2284 Train Acc: 0.9207 Eval Loss: 0.3323 Eval Acc: 0.8927 (LR: 0.00010000)
[2025-05-26 17:32:37,127]: [ResNet20_relu6] Epoch: 042 Train Loss: 0.2263 Train Acc: 0.9215 Eval Loss: 0.3348 Eval Acc: 0.8929 (LR: 0.00010000)
[2025-05-26 17:33:27,149]: [ResNet20_relu6] Epoch: 043 Train Loss: 0.2230 Train Acc: 0.9235 Eval Loss: 0.3365 Eval Acc: 0.8921 (LR: 0.00010000)
[2025-05-26 17:34:32,202]: [ResNet20_relu6] Epoch: 044 Train Loss: 0.2226 Train Acc: 0.9231 Eval Loss: 0.3283 Eval Acc: 0.8915 (LR: 0.00010000)
[2025-05-26 17:35:16,818]: [ResNet20_relu6] Epoch: 045 Train Loss: 0.2212 Train Acc: 0.9220 Eval Loss: 0.3313 Eval Acc: 0.8935 (LR: 0.00010000)
[2025-05-26 17:36:21,303]: [ResNet20_relu6] Epoch: 046 Train Loss: 0.2167 Train Acc: 0.9237 Eval Loss: 0.3319 Eval Acc: 0.8930 (LR: 0.00010000)
[2025-05-26 17:37:01,202]: [ResNet20_relu6] Epoch: 047 Train Loss: 0.2121 Train Acc: 0.9263 Eval Loss: 0.3345 Eval Acc: 0.8937 (LR: 0.00010000)
[2025-05-26 17:38:05,880]: [ResNet20_relu6] Epoch: 048 Train Loss: 0.2138 Train Acc: 0.9242 Eval Loss: 0.3329 Eval Acc: 0.8957 (LR: 0.00010000)
[2025-05-26 17:38:51,819]: [ResNet20_relu6] Epoch: 049 Train Loss: 0.2116 Train Acc: 0.9260 Eval Loss: 0.3412 Eval Acc: 0.8937 (LR: 0.00010000)
[2025-05-26 17:39:54,212]: [ResNet20_relu6] Epoch: 050 Train Loss: 0.2091 Train Acc: 0.9260 Eval Loss: 0.3344 Eval Acc: 0.8927 (LR: 0.00001000)
[2025-05-26 17:40:44,067]: [ResNet20_relu6] Epoch: 051 Train Loss: 0.2006 Train Acc: 0.9299 Eval Loss: 0.3316 Eval Acc: 0.8948 (LR: 0.00001000)
[2025-05-26 17:41:46,964]: [ResNet20_relu6] Epoch: 052 Train Loss: 0.1973 Train Acc: 0.9302 Eval Loss: 0.3325 Eval Acc: 0.8944 (LR: 0.00001000)
[2025-05-26 17:42:46,698]: [ResNet20_relu6] Epoch: 053 Train Loss: 0.2008 Train Acc: 0.9294 Eval Loss: 0.3269 Eval Acc: 0.8956 (LR: 0.00001000)
[2025-05-26 17:43:38,709]: [ResNet20_relu6] Epoch: 054 Train Loss: 0.2000 Train Acc: 0.9302 Eval Loss: 0.3299 Eval Acc: 0.8957 (LR: 0.00001000)
[2025-05-26 17:44:43,391]: [ResNet20_relu6] Epoch: 055 Train Loss: 0.1973 Train Acc: 0.9318 Eval Loss: 0.3286 Eval Acc: 0.8962 (LR: 0.00001000)
[2025-05-26 17:45:32,663]: [ResNet20_relu6] Epoch: 056 Train Loss: 0.1955 Train Acc: 0.9321 Eval Loss: 0.3281 Eval Acc: 0.8954 (LR: 0.00001000)
[2025-05-26 17:46:37,334]: [ResNet20_relu6] Epoch: 057 Train Loss: 0.1964 Train Acc: 0.9319 Eval Loss: 0.3292 Eval Acc: 0.8951 (LR: 0.00001000)
[2025-05-26 17:47:19,268]: [ResNet20_relu6] Epoch: 058 Train Loss: 0.1951 Train Acc: 0.9315 Eval Loss: 0.3295 Eval Acc: 0.8962 (LR: 0.00001000)
[2025-05-26 17:48:24,113]: [ResNet20_relu6] Epoch: 059 Train Loss: 0.1982 Train Acc: 0.9314 Eval Loss: 0.3294 Eval Acc: 0.8955 (LR: 0.00000100)
[2025-05-26 17:49:07,691]: [ResNet20_relu6] Epoch: 060 Train Loss: 0.1964 Train Acc: 0.9320 Eval Loss: 0.3303 Eval Acc: 0.8954 (LR: 0.00000100)
[2025-05-26 17:50:12,409]: [ResNet20_relu6] Epoch: 061 Train Loss: 0.1948 Train Acc: 0.9321 Eval Loss: 0.3288 Eval Acc: 0.8955 (LR: 0.00000100)
[2025-05-26 17:51:02,545]: [ResNet20_relu6] Epoch: 062 Train Loss: 0.1945 Train Acc: 0.9328 Eval Loss: 0.3269 Eval Acc: 0.8960 (LR: 0.00000100)
[2025-05-26 17:52:05,733]: [ResNet20_relu6] Epoch: 063 Train Loss: 0.1989 Train Acc: 0.9303 Eval Loss: 0.3296 Eval Acc: 0.8965 (LR: 0.00000100)
[2025-05-26 17:53:04,303]: [ResNet20_relu6] Epoch: 064 Train Loss: 0.1954 Train Acc: 0.9321 Eval Loss: 0.3313 Eval Acc: 0.8949 (LR: 0.00000100)
[2025-05-26 17:53:57,521]: [ResNet20_relu6] Epoch: 065 Train Loss: 0.1968 Train Acc: 0.9313 Eval Loss: 0.3309 Eval Acc: 0.8947 (LR: 0.00000010)
[2025-05-26 17:55:02,466]: [ResNet20_relu6] Epoch: 066 Train Loss: 0.1941 Train Acc: 0.9331 Eval Loss: 0.3293 Eval Acc: 0.8961 (LR: 0.00000010)
[2025-05-26 17:55:53,006]: [ResNet20_relu6] Epoch: 067 Train Loss: 0.1945 Train Acc: 0.9326 Eval Loss: 0.3288 Eval Acc: 0.8950 (LR: 0.00000010)
[2025-05-26 17:56:57,741]: [ResNet20_relu6] Epoch: 068 Train Loss: 0.1986 Train Acc: 0.9318 Eval Loss: 0.3290 Eval Acc: 0.8958 (LR: 0.00000010)
[2025-05-26 17:57:39,655]: [ResNet20_relu6] Epoch: 069 Train Loss: 0.1955 Train Acc: 0.9307 Eval Loss: 0.3300 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-05-26 17:58:44,647]: [ResNet20_relu6] Epoch: 070 Train Loss: 0.1972 Train Acc: 0.9315 Eval Loss: 0.3294 Eval Acc: 0.8956 (LR: 0.00000010)
[2025-05-26 17:59:27,371]: [ResNet20_relu6] Epoch: 071 Train Loss: 0.1968 Train Acc: 0.9307 Eval Loss: 0.3298 Eval Acc: 0.8957 (LR: 0.00000010)
[2025-05-26 18:00:32,180]: [ResNet20_relu6] Epoch: 072 Train Loss: 0.1939 Train Acc: 0.9316 Eval Loss: 0.3294 Eval Acc: 0.8959 (LR: 0.00000010)
[2025-05-26 18:01:22,991]: [ResNet20_relu6] Epoch: 073 Train Loss: 0.1966 Train Acc: 0.9313 Eval Loss: 0.3298 Eval Acc: 0.8951 (LR: 0.00000010)
[2025-05-26 18:02:25,867]: [ResNet20_relu6] Epoch: 074 Train Loss: 0.1917 Train Acc: 0.9343 Eval Loss: 0.3289 Eval Acc: 0.8956 (LR: 0.00000010)
[2025-05-26 18:03:22,491]: [ResNet20_relu6] Epoch: 075 Train Loss: 0.1968 Train Acc: 0.9312 Eval Loss: 0.3297 Eval Acc: 0.8951 (LR: 0.00000010)
[2025-05-26 18:04:17,621]: [ResNet20_relu6] Epoch: 076 Train Loss: 0.1918 Train Acc: 0.9326 Eval Loss: 0.3265 Eval Acc: 0.8961 (LR: 0.00000010)
[2025-05-26 18:05:22,599]: [ResNet20_relu6] Epoch: 077 Train Loss: 0.1973 Train Acc: 0.9307 Eval Loss: 0.3298 Eval Acc: 0.8964 (LR: 0.00000010)
[2025-05-26 18:06:11,639]: [ResNet20_relu6] Epoch: 078 Train Loss: 0.1933 Train Acc: 0.9331 Eval Loss: 0.3301 Eval Acc: 0.8957 (LR: 0.00000010)
[2025-05-26 18:07:16,192]: [ResNet20_relu6] Epoch: 079 Train Loss: 0.1967 Train Acc: 0.9323 Eval Loss: 0.3289 Eval Acc: 0.8962 (LR: 0.00000010)
[2025-05-26 18:08:02,120]: [ResNet20_relu6] Epoch: 080 Train Loss: 0.1946 Train Acc: 0.9326 Eval Loss: 0.3273 Eval Acc: 0.8967 (LR: 0.00000010)
[2025-05-26 18:09:06,864]: [ResNet20_relu6] Epoch: 081 Train Loss: 0.1949 Train Acc: 0.9323 Eval Loss: 0.3314 Eval Acc: 0.8950 (LR: 0.00000010)
[2025-05-26 18:09:47,166]: [ResNet20_relu6] Epoch: 082 Train Loss: 0.1929 Train Acc: 0.9327 Eval Loss: 0.3289 Eval Acc: 0.8958 (LR: 0.00000010)
[2025-05-26 18:10:52,408]: [ResNet20_relu6] Epoch: 083 Train Loss: 0.1959 Train Acc: 0.9312 Eval Loss: 0.3310 Eval Acc: 0.8952 (LR: 0.00000010)
[2025-05-26 18:11:38,899]: [ResNet20_relu6] Epoch: 084 Train Loss: 0.1934 Train Acc: 0.9326 Eval Loss: 0.3305 Eval Acc: 0.8957 (LR: 0.00000010)
[2025-05-26 18:12:41,596]: [ResNet20_relu6] Epoch: 085 Train Loss: 0.1966 Train Acc: 0.9307 Eval Loss: 0.3297 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-05-26 18:13:32,040]: [ResNet20_relu6] Epoch: 086 Train Loss: 0.1971 Train Acc: 0.9312 Eval Loss: 0.3289 Eval Acc: 0.8951 (LR: 0.00000010)
[2025-05-26 18:14:36,717]: [ResNet20_relu6] Epoch: 087 Train Loss: 0.1930 Train Acc: 0.9328 Eval Loss: 0.3294 Eval Acc: 0.8959 (LR: 0.00000010)
[2025-05-26 18:15:39,945]: [ResNet20_relu6] Epoch: 088 Train Loss: 0.1978 Train Acc: 0.9328 Eval Loss: 0.3319 Eval Acc: 0.8953 (LR: 0.00000010)
[2025-05-26 18:16:30,473]: [ResNet20_relu6] Epoch: 089 Train Loss: 0.1977 Train Acc: 0.9306 Eval Loss: 0.3296 Eval Acc: 0.8952 (LR: 0.00000010)
[2025-05-26 18:17:35,326]: [ResNet20_relu6] Epoch: 090 Train Loss: 0.1985 Train Acc: 0.9308 Eval Loss: 0.3303 Eval Acc: 0.8942 (LR: 0.00000010)
[2025-05-26 18:18:22,210]: [ResNet20_relu6] Epoch: 091 Train Loss: 0.1992 Train Acc: 0.9309 Eval Loss: 0.3286 Eval Acc: 0.8965 (LR: 0.00000010)
[2025-05-26 18:19:27,339]: [ResNet20_relu6] Epoch: 092 Train Loss: 0.1950 Train Acc: 0.9322 Eval Loss: 0.3301 Eval Acc: 0.8953 (LR: 0.00000010)
[2025-05-26 18:20:07,421]: [ResNet20_relu6] Epoch: 093 Train Loss: 0.1924 Train Acc: 0.9335 Eval Loss: 0.3295 Eval Acc: 0.8957 (LR: 0.00000010)
[2025-05-26 18:21:12,220]: [ResNet20_relu6] Epoch: 094 Train Loss: 0.1952 Train Acc: 0.9312 Eval Loss: 0.3280 Eval Acc: 0.8959 (LR: 0.00000010)
[2025-05-26 18:21:56,555]: [ResNet20_relu6] Epoch: 095 Train Loss: 0.1942 Train Acc: 0.9312 Eval Loss: 0.3264 Eval Acc: 0.8956 (LR: 0.00000010)
[2025-05-26 18:23:00,605]: [ResNet20_relu6] Epoch: 096 Train Loss: 0.1965 Train Acc: 0.9318 Eval Loss: 0.3293 Eval Acc: 0.8957 (LR: 0.00000010)
[2025-05-26 18:23:50,944]: [ResNet20_relu6] Epoch: 097 Train Loss: 0.1935 Train Acc: 0.9328 Eval Loss: 0.3284 Eval Acc: 0.8948 (LR: 0.00000010)
[2025-05-26 18:24:54,909]: [ResNet20_relu6] Epoch: 098 Train Loss: 0.1947 Train Acc: 0.9324 Eval Loss: 0.3274 Eval Acc: 0.8963 (LR: 0.00000010)
[2025-05-26 18:25:55,973]: [ResNet20_relu6] Epoch: 099 Train Loss: 0.1969 Train Acc: 0.9314 Eval Loss: 0.3286 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-05-26 18:26:47,319]: [ResNet20_relu6] Epoch: 100 Train Loss: 0.1943 Train Acc: 0.9314 Eval Loss: 0.3307 Eval Acc: 0.8956 (LR: 0.00000010)
[2025-05-26 18:26:47,320]: Early stopping was triggered!
[2025-05-26 18:26:47,320]: [ResNet20_relu6] Best Eval Accuracy: 0.8967
[2025-05-26 18:26:47,344]: 
Training of full-precision model finished!
[2025-05-26 18:26:47,344]: Model Architecture:
[2025-05-26 18:26:47,344]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-26 18:26:47,344]: 
Model Weights:
[2025-05-26 18:26:47,344]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-26 18:26:47,345]: Sample Values (25 elements): [-0.1703968048095703, -0.07382456213235855, -0.11460739374160767, -0.24677808582782745, -0.14300738275051117, 0.14457672834396362, -0.13756798207759857, 0.04287897050380707, 0.21108564734458923, 0.16997341811656952, -0.1703452169895172, 0.005887485109269619, 0.19156469404697418, 0.055447157472372055, -0.10056885331869125, 0.04999639093875885, 0.03594687581062317, 0.12179965525865555, 0.2148156613111496, 0.18599176406860352, 0.1169162467122078, -0.0633835420012474, 0.08853663504123688, -0.12850114703178406, 0.21081483364105225]
[2025-05-26 18:26:47,345]: Mean: -0.00526365
[2025-05-26 18:26:47,345]: Min: -0.43861437
[2025-05-26 18:26:47,345]: Max: 0.35229400
[2025-05-26 18:26:47,345]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,346]: Sample Values (16 elements): [1.2214045524597168, 0.9081518650054932, 0.8976117968559265, 0.860082745552063, 0.8763169646263123, 0.769730806350708, 0.8667412996292114, 0.8356739282608032, 1.172308325767517, 0.9436330199241638, 0.8619796633720398, 0.8724134564399719, 0.8948461413383484, 0.9851094484329224, 0.8481241464614868, 0.986409068107605]
[2025-05-26 18:26:47,346]: Mean: 0.92503357
[2025-05-26 18:26:47,346]: Min: 0.76973081
[2025-05-26 18:26:47,346]: Max: 1.22140455
[2025-05-26 18:26:47,346]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 18:26:47,347]: Sample Values (25 elements): [-0.1778753101825714, 0.001610916224308312, 0.15280237793922424, -0.0253573227673769, 0.02101561427116394, 0.024620873853564262, -0.013774914667010307, -0.11481820046901703, -0.1315128058195114, -0.11817901581525803, -0.06423855572938919, -0.09472058713436127, -0.03562388941645622, 0.11125282198190689, 0.09595636278390884, 0.06688942760229111, 0.1403469741344452, -0.0690060630440712, -0.14060261845588684, 0.0941445380449295, -0.051702383905649185, -0.137520432472229, -0.02360703982412815, -0.00019752014486584812, 0.12474419921636581]
[2025-05-26 18:26:47,347]: Mean: -0.01052370
[2025-05-26 18:26:47,347]: Min: -0.35576409
[2025-05-26 18:26:47,347]: Max: 0.44953614
[2025-05-26 18:26:47,347]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,348]: Sample Values (16 elements): [0.9237826466560364, 0.8288152813911438, 0.7837857007980347, 0.8552643060684204, 0.8360547423362732, 0.7396984100341797, 0.7306927442550659, 1.0868576765060425, 0.9679158329963684, 0.7651854753494263, 0.8715255856513977, 1.0808254480361938, 0.8265563249588013, 0.7416933178901672, 0.867412269115448, 0.6855383515357971]
[2025-05-26 18:26:47,348]: Mean: 0.84947526
[2025-05-26 18:26:47,348]: Min: 0.68553835
[2025-05-26 18:26:47,348]: Max: 1.08685768
[2025-05-26 18:26:47,348]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 18:26:47,349]: Sample Values (25 elements): [-0.1630852222442627, 0.09135708212852478, 0.0038885350804775953, -0.08659729361534119, 0.017547592520713806, -0.04986770823597908, -0.017658311873674393, 0.07052162289619446, 0.10664141178131104, 0.13635458052158356, 0.2425975799560547, -0.07256647944450378, 0.10293733328580856, 0.06526739150285721, 0.00708230584859848, -0.05964681878685951, 0.08213049173355103, 0.1319722980260849, -0.011611709371209145, 0.0665082260966301, -0.12508362531661987, -0.22648482024669647, -0.09685885161161423, 0.009529151022434235, 0.06221006438136101]
[2025-05-26 18:26:47,349]: Mean: -0.00454378
[2025-05-26 18:26:47,349]: Min: -0.50123268
[2025-05-26 18:26:47,349]: Max: 0.52219194
[2025-05-26 18:26:47,349]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,349]: Sample Values (16 elements): [0.7909431457519531, 0.5709112882614136, 0.9726815223693848, 1.1006046533584595, 0.8303148746490479, 0.9971835613250732, 0.6911265254020691, 0.8505391478538513, 0.844728946685791, 0.7631642818450928, 0.8549418449401855, 0.7206251621246338, 0.8768549561500549, 0.9278767704963684, 0.7776392102241516, 1.1371674537658691]
[2025-05-26 18:26:47,349]: Mean: 0.85670644
[2025-05-26 18:26:47,350]: Min: 0.57091129
[2025-05-26 18:26:47,350]: Max: 1.13716745
[2025-05-26 18:26:47,350]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 18:26:47,350]: Sample Values (25 elements): [0.057361021637916565, 0.04372047632932663, -0.1585436314344406, -0.09287212044000626, -0.10884895920753479, 0.033665090799331665, -0.16258539259433746, -0.20895400643348694, -0.06381729990243912, 0.01708773896098137, -0.0641094222664833, -0.04474806413054466, 0.051266465336084366, 0.1499396115541458, 0.07820620387792587, -0.08975180238485336, -0.010188370011746883, -0.009141949005424976, 0.12144427746534348, 0.13733205199241638, -0.02745969034731388, 0.07451096177101135, 0.1138395443558693, 0.18601438403129578, -0.09673333913087845]
[2025-05-26 18:26:47,350]: Mean: -0.00910965
[2025-05-26 18:26:47,351]: Min: -0.69861096
[2025-05-26 18:26:47,351]: Max: 0.37966380
[2025-05-26 18:26:47,351]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,351]: Sample Values (16 elements): [0.8525251150131226, 0.8023406863212585, 0.9711501598358154, 0.8452043533325195, 0.7510524392127991, 0.8529295325279236, 1.092149257659912, 0.6275065541267395, 0.8625486493110657, 0.8201419115066528, 0.8959583044052124, 0.7839667201042175, 0.8157069087028503, 0.7225565314292908, 0.9867967963218689, 1.144619345664978]
[2025-05-26 18:26:47,351]: Mean: 0.86419708
[2025-05-26 18:26:47,351]: Min: 0.62750655
[2025-05-26 18:26:47,352]: Max: 1.14461935
[2025-05-26 18:26:47,352]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 18:26:47,352]: Sample Values (25 elements): [0.046670202165842056, 0.2996337413787842, -0.005807728972285986, 0.051435161381959915, -0.18950115144252777, 0.01944720558822155, 0.08985152840614319, 0.005361831746995449, -0.13418428599834442, -0.0038985833525657654, -0.031208733096718788, 0.18145214021205902, 0.17384956777095795, 0.008486910723149776, 0.010117069818079472, 0.018603147938847542, -0.07828339189291, -0.005862227641046047, 0.04289121553301811, -0.02419416606426239, -0.19024665653705597, 0.01593080908060074, 0.04768839478492737, 0.10573405772447586, 0.07767193019390106]
[2025-05-26 18:26:47,352]: Mean: -0.00146435
[2025-05-26 18:26:47,352]: Min: -0.38143697
[2025-05-26 18:26:47,352]: Max: 0.40318865
[2025-05-26 18:26:47,352]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,353]: Sample Values (16 elements): [1.092090129852295, 0.6198471188545227, 0.719799816608429, 0.7774332761764526, 0.9413730502128601, 0.620976984500885, 0.8110315203666687, 0.9937652945518494, 0.5914916396141052, 0.7803834080696106, 0.7461931705474854, 0.9277666807174683, 0.8477314114570618, 0.7194382548332214, 0.7382639646530151, 1.05665922164917]
[2025-05-26 18:26:47,353]: Mean: 0.81151533
[2025-05-26 18:26:47,353]: Min: 0.59149164
[2025-05-26 18:26:47,353]: Max: 1.09209013
[2025-05-26 18:26:47,353]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 18:26:47,353]: Sample Values (25 elements): [-0.027546493336558342, 0.07864068448543549, 0.026356171816587448, 0.13242095708847046, -0.14913377165794373, 0.05250018090009689, 0.006740047596395016, -0.09826207906007767, 0.205260768532753, 0.07791955769062042, -0.11499959230422974, 0.11140606552362442, 0.02971770241856575, 0.004146159626543522, 0.0651518926024437, 0.04203551262617111, -0.05612774193286896, -0.07739046961069107, -0.10167793929576874, -0.16067121922969818, -0.09649284183979034, 0.03535214066505432, 0.12430324405431747, 0.08126727491617203, 0.052522867918014526]
[2025-05-26 18:26:47,354]: Mean: -0.01169901
[2025-05-26 18:26:47,354]: Min: -0.56821209
[2025-05-26 18:26:47,354]: Max: 0.40439776
[2025-05-26 18:26:47,354]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,354]: Sample Values (16 elements): [0.7444227933883667, 1.0474339723587036, 0.7956340909004211, 0.8858729004859924, 0.7816739678382874, 0.7134905457496643, 0.8301273584365845, 1.0059632062911987, 0.7090328335762024, 0.7389941811561584, 0.8581839799880981, 0.7968059182167053, 0.7622692584991455, 0.932431697845459, 0.9972553849220276, 0.9136694669723511]
[2025-05-26 18:26:47,354]: Mean: 0.84457886
[2025-05-26 18:26:47,355]: Min: 0.70903283
[2025-05-26 18:26:47,355]: Max: 1.04743397
[2025-05-26 18:26:47,355]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 18:26:47,355]: Sample Values (25 elements): [-0.10926569253206253, -0.04613083228468895, 0.14412765204906464, 0.0033756541088223457, 0.03849088400602341, 0.002799125388264656, 0.0625033900141716, -0.03332825005054474, -0.03986199572682381, 0.120272696018219, 0.050512924790382385, 0.0046797278337180614, 0.14567089080810547, 0.03256511315703392, -0.061881884932518005, 0.025505797937512398, 0.008892560377717018, -0.07360871881246567, -0.043590858578681946, 0.0014007490826770663, -0.013185492716729641, 0.07374285161495209, 0.18778164684772491, -0.013256572186946869, 0.1045394018292427]
[2025-05-26 18:26:47,355]: Mean: 0.00989248
[2025-05-26 18:26:47,355]: Min: -0.36482924
[2025-05-26 18:26:47,356]: Max: 0.45121706
[2025-05-26 18:26:47,356]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-26 18:26:47,356]: Sample Values (16 elements): [0.6812763810157776, 0.8385201096534729, 0.6044998168945312, 0.8871417045593262, 0.8425399661064148, 1.0266047716140747, 0.9440256953239441, 0.6708358526229858, 0.6248193979263306, 0.7396879196166992, 0.8341992497444153, 0.7739425301551819, 0.71048903465271, 0.6855211853981018, 0.9833351373672485, 0.6706700325012207]
[2025-05-26 18:26:47,356]: Mean: 0.78238177
[2025-05-26 18:26:47,356]: Min: 0.60449982
[2025-05-26 18:26:47,356]: Max: 1.02660477
[2025-05-26 18:26:47,356]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-26 18:26:47,357]: Sample Values (25 elements): [-0.030454307794570923, 0.036607418209314346, 0.020750215277075768, 0.03791211545467377, -0.09688906371593475, -0.026767242699861526, 0.003236938500776887, -0.009761522524058819, 0.12162233144044876, -0.27166804671287537, -0.18360982835292816, 0.11926183104515076, 0.001744819339364767, -0.04601320996880531, 0.018225764855742455, 0.15075178444385529, 0.0603862963616848, 0.00748945539817214, -0.05638394504785538, -0.06625494360923767, 0.10290680825710297, 0.07073935866355896, 0.016329148784279823, 0.007975292392075062, -0.025173887610435486]
[2025-05-26 18:26:47,357]: Mean: -0.00649935
[2025-05-26 18:26:47,357]: Min: -0.41585740
[2025-05-26 18:26:47,357]: Max: 0.33064121
[2025-05-26 18:26:47,357]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,357]: Sample Values (25 elements): [0.8778187036514282, 0.7986661195755005, 0.7688356041908264, 0.9092823266983032, 0.7794533371925354, 0.9036321640014648, 0.8507813811302185, 0.7322845458984375, 0.8763909339904785, 0.7971839904785156, 0.928665280342102, 0.8065284490585327, 0.7660480737686157, 0.7820585370063782, 0.8726409673690796, 0.8441351056098938, 0.8420767784118652, 0.8648293614387512, 0.9795145988464355, 0.890912652015686, 0.8445680737495422, 0.9377001523971558, 1.0372968912124634, 0.7632542252540588, 0.9079399704933167]
[2025-05-26 18:26:47,358]: Mean: 0.84864283
[2025-05-26 18:26:47,358]: Min: 0.73068064
[2025-05-26 18:26:47,358]: Max: 1.03729689
[2025-05-26 18:26:47,358]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 18:26:47,358]: Sample Values (25 elements): [-0.057290252298116684, 0.09077220410108566, -0.06227836012840271, -0.05962570756673813, 0.074830561876297, -0.0018937393324449658, 0.06711124628782272, -0.033021021634340286, -0.03240019455552101, -0.033833153545856476, -0.05758664757013321, -0.03904153034090996, 0.02893044613301754, -0.0791526734828949, -0.17635831236839294, -0.13389180600643158, 0.07578969746828079, -0.04238073527812958, -0.041628312319517136, 0.13434268534183502, 0.06344406306743622, -0.046549636870622635, -0.00022972578881308436, -0.08599697053432465, 0.08226942270994186]
[2025-05-26 18:26:47,358]: Mean: -0.00463895
[2025-05-26 18:26:47,359]: Min: -0.42124808
[2025-05-26 18:26:47,359]: Max: 0.36971405
[2025-05-26 18:26:47,359]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,359]: Sample Values (25 elements): [1.02268385887146, 0.9598841071128845, 0.8175945281982422, 0.8615338802337646, 0.9419578313827515, 1.0082733631134033, 0.9489073157310486, 0.9019355177879333, 0.8788660168647766, 0.8839334845542908, 0.8915712833404541, 0.8927052021026611, 0.9013534188270569, 0.8048386573791504, 1.0036342144012451, 0.8440311551094055, 0.8435525298118591, 0.9472033977508545, 0.8152375221252441, 0.8043778538703918, 1.0167169570922852, 0.8547030687332153, 0.9143649339675903, 1.0710396766662598, 0.9305236339569092]
[2025-05-26 18:26:47,359]: Mean: 0.90391827
[2025-05-26 18:26:47,359]: Min: 0.78170234
[2025-05-26 18:26:47,360]: Max: 1.07103968
[2025-05-26 18:26:47,360]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-26 18:26:47,360]: Sample Values (25 elements): [0.015420595183968544, 0.050422199070453644, 0.057288758456707, -0.11846596002578735, 0.25705763697624207, -0.07247401773929596, -0.1775558590888977, 0.10953287035226822, -0.09949787706136703, 0.00033676961902529, 0.07334454357624054, -0.16935595870018005, 0.13365133106708527, -0.048903658986091614, 0.17217499017715454, 0.10026543587446213, -0.11594220995903015, 0.005395256914198399, -0.039835430681705475, -0.11717744916677475, 0.10660554468631744, 0.31993618607521057, -0.035251080989837646, 0.048607636243104935, 0.11027157306671143]
[2025-05-26 18:26:47,360]: Mean: -0.00526170
[2025-05-26 18:26:47,360]: Min: -0.51067364
[2025-05-26 18:26:47,360]: Max: 0.51063794
[2025-05-26 18:26:47,360]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,361]: Sample Values (25 elements): [0.7625260949134827, 0.664460301399231, 0.8015592694282532, 0.8392611742019653, 0.8964941501617432, 0.6459793448448181, 0.7750442028045654, 0.4423075020313263, 0.9425783753395081, 0.7256383895874023, 0.5885261297225952, 0.732807457447052, 1.0243769884109497, 0.7183424830436707, 0.6318833231925964, 0.6493724584579468, 0.7734023332595825, 0.7195044755935669, 0.8804795742034912, 0.690229594707489, 0.6753655672073364, 0.7094894647598267, 0.6527168154716492, 0.5118627548217773, 0.6695285439491272]
[2025-05-26 18:26:47,361]: Mean: 0.70966178
[2025-05-26 18:26:47,361]: Min: 0.44230750
[2025-05-26 18:26:47,361]: Max: 1.02437699
[2025-05-26 18:26:47,361]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 18:26:47,362]: Sample Values (25 elements): [0.07322771847248077, -0.09891531616449356, -0.017714736983180046, -0.1364133358001709, 0.007131913676857948, -0.08241894841194153, -0.013434442691504955, 0.2050316333770752, 0.013835887424647808, 0.026475844904780388, -0.018185514956712723, 0.10565627366304398, 0.06707645207643509, 0.0772486999630928, 0.05375583469867706, 0.0013560958905145526, -0.09546986222267151, -0.07853709906339645, -0.04569726809859276, 0.0950428918004036, -0.07542286813259125, -0.0026282232720404863, 0.070879265666008, -0.006036053411662579, 0.10458024591207504]
[2025-05-26 18:26:47,362]: Mean: -0.00545993
[2025-05-26 18:26:47,362]: Min: -0.33824453
[2025-05-26 18:26:47,362]: Max: 0.31785810
[2025-05-26 18:26:47,362]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,362]: Sample Values (25 elements): [0.7638491988182068, 0.8448936343193054, 0.8621037602424622, 0.8167055249214172, 0.8910015821456909, 0.7729249000549316, 0.7777943015098572, 0.7998419404029846, 0.8296945095062256, 0.7738567590713501, 0.9145034551620483, 0.7867971658706665, 0.8129444122314453, 0.8276227712631226, 0.7944425940513611, 0.8770900368690491, 0.8854705095291138, 0.7500669360160828, 0.8589349389076233, 0.9631194472312927, 0.7947721481323242, 0.8191311955451965, 0.8440887331962585, 0.782420814037323, 0.7477269172668457]
[2025-05-26 18:26:47,362]: Mean: 0.82241893
[2025-05-26 18:26:47,363]: Min: 0.74772692
[2025-05-26 18:26:47,363]: Max: 0.96311945
[2025-05-26 18:26:47,363]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 18:26:47,363]: Sample Values (25 elements): [0.018312042579054832, 0.04334547743201256, -0.0031983410008251667, 0.0423324778676033, 0.020824195817112923, -0.16741177439689636, 0.05417809262871742, -0.03452003747224808, 0.04854646697640419, 0.17407454550266266, -0.026751317083835602, -0.10901141911745071, 0.012740193866193295, -0.18119540810585022, 0.018708163872361183, 0.03316375985741615, -0.026481278240680695, 0.026293788105249405, -0.0655946433544159, -0.04390908032655716, 0.07884305715560913, 0.013589855283498764, 0.03360511362552643, -0.0647970587015152, -0.025059610605239868]
[2025-05-26 18:26:47,363]: Mean: -0.00518896
[2025-05-26 18:26:47,363]: Min: -0.34654510
[2025-05-26 18:26:47,364]: Max: 0.34688765
[2025-05-26 18:26:47,364]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,364]: Sample Values (25 elements): [0.9669344425201416, 0.7082744836807251, 0.8852903842926025, 0.7391706109046936, 0.7997891306877136, 0.8424418568611145, 0.6704282760620117, 0.6261452436447144, 0.8906567096710205, 0.9140161871910095, 0.8482966423034668, 0.8104954957962036, 0.7307465076446533, 0.7968388199806213, 0.7322765588760376, 0.8480168581008911, 0.8921645283699036, 0.8346781730651855, 0.8206564784049988, 0.8609212040901184, 0.7499743700027466, 0.909620463848114, 0.6514565944671631, 0.8150404691696167, 0.9131492376327515]
[2025-05-26 18:26:47,364]: Mean: 0.80460715
[2025-05-26 18:26:47,364]: Min: 0.62614524
[2025-05-26 18:26:47,364]: Max: 0.96693444
[2025-05-26 18:26:47,364]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 18:26:47,365]: Sample Values (25 elements): [0.06228834018111229, -0.08090844750404358, 0.0389845184981823, -0.06485424190759659, -0.10367415100336075, -0.0545303039252758, 0.013367990031838417, -0.14433206617832184, 0.02438441850244999, -0.04187279939651489, -0.005642916541546583, 0.01377099845558405, 0.06943947076797485, -0.07160113006830215, -0.058004941791296005, -0.043508127331733704, 0.09414276480674744, 0.005758904851973057, -0.04234064742922783, -0.03636253625154495, -0.12222384661436081, 0.04248111695051193, -0.1673544943332672, 0.11975935101509094, 0.15397489070892334]
[2025-05-26 18:26:47,365]: Mean: -0.01077512
[2025-05-26 18:26:47,365]: Min: -0.37449166
[2025-05-26 18:26:47,365]: Max: 0.31778044
[2025-05-26 18:26:47,365]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,366]: Sample Values (25 elements): [0.7329927086830139, 0.6669684052467346, 0.8229954242706299, 0.7698877453804016, 0.8717883825302124, 0.7554959654808044, 0.7015613317489624, 0.8290409445762634, 0.850199282169342, 0.7603871822357178, 0.7720866203308105, 0.7250294089317322, 0.6946731805801392, 0.8982606530189514, 0.8063522577285767, 0.8121130466461182, 0.8893424272537231, 0.730026125907898, 0.734870195388794, 0.8304242491722107, 0.7087818384170532, 0.8730061054229736, 0.7453007698059082, 0.7170150876045227, 0.8323970437049866]
[2025-05-26 18:26:47,366]: Mean: 0.78495920
[2025-05-26 18:26:47,366]: Min: 0.66696841
[2025-05-26 18:26:47,366]: Max: 0.89826065
[2025-05-26 18:26:47,366]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 18:26:47,366]: Sample Values (25 elements): [-0.008145184256136417, 0.08969229459762573, -0.055796608328819275, 0.14258094131946564, 0.05764671415090561, 0.144741490483284, -0.04303734004497528, -0.04011964797973633, -0.02003401517868042, 0.007872282527387142, 0.06560161709785461, -0.09025423228740692, 0.0012616851599887013, -0.0810389444231987, 0.0799742341041565, -0.08936706930398941, -0.09639675915241241, 0.015509513206779957, 0.19007962942123413, -0.17163503170013428, 0.005170358344912529, -0.11038166284561157, 0.04236229509115219, -0.0021482955198735, -0.016040805727243423]
[2025-05-26 18:26:47,367]: Mean: -0.00149717
[2025-05-26 18:26:47,367]: Min: -0.32257396
[2025-05-26 18:26:47,367]: Max: 0.33639422
[2025-05-26 18:26:47,367]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-26 18:26:47,367]: Sample Values (25 elements): [0.790325403213501, 0.7593896389007568, 0.7605960965156555, 0.8121852874755859, 1.0131137371063232, 0.7652949094772339, 0.9681748747825623, 0.8109561204910278, 0.8752887845039368, 0.8142117261886597, 0.826947033405304, 0.8811801671981812, 0.8109428882598877, 0.7885643243789673, 0.9022168517112732, 0.8379188776016235, 0.6910572052001953, 0.808326005935669, 0.7294450402259827, 0.8797462582588196, 0.8320303559303284, 0.657468318939209, 0.8553192615509033, 0.6511651277542114, 0.6482630372047424]
[2025-05-26 18:26:47,367]: Mean: 0.79290402
[2025-05-26 18:26:47,367]: Min: 0.62181866
[2025-05-26 18:26:47,368]: Max: 1.01311374
[2025-05-26 18:26:47,368]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-26 18:26:47,368]: Sample Values (25 elements): [0.06352487206459045, -0.027737637981772423, 0.01566144824028015, 0.10924935340881348, -0.05279393866658211, -0.011225287802517414, -0.018382206559181213, 0.2898481488227844, -0.15486879646778107, -0.23494529724121094, -0.14683210849761963, 0.17904460430145264, -0.07809430360794067, 0.008254893124103546, 0.1315472573041916, -0.006875358056277037, 0.012624312192201614, 0.003308201441541314, -0.1166781336069107, 0.07237371057271957, 0.07385792583227158, -0.12045757472515106, -0.09093891829252243, -0.015059920027852058, -0.03515039384365082]
[2025-05-26 18:26:47,368]: Mean: -0.00596377
[2025-05-26 18:26:47,368]: Min: -0.38502681
[2025-05-26 18:26:47,369]: Max: 0.35451645
[2025-05-26 18:26:47,369]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,369]: Sample Values (25 elements): [0.7346726059913635, 0.7721067667007446, 0.723149299621582, 0.7236526608467102, 0.7448055744171143, 0.7002401351928711, 0.8120132088661194, 0.7935373187065125, 0.7526737451553345, 0.7114809155464172, 0.8556525707244873, 0.6240081787109375, 0.7129862904548645, 0.7487545013427734, 0.7404133081436157, 0.8040330410003662, 0.7249701619148254, 0.7121230959892273, 0.7297906875610352, 0.7301806807518005, 0.7651194334030151, 0.7176229953765869, 0.7107142806053162, 0.8036983013153076, 0.771684467792511]
[2025-05-26 18:26:47,369]: Mean: 0.76389468
[2025-05-26 18:26:47,369]: Min: 0.61466789
[2025-05-26 18:26:47,369]: Max: 0.96082282
[2025-05-26 18:26:47,369]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:26:47,370]: Sample Values (25 elements): [-0.1099512130022049, -0.11840403079986572, 0.040754664689302444, 0.0071662007831037045, 0.05418277159333229, 0.09410595148801804, 0.13993339240550995, 0.02568093314766884, 0.02165311574935913, -0.02576647512614727, -0.001174280303530395, -0.05702606961131096, 0.007814150303602219, -0.07527846843004227, -0.1360563486814499, 0.12148407846689224, -0.06572666019201279, -0.016148317605257034, 0.03211979195475578, -0.036949362605810165, -0.02258363552391529, 0.09757315367460251, -0.018390607088804245, 0.018534710630774498, 0.1216704472899437]
[2025-05-26 18:26:47,370]: Mean: -0.00391769
[2025-05-26 18:26:47,370]: Min: -0.34893814
[2025-05-26 18:26:47,370]: Max: 0.37027895
[2025-05-26 18:26:47,370]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,371]: Sample Values (25 elements): [0.9604960083961487, 0.8336257934570312, 1.0390827655792236, 1.097023367881775, 1.0290348529815674, 0.7327108383178711, 1.0718365907669067, 0.9857692718505859, 0.9733147621154785, 0.9402285218238831, 0.9623833298683167, 1.0591086149215698, 0.7856258749961853, 1.0384441614151, 0.7624301910400391, 0.8901540637016296, 1.070839285850525, 0.8684488534927368, 1.2133175134658813, 0.9818427562713623, 0.855114221572876, 0.971509575843811, 0.8896520137786865, 0.7912991046905518, 0.811944842338562]
[2025-05-26 18:26:47,371]: Mean: 0.93278742
[2025-05-26 18:26:47,371]: Min: 0.62986720
[2025-05-26 18:26:47,371]: Max: 1.21331751
[2025-05-26 18:26:47,371]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-26 18:26:47,371]: Sample Values (25 elements): [-0.007823916152119637, -0.13785339891910553, 0.07074578106403351, -0.0640874058008194, 0.07301593571901321, -0.24415500462055206, -0.11183565109968185, -0.13419263064861298, 0.009288925677537918, -0.14004957675933838, 0.06329087167978287, -0.11438747495412827, 0.16716492176055908, -0.14897331595420837, 0.0877482146024704, -0.11454261094331741, 0.12887954711914062, -0.169979065656662, -0.024259226396679878, -0.01707148738205433, -0.034812405705451965, -0.028781140223145485, -0.0612228624522686, -0.24988910555839539, -0.055039070546627045]
[2025-05-26 18:26:47,372]: Mean: -0.01012188
[2025-05-26 18:26:47,372]: Min: -0.36473781
[2025-05-26 18:26:47,372]: Max: 0.35100603
[2025-05-26 18:26:47,372]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,372]: Sample Values (25 elements): [0.4633064270019531, 0.7077824473381042, 0.5181180834770203, 0.680350661277771, 0.6147564053535461, 0.5712665319442749, 0.5762740969657898, 0.5184125304222107, 0.6230789422988892, 0.5710408091545105, 0.695151150226593, 0.5130741596221924, 0.5181586146354675, 0.6190641522407532, 0.5862505435943604, 0.7734150290489197, 0.6276730895042419, 0.5937251448631287, 0.6996105909347534, 0.604559063911438, 0.6209688186645508, 0.6219371557235718, 0.7915785908699036, 0.6065924167633057, 0.6302614212036133]
[2025-05-26 18:26:47,372]: Mean: 0.61691439
[2025-05-26 18:26:47,373]: Min: 0.44337648
[2025-05-26 18:26:47,373]: Max: 0.79157859
[2025-05-26 18:26:47,373]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:26:47,373]: Sample Values (25 elements): [0.042582299560308456, 0.06562670320272446, -0.09179273247718811, 0.04273620992898941, -0.010370620526373386, -0.16400350630283356, -0.025439489632844925, 0.03401889652013779, -0.03610141947865486, -0.07954395562410355, 0.08135196566581726, 0.06330588459968567, -0.042678892612457275, -0.030486101284623146, 0.08219578862190247, -0.017477374523878098, -0.021516865119338036, -0.01458200253546238, -0.01540409866720438, -0.054708853363990784, -0.18072372674942017, 0.0013258344260975718, 0.12248438596725464, -0.07568181306123734, -0.16813859343528748]
[2025-05-26 18:26:47,373]: Mean: -0.00682185
[2025-05-26 18:26:47,374]: Min: -0.41883665
[2025-05-26 18:26:47,374]: Max: 0.36013412
[2025-05-26 18:26:47,374]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,374]: Sample Values (25 elements): [0.7745359539985657, 0.8003256916999817, 0.8451690077781677, 0.6824886202812195, 0.6916071772575378, 0.8142406940460205, 0.6584183573722839, 0.8174126744270325, 0.6356870532035828, 0.8562881350517273, 0.802649974822998, 0.7154409885406494, 0.6927891373634338, 0.8970749378204346, 0.6416400671005249, 0.7244222164154053, 0.7981756329536438, 0.612964391708374, 0.6885740160942078, 0.7590339779853821, 0.7483586072921753, 0.8631769418716431, 0.740879237651825, 0.6641895174980164, 0.6590843200683594]
[2025-05-26 18:26:47,374]: Mean: 0.73509020
[2025-05-26 18:26:47,374]: Min: 0.55349511
[2025-05-26 18:26:47,375]: Max: 0.93527985
[2025-05-26 18:26:47,375]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:26:47,375]: Sample Values (25 elements): [-0.20400890707969666, -0.15333926677703857, -0.015824025496840477, -0.05994681268930435, 0.030213402584195137, -0.05870897322893143, 0.1406073272228241, -0.06798239052295685, -0.014253871515393257, 0.01836903765797615, -0.01864960417151451, 0.05363069847226143, 0.038514643907547, -0.036219533532857895, 0.0024748696014285088, -0.032143790274858475, 0.10356168448925018, 0.023988256230950356, -0.12401083111763, -0.01344798319041729, 0.07252971827983856, -0.05752295255661011, -0.07817419618368149, -0.03671599179506302, 0.033689290285110474]
[2025-05-26 18:26:47,375]: Mean: -0.00096092
[2025-05-26 18:26:47,375]: Min: -0.26209888
[2025-05-26 18:26:47,376]: Max: 0.23454349
[2025-05-26 18:26:47,376]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,376]: Sample Values (25 elements): [1.0097334384918213, 1.0049070119857788, 1.1385976076126099, 1.0249234437942505, 0.9508885145187378, 1.0740472078323364, 1.1082969903945923, 0.7954457402229309, 0.9978612661361694, 1.060229778289795, 0.9398013353347778, 1.0907841920852661, 1.0167672634124756, 1.1385393142700195, 1.1066021919250488, 0.9309044480323792, 1.1290087699890137, 1.0566902160644531, 0.8711851239204407, 0.9683341383934021, 1.20992910861969, 1.043394923210144, 1.0271646976470947, 0.8470305800437927, 1.0918747186660767]
[2025-05-26 18:26:47,376]: Mean: 1.02326941
[2025-05-26 18:26:47,376]: Min: 0.79544574
[2025-05-26 18:26:47,376]: Max: 1.20992911
[2025-05-26 18:26:47,376]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:26:47,377]: Sample Values (25 elements): [0.008764433674514294, 6.61290737249739e-23, 0.014720979146659374, -0.014292626641690731, 0.00174899201374501, -0.0349389910697937, 0.01023703720420599, -0.02388736978173256, 4.904965014676157e-41, 0.0012606052914634347, 0.10697196424007416, -0.027225295081734657, 0.011691863648593426, -0.05175290256738663, -0.045766402035951614, 0.08344560116529465, -0.06724601984024048, -0.06874995678663254, 0.03735647723078728, 0.05494832620024681, -0.026995204389095306, -0.0022799931466579437, -0.002311182441189885, 0.0009749856544658542, 0.01595212146639824]
[2025-05-26 18:26:47,377]: Mean: -0.00295447
[2025-05-26 18:26:47,377]: Min: -0.25645390
[2025-05-26 18:26:47,378]: Max: 0.26871192
[2025-05-26 18:26:47,378]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,378]: Sample Values (25 elements): [0.7429395318031311, 0.6545681953430176, 0.6102316379547119, 0.32958516478538513, 0.8332580327987671, 0.2876930236816406, -7.446818056337186e-28, 0.8630498647689819, 0.8213610649108887, 0.7336665987968445, 4.7018718779545665e-11, 0.5156278014183044, 6.322699874772297e-08, 0.6745226979255676, 0.7494269013404846, 0.5816002488136292, 0.23209881782531738, 0.6973331570625305, 0.7311262488365173, 0.5889797806739807, 0.19425822794437408, 0.6940703988075256, 0.623426079750061, 0.48533740639686584, 0.31118276715278625]
[2025-05-26 18:26:47,378]: Mean: 0.56012666
[2025-05-26 18:26:47,378]: Min: -0.00000000
[2025-05-26 18:26:47,378]: Max: 0.88014901
[2025-05-26 18:26:47,378]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:26:47,379]: Sample Values (25 elements): [-0.04411262646317482, 0.06935721635818481, 0.07473716884851456, -0.0003051627136301249, 0.0011470390018075705, 0.03224078193306923, -0.006199234165251255, 0.010088425129652023, -0.004828325007110834, 0.04695278778672218, -0.010384589433670044, -0.028109878301620483, -0.011193769983947277, 0.0043119266629219055, -0.057033829391002655, -0.057528186589479446, -0.01820097677409649, -0.034567590802907944, 0.029335422441363335, -0.026301179081201553, 0.02813844196498394, -0.022370968014001846, -0.10530463606119156, -4.90622618329405e-41, 0.011296899057924747]
[2025-05-26 18:26:47,379]: Mean: 0.00053455
[2025-05-26 18:26:47,379]: Min: -0.18259490
[2025-05-26 18:26:47,379]: Max: 0.18013866
[2025-05-26 18:26:47,379]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-26 18:26:47,380]: Sample Values (25 elements): [0.9345425963401794, 1.0843603610992432, 0.9381347894668579, 1.061949372291565, 0.9531088471412659, 1.0825742483139038, 1.1297757625579834, 1.0211880207061768, 1.116182565689087, 0.9865328073501587, 0.9478664994239807, 0.9156814813613892, 1.1578963994979858, 0.9777305722236633, 0.9745113849639893, 1.0344995260238647, 1.0161750316619873, 0.9731300473213196, 0.9771143794059753, 1.1207889318466187, 1.0301768779754639, 1.0777970552444458, 1.0629563331604004, 1.0815461874008179, 0.9755261540412903]
[2025-05-26 18:26:47,380]: Mean: 1.02171147
[2025-05-26 18:26:47,380]: Min: 0.87229985
[2025-05-26 18:26:47,380]: Max: 1.17864954
[2025-05-26 18:26:47,380]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-26 18:26:47,380]: Sample Values (25 elements): [0.09481294453144073, 0.21310429275035858, -0.2850428819656372, -0.17064836621284485, -0.30539634823799133, 0.1768137812614441, 0.3029530644416809, -0.2717336118221283, -0.3588087856769562, -0.11303283274173737, -0.07665977627038956, 0.10546819865703583, 0.2769824266433716, 0.28126823902130127, 0.16373023390769958, 0.01192857138812542, -0.33448517322540283, 0.09299034625291824, -0.01941348798573017, -0.16275398433208466, -0.3178350329399109, -0.10959426313638687, 0.2050458937883377, -0.371346652507782, 0.1077866479754448]
[2025-05-26 18:26:47,381]: Mean: -0.05554702
[2025-05-26 18:26:47,381]: Min: -0.69612592
[2025-05-26 18:26:47,381]: Max: 0.40720230
[2025-05-26 18:26:47,381]: Checkpoint of model at path [checkpoint/ResNet20_relu6.ckpt] will be used for QAT
[2025-05-27 03:33:07,255]: Checkpoint of model at path [checkpoint/ResNet20_relu6.ckpt] will be used for QAT
[2025-05-27 03:33:07,256]: 


QAT of ResNet20 with relu6 down to 4 bits...
[2025-05-27 03:33:07,549]: [ResNet20_relu6_quantized_4_bits] after configure_qat:
[2025-05-27 03:33:07,697]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 03:35:03,218]: [ResNet20_relu6_quantized_4_bits] Epoch: 001 Train Loss: 0.3607 Train Acc: 0.8737 Eval Loss: 0.4698 Eval Acc: 0.8510 (LR: 0.00100000)
[2025-05-27 03:36:58,501]: [ResNet20_relu6_quantized_4_bits] Epoch: 002 Train Loss: 0.3564 Train Acc: 0.8759 Eval Loss: 0.5214 Eval Acc: 0.8366 (LR: 0.00100000)
[2025-05-27 03:38:53,881]: [ResNet20_relu6_quantized_4_bits] Epoch: 003 Train Loss: 0.3638 Train Acc: 0.8727 Eval Loss: 0.5096 Eval Acc: 0.8341 (LR: 0.00100000)
[2025-05-27 03:40:49,147]: [ResNet20_relu6_quantized_4_bits] Epoch: 004 Train Loss: 0.3603 Train Acc: 0.8735 Eval Loss: 0.5000 Eval Acc: 0.8391 (LR: 0.00100000)
[2025-05-27 03:42:44,158]: [ResNet20_relu6_quantized_4_bits] Epoch: 005 Train Loss: 0.3564 Train Acc: 0.8752 Eval Loss: 0.4965 Eval Acc: 0.8440 (LR: 0.00100000)
[2025-05-27 03:44:38,394]: [ResNet20_relu6_quantized_4_bits] Epoch: 006 Train Loss: 0.3563 Train Acc: 0.8756 Eval Loss: 0.4582 Eval Acc: 0.8505 (LR: 0.00100000)
[2025-05-27 03:46:32,578]: [ResNet20_relu6_quantized_4_bits] Epoch: 007 Train Loss: 0.3541 Train Acc: 0.8756 Eval Loss: 0.4561 Eval Acc: 0.8568 (LR: 0.00100000)
[2025-05-27 03:48:27,264]: [ResNet20_relu6_quantized_4_bits] Epoch: 008 Train Loss: 0.3487 Train Acc: 0.8773 Eval Loss: 0.4553 Eval Acc: 0.8548 (LR: 0.00100000)
[2025-05-27 03:50:23,416]: [ResNet20_relu6_quantized_4_bits] Epoch: 009 Train Loss: 0.3483 Train Acc: 0.8787 Eval Loss: 0.4298 Eval Acc: 0.8602 (LR: 0.00100000)
[2025-05-27 03:52:18,705]: [ResNet20_relu6_quantized_4_bits] Epoch: 010 Train Loss: 0.3491 Train Acc: 0.8769 Eval Loss: 0.4379 Eval Acc: 0.8590 (LR: 0.00100000)
[2025-05-27 03:54:13,349]: [ResNet20_relu6_quantized_4_bits] Epoch: 011 Train Loss: 0.3403 Train Acc: 0.8812 Eval Loss: 0.4173 Eval Acc: 0.8627 (LR: 0.00100000)
[2025-05-27 03:56:07,319]: [ResNet20_relu6_quantized_4_bits] Epoch: 012 Train Loss: 0.3403 Train Acc: 0.8807 Eval Loss: 0.5887 Eval Acc: 0.8243 (LR: 0.00100000)
[2025-05-27 03:58:00,824]: [ResNet20_relu6_quantized_4_bits] Epoch: 013 Train Loss: 0.3372 Train Acc: 0.8822 Eval Loss: 0.4637 Eval Acc: 0.8477 (LR: 0.00100000)
[2025-05-27 03:59:52,899]: [ResNet20_relu6_quantized_4_bits] Epoch: 014 Train Loss: 0.3352 Train Acc: 0.8825 Eval Loss: 0.4951 Eval Acc: 0.8411 (LR: 0.00100000)
[2025-05-27 04:01:41,626]: [ResNet20_relu6_quantized_4_bits] Epoch: 015 Train Loss: 0.3332 Train Acc: 0.8832 Eval Loss: 0.4829 Eval Acc: 0.8489 (LR: 0.00100000)
[2025-05-27 04:03:22,447]: [ResNet20_relu6_quantized_4_bits] Epoch: 016 Train Loss: 0.3285 Train Acc: 0.8851 Eval Loss: 0.4579 Eval Acc: 0.8520 (LR: 0.00100000)
[2025-05-27 04:05:02,553]: [ResNet20_relu6_quantized_4_bits] Epoch: 017 Train Loss: 0.3293 Train Acc: 0.8840 Eval Loss: 0.4480 Eval Acc: 0.8571 (LR: 0.00010000)
[2025-05-27 04:06:42,001]: [ResNet20_relu6_quantized_4_bits] Epoch: 018 Train Loss: 0.2660 Train Acc: 0.9082 Eval Loss: 0.3302 Eval Acc: 0.8882 (LR: 0.00010000)
[2025-05-27 04:08:22,005]: [ResNet20_relu6_quantized_4_bits] Epoch: 019 Train Loss: 0.2425 Train Acc: 0.9161 Eval Loss: 0.3300 Eval Acc: 0.8911 (LR: 0.00010000)
[2025-05-27 04:10:13,790]: [ResNet20_relu6_quantized_4_bits] Epoch: 020 Train Loss: 0.2331 Train Acc: 0.9189 Eval Loss: 0.3208 Eval Acc: 0.8953 (LR: 0.00010000)
[2025-05-27 04:12:08,868]: [ResNet20_relu6_quantized_4_bits] Epoch: 021 Train Loss: 0.2312 Train Acc: 0.9184 Eval Loss: 0.3198 Eval Acc: 0.8948 (LR: 0.00010000)
[2025-05-27 04:13:58,789]: [ResNet20_relu6_quantized_4_bits] Epoch: 022 Train Loss: 0.2307 Train Acc: 0.9199 Eval Loss: 0.3285 Eval Acc: 0.8933 (LR: 0.00010000)
[2025-05-27 04:15:52,435]: [ResNet20_relu6_quantized_4_bits] Epoch: 023 Train Loss: 0.2247 Train Acc: 0.9207 Eval Loss: 0.3202 Eval Acc: 0.8961 (LR: 0.00010000)
[2025-05-27 04:17:48,809]: [ResNet20_relu6_quantized_4_bits] Epoch: 024 Train Loss: 0.2274 Train Acc: 0.9209 Eval Loss: 0.3228 Eval Acc: 0.8950 (LR: 0.00010000)
[2025-05-27 04:19:45,885]: [ResNet20_relu6_quantized_4_bits] Epoch: 025 Train Loss: 0.2190 Train Acc: 0.9234 Eval Loss: 0.3183 Eval Acc: 0.8961 (LR: 0.00010000)
[2025-05-27 04:21:44,410]: [ResNet20_relu6_quantized_4_bits] Epoch: 026 Train Loss: 0.2186 Train Acc: 0.9229 Eval Loss: 0.3263 Eval Acc: 0.8950 (LR: 0.00010000)
[2025-05-27 04:23:41,765]: [ResNet20_relu6_quantized_4_bits] Epoch: 027 Train Loss: 0.2186 Train Acc: 0.9238 Eval Loss: 0.3274 Eval Acc: 0.8946 (LR: 0.00010000)
[2025-05-27 04:25:38,242]: [ResNet20_relu6_quantized_4_bits] Epoch: 028 Train Loss: 0.2186 Train Acc: 0.9244 Eval Loss: 0.3287 Eval Acc: 0.8935 (LR: 0.00010000)
[2025-05-27 04:27:34,404]: [ResNet20_relu6_quantized_4_bits] Epoch: 029 Train Loss: 0.2114 Train Acc: 0.9260 Eval Loss: 0.3399 Eval Acc: 0.8930 (LR: 0.00010000)
[2025-05-27 04:29:30,526]: [ResNet20_relu6_quantized_4_bits] Epoch: 030 Train Loss: 0.2117 Train Acc: 0.9267 Eval Loss: 0.3253 Eval Acc: 0.8941 (LR: 0.00010000)
[2025-05-27 04:31:26,769]: [ResNet20_relu6_quantized_4_bits] Epoch: 031 Train Loss: 0.2120 Train Acc: 0.9258 Eval Loss: 0.3199 Eval Acc: 0.8984 (LR: 0.00001000)
[2025-05-27 04:33:23,666]: [ResNet20_relu6_quantized_4_bits] Epoch: 032 Train Loss: 0.2037 Train Acc: 0.9292 Eval Loss: 0.3214 Eval Acc: 0.8984 (LR: 0.00001000)
[2025-05-27 04:35:20,470]: [ResNet20_relu6_quantized_4_bits] Epoch: 033 Train Loss: 0.1991 Train Acc: 0.9300 Eval Loss: 0.3192 Eval Acc: 0.8980 (LR: 0.00001000)
[2025-05-27 04:37:18,623]: [ResNet20_relu6_quantized_4_bits] Epoch: 034 Train Loss: 0.1970 Train Acc: 0.9320 Eval Loss: 0.3153 Eval Acc: 0.8990 (LR: 0.00001000)
[2025-05-27 04:39:20,091]: [ResNet20_relu6_quantized_4_bits] Epoch: 035 Train Loss: 0.1998 Train Acc: 0.9307 Eval Loss: 0.3158 Eval Acc: 0.8981 (LR: 0.00001000)
[2025-05-27 04:41:24,003]: [ResNet20_relu6_quantized_4_bits] Epoch: 036 Train Loss: 0.1971 Train Acc: 0.9308 Eval Loss: 0.3133 Eval Acc: 0.8976 (LR: 0.00001000)
[2025-05-27 04:43:28,211]: [ResNet20_relu6_quantized_4_bits] Epoch: 037 Train Loss: 0.1945 Train Acc: 0.9322 Eval Loss: 0.3162 Eval Acc: 0.8975 (LR: 0.00001000)
[2025-05-27 04:45:30,346]: [ResNet20_relu6_quantized_4_bits] Epoch: 038 Train Loss: 0.1920 Train Acc: 0.9326 Eval Loss: 0.3192 Eval Acc: 0.8990 (LR: 0.00001000)
[2025-05-27 04:47:32,255]: [ResNet20_relu6_quantized_4_bits] Epoch: 039 Train Loss: 0.1969 Train Acc: 0.9306 Eval Loss: 0.3183 Eval Acc: 0.9017 (LR: 0.00001000)
[2025-05-27 04:49:34,344]: [ResNet20_relu6_quantized_4_bits] Epoch: 040 Train Loss: 0.1935 Train Acc: 0.9318 Eval Loss: 0.3179 Eval Acc: 0.8960 (LR: 0.00001000)
[2025-05-27 04:51:35,744]: [ResNet20_relu6_quantized_4_bits] Epoch: 041 Train Loss: 0.1925 Train Acc: 0.9319 Eval Loss: 0.3162 Eval Acc: 0.9010 (LR: 0.00001000)
[2025-05-27 04:53:34,097]: [ResNet20_relu6_quantized_4_bits] Epoch: 042 Train Loss: 0.1941 Train Acc: 0.9318 Eval Loss: 0.3196 Eval Acc: 0.8979 (LR: 0.00000100)
[2025-05-27 04:55:29,272]: [ResNet20_relu6_quantized_4_bits] Epoch: 043 Train Loss: 0.1939 Train Acc: 0.9310 Eval Loss: 0.3239 Eval Acc: 0.8992 (LR: 0.00000100)
[2025-05-27 04:57:21,575]: [ResNet20_relu6_quantized_4_bits] Epoch: 044 Train Loss: 0.1925 Train Acc: 0.9321 Eval Loss: 0.3189 Eval Acc: 0.8977 (LR: 0.00000100)
[2025-05-27 04:59:11,179]: [ResNet20_relu6_quantized_4_bits] Epoch: 045 Train Loss: 0.1949 Train Acc: 0.9325 Eval Loss: 0.3154 Eval Acc: 0.8987 (LR: 0.00000100)
[2025-05-27 05:00:50,001]: [ResNet20_relu6_quantized_4_bits] Epoch: 046 Train Loss: 0.1926 Train Acc: 0.9325 Eval Loss: 0.3211 Eval Acc: 0.8992 (LR: 0.00000100)
[2025-05-27 05:02:29,164]: [ResNet20_relu6_quantized_4_bits] Epoch: 047 Train Loss: 0.1917 Train Acc: 0.9322 Eval Loss: 0.3187 Eval Acc: 0.8994 (LR: 0.00000100)
[2025-05-27 05:04:09,482]: [ResNet20_relu6_quantized_4_bits] Epoch: 048 Train Loss: 0.1898 Train Acc: 0.9333 Eval Loss: 0.3167 Eval Acc: 0.8999 (LR: 0.00000010)
[2025-05-27 05:05:50,508]: [ResNet20_relu6_quantized_4_bits] Epoch: 049 Train Loss: 0.1902 Train Acc: 0.9334 Eval Loss: 0.3183 Eval Acc: 0.8980 (LR: 0.00000010)
[2025-05-27 05:07:36,104]: [ResNet20_relu6_quantized_4_bits] Epoch: 050 Train Loss: 0.1921 Train Acc: 0.9334 Eval Loss: 0.3134 Eval Acc: 0.9004 (LR: 0.00000010)
[2025-05-27 05:09:27,202]: [ResNet20_relu6_quantized_4_bits] Epoch: 051 Train Loss: 0.1903 Train Acc: 0.9332 Eval Loss: 0.3194 Eval Acc: 0.8992 (LR: 0.00000010)
[2025-05-27 05:11:19,493]: [ResNet20_relu6_quantized_4_bits] Epoch: 052 Train Loss: 0.1914 Train Acc: 0.9334 Eval Loss: 0.3154 Eval Acc: 0.9002 (LR: 0.00000010)
[2025-05-27 05:13:11,928]: [ResNet20_relu6_quantized_4_bits] Epoch: 053 Train Loss: 0.1924 Train Acc: 0.9326 Eval Loss: 0.3182 Eval Acc: 0.9013 (LR: 0.00000010)
[2025-05-27 05:15:05,146]: [ResNet20_relu6_quantized_4_bits] Epoch: 054 Train Loss: 0.1909 Train Acc: 0.9333 Eval Loss: 0.3208 Eval Acc: 0.8966 (LR: 0.00000010)
[2025-05-27 05:16:58,899]: [ResNet20_relu6_quantized_4_bits] Epoch: 055 Train Loss: 0.1908 Train Acc: 0.9330 Eval Loss: 0.3205 Eval Acc: 0.9003 (LR: 0.00000010)
[2025-05-27 05:18:52,685]: [ResNet20_relu6_quantized_4_bits] Epoch: 056 Train Loss: 0.1927 Train Acc: 0.9318 Eval Loss: 0.3167 Eval Acc: 0.9000 (LR: 0.00000010)
[2025-05-27 05:20:45,371]: [ResNet20_relu6_quantized_4_bits] Epoch: 057 Train Loss: 0.1869 Train Acc: 0.9356 Eval Loss: 0.3203 Eval Acc: 0.8990 (LR: 0.00000010)
[2025-05-27 05:22:38,128]: [ResNet20_relu6_quantized_4_bits] Epoch: 058 Train Loss: 0.1909 Train Acc: 0.9332 Eval Loss: 0.3149 Eval Acc: 0.9004 (LR: 0.00000010)
[2025-05-27 05:24:33,357]: [ResNet20_relu6_quantized_4_bits] Epoch: 059 Train Loss: 0.1921 Train Acc: 0.9332 Eval Loss: 0.3135 Eval Acc: 0.8999 (LR: 0.00000010)
[2025-05-27 05:26:28,837]: [ResNet20_relu6_quantized_4_bits] Epoch: 060 Train Loss: 0.1877 Train Acc: 0.9333 Eval Loss: 0.3193 Eval Acc: 0.8982 (LR: 0.00000010)
[2025-05-27 05:26:28,837]: [ResNet20_relu6_quantized_4_bits] Best Eval Accuracy: 0.9017
[2025-05-27 05:26:28,901]: 


Quantization of model down to 4 bits finished
[2025-05-27 05:26:28,901]: Model Architecture:
[2025-05-27 05:26:29,041]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0640], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.429908812046051, max_val=0.5294564962387085)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0845], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6060055494308472, max_val=0.6619075536727905)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0893], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8366944789886475, max_val=0.5023752450942993)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0645], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4850478768348694, max_val=0.48242247104644775)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0773], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6413191556930542, max_val=0.5187656879425049)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3356], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.034582614898682)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0707], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45954984426498413, max_val=0.6002248525619507)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0618], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4841957688331604, max_val=0.4432549476623535)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3863], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.794787883758545)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0591], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46164846420288086, max_val=0.42412757873535156)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0863], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.592693567276001, max_val=0.7022744417190552)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0493], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37928617000579834, max_val=0.3599609136581421)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3358], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.0370402336120605)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0511], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4209747910499573, max_val=0.3456959128379822)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0553], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4465656280517578, max_val=0.3822404146194458)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2936], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.404075622558594)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0491], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3820447623729706, max_val=0.3543281555175781)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0543], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4139276444911957, max_val=0.40042924880981445)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3149], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.723050594329834)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0515], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3681367039680481, max_val=0.40378111600875854)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0525], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40842318534851074, max_val=0.3791804909706116)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0568], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4396814703941345, max_val=0.41273611783981323)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3769], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.653007507324219)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0357], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2667621374130249, max_val=0.2687750458717346)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0369], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2511066198348999, max_val=0.30182766914367676)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3775], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.662359237670898)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0255], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18845288455486298, max_val=0.19376933574676514)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 05:26:29,041]: 
Model Weights:
[2025-05-27 05:26:29,041]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 05:26:29,042]: Sample Values (25 elements): [0.06166693568229675, -0.03317617252469063, 0.0011208516079932451, 0.1412179172039032, 0.2304205447435379, -0.1111968532204628, 0.021000567823648453, -0.17077183723449707, -0.147422656416893, 0.20266632735729218, 0.015610890462994576, 0.08723150193691254, 0.006066083442419767, -0.007238626480102539, 0.24185357987880707, -0.057112570852041245, -0.1672276109457016, 0.1550261527299881, -0.18856610357761383, 0.1347067654132843, -0.09368843585252762, 0.3733855187892914, -0.0793418288230896, 0.1548992097377777, -0.06337659060955048]
[2025-05-27 05:26:29,042]: Mean: -0.00528979
[2025-05-27 05:26:29,042]: Min: -0.53587884
[2025-05-27 05:26:29,042]: Max: 0.39002383
[2025-05-27 05:26:29,043]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,043]: Sample Values (16 elements): [1.13652765750885, 1.1399540901184082, 1.506011724472046, 1.075226068496704, 1.1678017377853394, 1.0062841176986694, 1.0194765329360962, 0.9557557106018066, 1.0279192924499512, 1.1041510105133057, 0.9783738255500793, 1.1068713665008545, 0.9999867081642151, 1.0219038724899292, 1.0201022624969482, 1.868491291999817]
[2025-05-27 05:26:29,043]: Mean: 1.13342738
[2025-05-27 05:26:29,044]: Min: 0.95575571
[2025-05-27 05:26:29,044]: Max: 1.86849129
[2025-05-27 05:26:29,047]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:29,047]: Sample Values (25 elements): [-0.06395769119262695, -0.19187307357788086, 0.0, -0.1279153823852539, 0.0, -0.1279153823852539, 0.1279153823852539, -0.1279153823852539, 0.0, 0.06395769119262695, 0.0, 0.06395769119262695, -0.06395769119262695, 0.0, -0.06395769119262695, 0.0, -0.06395769119262695, -0.06395769119262695, 0.1279153823852539, -0.19187307357788086, 0.19187307357788086, -0.19187307357788086, 0.1279153823852539, 0.19187307357788086, 0.0]
[2025-05-27 05:26:29,047]: Mean: -0.01218638
[2025-05-27 05:26:29,048]: Min: -0.44770384
[2025-05-27 05:26:29,048]: Max: 0.51166153
[2025-05-27 05:26:29,048]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,048]: Sample Values (16 elements): [1.3371834754943848, 0.8400896191596985, 0.9832061529159546, 1.063048005104065, 1.0186012983322144, 1.2123463153839111, 0.8751600980758667, 0.9376653432846069, 0.8981642127037048, 0.8424162864685059, 0.758975625038147, 1.0020623207092285, 0.7539404034614563, 0.8424679040908813, 0.7993796467781067, 0.8235644102096558]
[2025-05-27 05:26:29,048]: Mean: 0.93676692
[2025-05-27 05:26:29,049]: Min: 0.75394040
[2025-05-27 05:26:29,049]: Max: 1.33718348
[2025-05-27 05:26:29,050]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:29,052]: Sample Values (25 elements): [0.16905508935451508, 0.08452754467725754, 0.08452754467725754, 0.08452754467725754, 0.0, 0.0, 0.0, 0.16905508935451508, 0.0, 0.08452754467725754, -0.16905508935451508, 0.16905508935451508, 0.0, 0.08452754467725754, 0.253582626581192, 0.08452754467725754, 0.08452754467725754, 0.16905508935451508, -0.16905508935451508, -0.33811017870903015, 0.0, 0.08452754467725754, 0.0, -0.16905508935451508, 0.0]
[2025-05-27 05:26:29,052]: Mean: -0.00421904
[2025-05-27 05:26:29,052]: Min: -0.59169281
[2025-05-27 05:26:29,053]: Max: 0.67622036
[2025-05-27 05:26:29,053]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,053]: Sample Values (16 elements): [0.63756263256073, 0.7423945069313049, 0.8049734830856323, 0.7582629323005676, 1.132157325744629, 0.8987743258476257, 0.9941149353981018, 0.7925165295600891, 0.8296170234680176, 0.9893484115600586, 0.7794009447097778, 0.8639518618583679, 1.0644142627716064, 0.5579831004142761, 1.1009547710418701, 0.9993428587913513]
[2025-05-27 05:26:29,054]: Mean: 0.87161064
[2025-05-27 05:26:29,054]: Min: 0.55798310
[2025-05-27 05:26:29,054]: Max: 1.13215733
[2025-05-27 05:26:29,056]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:29,057]: Sample Values (25 elements): [0.08927132189273834, 0.08927132189273834, 0.0, 0.08927132189273834, 0.0, 0.0, 0.0, 0.0, 0.0, -0.17854264378547668, 0.08927132189273834, -0.17854264378547668, 0.0, 0.08927132189273834, 0.0, 0.08927132189273834, -0.6248992681503296, 0.0, 0.08927132189273834, -0.2678139805793762, -0.08927132189273834, 0.0, -0.17854264378547668, 0.0, 0.0]
[2025-05-27 05:26:29,057]: Mean: -0.01119766
[2025-05-27 05:26:29,057]: Min: -0.80344188
[2025-05-27 05:26:29,058]: Max: 0.53562796
[2025-05-27 05:26:29,058]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,058]: Sample Values (16 elements): [0.8112234473228455, 0.8764368295669556, 0.8713313341140747, 0.7891932725906372, 0.9952506422996521, 0.7462073564529419, 0.819912850856781, 1.200069785118103, 0.9260420203208923, 1.0318095684051514, 0.8294886946678162, 0.5666741728782654, 0.8406885266304016, 1.2387560606002808, 0.8992577791213989, 0.7886297106742859]
[2025-05-27 05:26:29,059]: Mean: 0.88943577
[2025-05-27 05:26:29,059]: Min: 0.56667417
[2025-05-27 05:26:29,059]: Max: 1.23875606
[2025-05-27 05:26:29,062]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:29,062]: Sample Values (25 elements): [-0.06449802964925766, 0.3224901556968689, -0.06449802964925766, 0.06449802964925766, -0.19349408149719238, -0.25799211859703064, -0.06449802964925766, 0.0, -0.06449802964925766, 0.0, 0.06449802964925766, -0.25799211859703064, 0.19349408149719238, 0.12899605929851532, -0.06449802964925766, 0.0, 0.0, -0.12899605929851532, -0.06449802964925766, -0.06449802964925766, -0.12899605929851532, -0.06449802964925766, -0.06449802964925766, 0.06449802964925766, 0.06449802964925766]
[2025-05-27 05:26:29,063]: Mean: -0.00067185
[2025-05-27 05:26:29,063]: Min: -0.51598424
[2025-05-27 05:26:29,063]: Max: 0.45148620
[2025-05-27 05:26:29,063]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,064]: Sample Values (16 elements): [0.8721412420272827, 0.8003905415534973, 0.9756979942321777, 1.1864125728607178, 0.840129554271698, 0.7830985188484192, 0.8288112282752991, 0.6274168491363525, 0.6544454097747803, 0.7955259084701538, 1.0245777368545532, 0.6944451332092285, 0.5128554701805115, 0.8761414289474487, 0.5660601258277893, 0.7385392785072327]
[2025-05-27 05:26:29,064]: Mean: 0.79854310
[2025-05-27 05:26:29,064]: Min: 0.51285547
[2025-05-27 05:26:29,064]: Max: 1.18641257
[2025-05-27 05:26:29,067]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:29,068]: Sample Values (25 elements): [0.3093559741973877, 0.07733899354934692, -0.07733899354934692, 0.15467798709869385, -0.23201698064804077, 0.07733899354934692, -0.07733899354934692, 0.0, 0.07733899354934692, -0.15467798709869385, -0.07733899354934692, 0.3866949677467346, 0.15467798709869385, 0.07733899354934692, -0.07733899354934692, 0.0, 0.0, -0.07733899354934692, -0.07733899354934692, -0.07733899354934692, -0.07733899354934692, 0.23201698064804077, 0.0, -0.15467798709869385, -0.07733899354934692]
[2025-05-27 05:26:29,068]: Mean: -0.01302410
[2025-05-27 05:26:29,068]: Min: -0.61871195
[2025-05-27 05:26:29,069]: Max: 0.54137295
[2025-05-27 05:26:29,069]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,069]: Sample Values (16 elements): [0.7390714287757874, 0.8417013883590698, 1.0543831586837769, 0.6691832542419434, 0.9896811246871948, 0.7912845611572266, 0.8789284229278564, 0.7129170894622803, 0.7672967314720154, 0.7305793762207031, 0.7974396347999573, 1.0257794857025146, 1.064963936805725, 0.8577679395675659, 0.7929434776306152, 1.0462086200714111]
[2025-05-27 05:26:29,070]: Mean: 0.86000812
[2025-05-27 05:26:29,070]: Min: 0.66918325
[2025-05-27 05:26:29,070]: Max: 1.06496394
[2025-05-27 05:26:29,073]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:29,074]: Sample Values (25 elements): [0.0, -0.14130328595638275, 0.07065164297819138, 0.0, 0.0, 0.07065164297819138, 0.21195492148399353, 0.07065164297819138, -0.21195492148399353, 0.21195492148399353, 0.0, -0.07065164297819138, -0.14130328595638275, 0.21195492148399353, 0.07065164297819138, 0.0, 0.0, 0.0, 0.07065164297819138, 0.14130328595638275, 0.14130328595638275, -0.14130328595638275, 0.0, -0.07065164297819138, 0.21195492148399353]
[2025-05-27 05:26:29,074]: Mean: 0.01422845
[2025-05-27 05:26:29,074]: Min: -0.49456149
[2025-05-27 05:26:29,074]: Max: 0.56521314
[2025-05-27 05:26:29,074]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 05:26:29,075]: Sample Values (16 elements): [0.630553662776947, 0.5940231084823608, 1.0431731939315796, 0.5232870578765869, 0.9638960957527161, 0.9911044239997864, 0.6827971339225769, 0.699888288974762, 0.6636643409729004, 0.8718430995941162, 0.7624610662460327, 0.7763859629631042, 0.9471789598464966, 0.8033096194267273, 0.7384403944015503, 0.8017131686210632]
[2025-05-27 05:26:29,075]: Mean: 0.78085750
[2025-05-27 05:26:29,076]: Min: 0.52328706
[2025-05-27 05:26:29,076]: Max: 1.04317319
[2025-05-27 05:26:29,077]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 05:26:29,078]: Sample Values (25 elements): [0.12366010248661041, -0.061830051243305206, -0.061830051243305206, 0.12366010248661041, 0.061830051243305206, -0.061830051243305206, 0.0, 0.061830051243305206, -0.24732020497322083, 0.12366010248661041, -0.061830051243305206, -0.12366010248661041, 0.12366010248661041, 0.12366010248661041, 0.18549016118049622, 0.12366010248661041, 0.061830051243305206, 0.061830051243305206, 0.061830051243305206, -0.061830051243305206, 0.18549016118049622, 0.0, 0.061830051243305206, 0.061830051243305206, -0.061830051243305206]
[2025-05-27 05:26:29,078]: Mean: -0.00544770
[2025-05-27 05:26:29,078]: Min: -0.49464041
[2025-05-27 05:26:29,079]: Max: 0.43281037
[2025-05-27 05:26:29,079]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,079]: Sample Values (25 elements): [0.8398065567016602, 0.9698750972747803, 0.9213640093803406, 0.8477048277854919, 0.8880554437637329, 0.882463812828064, 0.8251374363899231, 0.8039867281913757, 0.9018157720565796, 0.776589035987854, 0.7505402565002441, 0.8237767815589905, 0.8860430121421814, 0.9074318408966064, 0.7646600604057312, 1.0527302026748657, 0.8421419858932495, 0.8407796621322632, 0.9446149468421936, 0.9854232668876648, 0.8928380012512207, 0.916773796081543, 0.732911229133606, 0.9263556003570557, 0.8122556209564209]
[2025-05-27 05:26:29,079]: Mean: 0.87595046
[2025-05-27 05:26:29,080]: Min: 0.73291123
[2025-05-27 05:26:29,080]: Max: 1.05273020
[2025-05-27 05:26:29,081]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:29,082]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.17715522646903992, 0.059051740914583206, -0.11810348182916641, 0.059051740914583206, -0.11810348182916641, 0.11810348182916641, -0.11810348182916641, -0.29525870084762573, -0.23620696365833282, -0.11810348182916641, -0.059051740914583206, 0.0, -0.059051740914583206, -0.23620696365833282, -0.11810348182916641, -0.059051740914583206, 0.059051740914583206, -0.059051740914583206, 0.11810348182916641, -0.059051740914583206, -0.17715522646903992, -0.17715522646903992]
[2025-05-27 05:26:29,083]: Mean: -0.00613841
[2025-05-27 05:26:29,083]: Min: -0.47241393
[2025-05-27 05:26:29,083]: Max: 0.41336218
[2025-05-27 05:26:29,083]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,084]: Sample Values (25 elements): [0.8443371057510376, 0.8638989925384521, 1.0096813440322876, 0.9146037101745605, 0.960818350315094, 0.9683717489242554, 0.8174036741256714, 0.9002910256385803, 0.899904727935791, 0.9720102548599243, 0.7935400605201721, 0.8843315839767456, 0.8811699151992798, 0.8309831619262695, 0.9328479766845703, 1.1040431261062622, 0.8926066160202026, 0.9531169533729553, 0.9413877129554749, 0.9676159024238586, 0.9602125883102417, 0.9384571313858032, 1.0091222524642944, 0.8621296286582947, 0.924428403377533]
[2025-05-27 05:26:29,084]: Mean: 0.90826935
[2025-05-27 05:26:29,084]: Min: 0.75236064
[2025-05-27 05:26:29,085]: Max: 1.10404313
[2025-05-27 05:26:29,086]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 05:26:29,087]: Sample Values (25 elements): [-0.08633120357990265, 0.0, 0.0, 0.08633120357990265, 0.1726624071598053, -0.08633120357990265, 0.0, -0.08633120357990265, -0.25899362564086914, -0.1726624071598053, -0.1726624071598053, 0.08633120357990265, 0.25899362564086914, -0.1726624071598053, 0.08633120357990265, -0.08633120357990265, 0.25899362564086914, 0.1726624071598053, 0.25899362564086914, -0.25899362564086914, 0.0, 0.08633120357990265, 0.0, 0.08633120357990265, 0.25899362564086914]
[2025-05-27 05:26:29,087]: Mean: -0.00370954
[2025-05-27 05:26:29,088]: Min: -0.60431844
[2025-05-27 05:26:29,088]: Max: 0.69064963
[2025-05-27 05:26:29,088]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,088]: Sample Values (25 elements): [0.6785077452659607, 0.736996591091156, 0.5460518002510071, 0.6652560234069824, 0.6576206088066101, 0.5807620286941528, 0.5747649073600769, 0.33565372228622437, 0.6038919687271118, 0.7929612994194031, 0.9389951229095459, 0.69532710313797, 0.7865740656852722, 0.5880304574966431, 0.6879866123199463, 0.5660277605056763, 0.6362565159797668, 0.6871699094772339, 0.6576671600341797, 0.8351982831954956, 0.491607666015625, 0.6260595321655273, 0.4695734679698944, 0.7897417545318604, 0.7063749432563782]
[2025-05-27 05:26:29,088]: Mean: 0.64858294
[2025-05-27 05:26:29,089]: Min: 0.33565372
[2025-05-27 05:26:29,089]: Max: 0.93899512
[2025-05-27 05:26:29,091]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:29,092]: Sample Values (25 elements): [-0.049283143132925034, -0.09856628626585007, -0.09856628626585007, 0.1478494256734848, -0.1478494256734848, 0.1478494256734848, 0.049283143132925034, -0.049283143132925034, -0.09856628626585007, -0.19713257253170013, 0.1478494256734848, -0.049283143132925034, 0.0, 0.09856628626585007, -0.049283143132925034, 0.049283143132925034, -0.049283143132925034, 0.0, 0.0, -0.049283143132925034, -0.19713257253170013, 0.049283143132925034, 0.0, -0.09856628626585007, 0.0]
[2025-05-27 05:26:29,092]: Mean: -0.00591440
[2025-05-27 05:26:29,092]: Min: -0.39426515
[2025-05-27 05:26:29,093]: Max: 0.34498200
[2025-05-27 05:26:29,093]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,094]: Sample Values (25 elements): [0.8736525774002075, 0.7400919795036316, 0.7841864824295044, 0.7918930649757385, 0.8188299536705017, 0.7907639741897583, 0.9295684695243835, 0.7307532429695129, 0.8152481317520142, 0.7901374101638794, 0.7627019882202148, 0.7519561052322388, 0.9788536429405212, 0.8796114325523376, 0.8015555739402771, 0.791726291179657, 0.833001434803009, 0.7799923419952393, 0.8121920824050903, 0.7854794263839722, 0.8108677864074707, 0.859944224357605, 0.7898203730583191, 0.778899073600769, 0.7402238845825195]
[2025-05-27 05:26:29,094]: Mean: 0.81107402
[2025-05-27 05:26:29,094]: Min: 0.73075324
[2025-05-27 05:26:29,095]: Max: 0.97885364
[2025-05-27 05:26:29,097]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:29,097]: Sample Values (25 elements): [-0.051111381500959396, -0.3066682815551758, 0.10222276300191879, -0.1533341407775879, -0.051111381500959396, 0.10222276300191879, 0.0, 0.051111381500959396, -0.10222276300191879, -0.10222276300191879, 0.10222276300191879, 0.0, 0.0, 0.0, -0.051111381500959396, -0.10222276300191879, 0.1533341407775879, -0.1533341407775879, 0.051111381500959396, -0.051111381500959396, -0.1533341407775879, 0.10222276300191879, 0.0, 0.0, 0.0]
[2025-05-27 05:26:29,098]: Mean: -0.00490816
[2025-05-27 05:26:29,098]: Min: -0.40889105
[2025-05-27 05:26:29,098]: Max: 0.35777968
[2025-05-27 05:26:29,098]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,099]: Sample Values (25 elements): [0.8305035829544067, 0.8715751767158508, 0.7812595963478088, 0.5955026745796204, 0.6252291202545166, 0.728797435760498, 0.6718887686729431, 0.8027293682098389, 0.7151086330413818, 0.736559271812439, 0.7988439202308655, 0.6435852646827698, 0.8103322982788086, 0.7237576246261597, 0.6906172037124634, 0.7205962538719177, 0.9234414100646973, 0.7037780284881592, 0.872794508934021, 0.8909769654273987, 0.7757428288459778, 0.825934886932373, 0.8342130780220032, 0.7684512734413147, 0.9665011167526245]
[2025-05-27 05:26:29,099]: Mean: 0.77527571
[2025-05-27 05:26:29,099]: Min: 0.59550267
[2025-05-27 05:26:29,099]: Max: 0.96650112
[2025-05-27 05:26:29,101]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:29,102]: Sample Values (25 elements): [0.16576121747493744, -0.27626869082450867, 0.0, 0.11050748080015182, 0.0, 0.05525374040007591, 0.05525374040007591, 0.05525374040007591, -0.05525374040007591, 0.11050748080015182, -0.05525374040007591, 0.0, 0.0, 0.05525374040007591, 0.11050748080015182, -0.05525374040007591, -0.05525374040007591, 0.0, 0.0, 0.0, 0.0, -0.05525374040007591, 0.05525374040007591, 0.0, 0.11050748080015182]
[2025-05-27 05:26:29,103]: Mean: -0.01178698
[2025-05-27 05:26:29,103]: Min: -0.44202992
[2025-05-27 05:26:29,103]: Max: 0.38677618
[2025-05-27 05:26:29,103]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,104]: Sample Values (25 elements): [0.7554211020469666, 0.731728732585907, 0.7735294699668884, 0.6762046813964844, 0.727656364440918, 0.77066969871521, 0.8361420035362244, 0.8100596070289612, 0.848061740398407, 0.837090015411377, 0.7350022196769714, 0.7970027327537537, 0.8031911253929138, 0.8036514520645142, 0.7120193243026733, 0.7415086627006531, 0.7992154359817505, 0.7455961108207703, 0.8494921326637268, 0.8662610054016113, 0.6585493683815002, 0.6992821097373962, 0.6871860027313232, 0.7659996747970581, 0.8305562734603882]
[2025-05-27 05:26:29,104]: Mean: 0.75891471
[2025-05-27 05:26:29,104]: Min: 0.61952186
[2025-05-27 05:26:29,105]: Max: 0.91345215
[2025-05-27 05:26:29,107]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:29,108]: Sample Values (25 elements): [0.04909152910113335, 0.1963661164045334, -0.14727458357810974, 0.0, -0.14727458357810974, 0.1963661164045334, 0.0, -0.04909152910113335, -0.0981830582022667, -0.04909152910113335, -0.0981830582022667, 0.04909152910113335, -0.04909152910113335, -0.0981830582022667, -0.04909152910113335, -0.04909152910113335, 0.0, -0.1963661164045334, -0.04909152910113335, 0.1963661164045334, 0.0981830582022667, -0.0981830582022667, -0.0981830582022667, 0.0, -0.04909152910113335]
[2025-05-27 05:26:29,109]: Mean: -0.00150748
[2025-05-27 05:26:29,109]: Min: -0.39273223
[2025-05-27 05:26:29,109]: Max: 0.34364071
[2025-05-27 05:26:29,109]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 05:26:29,110]: Sample Values (25 elements): [0.6249290704727173, 0.819754958152771, 0.8837910890579224, 0.7359563708305359, 0.7977359890937805, 0.8587871789932251, 0.7937209606170654, 0.785158097743988, 0.832850992679596, 0.7881389856338501, 0.8464581370353699, 0.5613899230957031, 0.7220687866210938, 0.7270915508270264, 0.7855868935585022, 0.6659207940101624, 0.6913178563117981, 0.7741341590881348, 0.7880457639694214, 0.6135903596878052, 0.7690527439117432, 0.765408992767334, 0.6176006197929382, 0.6071226596832275, 0.886527419090271]
[2025-05-27 05:26:29,110]: Mean: 0.76539207
[2025-05-27 05:26:29,111]: Min: 0.56138992
[2025-05-27 05:26:29,111]: Max: 1.00096571
[2025-05-27 05:26:29,113]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 05:26:29,113]: Sample Values (25 elements): [-0.10858091711997986, -0.05429045855998993, -0.05429045855998993, 0.10858091711997986, 0.10858091711997986, 0.0, -0.10858091711997986, -0.05429045855998993, -0.1628713756799698, 0.0, 0.05429045855998993, 0.0, 0.0, 0.10858091711997986, 0.05429045855998993, -0.05429045855998993, 0.1628713756799698, 0.1628713756799698, 0.0, -0.10858091711997986, -0.1628713756799698, 0.1628713756799698, -0.1628713756799698, 0.0, 0.05429045855998993]
[2025-05-27 05:26:29,114]: Mean: -0.00600282
[2025-05-27 05:26:29,114]: Min: -0.43432367
[2025-05-27 05:26:29,114]: Max: 0.38003320
[2025-05-27 05:26:29,114]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,115]: Sample Values (25 elements): [0.6896340847015381, 0.6641659140586853, 0.7951642274856567, 0.8085165023803711, 0.7807947397232056, 0.7463372945785522, 0.7933146357536316, 0.6841582655906677, 0.7411144971847534, 0.7707909345626831, 0.7438364624977112, 0.7766520977020264, 0.710212230682373, 0.8611621260643005, 0.7271048426628113, 0.6957697868347168, 0.7731470465660095, 0.7707571387290955, 0.7186654210090637, 0.7389335036277771, 0.8582082986831665, 0.8977555632591248, 0.7122278809547424, 0.7992637753486633, 0.8072631359100342]
[2025-05-27 05:26:29,115]: Mean: 0.77073157
[2025-05-27 05:26:29,115]: Min: 0.63013601
[2025-05-27 05:26:29,115]: Max: 1.01030743
[2025-05-27 05:26:29,118]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:29,118]: Sample Values (25 elements): [0.0, -0.10292237997055054, -0.10292237997055054, 0.10292237997055054, 0.1543835699558258, 0.20584475994110107, 0.0, 0.05146118998527527, 0.0, -0.10292237997055054, -0.05146118998527527, -0.10292237997055054, -0.20584475994110107, -0.05146118998527527, 0.05146118998527527, -0.05146118998527527, -0.10292237997055054, 0.0, -0.05146118998527527, 0.05146118998527527, 0.10292237997055054, -0.05146118998527527, -0.05146118998527527, 0.05146118998527527, -0.20584475994110107]
[2025-05-27 05:26:29,119]: Mean: -0.00448108
[2025-05-27 05:26:29,121]: Min: -0.36022833
[2025-05-27 05:26:29,123]: Max: 0.41168952
[2025-05-27 05:26:29,123]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,134]: Sample Values (25 elements): [1.0379095077514648, 0.9576348662376404, 0.9573303461074829, 0.6750278472900391, 0.9011257886886597, 0.957424521446228, 0.7241016030311584, 0.9568628668785095, 0.9692652821540833, 1.1090086698532104, 1.0324277877807617, 0.8459953665733337, 0.9226120114326477, 0.8949711918830872, 0.9854141473770142, 0.6432445049285889, 0.9915784001350403, 0.8117722272872925, 1.095353364944458, 1.0456961393356323, 0.8495558500289917, 0.9063391089439392, 0.8498716950416565, 0.8060635924339294, 0.9447993636131287]
[2025-05-27 05:26:29,151]: Mean: 0.92584902
[2025-05-27 05:26:29,162]: Min: 0.64324450
[2025-05-27 05:26:29,167]: Max: 1.24152887
[2025-05-27 05:26:29,168]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 05:26:29,169]: Sample Values (25 elements): [0.05250691622495651, 0.367548406124115, 0.05250691622495651, -0.10501383244991302, -0.05250691622495651, -0.05250691622495651, 0.05250691622495651, 0.15752074122428894, -0.10501383244991302, 0.0, -0.15752074122428894, -0.05250691622495651, 0.05250691622495651, 0.10501383244991302, -0.05250691622495651, -0.21002766489982605, -0.10501383244991302, -0.15752074122428894, 0.10501383244991302, 0.05250691622495651, 0.15752074122428894, 0.0, 0.26253458857536316, 0.0, -0.10501383244991302]
[2025-05-27 05:26:29,170]: Mean: -0.01038345
[2025-05-27 05:26:29,170]: Min: -0.42005533
[2025-05-27 05:26:29,170]: Max: 0.36754841
[2025-05-27 05:26:29,170]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,171]: Sample Values (25 elements): [0.656944215297699, 0.5586870312690735, 0.40799733996391296, 0.5037909150123596, 0.41670605540275574, 0.5549479126930237, 0.5475302338600159, 0.5260701775550842, 0.491809606552124, 0.47902604937553406, 0.6091079115867615, 0.4976867139339447, 0.6499366760253906, 0.5313774347305298, 0.39537474513053894, 0.49928829073905945, 0.5170438289642334, 0.5216971039772034, 0.5356418490409851, 0.48580655455589294, 0.5009338855743408, 0.5386472940444946, 0.5154229402542114, 0.5740025043487549, 0.5551548600196838]
[2025-05-27 05:26:29,171]: Mean: 0.54068214
[2025-05-27 05:26:29,171]: Min: 0.37157676
[2025-05-27 05:26:29,171]: Max: 0.69979370
[2025-05-27 05:26:29,174]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:29,174]: Sample Values (25 elements): [-0.1704835295677185, -0.1704835295677185, 0.0568278431892395, 0.1704835295677185, -0.113655686378479, 0.0568278431892395, 0.0568278431892395, 0.0568278431892395, 0.113655686378479, 0.0568278431892395, 0.0568278431892395, 0.113655686378479, 0.0568278431892395, 0.0568278431892395, 0.1704835295677185, -0.1704835295677185, 0.227311372756958, -0.0568278431892395, -0.0568278431892395, 0.0568278431892395, 0.0, -0.113655686378479, 0.0, 0.113655686378479, 0.0568278431892395]
[2025-05-27 05:26:29,175]: Mean: -0.00628491
[2025-05-27 05:26:29,175]: Min: -0.45462275
[2025-05-27 05:26:29,175]: Max: 0.39779490
[2025-05-27 05:26:29,175]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,176]: Sample Values (25 elements): [0.706588625907898, 0.81202632188797, 0.7629257440567017, 0.7199212312698364, 0.7828678488731384, 0.7765880227088928, 0.7758188843727112, 0.7059085369110107, 0.9377108216285706, 0.6800357699394226, 0.7760014533996582, 0.8425047993659973, 0.6615644693374634, 0.6900587677955627, 0.6757124066352844, 0.704938530921936, 0.8676315546035767, 0.5459333658218384, 0.7645582556724548, 0.5940886735916138, 0.6292837262153625, 0.7969070076942444, 0.7982348203659058, 0.7127708196640015, 0.7210244536399841]
[2025-05-27 05:26:29,176]: Mean: 0.72754991
[2025-05-27 05:26:29,176]: Min: 0.54593337
[2025-05-27 05:26:29,177]: Max: 0.93771082
[2025-05-27 05:26:29,179]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:29,180]: Sample Values (25 elements): [0.07140496373176575, -0.07140496373176575, 0.0, -0.10710744559764862, -0.07140496373176575, 0.1428099274635315, -0.035702481865882874, 0.035702481865882874, -0.035702481865882874, 0.07140496373176575, 0.10710744559764862, -0.035702481865882874, -0.035702481865882874, -0.035702481865882874, -0.21421489119529724, -0.035702481865882874, -0.035702481865882874, -0.035702481865882874, 0.0, -0.07140496373176575, 0.0, 0.0, 0.035702481865882874, 0.035702481865882874, 0.0]
[2025-05-27 05:26:29,180]: Mean: -0.00141400
[2025-05-27 05:26:29,181]: Min: -0.24991737
[2025-05-27 05:26:29,182]: Max: 0.28561985
[2025-05-27 05:26:29,182]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,182]: Sample Values (25 elements): [1.202606201171875, 1.0366772413253784, 0.7399978637695312, 0.7615789771080017, 0.8826736211776733, 1.2097814083099365, 0.9760745763778687, 0.8268073797225952, 1.088791012763977, 1.0556303262710571, 0.8818712830543518, 1.1295524835586548, 0.9914795160293579, 1.0223416090011597, 0.962020754814148, 1.0618196725845337, 1.0854120254516602, 1.1226879358291626, 0.9354655742645264, 1.1297255754470825, 0.9826297760009766, 1.0139907598495483, 1.0825433731079102, 1.0324199199676514, 1.0292271375656128]
[2025-05-27 05:26:29,183]: Mean: 0.98447919
[2025-05-27 05:26:29,183]: Min: 0.71118277
[2025-05-27 05:26:29,183]: Max: 1.20978141
[2025-05-27 05:26:29,185]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:29,187]: Sample Values (25 elements): [0.0, -0.03686228767037392, 0.07372457534074783, 0.0, -0.07372457534074783, -0.07372457534074783, -0.03686228767037392, 0.11058686673641205, 0.03686228767037392, -0.11058686673641205, 0.0, -0.07372457534074783, 0.0, -0.03686228767037392, 0.0, 0.0, 0.03686228767037392, 0.0, -0.03686228767037392, -0.11058686673641205, 0.03686228767037392, 0.0, -0.03686228767037392, -0.03686228767037392, 0.0]
[2025-05-27 05:26:29,187]: Mean: -0.00266388
[2025-05-27 05:26:29,187]: Min: -0.25803602
[2025-05-27 05:26:29,188]: Max: 0.29489830
[2025-05-27 05:26:29,188]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,188]: Sample Values (25 elements): [0.5452480912208557, 0.6401183605194092, 0.00024265662068501115, -6.03441157692196e-41, 0.5578667521476746, 0.7241191267967224, 0.5494005084037781, 0.6949396729469299, 0.7723245620727539, 0.8075567483901978, 0.0267658568918705, 0.7086194753646851, 0.7328647971153259, 0.8139908909797668, 0.6478105187416077, 0.5824230313301086, 0.8070486187934875, -4.905385404215455e-41, 0.4167492389678955, -5.852663166099031e-41, 0.7347821593284607, 0.483038067817688, 0.7458899617195129, 0.6372969150543213, -5.438439340044615e-41]
[2025-05-27 05:26:29,189]: Mean: 0.47418261
[2025-05-27 05:26:29,189]: Min: -0.00000000
[2025-05-27 05:26:29,189]: Max: 0.84246266
[2025-05-27 05:26:29,192]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:29,193]: Sample Values (25 elements): [0.0, 0.02548147924244404, -0.05096295848488808, 0.05096295848488808, 0.0, 0.05096295848488808, -0.02548147924244404, 0.10192591696977615, 0.0, 0.02548147924244404, 0.02548147924244404, -0.05096295848488808, -0.02548147924244404, 0.02548147924244404, 0.0, 0.0, 0.0, 0.0, -0.07644443958997726, 0.0, -0.05096295848488808, -0.02548147924244404, 0.0, 0.10192591696977615, 0.05096295848488808]
[2025-05-27 05:26:29,193]: Mean: 0.00061174
[2025-05-27 05:26:29,193]: Min: -0.17837036
[2025-05-27 05:26:29,194]: Max: 0.20385183
[2025-05-27 05:26:29,194]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 05:26:29,194]: Sample Values (25 elements): [0.8505144119262695, 0.9063935875892639, 1.119107961654663, 1.0232574939727783, 1.004083514213562, 0.8962810039520264, 1.1144911050796509, 0.9433371424674988, 0.9350246787071228, 0.9726917743682861, 1.020123839378357, 1.0746386051177979, 1.1405402421951294, 0.9962666034698486, 1.0519912242889404, 1.0598902702331543, 0.8280297517776489, 1.0957560539245605, 0.9022067189216614, 1.0393301248550415, 1.0004901885986328, 1.0131715536117554, 0.8546745181083679, 0.9247595071792603, 0.8276147842407227]
[2025-05-27 05:26:29,194]: Mean: 0.96466142
[2025-05-27 05:26:29,195]: Min: 0.80716646
[2025-05-27 05:26:29,196]: Max: 1.14476061
[2025-05-27 05:26:29,196]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 05:26:29,197]: Sample Values (25 elements): [-0.25920793414115906, -0.48245710134506226, -0.3166801929473877, -0.026562711223959923, -0.15119317173957825, -0.17965580523014069, 0.13734661042690277, -0.26415932178497314, 0.11575149744749069, 0.20025646686553955, 0.25183263421058655, 0.4424157440662384, 0.015163756906986237, 0.0116830263286829, -0.4337652027606964, -0.1469549536705017, 0.1661963313817978, -0.2731929123401642, 0.05230715125799179, 0.31240275502204895, -0.11459185928106308, -0.1685144454240799, -0.7959898710250854, 0.2538200616836548, -0.07175350189208984]
[2025-05-27 05:26:29,197]: Mean: -0.06426903
[2025-05-27 05:26:29,197]: Min: -0.79598987
[2025-05-27 05:26:29,198]: Max: 0.45864093
[2025-05-27 05:26:29,198]: 


QAT of ResNet20 with relu6 down to 3 bits...
[2025-05-27 05:26:29,453]: [ResNet20_relu6_quantized_3_bits] after configure_qat:
[2025-05-27 05:26:29,585]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 05:28:24,757]: [ResNet20_relu6_quantized_3_bits] Epoch: 001 Train Loss: 0.5076 Train Acc: 0.8232 Eval Loss: 0.5212 Eval Acc: 0.8317 (LR: 0.00100000)
[2025-05-27 05:30:19,613]: [ResNet20_relu6_quantized_3_bits] Epoch: 002 Train Loss: 0.4850 Train Acc: 0.8307 Eval Loss: 0.6944 Eval Acc: 0.7776 (LR: 0.00100000)
[2025-05-27 05:32:14,989]: [ResNet20_relu6_quantized_3_bits] Epoch: 003 Train Loss: 0.4824 Train Acc: 0.8321 Eval Loss: 0.5841 Eval Acc: 0.8047 (LR: 0.00100000)
[2025-05-27 05:34:10,454]: [ResNet20_relu6_quantized_3_bits] Epoch: 004 Train Loss: 0.4729 Train Acc: 0.8338 Eval Loss: 0.5118 Eval Acc: 0.8273 (LR: 0.00100000)
[2025-05-27 05:36:05,839]: [ResNet20_relu6_quantized_3_bits] Epoch: 005 Train Loss: 0.4771 Train Acc: 0.8348 Eval Loss: 0.6099 Eval Acc: 0.7993 (LR: 0.00100000)
[2025-05-27 05:38:01,396]: [ResNet20_relu6_quantized_3_bits] Epoch: 006 Train Loss: 0.4627 Train Acc: 0.8368 Eval Loss: 0.5320 Eval Acc: 0.8201 (LR: 0.00100000)
[2025-05-27 05:39:56,756]: [ResNet20_relu6_quantized_3_bits] Epoch: 007 Train Loss: 0.4624 Train Acc: 0.8385 Eval Loss: 0.5719 Eval Acc: 0.8129 (LR: 0.00100000)
[2025-05-27 05:41:52,331]: [ResNet20_relu6_quantized_3_bits] Epoch: 008 Train Loss: 0.4631 Train Acc: 0.8390 Eval Loss: 0.7287 Eval Acc: 0.7744 (LR: 0.00100000)
[2025-05-27 05:43:48,833]: [ResNet20_relu6_quantized_3_bits] Epoch: 009 Train Loss: 0.4551 Train Acc: 0.8404 Eval Loss: 0.5876 Eval Acc: 0.8065 (LR: 0.00100000)
[2025-05-27 05:45:45,295]: [ResNet20_relu6_quantized_3_bits] Epoch: 010 Train Loss: 0.4541 Train Acc: 0.8414 Eval Loss: 0.5559 Eval Acc: 0.8144 (LR: 0.00010000)
[2025-05-27 05:47:43,054]: [ResNet20_relu6_quantized_3_bits] Epoch: 011 Train Loss: 0.3750 Train Acc: 0.8687 Eval Loss: 0.4013 Eval Acc: 0.8658 (LR: 0.00010000)
[2025-05-27 05:49:41,909]: [ResNet20_relu6_quantized_3_bits] Epoch: 012 Train Loss: 0.3562 Train Acc: 0.8760 Eval Loss: 0.3855 Eval Acc: 0.8736 (LR: 0.00010000)
[2025-05-27 05:51:40,913]: [ResNet20_relu6_quantized_3_bits] Epoch: 013 Train Loss: 0.3485 Train Acc: 0.8790 Eval Loss: 0.3988 Eval Acc: 0.8660 (LR: 0.00010000)
[2025-05-27 05:53:40,551]: [ResNet20_relu6_quantized_3_bits] Epoch: 014 Train Loss: 0.3430 Train Acc: 0.8804 Eval Loss: 0.3949 Eval Acc: 0.8681 (LR: 0.00010000)
[2025-05-27 05:55:37,997]: [ResNet20_relu6_quantized_3_bits] Epoch: 015 Train Loss: 0.3409 Train Acc: 0.8805 Eval Loss: 0.4116 Eval Acc: 0.8644 (LR: 0.00010000)
[2025-05-27 05:57:33,763]: [ResNet20_relu6_quantized_3_bits] Epoch: 016 Train Loss: 0.3399 Train Acc: 0.8819 Eval Loss: 0.4125 Eval Acc: 0.8651 (LR: 0.00010000)
[2025-05-27 05:59:29,360]: [ResNet20_relu6_quantized_3_bits] Epoch: 017 Train Loss: 0.3315 Train Acc: 0.8837 Eval Loss: 0.3971 Eval Acc: 0.8686 (LR: 0.00010000)
[2025-05-27 06:01:25,031]: [ResNet20_relu6_quantized_3_bits] Epoch: 018 Train Loss: 0.3367 Train Acc: 0.8819 Eval Loss: 0.4336 Eval Acc: 0.8564 (LR: 0.00001000)
[2025-05-27 06:03:21,001]: [ResNet20_relu6_quantized_3_bits] Epoch: 019 Train Loss: 0.3150 Train Acc: 0.8916 Eval Loss: 0.3775 Eval Acc: 0.8754 (LR: 0.00001000)
[2025-05-27 06:05:17,092]: [ResNet20_relu6_quantized_3_bits] Epoch: 020 Train Loss: 0.3119 Train Acc: 0.8924 Eval Loss: 0.3737 Eval Acc: 0.8774 (LR: 0.00001000)
[2025-05-27 06:07:13,778]: [ResNet20_relu6_quantized_3_bits] Epoch: 021 Train Loss: 0.3116 Train Acc: 0.8906 Eval Loss: 0.3741 Eval Acc: 0.8723 (LR: 0.00001000)
[2025-05-27 06:09:09,594]: [ResNet20_relu6_quantized_3_bits] Epoch: 022 Train Loss: 0.3085 Train Acc: 0.8930 Eval Loss: 0.3765 Eval Acc: 0.8758 (LR: 0.00001000)
[2025-05-27 06:11:02,768]: [ResNet20_relu6_quantized_3_bits] Epoch: 023 Train Loss: 0.3107 Train Acc: 0.8913 Eval Loss: 0.3706 Eval Acc: 0.8752 (LR: 0.00001000)
[2025-05-27 06:12:59,111]: [ResNet20_relu6_quantized_3_bits] Epoch: 024 Train Loss: 0.3089 Train Acc: 0.8924 Eval Loss: 0.3790 Eval Acc: 0.8763 (LR: 0.00001000)
[2025-05-27 06:14:55,755]: [ResNet20_relu6_quantized_3_bits] Epoch: 025 Train Loss: 0.3098 Train Acc: 0.8904 Eval Loss: 0.3769 Eval Acc: 0.8770 (LR: 0.00001000)
[2025-05-27 06:16:52,085]: [ResNet20_relu6_quantized_3_bits] Epoch: 026 Train Loss: 0.3096 Train Acc: 0.8921 Eval Loss: 0.3806 Eval Acc: 0.8738 (LR: 0.00001000)
[2025-05-27 06:18:50,553]: [ResNet20_relu6_quantized_3_bits] Epoch: 027 Train Loss: 0.3090 Train Acc: 0.8920 Eval Loss: 0.3726 Eval Acc: 0.8773 (LR: 0.00001000)
[2025-05-27 06:20:47,208]: [ResNet20_relu6_quantized_3_bits] Epoch: 028 Train Loss: 0.3047 Train Acc: 0.8942 Eval Loss: 0.3798 Eval Acc: 0.8757 (LR: 0.00001000)
[2025-05-27 06:22:42,806]: [ResNet20_relu6_quantized_3_bits] Epoch: 029 Train Loss: 0.3086 Train Acc: 0.8929 Eval Loss: 0.3752 Eval Acc: 0.8768 (LR: 0.00000100)
[2025-05-27 06:24:38,499]: [ResNet20_relu6_quantized_3_bits] Epoch: 030 Train Loss: 0.3040 Train Acc: 0.8935 Eval Loss: 0.3767 Eval Acc: 0.8767 (LR: 0.00000100)
[2025-05-27 06:26:34,127]: [ResNet20_relu6_quantized_3_bits] Epoch: 031 Train Loss: 0.3041 Train Acc: 0.8938 Eval Loss: 0.3692 Eval Acc: 0.8781 (LR: 0.00000100)
[2025-05-27 06:28:29,652]: [ResNet20_relu6_quantized_3_bits] Epoch: 032 Train Loss: 0.2991 Train Acc: 0.8968 Eval Loss: 0.3697 Eval Acc: 0.8790 (LR: 0.00000100)
[2025-05-27 06:30:24,189]: [ResNet20_relu6_quantized_3_bits] Epoch: 033 Train Loss: 0.3025 Train Acc: 0.8948 Eval Loss: 0.3764 Eval Acc: 0.8749 (LR: 0.00000100)
[2025-05-27 06:32:18,188]: [ResNet20_relu6_quantized_3_bits] Epoch: 034 Train Loss: 0.3057 Train Acc: 0.8932 Eval Loss: 0.3711 Eval Acc: 0.8787 (LR: 0.00000100)
[2025-05-27 06:34:12,580]: [ResNet20_relu6_quantized_3_bits] Epoch: 035 Train Loss: 0.3030 Train Acc: 0.8944 Eval Loss: 0.3755 Eval Acc: 0.8789 (LR: 0.00000100)
[2025-05-27 06:36:08,031]: [ResNet20_relu6_quantized_3_bits] Epoch: 036 Train Loss: 0.3009 Train Acc: 0.8946 Eval Loss: 0.3651 Eval Acc: 0.8790 (LR: 0.00000100)
[2025-05-27 06:38:03,831]: [ResNet20_relu6_quantized_3_bits] Epoch: 037 Train Loss: 0.3028 Train Acc: 0.8940 Eval Loss: 0.3709 Eval Acc: 0.8785 (LR: 0.00000100)
[2025-05-27 06:39:58,169]: [ResNet20_relu6_quantized_3_bits] Epoch: 038 Train Loss: 0.3031 Train Acc: 0.8944 Eval Loss: 0.3710 Eval Acc: 0.8788 (LR: 0.00000100)
[2025-05-27 06:41:52,183]: [ResNet20_relu6_quantized_3_bits] Epoch: 039 Train Loss: 0.2979 Train Acc: 0.8955 Eval Loss: 0.3702 Eval Acc: 0.8780 (LR: 0.00000100)
[2025-05-27 06:43:46,189]: [ResNet20_relu6_quantized_3_bits] Epoch: 040 Train Loss: 0.3013 Train Acc: 0.8952 Eval Loss: 0.3766 Eval Acc: 0.8765 (LR: 0.00000100)
[2025-05-27 06:45:39,629]: [ResNet20_relu6_quantized_3_bits] Epoch: 041 Train Loss: 0.3019 Train Acc: 0.8947 Eval Loss: 0.3733 Eval Acc: 0.8789 (LR: 0.00000100)
[2025-05-27 06:47:30,284]: [ResNet20_relu6_quantized_3_bits] Epoch: 042 Train Loss: 0.3033 Train Acc: 0.8935 Eval Loss: 0.3644 Eval Acc: 0.8778 (LR: 0.00000100)
[2025-05-27 06:49:13,030]: [ResNet20_relu6_quantized_3_bits] Epoch: 043 Train Loss: 0.3034 Train Acc: 0.8935 Eval Loss: 0.3726 Eval Acc: 0.8795 (LR: 0.00000100)
[2025-05-27 06:50:52,619]: [ResNet20_relu6_quantized_3_bits] Epoch: 044 Train Loss: 0.3002 Train Acc: 0.8950 Eval Loss: 0.3652 Eval Acc: 0.8818 (LR: 0.00000100)
[2025-05-27 06:52:31,982]: [ResNet20_relu6_quantized_3_bits] Epoch: 045 Train Loss: 0.3039 Train Acc: 0.8940 Eval Loss: 0.3737 Eval Acc: 0.8748 (LR: 0.00000100)
[2025-05-27 06:54:11,166]: [ResNet20_relu6_quantized_3_bits] Epoch: 046 Train Loss: 0.3048 Train Acc: 0.8938 Eval Loss: 0.3648 Eval Acc: 0.8807 (LR: 0.00000100)
[2025-05-27 06:55:55,653]: [ResNet20_relu6_quantized_3_bits] Epoch: 047 Train Loss: 0.3012 Train Acc: 0.8955 Eval Loss: 0.3722 Eval Acc: 0.8752 (LR: 0.00000100)
[2025-05-27 06:57:46,856]: [ResNet20_relu6_quantized_3_bits] Epoch: 048 Train Loss: 0.3002 Train Acc: 0.8963 Eval Loss: 0.3692 Eval Acc: 0.8777 (LR: 0.00000010)
[2025-05-27 06:59:36,626]: [ResNet20_relu6_quantized_3_bits] Epoch: 049 Train Loss: 0.3044 Train Acc: 0.8938 Eval Loss: 0.3764 Eval Acc: 0.8749 (LR: 0.00000010)
[2025-05-27 07:01:29,814]: [ResNet20_relu6_quantized_3_bits] Epoch: 050 Train Loss: 0.2993 Train Acc: 0.8954 Eval Loss: 0.3644 Eval Acc: 0.8802 (LR: 0.00000010)
[2025-05-27 07:03:25,778]: [ResNet20_relu6_quantized_3_bits] Epoch: 051 Train Loss: 0.3029 Train Acc: 0.8944 Eval Loss: 0.3644 Eval Acc: 0.8782 (LR: 0.00000010)
[2025-05-27 07:05:23,047]: [ResNet20_relu6_quantized_3_bits] Epoch: 052 Train Loss: 0.2969 Train Acc: 0.8950 Eval Loss: 0.3719 Eval Acc: 0.8762 (LR: 0.00000010)
[2025-05-27 07:07:21,517]: [ResNet20_relu6_quantized_3_bits] Epoch: 053 Train Loss: 0.2991 Train Acc: 0.8959 Eval Loss: 0.3720 Eval Acc: 0.8777 (LR: 0.00000010)
[2025-05-27 07:09:08,172]: [ResNet20_relu6_quantized_3_bits] Epoch: 054 Train Loss: 0.3029 Train Acc: 0.8947 Eval Loss: 0.3746 Eval Acc: 0.8760 (LR: 0.00000010)
[2025-05-27 07:10:41,306]: [ResNet20_relu6_quantized_3_bits] Epoch: 055 Train Loss: 0.3007 Train Acc: 0.8944 Eval Loss: 0.3669 Eval Acc: 0.8806 (LR: 0.00000010)
[2025-05-27 07:12:13,810]: [ResNet20_relu6_quantized_3_bits] Epoch: 056 Train Loss: 0.2996 Train Acc: 0.8958 Eval Loss: 0.3727 Eval Acc: 0.8767 (LR: 0.00000010)
[2025-05-27 07:13:46,446]: [ResNet20_relu6_quantized_3_bits] Epoch: 057 Train Loss: 0.2986 Train Acc: 0.8964 Eval Loss: 0.3674 Eval Acc: 0.8798 (LR: 0.00000010)
[2025-05-27 07:15:19,268]: [ResNet20_relu6_quantized_3_bits] Epoch: 058 Train Loss: 0.3012 Train Acc: 0.8942 Eval Loss: 0.3608 Eval Acc: 0.8795 (LR: 0.00000010)
[2025-05-27 07:16:53,620]: [ResNet20_relu6_quantized_3_bits] Epoch: 059 Train Loss: 0.3038 Train Acc: 0.8936 Eval Loss: 0.3667 Eval Acc: 0.8813 (LR: 0.00000010)
[2025-05-27 07:18:33,815]: [ResNet20_relu6_quantized_3_bits] Epoch: 060 Train Loss: 0.3004 Train Acc: 0.8952 Eval Loss: 0.3634 Eval Acc: 0.8782 (LR: 0.00000010)
[2025-05-27 07:18:33,815]: [ResNet20_relu6_quantized_3_bits] Best Eval Accuracy: 0.8818
[2025-05-27 07:18:33,872]: 


Quantization of model down to 3 bits finished
[2025-05-27 07:18:33,872]: Model Architecture:
[2025-05-27 07:18:33,927]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1322], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4368301033973694, max_val=0.4884805679321289)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1808], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6271954774856567, max_val=0.6386614441871643)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1900], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8358751535415649, max_val=0.49425846338272095)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1266], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4430495500564575, max_val=0.4430496096611023)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1692], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6131269931793213, max_val=0.5710881948471069)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7708], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.395388603210449)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1523], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48488569259643555, max_val=0.5813804864883423)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1298], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48719045519828796, max_val=0.4214283227920532)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8563], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.993967056274414)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1249], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4359620213508606, max_val=0.43838363885879517)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1811], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5374552011489868, max_val=0.7304056882858276)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1061], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36527591943740845, max_val=0.37729936838150024)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8261], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.782648086547852)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1115], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41398048400878906, max_val=0.366291880607605)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1078], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4132668375968933, max_val=0.3414556086063385)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6877], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.813549995422363)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0993], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3394373655319214, max_val=0.3559766411781311)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1166], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.432267427444458, max_val=0.3838781714439392)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7447], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.212942600250244)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1191], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43005287647247314, max_val=0.40387165546417236)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1148], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4365633726119995, max_val=0.3667478561401367)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1283], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4553448557853699, max_val=0.4427706003189087)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8500], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.9496588706970215)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0803], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27766042947769165, max_val=0.2845054268836975)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0716], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2257232367992401, max_val=0.27516424655914307)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8447], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.913207530975342)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20445013046264648, max_val=0.21152406930923462)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 07:18:33,928]: 
Model Weights:
[2025-05-27 07:18:33,928]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 07:18:33,928]: Sample Values (25 elements): [-0.025484098121523857, -0.1252221316099167, 0.18603076040744781, 0.27526238560676575, -0.12229840457439423, 0.07148534059524536, -0.23365457355976105, -0.3100201487541199, 0.05682404339313507, 0.3702240586280823, -0.015783201903104782, -0.27075091004371643, -0.12226552516222, 0.23553159832954407, 0.25600308179855347, 0.12935888767242432, -0.13411566615104675, 0.2818755805492401, -0.037576206028461456, -0.20941194891929626, -0.15060800313949585, -0.07276873290538788, 0.017565665766596794, 0.11572468280792236, -0.10344593971967697]
[2025-05-27 07:18:33,928]: Mean: -0.00435505
[2025-05-27 07:18:33,929]: Min: -0.51569515
[2025-05-27 07:18:33,929]: Max: 0.37022406
[2025-05-27 07:18:33,929]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,929]: Sample Values (16 elements): [1.2044340372085571, 1.3762129545211792, 1.3693516254425049, 1.495525598526001, 1.2425055503845215, 1.1290894746780396, 1.1888736486434937, 1.330538034439087, 1.1784290075302124, 1.8288683891296387, 1.2738760709762573, 2.2407538890838623, 1.2393834590911865, 1.161950945854187, 1.273311972618103, 1.060650110244751]
[2025-05-27 07:18:33,929]: Mean: 1.34960961
[2025-05-27 07:18:33,929]: Min: 1.06065011
[2025-05-27 07:18:33,930]: Max: 2.24075389
[2025-05-27 07:18:33,931]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:18:33,931]: Sample Values (25 elements): [-0.13218724727630615, 0.13218724727630615, 0.0, 0.13218724727630615, -0.13218724727630615, 0.0, -0.13218724727630615, 0.0, 0.0, 0.0, 0.0, -0.13218724727630615, 0.13218724727630615, 0.13218724727630615, 0.0, 0.0, -0.13218724727630615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,931]: Mean: -0.01325315
[2025-05-27 07:18:33,931]: Min: -0.39656174
[2025-05-27 07:18:33,932]: Max: 0.52874899
[2025-05-27 07:18:33,932]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,932]: Sample Values (16 elements): [0.7763710618019104, 1.1909761428833008, 0.9546425342559814, 0.9293273687362671, 1.046491265296936, 0.917954683303833, 1.0844625234603882, 1.0661616325378418, 0.9730829000473022, 1.1674972772598267, 1.0522346496582031, 0.8601614236831665, 1.5087025165557861, 1.014505386352539, 1.3590538501739502, 1.135611653327942]
[2025-05-27 07:18:33,932]: Mean: 1.06482732
[2025-05-27 07:18:33,932]: Min: 0.77637106
[2025-05-27 07:18:33,932]: Max: 1.50870252
[2025-05-27 07:18:33,933]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:18:33,934]: Sample Values (25 elements): [0.0, -0.18083670735359192, 0.18083670735359192, 0.0, -0.18083670735359192, 0.0, 0.0, 0.0, -0.18083670735359192, 0.18083670735359192, 0.0, -0.18083670735359192, 0.0, 0.0, -0.18083670735359192, 0.0, 0.0, 0.0, 0.18083670735359192, 0.0, 0.0, -0.18083670735359192, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,934]: Mean: -0.00368894
[2025-05-27 07:18:33,934]: Min: -0.54251015
[2025-05-27 07:18:33,934]: Max: 0.72334683
[2025-05-27 07:18:33,934]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,934]: Sample Values (16 elements): [0.9008407592773438, 1.0314786434173584, 0.9346086382865906, 0.9034977555274963, 1.0062508583068848, 1.1679235696792603, 1.0551313161849976, 0.6521696448326111, 1.0150043964385986, 0.8491225838661194, 0.811104953289032, 0.6187678575515747, 0.7487366199493408, 1.1120761632919312, 0.9528031945228577, 0.8528494834899902]
[2025-05-27 07:18:33,935]: Mean: 0.91327292
[2025-05-27 07:18:33,935]: Min: 0.61876786
[2025-05-27 07:18:33,935]: Max: 1.16792357
[2025-05-27 07:18:33,936]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:18:33,936]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.19001910090446472, -0.19001910090446472, -0.19001910090446472, 0.19001910090446472, 0.0, -0.19001910090446472, 0.19001910090446472, 0.0, 0.0, 0.0, 0.19001910090446472, -0.19001910090446472, 0.0, -0.19001910090446472, -0.19001910090446472, 0.19001910090446472, -0.19001910090446472, 0.0, 0.0, 0.0, 0.19001910090446472, 0.0]
[2025-05-27 07:18:33,936]: Mean: -0.01022672
[2025-05-27 07:18:33,937]: Min: -0.76007640
[2025-05-27 07:18:33,937]: Max: 0.57005727
[2025-05-27 07:18:33,937]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,937]: Sample Values (16 elements): [0.967464029788971, 0.8494114875793457, 0.8577336668968201, 0.8210338950157166, 0.9131103157997131, 1.1232006549835205, 0.9255459904670715, 0.9176913499832153, 1.1142007112503052, 0.9812092185020447, 0.9732766151428223, 1.3350098133087158, 1.0621418952941895, 0.6632060408592224, 0.9395963549613953, 1.3689640760421753]
[2025-05-27 07:18:33,938]: Mean: 0.98829973
[2025-05-27 07:18:33,938]: Min: 0.66320604
[2025-05-27 07:18:33,938]: Max: 1.36896408
[2025-05-27 07:18:33,939]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:18:33,939]: Sample Values (25 elements): [-0.12658560276031494, 0.2531712055206299, 0.12658560276031494, 0.0, 0.12658560276031494, -0.12658560276031494, 0.3797568082809448, -0.12658560276031494, 0.0, -0.2531712055206299, 0.0, -0.12658560276031494, 0.12658560276031494, 0.0, 0.0, 0.12658560276031494, 0.0, -0.12658560276031494, -0.2531712055206299, -0.12658560276031494, -0.12658560276031494, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,940]: Mean: 0.00203284
[2025-05-27 07:18:33,940]: Min: -0.37975681
[2025-05-27 07:18:33,940]: Max: 0.37975681
[2025-05-27 07:18:33,940]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,940]: Sample Values (16 elements): [0.8420872688293457, 0.882282555103302, 1.087875247001648, 0.7481754422187805, 1.1373370885849, 1.069994330406189, 0.9372407793998718, 0.647047758102417, 0.6007428169250488, 0.8744452595710754, 0.6759394407272339, 0.7887174487113953, 0.9170835614204407, 0.772635281085968, 0.5191652178764343, 0.8336228132247925]
[2025-05-27 07:18:33,940]: Mean: 0.83339953
[2025-05-27 07:18:33,941]: Min: 0.51916522
[2025-05-27 07:18:33,941]: Max: 1.13733709
[2025-05-27 07:18:33,942]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:18:33,942]: Sample Values (25 elements): [0.0, 0.16917361319065094, 0.0, 0.0, 0.0, -0.16917361319065094, 0.0, 0.0, 0.16917361319065094, 0.0, -0.16917361319065094, -0.16917361319065094, 0.0, 0.0, 0.16917361319065094, -0.3383472263813019, 0.16917361319065094, 0.0, 0.0, 0.0, -0.16917361319065094, -0.16917361319065094, 0.0, 0.16917361319065094, 0.16917361319065094]
[2025-05-27 07:18:33,942]: Mean: -0.01094048
[2025-05-27 07:18:33,942]: Min: -0.67669445
[2025-05-27 07:18:33,942]: Max: 0.50752085
[2025-05-27 07:18:33,942]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,943]: Sample Values (16 elements): [1.2240451574325562, 0.7350696325302124, 1.162252426147461, 0.995717465877533, 1.1598647832870483, 1.1260626316070557, 0.8069167733192444, 0.8754119873046875, 0.8784335255622864, 0.7855773568153381, 0.7058814764022827, 0.9122262001037598, 0.8736757636070251, 0.9810508489608765, 1.0965934991836548, 0.8245869278907776]
[2025-05-27 07:18:33,943]: Mean: 0.94646037
[2025-05-27 07:18:33,943]: Min: 0.70588148
[2025-05-27 07:18:33,943]: Max: 1.22404516
[2025-05-27 07:18:33,944]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:18:33,944]: Sample Values (25 elements): [0.15232375264167786, -0.15232375264167786, 0.15232375264167786, 0.15232375264167786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15232375264167786, 0.0, -0.15232375264167786, 0.0, 0.0, -0.15232375264167786, 0.15232375264167786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.15232375264167786, 0.15232375264167786]
[2025-05-27 07:18:33,945]: Mean: 0.01599928
[2025-05-27 07:18:33,945]: Min: -0.45697126
[2025-05-27 07:18:33,945]: Max: 0.60929501
[2025-05-27 07:18:33,945]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 07:18:33,945]: Sample Values (16 elements): [0.6121585369110107, 0.8760948777198792, 0.8122616410255432, 0.7889930009841919, 1.0835182666778564, 0.7479251027107239, 0.9929550886154175, 0.8461483120918274, 0.7330445647239685, 1.1117229461669922, 0.9151298403739929, 1.0301600694656372, 0.7372161149978638, 0.660054087638855, 0.7751345634460449, 0.6493222117424011]
[2025-05-27 07:18:33,945]: Mean: 0.83573997
[2025-05-27 07:18:33,945]: Min: 0.61215854
[2025-05-27 07:18:33,946]: Max: 1.11172295
[2025-05-27 07:18:33,947]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 07:18:33,947]: Sample Values (25 elements): [-0.12980268895626068, 0.12980268895626068, 0.0, -0.12980268895626068, -0.12980268895626068, 0.12980268895626068, 0.0, 0.0, 0.0, 0.12980268895626068, 0.12980268895626068, 0.0, 0.12980268895626068, 0.12980268895626068, 0.12980268895626068, 0.0, -0.12980268895626068, -0.12980268895626068, -0.25960537791252136, 0.0, 0.0, -0.12980268895626068, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,947]: Mean: -0.00445070
[2025-05-27 07:18:33,947]: Min: -0.51921076
[2025-05-27 07:18:33,947]: Max: 0.38940805
[2025-05-27 07:18:33,947]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,948]: Sample Values (25 elements): [0.8854859471321106, 0.9637867212295532, 0.8926282525062561, 0.9562692642211914, 1.0762273073196411, 1.0429540872573853, 1.1394153833389282, 0.9360651969909668, 0.9249774813652039, 1.0540837049484253, 0.9830846190452576, 0.990429699420929, 0.940144419670105, 0.8334043025970459, 0.8692371249198914, 0.9388630390167236, 0.9642149209976196, 0.8679654002189636, 0.949774444103241, 1.0438110828399658, 0.9768164157867432, 0.8944492340087891, 1.101785659790039, 1.0913430452346802, 0.844878077507019]
[2025-05-27 07:18:33,948]: Mean: 0.97438651
[2025-05-27 07:18:33,948]: Min: 0.83340430
[2025-05-27 07:18:33,948]: Max: 1.13941538
[2025-05-27 07:18:33,949]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:18:33,949]: Sample Values (25 elements): [0.124906525015831, 0.0, 0.249813050031662, 0.0, 0.249813050031662, 0.0, -0.124906525015831, 0.0, 0.0, 0.124906525015831, 0.124906525015831, -0.124906525015831, 0.249813050031662, 0.0, 0.0, 0.0, 0.0, -0.124906525015831, -0.124906525015831, 0.124906525015831, 0.0, -0.124906525015831, 0.124906525015831, -0.124906525015831, 0.0]
[2025-05-27 07:18:33,950]: Mean: -0.00577367
[2025-05-27 07:18:33,950]: Min: -0.37471956
[2025-05-27 07:18:33,950]: Max: 0.49962610
[2025-05-27 07:18:33,950]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,950]: Sample Values (25 elements): [0.8821717500686646, 0.9941237568855286, 0.9886189699172974, 1.0523674488067627, 1.0232279300689697, 0.9523399472236633, 0.8293043375015259, 0.9754546284675598, 0.9314507842063904, 0.8835430145263672, 0.8961631655693054, 1.108558177947998, 0.9595692157745361, 0.8533298373222351, 1.0329058170318604, 1.0930217504501343, 0.9361193776130676, 0.9624961018562317, 0.8583472967147827, 1.0155037641525269, 1.0039016008377075, 0.9080448746681213, 0.8881185054779053, 1.1761829853057861, 1.0269709825515747]
[2025-05-27 07:18:33,950]: Mean: 0.97256052
[2025-05-27 07:18:33,951]: Min: 0.82930434
[2025-05-27 07:18:33,951]: Max: 1.17618299
[2025-05-27 07:18:33,952]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 07:18:33,952]: Sample Values (25 elements): [0.18112298846244812, -0.18112298846244812, 0.36224597692489624, 0.18112298846244812, 0.18112298846244812, 0.0, -0.18112298846244812, 0.36224597692489624, -0.18112298846244812, 0.543368935585022, 0.0, 0.0, 0.0, -0.18112298846244812, 0.0, 0.0, 0.0, -0.18112298846244812, -0.18112298846244812, 0.0, 0.18112298846244812, -0.18112298846244812, -0.36224597692489624, 0.36224597692489624, 0.0]
[2025-05-27 07:18:33,952]: Mean: -0.00318380
[2025-05-27 07:18:33,952]: Min: -0.54336894
[2025-05-27 07:18:33,952]: Max: 0.72449195
[2025-05-27 07:18:33,952]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,953]: Sample Values (25 elements): [0.8344627618789673, 0.6885088086128235, 0.569663941860199, 0.38083386421203613, 0.4202551245689392, 0.664682924747467, 0.6633585691452026, 0.8846463561058044, 0.7590776085853577, 0.6705751419067383, 0.6510589718818665, 1.0211385488510132, 0.8376728892326355, 0.6574532389640808, 0.6750149726867676, 0.6866866946220398, 0.7208777070045471, 0.6518207788467407, 0.9709168076515198, 0.6655144691467285, 0.4868295192718506, 0.8205711245536804, 0.5712908506393433, 0.862377941608429, 0.6928799152374268]
[2025-05-27 07:18:33,953]: Mean: 0.69746470
[2025-05-27 07:18:33,953]: Min: 0.38083386
[2025-05-27 07:18:33,953]: Max: 1.02113855
[2025-05-27 07:18:33,954]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:18:33,955]: Sample Values (25 elements): [-0.21216437220573425, 0.0, 0.0, 0.21216437220573425, -0.10608218610286713, 0.0, 0.0, 0.21216437220573425, -0.10608218610286713, 0.0, 0.21216437220573425, 0.10608218610286713, 0.10608218610286713, 0.0, 0.10608218610286713, -0.21216437220573425, 0.0, 0.0, -0.10608218610286713, -0.10608218610286713, 0.0, 0.10608218610286713, 0.0, 0.0, -0.10608218610286713]
[2025-05-27 07:18:33,955]: Mean: -0.00479994
[2025-05-27 07:18:33,955]: Min: -0.31824654
[2025-05-27 07:18:33,955]: Max: 0.42432874
[2025-05-27 07:18:33,955]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,955]: Sample Values (25 elements): [0.8962569236755371, 0.905331552028656, 0.8397963047027588, 0.8495560884475708, 0.7807275652885437, 0.8496174812316895, 0.8710758090019226, 0.8394367694854736, 0.9618360996246338, 0.9486150741577148, 0.8960534334182739, 0.8642072677612305, 0.910804808139801, 0.938585638999939, 0.8554899096488953, 0.7999135851860046, 0.9234565496444702, 0.8336252570152283, 0.8460109233856201, 1.0659117698669434, 0.9106333255767822, 0.9727214574813843, 0.9934089779853821, 0.8553826808929443, 0.884786069393158]
[2025-05-27 07:18:33,955]: Mean: 0.88693213
[2025-05-27 07:18:33,956]: Min: 0.78072757
[2025-05-27 07:18:33,956]: Max: 1.06591177
[2025-05-27 07:18:33,957]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:18:33,957]: Sample Values (25 elements): [0.0, 0.0, -0.22293497622013092, -0.11146748811006546, 0.22293497622013092, -0.11146748811006546, 0.11146748811006546, 0.0, 0.11146748811006546, 0.22293497622013092, 0.11146748811006546, -0.334402471780777, 0.0, 0.0, 0.0, 0.22293497622013092, 0.0, 0.11146748811006546, 0.11146748811006546, 0.0, 0.0, 0.0, 0.0, 0.11146748811006546, 0.0]
[2025-05-27 07:18:33,957]: Mean: -0.00476543
[2025-05-27 07:18:33,957]: Min: -0.44586995
[2025-05-27 07:18:33,957]: Max: 0.33440247
[2025-05-27 07:18:33,958]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,958]: Sample Values (25 elements): [0.7704219818115234, 0.7220682501792908, 0.8616814017295837, 0.6671813130378723, 0.8803344368934631, 0.855012834072113, 0.9425995349884033, 0.8136061429977417, 0.8260256052017212, 0.6605425477027893, 0.7811293601989746, 0.9521570801734924, 0.7953046560287476, 0.8392042517662048, 0.8513978719711304, 0.7055382132530212, 0.8595532774925232, 0.7624340653419495, 0.6928274035453796, 0.8647006750106812, 0.9021695256233215, 0.9968157410621643, 0.8154295682907104, 0.9619432091712952, 0.9488452076911926]
[2025-05-27 07:18:33,958]: Mean: 0.82805729
[2025-05-27 07:18:33,958]: Min: 0.66054255
[2025-05-27 07:18:33,959]: Max: 0.99681574
[2025-05-27 07:18:33,960]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:18:33,960]: Sample Values (25 elements): [0.0, 0.0, 0.2156350016593933, -0.10781750082969666, 0.10781750082969666, 0.0, 0.0, -0.2156350016593933, 0.0, 0.0, 0.0, -0.10781750082969666, 0.0, 0.0, 0.0, -0.10781750082969666, -0.10781750082969666, -0.2156350016593933, 0.2156350016593933, 0.10781750082969666, 0.10781750082969666, 0.10781750082969666, 0.10781750082969666, 0.0, -0.10781750082969666]
[2025-05-27 07:18:33,960]: Mean: -0.01058755
[2025-05-27 07:18:33,960]: Min: -0.43127000
[2025-05-27 07:18:33,960]: Max: 0.32345250
[2025-05-27 07:18:33,960]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,961]: Sample Values (25 elements): [0.7344125509262085, 0.7820776104927063, 0.8003154993057251, 0.9284267425537109, 0.9106628894805908, 0.7958501577377319, 0.8496707081794739, 0.7287019491195679, 0.8869224190711975, 0.8160057663917542, 0.7541177272796631, 0.8467778563499451, 0.9738421440124512, 0.8080019950866699, 0.7308439612388611, 0.8952049612998962, 0.7830434441566467, 0.8532004952430725, 0.8268393874168396, 0.8101865649223328, 0.7638419270515442, 0.6860211491584778, 0.892517626285553, 0.7654363512992859, 0.8649527430534363]
[2025-05-27 07:18:33,961]: Mean: 0.81814575
[2025-05-27 07:18:33,961]: Min: 0.65829235
[2025-05-27 07:18:33,961]: Max: 0.97384214
[2025-05-27 07:18:33,962]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:18:33,962]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.09934486448764801, 0.0, 0.0, 0.09934486448764801, 0.0, 0.19868972897529602, 0.0, 0.09934486448764801, 0.0, 0.0, -0.09934486448764801, 0.0, -0.09934486448764801, -0.19868972897529602, 0.09934486448764801, 0.09934486448764801, 0.0, 0.0, 0.19868972897529602, 0.09934486448764801, 0.19868972897529602]
[2025-05-27 07:18:33,962]: Mean: -0.00104562
[2025-05-27 07:18:33,963]: Min: -0.29803461
[2025-05-27 07:18:33,963]: Max: 0.39737946
[2025-05-27 07:18:33,963]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 07:18:33,963]: Sample Values (25 elements): [0.697527289390564, 0.810052752494812, 1.0768603086471558, 0.8323315978050232, 0.8143647909164429, 0.7097575068473816, 0.8917126059532166, 0.8551051020622253, 0.812214732170105, 0.8516659140586853, 0.6889681220054626, 0.6529473662376404, 0.9109282493591309, 1.0722981691360474, 0.8157873153686523, 0.8480622172355652, 0.9342256188392639, 0.6820958256721497, 0.7974844574928284, 0.8383655548095703, 0.593894898891449, 0.8368571996688843, 0.8653334975242615, 0.700459361076355, 0.7930614352226257]
[2025-05-27 07:18:33,963]: Mean: 0.82183564
[2025-05-27 07:18:33,963]: Min: 0.59389490
[2025-05-27 07:18:33,964]: Max: 1.07686031
[2025-05-27 07:18:33,965]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 07:18:33,965]: Sample Values (25 elements): [0.11659223586320877, 0.0, 0.11659223586320877, 0.0, 0.11659223586320877, -0.11659223586320877, 0.23318447172641754, 0.23318447172641754, -0.11659223586320877, 0.0, 0.0, -0.11659223586320877, -0.23318447172641754, 0.0, 0.23318447172641754, 0.0, 0.11659223586320877, -0.11659223586320877, 0.0, 0.11659223586320877, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,965]: Mean: -0.00498452
[2025-05-27 07:18:33,965]: Min: -0.46636894
[2025-05-27 07:18:33,965]: Max: 0.34977672
[2025-05-27 07:18:33,966]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,966]: Sample Values (25 elements): [0.8114607334136963, 0.9687376618385315, 0.8566344976425171, 0.7560902237892151, 0.9871734976768494, 0.8200426697731018, 0.838862955570221, 0.9054529070854187, 1.0969116687774658, 0.7311949729919434, 0.8147053122520447, 0.727152943611145, 0.7625761032104492, 0.8613909482955933, 0.8719309568405151, 0.8551851511001587, 0.8080955147743225, 0.8588042855262756, 0.8589442372322083, 0.7998775839805603, 0.873231053352356, 0.91679847240448, 0.8419320583343506, 0.7844289541244507, 0.9189295768737793]
[2025-05-27 07:18:33,966]: Mean: 0.85387743
[2025-05-27 07:18:33,966]: Min: 0.72715294
[2025-05-27 07:18:33,966]: Max: 1.09691167
[2025-05-27 07:18:33,967]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:18:33,968]: Sample Values (25 elements): [0.0, -0.11913207918405533, 0.11913207918405533, 0.0, 0.11913207918405533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11913207918405533, 0.11913207918405533, 0.0, -0.11913207918405533, 0.11913207918405533, 0.0, 0.11913207918405533, 0.0, 0.0, -0.11913207918405533, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,968]: Mean: -0.00387477
[2025-05-27 07:18:33,968]: Min: -0.47652832
[2025-05-27 07:18:33,968]: Max: 0.35739625
[2025-05-27 07:18:33,968]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,969]: Sample Values (25 elements): [1.0425152778625488, 0.8783717155456543, 0.9133777618408203, 1.0179115533828735, 0.8570669889450073, 1.0482454299926758, 1.0304158926010132, 1.1433731317520142, 0.8639316558837891, 0.639782190322876, 0.9973613023757935, 1.0074968338012695, 0.7987865805625916, 0.8949925899505615, 1.1194387674331665, 1.0120676755905151, 1.0221956968307495, 1.0204994678497314, 0.9999850392341614, 1.1456648111343384, 0.8903822302818298, 1.0578380823135376, 1.0321599245071411, 1.1599204540252686, 1.0000584125518799]
[2025-05-27 07:18:33,969]: Mean: 0.97581607
[2025-05-27 07:18:33,969]: Min: 0.63978219
[2025-05-27 07:18:33,969]: Max: 1.34035528
[2025-05-27 07:18:33,970]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 07:18:33,970]: Sample Values (25 elements): [-0.3442762494087219, -0.11475875228643417, 0.11475875228643417, 0.11475875228643417, 0.11475875228643417, 0.22951750457286835, 0.0, -0.22951750457286835, -0.11475875228643417, 0.0, 0.22951750457286835, 0.0, 0.0, 0.11475875228643417, 0.0, 0.22951750457286835, -0.11475875228643417, 0.0, -0.11475875228643417, 0.0, 0.11475875228643417, 0.0, 0.22951750457286835, 0.11475875228643417, 0.22951750457286835]
[2025-05-27 07:18:33,970]: Mean: -0.01025432
[2025-05-27 07:18:33,970]: Min: -0.45903501
[2025-05-27 07:18:33,971]: Max: 0.34427625
[2025-05-27 07:18:33,971]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,971]: Sample Values (25 elements): [0.46891435980796814, 0.5157806277275085, 0.5329073667526245, 0.7389109134674072, 0.6036828756332397, 0.6174618601799011, 0.6071687936782837, 0.6460474133491516, 0.6948702335357666, 0.7518509030342102, 0.6273756623268127, 0.6040346622467041, 0.5629022717475891, 0.7442081570625305, 0.5249994397163391, 0.574175238609314, 0.463346928358078, 0.6720417737960815, 0.632834255695343, 0.6372852921485901, 0.6839720606803894, 0.49265116453170776, 0.5066328644752502, 0.5573593378067017, 0.5158490538597107]
[2025-05-27 07:18:33,971]: Mean: 0.59064841
[2025-05-27 07:18:33,971]: Min: 0.38402185
[2025-05-27 07:18:33,971]: Max: 0.75185090
[2025-05-27 07:18:33,973]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:18:33,973]: Sample Values (25 elements): [0.0, 0.0, -0.1283022165298462, 0.0, 0.1283022165298462, 0.0, -0.1283022165298462, 0.0, 0.0, -0.1283022165298462, 0.0, 0.1283022165298462, 0.0, 0.0, -0.2566044330596924, 0.0, -0.2566044330596924, -0.2566044330596924, 0.0, -0.1283022165298462, 0.0, 0.0, -0.1283022165298462, 0.1283022165298462, -0.1283022165298462]
[2025-05-27 07:18:33,973]: Mean: -0.00485519
[2025-05-27 07:18:33,974]: Min: -0.51320887
[2025-05-27 07:18:33,974]: Max: 0.38490665
[2025-05-27 07:18:33,974]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,974]: Sample Values (25 elements): [0.7868422865867615, 0.8842875361442566, 0.7965708374977112, 0.834384024143219, 0.811698853969574, 0.7838571071624756, 0.7894005179405212, 0.8740851879119873, 0.9058491587638855, 0.7667320370674133, 0.696111261844635, 0.7507988810539246, 0.713581383228302, 0.9023056626319885, 0.8757272362709045, 0.7094751596450806, 0.8158146739006042, 0.7494901418685913, 0.9639155864715576, 0.7387938499450684, 0.8628861308097839, 0.8864120841026306, 0.8331196904182434, 0.7741304039955139, 0.7516191601753235]
[2025-05-27 07:18:33,974]: Mean: 0.79886472
[2025-05-27 07:18:33,974]: Min: 0.60610980
[2025-05-27 07:18:33,975]: Max: 1.05431700
[2025-05-27 07:18:33,975]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:18:33,976]: Sample Values (25 elements): [0.0, 0.0803094133734703, -0.0803094133734703, 0.0803094133734703, 0.0, -0.0803094133734703, 0.1606188267469406, 0.0, -0.0803094133734703, 0.0, 0.0, 0.0803094133734703, 0.0803094133734703, 0.0, 0.0, 0.0803094133734703, 0.0803094133734703, -0.0803094133734703, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0803094133734703, 0.0]
[2025-05-27 07:18:33,977]: Mean: -0.00101737
[2025-05-27 07:18:33,977]: Min: -0.24092823
[2025-05-27 07:18:33,977]: Max: 0.32123765
[2025-05-27 07:18:33,977]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,977]: Sample Values (25 elements): [1.1399459838867188, 0.8444063663482666, 0.7791361212730408, 0.9955443739891052, 1.0882586240768433, 0.9993809461593628, 1.0968064069747925, 1.1124546527862549, 0.8771472573280334, 0.8184108138084412, 0.8963304758071899, 1.0188783407211304, 0.7000257968902588, 0.8215434551239014, 1.045612096786499, 1.030362606048584, 1.1690268516540527, 1.0302776098251343, 0.8765015602111816, 0.8980435729026794, 0.9991163015365601, 1.1683017015457153, 1.0254188776016235, 0.870428204536438, 1.0861035585403442]
[2025-05-27 07:18:33,977]: Mean: 0.97925293
[2025-05-27 07:18:33,978]: Min: 0.70002580
[2025-05-27 07:18:33,978]: Max: 1.16902685
[2025-05-27 07:18:33,979]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:18:33,979]: Sample Values (25 elements): [-0.07155536115169525, 0.0, 0.0, 0.0, 0.07155536115169525, 0.0, -0.07155536115169525, 0.0, 0.0, 0.0, 0.07155536115169525, 0.0, 0.0, -0.07155536115169525, 0.0, 0.0, 0.07155536115169525, 0.0, 0.0, -0.07155536115169525, -0.07155536115169525, 0.0, 0.07155536115169525, 0.0, 0.0]
[2025-05-27 07:18:33,980]: Mean: -0.00142280
[2025-05-27 07:18:33,980]: Min: -0.21466608
[2025-05-27 07:18:33,980]: Max: 0.28622144
[2025-05-27 07:18:33,980]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,980]: Sample Values (25 elements): [0.5937120318412781, 0.8469100594520569, 0.7529646158218384, 0.6169207096099854, 0.5597397685050964, -5.108013162156823e-41, 0.700765073299408, 0.7990648746490479, 0.7465806007385254, 0.6011272668838501, 0.45286720991134644, 0.8809334635734558, 0.5868284702301025, 0.6636699438095093, 0.5931734442710876, 0.5357834100723267, 0.3164350986480713, 0.7454622983932495, -4.96213799202061e-41, 0.18465445935726166, 5.881670044310555e-41, -5.852663166099031e-41, 0.4917234778404236, -4.950367084920281e-41, 0.7752798795700073]
[2025-05-27 07:18:33,980]: Mean: 0.51881504
[2025-05-27 07:18:33,981]: Min: -0.00000000
[2025-05-27 07:18:33,981]: Max: 0.89043587
[2025-05-27 07:18:33,982]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:18:33,982]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.05942488834261894, 0.0, 0.05942488834261894, -0.05942488834261894, 0.05942488834261894, 0.05942488834261894, 0.05942488834261894, -0.05942488834261894, -0.05942488834261894, 0.0, 0.0, -0.05942488834261894, 0.0, 0.0, 0.0, -0.05942488834261894, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 07:18:33,982]: Mean: 0.00198921
[2025-05-27 07:18:33,983]: Min: -0.17827466
[2025-05-27 07:18:33,983]: Max: 0.23769955
[2025-05-27 07:18:33,983]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 07:18:33,983]: Sample Values (25 elements): [0.9826468825340271, 1.01211416721344, 0.845909833908081, 0.8961901068687439, 0.9565355181694031, 0.8779994249343872, 0.8496720194816589, 0.8804031610488892, 0.9951487183570862, 0.7868677377700806, 1.055281400680542, 0.8914944529533386, 0.89256352186203, 0.8999936580657959, 0.9352484345436096, 1.0347129106521606, 0.9405916333198547, 0.9357594847679138, 0.8970853090286255, 0.9277693033218384, 0.9518647193908691, 0.9865142703056335, 0.9659267067909241, 0.8423318862915039, 0.9812853932380676]
[2025-05-27 07:18:33,983]: Mean: 0.92130911
[2025-05-27 07:18:33,983]: Min: 0.76781166
[2025-05-27 07:18:33,984]: Max: 1.10694301
[2025-05-27 07:18:33,984]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 07:18:33,984]: Sample Values (25 elements): [0.09695695340633392, 0.04541199281811714, -0.3766580820083618, 0.10629518330097198, 0.2406211495399475, 0.25698477029800415, 0.1549539417028427, 0.23108604550361633, -0.39805126190185547, 0.1251377910375595, 0.2223033756017685, 0.22184644639492035, -0.3568795323371887, -0.3657428026199341, 0.03888630121946335, -0.26100972294807434, -0.30617010593414307, -0.44037511944770813, -0.0386509969830513, 0.09708574414253235, 0.19395533204078674, 0.25318264961242676, 0.3390604853630066, -0.1466437131166458, -0.4121907353401184]
[2025-05-27 07:18:33,984]: Mean: -0.05604643
[2025-05-27 07:18:33,984]: Min: -0.71292394
[2025-05-27 07:18:33,985]: Max: 0.42749682
[2025-05-27 07:18:33,985]: 


QAT of ResNet20 with relu6 down to 2 bits...
[2025-05-27 07:18:34,097]: [ResNet20_relu6_quantized_2_bits] after configure_qat:
[2025-05-27 07:18:34,153]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 07:20:14,469]: [ResNet20_relu6_quantized_2_bits] Epoch: 001 Train Loss: 1.2983 Train Acc: 0.5494 Eval Loss: 1.2827 Eval Acc: 0.6106 (LR: 0.00100000)
[2025-05-27 07:21:56,854]: [ResNet20_relu6_quantized_2_bits] Epoch: 002 Train Loss: 0.9167 Train Acc: 0.6774 Eval Loss: 0.9406 Eval Acc: 0.6801 (LR: 0.00100000)
[2025-05-27 07:23:40,066]: [ResNet20_relu6_quantized_2_bits] Epoch: 003 Train Loss: 0.8431 Train Acc: 0.7061 Eval Loss: 0.8676 Eval Acc: 0.7041 (LR: 0.00100000)
[2025-05-27 07:25:22,878]: [ResNet20_relu6_quantized_2_bits] Epoch: 004 Train Loss: 0.8062 Train Acc: 0.7185 Eval Loss: 1.1139 Eval Acc: 0.6505 (LR: 0.00100000)
[2025-05-27 07:27:04,757]: [ResNet20_relu6_quantized_2_bits] Epoch: 005 Train Loss: 0.7833 Train Acc: 0.7238 Eval Loss: 1.0113 Eval Acc: 0.6632 (LR: 0.00100000)
[2025-05-27 07:28:47,172]: [ResNet20_relu6_quantized_2_bits] Epoch: 006 Train Loss: 0.7699 Train Acc: 0.7292 Eval Loss: 0.9381 Eval Acc: 0.6857 (LR: 0.00100000)
[2025-05-27 07:30:33,868]: [ResNet20_relu6_quantized_2_bits] Epoch: 007 Train Loss: 0.7595 Train Acc: 0.7343 Eval Loss: 1.4538 Eval Acc: 0.5851 (LR: 0.00100000)
[2025-05-27 07:32:16,396]: [ResNet20_relu6_quantized_2_bits] Epoch: 008 Train Loss: 0.7514 Train Acc: 0.7366 Eval Loss: 0.7755 Eval Acc: 0.7354 (LR: 0.00100000)
[2025-05-27 07:33:57,698]: [ResNet20_relu6_quantized_2_bits] Epoch: 009 Train Loss: 0.7478 Train Acc: 0.7370 Eval Loss: 1.1099 Eval Acc: 0.6493 (LR: 0.00100000)
[2025-05-27 07:35:35,840]: [ResNet20_relu6_quantized_2_bits] Epoch: 010 Train Loss: 0.7407 Train Acc: 0.7417 Eval Loss: 0.8036 Eval Acc: 0.7250 (LR: 0.00100000)
[2025-05-27 07:37:12,704]: [ResNet20_relu6_quantized_2_bits] Epoch: 011 Train Loss: 0.7232 Train Acc: 0.7462 Eval Loss: 0.8179 Eval Acc: 0.7259 (LR: 0.00100000)
[2025-05-27 07:38:49,963]: [ResNet20_relu6_quantized_2_bits] Epoch: 012 Train Loss: 0.7208 Train Acc: 0.7481 Eval Loss: 0.8486 Eval Acc: 0.7151 (LR: 0.00100000)
[2025-05-27 07:40:28,525]: [ResNet20_relu6_quantized_2_bits] Epoch: 013 Train Loss: 0.7157 Train Acc: 0.7506 Eval Loss: 1.0211 Eval Acc: 0.6783 (LR: 0.00100000)
[2025-05-27 07:42:12,333]: [ResNet20_relu6_quantized_2_bits] Epoch: 014 Train Loss: 0.7054 Train Acc: 0.7545 Eval Loss: 0.7514 Eval Acc: 0.7437 (LR: 0.00100000)
[2025-05-27 07:43:52,678]: [ResNet20_relu6_quantized_2_bits] Epoch: 015 Train Loss: 0.7005 Train Acc: 0.7558 Eval Loss: 0.7875 Eval Acc: 0.7312 (LR: 0.00100000)
[2025-05-27 07:45:32,870]: [ResNet20_relu6_quantized_2_bits] Epoch: 016 Train Loss: 0.6933 Train Acc: 0.7598 Eval Loss: 0.9169 Eval Acc: 0.6890 (LR: 0.00100000)
[2025-05-27 07:47:13,692]: [ResNet20_relu6_quantized_2_bits] Epoch: 017 Train Loss: 0.6885 Train Acc: 0.7597 Eval Loss: 0.7295 Eval Acc: 0.7486 (LR: 0.00100000)
[2025-05-27 07:49:00,021]: [ResNet20_relu6_quantized_2_bits] Epoch: 018 Train Loss: 0.6831 Train Acc: 0.7618 Eval Loss: 1.0849 Eval Acc: 0.6533 (LR: 0.00100000)
[2025-05-27 07:50:43,427]: [ResNet20_relu6_quantized_2_bits] Epoch: 019 Train Loss: 0.6801 Train Acc: 0.7623 Eval Loss: 0.8835 Eval Acc: 0.7182 (LR: 0.00100000)
[2025-05-27 07:52:28,095]: [ResNet20_relu6_quantized_2_bits] Epoch: 020 Train Loss: 0.6798 Train Acc: 0.7632 Eval Loss: 1.1409 Eval Acc: 0.6496 (LR: 0.00100000)
[2025-05-27 07:54:09,557]: [ResNet20_relu6_quantized_2_bits] Epoch: 021 Train Loss: 0.6740 Train Acc: 0.7683 Eval Loss: 0.7241 Eval Acc: 0.7503 (LR: 0.00100000)
[2025-05-27 07:55:51,176]: [ResNet20_relu6_quantized_2_bits] Epoch: 022 Train Loss: 0.6732 Train Acc: 0.7665 Eval Loss: 0.7995 Eval Acc: 0.7285 (LR: 0.00100000)
[2025-05-27 07:57:35,387]: [ResNet20_relu6_quantized_2_bits] Epoch: 023 Train Loss: 0.6680 Train Acc: 0.7680 Eval Loss: 0.7317 Eval Acc: 0.7579 (LR: 0.00100000)
[2025-05-27 07:59:14,695]: [ResNet20_relu6_quantized_2_bits] Epoch: 024 Train Loss: 0.6661 Train Acc: 0.7682 Eval Loss: 0.8120 Eval Acc: 0.7265 (LR: 0.00100000)
[2025-05-27 08:00:54,068]: [ResNet20_relu6_quantized_2_bits] Epoch: 025 Train Loss: 0.6673 Train Acc: 0.7670 Eval Loss: 0.7077 Eval Acc: 0.7579 (LR: 0.00100000)
[2025-05-27 08:02:32,243]: [ResNet20_relu6_quantized_2_bits] Epoch: 026 Train Loss: 0.6593 Train Acc: 0.7700 Eval Loss: 0.9661 Eval Acc: 0.6935 (LR: 0.00100000)
[2025-05-27 08:04:09,472]: [ResNet20_relu6_quantized_2_bits] Epoch: 027 Train Loss: 0.6547 Train Acc: 0.7715 Eval Loss: 0.7629 Eval Acc: 0.7467 (LR: 0.00100000)
[2025-05-27 08:05:47,351]: [ResNet20_relu6_quantized_2_bits] Epoch: 028 Train Loss: 0.6593 Train Acc: 0.7685 Eval Loss: 0.8214 Eval Acc: 0.7276 (LR: 0.00100000)
[2025-05-27 08:07:24,945]: [ResNet20_relu6_quantized_2_bits] Epoch: 029 Train Loss: 0.6524 Train Acc: 0.7725 Eval Loss: 0.9176 Eval Acc: 0.7014 (LR: 0.00100000)
[2025-05-27 08:09:03,113]: [ResNet20_relu6_quantized_2_bits] Epoch: 030 Train Loss: 0.6568 Train Acc: 0.7713 Eval Loss: 0.7263 Eval Acc: 0.7504 (LR: 0.00100000)
[2025-05-27 08:10:41,413]: [ResNet20_relu6_quantized_2_bits] Epoch: 031 Train Loss: 0.6532 Train Acc: 0.7725 Eval Loss: 0.8117 Eval Acc: 0.7231 (LR: 0.00010000)
[2025-05-27 08:12:21,331]: [ResNet20_relu6_quantized_2_bits] Epoch: 032 Train Loss: 0.5606 Train Acc: 0.8065 Eval Loss: 0.5443 Eval Acc: 0.8109 (LR: 0.00010000)
[2025-05-27 08:14:04,071]: [ResNet20_relu6_quantized_2_bits] Epoch: 033 Train Loss: 0.5408 Train Acc: 0.8115 Eval Loss: 0.5622 Eval Acc: 0.8052 (LR: 0.00010000)
[2025-05-27 08:15:49,758]: [ResNet20_relu6_quantized_2_bits] Epoch: 034 Train Loss: 0.5386 Train Acc: 0.8129 Eval Loss: 0.5462 Eval Acc: 0.8118 (LR: 0.00010000)
[2025-05-27 08:17:35,127]: [ResNet20_relu6_quantized_2_bits] Epoch: 035 Train Loss: 0.5389 Train Acc: 0.8138 Eval Loss: 0.5446 Eval Acc: 0.8164 (LR: 0.00010000)
[2025-05-27 08:19:18,660]: [ResNet20_relu6_quantized_2_bits] Epoch: 036 Train Loss: 0.5327 Train Acc: 0.8155 Eval Loss: 0.6608 Eval Acc: 0.7819 (LR: 0.00010000)
[2025-05-27 08:21:01,507]: [ResNet20_relu6_quantized_2_bits] Epoch: 037 Train Loss: 0.5332 Train Acc: 0.8139 Eval Loss: 0.5533 Eval Acc: 0.8130 (LR: 0.00010000)
[2025-05-27 08:22:41,944]: [ResNet20_relu6_quantized_2_bits] Epoch: 038 Train Loss: 0.5329 Train Acc: 0.8137 Eval Loss: 0.5780 Eval Acc: 0.8006 (LR: 0.00001000)
[2025-05-27 08:24:24,296]: [ResNet20_relu6_quantized_2_bits] Epoch: 039 Train Loss: 0.5054 Train Acc: 0.8235 Eval Loss: 0.5150 Eval Acc: 0.8242 (LR: 0.00001000)
[2025-05-27 08:26:04,882]: [ResNet20_relu6_quantized_2_bits] Epoch: 040 Train Loss: 0.5011 Train Acc: 0.8244 Eval Loss: 0.5023 Eval Acc: 0.8286 (LR: 0.00001000)
[2025-05-27 08:27:45,208]: [ResNet20_relu6_quantized_2_bits] Epoch: 041 Train Loss: 0.5011 Train Acc: 0.8280 Eval Loss: 0.5032 Eval Acc: 0.8311 (LR: 0.00001000)
[2025-05-27 08:29:23,122]: [ResNet20_relu6_quantized_2_bits] Epoch: 042 Train Loss: 0.5049 Train Acc: 0.8249 Eval Loss: 0.5013 Eval Acc: 0.8279 (LR: 0.00001000)
[2025-05-27 08:30:59,212]: [ResNet20_relu6_quantized_2_bits] Epoch: 043 Train Loss: 0.4978 Train Acc: 0.8254 Eval Loss: 0.5318 Eval Acc: 0.8217 (LR: 0.00001000)
[2025-05-27 08:32:36,504]: [ResNet20_relu6_quantized_2_bits] Epoch: 044 Train Loss: 0.5043 Train Acc: 0.8237 Eval Loss: 0.5678 Eval Acc: 0.8067 (LR: 0.00001000)
[2025-05-27 08:34:12,658]: [ResNet20_relu6_quantized_2_bits] Epoch: 045 Train Loss: 0.5012 Train Acc: 0.8253 Eval Loss: 0.7533 Eval Acc: 0.7513 (LR: 0.00001000)
[2025-05-27 08:35:49,413]: [ResNet20_relu6_quantized_2_bits] Epoch: 046 Train Loss: 0.5032 Train Acc: 0.8246 Eval Loss: 0.5271 Eval Acc: 0.8209 (LR: 0.00001000)
[2025-05-27 08:37:29,081]: [ResNet20_relu6_quantized_2_bits] Epoch: 047 Train Loss: 0.5001 Train Acc: 0.8268 Eval Loss: 0.5091 Eval Acc: 0.8258 (LR: 0.00001000)
[2025-05-27 08:39:16,056]: [ResNet20_relu6_quantized_2_bits] Epoch: 048 Train Loss: 0.5008 Train Acc: 0.8253 Eval Loss: 0.5193 Eval Acc: 0.8218 (LR: 0.00000100)
[2025-05-27 08:41:04,123]: [ResNet20_relu6_quantized_2_bits] Epoch: 049 Train Loss: 0.4871 Train Acc: 0.8290 Eval Loss: 0.5300 Eval Acc: 0.8225 (LR: 0.00000100)
[2025-05-27 08:42:46,320]: [ResNet20_relu6_quantized_2_bits] Epoch: 050 Train Loss: 0.4852 Train Acc: 0.8321 Eval Loss: 0.4990 Eval Acc: 0.8264 (LR: 0.00000100)
[2025-05-27 08:44:33,119]: [ResNet20_relu6_quantized_2_bits] Epoch: 051 Train Loss: 0.4909 Train Acc: 0.8276 Eval Loss: 0.5048 Eval Acc: 0.8276 (LR: 0.00000100)
[2025-05-27 08:46:19,911]: [ResNet20_relu6_quantized_2_bits] Epoch: 052 Train Loss: 0.4905 Train Acc: 0.8287 Eval Loss: 0.5113 Eval Acc: 0.8271 (LR: 0.00000100)
[2025-05-27 08:48:06,725]: [ResNet20_relu6_quantized_2_bits] Epoch: 053 Train Loss: 0.4894 Train Acc: 0.8294 Eval Loss: 0.5065 Eval Acc: 0.8288 (LR: 0.00000100)
[2025-05-27 08:49:51,312]: [ResNet20_relu6_quantized_2_bits] Epoch: 054 Train Loss: 0.4905 Train Acc: 0.8285 Eval Loss: 0.5279 Eval Acc: 0.8174 (LR: 0.00000100)
[2025-05-27 08:51:35,656]: [ResNet20_relu6_quantized_2_bits] Epoch: 055 Train Loss: 0.4907 Train Acc: 0.8278 Eval Loss: 0.5032 Eval Acc: 0.8277 (LR: 0.00000100)
[2025-05-27 08:52:55,515]: [ResNet20_relu6_quantized_2_bits] Epoch: 056 Train Loss: 0.4893 Train Acc: 0.8281 Eval Loss: 0.5184 Eval Acc: 0.8219 (LR: 0.00000010)
[2025-05-27 08:54:13,567]: [ResNet20_relu6_quantized_2_bits] Epoch: 057 Train Loss: 0.4843 Train Acc: 0.8310 Eval Loss: 0.4913 Eval Acc: 0.8372 (LR: 0.00000010)
[2025-05-27 08:55:30,789]: [ResNet20_relu6_quantized_2_bits] Epoch: 058 Train Loss: 0.4825 Train Acc: 0.8303 Eval Loss: 0.5031 Eval Acc: 0.8270 (LR: 0.00000010)
[2025-05-27 08:56:48,287]: [ResNet20_relu6_quantized_2_bits] Epoch: 059 Train Loss: 0.4815 Train Acc: 0.8332 Eval Loss: 0.5085 Eval Acc: 0.8268 (LR: 0.00000010)
[2025-05-27 08:58:11,466]: [ResNet20_relu6_quantized_2_bits] Epoch: 060 Train Loss: 0.4834 Train Acc: 0.8330 Eval Loss: 0.4863 Eval Acc: 0.8323 (LR: 0.00000010)
[2025-05-27 08:58:11,466]: [ResNet20_relu6_quantized_2_bits] Best Eval Accuracy: 0.8372
[2025-05-27 08:58:11,635]: 


Quantization of model down to 2 bits finished
[2025-05-27 08:58:11,635]: Model Architecture:
[2025-05-27 08:58:11,718]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3887], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5748710632324219, max_val=0.5910904407501221)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5282], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7923489809036255, max_val=0.7923530340194702)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7626], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2702751159667969, max_val=1.0174181461334229)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4642], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6963226795196533, max_val=0.6963332891464233)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6005], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9003069400787354, max_val=0.9010523557662964)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4246], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5534254312515259, max_val=0.7202440500259399)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4119], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6167782545089722, max_val=0.6188201904296875)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3957], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5897241830825806, max_val=0.5974409580230713)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7219], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8122801780700684, max_val=1.3534810543060303)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3823], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5600446462631226, max_val=0.5867397785186768)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3286], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47224104404449463, max_val=0.513596773147583)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3638], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5585470199584961, max_val=0.5329869985580444)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9998], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.999399185180664)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3246], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47758597135543823, max_val=0.4961760640144348)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3602], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5437588691711426, max_val=0.5366916656494141)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3279], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4584391713142395, max_val=0.5253584384918213)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4357], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6535861492156982, max_val=0.653590202331543)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3618], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5236324071884155, max_val=0.5618315935134888)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2493], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36929088830947876, max_val=0.37854528427124023)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2674], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33350270986557007, max_val=0.4685899615287781)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9996], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.998837947845459)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1872], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27948617935180664, max_val=0.2821430563926697)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 08:58:11,718]: 
Model Weights:
[2025-05-27 08:58:11,718]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 08:58:11,719]: Sample Values (25 elements): [-0.03922341763973236, 0.23861081898212433, -0.2793382704257965, -0.19562776386737823, -0.008460183627903461, 0.2095905840396881, 0.12150199711322784, -0.06555633246898651, 0.07579665631055832, -0.15642602741718292, -0.003754328703507781, -0.031466446816921234, 0.12866273522377014, -0.07238203287124634, 0.013494632206857204, -0.09192245453596115, 0.10379139333963394, -0.28948378562927246, -0.3118550181388855, 0.06779375672340393, -0.3061982989311218, -0.2533697187900543, 0.17994484305381775, 0.10666771978139877, -0.10091602802276611]
[2025-05-27 08:58:11,720]: Mean: -0.00244291
[2025-05-27 08:58:11,720]: Min: -0.59242994
[2025-05-27 08:58:11,720]: Max: 0.44919294
[2025-05-27 08:58:11,720]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,721]: Sample Values (16 elements): [2.1745858192443848, 2.729588270187378, 1.87090265750885, 2.4106552600860596, 3.052382230758667, 3.2152576446533203, 2.2930257320404053, 2.095259428024292, 4.064905166625977, 1.7856262922286987, 3.4500772953033447, 1.6944003105163574, 1.8987033367156982, 2.5249221324920654, 2.4950101375579834, 1.9142675399780273]
[2025-05-27 08:58:11,721]: Mean: 2.47934818
[2025-05-27 08:58:11,721]: Min: 1.69440031
[2025-05-27 08:58:11,721]: Max: 4.06490517
[2025-05-27 08:58:11,722]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:58:11,722]: Sample Values (25 elements): [0.38865384459495544, -0.38865384459495544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38865384459495544, 0.0, 0.0, 0.0, 0.0, 0.38865384459495544, 0.0, 0.0, 0.0, -0.38865384459495544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.38865384459495544]
[2025-05-27 08:58:11,723]: Mean: -0.01079594
[2025-05-27 08:58:11,723]: Min: -0.38865384
[2025-05-27 08:58:11,723]: Max: 0.77730769
[2025-05-27 08:58:11,723]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,723]: Sample Values (16 elements): [1.2769343852996826, 1.2803298234939575, 1.4080076217651367, 2.10064435005188, 1.2405073642730713, 0.9877328872680664, 1.1830106973648071, 1.269955039024353, 1.906941533088684, 1.115286111831665, 1.1962230205535889, 1.4130812883377075, 1.225774884223938, 1.2038239240646362, 1.11591374874115, 1.7051819562911987]
[2025-05-27 08:58:11,724]: Mean: 1.35183430
[2025-05-27 08:58:11,724]: Min: 0.98773289
[2025-05-27 08:58:11,724]: Max: 2.10064435
[2025-05-27 08:58:11,725]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:58:11,725]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.5282340049743652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5282340049743652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,725]: Mean: 0.00412683
[2025-05-27 08:58:11,726]: Min: -0.52823400
[2025-05-27 08:58:11,726]: Max: 0.52823400
[2025-05-27 08:58:11,726]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,726]: Sample Values (16 elements): [0.9435699582099915, 0.9919499158859253, 0.8870508670806885, 0.7071130871772766, 1.05023992061615, 0.6604815721511841, 1.28513765335083, 0.6560422778129578, 0.8057169318199158, 1.0647872686386108, 1.2372889518737793, 0.8830912113189697, 0.7905864119529724, 0.8653799295425415, 1.052729606628418, 1.2396737337112427]
[2025-05-27 08:58:11,726]: Mean: 0.94505244
[2025-05-27 08:58:11,726]: Min: 0.65604228
[2025-05-27 08:58:11,727]: Max: 1.28513765
[2025-05-27 08:58:11,728]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:58:11,728]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.7625644207000732, 0.0, 0.0, -0.7625644207000732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7625644207000732, 0.0, 0.0]
[2025-05-27 08:58:11,738]: Mean: -0.00364072
[2025-05-27 08:58:11,738]: Min: -1.52512884
[2025-05-27 08:58:11,739]: Max: 0.76256442
[2025-05-27 08:58:11,739]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,739]: Sample Values (16 elements): [0.9622781872749329, 1.046120285987854, 0.8815380930900574, 0.9212877154350281, 1.55879545211792, 1.4484575986862183, 1.1753520965576172, 1.440974473953247, 1.1770824193954468, 2.009997844696045, 1.4749411344528198, 1.5778807401657104, 0.9454271793365479, 0.9128157496452332, 1.22517991065979, 1.5090770721435547]
[2025-05-27 08:58:11,739]: Mean: 1.26670027
[2025-05-27 08:58:11,739]: Min: 0.88153809
[2025-05-27 08:58:11,739]: Max: 2.00999784
[2025-05-27 08:58:11,741]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:58:11,741]: Sample Values (25 elements): [-0.4642186760902405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4642186760902405, -0.4642186760902405, 0.4642186760902405, -0.4642186760902405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4642186760902405, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,741]: Mean: 0.00705193
[2025-05-27 08:58:11,741]: Min: -0.46421868
[2025-05-27 08:58:11,741]: Max: 0.92843735
[2025-05-27 08:58:11,741]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,742]: Sample Values (16 elements): [0.716515302658081, 0.9864354133605957, 0.8967238068580627, 0.7177901864051819, 1.117425560951233, 0.9080031514167786, 1.137816309928894, 0.8319745659828186, 0.9995531439781189, 0.478376567363739, 0.46593713760375977, 0.9161744713783264, 1.2355238199234009, 0.7266427874565125, 0.8385192155838013, 0.7971752882003784]
[2025-05-27 08:58:11,742]: Mean: 0.86066175
[2025-05-27 08:58:11,742]: Min: 0.46593714
[2025-05-27 08:58:11,742]: Max: 1.23552382
[2025-05-27 08:58:11,743]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:58:11,743]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6004531383514404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6004531383514404, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,744]: Mean: -0.00390920
[2025-05-27 08:58:11,744]: Min: -0.60045314
[2025-05-27 08:58:11,744]: Max: 1.20090628
[2025-05-27 08:58:11,744]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,744]: Sample Values (16 elements): [1.5595924854278564, 1.0689914226531982, 1.2744485139846802, 1.324197769165039, 1.0241926908493042, 1.0640300512313843, 1.6941124200820923, 1.4868606328964233, 2.0624442100524902, 1.157584547996521, 1.4957611560821533, 1.3589390516281128, 0.854956328868866, 1.8208988904953003, 1.522456169128418, 1.8874431848526]
[2025-05-27 08:58:11,744]: Mean: 1.41605687
[2025-05-27 08:58:11,744]: Min: 0.85495633
[2025-05-27 08:58:11,745]: Max: 2.06244421
[2025-05-27 08:58:11,746]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:58:11,746]: Sample Values (25 elements): [-0.4245564937591553, -0.4245564937591553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4245564937591553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4245564937591553, 0.0, 0.0, -0.4245564937591553, 0.4245564937591553, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,746]: Mean: 0.02045389
[2025-05-27 08:58:11,746]: Min: -0.42455649
[2025-05-27 08:58:11,746]: Max: 0.84911299
[2025-05-27 08:58:11,746]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 08:58:11,747]: Sample Values (16 elements): [0.7457300424575806, 0.7358997464179993, 1.17079758644104, 0.975016713142395, 1.0826131105422974, 1.6612544059753418, 1.1819700002670288, 0.9309230446815491, 1.3752919435501099, 0.7231912016868591, 0.6883561015129089, 1.050421953201294, 0.4805784225463867, 0.7316223978996277, 1.3198561668395996, 1.2943023443222046]
[2025-05-27 08:58:11,747]: Mean: 1.00923908
[2025-05-27 08:58:11,747]: Min: 0.48057842
[2025-05-27 08:58:11,747]: Max: 1.66125441
[2025-05-27 08:58:11,748]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 08:58:11,748]: Sample Values (25 elements): [0.0, -0.411866158246994, 0.411866158246994, 0.411866158246994, 0.0, 0.0, 0.0, 0.0, 0.0, -0.411866158246994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.411866158246994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,748]: Mean: -0.00071505
[2025-05-27 08:58:11,749]: Min: -0.41186616
[2025-05-27 08:58:11,749]: Max: 0.82373232
[2025-05-27 08:58:11,749]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,749]: Sample Values (25 elements): [1.307237148284912, 1.3068232536315918, 1.6919009685516357, 1.6110179424285889, 1.2619942426681519, 1.5434218645095825, 1.548282504081726, 1.75757896900177, 1.593100666999817, 1.5494105815887451, 1.041666865348816, 1.50271475315094, 1.6176059246063232, 1.5873992443084717, 1.4519175291061401, 1.4206846952438354, 1.5188004970550537, 1.7200888395309448, 1.5499413013458252, 1.5706654787063599, 1.2230212688446045, 1.4694188833236694, 1.5393245220184326, 1.156006097793579, 1.473976969718933]
[2025-05-27 08:58:11,749]: Mean: 1.48835015
[2025-05-27 08:58:11,749]: Min: 1.04166687
[2025-05-27 08:58:11,750]: Max: 1.75757897
[2025-05-27 08:58:11,751]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:58:11,751]: Sample Values (25 elements): [0.3957217335700989, 0.0, 0.0, 0.0, 0.0, 0.3957217335700989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3957217335700989, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,751]: Mean: -0.00103053
[2025-05-27 08:58:11,752]: Min: -0.39572173
[2025-05-27 08:58:11,752]: Max: 0.79144347
[2025-05-27 08:58:11,752]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,752]: Sample Values (25 elements): [1.374841570854187, 1.2002077102661133, 1.3969296216964722, 0.979552686214447, 1.3776798248291016, 1.6367268562316895, 1.3365665674209595, 1.3338934183120728, 1.2642327547073364, 1.4516290426254272, 1.3153992891311646, 1.2877869606018066, 1.3411171436309814, 1.7484086751937866, 1.3415907621383667, 1.4401004314422607, 1.39260733127594, 1.229814052581787, 1.2842711210250854, 1.2563426494598389, 1.2916465997695923, 1.5344431400299072, 1.4156397581100464, 1.2817126512527466, 1.376983642578125]
[2025-05-27 08:58:11,752]: Mean: 1.35360503
[2025-05-27 08:58:11,752]: Min: 0.97955269
[2025-05-27 08:58:11,753]: Max: 1.74840868
[2025-05-27 08:58:11,754]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 08:58:11,754]: Sample Values (25 elements): [0.7219204306602478, 0.0, 0.0, -0.7219204306602478, 0.0, 0.0, -0.7219204306602478, 0.7219204306602478, -0.7219204306602478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7219204306602478, 0.0, 0.0, -0.7219204306602478, 0.0, 0.0, 0.0, -0.7219204306602478, 0.0, 0.0]
[2025-05-27 08:58:11,754]: Mean: 0.00282000
[2025-05-27 08:58:11,754]: Min: -0.72192043
[2025-05-27 08:58:11,754]: Max: 1.44384086
[2025-05-27 08:58:11,754]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,755]: Sample Values (25 elements): [0.8265451192855835, 0.7603094577789307, 0.6264766454696655, 0.3799450695514679, 0.731496274471283, 0.7212827205657959, 0.6108189225196838, 0.6159579157829285, 0.5622348785400391, 0.8012654781341553, 0.7361845374107361, 0.671440064907074, 0.8770830631256104, 0.5900548696517944, 0.8930159211158752, 0.7696657776832581, 1.0047849416732788, 0.48556938767433167, 0.6395381093025208, 0.927615225315094, 0.4899277091026306, 0.8930545449256897, 0.9637704491615295, 0.7703836560249329, 0.5680248141288757]
[2025-05-27 08:58:11,755]: Mean: 0.72588789
[2025-05-27 08:58:11,755]: Min: 0.37994507
[2025-05-27 08:58:11,755]: Max: 1.15886986
[2025-05-27 08:58:11,756]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:58:11,756]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3822614848613739, 0.3822614848613739, 0.0, 0.3822614848613739, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,756]: Mean: -0.00435519
[2025-05-27 08:58:11,757]: Min: -0.38226148
[2025-05-27 08:58:11,757]: Max: 0.76452297
[2025-05-27 08:58:11,757]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,757]: Sample Values (25 elements): [1.3860677480697632, 1.4872630834579468, 0.9703978300094604, 1.2115386724472046, 1.2955150604248047, 1.1680073738098145, 1.0570099353790283, 1.2305288314819336, 0.8850585222244263, 1.2722595930099487, 1.2661110162734985, 1.1909146308898926, 1.1641509532928467, 1.2155702114105225, 0.9747148752212524, 1.4316972494125366, 1.3880319595336914, 1.1745158433914185, 1.122864007949829, 1.3782380819320679, 1.2717574834823608, 1.10397469997406, 0.9933261871337891, 0.9649308919906616, 1.250792145729065]
[2025-05-27 08:58:11,757]: Mean: 1.19296813
[2025-05-27 08:58:11,758]: Min: 0.88505852
[2025-05-27 08:58:11,758]: Max: 1.48726308
[2025-05-27 08:58:11,759]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:58:11,759]: Sample Values (25 elements): [0.32861262559890747, 0.32861262559890747, 0.0, 0.0, -0.32861262559890747, 0.0, 0.0, 0.0, 0.0, 0.32861262559890747, 0.0, 0.0, 0.0, -0.32861262559890747, 0.32861262559890747, 0.0, 0.32861262559890747, 0.32861262559890747, -0.32861262559890747, 0.0, -0.32861262559890747, 0.0, 0.0, 0.0, -0.32861262559890747]
[2025-05-27 08:58:11,759]: Mean: -0.00039222
[2025-05-27 08:58:11,760]: Min: -0.32861263
[2025-05-27 08:58:11,760]: Max: 0.65722525
[2025-05-27 08:58:11,760]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,760]: Sample Values (25 elements): [0.9834182858467102, 1.0604668855667114, 1.129198670387268, 1.5027192831039429, 1.0109374523162842, 0.8850529193878174, 0.8941329121589661, 0.8427497148513794, 0.8891456127166748, 1.144771933555603, 1.1873849630355835, 0.8628786206245422, 0.8939209580421448, 0.9348196983337402, 0.9636266827583313, 0.9655784368515015, 0.9264119267463684, 0.79146808385849, 0.9490683674812317, 0.7788546085357666, 0.934632420539856, 1.0384266376495361, 1.0002272129058838, 1.1027946472167969, 0.9108016490936279]
[2025-05-27 08:58:11,760]: Mean: 0.97858375
[2025-05-27 08:58:11,760]: Min: 0.77885461
[2025-05-27 08:58:11,761]: Max: 1.50271928
[2025-05-27 08:58:11,762]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:58:11,762]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36384469270706177, -0.36384469270706177, 0.0, -0.36384469270706177, 0.0, 0.0, 0.0, -0.36384469270706177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,762]: Mean: -0.00568507
[2025-05-27 08:58:11,762]: Min: -0.72768939
[2025-05-27 08:58:11,762]: Max: 0.36384469
[2025-05-27 08:58:11,762]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,763]: Sample Values (25 elements): [0.8971683382987976, 1.1603808403015137, 1.0199331045150757, 1.3692493438720703, 0.9739962816238403, 1.0920294523239136, 0.9173827767372131, 0.8643880486488342, 1.2252572774887085, 1.130385398864746, 0.9130852222442627, 0.9269226789474487, 0.8731440305709839, 1.0068291425704956, 1.1864454746246338, 1.314099907875061, 1.3213518857955933, 0.999740481376648, 0.9797437191009521, 1.0462599992752075, 0.887253999710083, 0.9674738049507141, 1.2871187925338745, 1.0658115148544312, 1.1468173265457153]
[2025-05-27 08:58:11,763]: Mean: 1.04843855
[2025-05-27 08:58:11,763]: Min: 0.64167464
[2025-05-27 08:58:11,763]: Max: 1.36924934
[2025-05-27 08:58:11,764]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:58:11,764]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.324587345123291, 0.0, 0.324587345123291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.324587345123291, 0.0, 0.0, -0.324587345123291, -0.324587345123291]
[2025-05-27 08:58:11,765]: Mean: -0.00510690
[2025-05-27 08:58:11,765]: Min: -0.32458735
[2025-05-27 08:58:11,765]: Max: 0.64917469
[2025-05-27 08:58:11,765]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 08:58:11,765]: Sample Values (25 elements): [0.9265405535697937, 1.0784084796905518, 0.9724008440971375, 0.9167200922966003, 0.846916139125824, 1.1262292861938477, 1.0314958095550537, 1.1838922500610352, 0.9376282691955566, 0.8179298043251038, 0.7623742818832397, 0.9531813859939575, 0.7871779203414917, 0.8511999845504761, 0.8432074189186096, 0.903846800327301, 0.9614073634147644, 0.8418102860450745, 0.8201714158058167, 0.9099134802818298, 0.9343801140785217, 0.9886727929115295, 0.8460177183151245, 1.4248522520065308, 1.1123253107070923]
[2025-05-27 08:58:11,765]: Mean: 0.94463801
[2025-05-27 08:58:11,766]: Min: 0.75099611
[2025-05-27 08:58:11,766]: Max: 1.42485225
[2025-05-27 08:58:11,767]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 08:58:11,767]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.36015018820762634, 0.0, 0.0, 0.0, 0.0, -0.36015018820762634, 0.0, 0.0, 0.36015018820762634, 0.0, 0.0, 0.0, 0.0, -0.36015018820762634, 0.0]
[2025-05-27 08:58:11,767]: Mean: 0.00103559
[2025-05-27 08:58:11,767]: Min: -0.72030038
[2025-05-27 08:58:11,767]: Max: 0.36015019
[2025-05-27 08:58:11,768]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,768]: Sample Values (25 elements): [1.2873873710632324, 1.2241125106811523, 1.295237421989441, 1.2660714387893677, 1.1569957733154297, 1.2060902118682861, 1.4470634460449219, 1.372211217880249, 1.5037448406219482, 1.705869436264038, 1.351702332496643, 1.2361797094345093, 1.2894974946975708, 1.115992546081543, 1.3730628490447998, 1.3816626071929932, 1.4522782564163208, 1.4212483167648315, 1.1381944417953491, 1.3496880531311035, 1.2837538719177246, 1.1212668418884277, 1.2702254056930542, 1.2671841382980347, 1.2589730024337769]
[2025-05-27 08:58:11,768]: Mean: 1.30759859
[2025-05-27 08:58:11,768]: Min: 0.94775790
[2025-05-27 08:58:11,768]: Max: 1.81810558
[2025-05-27 08:58:11,769]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:58:11,770]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.32793253660202026, 0.32793253660202026, 0.0, 0.0, 0.32793253660202026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32793253660202026, 0.0, 0.0]
[2025-05-27 08:58:11,770]: Mean: -0.00077393
[2025-05-27 08:58:11,770]: Min: -0.32793254
[2025-05-27 08:58:11,770]: Max: 0.65586507
[2025-05-27 08:58:11,770]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,771]: Sample Values (25 elements): [1.5549991130828857, 1.1262866258621216, 1.1310909986495972, 1.429137945175171, 1.27545964717865, 1.0160287618637085, 0.9994063377380371, 1.1792048215866089, 1.164947748184204, 1.252185583114624, 1.2860326766967773, 0.9232550263404846, 0.9842308759689331, 1.3098735809326172, 1.5335302352905273, 1.4444431066513062, 1.208006739616394, 1.0492048263549805, 1.28873610496521, 1.1571369171142578, 1.2758489847183228, 1.4119356870651245, 1.12297785282135, 1.1838258504867554, 1.1287556886672974]
[2025-05-27 08:58:11,771]: Mean: 1.21368337
[2025-05-27 08:58:11,771]: Min: 0.92325503
[2025-05-27 08:58:11,771]: Max: 1.73113751
[2025-05-27 08:58:11,772]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 08:58:11,772]: Sample Values (25 elements): [0.0, -0.43572545051574707, 0.0, -0.43572545051574707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43572545051574707, 0.0, -0.43572545051574707, 0.0, 0.0, 0.0, 0.0, -0.43572545051574707, 0.0, 0.0, 0.0, -0.43572545051574707, 0.0, 0.0, -0.43572545051574707, 0.0]
[2025-05-27 08:58:11,773]: Mean: -0.00276584
[2025-05-27 08:58:11,773]: Min: -0.43572545
[2025-05-27 08:58:11,773]: Max: 0.43572545
[2025-05-27 08:58:11,773]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,773]: Sample Values (25 elements): [0.5946289896965027, 0.7154644727706909, 0.7243188619613647, 0.7756649851799011, 0.5221740007400513, 0.6637779474258423, 0.5911417007446289, 0.5821989178657532, 0.749107301235199, 0.6257046461105347, 0.8369954824447632, 0.4559113383293152, 0.6348463296890259, 0.7070687413215637, 0.681271493434906, 0.39513713121414185, 0.5799421072006226, 0.7269063591957092, 0.4862072765827179, 0.5184199810028076, 0.7881242036819458, 0.6025888919830322, 0.6501777768135071, 0.6571642160415649, 0.6348165273666382]
[2025-05-27 08:58:11,773]: Mean: 0.63081658
[2025-05-27 08:58:11,773]: Min: 0.26611933
[2025-05-27 08:58:11,774]: Max: 0.88039821
[2025-05-27 08:58:11,775]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:58:11,775]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36182135343551636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.36182135343551636, 0.36182135343551636, -0.36182135343551636]
[2025-05-27 08:58:11,775]: Mean: 0.00002945
[2025-05-27 08:58:11,775]: Min: -0.36182135
[2025-05-27 08:58:11,776]: Max: 0.72364271
[2025-05-27 08:58:11,776]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,776]: Sample Values (25 elements): [0.9522709846496582, 1.021161437034607, 0.9405297040939331, 1.1828123331069946, 0.8846753835678101, 1.2272562980651855, 1.0731678009033203, 1.1440280675888062, 1.0224016904830933, 1.0067051649093628, 0.7417253851890564, 1.0075626373291016, 1.1140683889389038, 0.945081353187561, 1.039041519165039, 1.0987352132797241, 1.2659268379211426, 1.1585553884506226, 1.204504132270813, 1.2219700813293457, 1.2198295593261719, 0.9256610870361328, 0.9849218726158142, 1.1598469018936157, 1.0412031412124634]
[2025-05-27 08:58:11,776]: Mean: 1.04584718
[2025-05-27 08:58:11,776]: Min: 0.72665268
[2025-05-27 08:58:11,776]: Max: 1.61620259
[2025-05-27 08:58:11,777]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:58:11,778]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.249278724193573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.249278724193573, 0.249278724193573, 0.0, 0.0]
[2025-05-27 08:58:11,778]: Mean: 0.00008791
[2025-05-27 08:58:11,778]: Min: -0.24927872
[2025-05-27 08:58:11,778]: Max: 0.49855745
[2025-05-27 08:58:11,778]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,779]: Sample Values (25 elements): [1.017309308052063, 1.1193772554397583, 1.1859403848648071, 1.3162659406661987, 0.9003865122795105, 0.9699993133544922, 1.0383756160736084, 0.9078783392906189, 1.0596603155136108, 1.0932866334915161, 0.955949604511261, 0.944775402545929, 1.0517990589141846, 0.8571739792823792, 1.1483505964279175, 0.9441244602203369, 1.1381409168243408, 1.0236696004867554, 1.177420735359192, 1.0725533962249756, 0.9240590333938599, 0.9054877161979675, 0.7140167355537415, 0.918642520904541, 1.0957080125808716]
[2025-05-27 08:58:11,779]: Mean: 1.02156115
[2025-05-27 08:58:11,779]: Min: 0.71401674
[2025-05-27 08:58:11,779]: Max: 1.31626594
[2025-05-27 08:58:11,780]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:58:11,781]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2673642337322235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,781]: Mean: 0.00055846
[2025-05-27 08:58:11,781]: Min: -0.26736423
[2025-05-27 08:58:11,781]: Max: 0.53472847
[2025-05-27 08:58:11,781]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,782]: Sample Values (25 elements): [0.8654673099517822, 0.5087531208992004, 1.1064472198486328, 0.5803016424179077, 0.6529913544654846, 0.3993128538131714, 0.6284220814704895, 0.8851596713066101, 0.8937626481056213, 5.557970099051522e-41, 0.45388323068618774, 0.7286186218261719, 0.7632984519004822, 1.1183407306671143, 0.7362374067306519, 0.7094314098358154, -4.916315532237188e-41, 0.7351509928703308, 0.6025619506835938, 0.8363762497901917, 0.9033519625663757, 0.712547242641449, -5.438439340044615e-41, 0.9115098118782043, 0.9017403721809387]
[2025-05-27 08:58:11,782]: Mean: 0.58963358
[2025-05-27 08:58:11,782]: Min: -0.00000000
[2025-05-27 08:58:11,782]: Max: 1.20114827
[2025-05-27 08:58:11,783]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:58:11,783]: Sample Values (25 elements): [0.18720975518226624, 0.18720975518226624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:58:11,784]: Mean: -0.00155399
[2025-05-27 08:58:11,784]: Min: -0.18720976
[2025-05-27 08:58:11,784]: Max: 0.37441951
[2025-05-27 08:58:11,784]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 08:58:11,784]: Sample Values (25 elements): [0.8405573964118958, 1.0146656036376953, 1.0076898336410522, 0.9841554760932922, 0.9424227476119995, 0.7588121294975281, 0.7803314328193665, 0.8688359260559082, 0.9559000134468079, 0.9547352194786072, 0.8041735887527466, 0.9529755711555481, 0.6937237977981567, 0.832452654838562, 0.8361857533454895, 0.8477450609207153, 0.9392249584197998, 0.7053775191307068, 0.876088559627533, 0.8589561581611633, 0.8129334449768066, 0.7932698726654053, 0.9351977109909058, 0.7635337114334106, 0.6914046406745911]
[2025-05-27 08:58:11,784]: Mean: 0.87234312
[2025-05-27 08:58:11,785]: Min: 0.59386325
[2025-05-27 08:58:11,785]: Max: 1.12032247
[2025-05-27 08:58:11,785]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 08:58:11,785]: Sample Values (25 elements): [-0.12903401255607605, -0.005757119972258806, 0.20209020376205444, 0.3178286552429199, -0.405242919921875, 0.3085768222808838, -0.012719182297587395, -0.09097619354724884, -0.3075582981109619, 0.18599793314933777, 0.027361789718270302, -0.1258382797241211, 0.12891113758087158, 0.10192971676588058, -0.47241008281707764, 0.1239074096083641, -0.1241443008184433, 0.04239417985081673, -0.006697445642203093, -0.6155142784118652, -0.3188415765762329, -0.33477503061294556, -0.39808815717697144, 0.06756249070167542, 0.2496713399887085]
[2025-05-27 08:58:11,785]: Mean: -0.05510269
[2025-05-27 08:58:11,785]: Min: -0.72935957
[2025-05-27 08:58:11,786]: Max: 0.45444959
