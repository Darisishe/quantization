[2025-05-04 05:28:33,779]: 
Training ResNet20 with relu6
[2025-05-04 05:29:47,275]: [ResNet20_relu6] Epoch: 001 Train Loss: 1.8454 Train Acc: 0.3078 Eval Loss: 1.5940 Eval Acc: 0.4057 (LR: 0.001000)
[2025-05-04 05:30:57,329]: [ResNet20_relu6] Epoch: 002 Train Loss: 1.5333 Train Acc: 0.4279 Eval Loss: 1.4126 Eval Acc: 0.4794 (LR: 0.001000)
[2025-05-04 05:32:07,817]: [ResNet20_relu6] Epoch: 003 Train Loss: 1.3783 Train Acc: 0.4949 Eval Loss: 1.3087 Eval Acc: 0.5251 (LR: 0.001000)
[2025-05-04 05:33:16,871]: [ResNet20_relu6] Epoch: 004 Train Loss: 1.2604 Train Acc: 0.5436 Eval Loss: 1.2309 Eval Acc: 0.5551 (LR: 0.001000)
[2025-05-04 05:34:25,629]: [ResNet20_relu6] Epoch: 005 Train Loss: 1.1685 Train Acc: 0.5775 Eval Loss: 1.2026 Eval Acc: 0.5742 (LR: 0.001000)
[2025-05-04 05:35:35,141]: [ResNet20_relu6] Epoch: 006 Train Loss: 1.0894 Train Acc: 0.6091 Eval Loss: 1.1082 Eval Acc: 0.6135 (LR: 0.001000)
[2025-05-04 05:36:43,431]: [ResNet20_relu6] Epoch: 007 Train Loss: 1.0302 Train Acc: 0.6302 Eval Loss: 1.0292 Eval Acc: 0.6358 (LR: 0.001000)
[2025-05-04 05:37:52,283]: [ResNet20_relu6] Epoch: 008 Train Loss: 0.9868 Train Acc: 0.6465 Eval Loss: 1.0588 Eval Acc: 0.6336 (LR: 0.001000)
[2025-05-04 05:39:00,138]: [ResNet20_relu6] Epoch: 009 Train Loss: 0.9395 Train Acc: 0.6661 Eval Loss: 0.9268 Eval Acc: 0.6754 (LR: 0.001000)
[2025-05-04 05:40:10,050]: [ResNet20_relu6] Epoch: 010 Train Loss: 0.9004 Train Acc: 0.6795 Eval Loss: 0.9333 Eval Acc: 0.6714 (LR: 0.001000)
[2025-05-04 05:41:20,014]: [ResNet20_relu6] Epoch: 011 Train Loss: 0.8643 Train Acc: 0.6926 Eval Loss: 0.9864 Eval Acc: 0.6632 (LR: 0.001000)
[2025-05-04 05:42:29,975]: [ResNet20_relu6] Epoch: 012 Train Loss: 0.8285 Train Acc: 0.7066 Eval Loss: 0.8614 Eval Acc: 0.7003 (LR: 0.001000)
[2025-05-04 05:43:40,206]: [ResNet20_relu6] Epoch: 013 Train Loss: 0.8022 Train Acc: 0.7166 Eval Loss: 0.8542 Eval Acc: 0.7063 (LR: 0.001000)
[2025-05-04 05:44:50,421]: [ResNet20_relu6] Epoch: 014 Train Loss: 0.7739 Train Acc: 0.7264 Eval Loss: 0.8103 Eval Acc: 0.7247 (LR: 0.001000)
[2025-05-04 05:45:58,978]: [ResNet20_relu6] Epoch: 015 Train Loss: 0.7525 Train Acc: 0.7363 Eval Loss: 0.7820 Eval Acc: 0.7280 (LR: 0.001000)
[2025-05-04 05:47:08,375]: [ResNet20_relu6] Epoch: 016 Train Loss: 0.7291 Train Acc: 0.7426 Eval Loss: 0.7786 Eval Acc: 0.7298 (LR: 0.001000)
[2025-05-04 05:48:16,882]: [ResNet20_relu6] Epoch: 017 Train Loss: 0.7074 Train Acc: 0.7502 Eval Loss: 0.7903 Eval Acc: 0.7341 (LR: 0.001000)
[2025-05-04 05:49:25,730]: [ResNet20_relu6] Epoch: 018 Train Loss: 0.6892 Train Acc: 0.7594 Eval Loss: 0.7144 Eval Acc: 0.7533 (LR: 0.001000)
[2025-05-04 05:50:34,250]: [ResNet20_relu6] Epoch: 019 Train Loss: 0.6701 Train Acc: 0.7661 Eval Loss: 0.6925 Eval Acc: 0.7617 (LR: 0.001000)
[2025-05-04 05:51:44,478]: [ResNet20_relu6] Epoch: 020 Train Loss: 0.6541 Train Acc: 0.7710 Eval Loss: 0.6832 Eval Acc: 0.7651 (LR: 0.001000)
[2025-05-04 05:52:56,725]: [ResNet20_relu6] Epoch: 021 Train Loss: 0.6402 Train Acc: 0.7761 Eval Loss: 0.7488 Eval Acc: 0.7509 (LR: 0.001000)
[2025-05-04 05:54:07,010]: [ResNet20_relu6] Epoch: 022 Train Loss: 0.6217 Train Acc: 0.7835 Eval Loss: 0.6941 Eval Acc: 0.7702 (LR: 0.001000)
[2025-05-04 05:55:18,775]: [ResNet20_relu6] Epoch: 023 Train Loss: 0.6116 Train Acc: 0.7874 Eval Loss: 0.6462 Eval Acc: 0.7852 (LR: 0.001000)
[2025-05-04 05:56:28,212]: [ResNet20_relu6] Epoch: 024 Train Loss: 0.5950 Train Acc: 0.7923 Eval Loss: 0.6125 Eval Acc: 0.7855 (LR: 0.001000)
[2025-05-04 05:57:37,160]: [ResNet20_relu6] Epoch: 025 Train Loss: 0.5858 Train Acc: 0.7960 Eval Loss: 0.6847 Eval Acc: 0.7659 (LR: 0.001000)
[2025-05-04 05:58:50,015]: [ResNet20_relu6] Epoch: 026 Train Loss: 0.5817 Train Acc: 0.7983 Eval Loss: 0.6367 Eval Acc: 0.7905 (LR: 0.001000)
[2025-05-04 06:00:00,323]: [ResNet20_relu6] Epoch: 027 Train Loss: 0.5623 Train Acc: 0.8044 Eval Loss: 0.5892 Eval Acc: 0.8010 (LR: 0.001000)
[2025-05-04 06:01:08,993]: [ResNet20_relu6] Epoch: 028 Train Loss: 0.5579 Train Acc: 0.8053 Eval Loss: 0.6108 Eval Acc: 0.7943 (LR: 0.001000)
[2025-05-04 06:02:17,356]: [ResNet20_relu6] Epoch: 029 Train Loss: 0.5473 Train Acc: 0.8093 Eval Loss: 0.5663 Eval Acc: 0.8069 (LR: 0.001000)
[2025-05-04 06:03:23,815]: [ResNet20_relu6] Epoch: 030 Train Loss: 0.5368 Train Acc: 0.8132 Eval Loss: 0.5941 Eval Acc: 0.8055 (LR: 0.001000)
[2025-05-04 06:04:28,330]: [ResNet20_relu6] Epoch: 031 Train Loss: 0.5306 Train Acc: 0.8146 Eval Loss: 0.5825 Eval Acc: 0.8059 (LR: 0.001000)
[2025-05-04 06:05:31,231]: [ResNet20_relu6] Epoch: 032 Train Loss: 0.5253 Train Acc: 0.8151 Eval Loss: 0.5815 Eval Acc: 0.8045 (LR: 0.001000)
[2025-05-04 06:06:31,284]: [ResNet20_relu6] Epoch: 033 Train Loss: 0.5134 Train Acc: 0.8204 Eval Loss: 0.5546 Eval Acc: 0.8100 (LR: 0.001000)
[2025-05-04 06:07:37,786]: [ResNet20_relu6] Epoch: 034 Train Loss: 0.5068 Train Acc: 0.8227 Eval Loss: 0.6015 Eval Acc: 0.7960 (LR: 0.001000)
[2025-05-04 06:08:39,506]: [ResNet20_relu6] Epoch: 035 Train Loss: 0.4965 Train Acc: 0.8278 Eval Loss: 0.5420 Eval Acc: 0.8157 (LR: 0.001000)
[2025-05-04 06:09:47,663]: [ResNet20_relu6] Epoch: 036 Train Loss: 0.4930 Train Acc: 0.8277 Eval Loss: 0.6007 Eval Acc: 0.8001 (LR: 0.001000)
[2025-05-04 06:10:47,356]: [ResNet20_relu6] Epoch: 037 Train Loss: 0.4865 Train Acc: 0.8321 Eval Loss: 0.5223 Eval Acc: 0.8218 (LR: 0.001000)
[2025-05-04 06:11:47,208]: [ResNet20_relu6] Epoch: 038 Train Loss: 0.4811 Train Acc: 0.8335 Eval Loss: 0.5301 Eval Acc: 0.8230 (LR: 0.001000)
[2025-05-04 06:12:47,705]: [ResNet20_relu6] Epoch: 039 Train Loss: 0.4737 Train Acc: 0.8365 Eval Loss: 0.5237 Eval Acc: 0.8211 (LR: 0.001000)
[2025-05-04 06:13:47,390]: [ResNet20_relu6] Epoch: 040 Train Loss: 0.4653 Train Acc: 0.8385 Eval Loss: 0.5596 Eval Acc: 0.8191 (LR: 0.001000)
[2025-05-04 06:14:47,226]: [ResNet20_relu6] Epoch: 041 Train Loss: 0.4611 Train Acc: 0.8377 Eval Loss: 0.5235 Eval Acc: 0.8223 (LR: 0.001000)
[2025-05-04 06:15:46,971]: [ResNet20_relu6] Epoch: 042 Train Loss: 0.4510 Train Acc: 0.8435 Eval Loss: 0.5063 Eval Acc: 0.8299 (LR: 0.001000)
[2025-05-04 06:16:49,555]: [ResNet20_relu6] Epoch: 043 Train Loss: 0.4443 Train Acc: 0.8452 Eval Loss: 0.5119 Eval Acc: 0.8320 (LR: 0.001000)
[2025-05-04 06:17:53,360]: [ResNet20_relu6] Epoch: 044 Train Loss: 0.4437 Train Acc: 0.8459 Eval Loss: 0.5213 Eval Acc: 0.8277 (LR: 0.001000)
[2025-05-04 06:18:57,611]: [ResNet20_relu6] Epoch: 045 Train Loss: 0.4419 Train Acc: 0.8461 Eval Loss: 0.5025 Eval Acc: 0.8305 (LR: 0.001000)
[2025-05-04 06:20:01,730]: [ResNet20_relu6] Epoch: 046 Train Loss: 0.4358 Train Acc: 0.8477 Eval Loss: 0.6013 Eval Acc: 0.8042 (LR: 0.001000)
[2025-05-04 06:21:02,008]: [ResNet20_relu6] Epoch: 047 Train Loss: 0.4283 Train Acc: 0.8512 Eval Loss: 0.5079 Eval Acc: 0.8333 (LR: 0.001000)
[2025-05-04 06:22:01,628]: [ResNet20_relu6] Epoch: 048 Train Loss: 0.4216 Train Acc: 0.8535 Eval Loss: 0.5072 Eval Acc: 0.8308 (LR: 0.001000)
[2025-05-04 06:23:09,351]: [ResNet20_relu6] Epoch: 049 Train Loss: 0.4139 Train Acc: 0.8559 Eval Loss: 0.4779 Eval Acc: 0.8412 (LR: 0.001000)
[2025-05-04 06:24:12,303]: [ResNet20_relu6] Epoch: 050 Train Loss: 0.4165 Train Acc: 0.8556 Eval Loss: 0.5388 Eval Acc: 0.8222 (LR: 0.001000)
[2025-05-04 06:25:13,145]: [ResNet20_relu6] Epoch: 051 Train Loss: 0.4083 Train Acc: 0.8588 Eval Loss: 0.5277 Eval Acc: 0.8246 (LR: 0.001000)
[2025-05-04 06:26:13,570]: [ResNet20_relu6] Epoch: 052 Train Loss: 0.4080 Train Acc: 0.8588 Eval Loss: 0.5025 Eval Acc: 0.8293 (LR: 0.001000)
[2025-05-04 06:27:13,404]: [ResNet20_relu6] Epoch: 053 Train Loss: 0.4044 Train Acc: 0.8575 Eval Loss: 0.5107 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-04 06:28:15,789]: [ResNet20_relu6] Epoch: 054 Train Loss: 0.3958 Train Acc: 0.8621 Eval Loss: 0.4984 Eval Acc: 0.8365 (LR: 0.001000)
[2025-05-04 06:29:24,016]: [ResNet20_relu6] Epoch: 055 Train Loss: 0.3944 Train Acc: 0.8624 Eval Loss: 0.4972 Eval Acc: 0.8357 (LR: 0.001000)
[2025-05-04 06:30:35,065]: [ResNet20_relu6] Epoch: 056 Train Loss: 0.3871 Train Acc: 0.8647 Eval Loss: 0.5418 Eval Acc: 0.8217 (LR: 0.001000)
[2025-05-04 06:31:46,298]: [ResNet20_relu6] Epoch: 057 Train Loss: 0.3859 Train Acc: 0.8655 Eval Loss: 0.4839 Eval Acc: 0.8419 (LR: 0.001000)
[2025-05-04 06:32:57,856]: [ResNet20_relu6] Epoch: 058 Train Loss: 0.3803 Train Acc: 0.8670 Eval Loss: 0.4920 Eval Acc: 0.8403 (LR: 0.001000)
[2025-05-04 06:34:09,025]: [ResNet20_relu6] Epoch: 059 Train Loss: 0.3812 Train Acc: 0.8674 Eval Loss: 0.4980 Eval Acc: 0.8391 (LR: 0.001000)
[2025-05-04 06:35:20,349]: [ResNet20_relu6] Epoch: 060 Train Loss: 0.3791 Train Acc: 0.8678 Eval Loss: 0.4809 Eval Acc: 0.8393 (LR: 0.001000)
[2025-05-04 06:36:31,805]: [ResNet20_relu6] Epoch: 061 Train Loss: 0.3707 Train Acc: 0.8708 Eval Loss: 0.4593 Eval Acc: 0.8476 (LR: 0.001000)
[2025-05-04 06:37:42,872]: [ResNet20_relu6] Epoch: 062 Train Loss: 0.3746 Train Acc: 0.8694 Eval Loss: 0.4788 Eval Acc: 0.8418 (LR: 0.001000)
[2025-05-04 06:38:53,926]: [ResNet20_relu6] Epoch: 063 Train Loss: 0.3658 Train Acc: 0.8722 Eval Loss: 0.4609 Eval Acc: 0.8487 (LR: 0.001000)
[2025-05-04 06:40:05,685]: [ResNet20_relu6] Epoch: 064 Train Loss: 0.3592 Train Acc: 0.8753 Eval Loss: 0.4898 Eval Acc: 0.8383 (LR: 0.001000)
[2025-05-04 06:41:17,178]: [ResNet20_relu6] Epoch: 065 Train Loss: 0.3624 Train Acc: 0.8726 Eval Loss: 0.4825 Eval Acc: 0.8429 (LR: 0.001000)
[2025-05-04 06:42:28,178]: [ResNet20_relu6] Epoch: 066 Train Loss: 0.3577 Train Acc: 0.8758 Eval Loss: 0.4554 Eval Acc: 0.8505 (LR: 0.001000)
[2025-05-04 06:43:39,292]: [ResNet20_relu6] Epoch: 067 Train Loss: 0.3550 Train Acc: 0.8753 Eval Loss: 0.5102 Eval Acc: 0.8391 (LR: 0.001000)
[2025-05-04 06:44:50,788]: [ResNet20_relu6] Epoch: 068 Train Loss: 0.3480 Train Acc: 0.8797 Eval Loss: 0.4620 Eval Acc: 0.8518 (LR: 0.001000)
[2025-05-04 06:46:03,286]: [ResNet20_relu6] Epoch: 069 Train Loss: 0.3473 Train Acc: 0.8776 Eval Loss: 0.4782 Eval Acc: 0.8427 (LR: 0.001000)
[2025-05-04 06:47:23,549]: [ResNet20_relu6] Epoch: 070 Train Loss: 0.3471 Train Acc: 0.8779 Eval Loss: 0.4509 Eval Acc: 0.8506 (LR: 0.000100)
[2025-05-04 06:48:32,779]: [ResNet20_relu6] Epoch: 071 Train Loss: 0.3072 Train Acc: 0.8934 Eval Loss: 0.4019 Eval Acc: 0.8664 (LR: 0.000100)
[2025-05-04 06:49:43,934]: [ResNet20_relu6] Epoch: 072 Train Loss: 0.2966 Train Acc: 0.8977 Eval Loss: 0.3994 Eval Acc: 0.8672 (LR: 0.000100)
[2025-05-04 06:50:55,313]: [ResNet20_relu6] Epoch: 073 Train Loss: 0.2956 Train Acc: 0.8972 Eval Loss: 0.4009 Eval Acc: 0.8680 (LR: 0.000100)
[2025-05-04 06:52:05,718]: [ResNet20_relu6] Epoch: 074 Train Loss: 0.2951 Train Acc: 0.8979 Eval Loss: 0.3979 Eval Acc: 0.8681 (LR: 0.000100)
[2025-05-04 06:53:17,085]: [ResNet20_relu6] Epoch: 075 Train Loss: 0.2912 Train Acc: 0.8999 Eval Loss: 0.3991 Eval Acc: 0.8677 (LR: 0.000100)
[2025-05-04 06:54:27,587]: [ResNet20_relu6] Epoch: 076 Train Loss: 0.2890 Train Acc: 0.9014 Eval Loss: 0.3983 Eval Acc: 0.8685 (LR: 0.000100)
[2025-05-04 06:55:38,451]: [ResNet20_relu6] Epoch: 077 Train Loss: 0.2883 Train Acc: 0.9011 Eval Loss: 0.4012 Eval Acc: 0.8678 (LR: 0.000100)
[2025-05-04 06:56:46,578]: [ResNet20_relu6] Epoch: 078 Train Loss: 0.2875 Train Acc: 0.8997 Eval Loss: 0.3999 Eval Acc: 0.8680 (LR: 0.000100)
[2025-05-04 06:57:51,365]: [ResNet20_relu6] Epoch: 079 Train Loss: 0.2872 Train Acc: 0.9011 Eval Loss: 0.3994 Eval Acc: 0.8680 (LR: 0.000100)
[2025-05-04 06:59:02,970]: [ResNet20_relu6] Epoch: 080 Train Loss: 0.2851 Train Acc: 0.9026 Eval Loss: 0.3988 Eval Acc: 0.8672 (LR: 0.000100)
[2025-05-04 07:00:13,745]: [ResNet20_relu6] Epoch: 081 Train Loss: 0.2835 Train Acc: 0.9021 Eval Loss: 0.3975 Eval Acc: 0.8698 (LR: 0.000100)
[2025-05-04 07:01:24,940]: [ResNet20_relu6] Epoch: 082 Train Loss: 0.2827 Train Acc: 0.9012 Eval Loss: 0.3969 Eval Acc: 0.8696 (LR: 0.000100)
[2025-05-04 07:02:35,542]: [ResNet20_relu6] Epoch: 083 Train Loss: 0.2836 Train Acc: 0.9011 Eval Loss: 0.3992 Eval Acc: 0.8687 (LR: 0.000100)
[2025-05-04 07:03:46,123]: [ResNet20_relu6] Epoch: 084 Train Loss: 0.2808 Train Acc: 0.9027 Eval Loss: 0.3973 Eval Acc: 0.8681 (LR: 0.000100)
[2025-05-04 07:04:51,023]: [ResNet20_relu6] Epoch: 085 Train Loss: 0.2786 Train Acc: 0.9044 Eval Loss: 0.3968 Eval Acc: 0.8686 (LR: 0.000100)
[2025-05-04 07:05:52,746]: [ResNet20_relu6] Epoch: 086 Train Loss: 0.2779 Train Acc: 0.9047 Eval Loss: 0.3973 Eval Acc: 0.8701 (LR: 0.000100)
[2025-05-04 07:07:00,736]: [ResNet20_relu6] Epoch: 087 Train Loss: 0.2797 Train Acc: 0.9028 Eval Loss: 0.3974 Eval Acc: 0.8684 (LR: 0.000100)
[2025-05-04 07:08:11,416]: [ResNet20_relu6] Epoch: 088 Train Loss: 0.2766 Train Acc: 0.9048 Eval Loss: 0.4007 Eval Acc: 0.8689 (LR: 0.000100)
[2025-05-04 07:09:23,013]: [ResNet20_relu6] Epoch: 089 Train Loss: 0.2774 Train Acc: 0.9045 Eval Loss: 0.3990 Eval Acc: 0.8692 (LR: 0.000100)
[2025-05-04 07:10:34,410]: [ResNet20_relu6] Epoch: 090 Train Loss: 0.2783 Train Acc: 0.9037 Eval Loss: 0.3967 Eval Acc: 0.8702 (LR: 0.000100)
[2025-05-04 07:11:45,487]: [ResNet20_relu6] Epoch: 091 Train Loss: 0.2765 Train Acc: 0.9043 Eval Loss: 0.4048 Eval Acc: 0.8664 (LR: 0.000100)
[2025-05-04 07:12:57,269]: [ResNet20_relu6] Epoch: 092 Train Loss: 0.2795 Train Acc: 0.9031 Eval Loss: 0.4044 Eval Acc: 0.8690 (LR: 0.000100)
[2025-05-04 07:14:07,840]: [ResNet20_relu6] Epoch: 093 Train Loss: 0.2753 Train Acc: 0.9056 Eval Loss: 0.4019 Eval Acc: 0.8675 (LR: 0.000100)
[2025-05-04 07:15:18,461]: [ResNet20_relu6] Epoch: 094 Train Loss: 0.2756 Train Acc: 0.9037 Eval Loss: 0.3999 Eval Acc: 0.8689 (LR: 0.000100)
[2025-05-04 07:16:28,151]: [ResNet20_relu6] Epoch: 095 Train Loss: 0.2781 Train Acc: 0.9033 Eval Loss: 0.3972 Eval Acc: 0.8717 (LR: 0.000100)
[2025-05-04 07:17:32,321]: [ResNet20_relu6] Epoch: 096 Train Loss: 0.2765 Train Acc: 0.9037 Eval Loss: 0.3957 Eval Acc: 0.8715 (LR: 0.000100)
[2025-05-04 07:18:33,799]: [ResNet20_relu6] Epoch: 097 Train Loss: 0.2740 Train Acc: 0.9045 Eval Loss: 0.3994 Eval Acc: 0.8704 (LR: 0.000100)
[2025-05-04 07:19:36,359]: [ResNet20_relu6] Epoch: 098 Train Loss: 0.2729 Train Acc: 0.9060 Eval Loss: 0.3991 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-04 07:20:38,410]: [ResNet20_relu6] Epoch: 099 Train Loss: 0.2751 Train Acc: 0.9049 Eval Loss: 0.3997 Eval Acc: 0.8706 (LR: 0.000100)
[2025-05-04 07:21:39,521]: [ResNet20_relu6] Epoch: 100 Train Loss: 0.2729 Train Acc: 0.9050 Eval Loss: 0.4079 Eval Acc: 0.8670 (LR: 0.000010)
[2025-05-04 07:22:42,536]: [ResNet20_relu6] Epoch: 101 Train Loss: 0.2703 Train Acc: 0.9058 Eval Loss: 0.3968 Eval Acc: 0.8718 (LR: 0.000010)
[2025-05-04 07:23:45,005]: [ResNet20_relu6] Epoch: 102 Train Loss: 0.2674 Train Acc: 0.9071 Eval Loss: 0.3965 Eval Acc: 0.8725 (LR: 0.000010)
[2025-05-04 07:24:47,426]: [ResNet20_relu6] Epoch: 103 Train Loss: 0.2668 Train Acc: 0.9071 Eval Loss: 0.3960 Eval Acc: 0.8719 (LR: 0.000010)
[2025-05-04 07:25:49,362]: [ResNet20_relu6] Epoch: 104 Train Loss: 0.2674 Train Acc: 0.9082 Eval Loss: 0.3972 Eval Acc: 0.8715 (LR: 0.000010)
[2025-05-04 07:26:53,088]: [ResNet20_relu6] Epoch: 105 Train Loss: 0.2653 Train Acc: 0.9077 Eval Loss: 0.3987 Eval Acc: 0.8708 (LR: 0.000010)
[2025-05-04 07:27:52,867]: [ResNet20_relu6] Epoch: 106 Train Loss: 0.2657 Train Acc: 0.9085 Eval Loss: 0.3962 Eval Acc: 0.8714 (LR: 0.000010)
[2025-05-04 07:28:52,952]: [ResNet20_relu6] Epoch: 107 Train Loss: 0.2673 Train Acc: 0.9079 Eval Loss: 0.3979 Eval Acc: 0.8700 (LR: 0.000010)
[2025-05-04 07:29:38,558]: [ResNet20_relu6] Epoch: 108 Train Loss: 0.2668 Train Acc: 0.9074 Eval Loss: 0.3974 Eval Acc: 0.8712 (LR: 0.000010)
[2025-05-04 07:30:22,898]: [ResNet20_relu6] Epoch: 109 Train Loss: 0.2673 Train Acc: 0.9077 Eval Loss: 0.3967 Eval Acc: 0.8719 (LR: 0.000010)
[2025-05-04 07:31:07,361]: [ResNet20_relu6] Epoch: 110 Train Loss: 0.2692 Train Acc: 0.9063 Eval Loss: 0.3958 Eval Acc: 0.8724 (LR: 0.000010)
[2025-05-04 07:31:52,768]: [ResNet20_relu6] Epoch: 111 Train Loss: 0.2679 Train Acc: 0.9075 Eval Loss: 0.3966 Eval Acc: 0.8722 (LR: 0.000010)
[2025-05-04 07:32:38,643]: [ResNet20_relu6] Epoch: 112 Train Loss: 0.2668 Train Acc: 0.9076 Eval Loss: 0.3971 Eval Acc: 0.8719 (LR: 0.000010)
[2025-05-04 07:33:25,286]: [ResNet20_relu6] Epoch: 113 Train Loss: 0.2660 Train Acc: 0.9087 Eval Loss: 0.3989 Eval Acc: 0.8717 (LR: 0.000010)
[2025-05-04 07:34:15,616]: [ResNet20_relu6] Epoch: 114 Train Loss: 0.2686 Train Acc: 0.9071 Eval Loss: 0.3984 Eval Acc: 0.8717 (LR: 0.000010)
[2025-05-04 07:35:01,569]: [ResNet20_relu6] Epoch: 115 Train Loss: 0.2692 Train Acc: 0.9065 Eval Loss: 0.3985 Eval Acc: 0.8712 (LR: 0.000010)
[2025-05-04 07:35:46,669]: [ResNet20_relu6] Epoch: 116 Train Loss: 0.2663 Train Acc: 0.9080 Eval Loss: 0.3976 Eval Acc: 0.8725 (LR: 0.000010)
[2025-05-04 07:36:31,449]: [ResNet20_relu6] Epoch: 117 Train Loss: 0.2640 Train Acc: 0.9095 Eval Loss: 0.3963 Eval Acc: 0.8725 (LR: 0.000010)
[2025-05-04 07:36:31,449]: Early stopping was triggered!
[2025-05-04 07:36:31,477]: 
Training of full-precision model finished!
[2025-05-04 07:36:31,478]: Model Architecture:
[2025-05-04 07:36:31,479]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU6(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU6(inplace=True)
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-04 07:36:31,479]: 
Model Parameters with Weights:
[2025-05-04 07:36:31,479]: 
Parameter: initial_layer.0.weight
[2025-05-04 07:36:31,479]: Shape: torch.Size([16, 3, 3, 3])
[2025-05-04 07:36:31,511]: Sample Values (16 elements): [0.07916346192359924, 0.04515552148222923, 0.039497170597314835, -0.2147836834192276, -0.20753417909145355, -0.122551828622818, -0.1912747323513031, -0.010939427651464939, 0.1504778265953064, -0.164651021361351, 0.12030570209026337, -0.2820832133293152, 0.18094754219055176, -0.04617907851934433, 0.06852949410676956, 0.049409329891204834]
[2025-05-04 07:36:31,526]: Mean: -0.0042
[2025-05-04 07:36:31,530]: Std: 0.1612
[2025-05-04 07:36:31,552]: Min: -0.5470
[2025-05-04 07:36:31,566]: Max: 0.4420
[2025-05-04 07:36:31,566]: 
Parameter: initial_layer.1.weight
[2025-05-04 07:36:31,566]: Shape: torch.Size([16])
[2025-05-04 07:36:31,567]: Sample Values (16 elements): [0.8236367702484131, 1.0403167009353638, 0.8957263231277466, 0.9644500017166138, 1.0402730703353882, 0.8753491640090942, 1.0423681735992432, 0.7818368077278137, 0.87024986743927, 1.003503441810608, 0.8481106162071228, 1.0505387783050537, 0.8179961442947388, 1.0170258283615112, 0.8344235420227051, 1.0105022192001343]
[2025-05-04 07:36:31,568]: Mean: 0.9323
[2025-05-04 07:36:31,568]: Std: 0.0971
[2025-05-04 07:36:31,569]: Min: 0.7818
[2025-05-04 07:36:31,569]: Max: 1.0505
[2025-05-04 07:36:31,569]: 
Parameter: initial_layer.1.bias
[2025-05-04 07:36:31,569]: Shape: torch.Size([16])
[2025-05-04 07:36:31,570]: Sample Values (16 elements): [-0.02417217195034027, 0.20017433166503906, 0.14566025137901306, 0.2642415165901184, 0.3025396764278412, 0.20315225422382355, -0.011877191253006458, 0.11917474120855331, 0.2610227167606354, 0.14848540723323822, -0.11604131013154984, 0.3143416941165924, 0.11627525836229324, 0.4010239839553833, 0.10616128891706467, -0.13466478884220123]
[2025-05-04 07:36:31,570]: Mean: 0.1435
[2025-05-04 07:36:31,571]: Std: 0.1540
[2025-05-04 07:36:31,571]: Min: -0.1347
[2025-05-04 07:36:31,572]: Max: 0.4010
[2025-05-04 07:36:31,572]: 
Parameter: layer1.0.conv1.weight
[2025-05-04 07:36:31,572]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:36:31,580]: Sample Values (16 elements): [-0.010251767933368683, -0.017190080136060715, -0.021690672263503075, 0.11209706217050552, 0.07348646968603134, -0.07041889429092407, 0.033713359385728836, -0.04110242798924446, -0.04327792301774025, -0.009111667051911354, -0.007014253176748753, 0.02999420464038849, -0.07997260242700577, -0.019504379481077194, 0.06658574193716049, 0.0008952964562922716]
[2025-05-04 07:36:31,582]: Mean: -0.0037
[2025-05-04 07:36:31,585]: Std: 0.0629
[2025-05-04 07:36:31,593]: Min: -0.1875
[2025-05-04 07:36:31,597]: Max: 0.2265
[2025-05-04 07:36:31,597]: 
Parameter: layer1.0.bn1.weight
[2025-05-04 07:36:31,597]: Shape: torch.Size([16])
[2025-05-04 07:36:31,611]: Sample Values (16 elements): [1.0211986303329468, 0.9414296746253967, 1.0197440385818481, 0.9073984026908875, 1.0054994821548462, 0.9765118956565857, 0.9130345582962036, 1.0427861213684082, 0.8994613885879517, 0.9556550979614258, 0.8678727746009827, 0.9398626089096069, 0.9987841248512268, 1.0358407497406006, 1.0055128335952759, 0.9872925281524658]
[2025-05-04 07:36:31,632]: Mean: 0.9699
[2025-05-04 07:36:31,638]: Std: 0.0535
[2025-05-04 07:36:31,640]: Min: 0.8679
[2025-05-04 07:36:31,641]: Max: 1.0428
[2025-05-04 07:36:31,642]: 
Parameter: layer1.0.bn1.bias
[2025-05-04 07:36:31,642]: Shape: torch.Size([16])
[2025-05-04 07:36:31,646]: Sample Values (16 elements): [0.18799065053462982, 0.1376054584980011, -0.07362233102321625, -0.03836007043719292, -0.07248401641845703, 0.07165093719959259, 0.05407405272126198, 0.08697225898504257, -0.005566812120378017, 0.019970105960965157, -0.12209612131118774, -0.018743084743618965, 0.0907929316163063, -0.00883534923195839, 0.05600506439805031, 0.06058890372514725]
[2025-05-04 07:36:31,647]: Mean: 0.0266
[2025-05-04 07:36:31,648]: Std: 0.0823
[2025-05-04 07:36:31,650]: Min: -0.1221
[2025-05-04 07:36:31,654]: Max: 0.1880
[2025-05-04 07:36:31,654]: 
Parameter: layer1.0.conv2.weight
[2025-05-04 07:36:31,654]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:36:31,673]: Sample Values (16 elements): [0.08291779458522797, -0.10336242616176605, 0.08916690200567245, -0.017910286784172058, -0.003053773893043399, 0.08220869302749634, 0.13413836061954498, -0.00800766795873642, 0.012081452645361423, 0.08140284568071365, 0.13488136231899261, -0.11944747716188431, 0.025428904220461845, -0.09533406794071198, -0.029904598370194435, -0.03932623937726021]
[2025-05-04 07:36:31,682]: Mean: -0.0043
[2025-05-04 07:36:31,682]: Std: 0.0606
[2025-05-04 07:36:31,682]: Min: -0.2547
[2025-05-04 07:36:31,683]: Max: 0.2065
[2025-05-04 07:36:31,683]: 
Parameter: layer1.0.bn2.weight
[2025-05-04 07:36:31,683]: Shape: torch.Size([16])
[2025-05-04 07:36:31,684]: Sample Values (16 elements): [1.054089069366455, 1.024375557899475, 0.9300198554992676, 0.9658990502357483, 0.9851027131080627, 1.0219032764434814, 1.0332012176513672, 1.0063090324401855, 1.001444697380066, 0.9702975749969482, 0.9564521908760071, 1.1016089916229248, 0.9096195101737976, 1.0289472341537476, 1.0229206085205078, 1.1595960855484009]
[2025-05-04 07:36:31,685]: Mean: 1.0107
[2025-05-04 07:36:31,685]: Std: 0.0620
[2025-05-04 07:36:31,685]: Min: 0.9096
[2025-05-04 07:36:31,686]: Max: 1.1596
[2025-05-04 07:36:31,686]: 
Parameter: layer1.0.bn2.bias
[2025-05-04 07:36:31,686]: Shape: torch.Size([16])
[2025-05-04 07:36:31,687]: Sample Values (16 elements): [0.09743022918701172, 0.08517932146787643, 0.08874400705099106, 0.01853153295814991, 0.020089445635676384, -0.04730478301644325, 0.128985196352005, 0.011339630000293255, -0.0025654665660113096, 0.1994381546974182, 0.12134303897619247, 0.07634425908327103, 0.0748283714056015, 0.07902149856090546, 0.11234142631292343, 0.010776014998555183]
[2025-05-04 07:36:31,687]: Mean: 0.0672
[2025-05-04 07:36:31,688]: Std: 0.0617
[2025-05-04 07:36:31,688]: Min: -0.0473
[2025-05-04 07:36:31,691]: Max: 0.1994
[2025-05-04 07:36:31,691]: 
Parameter: layer1.1.conv1.weight
[2025-05-04 07:36:31,691]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:36:31,695]: Sample Values (16 elements): [-0.1090938150882721, 0.05098721757531166, 0.005774270743131638, -0.02762550301849842, 0.025558974593877792, -0.07404235750436783, 0.015076065436005592, -0.06304462254047394, 0.029124842956662178, 0.013099119067192078, -0.07745092362165451, 0.18641991913318634, -0.004670740570873022, -0.029276790097355843, 0.0045956713147461414, 0.024007383733987808]
[2025-05-04 07:36:31,699]: Mean: -0.0014
[2025-05-04 07:36:31,702]: Std: 0.0611
[2025-05-04 07:36:31,703]: Min: -0.3408
[2025-05-04 07:36:31,704]: Max: 0.2688
[2025-05-04 07:36:31,704]: 
Parameter: layer1.1.bn1.weight
[2025-05-04 07:36:31,704]: Shape: torch.Size([16])
[2025-05-04 07:36:31,708]: Sample Values (16 elements): [1.0185816287994385, 0.9555760025978088, 0.9801194667816162, 0.9619393944740295, 1.0036749839782715, 0.9797201156616211, 0.9259322285652161, 0.9682489037513733, 0.9254246354103088, 0.9466891288757324, 0.964263379573822, 1.138464093208313, 0.8902931809425354, 0.9743630886077881, 0.9486996531486511, 0.9511116147041321]
[2025-05-04 07:36:31,711]: Mean: 0.9708
[2025-05-04 07:36:31,716]: Std: 0.0542
[2025-05-04 07:36:31,727]: Min: 0.8903
[2025-05-04 07:36:31,739]: Max: 1.1385
[2025-05-04 07:36:31,739]: 
Parameter: layer1.1.bn1.bias
[2025-05-04 07:36:31,739]: Shape: torch.Size([16])
[2025-05-04 07:36:31,742]: Sample Values (16 elements): [-0.043123066425323486, 0.0476248599588871, -0.01778336614370346, -0.04839615523815155, 0.03022231161594391, -0.08626440912485123, -0.05839461460709572, -0.02784835174679756, -0.0003318295057397336, 0.010123169980943203, -0.061382412910461426, -0.02498154155910015, 0.015464725904166698, -0.03454967960715294, 0.03615214303135872, 0.05990025773644447]
[2025-05-04 07:36:31,742]: Mean: -0.0127
[2025-05-04 07:36:31,743]: Std: 0.0429
[2025-05-04 07:36:31,743]: Min: -0.0863
[2025-05-04 07:36:31,743]: Max: 0.0599
[2025-05-04 07:36:31,744]: 
Parameter: layer1.1.conv2.weight
[2025-05-04 07:36:31,744]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:36:31,744]: Sample Values (16 elements): [-0.08444114774465561, 0.01593642495572567, 0.07228177040815353, -0.040626540780067444, -0.05599215626716614, 0.07876989245414734, 0.11129734665155411, -0.00836584810167551, 0.04800569638609886, -0.08355657756328583, -0.03913119435310364, -0.05762375146150589, 0.08310851454734802, -0.0512717142701149, -0.0054746633395552635, 0.06858138740062714]
[2025-05-04 07:36:31,745]: Mean: 0.0019
[2025-05-04 07:36:31,745]: Std: 0.0576
[2025-05-04 07:36:31,746]: Min: -0.1698
[2025-05-04 07:36:31,746]: Max: 0.2114
[2025-05-04 07:36:31,746]: 
Parameter: layer1.1.bn2.weight
[2025-05-04 07:36:31,746]: Shape: torch.Size([16])
[2025-05-04 07:36:31,748]: Sample Values (16 elements): [0.9520692825317383, 0.9002373218536377, 0.9425448775291443, 0.9706606864929199, 0.9609611630439758, 0.9763571619987488, 1.0044859647750854, 0.9693623781204224, 0.9543860554695129, 0.9683557748794556, 0.9943419694900513, 0.9640865921974182, 1.0301706790924072, 0.9027103781700134, 0.9231959581375122, 0.9286637306213379]
[2025-05-04 07:36:31,748]: Mean: 0.9589
[2025-05-04 07:36:31,749]: Std: 0.0348
[2025-05-04 07:36:31,749]: Min: 0.9002
[2025-05-04 07:36:31,750]: Max: 1.0302
[2025-05-04 07:36:31,750]: 
Parameter: layer1.1.bn2.bias
[2025-05-04 07:36:31,750]: Shape: torch.Size([16])
[2025-05-04 07:36:31,758]: Sample Values (16 elements): [0.1442146897315979, 0.06081605330109596, 0.0840926542878151, 0.016898412257432938, 0.09799060970544815, 0.07336778193712234, 0.06958532333374023, 0.0668991282582283, 0.0982271358370781, 0.056810177862644196, -0.05201626196503639, 0.056636743247509, -0.012289140373468399, 0.08017616719007492, 0.029631422832608223, -0.012247908860445023]
[2025-05-04 07:36:31,761]: Mean: 0.0537
[2025-05-04 07:36:31,762]: Std: 0.0493
[2025-05-04 07:36:31,764]: Min: -0.0520
[2025-05-04 07:36:31,766]: Max: 0.1442
[2025-05-04 07:36:31,766]: 
Parameter: layer1.2.conv1.weight
[2025-05-04 07:36:31,766]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:36:31,772]: Sample Values (16 elements): [-0.025917738676071167, 0.06146809086203575, 0.03301309049129486, -0.03701713681221008, 0.014465718530118465, 0.0769379660487175, -0.07933846861124039, 0.016390645876526833, 0.055016640573740005, -0.06899260729551315, 0.005638314876705408, 0.061852116137742996, -0.07289741933345795, -0.08595287799835205, -0.018858429044485092, 0.0030689947307109833]
[2025-05-04 07:36:31,777]: Mean: -0.0024
[2025-05-04 07:36:31,799]: Std: 0.0569
[2025-05-04 07:36:31,800]: Min: -0.2069
[2025-05-04 07:36:31,800]: Max: 0.1699
[2025-05-04 07:36:31,801]: 
Parameter: layer1.2.bn1.weight
[2025-05-04 07:36:31,801]: Shape: torch.Size([16])
[2025-05-04 07:36:31,802]: Sample Values (16 elements): [1.0429346561431885, 0.9744626879692078, 0.9982739686965942, 0.9303603768348694, 0.9870772957801819, 1.0221551656723022, 0.9396462440490723, 0.9298244714736938, 0.9731248617172241, 0.9386904835700989, 1.027869462966919, 0.95429927110672, 0.9404537677764893, 0.9962499737739563, 0.9307259917259216, 0.952605664730072]
[2025-05-04 07:36:31,802]: Mean: 0.9712
[2025-05-04 07:36:31,802]: Std: 0.0376
[2025-05-04 07:36:31,803]: Min: 0.9298
[2025-05-04 07:36:31,804]: Max: 1.0429
[2025-05-04 07:36:31,804]: 
Parameter: layer1.2.bn1.bias
[2025-05-04 07:36:31,804]: Shape: torch.Size([16])
[2025-05-04 07:36:31,813]: Sample Values (16 elements): [-0.018106963485479355, 0.0163078922778368, 0.09493110328912735, 0.010280834510922432, -0.017577169463038445, -0.017201144248247147, 0.024098506197333336, -0.032193873077631, 0.01595459133386612, -0.006182650104165077, -0.024184633046388626, -0.02497200481593609, 0.02380322478711605, -0.06096236780285835, 0.012712921015918255, 0.000856024504173547]
[2025-05-04 07:36:31,816]: Mean: -0.0002
[2025-05-04 07:36:31,819]: Std: 0.0345
[2025-05-04 07:36:31,821]: Min: -0.0610
[2025-05-04 07:36:31,822]: Max: 0.0949
[2025-05-04 07:36:31,822]: 
Parameter: layer1.2.conv2.weight
[2025-05-04 07:36:31,822]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:36:31,824]: Sample Values (16 elements): [-0.05041470378637314, -0.004124179482460022, 0.024568190798163414, 0.0074467784725129604, 0.042306821793317795, -0.007818762212991714, -0.01881776936352253, 0.04361043497920036, -0.026249175891280174, 0.03390116989612579, -0.04316919296979904, 0.00013166472490411252, -0.034236300736665726, 0.02954188361763954, -0.05696473643183708, 0.006823568604886532]
[2025-05-04 07:36:31,826]: Mean: -0.0011
[2025-05-04 07:36:31,831]: Std: 0.0539
[2025-05-04 07:36:31,843]: Min: -0.1651
[2025-05-04 07:36:31,852]: Max: 0.1748
[2025-05-04 07:36:31,852]: 
Parameter: layer1.2.bn2.weight
[2025-05-04 07:36:31,852]: Shape: torch.Size([16])
[2025-05-04 07:36:31,856]: Sample Values (16 elements): [0.9724392294883728, 0.9591149687767029, 0.9899391531944275, 0.9809720516204834, 1.0134018659591675, 1.0224004983901978, 0.9437853097915649, 0.9142408967018127, 1.0378447771072388, 0.9119560122489929, 0.9890534281730652, 0.911601185798645, 0.9600826501846313, 0.9483600854873657, 0.9305382966995239, 0.9786733984947205]
[2025-05-04 07:36:31,857]: Mean: 0.9665
[2025-05-04 07:36:31,857]: Std: 0.0390
[2025-05-04 07:36:31,858]: Min: 0.9116
[2025-05-04 07:36:31,858]: Max: 1.0378
[2025-05-04 07:36:31,859]: 
Parameter: layer1.2.bn2.bias
[2025-05-04 07:36:31,859]: Shape: torch.Size([16])
[2025-05-04 07:36:31,859]: Sample Values (16 elements): [0.07052887231111526, 0.06633959710597992, 0.07433871179819107, 0.02842685580253601, 0.139523446559906, 0.016460591927170753, 0.024780817329883575, 0.05678808316588402, 0.049186382442712784, 0.03450603038072586, 0.0663250982761383, 0.0743437111377716, 0.05824430659413338, 0.03723805397748947, 0.018693052232265472, 0.05508380010724068]
[2025-05-04 07:36:31,860]: Mean: 0.0544
[2025-05-04 07:36:31,860]: Std: 0.0301
[2025-05-04 07:36:31,861]: Min: 0.0165
[2025-05-04 07:36:31,862]: Max: 0.1395
[2025-05-04 07:36:31,862]: 
Parameter: layer2.0.conv1.weight
[2025-05-04 07:36:31,862]: Shape: torch.Size([32, 16, 3, 3])
[2025-05-04 07:36:31,869]: Sample Values (16 elements): [0.037755463272333145, -0.013607642613351345, -0.05107283964753151, 0.012136365287005901, -0.024100802838802338, -0.007835150696337223, -0.021214071661233902, 0.028316598385572433, 0.003943858202546835, -0.0590340718626976, 0.004347251728177071, -0.0019009821116924286, -0.09501703828573227, -0.06276264041662216, -0.04973432421684265, -0.04092550277709961]
[2025-05-04 07:36:31,871]: Mean: -0.0034
[2025-05-04 07:36:31,873]: Std: 0.0521
[2025-05-04 07:36:31,877]: Min: -0.1514
[2025-05-04 07:36:31,880]: Max: 0.1530
[2025-05-04 07:36:31,880]: 
Parameter: layer2.0.bn1.weight
[2025-05-04 07:36:31,880]: Shape: torch.Size([32])
[2025-05-04 07:36:31,882]: Sample Values (16 elements): [1.0238959789276123, 0.9652488231658936, 0.9527941346168518, 0.9528417587280273, 0.9883714914321899, 0.9432733654975891, 0.9518269300460815, 0.997361958026886, 0.9473474025726318, 0.9563004970550537, 0.96461421251297, 0.9926038384437561, 0.9671638607978821, 0.9790757298469543, 0.9802363514900208, 0.9583080410957336]
[2025-05-04 07:36:31,883]: Mean: 0.9717
[2025-05-04 07:36:31,888]: Std: 0.0191
[2025-05-04 07:36:31,900]: Min: 0.9429
[2025-05-04 07:36:31,902]: Max: 1.0239
[2025-05-04 07:36:31,902]: 
Parameter: layer2.0.bn1.bias
[2025-05-04 07:36:31,902]: Shape: torch.Size([32])
[2025-05-04 07:36:31,907]: Sample Values (16 elements): [0.0065683117136359215, -0.0059397537261247635, -0.013580008409917355, 0.00452797906473279, -0.024638596922159195, 0.03418223187327385, -0.00014032467151992023, 0.022273126989603043, 0.03275883197784424, 0.03621349111199379, 0.0023744595237076283, 0.0033712375443428755, 0.020261622965335846, 0.016799483448266983, -0.009466692805290222, 0.02068379707634449]
[2025-05-04 07:36:31,919]: Mean: 0.0049
[2025-05-04 07:36:31,924]: Std: 0.0182
[2025-05-04 07:36:31,924]: Min: -0.0429
[2025-05-04 07:36:31,941]: Max: 0.0362
[2025-05-04 07:36:31,941]: 
Parameter: layer2.0.conv2.weight
[2025-05-04 07:36:31,942]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:36:31,947]: Sample Values (16 elements): [0.003512636059895158, 0.0067549510858953, 0.004583857022225857, -0.00362399872392416, -0.01550211850553751, -0.02744312398135662, 0.04453229904174805, 0.035286977887153625, -0.01229606568813324, -0.006857915781438351, -0.011694814078509808, -0.009991738013923168, -0.006120426580309868, -0.041031695902347565, 0.04336986690759659, -0.02805742807686329]
[2025-05-04 07:36:31,947]: Mean: -0.0004
[2025-05-04 07:36:31,948]: Std: 0.0393
[2025-05-04 07:36:31,948]: Min: -0.1612
[2025-05-04 07:36:31,949]: Max: 0.1585
[2025-05-04 07:36:31,949]: 
Parameter: layer2.0.bn2.weight
[2025-05-04 07:36:31,949]: Shape: torch.Size([32])
[2025-05-04 07:36:31,949]: Sample Values (16 elements): [0.9768469929695129, 0.9679992198944092, 1.003234624862671, 0.9657084345817566, 0.9625725746154785, 0.9906494617462158, 0.994775116443634, 0.9776377081871033, 0.9954195618629456, 0.9502686262130737, 0.9666562676429749, 0.9557013511657715, 0.9799550175666809, 0.9705840349197388, 0.9517313241958618, 1.01358163356781]
[2025-05-04 07:36:31,950]: Mean: 0.9799
[2025-05-04 07:36:31,950]: Std: 0.0211
[2025-05-04 07:36:31,951]: Min: 0.9488
[2025-05-04 07:36:31,951]: Max: 1.0311
[2025-05-04 07:36:31,951]: 
Parameter: layer2.0.bn2.bias
[2025-05-04 07:36:31,951]: Shape: torch.Size([32])
[2025-05-04 07:36:31,955]: Sample Values (16 elements): [0.024865582585334778, 0.028241675347089767, 0.09250291436910629, -0.011950965039432049, 0.0476369671523571, 0.05131849646568298, 0.025388246402144432, -0.0020767298992723227, 0.022971659898757935, 0.03471878916025162, 0.00023824158415663987, 0.024672027677297592, 0.008071832358837128, 0.015796832740306854, 0.01439184881746769, 0.04408232867717743]
[2025-05-04 07:36:31,955]: Mean: 0.0170
[2025-05-04 07:36:31,956]: Std: 0.0262
[2025-05-04 07:36:31,956]: Min: -0.0431
[2025-05-04 07:36:31,957]: Max: 0.0925
[2025-05-04 07:36:31,957]: 
Parameter: layer2.0.downsample.0.weight
[2025-05-04 07:36:31,957]: Shape: torch.Size([32, 16, 1, 1])
[2025-05-04 07:36:31,960]: Sample Values (16 elements): [-0.04099641367793083, -0.12879806756973267, -0.1519954800605774, 0.08953928202390671, -0.14148950576782227, -0.05567440390586853, -0.05842457339167595, 0.11748121678829193, -0.08748069405555725, 0.05493338778614998, -0.17148859798908234, 0.08216775953769684, 0.012165360152721405, 0.1305781453847885, -0.22227603197097778, 0.15625694394111633]
[2025-05-04 07:36:31,964]: Mean: -0.0016
[2025-05-04 07:36:31,968]: Std: 0.1456
[2025-05-04 07:36:31,968]: Min: -0.3525
[2025-05-04 07:36:31,969]: Max: 0.2906
[2025-05-04 07:36:31,969]: 
Parameter: layer2.0.downsample.1.weight
[2025-05-04 07:36:31,969]: Shape: torch.Size([32])
[2025-05-04 07:36:31,971]: Sample Values (16 elements): [0.887843668460846, 0.9081549048423767, 0.9499088525772095, 0.8977646231651306, 0.9391433000564575, 0.9275205135345459, 0.8694157004356384, 0.8865131139755249, 0.871845543384552, 0.9108482003211975, 0.9447769522666931, 0.9300680160522461, 0.8999354243278503, 0.930711567401886, 0.9157818555831909, 0.9153946042060852]
[2025-05-04 07:36:31,973]: Mean: 0.9089
[2025-05-04 07:36:31,974]: Std: 0.0286
[2025-05-04 07:36:31,978]: Min: 0.8608
[2025-05-04 07:36:31,988]: Max: 0.9642
[2025-05-04 07:36:31,989]: 
Parameter: layer2.0.downsample.1.bias
[2025-05-04 07:36:31,989]: Shape: torch.Size([32])
[2025-05-04 07:36:31,997]: Sample Values (16 elements): [0.0031591951847076416, 0.09250291436910629, 0.024865582585334778, 0.028241675347089767, 0.015796832740306854, 0.01439184881746769, 0.04408232867717743, 0.007261756341904402, 0.025388246402144432, -0.0071780141443014145, 0.0476369671523571, 0.00023824158415663987, 0.022971659898757935, -0.008341088891029358, 0.013411433435976505, 0.009689422324299812]
[2025-05-04 07:36:32,010]: Mean: 0.0170
[2025-05-04 07:36:32,011]: Std: 0.0262
[2025-05-04 07:36:32,011]: Min: -0.0431
[2025-05-04 07:36:32,012]: Max: 0.0925
[2025-05-04 07:36:32,012]: 
Parameter: layer2.1.conv1.weight
[2025-05-04 07:36:32,012]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:36:32,013]: Sample Values (16 elements): [-0.04680410027503967, 0.08270210772752762, -0.0019535899627953768, 0.05836539342999458, 0.0004989136359654367, -0.014205409213900566, 0.020821252837777138, 0.019000336527824402, -0.03879331797361374, -0.03589097410440445, -0.023487815633416176, 0.021043475717306137, -0.011020801961421967, 0.09672258049249649, -0.07141943275928497, 0.11149980872869492]
[2025-05-04 07:36:32,014]: Mean: -0.0008
[2025-05-04 07:36:32,014]: Std: 0.0400
[2025-05-04 07:36:32,015]: Min: -0.1424
[2025-05-04 07:36:32,015]: Max: 0.1493
[2025-05-04 07:36:32,015]: 
Parameter: layer2.1.bn1.weight
[2025-05-04 07:36:32,015]: Shape: torch.Size([32])
[2025-05-04 07:36:32,016]: Sample Values (16 elements): [1.0002638101577759, 1.0046590566635132, 0.9624930620193481, 0.9907373785972595, 0.9587595462799072, 1.0255100727081299, 1.009447693824768, 0.9715034365653992, 0.9823866486549377, 0.9496278166770935, 0.9570415616035461, 0.9415949583053589, 0.9617947340011597, 0.9763963222503662, 0.9736571907997131, 0.9456104636192322]
[2025-05-04 07:36:32,017]: Mean: 0.9716
[2025-05-04 07:36:32,017]: Std: 0.0240
[2025-05-04 07:36:32,018]: Min: 0.9354
[2025-05-04 07:36:32,018]: Max: 1.0255
[2025-05-04 07:36:32,018]: 
Parameter: layer2.1.bn1.bias
[2025-05-04 07:36:32,018]: Shape: torch.Size([32])
[2025-05-04 07:36:32,021]: Sample Values (16 elements): [-0.037431832402944565, -0.049664147198200226, -0.0186848733574152, -0.021672965958714485, 0.009670018218457699, 0.019506175071001053, -0.03133964538574219, -0.013558095321059227, -0.019094567745923996, -0.022955000400543213, 0.002635333454236388, 0.014629949815571308, -0.0163061935454607, -0.0030758953653275967, -0.014345621690154076, 0.015501636080443859]
[2025-05-04 07:36:32,026]: Mean: -0.0146
[2025-05-04 07:36:32,028]: Std: 0.0166
[2025-05-04 07:36:32,031]: Min: -0.0497
[2025-05-04 07:36:32,032]: Max: 0.0195
[2025-05-04 07:36:32,032]: 
Parameter: layer2.1.conv2.weight
[2025-05-04 07:36:32,032]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:36:32,035]: Sample Values (16 elements): [-0.040714483708143234, -0.01278324518352747, -0.05935344099998474, -0.02674742601811886, -0.035401467233896255, 0.03485499322414398, -0.052635472267866135, -0.06574644893407822, -0.011505321599543095, 0.039675503969192505, -0.055327776819467545, -0.029014287516474724, 0.05276352912187576, 0.0020040676463395357, 0.013959735631942749, 0.013749372214078903]
[2025-05-04 07:36:32,038]: Mean: -0.0003
[2025-05-04 07:36:32,043]: Std: 0.0392
[2025-05-04 07:36:32,046]: Min: -0.1459
[2025-05-04 07:36:32,064]: Max: 0.1216
[2025-05-04 07:36:32,064]: 
Parameter: layer2.1.bn2.weight
[2025-05-04 07:36:32,064]: Shape: torch.Size([32])
[2025-05-04 07:36:32,070]: Sample Values (16 elements): [1.001212477684021, 0.9665312767028809, 0.9573642015457153, 0.9339757561683655, 0.9742059707641602, 1.0043516159057617, 0.9920709729194641, 0.9981591105461121, 0.986059308052063, 1.0489147901535034, 0.9529989361763, 0.9825852513313293, 0.9275679588317871, 0.9675332903862, 1.0236546993255615, 0.9910783767700195]
[2025-05-04 07:36:32,070]: Mean: 0.9762
[2025-05-04 07:36:32,071]: Std: 0.0316
[2025-05-04 07:36:32,072]: Min: 0.8968
[2025-05-04 07:36:32,072]: Max: 1.0489
[2025-05-04 07:36:32,072]: 
Parameter: layer2.1.bn2.bias
[2025-05-04 07:36:32,072]: Shape: torch.Size([32])
[2025-05-04 07:36:32,073]: Sample Values (16 elements): [-0.03987035155296326, 0.0412428081035614, -0.03254842013120651, -0.006125103682279587, 0.016490982845425606, 0.019367409870028496, 0.03619270399212837, -0.0006497176946140826, 0.008588439784944057, 0.027826927602291107, 0.033877160400152206, 0.027474649250507355, 0.0027566563803702593, 0.04157167673110962, -0.010382973589003086, -0.013269397430121899]
[2025-05-04 07:36:32,074]: Mean: 0.0096
[2025-05-04 07:36:32,075]: Std: 0.0216
[2025-05-04 07:36:32,078]: Min: -0.0399
[2025-05-04 07:36:32,081]: Max: 0.0432
[2025-05-04 07:36:32,081]: 
Parameter: layer2.2.conv1.weight
[2025-05-04 07:36:32,081]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:36:32,088]: Sample Values (16 elements): [-0.05741473659873009, -0.015103500336408615, -0.02430409938097, 0.03582810238003731, 0.005575224291533232, -0.028522934764623642, -0.0006965600186958909, 0.027198169380426407, 0.03164485841989517, 0.06232074275612831, 0.001369768870063126, -0.05207055062055588, -0.006205095909535885, 0.010112295858561993, -0.014714156277477741, 0.01872258260846138]
[2025-05-04 07:36:32,090]: Mean: -0.0012
[2025-05-04 07:36:32,091]: Std: 0.0383
[2025-05-04 07:36:32,092]: Min: -0.1180
[2025-05-04 07:36:32,093]: Max: 0.1401
[2025-05-04 07:36:32,093]: 
Parameter: layer2.2.bn1.weight
[2025-05-04 07:36:32,094]: Shape: torch.Size([32])
[2025-05-04 07:36:32,096]: Sample Values (16 elements): [0.9491555094718933, 0.986102819442749, 0.9855794310569763, 0.9454618692398071, 0.9960530996322632, 0.9860586524009705, 0.9838835000991821, 0.9928198456764221, 0.9638511538505554, 1.0056020021438599, 0.9784238934516907, 0.9542869925498962, 0.9776564836502075, 1.0035912990570068, 0.9385597705841064, 0.94106125831604]
[2025-05-04 07:36:32,100]: Mean: 0.9713
[2025-05-04 07:36:32,105]: Std: 0.0215
[2025-05-04 07:36:32,122]: Min: 0.9290
[2025-05-04 07:36:32,128]: Max: 1.0078
[2025-05-04 07:36:32,128]: 
Parameter: layer2.2.bn1.bias
[2025-05-04 07:36:32,128]: Shape: torch.Size([32])
[2025-05-04 07:36:32,129]: Sample Values (16 elements): [0.023065634071826935, -0.03279103711247444, 0.009020941331982613, -0.033673159778118134, -0.009935898706316948, 0.04961516335606575, -0.023752480745315552, -0.0402047261595726, -0.0074253324419260025, -0.029880160465836525, -0.007839852012693882, -0.013471895828843117, -0.02847365103662014, -0.03672610595822334, -0.03305885195732117, -0.014182032085955143]
[2025-05-04 07:36:32,129]: Mean: -0.0188
[2025-05-04 07:36:32,130]: Std: 0.0228
[2025-05-04 07:36:32,131]: Min: -0.0711
[2025-05-04 07:36:32,131]: Max: 0.0496
[2025-05-04 07:36:32,131]: 
Parameter: layer2.2.conv2.weight
[2025-05-04 07:36:32,131]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:36:32,132]: Sample Values (16 elements): [-0.01583033986389637, -0.05092095583677292, 0.061651699244976044, 0.019946323707699776, 0.04042286053299904, 0.023412156850099564, 0.01921110227704048, -0.011875265277922153, 0.023477477952837944, -0.04711150750517845, 0.04650144279003143, 0.04789197817444801, -0.034712184220552444, -0.043095074594020844, -0.06345050781965256, 0.07362924516201019]
[2025-05-04 07:36:32,133]: Mean: -0.0006
[2025-05-04 07:36:32,133]: Std: 0.0373
[2025-05-04 07:36:32,134]: Min: -0.1106
[2025-05-04 07:36:32,134]: Max: 0.1125
[2025-05-04 07:36:32,134]: 
Parameter: layer2.2.bn2.weight
[2025-05-04 07:36:32,134]: Shape: torch.Size([32])
[2025-05-04 07:36:32,135]: Sample Values (16 elements): [0.9748618602752686, 0.9991915822029114, 0.9572953581809998, 0.9784014821052551, 0.9895776510238647, 1.0240134000778198, 0.9776467084884644, 0.995735228061676, 0.9884412884712219, 0.9934632778167725, 1.007253885269165, 1.0062487125396729, 0.9822120070457458, 0.9934271574020386, 0.9873299598693848, 1.027274489402771]
[2025-05-04 07:36:32,136]: Mean: 0.9954
[2025-05-04 07:36:32,136]: Std: 0.0181
[2025-05-04 07:36:32,137]: Min: 0.9573
[2025-05-04 07:36:32,137]: Max: 1.0403
[2025-05-04 07:36:32,137]: 
Parameter: layer2.2.bn2.bias
[2025-05-04 07:36:32,137]: Shape: torch.Size([32])
[2025-05-04 07:36:32,140]: Sample Values (16 elements): [0.018273916095495224, 0.01429897639900446, -0.015149536542594433, 0.0021290930453687906, -0.00318315951153636, -0.0012623072834685445, 0.03444807603955269, 0.006305057555437088, -0.010435948148369789, 0.025507701560854912, 0.029004443436861038, 0.007156336214393377, 0.012368230149149895, 0.008985182270407677, 0.04629931226372719, 0.017666984349489212]
[2025-05-04 07:36:32,145]: Mean: 0.0113
[2025-05-04 07:36:32,148]: Std: 0.0182
[2025-05-04 07:36:32,149]: Min: -0.0213
[2025-05-04 07:36:32,151]: Max: 0.0687
[2025-05-04 07:36:32,151]: 
Parameter: layer3.0.conv1.weight
[2025-05-04 07:36:32,152]: Shape: torch.Size([64, 32, 3, 3])
[2025-05-04 07:36:32,154]: Sample Values (16 elements): [-0.03548092022538185, 0.014835366979241371, 0.04886027052998543, -0.01766273006796837, 0.022815829142928123, 0.024012546986341476, -0.0043770014308393, -0.035892926156520844, 0.045145485550165176, -0.0025456531438976526, -0.009805632755160332, -0.011936224065721035, -0.06929068267345428, 0.043729983270168304, -0.022988637909293175, 0.05070848762989044]
[2025-05-04 07:36:32,159]: Mean: -0.0005
[2025-05-04 07:36:32,163]: Std: 0.0359
[2025-05-04 07:36:32,175]: Min: -0.1129
[2025-05-04 07:36:32,190]: Max: 0.1121
[2025-05-04 07:36:32,190]: 
Parameter: layer3.0.bn1.weight
[2025-05-04 07:36:32,190]: Shape: torch.Size([64])
[2025-05-04 07:36:32,191]: Sample Values (16 elements): [0.985587477684021, 0.9660707712173462, 0.9839898943901062, 0.9732592105865479, 0.9720828533172607, 0.9708800911903381, 0.989499568939209, 0.963950514793396, 0.9562973380088806, 0.9291688799858093, 0.9831685423851013, 0.9577450752258301, 0.9705129861831665, 0.9858179092407227, 0.9837409853935242, 0.977729082107544]
[2025-05-04 07:36:32,191]: Mean: 0.9716
[2025-05-04 07:36:32,192]: Std: 0.0121
[2025-05-04 07:36:32,192]: Min: 0.9292
[2025-05-04 07:36:32,193]: Max: 0.9994
[2025-05-04 07:36:32,193]: 
Parameter: layer3.0.bn1.bias
[2025-05-04 07:36:32,193]: Shape: torch.Size([64])
[2025-05-04 07:36:32,194]: Sample Values (16 elements): [-0.014702831394970417, 0.011028640903532505, -0.01604747585952282, -0.01920877955853939, 0.004675667732954025, 0.01730175130069256, -0.012142590247094631, -0.037729211151599884, -0.012751000933349133, -0.043533697724342346, -0.03497455641627312, -0.026193831115961075, -0.009917233139276505, -0.018523698672652245, -0.0018930986989289522, -0.03431637957692146]
[2025-05-04 07:36:32,194]: Mean: -0.0159
[2025-05-04 07:36:32,198]: Std: 0.0142
[2025-05-04 07:36:32,198]: Min: -0.0437
[2025-05-04 07:36:32,199]: Max: 0.0173
[2025-05-04 07:36:32,199]: 
Parameter: layer3.0.conv2.weight
[2025-05-04 07:36:32,199]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:36:32,204]: Sample Values (16 elements): [0.010882020927965641, 0.01248148363083601, 0.014330235309898853, -0.019523028284311295, -0.004952522926032543, -0.045648764818906784, 0.009797628037631512, 0.005905902944505215, -0.04784559831023216, 0.035623855888843536, 0.04101894050836563, -0.005527596455067396, -0.021049048751592636, -0.02622159570455551, -0.0021780862007290125, -0.02957623451948166]
[2025-05-04 07:36:32,208]: Mean: -0.0005
[2025-05-04 07:36:32,210]: Std: 0.0271
[2025-05-04 07:36:32,211]: Min: -0.0980
[2025-05-04 07:36:32,213]: Max: 0.1025
[2025-05-04 07:36:32,213]: 
Parameter: layer3.0.bn2.weight
[2025-05-04 07:36:32,213]: Shape: torch.Size([64])
[2025-05-04 07:36:32,216]: Sample Values (16 elements): [1.0024884939193726, 1.0174635648727417, 1.0129386186599731, 0.9943857192993164, 1.020866870880127, 1.0015708208084106, 1.0513631105422974, 0.9956660866737366, 0.972473680973053, 1.0078039169311523, 1.0176352262496948, 1.0043504238128662, 1.0073175430297852, 1.0010082721710205, 0.9842332005500793, 1.02933669090271]
[2025-05-04 07:36:32,219]: Mean: 1.0031
[2025-05-04 07:36:32,221]: Std: 0.0182
[2025-05-04 07:36:32,228]: Min: 0.9576
[2025-05-04 07:36:32,228]: Max: 1.0545
[2025-05-04 07:36:32,228]: 
Parameter: layer3.0.bn2.bias
[2025-05-04 07:36:32,229]: Shape: torch.Size([64])
[2025-05-04 07:36:32,241]: Sample Values (16 elements): [-0.014407842420041561, -0.035511452704668045, -0.010142229497432709, -0.008867990225553513, 0.009010322391986847, -0.01214644405990839, -0.02186911553144455, -0.026951922103762627, -0.004965107422322035, -0.010638084262609482, -0.011010934598743916, -0.006616566795855761, -0.02281338907778263, -0.012422938831150532, 0.013925542123615742, -0.006975745316594839]
[2025-05-04 07:36:32,251]: Mean: -0.0108
[2025-05-04 07:36:32,251]: Std: 0.0152
[2025-05-04 07:36:32,252]: Min: -0.0527
[2025-05-04 07:36:32,252]: Max: 0.0178
[2025-05-04 07:36:32,252]: 
Parameter: layer3.0.downsample.0.weight
[2025-05-04 07:36:32,253]: Shape: torch.Size([64, 32, 1, 1])
[2025-05-04 07:36:32,253]: Sample Values (16 elements): [-0.060530077666044235, -0.12849152088165283, -0.018670637160539627, -0.0894869789481163, -0.0005195682169869542, 0.06133277341723442, 0.02080591954290867, 0.15139730274677277, 0.16778965294361115, -0.11823450028896332, 0.0013481045607477427, -0.03404504433274269, -0.051482606679201126, 0.08775395154953003, 0.18753111362457275, -0.07077602297067642]
[2025-05-04 07:36:32,254]: Mean: -0.0007
[2025-05-04 07:36:32,254]: Std: 0.1004
[2025-05-04 07:36:32,255]: Min: -0.2246
[2025-05-04 07:36:32,255]: Max: 0.2393
[2025-05-04 07:36:32,255]: 
Parameter: layer3.0.downsample.1.weight
[2025-05-04 07:36:32,255]: Shape: torch.Size([64])
[2025-05-04 07:36:32,257]: Sample Values (16 elements): [0.9197344779968262, 0.9224543571472168, 0.964669406414032, 0.9183048009872437, 0.8950297236442566, 0.9304542541503906, 0.9432646036148071, 0.9227772355079651, 0.9531716108322144, 0.9560648798942566, 0.9237486124038696, 0.9152071475982666, 0.8748632669448853, 0.9306069612503052, 0.9557695984840393, 0.9543598890304565]
[2025-05-04 07:36:32,258]: Mean: 0.9340
[2025-05-04 07:36:32,266]: Std: 0.0171
[2025-05-04 07:36:32,271]: Min: 0.8749
[2025-05-04 07:36:32,272]: Max: 0.9754
[2025-05-04 07:36:32,272]: 
Parameter: layer3.0.downsample.1.bias
[2025-05-04 07:36:32,272]: Shape: torch.Size([64])
[2025-05-04 07:36:32,274]: Sample Values (16 elements): [0.010639829561114311, -0.021141929551959038, -0.006118450313806534, -0.018648235127329826, -0.03225334361195564, -0.02943030185997486, 0.012878084555268288, -0.006975745316594839, -0.026020146906375885, -0.009216416627168655, -0.0009293917682953179, -0.017327627167105675, -0.024075839668512344, -0.01214644405990839, -0.010638084262609482, -0.006051087751984596]
[2025-05-04 07:36:32,277]: Mean: -0.0108
[2025-05-04 07:36:32,280]: Std: 0.0152
[2025-05-04 07:36:32,283]: Min: -0.0527
[2025-05-04 07:36:32,286]: Max: 0.0178
[2025-05-04 07:36:32,286]: 
Parameter: layer3.1.conv1.weight
[2025-05-04 07:36:32,286]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:36:32,310]: Sample Values (16 elements): [-0.03631637245416641, 0.00496284943073988, 0.00794342439621687, 0.04327075555920601, -0.026629038155078888, 0.025328241288661957, -0.010459412820637226, 0.0269357617944479, -0.02722334675490856, -0.018120208755135536, -0.0011771325953304768, -0.039424315094947815, -0.025404613465070724, 0.034904368221759796, 0.0313887856900692, 0.0015556744765490294]
[2025-05-04 07:36:32,311]: Mean: -0.0012
[2025-05-04 07:36:32,311]: Std: 0.0275
[2025-05-04 07:36:32,312]: Min: -0.0971
[2025-05-04 07:36:32,312]: Max: 0.1041
[2025-05-04 07:36:32,312]: 
Parameter: layer3.1.bn1.weight
[2025-05-04 07:36:32,312]: Shape: torch.Size([64])
[2025-05-04 07:36:32,313]: Sample Values (16 elements): [0.9727234244346619, 0.9615809321403503, 0.9908636808395386, 0.9676874876022339, 0.9669528007507324, 0.9706652760505676, 0.9849026203155518, 0.9741665720939636, 0.9549924731254578, 0.9848096966743469, 0.9730730652809143, 0.979059636592865, 0.9776507616043091, 0.9597983956336975, 0.976870596408844, 0.9726137518882751]
[2025-05-04 07:36:32,315]: Mean: 0.9711
[2025-05-04 07:36:32,316]: Std: 0.0103
[2025-05-04 07:36:32,316]: Min: 0.9463
[2025-05-04 07:36:32,316]: Max: 0.9909
[2025-05-04 07:36:32,316]: 
Parameter: layer3.1.bn1.bias
[2025-05-04 07:36:32,317]: Shape: torch.Size([64])
[2025-05-04 07:36:32,319]: Sample Values (16 elements): [-0.035082511603832245, -0.08415868133306503, -0.01786705106496811, -0.05115988105535507, -0.024872437119483948, -0.032182008028030396, -0.03370166942477226, -0.0464041642844677, -0.06682509928941727, -0.02019641175866127, -0.04354557394981384, -0.0027809031307697296, -0.013846081681549549, 0.0056326137855648994, -0.01606137678027153, -0.036707762628793716]
[2025-05-04 07:36:32,325]: Mean: -0.0338
[2025-05-04 07:36:32,328]: Std: 0.0181
[2025-05-04 07:36:32,329]: Min: -0.0859
[2025-05-04 07:36:32,330]: Max: 0.0056
[2025-05-04 07:36:32,331]: 
Parameter: layer3.1.conv2.weight
[2025-05-04 07:36:32,331]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:36:32,352]: Sample Values (16 elements): [-0.024637574329972267, 0.01749861054122448, -0.030268043279647827, 0.02314792014658451, 0.010462700389325619, -0.03257754445075989, 0.01915508508682251, -0.008910572156310081, 0.05305226519703865, 0.014122080989181995, 0.026654044166207314, 0.0053537869825959206, -0.028374113142490387, 0.019234733656048775, -0.04366873949766159, 0.01178270298987627]
[2025-05-04 07:36:32,361]: Mean: 0.0001
[2025-05-04 07:36:32,365]: Std: 0.0250
[2025-05-04 07:36:32,366]: Min: -0.0815
[2025-05-04 07:36:32,366]: Max: 0.0883
[2025-05-04 07:36:32,366]: 
Parameter: layer3.1.bn2.weight
[2025-05-04 07:36:32,366]: Shape: torch.Size([64])
[2025-05-04 07:36:32,367]: Sample Values (16 elements): [0.9971174001693726, 1.029542326927185, 1.0434520244598389, 1.023464560508728, 1.0166529417037964, 1.0171289443969727, 1.0203100442886353, 1.048722743988037, 1.043283224105835, 1.035178542137146, 1.0181554555892944, 1.0528109073638916, 1.0159943103790283, 1.0214048624038696, 1.019989252090454, 1.0130136013031006]
[2025-05-04 07:36:32,368]: Mean: 1.0221
[2025-05-04 07:36:32,368]: Std: 0.0143
[2025-05-04 07:36:32,369]: Min: 0.9864
[2025-05-04 07:36:32,370]: Max: 1.0528
[2025-05-04 07:36:32,370]: 
Parameter: layer3.1.bn2.bias
[2025-05-04 07:36:32,370]: Shape: torch.Size([64])
[2025-05-04 07:36:32,370]: Sample Values (16 elements): [0.0017515331273898482, 0.014367123134434223, 0.01416044495999813, 0.00041872376459650695, -0.019559890031814575, 0.0381789356470108, 0.008620752952992916, 0.00870738085359335, 0.016861459240317345, 0.04451978951692581, 0.022457005456089973, 0.015101099386811256, 0.038327399641275406, 0.008885025978088379, 0.01976289041340351, 0.03574460372328758]
[2025-05-04 07:36:32,371]: Mean: 0.0114
[2025-05-04 07:36:32,375]: Std: 0.0144
[2025-05-04 07:36:32,379]: Min: -0.0250
[2025-05-04 07:36:32,382]: Max: 0.0445
[2025-05-04 07:36:32,382]: 
Parameter: layer3.2.conv1.weight
[2025-05-04 07:36:32,382]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:36:32,398]: Sample Values (16 elements): [0.0259622223675251, -0.006631326396018267, 0.024315455928444862, -0.023021161556243896, -0.04241180792450905, -0.016031475737690926, 0.005305778700858355, -0.011637760326266289, 0.039060939103364944, -0.024716734886169434, 0.008045435883104801, -0.0230769794434309, -0.004154388792812824, -0.03888344764709473, -0.03742314875125885, 0.009138697758316994]
[2025-05-04 07:36:32,410]: Mean: -0.0001
[2025-05-04 07:36:32,423]: Std: 0.0245
[2025-05-04 07:36:32,423]: Min: -0.0764
[2025-05-04 07:36:32,423]: Max: 0.0775
[2025-05-04 07:36:32,423]: 
Parameter: layer3.2.bn1.weight
[2025-05-04 07:36:32,424]: Shape: torch.Size([64])
[2025-05-04 07:36:32,424]: Sample Values (16 elements): [0.9692627787590027, 0.9553506374359131, 0.975820779800415, 0.9674690365791321, 0.9763919711112976, 0.969741940498352, 0.9599056243896484, 0.9614831209182739, 0.9771625399589539, 0.973703145980835, 0.9848549962043762, 0.9709304571151733, 0.9628065824508667, 0.977160632610321, 0.9662039279937744, 0.9652979969978333]
[2025-05-04 07:36:32,425]: Mean: 0.9717
[2025-05-04 07:36:32,425]: Std: 0.0100
[2025-05-04 07:36:32,426]: Min: 0.9554
[2025-05-04 07:36:32,426]: Max: 0.9974
[2025-05-04 07:36:32,426]: 
Parameter: layer3.2.bn1.bias
[2025-05-04 07:36:32,426]: Shape: torch.Size([64])
[2025-05-04 07:36:32,427]: Sample Values (16 elements): [-0.01104081142693758, -0.017948295921087265, -0.009667767211794853, -0.022040974348783493, -0.018164081498980522, -0.007696142420172691, -0.021215887740254402, -0.005833833012729883, -0.004209969658404589, -0.008968843147158623, -0.008582006208598614, -0.0035772770643234253, -0.02543070912361145, -0.018482817336916924, -0.011811316013336182, -0.01732839271426201]
[2025-05-04 07:36:32,427]: Mean: -0.0146
[2025-05-04 07:36:32,428]: Std: 0.0091
[2025-05-04 07:36:32,428]: Min: -0.0364
[2025-05-04 07:36:32,431]: Max: 0.0018
[2025-05-04 07:36:32,431]: 
Parameter: layer3.2.conv2.weight
[2025-05-04 07:36:32,431]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:36:32,446]: Sample Values (16 elements): [-0.0166311152279377, -0.037825554609298706, 0.02775879018008709, -0.01608823984861374, 0.018300706520676613, -0.01793919876217842, 0.01585172303020954, -0.03444169461727142, -0.025811849161982536, 0.02313428372144699, 0.008493456058204174, 0.016257164999842644, 0.017762063071131706, 0.016329945996403694, 0.016156218945980072, 0.0008308900869451463]
[2025-05-04 07:36:32,451]: Mean: 0.0005
[2025-05-04 07:36:32,457]: Std: 0.0238
[2025-05-04 07:36:32,465]: Min: -0.0615
[2025-05-04 07:36:32,475]: Max: 0.0693
[2025-05-04 07:36:32,475]: 
Parameter: layer3.2.bn2.weight
[2025-05-04 07:36:32,476]: Shape: torch.Size([64])
[2025-05-04 07:36:32,480]: Sample Values (16 elements): [1.0679901838302612, 1.0449743270874023, 1.0585495233535767, 1.0572372674942017, 1.0847580432891846, 1.046667456626892, 1.0668950080871582, 1.0730868577957153, 1.0563398599624634, 1.0391772985458374, 1.075255274772644, 1.0770721435546875, 1.0706998109817505, 1.0793341398239136, 1.0889889001846313, 1.051170825958252]
[2025-05-04 07:36:32,480]: Mean: 1.0583
[2025-05-04 07:36:32,486]: Std: 0.0176
[2025-05-04 07:36:32,489]: Min: 1.0128
[2025-05-04 07:36:32,500]: Max: 1.0950
[2025-05-04 07:36:32,500]: 
Parameter: layer3.2.bn2.bias
[2025-05-04 07:36:32,500]: Shape: torch.Size([64])
[2025-05-04 07:36:32,504]: Sample Values (16 elements): [0.016931382939219475, 0.042767152190208435, 0.031355708837509155, 0.025374986231327057, 0.048553019762039185, 0.010874589905142784, 0.03437011316418648, 0.029400868341326714, 0.07356318086385727, 0.01874231919646263, 0.017018666490912437, 0.03976883739233017, 0.01931779459118843, 0.02584749460220337, 0.036816827952861786, 0.027552269399166107]
[2025-05-04 07:36:32,505]: Mean: 0.0298
[2025-05-04 07:36:32,507]: Std: 0.0123
[2025-05-04 07:36:32,508]: Min: 0.0096
[2025-05-04 07:36:32,516]: Max: 0.0736
[2025-05-04 07:36:32,516]: 
Parameter: fc.weight
[2025-05-04 07:36:32,516]: Shape: torch.Size([10, 64])
[2025-05-04 07:36:32,519]: Sample Values (16 elements): [-0.1395401805639267, -0.10952488332986832, 0.26280269026756287, 0.1570589542388916, 0.4714982807636261, -0.1853076070547104, -0.12817850708961487, -0.22205503284931183, -0.07233834266662598, -0.3156934678554535, 0.27095165848731995, 0.41508203744888306, 0.048062440007925034, 0.221721813082695, -0.19757778942584991, 0.11609449982643127]
[2025-05-04 07:36:32,523]: Mean: -0.0024
[2025-05-04 07:36:32,535]: Std: 0.2021
[2025-05-04 07:36:32,541]: Min: -0.3379
[2025-05-04 07:36:32,556]: Max: 0.5307
[2025-05-04 07:36:32,556]: 
Parameter: fc.bias
[2025-05-04 07:36:32,556]: Shape: torch.Size([10])
[2025-05-04 07:36:32,568]: Sample Values (10 elements): [0.011541330255568027, -0.021985994651913643, 0.1241011694073677, -0.03723112866282463, 0.10221020877361298, 0.11287830770015717, -0.007327045779675245, 0.06690388917922974, 0.05590612441301346, 0.053901635110378265]
[2025-05-04 07:36:32,572]: Mean: 0.0461
[2025-05-04 07:36:32,573]: Std: 0.0576
[2025-05-04 07:36:32,574]: Min: -0.0372
[2025-05-04 07:36:32,574]: Max: 0.1241
[2025-05-04 07:36:32,574]: 


QAT of ResNet20 with relu6 down to 4 bits...
[2025-05-04 07:36:33,797]: [ResNet20_relu6_quantized_4_bits] after configure_qat:
[2025-05-04 07:36:34,372]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-04 07:39:27,694]: [ResNet20_relu6_quantized_4_bits] Epoch: 001 Train Loss: 0.3992 Train Acc: 0.8603 Eval Loss: 0.5408 Eval Acc: 0.8294 (LR: 0.001000)
[2025-05-04 07:41:36,634]: [ResNet20_relu6_quantized_4_bits] Epoch: 002 Train Loss: 0.3841 Train Acc: 0.8656 Eval Loss: 0.5944 Eval Acc: 0.8148 (LR: 0.001000)
[2025-05-04 07:43:57,129]: [ResNet20_relu6_quantized_4_bits] Epoch: 003 Train Loss: 0.3873 Train Acc: 0.8639 Eval Loss: 0.4905 Eval Acc: 0.8400 (LR: 0.001000)
[2025-05-04 07:46:03,800]: [ResNet20_relu6_quantized_4_bits] Epoch: 004 Train Loss: 0.3842 Train Acc: 0.8637 Eval Loss: 0.5086 Eval Acc: 0.8350 (LR: 0.001000)
[2025-05-04 07:48:09,858]: [ResNet20_relu6_quantized_4_bits] Epoch: 005 Train Loss: 0.3810 Train Acc: 0.8662 Eval Loss: 0.4904 Eval Acc: 0.8420 (LR: 0.001000)
[2025-05-04 07:50:18,079]: [ResNet20_relu6_quantized_4_bits] Epoch: 006 Train Loss: 0.3750 Train Acc: 0.8692 Eval Loss: 0.4995 Eval Acc: 0.8352 (LR: 0.001000)
[2025-05-04 07:52:28,456]: [ResNet20_relu6_quantized_4_bits] Epoch: 007 Train Loss: 0.3750 Train Acc: 0.8681 Eval Loss: 0.4795 Eval Acc: 0.8408 (LR: 0.001000)
[2025-05-04 07:54:40,319]: [ResNet20_relu6_quantized_4_bits] Epoch: 008 Train Loss: 0.3749 Train Acc: 0.8702 Eval Loss: 0.6419 Eval Acc: 0.8009 (LR: 0.001000)
[2025-05-04 07:56:52,389]: [ResNet20_relu6_quantized_4_bits] Epoch: 009 Train Loss: 0.3710 Train Acc: 0.8704 Eval Loss: 0.5176 Eval Acc: 0.8329 (LR: 0.001000)
[2025-05-04 07:59:04,158]: [ResNet20_relu6_quantized_4_bits] Epoch: 010 Train Loss: 0.3710 Train Acc: 0.8696 Eval Loss: 0.4681 Eval Acc: 0.8454 (LR: 0.001000)
[2025-05-04 08:01:14,659]: [ResNet20_relu6_quantized_4_bits] Epoch: 011 Train Loss: 0.3665 Train Acc: 0.8718 Eval Loss: 0.4800 Eval Acc: 0.8453 (LR: 0.001000)
[2025-05-04 08:03:26,836]: [ResNet20_relu6_quantized_4_bits] Epoch: 012 Train Loss: 0.3591 Train Acc: 0.8733 Eval Loss: 0.4839 Eval Acc: 0.8424 (LR: 0.001000)
[2025-05-04 08:05:38,667]: [ResNet20_relu6_quantized_4_bits] Epoch: 013 Train Loss: 0.3664 Train Acc: 0.8723 Eval Loss: 0.4874 Eval Acc: 0.8407 (LR: 0.001000)
[2025-05-04 08:07:48,325]: [ResNet20_relu6_quantized_4_bits] Epoch: 014 Train Loss: 0.3561 Train Acc: 0.8747 Eval Loss: 0.4726 Eval Acc: 0.8470 (LR: 0.001000)
[2025-05-04 08:09:51,749]: [ResNet20_relu6_quantized_4_bits] Epoch: 015 Train Loss: 0.3557 Train Acc: 0.8741 Eval Loss: 0.5199 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-04 08:15:49,280]: [ResNet20_relu6_quantized_4_bits] Epoch: 016 Train Loss: 0.3573 Train Acc: 0.8752 Eval Loss: 0.4974 Eval Acc: 0.8404 (LR: 0.001000)
[2025-05-04 08:23:11,480]: [ResNet20_relu6_quantized_4_bits] Epoch: 017 Train Loss: 0.3515 Train Acc: 0.8765 Eval Loss: 0.5498 Eval Acc: 0.8234 (LR: 0.001000)
