[2025-05-04 05:28:33,736]: 
Training LeNet5 with relu6
[2025-05-04 05:28:53,476]: [LeNet5_relu6] Epoch: 001 Train Loss: 2.2970 Train Acc: 0.1162 Eval Loss: 2.2798 Eval Acc: 0.1604 (LR: 0.001000)
[2025-05-04 05:29:11,208]: [LeNet5_relu6] Epoch: 002 Train Loss: 2.2089 Train Acc: 0.1757 Eval Loss: 2.0727 Eval Acc: 0.2556 (LR: 0.001000)
[2025-05-04 05:29:29,856]: [LeNet5_relu6] Epoch: 003 Train Loss: 2.0157 Train Acc: 0.2629 Eval Loss: 1.9402 Eval Acc: 0.2976 (LR: 0.001000)
[2025-05-04 05:29:46,753]: [LeNet5_relu6] Epoch: 004 Train Loss: 1.9190 Train Acc: 0.2968 Eval Loss: 1.8211 Eval Acc: 0.3408 (LR: 0.001000)
[2025-05-04 05:30:05,594]: [LeNet5_relu6] Epoch: 005 Train Loss: 1.8218 Train Acc: 0.3306 Eval Loss: 1.7174 Eval Acc: 0.3712 (LR: 0.001000)
[2025-05-04 05:30:23,124]: [LeNet5_relu6] Epoch: 006 Train Loss: 1.7497 Train Acc: 0.3515 Eval Loss: 1.6593 Eval Acc: 0.3921 (LR: 0.001000)
[2025-05-04 05:30:40,806]: [LeNet5_relu6] Epoch: 007 Train Loss: 1.6941 Train Acc: 0.3731 Eval Loss: 1.5970 Eval Acc: 0.4165 (LR: 0.001000)
[2025-05-04 05:30:59,049]: [LeNet5_relu6] Epoch: 008 Train Loss: 1.6508 Train Acc: 0.3888 Eval Loss: 1.5376 Eval Acc: 0.4377 (LR: 0.001000)
[2025-05-04 05:31:16,831]: [LeNet5_relu6] Epoch: 009 Train Loss: 1.6222 Train Acc: 0.4016 Eval Loss: 1.5171 Eval Acc: 0.4453 (LR: 0.001000)
[2025-05-04 05:31:34,962]: [LeNet5_relu6] Epoch: 010 Train Loss: 1.5968 Train Acc: 0.4108 Eval Loss: 1.4870 Eval Acc: 0.4562 (LR: 0.001000)
[2025-05-04 05:31:59,341]: [LeNet5_relu6] Epoch: 011 Train Loss: 1.5802 Train Acc: 0.4200 Eval Loss: 1.4670 Eval Acc: 0.4631 (LR: 0.001000)
[2025-05-04 05:32:20,078]: [LeNet5_relu6] Epoch: 012 Train Loss: 1.5574 Train Acc: 0.4267 Eval Loss: 1.4672 Eval Acc: 0.4597 (LR: 0.001000)
[2025-05-04 05:32:38,602]: [LeNet5_relu6] Epoch: 013 Train Loss: 1.5433 Train Acc: 0.4326 Eval Loss: 1.4312 Eval Acc: 0.4768 (LR: 0.001000)
[2025-05-04 05:32:57,079]: [LeNet5_relu6] Epoch: 014 Train Loss: 1.5208 Train Acc: 0.4411 Eval Loss: 1.4071 Eval Acc: 0.4893 (LR: 0.001000)
[2025-05-04 05:33:15,320]: [LeNet5_relu6] Epoch: 015 Train Loss: 1.5003 Train Acc: 0.4525 Eval Loss: 1.3834 Eval Acc: 0.4976 (LR: 0.001000)
[2025-05-04 05:33:34,229]: [LeNet5_relu6] Epoch: 016 Train Loss: 1.4782 Train Acc: 0.4574 Eval Loss: 1.3789 Eval Acc: 0.4998 (LR: 0.001000)
[2025-05-04 05:33:52,423]: [LeNet5_relu6] Epoch: 017 Train Loss: 1.4674 Train Acc: 0.4645 Eval Loss: 1.3769 Eval Acc: 0.5002 (LR: 0.001000)
[2025-05-04 05:34:10,564]: [LeNet5_relu6] Epoch: 018 Train Loss: 1.4477 Train Acc: 0.4729 Eval Loss: 1.3408 Eval Acc: 0.5186 (LR: 0.001000)
[2025-05-04 05:34:28,559]: [LeNet5_relu6] Epoch: 019 Train Loss: 1.4321 Train Acc: 0.4788 Eval Loss: 1.3352 Eval Acc: 0.5215 (LR: 0.001000)
[2025-05-04 05:34:46,912]: [LeNet5_relu6] Epoch: 020 Train Loss: 1.4117 Train Acc: 0.4874 Eval Loss: 1.3207 Eval Acc: 0.5227 (LR: 0.001000)
[2025-05-04 05:35:05,200]: [LeNet5_relu6] Epoch: 021 Train Loss: 1.3973 Train Acc: 0.4946 Eval Loss: 1.2794 Eval Acc: 0.5434 (LR: 0.001000)
[2025-05-04 05:35:23,565]: [LeNet5_relu6] Epoch: 022 Train Loss: 1.3863 Train Acc: 0.4987 Eval Loss: 1.2749 Eval Acc: 0.5360 (LR: 0.001000)
[2025-05-04 05:35:42,253]: [LeNet5_relu6] Epoch: 023 Train Loss: 1.3704 Train Acc: 0.5047 Eval Loss: 1.2790 Eval Acc: 0.5380 (LR: 0.001000)
[2025-05-04 05:36:00,617]: [LeNet5_relu6] Epoch: 024 Train Loss: 1.3588 Train Acc: 0.5079 Eval Loss: 1.2543 Eval Acc: 0.5464 (LR: 0.001000)
[2025-05-04 05:36:18,228]: [LeNet5_relu6] Epoch: 025 Train Loss: 1.3450 Train Acc: 0.5159 Eval Loss: 1.2536 Eval Acc: 0.5445 (LR: 0.001000)
[2025-05-04 05:36:37,007]: [LeNet5_relu6] Epoch: 026 Train Loss: 1.3310 Train Acc: 0.5208 Eval Loss: 1.2206 Eval Acc: 0.5628 (LR: 0.001000)
[2025-05-04 05:36:55,144]: [LeNet5_relu6] Epoch: 027 Train Loss: 1.3233 Train Acc: 0.5241 Eval Loss: 1.2128 Eval Acc: 0.5649 (LR: 0.001000)
[2025-05-04 05:37:14,610]: [LeNet5_relu6] Epoch: 028 Train Loss: 1.3096 Train Acc: 0.5313 Eval Loss: 1.1961 Eval Acc: 0.5711 (LR: 0.001000)
[2025-05-04 05:37:33,959]: [LeNet5_relu6] Epoch: 029 Train Loss: 1.3058 Train Acc: 0.5317 Eval Loss: 1.1885 Eval Acc: 0.5762 (LR: 0.001000)
[2025-05-04 05:37:52,642]: [LeNet5_relu6] Epoch: 030 Train Loss: 1.2847 Train Acc: 0.5385 Eval Loss: 1.1914 Eval Acc: 0.5752 (LR: 0.001000)
[2025-05-04 05:38:10,460]: [LeNet5_relu6] Epoch: 031 Train Loss: 1.2784 Train Acc: 0.5411 Eval Loss: 1.1666 Eval Acc: 0.5868 (LR: 0.001000)
[2025-05-04 05:38:28,124]: [LeNet5_relu6] Epoch: 032 Train Loss: 1.2724 Train Acc: 0.5432 Eval Loss: 1.1662 Eval Acc: 0.5843 (LR: 0.001000)
[2025-05-04 05:38:45,597]: [LeNet5_relu6] Epoch: 033 Train Loss: 1.2585 Train Acc: 0.5510 Eval Loss: 1.1697 Eval Acc: 0.5894 (LR: 0.001000)
[2025-05-04 05:39:04,209]: [LeNet5_relu6] Epoch: 034 Train Loss: 1.2548 Train Acc: 0.5518 Eval Loss: 1.1722 Eval Acc: 0.5847 (LR: 0.001000)
[2025-05-04 05:39:21,989]: [LeNet5_relu6] Epoch: 035 Train Loss: 1.2456 Train Acc: 0.5549 Eval Loss: 1.1475 Eval Acc: 0.5945 (LR: 0.001000)
[2025-05-04 05:39:40,736]: [LeNet5_relu6] Epoch: 036 Train Loss: 1.2364 Train Acc: 0.5587 Eval Loss: 1.1266 Eval Acc: 0.6014 (LR: 0.001000)
[2025-05-04 05:39:59,028]: [LeNet5_relu6] Epoch: 037 Train Loss: 1.2294 Train Acc: 0.5635 Eval Loss: 1.1265 Eval Acc: 0.6039 (LR: 0.001000)
[2025-05-04 05:40:17,547]: [LeNet5_relu6] Epoch: 038 Train Loss: 1.2205 Train Acc: 0.5656 Eval Loss: 1.1160 Eval Acc: 0.6086 (LR: 0.001000)
[2025-05-04 05:40:35,844]: [LeNet5_relu6] Epoch: 039 Train Loss: 1.2109 Train Acc: 0.5662 Eval Loss: 1.1095 Eval Acc: 0.6101 (LR: 0.001000)
[2025-05-04 05:40:54,711]: [LeNet5_relu6] Epoch: 040 Train Loss: 1.2040 Train Acc: 0.5717 Eval Loss: 1.1102 Eval Acc: 0.6139 (LR: 0.001000)
[2025-05-04 05:41:13,440]: [LeNet5_relu6] Epoch: 041 Train Loss: 1.1980 Train Acc: 0.5723 Eval Loss: 1.1484 Eval Acc: 0.5937 (LR: 0.001000)
[2025-05-04 05:41:31,783]: [LeNet5_relu6] Epoch: 042 Train Loss: 1.1880 Train Acc: 0.5778 Eval Loss: 1.0843 Eval Acc: 0.6198 (LR: 0.001000)
[2025-05-04 05:41:50,008]: [LeNet5_relu6] Epoch: 043 Train Loss: 1.1880 Train Acc: 0.5765 Eval Loss: 1.0927 Eval Acc: 0.6168 (LR: 0.001000)
[2025-05-04 05:42:08,274]: [LeNet5_relu6] Epoch: 044 Train Loss: 1.1762 Train Acc: 0.5817 Eval Loss: 1.1230 Eval Acc: 0.6001 (LR: 0.001000)
[2025-05-04 05:42:26,837]: [LeNet5_relu6] Epoch: 045 Train Loss: 1.1765 Train Acc: 0.5840 Eval Loss: 1.0833 Eval Acc: 0.6195 (LR: 0.001000)
[2025-05-04 05:42:46,310]: [LeNet5_relu6] Epoch: 046 Train Loss: 1.1714 Train Acc: 0.5803 Eval Loss: 1.0942 Eval Acc: 0.6111 (LR: 0.001000)
[2025-05-04 05:43:05,174]: [LeNet5_relu6] Epoch: 047 Train Loss: 1.1649 Train Acc: 0.5856 Eval Loss: 1.0607 Eval Acc: 0.6274 (LR: 0.001000)
[2025-05-04 05:43:24,271]: [LeNet5_relu6] Epoch: 048 Train Loss: 1.1584 Train Acc: 0.5900 Eval Loss: 1.0838 Eval Acc: 0.6222 (LR: 0.001000)
[2025-05-04 05:43:43,499]: [LeNet5_relu6] Epoch: 049 Train Loss: 1.1502 Train Acc: 0.5902 Eval Loss: 1.0618 Eval Acc: 0.6296 (LR: 0.001000)
[2025-05-04 05:44:01,537]: [LeNet5_relu6] Epoch: 050 Train Loss: 1.1451 Train Acc: 0.5924 Eval Loss: 1.0463 Eval Acc: 0.6324 (LR: 0.001000)
[2025-05-04 05:44:19,927]: [LeNet5_relu6] Epoch: 051 Train Loss: 1.1408 Train Acc: 0.5925 Eval Loss: 1.0389 Eval Acc: 0.6364 (LR: 0.001000)
[2025-05-04 05:44:38,644]: [LeNet5_relu6] Epoch: 052 Train Loss: 1.1383 Train Acc: 0.5955 Eval Loss: 1.0439 Eval Acc: 0.6359 (LR: 0.001000)
[2025-05-04 05:44:58,547]: [LeNet5_relu6] Epoch: 053 Train Loss: 1.1288 Train Acc: 0.5998 Eval Loss: 1.0517 Eval Acc: 0.6317 (LR: 0.001000)
[2025-05-04 05:45:17,583]: [LeNet5_relu6] Epoch: 054 Train Loss: 1.1257 Train Acc: 0.6010 Eval Loss: 1.0326 Eval Acc: 0.6392 (LR: 0.001000)
[2025-05-04 05:45:35,706]: [LeNet5_relu6] Epoch: 055 Train Loss: 1.1242 Train Acc: 0.6003 Eval Loss: 1.0429 Eval Acc: 0.6368 (LR: 0.001000)
[2025-05-04 05:45:53,715]: [LeNet5_relu6] Epoch: 056 Train Loss: 1.1215 Train Acc: 0.6009 Eval Loss: 1.0360 Eval Acc: 0.6355 (LR: 0.001000)
[2025-05-04 05:46:12,893]: [LeNet5_relu6] Epoch: 057 Train Loss: 1.1180 Train Acc: 0.6040 Eval Loss: 1.0155 Eval Acc: 0.6437 (LR: 0.001000)
[2025-05-04 05:46:31,731]: [LeNet5_relu6] Epoch: 058 Train Loss: 1.1121 Train Acc: 0.6069 Eval Loss: 1.0452 Eval Acc: 0.6307 (LR: 0.001000)
[2025-05-04 05:46:50,079]: [LeNet5_relu6] Epoch: 059 Train Loss: 1.1053 Train Acc: 0.6075 Eval Loss: 1.0355 Eval Acc: 0.6386 (LR: 0.001000)
[2025-05-04 05:47:08,538]: [LeNet5_relu6] Epoch: 060 Train Loss: 1.0983 Train Acc: 0.6103 Eval Loss: 1.0121 Eval Acc: 0.6443 (LR: 0.001000)
[2025-05-04 05:47:27,153]: [LeNet5_relu6] Epoch: 061 Train Loss: 1.0952 Train Acc: 0.6135 Eval Loss: 0.9997 Eval Acc: 0.6463 (LR: 0.001000)
[2025-05-04 05:47:45,069]: [LeNet5_relu6] Epoch: 062 Train Loss: 1.0959 Train Acc: 0.6127 Eval Loss: 1.0106 Eval Acc: 0.6483 (LR: 0.001000)
[2025-05-04 05:48:03,023]: [LeNet5_relu6] Epoch: 063 Train Loss: 1.0872 Train Acc: 0.6144 Eval Loss: 1.0170 Eval Acc: 0.6402 (LR: 0.001000)
[2025-05-04 05:48:21,142]: [LeNet5_relu6] Epoch: 064 Train Loss: 1.0833 Train Acc: 0.6166 Eval Loss: 1.0015 Eval Acc: 0.6503 (LR: 0.001000)
[2025-05-04 05:48:39,095]: [LeNet5_relu6] Epoch: 065 Train Loss: 1.0833 Train Acc: 0.6170 Eval Loss: 0.9858 Eval Acc: 0.6540 (LR: 0.001000)
[2025-05-04 05:48:56,638]: [LeNet5_relu6] Epoch: 066 Train Loss: 1.0766 Train Acc: 0.6167 Eval Loss: 1.0133 Eval Acc: 0.6463 (LR: 0.001000)
[2025-05-04 05:49:14,679]: [LeNet5_relu6] Epoch: 067 Train Loss: 1.0778 Train Acc: 0.6175 Eval Loss: 0.9827 Eval Acc: 0.6520 (LR: 0.001000)
[2025-05-04 05:49:33,037]: [LeNet5_relu6] Epoch: 068 Train Loss: 1.0738 Train Acc: 0.6191 Eval Loss: 0.9808 Eval Acc: 0.6515 (LR: 0.001000)
[2025-05-04 05:49:50,462]: [LeNet5_relu6] Epoch: 069 Train Loss: 1.0681 Train Acc: 0.6203 Eval Loss: 0.9953 Eval Acc: 0.6470 (LR: 0.001000)
[2025-05-04 05:50:09,804]: [LeNet5_relu6] Epoch: 070 Train Loss: 1.0712 Train Acc: 0.6192 Eval Loss: 0.9803 Eval Acc: 0.6533 (LR: 0.000100)
[2025-05-04 05:50:27,788]: [LeNet5_relu6] Epoch: 071 Train Loss: 1.0376 Train Acc: 0.6338 Eval Loss: 0.9629 Eval Acc: 0.6600 (LR: 0.000100)
[2025-05-04 05:50:46,243]: [LeNet5_relu6] Epoch: 072 Train Loss: 1.0266 Train Acc: 0.6363 Eval Loss: 0.9609 Eval Acc: 0.6609 (LR: 0.000100)
[2025-05-04 05:51:05,015]: [LeNet5_relu6] Epoch: 073 Train Loss: 1.0308 Train Acc: 0.6335 Eval Loss: 0.9570 Eval Acc: 0.6623 (LR: 0.000100)
[2025-05-04 05:51:24,186]: [LeNet5_relu6] Epoch: 074 Train Loss: 1.0293 Train Acc: 0.6352 Eval Loss: 0.9592 Eval Acc: 0.6652 (LR: 0.000100)
[2025-05-04 05:51:44,568]: [LeNet5_relu6] Epoch: 075 Train Loss: 1.0265 Train Acc: 0.6375 Eval Loss: 0.9554 Eval Acc: 0.6648 (LR: 0.000100)
[2025-05-04 05:52:03,651]: [LeNet5_relu6] Epoch: 076 Train Loss: 1.0271 Train Acc: 0.6391 Eval Loss: 0.9568 Eval Acc: 0.6620 (LR: 0.000100)
[2025-05-04 05:52:23,179]: [LeNet5_relu6] Epoch: 077 Train Loss: 1.0274 Train Acc: 0.6361 Eval Loss: 0.9565 Eval Acc: 0.6644 (LR: 0.000100)
[2025-05-04 05:52:42,214]: [LeNet5_relu6] Epoch: 078 Train Loss: 1.0267 Train Acc: 0.6383 Eval Loss: 0.9607 Eval Acc: 0.6645 (LR: 0.000100)
[2025-05-04 05:53:02,381]: [LeNet5_relu6] Epoch: 079 Train Loss: 1.0282 Train Acc: 0.6365 Eval Loss: 0.9518 Eval Acc: 0.6671 (LR: 0.000100)
[2025-05-04 05:53:20,923]: [LeNet5_relu6] Epoch: 080 Train Loss: 1.0263 Train Acc: 0.6375 Eval Loss: 0.9525 Eval Acc: 0.6644 (LR: 0.000100)
[2025-05-04 05:53:40,152]: [LeNet5_relu6] Epoch: 081 Train Loss: 1.0262 Train Acc: 0.6368 Eval Loss: 0.9525 Eval Acc: 0.6650 (LR: 0.000100)
[2025-05-04 05:53:58,942]: [LeNet5_relu6] Epoch: 082 Train Loss: 1.0271 Train Acc: 0.6393 Eval Loss: 0.9636 Eval Acc: 0.6606 (LR: 0.000100)
[2025-05-04 05:54:21,597]: [LeNet5_relu6] Epoch: 083 Train Loss: 1.0236 Train Acc: 0.6390 Eval Loss: 0.9525 Eval Acc: 0.6637 (LR: 0.000100)
[2025-05-04 05:54:41,788]: [LeNet5_relu6] Epoch: 084 Train Loss: 1.0222 Train Acc: 0.6382 Eval Loss: 0.9526 Eval Acc: 0.6645 (LR: 0.000100)
[2025-05-04 05:55:02,085]: [LeNet5_relu6] Epoch: 085 Train Loss: 1.0244 Train Acc: 0.6377 Eval Loss: 0.9532 Eval Acc: 0.6672 (LR: 0.000100)
[2025-05-04 05:55:22,323]: [LeNet5_relu6] Epoch: 086 Train Loss: 1.0206 Train Acc: 0.6409 Eval Loss: 0.9493 Eval Acc: 0.6653 (LR: 0.000100)
[2025-05-04 05:55:41,613]: [LeNet5_relu6] Epoch: 087 Train Loss: 1.0206 Train Acc: 0.6403 Eval Loss: 0.9524 Eval Acc: 0.6633 (LR: 0.000100)
[2025-05-04 05:56:00,632]: [LeNet5_relu6] Epoch: 088 Train Loss: 1.0178 Train Acc: 0.6412 Eval Loss: 0.9500 Eval Acc: 0.6661 (LR: 0.000100)
[2025-05-04 05:56:19,542]: [LeNet5_relu6] Epoch: 089 Train Loss: 1.0190 Train Acc: 0.6400 Eval Loss: 0.9491 Eval Acc: 0.6673 (LR: 0.000100)
[2025-05-04 05:56:38,693]: [LeNet5_relu6] Epoch: 090 Train Loss: 1.0236 Train Acc: 0.6395 Eval Loss: 0.9487 Eval Acc: 0.6647 (LR: 0.000100)
[2025-05-04 05:56:57,613]: [LeNet5_relu6] Epoch: 091 Train Loss: 1.0216 Train Acc: 0.6413 Eval Loss: 0.9464 Eval Acc: 0.6663 (LR: 0.000100)
[2025-05-04 05:57:16,673]: [LeNet5_relu6] Epoch: 092 Train Loss: 1.0207 Train Acc: 0.6404 Eval Loss: 0.9526 Eval Acc: 0.6643 (LR: 0.000100)
[2025-05-04 05:57:35,880]: [LeNet5_relu6] Epoch: 093 Train Loss: 1.0164 Train Acc: 0.6414 Eval Loss: 0.9507 Eval Acc: 0.6655 (LR: 0.000100)
[2025-05-04 05:57:55,466]: [LeNet5_relu6] Epoch: 094 Train Loss: 1.0206 Train Acc: 0.6399 Eval Loss: 0.9440 Eval Acc: 0.6702 (LR: 0.000100)
[2025-05-04 05:58:15,956]: [LeNet5_relu6] Epoch: 095 Train Loss: 1.0174 Train Acc: 0.6399 Eval Loss: 0.9507 Eval Acc: 0.6642 (LR: 0.000100)
[2025-05-04 05:58:36,264]: [LeNet5_relu6] Epoch: 096 Train Loss: 1.0175 Train Acc: 0.6394 Eval Loss: 0.9480 Eval Acc: 0.6646 (LR: 0.000100)
[2025-05-04 05:58:56,686]: [LeNet5_relu6] Epoch: 097 Train Loss: 1.0169 Train Acc: 0.6399 Eval Loss: 0.9478 Eval Acc: 0.6668 (LR: 0.000100)
[2025-05-04 05:59:16,636]: [LeNet5_relu6] Epoch: 098 Train Loss: 1.0193 Train Acc: 0.6409 Eval Loss: 0.9462 Eval Acc: 0.6653 (LR: 0.000100)
[2025-05-04 05:59:37,030]: [LeNet5_relu6] Epoch: 099 Train Loss: 1.0208 Train Acc: 0.6380 Eval Loss: 0.9466 Eval Acc: 0.6674 (LR: 0.000100)
[2025-05-04 05:59:56,818]: [LeNet5_relu6] Epoch: 100 Train Loss: 1.0169 Train Acc: 0.6410 Eval Loss: 0.9445 Eval Acc: 0.6663 (LR: 0.000010)
[2025-05-04 06:00:16,148]: [LeNet5_relu6] Epoch: 101 Train Loss: 1.0083 Train Acc: 0.6445 Eval Loss: 0.9440 Eval Acc: 0.6670 (LR: 0.000010)
[2025-05-04 06:00:34,530]: [LeNet5_relu6] Epoch: 102 Train Loss: 1.0139 Train Acc: 0.6425 Eval Loss: 0.9431 Eval Acc: 0.6675 (LR: 0.000010)
[2025-05-04 06:00:53,086]: [LeNet5_relu6] Epoch: 103 Train Loss: 1.0122 Train Acc: 0.6432 Eval Loss: 0.9436 Eval Acc: 0.6686 (LR: 0.000010)
[2025-05-04 06:01:11,067]: [LeNet5_relu6] Epoch: 104 Train Loss: 1.0134 Train Acc: 0.6423 Eval Loss: 0.9427 Eval Acc: 0.6693 (LR: 0.000010)
[2025-05-04 06:01:29,027]: [LeNet5_relu6] Epoch: 105 Train Loss: 1.0084 Train Acc: 0.6436 Eval Loss: 0.9428 Eval Acc: 0.6691 (LR: 0.000010)
[2025-05-04 06:01:47,706]: [LeNet5_relu6] Epoch: 106 Train Loss: 1.0129 Train Acc: 0.6436 Eval Loss: 0.9433 Eval Acc: 0.6690 (LR: 0.000010)
[2025-05-04 06:02:06,477]: [LeNet5_relu6] Epoch: 107 Train Loss: 1.0088 Train Acc: 0.6434 Eval Loss: 0.9426 Eval Acc: 0.6693 (LR: 0.000010)
[2025-05-04 06:02:25,357]: [LeNet5_relu6] Epoch: 108 Train Loss: 1.0126 Train Acc: 0.6430 Eval Loss: 0.9434 Eval Acc: 0.6685 (LR: 0.000010)
[2025-05-04 06:02:44,093]: [LeNet5_relu6] Epoch: 109 Train Loss: 1.0136 Train Acc: 0.6412 Eval Loss: 0.9446 Eval Acc: 0.6686 (LR: 0.000010)
[2025-05-04 06:02:44,106]: Early stopping was triggered!
[2025-05-04 06:02:44,113]: 
Training of full-precision model finished!
[2025-05-04 06:02:44,114]: Model Architecture:
[2025-05-04 06:02:44,114]: LeNet5(
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): ReLU6(inplace=True)
  )
  (fc2): Sequential(
    (0): Linear(in_features=120, out_features=84, bias=True)
    (1): ReLU6(inplace=True)
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-04 06:02:44,115]: 
Model Parameters with Weights:
[2025-05-04 06:02:44,115]: 
Parameter: conv1.0.weight
[2025-05-04 06:02:44,115]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-04 06:02:44,236]: Sample Values (16 elements): [0.10171252489089966, 0.21626894176006317, 0.04562053829431534, -0.031057769432663918, 0.049542952328920364, 0.19331717491149902, -0.032495398074388504, -0.08911911398172379, -0.020758049562573433, 0.16199080646038055, 0.3196439743041992, 0.07029816508293152, 0.060879290103912354, 0.02871311642229557, -0.11327702552080154, 0.20310768485069275]
[2025-05-04 06:02:44,298]: Mean: -0.0009
[2025-05-04 06:02:44,305]: Std: 0.1770
[2025-05-04 06:02:44,332]: Min: -0.7929
[2025-05-04 06:02:44,340]: Max: 0.7595
[2025-05-04 06:02:44,340]: 
Parameter: conv1.0.bias
[2025-05-04 06:02:44,340]: Shape: torch.Size([6])
[2025-05-04 06:02:44,348]: Sample Values (6 elements): [-0.0915069431066513, 0.023941142484545708, 0.04814755544066429, 0.3019459843635559, 0.5945525765419006, 0.7429987788200378]
[2025-05-04 06:02:44,352]: Mean: 0.2700
[2025-05-04 06:02:44,354]: Std: 0.3378
[2025-05-04 06:02:44,357]: Min: -0.0915
[2025-05-04 06:02:44,358]: Max: 0.7430
[2025-05-04 06:02:44,359]: 
Parameter: conv2.0.weight
[2025-05-04 06:02:44,359]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-04 06:02:44,362]: Sample Values (6 elements): [-0.03439796343445778, 0.01088108029216528, -0.038236964493989944, 0.12944822013378143, -0.030238140374422073, -0.01860717311501503]
[2025-05-04 06:02:44,363]: Mean: 0.0027
[2025-05-04 06:02:44,365]: Std: 0.0904
[2025-05-04 06:02:44,367]: Min: -0.3538
[2025-05-04 06:02:44,370]: Max: 0.3453
[2025-05-04 06:02:44,370]: 
Parameter: conv2.0.bias
[2025-05-04 06:02:44,370]: Shape: torch.Size([16])
[2025-05-04 06:02:44,373]: Sample Values (6 elements): [-0.2458016276359558, 0.07502516359090805, 0.22815871238708496, -0.17624543607234955, 0.05258936062455177, 0.16767984628677368]
[2025-05-04 06:02:44,375]: Mean: 0.0146
[2025-05-04 06:02:44,377]: Std: 0.1592
[2025-05-04 06:02:44,378]: Min: -0.2458
[2025-05-04 06:02:44,379]: Max: 0.2282
[2025-05-04 06:02:44,379]: 
Parameter: fc1.0.weight
[2025-05-04 06:02:44,379]: Shape: torch.Size([120, 400])
[2025-05-04 06:02:44,440]: Sample Values (6 elements): [-0.010607019998133183, -0.029952019453048706, 0.011814466677606106, -0.014921062625944614, -0.023696497082710266, 0.02134653739631176]
[2025-05-04 06:02:44,457]: Mean: 0.0013
[2025-05-04 06:02:44,472]: Std: 0.0364
[2025-05-04 06:02:44,478]: Min: -0.1833
[2025-05-04 06:02:44,483]: Max: 0.1876
[2025-05-04 06:02:44,483]: 
Parameter: fc1.0.bias
[2025-05-04 06:02:44,483]: Shape: torch.Size([120])
[2025-05-04 06:02:44,484]: Sample Values (6 elements): [-0.011743946000933647, -0.01699811965227127, 0.004310669843107462, 0.0559038408100605, 0.04400083050131798, 0.04237719625234604]
[2025-05-04 06:02:44,484]: Mean: 0.0009
[2025-05-04 06:02:44,484]: Std: 0.0332
[2025-05-04 06:02:44,486]: Min: -0.0614
[2025-05-04 06:02:44,490]: Max: 0.0765
[2025-05-04 06:02:44,490]: 
Parameter: fc2.0.weight
[2025-05-04 06:02:44,490]: Shape: torch.Size([84, 120])
[2025-05-04 06:02:44,497]: Sample Values (6 elements): [-0.043842289596796036, 0.08629953861236572, -0.05641891434788704, 0.10404414683580399, 0.014517273753881454, -0.07254069298505783]
[2025-05-04 06:02:44,506]: Mean: 0.0005
[2025-05-04 06:02:44,507]: Std: 0.0645
[2025-05-04 06:02:44,509]: Min: -0.2157
[2025-05-04 06:02:44,514]: Max: 0.2617
[2025-05-04 06:02:44,514]: 
Parameter: fc2.0.bias
[2025-05-04 06:02:44,514]: Shape: torch.Size([84])
[2025-05-04 06:02:44,517]: Sample Values (6 elements): [0.11203952133655548, -0.0006884949398227036, 0.06725408881902695, -0.004111376125365496, -0.07354303449392319, 0.06628712266683578]
[2025-05-04 06:02:44,519]: Mean: 0.0136
[2025-05-04 06:02:44,521]: Std: 0.0600
[2025-05-04 06:02:44,524]: Min: -0.1090
[2025-05-04 06:02:44,525]: Max: 0.1191
[2025-05-04 06:02:44,526]: 
Parameter: fc3.weight
[2025-05-04 06:02:44,526]: Shape: torch.Size([10, 84])
[2025-05-04 06:02:44,531]: Sample Values (6 elements): [-0.0346321240067482, 0.026502501219511032, 0.15707696974277496, -0.07622972875833511, 0.13324928283691406, 0.2083350121974945]
[2025-05-04 06:02:44,533]: Mean: 0.0010
[2025-05-04 06:02:44,535]: Std: 0.1354
[2025-05-04 06:02:44,537]: Min: -0.3384
[2025-05-04 06:02:44,539]: Max: 0.4459
[2025-05-04 06:02:44,540]: 
Parameter: fc3.bias
[2025-05-04 06:02:44,540]: Shape: torch.Size([10])
[2025-05-04 06:02:44,549]: Sample Values (6 elements): [-0.11991003155708313, -0.2684357166290283, 0.10307547450065613, -0.24707745015621185, 0.1272134929895401, -0.0012764600105583668]
[2025-05-04 06:02:44,560]: Mean: 0.0141
[2025-05-04 06:02:44,564]: Std: 0.1701
[2025-05-04 06:02:44,570]: Min: -0.2684
[2025-05-04 06:02:44,584]: Max: 0.2254
[2025-05-04 06:02:44,584]: 


QAT of LeNet5 with relu6 down to 4 bits...
[2025-05-04 06:02:45,086]: [LeNet5_relu6_quantized_4_bits] after configure_qat:
[2025-05-04 06:02:45,384]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-04 06:04:19,973]: [LeNet5_relu6_quantized_4_bits] Epoch: 001 Train Loss: 1.0754 Train Acc: 0.6171 Eval Loss: 0.9979 Eval Acc: 0.6476 (LR: 0.001000)
[2025-05-04 06:05:46,258]: [LeNet5_relu6_quantized_4_bits] Epoch: 002 Train Loss: 1.0853 Train Acc: 0.6155 Eval Loss: 0.9981 Eval Acc: 0.6495 (LR: 0.001000)
[2025-05-04 06:07:13,518]: [LeNet5_relu6_quantized_4_bits] Epoch: 003 Train Loss: 1.0795 Train Acc: 0.6177 Eval Loss: 1.0068 Eval Acc: 0.6463 (LR: 0.001000)
[2025-05-04 06:08:43,897]: [LeNet5_relu6_quantized_4_bits] Epoch: 004 Train Loss: 1.0760 Train Acc: 0.6201 Eval Loss: 1.0137 Eval Acc: 0.6495 (LR: 0.001000)
[2025-05-04 06:10:14,478]: [LeNet5_relu6_quantized_4_bits] Epoch: 005 Train Loss: 1.0719 Train Acc: 0.6209 Eval Loss: 0.9839 Eval Acc: 0.6521 (LR: 0.001000)
[2025-05-04 06:11:39,710]: [LeNet5_relu6_quantized_4_bits] Epoch: 006 Train Loss: 1.0637 Train Acc: 0.6238 Eval Loss: 0.9841 Eval Acc: 0.6534 (LR: 0.001000)
[2025-05-04 06:13:03,760]: [LeNet5_relu6_quantized_4_bits] Epoch: 007 Train Loss: 1.0665 Train Acc: 0.6211 Eval Loss: 0.9682 Eval Acc: 0.6575 (LR: 0.001000)
[2025-05-04 06:14:28,650]: [LeNet5_relu6_quantized_4_bits] Epoch: 008 Train Loss: 1.0723 Train Acc: 0.6225 Eval Loss: 0.9610 Eval Acc: 0.6658 (LR: 0.001000)
[2025-05-04 06:15:52,693]: [LeNet5_relu6_quantized_4_bits] Epoch: 009 Train Loss: 1.0634 Train Acc: 0.6225 Eval Loss: 0.9802 Eval Acc: 0.6530 (LR: 0.001000)
[2025-05-04 06:17:22,012]: [LeNet5_relu6_quantized_4_bits] Epoch: 010 Train Loss: 1.0576 Train Acc: 0.6253 Eval Loss: 0.9730 Eval Acc: 0.6527 (LR: 0.001000)
[2025-05-04 06:18:52,660]: [LeNet5_relu6_quantized_4_bits] Epoch: 011 Train Loss: 1.0634 Train Acc: 0.6258 Eval Loss: 0.9604 Eval Acc: 0.6622 (LR: 0.001000)
[2025-05-04 06:20:19,579]: [LeNet5_relu6_quantized_4_bits] Epoch: 012 Train Loss: 1.0556 Train Acc: 0.6285 Eval Loss: 0.9631 Eval Acc: 0.6594 (LR: 0.001000)
[2025-05-04 06:21:45,123]: [LeNet5_relu6_quantized_4_bits] Epoch: 013 Train Loss: 1.0506 Train Acc: 0.6277 Eval Loss: 0.9584 Eval Acc: 0.6649 (LR: 0.001000)
[2025-05-04 06:23:14,122]: [LeNet5_relu6_quantized_4_bits] Epoch: 014 Train Loss: 1.0529 Train Acc: 0.6263 Eval Loss: 0.9669 Eval Acc: 0.6574 (LR: 0.001000)
[2025-05-04 06:24:40,178]: [LeNet5_relu6_quantized_4_bits] Epoch: 015 Train Loss: 1.0452 Train Acc: 0.6311 Eval Loss: 0.9674 Eval Acc: 0.6633 (LR: 0.001000)
[2025-05-04 06:26:06,303]: [LeNet5_relu6_quantized_4_bits] Epoch: 016 Train Loss: 1.0472 Train Acc: 0.6278 Eval Loss: 0.9449 Eval Acc: 0.6659 (LR: 0.001000)
[2025-05-04 06:27:31,011]: [LeNet5_relu6_quantized_4_bits] Epoch: 017 Train Loss: 1.0458 Train Acc: 0.6303 Eval Loss: 0.9412 Eval Acc: 0.6706 (LR: 0.001000)
[2025-05-04 06:29:03,486]: [LeNet5_relu6_quantized_4_bits] Epoch: 018 Train Loss: 1.0376 Train Acc: 0.6331 Eval Loss: 0.9697 Eval Acc: 0.6615 (LR: 0.001000)
[2025-05-04 06:30:41,387]: [LeNet5_relu6_quantized_4_bits] Epoch: 019 Train Loss: 1.0377 Train Acc: 0.6323 Eval Loss: 0.9448 Eval Acc: 0.6668 (LR: 0.001000)
[2025-05-04 06:32:20,205]: [LeNet5_relu6_quantized_4_bits] Epoch: 020 Train Loss: 1.0380 Train Acc: 0.6331 Eval Loss: 0.9611 Eval Acc: 0.6623 (LR: 0.001000)
[2025-05-04 06:33:57,990]: [LeNet5_relu6_quantized_4_bits] Epoch: 021 Train Loss: 1.0355 Train Acc: 0.6353 Eval Loss: 0.9644 Eval Acc: 0.6602 (LR: 0.001000)
[2025-05-04 06:35:35,944]: [LeNet5_relu6_quantized_4_bits] Epoch: 022 Train Loss: 1.0365 Train Acc: 0.6319 Eval Loss: 0.9534 Eval Acc: 0.6623 (LR: 0.001000)
[2025-05-04 06:37:15,706]: [LeNet5_relu6_quantized_4_bits] Epoch: 023 Train Loss: 1.0318 Train Acc: 0.6350 Eval Loss: 0.9398 Eval Acc: 0.6712 (LR: 0.001000)
[2025-05-04 06:38:54,370]: [LeNet5_relu6_quantized_4_bits] Epoch: 024 Train Loss: 1.0284 Train Acc: 0.6368 Eval Loss: 0.9492 Eval Acc: 0.6684 (LR: 0.001000)
[2025-05-04 06:40:32,996]: [LeNet5_relu6_quantized_4_bits] Epoch: 025 Train Loss: 1.0299 Train Acc: 0.6373 Eval Loss: 0.9324 Eval Acc: 0.6728 (LR: 0.001000)
[2025-05-04 06:42:12,564]: [LeNet5_relu6_quantized_4_bits] Epoch: 026 Train Loss: 1.0242 Train Acc: 0.6389 Eval Loss: 0.9303 Eval Acc: 0.6709 (LR: 0.001000)
[2025-05-04 06:43:52,402]: [LeNet5_relu6_quantized_4_bits] Epoch: 027 Train Loss: 1.0309 Train Acc: 0.6346 Eval Loss: 0.9223 Eval Acc: 0.6742 (LR: 0.001000)
[2025-05-04 06:45:32,178]: [LeNet5_relu6_quantized_4_bits] Epoch: 028 Train Loss: 1.0258 Train Acc: 0.6365 Eval Loss: 0.9430 Eval Acc: 0.6705 (LR: 0.001000)
[2025-05-04 06:47:15,430]: [LeNet5_relu6_quantized_4_bits] Epoch: 029 Train Loss: 1.0185 Train Acc: 0.6426 Eval Loss: 0.9212 Eval Acc: 0.6755 (LR: 0.001000)
[2025-05-04 06:48:54,085]: [LeNet5_relu6_quantized_4_bits] Epoch: 030 Train Loss: 1.0174 Train Acc: 0.6420 Eval Loss: 0.9611 Eval Acc: 0.6603 (LR: 0.000250)
[2025-05-04 06:50:32,385]: [LeNet5_relu6_quantized_4_bits] Epoch: 031 Train Loss: 0.9880 Train Acc: 0.6517 Eval Loss: 0.9059 Eval Acc: 0.6775 (LR: 0.000250)
[2025-05-04 06:52:10,101]: [LeNet5_relu6_quantized_4_bits] Epoch: 032 Train Loss: 0.9836 Train Acc: 0.6518 Eval Loss: 0.9166 Eval Acc: 0.6763 (LR: 0.000250)
[2025-05-04 06:53:48,009]: [LeNet5_relu6_quantized_4_bits] Epoch: 033 Train Loss: 0.9829 Train Acc: 0.6531 Eval Loss: 0.9030 Eval Acc: 0.6824 (LR: 0.000250)
[2025-05-04 06:55:25,733]: [LeNet5_relu6_quantized_4_bits] Epoch: 034 Train Loss: 0.9766 Train Acc: 0.6540 Eval Loss: 0.9216 Eval Acc: 0.6784 (LR: 0.000250)
[2025-05-04 06:56:58,886]: [LeNet5_relu6_quantized_4_bits] Epoch: 035 Train Loss: 0.9894 Train Acc: 0.6520 Eval Loss: 0.9058 Eval Acc: 0.6801 (LR: 0.000250)
[2025-05-04 06:58:34,433]: [LeNet5_relu6_quantized_4_bits] Epoch: 036 Train Loss: 0.9804 Train Acc: 0.6535 Eval Loss: 0.8998 Eval Acc: 0.6862 (LR: 0.000250)
[2025-05-04 07:00:13,085]: [LeNet5_relu6_quantized_4_bits] Epoch: 037 Train Loss: 0.9812 Train Acc: 0.6531 Eval Loss: 0.8999 Eval Acc: 0.6853 (LR: 0.000250)
[2025-05-04 07:01:51,901]: [LeNet5_relu6_quantized_4_bits] Epoch: 038 Train Loss: 0.9771 Train Acc: 0.6550 Eval Loss: 0.8972 Eval Acc: 0.6857 (LR: 0.000250)
[2025-05-04 07:03:30,760]: [LeNet5_relu6_quantized_4_bits] Epoch: 039 Train Loss: 0.9755 Train Acc: 0.6562 Eval Loss: 0.8945 Eval Acc: 0.6870 (LR: 0.000250)
[2025-05-04 07:05:03,261]: [LeNet5_relu6_quantized_4_bits] Epoch: 040 Train Loss: 0.9808 Train Acc: 0.6549 Eval Loss: 0.9157 Eval Acc: 0.6797 (LR: 0.000250)
[2025-05-04 07:06:33,878]: [LeNet5_relu6_quantized_4_bits] Epoch: 041 Train Loss: 0.9817 Train Acc: 0.6516 Eval Loss: 0.9022 Eval Acc: 0.6815 (LR: 0.000250)
[2025-05-04 07:08:12,684]: [LeNet5_relu6_quantized_4_bits] Epoch: 042 Train Loss: 0.9815 Train Acc: 0.6535 Eval Loss: 0.8984 Eval Acc: 0.6818 (LR: 0.000250)
[2025-05-04 07:09:50,269]: [LeNet5_relu6_quantized_4_bits] Epoch: 043 Train Loss: 0.9838 Train Acc: 0.6519 Eval Loss: 0.8936 Eval Acc: 0.6882 (LR: 0.000250)
[2025-05-04 07:11:28,720]: [LeNet5_relu6_quantized_4_bits] Epoch: 044 Train Loss: 0.9817 Train Acc: 0.6517 Eval Loss: 0.8992 Eval Acc: 0.6820 (LR: 0.000250)
[2025-05-04 07:13:05,825]: [LeNet5_relu6_quantized_4_bits] Epoch: 045 Train Loss: 0.9792 Train Acc: 0.6545 Eval Loss: 0.9013 Eval Acc: 0.6839 (LR: 0.000063)
[2025-05-04 07:14:43,429]: [LeNet5_relu6_quantized_4_bits] Epoch: 046 Train Loss: 0.9697 Train Acc: 0.6559 Eval Loss: 0.8967 Eval Acc: 0.6827 (LR: 0.000063)
[2025-05-04 07:16:22,517]: [LeNet5_relu6_quantized_4_bits] Epoch: 047 Train Loss: 0.9688 Train Acc: 0.6587 Eval Loss: 0.8972 Eval Acc: 0.6842 (LR: 0.000063)
[2025-05-04 07:17:53,953]: [LeNet5_relu6_quantized_4_bits] Epoch: 048 Train Loss: 0.9688 Train Acc: 0.6594 Eval Loss: 0.8923 Eval Acc: 0.6871 (LR: 0.000063)
[2025-05-04 07:19:25,587]: [LeNet5_relu6_quantized_4_bits] Epoch: 049 Train Loss: 0.9621 Train Acc: 0.6616 Eval Loss: 0.9048 Eval Acc: 0.6845 (LR: 0.000063)
[2025-05-04 07:20:54,312]: [LeNet5_relu6_quantized_4_bits] Epoch: 050 Train Loss: 0.9632 Train Acc: 0.6595 Eval Loss: 0.8903 Eval Acc: 0.6878 (LR: 0.000063)
[2025-05-04 07:22:24,524]: [LeNet5_relu6_quantized_4_bits] Epoch: 051 Train Loss: 0.9648 Train Acc: 0.6592 Eval Loss: 0.8883 Eval Acc: 0.6873 (LR: 0.000063)
[2025-05-04 07:23:53,769]: [LeNet5_relu6_quantized_4_bits] Epoch: 052 Train Loss: 0.9620 Train Acc: 0.6588 Eval Loss: 0.8869 Eval Acc: 0.6890 (LR: 0.000063)
[2025-05-04 07:25:24,914]: [LeNet5_relu6_quantized_4_bits] Epoch: 053 Train Loss: 0.9672 Train Acc: 0.6580 Eval Loss: 0.8841 Eval Acc: 0.6891 (LR: 0.000063)
[2025-05-04 07:26:52,560]: [LeNet5_relu6_quantized_4_bits] Epoch: 054 Train Loss: 0.9632 Train Acc: 0.6596 Eval Loss: 0.8932 Eval Acc: 0.6851 (LR: 0.000063)
[2025-05-04 07:28:17,593]: [LeNet5_relu6_quantized_4_bits] Epoch: 055 Train Loss: 0.9708 Train Acc: 0.6550 Eval Loss: 0.8923 Eval Acc: 0.6854 (LR: 0.000063)
[2025-05-04 07:29:34,720]: [LeNet5_relu6_quantized_4_bits] Epoch: 056 Train Loss: 0.9652 Train Acc: 0.6608 Eval Loss: 0.8936 Eval Acc: 0.6880 (LR: 0.000063)
[2025-05-04 07:30:44,004]: [LeNet5_relu6_quantized_4_bits] Epoch: 057 Train Loss: 0.9655 Train Acc: 0.6588 Eval Loss: 0.8889 Eval Acc: 0.6894 (LR: 0.000063)
[2025-05-04 07:31:56,303]: [LeNet5_relu6_quantized_4_bits] Epoch: 058 Train Loss: 0.9608 Train Acc: 0.6608 Eval Loss: 0.8855 Eval Acc: 0.6895 (LR: 0.000063)
[2025-05-04 07:33:09,092]: [LeNet5_relu6_quantized_4_bits] Epoch: 059 Train Loss: 0.9670 Train Acc: 0.6579 Eval Loss: 0.8888 Eval Acc: 0.6880 (LR: 0.000063)
[2025-05-04 07:34:25,265]: [LeNet5_relu6_quantized_4_bits] Epoch: 060 Train Loss: 0.9635 Train Acc: 0.6602 Eval Loss: 0.8930 Eval Acc: 0.6860 (LR: 0.000063)
[2025-05-04 07:34:25,282]: 


Quantization of model down to 4 bits finished
[2025-05-04 07:34:25,283]: Model Architecture:
[2025-05-04 07:34:25,574]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0446, 0.0324, 0.0497, 0.0579, 0.0543, 0.0492, 0.0544, 0.0308, 0.0422,
                0.0392, 0.0343, 0.0365, 0.0302, 0.0529, 0.0335, 0.0656],
               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.3347, -0.1695, -0.3726, -0.4346, -0.4070, -0.2017, -0.4081, -0.1781,
                  -0.3166, -0.2117, -0.2571, -0.2611, -0.2059, -0.3526, -0.2514, -0.4919],
                 device='cuda:0'), max_val=tensor([0.3146, 0.2430, 0.3727, 0.2347, 0.4057, 0.3689, 0.2693, 0.2308, 0.3141,
                  0.2937, 0.2503, 0.2735, 0.2267, 0.3967, 0.2191, 0.3037],
                 device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0155, 0.0127, 0.0101, 0.0090, 0.0158, 0.0104, 0.0173, 0.0142, 0.0158,
                0.0156, 0.0165, 0.0214, 0.0146, 0.0159, 0.0132, 0.0114, 0.0132, 0.0169,
                0.0200, 0.0212, 0.0186, 0.0123, 0.0126, 0.0064, 0.0200, 0.0085, 0.0179,
                0.0149, 0.0144, 0.0118, 0.0169, 0.0147, 0.0125, 0.0162, 0.0213, 0.0139,
                0.0158, 0.0167, 0.0163, 0.0198, 0.0159, 0.0143, 0.0179, 0.0131, 0.0069,
                0.0178, 0.0171, 0.0156, 0.0184, 0.0193, 0.0135, 0.0148, 0.0180, 0.0159,
                0.0172, 0.0140, 0.0137, 0.0176, 0.0156, 0.0138, 0.0148, 0.0086, 0.0177,
                0.0132, 0.0196, 0.0139, 0.0262, 0.0199, 0.0185, 0.0067, 0.0139, 0.0162,
                0.0164, 0.0149, 0.0164, 0.0191, 0.0171, 0.0169, 0.0147, 0.0084, 0.0185,
                0.0146, 0.0071, 0.0170, 0.0158, 0.0296, 0.0178, 0.0158, 0.0157, 0.0158,
                0.0229, 0.0221, 0.0159, 0.0170, 0.0114, 0.0148, 0.0157, 0.0188, 0.0165,
                0.0162, 0.0153, 0.0145, 0.0188, 0.0134, 0.0207, 0.0152, 0.0208, 0.0170,
                0.0119, 0.0180, 0.0284, 0.0169, 0.0146, 0.0147, 0.0182, 0.0124, 0.0177,
                0.0182, 0.0155, 0.0169], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.1121, -0.0890, -0.0748, -0.0669, -0.1187, -0.0779, -0.1239, -0.1061,
                  -0.1136, -0.1172, -0.1074, -0.1604, -0.1083, -0.1194, -0.0953, -0.0825,
                  -0.0993, -0.1256, -0.1503, -0.1030, -0.1137, -0.0925, -0.0891, -0.0481,
                  -0.1473, -0.0635, -0.1342, -0.0987, -0.1023, -0.0888, -0.1136, -0.1096,
                  -0.0869, -0.1218, -0.1596, -0.1015, -0.1188, -0.1256, -0.1220, -0.1483,
                  -0.1190, -0.1076, -0.1324, -0.0985, -0.0516, -0.1212, -0.1282, -0.1133,
                  -0.1383, -0.1445, -0.1009, -0.1061, -0.1352, -0.1194, -0.1157, -0.1051,
                  -0.0983, -0.1205, -0.1039, -0.1028, -0.0842, -0.0648, -0.1328, -0.0395,
                  -0.1470, -0.0953, -0.1221, -0.1492, -0.1385, -0.0505, -0.1040, -0.0977,
                  -0.1234, -0.1120, -0.1229, -0.1432, -0.1280, -0.0898, -0.0387, -0.0634,
                  -0.0989, -0.0955, -0.0522, -0.1273, -0.1184, -0.2219, -0.1291, -0.1188,
                  -0.1158, -0.1165, -0.1719, -0.1659, -0.1194, -0.1277, -0.0851, -0.1097,
                  -0.1179, -0.1411, -0.1239, -0.1185, -0.1016, -0.1086, -0.1384, -0.0998,
                  -0.1553, -0.1117, -0.1560, -0.0877, -0.0777, -0.1352, -0.2129, -0.1264,
                  -0.0816, -0.1072, -0.1368, -0.0930, -0.1325, -0.1367, -0.1162, -0.1266],
                 device='cuda:0'), max_val=tensor([0.1161, 0.0953, 0.0759, 0.0674, 0.1071, 0.0758, 0.1294, 0.1064, 0.1189,
                  0.0961, 0.1237, 0.1031, 0.1099, 0.1054, 0.0991, 0.0858, 0.0883, 0.1267,
                  0.1061, 0.1591, 0.1396, 0.0926, 0.0944, 0.0460, 0.1504, 0.0628, 0.1028,
                  0.1118, 0.1079, 0.0760, 0.1268, 0.1101, 0.0941, 0.1187, 0.1401, 0.1045,
                  0.1011, 0.1043, 0.1050, 0.1336, 0.1107, 0.1072, 0.1341, 0.0944, 0.0485,
                  0.1332, 0.1170, 0.1169, 0.0975, 0.1300, 0.1009, 0.1107, 0.1009, 0.0949,
                  0.1289, 0.0967, 0.1024, 0.1319, 0.1173, 0.1032, 0.1113, 0.0647, 0.1166,
                  0.0989, 0.1282, 0.1042, 0.1961, 0.1399, 0.1018, 0.0503, 0.1029, 0.1214,
                  0.1199, 0.0957, 0.1232, 0.1116, 0.1169, 0.1268, 0.1100, 0.0632, 0.1387,
                  0.1096, 0.0531, 0.1190, 0.0956, 0.0850, 0.1331, 0.1025, 0.1176, 0.1186,
                  0.1310, 0.1541, 0.1045, 0.0847, 0.0766, 0.1112, 0.0901, 0.1208, 0.0910,
                  0.1214, 0.1147, 0.1083, 0.1407, 0.1004, 0.1113, 0.1138, 0.1294, 0.1273,
                  0.0895, 0.1127, 0.1012, 0.1064, 0.1095, 0.1106, 0.1179, 0.0823, 0.1320,
                  0.1124, 0.1090, 0.1144], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0184, 0.0204, 0.0167, 0.0186, 0.0229, 0.0239, 0.0275, 0.0116, 0.0216,
                0.0113, 0.0186, 0.0238, 0.0259, 0.0215, 0.0197, 0.0235, 0.0277, 0.0127,
                0.0193, 0.0166, 0.0222, 0.0317, 0.0287, 0.0147, 0.0276, 0.0363, 0.0325,
                0.0117, 0.0203, 0.0280, 0.0209, 0.0223, 0.0233, 0.0201, 0.0353, 0.0255,
                0.0268, 0.0218, 0.0237, 0.0281, 0.0116, 0.0236, 0.0117, 0.0197, 0.0245,
                0.0213, 0.0243, 0.0387, 0.0228, 0.0175, 0.0303, 0.0271, 0.0307, 0.0121,
                0.0170, 0.0125, 0.0219, 0.0278, 0.0172, 0.0232, 0.0221, 0.0247, 0.0312,
                0.0244, 0.0180, 0.0290, 0.0206, 0.0225, 0.0239, 0.0116, 0.0235, 0.0315,
                0.0203, 0.0237, 0.0198, 0.0200, 0.0199, 0.0242, 0.0239, 0.0255, 0.0275,
                0.0202, 0.0310, 0.0116], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.1156, -0.1530, -0.1240, -0.1152, -0.1696, -0.1556, -0.1574, -0.0872,
                  -0.1439, -0.0850, -0.1358, -0.1571, -0.1944, -0.1391, -0.1477, -0.1766,
                  -0.2075, -0.0951, -0.1445, -0.1227, -0.1665, -0.1885, -0.1676, -0.1099,
                  -0.1627, -0.1638, -0.1549, -0.0876, -0.1522, -0.2103, -0.1488, -0.1673,
                  -0.1320, -0.1497, -0.2007, -0.1733, -0.1657, -0.1362, -0.1605, -0.1757,
                  -0.0874, -0.1604, -0.0874, -0.1383, -0.1615, -0.1599, -0.1542, -0.1885,
                  -0.1647, -0.1009, -0.1727, -0.1911, -0.2299, -0.0904, -0.1253, -0.0885,
                  -0.1461, -0.1474, -0.1290, -0.1741, -0.1656, -0.1853, -0.1810, -0.1677,
                  -0.1347, -0.2178, -0.1532, -0.1668, -0.1412, -0.0863, -0.1688, -0.2272,
                  -0.1459, -0.1778, -0.1486, -0.1367, -0.1151, -0.1685, -0.1792, -0.1206,
                  -0.2065, -0.1515, -0.1328, -0.0856], device='cuda:0'), max_val=tensor([0.1383, 0.1357, 0.1251, 0.1393, 0.1719, 0.1792, 0.2064, 0.0870, 0.1622,
                  0.0849, 0.1398, 0.1782, 0.1765, 0.1612, 0.1450, 0.1745, 0.1774, 0.0857,
                  0.1204, 0.1243, 0.1354, 0.2376, 0.2156, 0.1055, 0.2066, 0.2720, 0.2436,
                  0.0817, 0.1357, 0.2061, 0.1566, 0.1482, 0.1748, 0.1507, 0.2649, 0.1913,
                  0.2009, 0.1638, 0.1777, 0.2106, 0.0844, 0.1767, 0.0837, 0.1476, 0.1838,
                  0.1587, 0.1820, 0.2901, 0.1708, 0.1313, 0.2271, 0.2030, 0.1992, 0.0842,
                  0.1276, 0.0940, 0.1645, 0.2088, 0.1245, 0.1658, 0.1658, 0.1854, 0.2340,
                  0.1828, 0.1347, 0.1959, 0.1542, 0.1685, 0.1795, 0.0868, 0.1762, 0.2361,
                  0.1519, 0.1580, 0.1414, 0.1497, 0.1495, 0.1818, 0.1542, 0.1911, 0.1933,
                  0.1312, 0.2325, 0.0871], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-04 07:34:25,574]: 
Model Parameters with Weights:
[2025-05-04 07:34:25,575]: 
Parameter: conv1.0.weight
[2025-05-04 07:34:25,575]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-04 07:34:25,583]: Sample Values (16 elements): [-0.1257707178592682, -0.3212140202522278, -0.07052579522132874, 0.0031492947600781918, 0.0876380056142807, -0.04570440575480461, 0.14132508635520935, 0.24623656272888184, 0.06928108632564545, 0.1017451137304306, 0.07761668413877487, -0.03934625908732414, -0.10654482990503311, -0.13980823755264282, 0.10302647203207016, -0.11141609400510788]
[2025-05-04 07:34:25,586]: Mean: -0.0004
[2025-05-04 07:34:25,588]: Std: 0.2017
[2025-05-04 07:34:25,589]: Min: -0.9216
[2025-05-04 07:34:25,591]: Max: 0.8409
[2025-05-04 07:34:25,591]: 
Parameter: conv1.0.bias
[2025-05-04 07:34:25,591]: Shape: torch.Size([6])
[2025-05-04 07:34:25,604]: Sample Values (6 elements): [0.8127066493034363, -0.10628847777843475, 0.10478482395410538, -0.03923821076750755, -0.06814873963594437, 0.5920999646186829]
[2025-05-04 07:34:25,614]: Mean: 0.2160
[2025-05-04 07:34:25,618]: Std: 0.3898
[2025-05-04 07:34:25,619]: Min: -0.1063
[2025-05-04 07:34:25,619]: Max: 0.8127
[2025-05-04 07:34:25,619]: 
Parameter: conv2.0.weight
[2025-05-04 07:34:25,619]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-04 07:34:25,629]: Sample Values (6 elements): [0.05753477290272713, 0.20009279251098633, -0.2483760565519333, 0.1631585657596588, 0.0952378660440445, 0.0978807657957077]
[2025-05-04 07:34:25,631]: Mean: 0.0013
[2025-05-04 07:34:25,638]: Std: 0.1080
[2025-05-04 07:34:25,639]: Min: -0.4919
[2025-05-04 07:34:25,643]: Max: 0.4058
[2025-05-04 07:34:25,643]: 
Parameter: conv2.0.bias
[2025-05-04 07:34:25,643]: Shape: torch.Size([16])
[2025-05-04 07:34:25,645]: Sample Values (6 elements): [0.0785488709807396, -0.09491273760795593, 0.25346699357032776, -0.3182937502861023, 0.02737262286245823, 0.23487910628318787]
[2025-05-04 07:34:25,653]: Mean: -0.0030
[2025-05-04 07:34:25,654]: Std: 0.1957
[2025-05-04 07:34:25,657]: Min: -0.3374
[2025-05-04 07:34:25,661]: Max: 0.2535
[2025-05-04 07:34:25,661]: 
Parameter: fc1.0.weight
[2025-05-04 07:34:25,661]: Shape: torch.Size([120, 400])
[2025-05-04 07:34:25,663]: Sample Values (6 elements): [-0.007983459159731865, 0.0175034049898386, 0.0018621663330122828, -0.0010068827541545033, 0.09223447740077972, 0.09552649408578873]
[2025-05-04 07:34:25,666]: Mean: 0.0013
[2025-05-04 07:34:25,672]: Std: 0.0393
[2025-05-04 07:34:25,681]: Min: -0.2220
[2025-05-04 07:34:25,696]: Max: 0.1961
[2025-05-04 07:34:25,696]: 
Parameter: fc1.0.bias
[2025-05-04 07:34:25,696]: Shape: torch.Size([120])
[2025-05-04 07:34:25,702]: Sample Values (6 elements): [-0.018279191106557846, -0.008839095942676067, 0.019496791064739227, 0.05314861610531807, 0.04561169818043709, -0.012402159161865711]
[2025-05-04 07:34:25,704]: Mean: 0.0013
[2025-05-04 07:34:25,705]: Std: 0.0348
[2025-05-04 07:34:25,707]: Min: -0.0751
[2025-05-04 07:34:25,707]: Max: 0.0835
[2025-05-04 07:34:25,707]: 
Parameter: fc2.0.weight
[2025-05-04 07:34:25,707]: Shape: torch.Size([84, 120])
[2025-05-04 07:34:25,710]: Sample Values (6 elements): [0.039361100643873215, 0.06925596296787262, -0.06472859531641006, 0.04086586833000183, -0.006619927007704973, -0.06942124664783478]
[2025-05-04 07:34:25,711]: Mean: 0.0001
[2025-05-04 07:34:25,712]: Std: 0.0682
[2025-05-04 07:34:25,715]: Min: -0.2298
[2025-05-04 07:34:25,719]: Max: 0.2901
[2025-05-04 07:34:25,719]: 
Parameter: fc2.0.bias
[2025-05-04 07:34:25,719]: Shape: torch.Size([84])
[2025-05-04 07:34:25,731]: Sample Values (6 elements): [0.03190542012453079, 0.012672278098762035, 0.04315683990716934, -0.01660514436662197, 0.08269062638282776, -0.03876783698797226]
[2025-05-04 07:34:25,732]: Mean: 0.0149
[2025-05-04 07:34:25,734]: Std: 0.0598
[2025-05-04 07:34:25,737]: Min: -0.1084
[2025-05-04 07:34:25,740]: Max: 0.1230
[2025-05-04 07:34:25,740]: 
Parameter: fc3.weight
[2025-05-04 07:34:25,740]: Shape: torch.Size([10, 84])
[2025-05-04 07:34:25,742]: Sample Values (6 elements): [-0.2706138491630554, -0.12776680290699005, -0.06875629723072052, -0.14015716314315796, -0.03098350577056408, -0.060711633414030075]
[2025-05-04 07:34:25,746]: Mean: 0.0010
[2025-05-04 07:34:25,752]: Std: 0.1335
[2025-05-04 07:34:25,767]: Min: -0.3342
[2025-05-04 07:34:25,779]: Max: 0.4639
[2025-05-04 07:34:25,779]: 
Parameter: fc3.bias
[2025-05-04 07:34:25,780]: Shape: torch.Size([10])
[2025-05-04 07:34:25,785]: Sample Values (6 elements): [-0.007239076774567366, 0.2298731803894043, 0.14317834377288818, -0.24518075585365295, -0.2736053764820099, 0.10857465118169785]
[2025-05-04 07:34:25,786]: Mean: 0.0140
[2025-05-04 07:34:25,787]: Std: 0.1718
[2025-05-04 07:34:25,787]: Min: -0.2736
[2025-05-04 07:34:25,788]: Max: 0.2299
[2025-05-04 07:34:25,788]: 


QAT of LeNet5 with relu6 down to 3 bits...
[2025-05-04 07:34:25,986]: [LeNet5_relu6_quantized_3_bits] after configure_qat:
[2025-05-04 07:34:26,088]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-04 07:35:39,633]: [LeNet5_relu6_quantized_3_bits] Epoch: 001 Train Loss: 1.1401 Train Acc: 0.5982 Eval Loss: 1.0278 Eval Acc: 0.6408 (LR: 0.001000)
[2025-05-04 07:36:44,642]: [LeNet5_relu6_quantized_3_bits] Epoch: 002 Train Loss: 1.1467 Train Acc: 0.5936 Eval Loss: 1.0367 Eval Acc: 0.6306 (LR: 0.001000)
[2025-05-04 07:37:36,046]: [LeNet5_relu6_quantized_3_bits] Epoch: 003 Train Loss: 1.1428 Train Acc: 0.5918 Eval Loss: 1.0359 Eval Acc: 0.6312 (LR: 0.001000)
[2025-05-04 07:38:17,328]: [LeNet5_relu6_quantized_3_bits] Epoch: 004 Train Loss: 1.1429 Train Acc: 0.5956 Eval Loss: 1.0341 Eval Acc: 0.6333 (LR: 0.001000)
[2025-05-04 07:38:45,399]: [LeNet5_relu6_quantized_3_bits] Epoch: 005 Train Loss: 1.1430 Train Acc: 0.5946 Eval Loss: 1.0373 Eval Acc: 0.6321 (LR: 0.001000)
[2025-05-04 07:39:12,006]: [LeNet5_relu6_quantized_3_bits] Epoch: 006 Train Loss: 1.1258 Train Acc: 0.5993 Eval Loss: 1.0237 Eval Acc: 0.6449 (LR: 0.001000)
[2025-05-04 07:39:39,310]: [LeNet5_relu6_quantized_3_bits] Epoch: 007 Train Loss: 1.1260 Train Acc: 0.6019 Eval Loss: 1.0193 Eval Acc: 0.6413 (LR: 0.001000)
[2025-05-04 07:40:06,912]: [LeNet5_relu6_quantized_3_bits] Epoch: 008 Train Loss: 1.1351 Train Acc: 0.5981 Eval Loss: 1.0275 Eval Acc: 0.6355 (LR: 0.001000)
[2025-05-04 07:40:34,111]: [LeNet5_relu6_quantized_3_bits] Epoch: 009 Train Loss: 1.1303 Train Acc: 0.6011 Eval Loss: 1.0397 Eval Acc: 0.6255 (LR: 0.001000)
[2025-05-04 07:41:01,433]: [LeNet5_relu6_quantized_3_bits] Epoch: 010 Train Loss: 1.1239 Train Acc: 0.6022 Eval Loss: 1.0173 Eval Acc: 0.6441 (LR: 0.001000)
[2025-05-04 07:41:28,555]: [LeNet5_relu6_quantized_3_bits] Epoch: 011 Train Loss: 1.1257 Train Acc: 0.6006 Eval Loss: 1.0628 Eval Acc: 0.6229 (LR: 0.001000)
[2025-05-04 07:41:58,992]: [LeNet5_relu6_quantized_3_bits] Epoch: 012 Train Loss: 1.1257 Train Acc: 0.6009 Eval Loss: 1.0267 Eval Acc: 0.6447 (LR: 0.001000)
[2025-05-04 07:42:30,723]: [LeNet5_relu6_quantized_3_bits] Epoch: 013 Train Loss: 1.1171 Train Acc: 0.6041 Eval Loss: 1.0159 Eval Acc: 0.6484 (LR: 0.001000)
[2025-05-04 07:43:02,025]: [LeNet5_relu6_quantized_3_bits] Epoch: 014 Train Loss: 1.1200 Train Acc: 0.6008 Eval Loss: 1.0055 Eval Acc: 0.6436 (LR: 0.001000)
[2025-05-04 07:43:30,546]: [LeNet5_relu6_quantized_3_bits] Epoch: 015 Train Loss: 1.1163 Train Acc: 0.6030 Eval Loss: 1.0104 Eval Acc: 0.6483 (LR: 0.001000)
[2025-05-04 07:43:58,010]: [LeNet5_relu6_quantized_3_bits] Epoch: 016 Train Loss: 1.1166 Train Acc: 0.6046 Eval Loss: 1.0271 Eval Acc: 0.6379 (LR: 0.001000)
[2025-05-04 07:44:26,784]: [LeNet5_relu6_quantized_3_bits] Epoch: 017 Train Loss: 1.1073 Train Acc: 0.6086 Eval Loss: 1.0165 Eval Acc: 0.6438 (LR: 0.001000)
[2025-05-04 07:44:53,609]: [LeNet5_relu6_quantized_3_bits] Epoch: 018 Train Loss: 1.1021 Train Acc: 0.6101 Eval Loss: 1.0089 Eval Acc: 0.6503 (LR: 0.001000)
[2025-05-04 07:45:20,996]: [LeNet5_relu6_quantized_3_bits] Epoch: 019 Train Loss: 1.1096 Train Acc: 0.6079 Eval Loss: 1.0455 Eval Acc: 0.6249 (LR: 0.001000)
[2025-05-04 07:45:47,553]: [LeNet5_relu6_quantized_3_bits] Epoch: 020 Train Loss: 1.1029 Train Acc: 0.6075 Eval Loss: 0.9944 Eval Acc: 0.6489 (LR: 0.001000)
[2025-05-04 07:46:13,899]: [LeNet5_relu6_quantized_3_bits] Epoch: 021 Train Loss: 1.1010 Train Acc: 0.6096 Eval Loss: 1.0351 Eval Acc: 0.6321 (LR: 0.001000)
[2025-05-04 07:46:41,898]: [LeNet5_relu6_quantized_3_bits] Epoch: 022 Train Loss: 1.0977 Train Acc: 0.6098 Eval Loss: 1.0007 Eval Acc: 0.6491 (LR: 0.001000)
[2025-05-04 07:47:08,076]: [LeNet5_relu6_quantized_3_bits] Epoch: 023 Train Loss: 1.0963 Train Acc: 0.6116 Eval Loss: 1.0472 Eval Acc: 0.6348 (LR: 0.001000)
[2025-05-04 07:47:34,402]: [LeNet5_relu6_quantized_3_bits] Epoch: 024 Train Loss: 1.1004 Train Acc: 0.6099 Eval Loss: 1.0021 Eval Acc: 0.6465 (LR: 0.001000)
[2025-05-04 07:48:02,541]: [LeNet5_relu6_quantized_3_bits] Epoch: 025 Train Loss: 1.0933 Train Acc: 0.6134 Eval Loss: 0.9848 Eval Acc: 0.6584 (LR: 0.001000)
[2025-05-04 07:48:30,383]: [LeNet5_relu6_quantized_3_bits] Epoch: 026 Train Loss: 1.0867 Train Acc: 0.6150 Eval Loss: 0.9826 Eval Acc: 0.6505 (LR: 0.001000)
[2025-05-04 07:48:57,602]: [LeNet5_relu6_quantized_3_bits] Epoch: 027 Train Loss: 1.0988 Train Acc: 0.6100 Eval Loss: 0.9682 Eval Acc: 0.6574 (LR: 0.001000)
[2025-05-04 07:49:24,931]: [LeNet5_relu6_quantized_3_bits] Epoch: 028 Train Loss: 1.0900 Train Acc: 0.6142 Eval Loss: 1.0064 Eval Acc: 0.6394 (LR: 0.001000)
[2025-05-04 07:49:52,068]: [LeNet5_relu6_quantized_3_bits] Epoch: 029 Train Loss: 1.0829 Train Acc: 0.6183 Eval Loss: 0.9862 Eval Acc: 0.6540 (LR: 0.001000)
[2025-05-04 07:50:19,217]: [LeNet5_relu6_quantized_3_bits] Epoch: 030 Train Loss: 1.0881 Train Acc: 0.6154 Eval Loss: 0.9830 Eval Acc: 0.6580 (LR: 0.000250)
[2025-05-04 07:50:46,671]: [LeNet5_relu6_quantized_3_bits] Epoch: 031 Train Loss: 1.0513 Train Acc: 0.6291 Eval Loss: 0.9597 Eval Acc: 0.6651 (LR: 0.000250)
[2025-05-04 07:51:13,408]: [LeNet5_relu6_quantized_3_bits] Epoch: 032 Train Loss: 1.0498 Train Acc: 0.6272 Eval Loss: 0.9655 Eval Acc: 0.6657 (LR: 0.000250)
[2025-05-04 07:51:41,938]: [LeNet5_relu6_quantized_3_bits] Epoch: 033 Train Loss: 1.0484 Train Acc: 0.6295 Eval Loss: 0.9569 Eval Acc: 0.6649 (LR: 0.000250)
[2025-05-04 07:52:10,029]: [LeNet5_relu6_quantized_3_bits] Epoch: 034 Train Loss: 1.0470 Train Acc: 0.6314 Eval Loss: 0.9624 Eval Acc: 0.6596 (LR: 0.000250)
[2025-05-04 07:52:37,692]: [LeNet5_relu6_quantized_3_bits] Epoch: 035 Train Loss: 1.0518 Train Acc: 0.6247 Eval Loss: 0.9474 Eval Acc: 0.6649 (LR: 0.000250)
[2025-05-04 07:53:06,636]: [LeNet5_relu6_quantized_3_bits] Epoch: 036 Train Loss: 1.0482 Train Acc: 0.6302 Eval Loss: 0.9520 Eval Acc: 0.6646 (LR: 0.000250)
[2025-05-04 07:53:34,486]: [LeNet5_relu6_quantized_3_bits] Epoch: 037 Train Loss: 1.0500 Train Acc: 0.6292 Eval Loss: 0.9478 Eval Acc: 0.6676 (LR: 0.000250)
[2025-05-04 07:54:02,701]: [LeNet5_relu6_quantized_3_bits] Epoch: 038 Train Loss: 1.0472 Train Acc: 0.6294 Eval Loss: 0.9505 Eval Acc: 0.6659 (LR: 0.000250)
[2025-05-04 07:54:30,629]: [LeNet5_relu6_quantized_3_bits] Epoch: 039 Train Loss: 1.0476 Train Acc: 0.6283 Eval Loss: 0.9580 Eval Acc: 0.6610 (LR: 0.000250)
[2025-05-04 07:54:58,614]: [LeNet5_relu6_quantized_3_bits] Epoch: 040 Train Loss: 1.0527 Train Acc: 0.6283 Eval Loss: 0.9734 Eval Acc: 0.6580 (LR: 0.000250)
[2025-05-04 07:55:27,773]: [LeNet5_relu6_quantized_3_bits] Epoch: 041 Train Loss: 1.0443 Train Acc: 0.6319 Eval Loss: 0.9690 Eval Acc: 0.6613 (LR: 0.000250)
[2025-05-04 07:55:55,894]: [LeNet5_relu6_quantized_3_bits] Epoch: 042 Train Loss: 1.0526 Train Acc: 0.6274 Eval Loss: 0.9398 Eval Acc: 0.6686 (LR: 0.000250)
[2025-05-04 07:56:23,927]: [LeNet5_relu6_quantized_3_bits] Epoch: 043 Train Loss: 1.0497 Train Acc: 0.6272 Eval Loss: 0.9644 Eval Acc: 0.6624 (LR: 0.000250)
[2025-05-04 07:56:51,386]: [LeNet5_relu6_quantized_3_bits] Epoch: 044 Train Loss: 1.0510 Train Acc: 0.6301 Eval Loss: 0.9681 Eval Acc: 0.6559 (LR: 0.000250)
[2025-05-04 07:57:20,055]: [LeNet5_relu6_quantized_3_bits] Epoch: 045 Train Loss: 1.0526 Train Acc: 0.6274 Eval Loss: 0.9489 Eval Acc: 0.6672 (LR: 0.000063)
[2025-05-04 07:57:47,951]: [LeNet5_relu6_quantized_3_bits] Epoch: 046 Train Loss: 1.0283 Train Acc: 0.6379 Eval Loss: 0.9386 Eval Acc: 0.6731 (LR: 0.000063)
[2025-05-04 07:58:16,449]: [LeNet5_relu6_quantized_3_bits] Epoch: 047 Train Loss: 1.0333 Train Acc: 0.6345 Eval Loss: 0.9429 Eval Acc: 0.6720 (LR: 0.000063)
[2025-05-04 07:58:44,320]: [LeNet5_relu6_quantized_3_bits] Epoch: 048 Train Loss: 1.0342 Train Acc: 0.6323 Eval Loss: 0.9401 Eval Acc: 0.6705 (LR: 0.000063)
[2025-05-04 07:59:11,383]: [LeNet5_relu6_quantized_3_bits] Epoch: 049 Train Loss: 1.0345 Train Acc: 0.6335 Eval Loss: 0.9416 Eval Acc: 0.6692 (LR: 0.000063)
[2025-05-04 07:59:39,768]: [LeNet5_relu6_quantized_3_bits] Epoch: 050 Train Loss: 1.0339 Train Acc: 0.6350 Eval Loss: 0.9464 Eval Acc: 0.6681 (LR: 0.000063)
[2025-05-04 08:00:08,312]: [LeNet5_relu6_quantized_3_bits] Epoch: 051 Train Loss: 1.0324 Train Acc: 0.6348 Eval Loss: 0.9287 Eval Acc: 0.6748 (LR: 0.000063)
[2025-05-04 08:00:36,621]: [LeNet5_relu6_quantized_3_bits] Epoch: 052 Train Loss: 1.0309 Train Acc: 0.6344 Eval Loss: 0.9408 Eval Acc: 0.6710 (LR: 0.000063)
[2025-05-04 08:01:04,657]: [LeNet5_relu6_quantized_3_bits] Epoch: 053 Train Loss: 1.0296 Train Acc: 0.6372 Eval Loss: 0.9449 Eval Acc: 0.6654 (LR: 0.000063)
[2025-05-04 08:01:32,678]: [LeNet5_relu6_quantized_3_bits] Epoch: 054 Train Loss: 1.0355 Train Acc: 0.6331 Eval Loss: 0.9454 Eval Acc: 0.6684 (LR: 0.000063)
[2025-05-04 08:02:01,232]: [LeNet5_relu6_quantized_3_bits] Epoch: 055 Train Loss: 1.0314 Train Acc: 0.6352 Eval Loss: 0.9488 Eval Acc: 0.6673 (LR: 0.000063)
[2025-05-04 08:02:29,253]: [LeNet5_relu6_quantized_3_bits] Epoch: 056 Train Loss: 1.0336 Train Acc: 0.6350 Eval Loss: 0.9380 Eval Acc: 0.6699 (LR: 0.000063)
[2025-05-04 08:02:57,678]: [LeNet5_relu6_quantized_3_bits] Epoch: 057 Train Loss: 1.0345 Train Acc: 0.6354 Eval Loss: 0.9412 Eval Acc: 0.6684 (LR: 0.000063)
[2025-05-04 08:03:25,334]: [LeNet5_relu6_quantized_3_bits] Epoch: 058 Train Loss: 1.0291 Train Acc: 0.6356 Eval Loss: 0.9451 Eval Acc: 0.6715 (LR: 0.000063)
[2025-05-04 08:03:53,865]: [LeNet5_relu6_quantized_3_bits] Epoch: 059 Train Loss: 1.0330 Train Acc: 0.6335 Eval Loss: 0.9428 Eval Acc: 0.6698 (LR: 0.000063)
[2025-05-04 08:04:21,374]: [LeNet5_relu6_quantized_3_bits] Epoch: 060 Train Loss: 1.0364 Train Acc: 0.6322 Eval Loss: 0.9364 Eval Acc: 0.6743 (LR: 0.000063)
[2025-05-04 08:04:21,386]: 


Quantization of model down to 3 bits finished
[2025-05-04 08:04:21,386]: Model Architecture:
[2025-05-04 08:04:21,610]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.1026, 0.0737, 0.1148, 0.1310, 0.1206, 0.1080, 0.1218, 0.0673, 0.0940,
                0.0846, 0.0770, 0.0839, 0.0714, 0.1128, 0.0697, 0.1428],
               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.3590, -0.1796, -0.4019, -0.4583, -0.4221, -0.2134, -0.4261, -0.2246,
                  -0.3269, -0.2116, -0.2695, -0.2913, -0.2241, -0.3444, -0.2239, -0.4999],
                 device='cuda:0'), max_val=tensor([0.3357, 0.2580, 0.3893, 0.2501, 0.4117, 0.3779, 0.3045, 0.2356, 0.3290,
                  0.2963, 0.2642, 0.2935, 0.2500, 0.3947, 0.2438, 0.3961],
                 device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0349, 0.0268, 0.0222, 0.0205, 0.0342, 0.0237, 0.0363, 0.0325, 0.0338,
                0.0308, 0.0355, 0.0432, 0.0328, 0.0359, 0.0287, 0.0242, 0.0305, 0.0372,
                0.0451, 0.0460, 0.0405, 0.0279, 0.0278, 0.0138, 0.0411, 0.0199, 0.0372,
                0.0313, 0.0310, 0.0243, 0.0363, 0.0329, 0.0279, 0.0342, 0.0491, 0.0297,
                0.0347, 0.0355, 0.0337, 0.0434, 0.0358, 0.0308, 0.0380, 0.0287, 0.0154,
                0.0371, 0.0397, 0.0352, 0.0407, 0.0419, 0.0308, 0.0303, 0.0396, 0.0328,
                0.0397, 0.0288, 0.0293, 0.0392, 0.0335, 0.0310, 0.0312, 0.0186, 0.0389,
                0.0283, 0.0418, 0.0298, 0.0552, 0.0481, 0.0357, 0.0149, 0.0302, 0.0340,
                0.0363, 0.0322, 0.0352, 0.0412, 0.0411, 0.0344, 0.0314, 0.0172, 0.0405,
                0.0319, 0.0162, 0.0354, 0.0368, 0.0642, 0.0378, 0.0361, 0.0351, 0.0354,
                0.0509, 0.0496, 0.0348, 0.0381, 0.0261, 0.0334, 0.0354, 0.0364, 0.0341,
                0.0371, 0.0312, 0.0313, 0.0400, 0.0320, 0.0446, 0.0337, 0.0439, 0.0369,
                0.0255, 0.0406, 0.0644, 0.0371, 0.0307, 0.0310, 0.0400, 0.0258, 0.0386,
                0.0402, 0.0337, 0.0389], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.1111, -0.0883, -0.0743, -0.0719, -0.1196, -0.0831, -0.1244, -0.1137,
                  -0.1182, -0.1080, -0.1201, -0.1513, -0.1147, -0.1258, -0.0987, -0.0848,
                  -0.1069, -0.1303, -0.1579, -0.1037, -0.1085, -0.0975, -0.0891, -0.0482,
                  -0.1417, -0.0697, -0.1302, -0.1033, -0.1083, -0.0849, -0.1269, -0.1153,
                  -0.0915, -0.1197, -0.1720, -0.1019, -0.1214, -0.1244, -0.1179, -0.1520,
                  -0.1252, -0.1077, -0.1280, -0.1004, -0.0540, -0.1260, -0.1389, -0.1233,
                  -0.1423, -0.1447, -0.1079, -0.0908, -0.1387, -0.1147, -0.1388, -0.1007,
                  -0.1026, -0.1336, -0.1136, -0.1085, -0.0909, -0.0650, -0.1360, -0.0393,
                  -0.1462, -0.0945, -0.1305, -0.1683, -0.1250, -0.0520, -0.1035, -0.0895,
                  -0.1270, -0.1128, -0.1228, -0.1441, -0.1438, -0.0878, -0.0388, -0.0603,
                  -0.0981, -0.1053, -0.0542, -0.1235, -0.1287, -0.2247, -0.1323, -0.1262,
                  -0.1227, -0.1241, -0.1780, -0.1735, -0.1219, -0.1333, -0.0913, -0.1170,
                  -0.1239, -0.1274, -0.1193, -0.1201, -0.1035, -0.1096, -0.1399, -0.1118,
                  -0.1561, -0.1068, -0.1538, -0.0880, -0.0785, -0.1421, -0.2255, -0.1297,
                  -0.0876, -0.1051, -0.1401, -0.0904, -0.1279, -0.1407, -0.1178, -0.1360],
                 device='cuda:0'), max_val=tensor([0.1222, 0.0938, 0.0778, 0.0719, 0.1061, 0.0726, 0.1269, 0.1133, 0.1182,
                  0.0956, 0.1243, 0.1055, 0.1136, 0.1034, 0.1006, 0.0844, 0.0928, 0.1262,
                  0.1071, 0.1609, 0.1419, 0.0966, 0.0972, 0.0464, 0.1438, 0.0654, 0.1126,
                  0.1096, 0.1084, 0.0771, 0.1270, 0.1123, 0.0978, 0.1198, 0.1303, 0.1040,
                  0.0948, 0.1086, 0.1068, 0.1297, 0.1148, 0.1077, 0.1330, 0.0971, 0.0491,
                  0.1298, 0.1174, 0.1183, 0.0971, 0.1465, 0.1052, 0.1062, 0.1003, 0.1064,
                  0.1387, 0.0977, 0.1023, 0.1372, 0.1171, 0.1062, 0.1092, 0.0650, 0.1119,
                  0.0989, 0.1259, 0.1044, 0.1931, 0.1497, 0.0964, 0.0466, 0.1057, 0.1190,
                  0.1264, 0.1034, 0.1231, 0.1282, 0.1119, 0.1206, 0.1101, 0.0604, 0.1419,
                  0.1118, 0.0566, 0.1240, 0.0950, 0.0913, 0.1323, 0.1070, 0.1226, 0.1189,
                  0.1364, 0.1707, 0.0977, 0.0854, 0.0756, 0.1088, 0.0946, 0.1127, 0.0890,
                  0.1297, 0.1092, 0.1060, 0.1393, 0.1094, 0.1108, 0.1181, 0.1282, 0.1293,
                  0.0893, 0.1123, 0.1061, 0.1002, 0.1074, 0.1083, 0.1304, 0.0853, 0.1352,
                  0.1295, 0.1107, 0.1076], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0392, 0.0447, 0.0375, 0.0394, 0.0493, 0.0517, 0.0570, 0.0250, 0.0466,
                0.0246, 0.0429, 0.0508, 0.0540, 0.0461, 0.0432, 0.0495, 0.0562, 0.0272,
                0.0414, 0.0405, 0.0500, 0.0676, 0.0637, 0.0291, 0.0593, 0.0778, 0.0665,
                0.0250, 0.0455, 0.0587, 0.0425, 0.0480, 0.0509, 0.0430, 0.0758, 0.0562,
                0.0611, 0.0475, 0.0512, 0.0570, 0.0250, 0.0507, 0.0250, 0.0418, 0.0517,
                0.0486, 0.0566, 0.0818, 0.0506, 0.0381, 0.0673, 0.0576, 0.0707, 0.0259,
                0.0373, 0.0276, 0.0466, 0.0641, 0.0384, 0.0497, 0.0461, 0.0514, 0.0667,
                0.0511, 0.0404, 0.0556, 0.0445, 0.0489, 0.0512, 0.0248, 0.0510, 0.0641,
                0.0442, 0.0497, 0.0409, 0.0430, 0.0447, 0.0517, 0.0505, 0.0568, 0.0535,
                0.0457, 0.0659, 0.0250], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.1170, -0.1564, -0.1200, -0.1154, -0.1727, -0.1442, -0.1487, -0.0874,
                  -0.1541, -0.0862, -0.1499, -0.1485, -0.1889, -0.1506, -0.1513, -0.1677,
                  -0.1966, -0.0951, -0.1450, -0.1419, -0.1749, -0.1685, -0.1649, -0.1020,
                  -0.1713, -0.1681, -0.1664, -0.0875, -0.1593, -0.2056, -0.1469, -0.1679,
                  -0.1304, -0.1397, -0.1922, -0.1747, -0.1612, -0.1377, -0.1543, -0.1778,
                  -0.0873, -0.1646, -0.0874, -0.1439, -0.1383, -0.1702, -0.1445, -0.1737,
                  -0.1709, -0.1042, -0.1943, -0.1923, -0.2475, -0.0906, -0.1304, -0.0884,
                  -0.1538, -0.1493, -0.1342, -0.1737, -0.1612, -0.1798, -0.1630, -0.1627,
                  -0.1415, -0.1912, -0.1558, -0.1600, -0.1350, -0.0863, -0.1785, -0.2078,
                  -0.1540, -0.1736, -0.1362, -0.1285, -0.1158, -0.1720, -0.1768, -0.1177,
                  -0.1871, -0.1599, -0.1320, -0.0860], device='cuda:0'), max_val=tensor([0.1373, 0.1564, 0.1311, 0.1380, 0.1726, 0.1808, 0.1996, 0.0870, 0.1632,
                  0.0852, 0.1502, 0.1778, 0.1836, 0.1612, 0.1486, 0.1731, 0.1702, 0.0855,
                  0.1281, 0.1308, 0.1287, 0.2365, 0.2231, 0.1012, 0.2074, 0.2724, 0.2329,
                  0.0822, 0.1384, 0.2055, 0.1486, 0.1552, 0.1780, 0.1505, 0.2655, 0.1968,
                  0.2137, 0.1664, 0.1794, 0.1996, 0.0847, 0.1773, 0.0837, 0.1462, 0.1810,
                  0.1532, 0.1980, 0.2862, 0.1773, 0.1333, 0.2356, 0.2015, 0.2051, 0.0850,
                  0.1295, 0.0965, 0.1630, 0.2244, 0.1322, 0.1738, 0.1612, 0.1799, 0.2334,
                  0.1787, 0.1375, 0.1945, 0.1558, 0.1711, 0.1792, 0.0868, 0.1767, 0.2244,
                  0.1547, 0.1738, 0.1433, 0.1504, 0.1566, 0.1809, 0.1508, 0.1988, 0.1828,
                  0.1325, 0.2306, 0.0875], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-04 08:04:21,610]: 
Model Parameters with Weights:
[2025-05-04 08:04:21,611]: 
Parameter: conv1.0.weight
[2025-05-04 08:04:21,611]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-04 08:04:21,611]: Sample Values (16 elements): [0.12902279198169708, 0.07494311034679413, 0.13732603192329407, -0.03687477856874466, 0.07164928317070007, 0.22388164699077606, 0.016021432355046272, -0.1454993039369583, 0.26357367634773254, -0.003319039009511471, 0.3130440413951874, -0.12564852833747864, 0.08089607208967209, -0.15535682439804077, 0.18141232430934906, -0.031314682215452194]
[2025-05-04 08:04:21,612]: Mean: -0.0003
[2025-05-04 08:04:21,613]: Std: 0.2059
[2025-05-04 08:04:21,613]: Min: -0.9232
[2025-05-04 08:04:21,613]: Max: 0.8457
[2025-05-04 08:04:21,614]: 
Parameter: conv1.0.bias
[2025-05-04 08:04:21,614]: Shape: torch.Size([6])
[2025-05-04 08:04:21,616]: Sample Values (6 elements): [0.6521280407905579, 0.5457033514976501, -0.16213935613632202, -0.05884804576635361, -0.10118132084608078, 0.012562542222440243]
[2025-05-04 08:04:21,616]: Mean: 0.1480
[2025-05-04 08:04:21,616]: Std: 0.3554
[2025-05-04 08:04:21,617]: Min: -0.1621
[2025-05-04 08:04:21,617]: Max: 0.6521
[2025-05-04 08:04:21,617]: 
Parameter: conv2.0.weight
[2025-05-04 08:04:21,618]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-04 08:04:21,618]: Sample Values (6 elements): [-0.022381771355867386, -0.03812351077795029, -0.052815306931734085, -0.029837951064109802, 0.1576039046049118, 0.041505035012960434]
[2025-05-04 08:04:21,619]: Mean: 0.0010
[2025-05-04 08:04:21,619]: Std: 0.1169
[2025-05-04 08:04:21,620]: Min: -0.5000
[2025-05-04 08:04:21,620]: Max: 0.4117
[2025-05-04 08:04:21,621]: 
Parameter: conv2.0.bias
[2025-05-04 08:04:21,621]: Shape: torch.Size([16])
[2025-05-04 08:04:21,621]: Sample Values (6 elements): [-0.11488048732280731, -0.29056835174560547, 0.20012697577476501, 0.2403680831193924, 0.16754166781902313, -0.14584942162036896]
[2025-05-04 08:04:21,622]: Mean: -0.0266
[2025-05-04 08:04:21,622]: Std: 0.2097
[2025-05-04 08:04:21,624]: Min: -0.4279
[2025-05-04 08:04:21,625]: Max: 0.2426
[2025-05-04 08:04:21,625]: 
Parameter: fc1.0.weight
[2025-05-04 08:04:21,625]: Shape: torch.Size([120, 400])
[2025-05-04 08:04:21,627]: Sample Values (6 elements): [-0.019394855946302414, -0.12600773572921753, 0.047283440828323364, 0.0072151836939156055, 0.06846366077661514, -0.014751987531781197]
[2025-05-04 08:04:21,628]: Mean: 0.0013
[2025-05-04 08:04:21,629]: Std: 0.0399
[2025-05-04 08:04:21,630]: Min: -0.2256
[2025-05-04 08:04:21,631]: Max: 0.1931
[2025-05-04 08:04:21,631]: 
Parameter: fc1.0.bias
[2025-05-04 08:04:21,631]: Shape: torch.Size([120])
[2025-05-04 08:04:21,632]: Sample Values (6 elements): [0.07044269144535065, -0.03322778269648552, -0.08374772220849991, -0.01927311159670353, -0.042756568640470505, 0.024736054241657257]
[2025-05-04 08:04:21,633]: Mean: 0.0000
[2025-05-04 08:04:21,634]: Std: 0.0366
[2025-05-04 08:04:21,635]: Min: -0.0837
[2025-05-04 08:04:21,636]: Max: 0.0853
[2025-05-04 08:04:21,636]: 
Parameter: fc2.0.weight
[2025-05-04 08:04:21,636]: Shape: torch.Size([84, 120])
[2025-05-04 08:04:21,639]: Sample Values (6 elements): [0.15306240320205688, 0.09141536056995392, 0.0445600226521492, -0.027159813791513443, -0.010769960470497608, -0.029506905004382133]
[2025-05-04 08:04:21,644]: Mean: 0.0007
[2025-05-04 08:04:21,647]: Std: 0.0686
[2025-05-04 08:04:21,652]: Min: -0.2475
[2025-05-04 08:04:21,666]: Max: 0.2862
[2025-05-04 08:04:21,666]: 
Parameter: fc2.0.bias
[2025-05-04 08:04:21,666]: Shape: torch.Size([84])
[2025-05-04 08:04:21,677]: Sample Values (6 elements): [0.051595769822597504, 0.09966897964477539, 0.01486222818493843, 0.0799943134188652, -0.05542661249637604, -0.035816408693790436]
[2025-05-04 08:04:21,681]: Mean: 0.0154
[2025-05-04 08:04:21,681]: Std: 0.0613
[2025-05-04 08:04:21,682]: Min: -0.1103
[2025-05-04 08:04:21,682]: Max: 0.1309
[2025-05-04 08:04:21,682]: 
Parameter: fc3.weight
[2025-05-04 08:04:21,682]: Shape: torch.Size([10, 84])
[2025-05-04 08:04:21,684]: Sample Values (6 elements): [-0.0426558218896389, -0.08309335261583328, -0.058002322912216187, -0.013399865478277206, -0.05131227895617485, -0.0494404211640358]
[2025-05-04 08:04:21,684]: Mean: 0.0010
[2025-05-04 08:04:21,684]: Std: 0.1078
[2025-05-04 08:04:21,685]: Min: -0.2667
[2025-05-04 08:04:21,687]: Max: 0.3505
[2025-05-04 08:04:21,688]: 
Parameter: fc3.bias
[2025-05-04 08:04:21,688]: Shape: torch.Size([10])
[2025-05-04 08:04:21,690]: Sample Values (6 elements): [-0.2451523095369339, -0.12358126044273376, 0.2459585964679718, 0.1311017870903015, -0.012564617209136486, 0.14465172588825226]
[2025-05-04 08:04:21,694]: Mean: 0.0140
[2025-05-04 08:04:21,699]: Std: 0.1753
[2025-05-04 08:04:21,709]: Min: -0.2779
[2025-05-04 08:04:21,718]: Max: 0.2460
[2025-05-04 08:04:21,718]: 


QAT of LeNet5 with relu6 down to 2 bits...
[2025-05-04 08:04:21,780]: [LeNet5_relu6_quantized_2_bits] after configure_qat:
[2025-05-04 08:04:21,832]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU6(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): ReLU6(
      inplace=True
      (activation_post_process): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-04 08:04:50,122]: [LeNet5_relu6_quantized_2_bits] Epoch: 001 Train Loss: 1.4710 Train Acc: 0.4891 Eval Loss: 1.3095 Eval Acc: 0.5355 (LR: 0.001000)
[2025-05-04 08:05:17,504]: [LeNet5_relu6_quantized_2_bits] Epoch: 002 Train Loss: 1.3883 Train Acc: 0.5093 Eval Loss: 1.2684 Eval Acc: 0.5492 (LR: 0.001000)
[2025-05-04 08:05:45,090]: [LeNet5_relu6_quantized_2_bits] Epoch: 003 Train Loss: 1.3622 Train Acc: 0.5194 Eval Loss: 1.2439 Eval Acc: 0.5579 (LR: 0.001000)
[2025-05-04 08:06:13,826]: [LeNet5_relu6_quantized_2_bits] Epoch: 004 Train Loss: 1.3628 Train Acc: 0.5149 Eval Loss: 1.2404 Eval Acc: 0.5583 (LR: 0.001000)
[2025-05-04 08:06:41,408]: [LeNet5_relu6_quantized_2_bits] Epoch: 005 Train Loss: 1.3490 Train Acc: 0.5219 Eval Loss: 1.2600 Eval Acc: 0.5520 (LR: 0.001000)
[2025-05-04 08:07:09,231]: [LeNet5_relu6_quantized_2_bits] Epoch: 006 Train Loss: 1.3319 Train Acc: 0.5260 Eval Loss: 1.2310 Eval Acc: 0.5603 (LR: 0.001000)
[2025-05-04 08:07:35,950]: [LeNet5_relu6_quantized_2_bits] Epoch: 007 Train Loss: 1.3310 Train Acc: 0.5254 Eval Loss: 1.2952 Eval Acc: 0.5399 (LR: 0.001000)
[2025-05-04 08:08:02,440]: [LeNet5_relu6_quantized_2_bits] Epoch: 008 Train Loss: 1.3271 Train Acc: 0.5275 Eval Loss: 1.2088 Eval Acc: 0.5713 (LR: 0.001000)
[2025-05-04 08:08:29,396]: [LeNet5_relu6_quantized_2_bits] Epoch: 009 Train Loss: 1.3156 Train Acc: 0.5302 Eval Loss: 1.1950 Eval Acc: 0.5759 (LR: 0.001000)
[2025-05-04 08:08:55,698]: [LeNet5_relu6_quantized_2_bits] Epoch: 010 Train Loss: 1.3179 Train Acc: 0.5312 Eval Loss: 1.1981 Eval Acc: 0.5706 (LR: 0.001000)
[2025-05-04 08:09:22,253]: [LeNet5_relu6_quantized_2_bits] Epoch: 011 Train Loss: 1.3106 Train Acc: 0.5332 Eval Loss: 1.2146 Eval Acc: 0.5668 (LR: 0.001000)
[2025-05-04 08:09:48,566]: [LeNet5_relu6_quantized_2_bits] Epoch: 012 Train Loss: 1.2991 Train Acc: 0.5365 Eval Loss: 1.1772 Eval Acc: 0.5787 (LR: 0.001000)
[2025-05-04 08:10:37,045]: [LeNet5_relu6_quantized_2_bits] Epoch: 013 Train Loss: 1.2954 Train Acc: 0.5361 Eval Loss: 1.1816 Eval Acc: 0.5780 (LR: 0.001000)
[2025-05-04 08:11:38,481]: [LeNet5_relu6_quantized_2_bits] Epoch: 014 Train Loss: 1.2940 Train Acc: 0.5436 Eval Loss: 1.1999 Eval Acc: 0.5737 (LR: 0.001000)
[2025-05-04 08:12:25,980]: [LeNet5_relu6_quantized_2_bits] Epoch: 015 Train Loss: 1.2947 Train Acc: 0.5406 Eval Loss: 1.1820 Eval Acc: 0.5760 (LR: 0.001000)
[2025-05-04 08:13:04,328]: [LeNet5_relu6_quantized_2_bits] Epoch: 016 Train Loss: 1.2889 Train Acc: 0.5430 Eval Loss: 1.1781 Eval Acc: 0.5815 (LR: 0.001000)
[2025-05-04 08:13:46,954]: [LeNet5_relu6_quantized_2_bits] Epoch: 017 Train Loss: 1.2819 Train Acc: 0.5443 Eval Loss: 1.1882 Eval Acc: 0.5732 (LR: 0.001000)
[2025-05-04 08:14:32,242]: [LeNet5_relu6_quantized_2_bits] Epoch: 018 Train Loss: 1.2913 Train Acc: 0.5402 Eval Loss: 1.1950 Eval Acc: 0.5780 (LR: 0.001000)
[2025-05-04 08:15:12,753]: [LeNet5_relu6_quantized_2_bits] Epoch: 019 Train Loss: 1.2899 Train Acc: 0.5417 Eval Loss: 1.2552 Eval Acc: 0.5541 (LR: 0.001000)
[2025-05-04 08:15:52,048]: [LeNet5_relu6_quantized_2_bits] Epoch: 020 Train Loss: 1.2811 Train Acc: 0.5477 Eval Loss: 1.1904 Eval Acc: 0.5743 (LR: 0.001000)
[2025-05-04 08:16:30,161]: [LeNet5_relu6_quantized_2_bits] Epoch: 021 Train Loss: 1.2847 Train Acc: 0.5441 Eval Loss: 1.2060 Eval Acc: 0.5744 (LR: 0.001000)
[2025-05-04 08:17:09,406]: [LeNet5_relu6_quantized_2_bits] Epoch: 022 Train Loss: 1.2844 Train Acc: 0.5410 Eval Loss: 1.1711 Eval Acc: 0.5794 (LR: 0.001000)
[2025-05-04 08:17:50,935]: [LeNet5_relu6_quantized_2_bits] Epoch: 023 Train Loss: 1.2834 Train Acc: 0.5427 Eval Loss: 1.2035 Eval Acc: 0.5735 (LR: 0.001000)
[2025-05-04 08:18:30,598]: [LeNet5_relu6_quantized_2_bits] Epoch: 024 Train Loss: 1.2881 Train Acc: 0.5406 Eval Loss: 1.1619 Eval Acc: 0.5824 (LR: 0.001000)
[2025-05-04 08:19:10,411]: [LeNet5_relu6_quantized_2_bits] Epoch: 025 Train Loss: 1.2755 Train Acc: 0.5461 Eval Loss: 1.1904 Eval Acc: 0.5759 (LR: 0.001000)
[2025-05-04 08:19:49,040]: [LeNet5_relu6_quantized_2_bits] Epoch: 026 Train Loss: 1.2667 Train Acc: 0.5492 Eval Loss: 1.1601 Eval Acc: 0.5948 (LR: 0.001000)
[2025-05-04 08:20:29,058]: [LeNet5_relu6_quantized_2_bits] Epoch: 027 Train Loss: 1.2786 Train Acc: 0.5461 Eval Loss: 1.1655 Eval Acc: 0.5855 (LR: 0.001000)
[2025-05-04 08:21:10,143]: [LeNet5_relu6_quantized_2_bits] Epoch: 028 Train Loss: 1.2657 Train Acc: 0.5506 Eval Loss: 1.1520 Eval Acc: 0.5913 (LR: 0.001000)
[2025-05-04 08:21:51,760]: [LeNet5_relu6_quantized_2_bits] Epoch: 029 Train Loss: 1.2743 Train Acc: 0.5468 Eval Loss: 1.1585 Eval Acc: 0.5850 (LR: 0.001000)
[2025-05-04 08:22:33,179]: [LeNet5_relu6_quantized_2_bits] Epoch: 030 Train Loss: 1.2692 Train Acc: 0.5487 Eval Loss: 1.1547 Eval Acc: 0.5882 (LR: 0.000250)
[2025-05-04 08:23:12,441]: [LeNet5_relu6_quantized_2_bits] Epoch: 031 Train Loss: 1.2313 Train Acc: 0.5627 Eval Loss: 1.1467 Eval Acc: 0.5949 (LR: 0.000250)
[2025-05-04 08:23:52,034]: [LeNet5_relu6_quantized_2_bits] Epoch: 032 Train Loss: 1.2288 Train Acc: 0.5626 Eval Loss: 1.1250 Eval Acc: 0.5975 (LR: 0.000250)
[2025-05-04 08:24:33,339]: [LeNet5_relu6_quantized_2_bits] Epoch: 033 Train Loss: 1.2388 Train Acc: 0.5610 Eval Loss: 1.1168 Eval Acc: 0.6069 (LR: 0.000250)
[2025-05-04 08:25:20,446]: [LeNet5_relu6_quantized_2_bits] Epoch: 034 Train Loss: 1.2353 Train Acc: 0.5635 Eval Loss: 1.1182 Eval Acc: 0.6037 (LR: 0.000250)
