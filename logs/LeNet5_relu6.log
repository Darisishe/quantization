[2025-05-26 22:40:29,737]: 
Training LeNet5 with relu6
[2025-05-26 22:41:03,832]: [LeNet5_relu6] Epoch: 001 Train Loss: 1.7941 Train Acc: 0.3357 Eval Loss: 1.5527 Eval Acc: 0.4433 (LR: 0.00100000)
[2025-05-26 22:41:34,802]: [LeNet5_relu6] Epoch: 002 Train Loss: 1.5520 Train Acc: 0.4285 Eval Loss: 1.3910 Eval Acc: 0.4885 (LR: 0.00100000)
[2025-05-26 22:42:02,380]: [LeNet5_relu6] Epoch: 003 Train Loss: 1.4591 Train Acc: 0.4697 Eval Loss: 1.3045 Eval Acc: 0.5363 (LR: 0.00100000)
[2025-05-26 22:42:29,736]: [LeNet5_relu6] Epoch: 004 Train Loss: 1.3905 Train Acc: 0.4953 Eval Loss: 1.2352 Eval Acc: 0.5600 (LR: 0.00100000)
[2025-05-26 22:42:58,994]: [LeNet5_relu6] Epoch: 005 Train Loss: 1.3354 Train Acc: 0.5181 Eval Loss: 1.2282 Eval Acc: 0.5653 (LR: 0.00100000)
[2025-05-26 22:43:30,367]: [LeNet5_relu6] Epoch: 006 Train Loss: 1.2955 Train Acc: 0.5337 Eval Loss: 1.1597 Eval Acc: 0.5851 (LR: 0.00100000)
[2025-05-26 22:44:01,763]: [LeNet5_relu6] Epoch: 007 Train Loss: 1.2619 Train Acc: 0.5467 Eval Loss: 1.1855 Eval Acc: 0.5740 (LR: 0.00100000)
[2025-05-26 22:44:32,703]: [LeNet5_relu6] Epoch: 008 Train Loss: 1.2345 Train Acc: 0.5581 Eval Loss: 1.1261 Eval Acc: 0.6023 (LR: 0.00100000)
[2025-05-26 22:45:03,584]: [LeNet5_relu6] Epoch: 009 Train Loss: 1.2161 Train Acc: 0.5648 Eval Loss: 1.1109 Eval Acc: 0.6063 (LR: 0.00100000)
[2025-05-26 22:45:34,693]: [LeNet5_relu6] Epoch: 010 Train Loss: 1.1958 Train Acc: 0.5733 Eval Loss: 1.1235 Eval Acc: 0.5970 (LR: 0.00100000)
[2025-05-26 22:46:05,513]: [LeNet5_relu6] Epoch: 011 Train Loss: 1.1833 Train Acc: 0.5761 Eval Loss: 1.0681 Eval Acc: 0.6209 (LR: 0.00100000)
[2025-05-26 22:46:35,963]: [LeNet5_relu6] Epoch: 012 Train Loss: 1.1633 Train Acc: 0.5850 Eval Loss: 1.0515 Eval Acc: 0.6271 (LR: 0.00100000)
[2025-05-26 22:47:06,751]: [LeNet5_relu6] Epoch: 013 Train Loss: 1.1542 Train Acc: 0.5887 Eval Loss: 1.0588 Eval Acc: 0.6225 (LR: 0.00100000)
[2025-05-26 22:47:37,334]: [LeNet5_relu6] Epoch: 014 Train Loss: 1.1407 Train Acc: 0.5913 Eval Loss: 1.0510 Eval Acc: 0.6274 (LR: 0.00100000)
[2025-05-26 22:48:08,027]: [LeNet5_relu6] Epoch: 015 Train Loss: 1.1312 Train Acc: 0.5970 Eval Loss: 1.0647 Eval Acc: 0.6218 (LR: 0.00100000)
[2025-05-26 22:48:38,765]: [LeNet5_relu6] Epoch: 016 Train Loss: 1.1268 Train Acc: 0.5988 Eval Loss: 1.0238 Eval Acc: 0.6377 (LR: 0.00100000)
[2025-05-26 22:49:09,657]: [LeNet5_relu6] Epoch: 017 Train Loss: 1.1118 Train Acc: 0.6047 Eval Loss: 1.0334 Eval Acc: 0.6321 (LR: 0.00100000)
[2025-05-26 22:49:40,382]: [LeNet5_relu6] Epoch: 018 Train Loss: 1.1108 Train Acc: 0.6047 Eval Loss: 1.0166 Eval Acc: 0.6389 (LR: 0.00100000)
[2025-05-26 22:50:11,618]: [LeNet5_relu6] Epoch: 019 Train Loss: 1.0975 Train Acc: 0.6089 Eval Loss: 0.9939 Eval Acc: 0.6466 (LR: 0.00100000)
[2025-05-26 22:50:42,302]: [LeNet5_relu6] Epoch: 020 Train Loss: 1.0943 Train Acc: 0.6098 Eval Loss: 1.0260 Eval Acc: 0.6380 (LR: 0.00100000)
[2025-05-26 22:51:12,590]: [LeNet5_relu6] Epoch: 021 Train Loss: 1.0861 Train Acc: 0.6143 Eval Loss: 0.9890 Eval Acc: 0.6417 (LR: 0.00100000)
[2025-05-26 22:51:42,727]: [LeNet5_relu6] Epoch: 022 Train Loss: 1.0747 Train Acc: 0.6170 Eval Loss: 1.0112 Eval Acc: 0.6397 (LR: 0.00100000)
[2025-05-26 22:52:14,215]: [LeNet5_relu6] Epoch: 023 Train Loss: 1.0699 Train Acc: 0.6171 Eval Loss: 0.9828 Eval Acc: 0.6488 (LR: 0.00100000)
[2025-05-26 22:52:44,662]: [LeNet5_relu6] Epoch: 024 Train Loss: 1.0686 Train Acc: 0.6210 Eval Loss: 0.9713 Eval Acc: 0.6572 (LR: 0.00100000)
[2025-05-26 22:53:14,697]: [LeNet5_relu6] Epoch: 025 Train Loss: 1.0651 Train Acc: 0.6205 Eval Loss: 0.9939 Eval Acc: 0.6471 (LR: 0.00100000)
[2025-05-26 22:53:44,571]: [LeNet5_relu6] Epoch: 026 Train Loss: 1.0582 Train Acc: 0.6219 Eval Loss: 0.9920 Eval Acc: 0.6431 (LR: 0.00100000)
[2025-05-26 22:54:14,666]: [LeNet5_relu6] Epoch: 027 Train Loss: 1.0589 Train Acc: 0.6212 Eval Loss: 0.9668 Eval Acc: 0.6538 (LR: 0.00100000)
[2025-05-26 22:54:44,511]: [LeNet5_relu6] Epoch: 028 Train Loss: 1.0481 Train Acc: 0.6270 Eval Loss: 0.9615 Eval Acc: 0.6536 (LR: 0.00100000)
[2025-05-26 22:55:14,261]: [LeNet5_relu6] Epoch: 029 Train Loss: 1.0510 Train Acc: 0.6245 Eval Loss: 0.9406 Eval Acc: 0.6640 (LR: 0.00100000)
[2025-05-26 22:55:44,008]: [LeNet5_relu6] Epoch: 030 Train Loss: 1.0428 Train Acc: 0.6291 Eval Loss: 0.9560 Eval Acc: 0.6562 (LR: 0.00100000)
[2025-05-26 22:56:14,118]: [LeNet5_relu6] Epoch: 031 Train Loss: 1.0436 Train Acc: 0.6269 Eval Loss: 0.9671 Eval Acc: 0.6551 (LR: 0.00100000)
[2025-05-26 22:56:43,331]: [LeNet5_relu6] Epoch: 032 Train Loss: 1.0342 Train Acc: 0.6314 Eval Loss: 0.9443 Eval Acc: 0.6659 (LR: 0.00100000)
[2025-05-26 22:57:12,229]: [LeNet5_relu6] Epoch: 033 Train Loss: 1.0350 Train Acc: 0.6347 Eval Loss: 0.9789 Eval Acc: 0.6506 (LR: 0.00100000)
[2025-05-26 22:57:41,032]: [LeNet5_relu6] Epoch: 034 Train Loss: 1.0309 Train Acc: 0.6345 Eval Loss: 0.9259 Eval Acc: 0.6699 (LR: 0.00100000)
[2025-05-26 22:58:10,251]: [LeNet5_relu6] Epoch: 035 Train Loss: 1.0279 Train Acc: 0.6346 Eval Loss: 0.9839 Eval Acc: 0.6537 (LR: 0.00100000)
[2025-05-26 22:58:39,067]: [LeNet5_relu6] Epoch: 036 Train Loss: 1.0284 Train Acc: 0.6350 Eval Loss: 0.9578 Eval Acc: 0.6598 (LR: 0.00100000)
[2025-05-26 22:59:07,942]: [LeNet5_relu6] Epoch: 037 Train Loss: 1.0228 Train Acc: 0.6342 Eval Loss: 0.9525 Eval Acc: 0.6648 (LR: 0.00100000)
[2025-05-26 22:59:36,665]: [LeNet5_relu6] Epoch: 038 Train Loss: 1.0245 Train Acc: 0.6363 Eval Loss: 0.9262 Eval Acc: 0.6756 (LR: 0.00100000)
[2025-05-26 23:00:05,731]: [LeNet5_relu6] Epoch: 039 Train Loss: 1.0181 Train Acc: 0.6390 Eval Loss: 0.9224 Eval Acc: 0.6688 (LR: 0.00100000)
[2025-05-26 23:00:35,049]: [LeNet5_relu6] Epoch: 040 Train Loss: 1.0145 Train Acc: 0.6404 Eval Loss: 0.9243 Eval Acc: 0.6721 (LR: 0.00100000)
[2025-05-26 23:01:04,979]: [LeNet5_relu6] Epoch: 041 Train Loss: 1.0089 Train Acc: 0.6403 Eval Loss: 0.9270 Eval Acc: 0.6710 (LR: 0.00100000)
[2025-05-26 23:01:35,871]: [LeNet5_relu6] Epoch: 042 Train Loss: 1.0089 Train Acc: 0.6421 Eval Loss: 0.9030 Eval Acc: 0.6767 (LR: 0.00100000)
[2025-05-26 23:02:06,801]: [LeNet5_relu6] Epoch: 043 Train Loss: 1.0098 Train Acc: 0.6423 Eval Loss: 0.9312 Eval Acc: 0.6660 (LR: 0.00100000)
[2025-05-26 23:02:35,790]: [LeNet5_relu6] Epoch: 044 Train Loss: 1.0029 Train Acc: 0.6427 Eval Loss: 0.9136 Eval Acc: 0.6726 (LR: 0.00100000)
[2025-05-26 23:03:04,407]: [LeNet5_relu6] Epoch: 045 Train Loss: 0.9989 Train Acc: 0.6447 Eval Loss: 0.8978 Eval Acc: 0.6788 (LR: 0.00100000)
[2025-05-26 23:03:31,773]: [LeNet5_relu6] Epoch: 046 Train Loss: 1.0020 Train Acc: 0.6428 Eval Loss: 0.9126 Eval Acc: 0.6713 (LR: 0.00100000)
[2025-05-26 23:04:01,292]: [LeNet5_relu6] Epoch: 047 Train Loss: 0.9987 Train Acc: 0.6454 Eval Loss: 0.9229 Eval Acc: 0.6712 (LR: 0.00100000)
[2025-05-26 23:04:30,990]: [LeNet5_relu6] Epoch: 048 Train Loss: 0.9914 Train Acc: 0.6459 Eval Loss: 0.9034 Eval Acc: 0.6810 (LR: 0.00100000)
[2025-05-26 23:04:58,009]: [LeNet5_relu6] Epoch: 049 Train Loss: 0.9946 Train Acc: 0.6457 Eval Loss: 0.9082 Eval Acc: 0.6780 (LR: 0.00100000)
[2025-05-26 23:05:24,994]: [LeNet5_relu6] Epoch: 050 Train Loss: 0.9870 Train Acc: 0.6498 Eval Loss: 0.8976 Eval Acc: 0.6792 (LR: 0.00100000)
[2025-05-26 23:05:51,941]: [LeNet5_relu6] Epoch: 051 Train Loss: 0.9889 Train Acc: 0.6470 Eval Loss: 0.9038 Eval Acc: 0.6799 (LR: 0.00100000)
[2025-05-26 23:06:18,868]: [LeNet5_relu6] Epoch: 052 Train Loss: 0.9865 Train Acc: 0.6510 Eval Loss: 0.8957 Eval Acc: 0.6834 (LR: 0.00100000)
[2025-05-26 23:06:45,652]: [LeNet5_relu6] Epoch: 053 Train Loss: 0.9846 Train Acc: 0.6484 Eval Loss: 0.8816 Eval Acc: 0.6889 (LR: 0.00100000)
[2025-05-26 23:07:12,536]: [LeNet5_relu6] Epoch: 054 Train Loss: 0.9803 Train Acc: 0.6545 Eval Loss: 0.8844 Eval Acc: 0.6886 (LR: 0.00100000)
[2025-05-26 23:07:39,385]: [LeNet5_relu6] Epoch: 055 Train Loss: 0.9797 Train Acc: 0.6508 Eval Loss: 0.8922 Eval Acc: 0.6832 (LR: 0.00100000)
[2025-05-26 23:08:06,099]: [LeNet5_relu6] Epoch: 056 Train Loss: 0.9743 Train Acc: 0.6548 Eval Loss: 0.8958 Eval Acc: 0.6836 (LR: 0.00100000)
[2025-05-26 23:08:32,911]: [LeNet5_relu6] Epoch: 057 Train Loss: 0.9832 Train Acc: 0.6517 Eval Loss: 0.9049 Eval Acc: 0.6790 (LR: 0.00100000)
[2025-05-26 23:08:59,583]: [LeNet5_relu6] Epoch: 058 Train Loss: 0.9819 Train Acc: 0.6518 Eval Loss: 0.8889 Eval Acc: 0.6835 (LR: 0.00100000)
[2025-05-26 23:09:26,136]: [LeNet5_relu6] Epoch: 059 Train Loss: 0.9766 Train Acc: 0.6550 Eval Loss: 0.8746 Eval Acc: 0.6944 (LR: 0.00100000)
[2025-05-26 23:09:53,136]: [LeNet5_relu6] Epoch: 060 Train Loss: 0.9791 Train Acc: 0.6514 Eval Loss: 0.8741 Eval Acc: 0.6918 (LR: 0.00100000)
[2025-05-26 23:10:19,572]: [LeNet5_relu6] Epoch: 061 Train Loss: 0.9718 Train Acc: 0.6541 Eval Loss: 0.8900 Eval Acc: 0.6842 (LR: 0.00100000)
[2025-05-26 23:10:45,910]: [LeNet5_relu6] Epoch: 062 Train Loss: 0.9751 Train Acc: 0.6529 Eval Loss: 0.8915 Eval Acc: 0.6860 (LR: 0.00100000)
[2025-05-26 23:11:12,320]: [LeNet5_relu6] Epoch: 063 Train Loss: 0.9752 Train Acc: 0.6527 Eval Loss: 0.8789 Eval Acc: 0.6865 (LR: 0.00100000)
[2025-05-26 23:11:38,743]: [LeNet5_relu6] Epoch: 064 Train Loss: 0.9672 Train Acc: 0.6563 Eval Loss: 0.8768 Eval Acc: 0.6851 (LR: 0.00100000)
[2025-05-26 23:12:05,157]: [LeNet5_relu6] Epoch: 065 Train Loss: 0.9732 Train Acc: 0.6544 Eval Loss: 0.8879 Eval Acc: 0.6880 (LR: 0.00100000)
[2025-05-26 23:12:31,616]: [LeNet5_relu6] Epoch: 066 Train Loss: 0.9640 Train Acc: 0.6570 Eval Loss: 0.9087 Eval Acc: 0.6808 (LR: 0.00100000)
[2025-05-26 23:12:58,007]: [LeNet5_relu6] Epoch: 067 Train Loss: 0.9677 Train Acc: 0.6563 Eval Loss: 0.9037 Eval Acc: 0.6810 (LR: 0.00100000)
[2025-05-26 23:13:24,561]: [LeNet5_relu6] Epoch: 068 Train Loss: 0.9664 Train Acc: 0.6575 Eval Loss: 0.8591 Eval Acc: 0.6960 (LR: 0.00100000)
[2025-05-26 23:13:51,306]: [LeNet5_relu6] Epoch: 069 Train Loss: 0.9647 Train Acc: 0.6601 Eval Loss: 0.8805 Eval Acc: 0.6887 (LR: 0.00100000)
[2025-05-26 23:14:17,920]: [LeNet5_relu6] Epoch: 070 Train Loss: 0.9625 Train Acc: 0.6598 Eval Loss: 0.8664 Eval Acc: 0.6943 (LR: 0.00100000)
[2025-05-26 23:14:44,492]: [LeNet5_relu6] Epoch: 071 Train Loss: 0.9653 Train Acc: 0.6561 Eval Loss: 0.8625 Eval Acc: 0.6972 (LR: 0.00100000)
[2025-05-26 23:15:10,731]: [LeNet5_relu6] Epoch: 072 Train Loss: 0.9624 Train Acc: 0.6593 Eval Loss: 0.8761 Eval Acc: 0.6900 (LR: 0.00100000)
[2025-05-26 23:15:37,113]: [LeNet5_relu6] Epoch: 073 Train Loss: 0.9621 Train Acc: 0.6597 Eval Loss: 0.8633 Eval Acc: 0.6959 (LR: 0.00100000)
[2025-05-26 23:16:03,605]: [LeNet5_relu6] Epoch: 074 Train Loss: 0.9561 Train Acc: 0.6619 Eval Loss: 0.8650 Eval Acc: 0.6971 (LR: 0.00100000)
[2025-05-26 23:16:30,085]: [LeNet5_relu6] Epoch: 075 Train Loss: 0.9577 Train Acc: 0.6596 Eval Loss: 0.8680 Eval Acc: 0.6959 (LR: 0.00100000)
[2025-05-26 23:16:56,686]: [LeNet5_relu6] Epoch: 076 Train Loss: 0.9560 Train Acc: 0.6599 Eval Loss: 0.8780 Eval Acc: 0.6930 (LR: 0.00100000)
[2025-05-26 23:17:23,283]: [LeNet5_relu6] Epoch: 077 Train Loss: 0.9517 Train Acc: 0.6617 Eval Loss: 0.8857 Eval Acc: 0.6884 (LR: 0.00100000)
[2025-05-26 23:17:50,009]: [LeNet5_relu6] Epoch: 078 Train Loss: 0.9495 Train Acc: 0.6646 Eval Loss: 0.8657 Eval Acc: 0.6903 (LR: 0.00100000)
[2025-05-26 23:18:16,846]: [LeNet5_relu6] Epoch: 079 Train Loss: 0.9567 Train Acc: 0.6627 Eval Loss: 0.8718 Eval Acc: 0.6939 (LR: 0.00010000)
[2025-05-26 23:18:43,613]: [LeNet5_relu6] Epoch: 080 Train Loss: 0.9103 Train Acc: 0.6774 Eval Loss: 0.8218 Eval Acc: 0.7106 (LR: 0.00010000)
[2025-05-26 23:19:10,200]: [LeNet5_relu6] Epoch: 081 Train Loss: 0.8951 Train Acc: 0.6849 Eval Loss: 0.8198 Eval Acc: 0.7117 (LR: 0.00010000)
[2025-05-26 23:19:36,595]: [LeNet5_relu6] Epoch: 082 Train Loss: 0.8936 Train Acc: 0.6855 Eval Loss: 0.8158 Eval Acc: 0.7139 (LR: 0.00010000)
[2025-05-26 23:20:03,108]: [LeNet5_relu6] Epoch: 083 Train Loss: 0.8876 Train Acc: 0.6855 Eval Loss: 0.8119 Eval Acc: 0.7150 (LR: 0.00010000)
[2025-05-26 23:20:29,514]: [LeNet5_relu6] Epoch: 084 Train Loss: 0.8921 Train Acc: 0.6835 Eval Loss: 0.8191 Eval Acc: 0.7111 (LR: 0.00010000)
[2025-05-26 23:20:56,153]: [LeNet5_relu6] Epoch: 085 Train Loss: 0.8858 Train Acc: 0.6854 Eval Loss: 0.8138 Eval Acc: 0.7125 (LR: 0.00010000)
[2025-05-26 23:21:22,456]: [LeNet5_relu6] Epoch: 086 Train Loss: 0.8880 Train Acc: 0.6862 Eval Loss: 0.8110 Eval Acc: 0.7126 (LR: 0.00010000)
[2025-05-26 23:21:48,770]: [LeNet5_relu6] Epoch: 087 Train Loss: 0.8849 Train Acc: 0.6879 Eval Loss: 0.8145 Eval Acc: 0.7120 (LR: 0.00010000)
[2025-05-26 23:22:15,190]: [LeNet5_relu6] Epoch: 088 Train Loss: 0.8818 Train Acc: 0.6884 Eval Loss: 0.8090 Eval Acc: 0.7131 (LR: 0.00010000)
[2025-05-26 23:22:41,787]: [LeNet5_relu6] Epoch: 089 Train Loss: 0.8845 Train Acc: 0.6867 Eval Loss: 0.8133 Eval Acc: 0.7149 (LR: 0.00010000)
[2025-05-26 23:23:08,458]: [LeNet5_relu6] Epoch: 090 Train Loss: 0.8838 Train Acc: 0.6872 Eval Loss: 0.8082 Eval Acc: 0.7166 (LR: 0.00010000)
[2025-05-26 23:23:34,949]: [LeNet5_relu6] Epoch: 091 Train Loss: 0.8824 Train Acc: 0.6870 Eval Loss: 0.8058 Eval Acc: 0.7158 (LR: 0.00010000)
[2025-05-26 23:24:01,467]: [LeNet5_relu6] Epoch: 092 Train Loss: 0.8792 Train Acc: 0.6897 Eval Loss: 0.8093 Eval Acc: 0.7145 (LR: 0.00010000)
[2025-05-26 23:24:27,979]: [LeNet5_relu6] Epoch: 093 Train Loss: 0.8747 Train Acc: 0.6904 Eval Loss: 0.8088 Eval Acc: 0.7149 (LR: 0.00010000)
[2025-05-26 23:24:54,635]: [LeNet5_relu6] Epoch: 094 Train Loss: 0.8719 Train Acc: 0.6912 Eval Loss: 0.8067 Eval Acc: 0.7151 (LR: 0.00010000)
[2025-05-26 23:25:24,334]: [LeNet5_relu6] Epoch: 095 Train Loss: 0.8784 Train Acc: 0.6872 Eval Loss: 0.8049 Eval Acc: 0.7151 (LR: 0.00010000)
[2025-05-26 23:25:51,740]: [LeNet5_relu6] Epoch: 096 Train Loss: 0.8778 Train Acc: 0.6886 Eval Loss: 0.8077 Eval Acc: 0.7146 (LR: 0.00010000)
[2025-05-26 23:26:19,560]: [LeNet5_relu6] Epoch: 097 Train Loss: 0.8765 Train Acc: 0.6884 Eval Loss: 0.8044 Eval Acc: 0.7159 (LR: 0.00010000)
[2025-05-26 23:26:48,031]: [LeNet5_relu6] Epoch: 098 Train Loss: 0.8779 Train Acc: 0.6905 Eval Loss: 0.8057 Eval Acc: 0.7166 (LR: 0.00010000)
[2025-05-26 23:27:18,302]: [LeNet5_relu6] Epoch: 099 Train Loss: 0.8722 Train Acc: 0.6915 Eval Loss: 0.8080 Eval Acc: 0.7155 (LR: 0.00010000)
[2025-05-26 23:27:47,629]: [LeNet5_relu6] Epoch: 100 Train Loss: 0.8736 Train Acc: 0.6905 Eval Loss: 0.8043 Eval Acc: 0.7171 (LR: 0.00010000)
[2025-05-26 23:28:17,021]: [LeNet5_relu6] Epoch: 101 Train Loss: 0.8750 Train Acc: 0.6898 Eval Loss: 0.8123 Eval Acc: 0.7156 (LR: 0.00010000)
[2025-05-26 23:28:45,060]: [LeNet5_relu6] Epoch: 102 Train Loss: 0.8698 Train Acc: 0.6935 Eval Loss: 0.8041 Eval Acc: 0.7170 (LR: 0.00010000)
[2025-05-26 23:29:14,535]: [LeNet5_relu6] Epoch: 103 Train Loss: 0.8781 Train Acc: 0.6910 Eval Loss: 0.8002 Eval Acc: 0.7211 (LR: 0.00010000)
[2025-05-26 23:29:43,567]: [LeNet5_relu6] Epoch: 104 Train Loss: 0.8727 Train Acc: 0.6910 Eval Loss: 0.8044 Eval Acc: 0.7186 (LR: 0.00010000)
[2025-05-26 23:30:13,373]: [LeNet5_relu6] Epoch: 105 Train Loss: 0.8717 Train Acc: 0.6908 Eval Loss: 0.8037 Eval Acc: 0.7189 (LR: 0.00010000)
[2025-05-26 23:30:42,085]: [LeNet5_relu6] Epoch: 106 Train Loss: 0.8765 Train Acc: 0.6897 Eval Loss: 0.8088 Eval Acc: 0.7168 (LR: 0.00010000)
[2025-05-26 23:31:09,887]: [LeNet5_relu6] Epoch: 107 Train Loss: 0.8717 Train Acc: 0.6919 Eval Loss: 0.8044 Eval Acc: 0.7190 (LR: 0.00010000)
[2025-05-26 23:31:37,462]: [LeNet5_relu6] Epoch: 108 Train Loss: 0.8692 Train Acc: 0.6928 Eval Loss: 0.7991 Eval Acc: 0.7203 (LR: 0.00010000)
[2025-05-26 23:32:05,776]: [LeNet5_relu6] Epoch: 109 Train Loss: 0.8714 Train Acc: 0.6917 Eval Loss: 0.8045 Eval Acc: 0.7170 (LR: 0.00010000)
[2025-05-26 23:32:33,841]: [LeNet5_relu6] Epoch: 110 Train Loss: 0.8715 Train Acc: 0.6915 Eval Loss: 0.8032 Eval Acc: 0.7195 (LR: 0.00010000)
[2025-05-26 23:33:01,773]: [LeNet5_relu6] Epoch: 111 Train Loss: 0.8700 Train Acc: 0.6919 Eval Loss: 0.8062 Eval Acc: 0.7191 (LR: 0.00010000)
[2025-05-26 23:33:29,944]: [LeNet5_relu6] Epoch: 112 Train Loss: 0.8756 Train Acc: 0.6920 Eval Loss: 0.8071 Eval Acc: 0.7181 (LR: 0.00010000)
[2025-05-26 23:33:59,771]: [LeNet5_relu6] Epoch: 113 Train Loss: 0.8686 Train Acc: 0.6937 Eval Loss: 0.8037 Eval Acc: 0.7194 (LR: 0.00010000)
[2025-05-26 23:34:29,321]: [LeNet5_relu6] Epoch: 114 Train Loss: 0.8709 Train Acc: 0.6930 Eval Loss: 0.8008 Eval Acc: 0.7179 (LR: 0.00010000)
[2025-05-26 23:34:57,396]: [LeNet5_relu6] Epoch: 115 Train Loss: 0.8694 Train Acc: 0.6934 Eval Loss: 0.8014 Eval Acc: 0.7188 (LR: 0.00010000)
[2025-05-26 23:35:26,992]: [LeNet5_relu6] Epoch: 116 Train Loss: 0.8720 Train Acc: 0.6915 Eval Loss: 0.8052 Eval Acc: 0.7176 (LR: 0.00010000)
[2025-05-26 23:35:55,722]: [LeNet5_relu6] Epoch: 117 Train Loss: 0.8708 Train Acc: 0.6897 Eval Loss: 0.8011 Eval Acc: 0.7190 (LR: 0.00010000)
[2025-05-26 23:36:23,691]: [LeNet5_relu6] Epoch: 118 Train Loss: 0.8702 Train Acc: 0.6928 Eval Loss: 0.8042 Eval Acc: 0.7203 (LR: 0.00010000)
[2025-05-26 23:36:53,494]: [LeNet5_relu6] Epoch: 119 Train Loss: 0.8658 Train Acc: 0.6935 Eval Loss: 0.7987 Eval Acc: 0.7196 (LR: 0.00010000)
[2025-05-26 23:37:23,050]: [LeNet5_relu6] Epoch: 120 Train Loss: 0.8677 Train Acc: 0.6934 Eval Loss: 0.7991 Eval Acc: 0.7187 (LR: 0.00010000)
[2025-05-26 23:37:51,555]: [LeNet5_relu6] Epoch: 121 Train Loss: 0.8664 Train Acc: 0.6909 Eval Loss: 0.7978 Eval Acc: 0.7205 (LR: 0.00010000)
[2025-05-26 23:38:21,572]: [LeNet5_relu6] Epoch: 122 Train Loss: 0.8678 Train Acc: 0.6916 Eval Loss: 0.8020 Eval Acc: 0.7190 (LR: 0.00010000)
[2025-05-26 23:38:50,617]: [LeNet5_relu6] Epoch: 123 Train Loss: 0.8721 Train Acc: 0.6904 Eval Loss: 0.7969 Eval Acc: 0.7174 (LR: 0.00010000)
[2025-05-26 23:39:18,928]: [LeNet5_relu6] Epoch: 124 Train Loss: 0.8632 Train Acc: 0.6943 Eval Loss: 0.7985 Eval Acc: 0.7180 (LR: 0.00010000)
[2025-05-26 23:39:49,475]: [LeNet5_relu6] Epoch: 125 Train Loss: 0.8661 Train Acc: 0.6942 Eval Loss: 0.7985 Eval Acc: 0.7198 (LR: 0.00010000)
[2025-05-26 23:40:20,098]: [LeNet5_relu6] Epoch: 126 Train Loss: 0.8630 Train Acc: 0.6966 Eval Loss: 0.7975 Eval Acc: 0.7214 (LR: 0.00010000)
[2025-05-26 23:40:50,671]: [LeNet5_relu6] Epoch: 127 Train Loss: 0.8667 Train Acc: 0.6928 Eval Loss: 0.7981 Eval Acc: 0.7207 (LR: 0.00010000)
[2025-05-26 23:41:21,135]: [LeNet5_relu6] Epoch: 128 Train Loss: 0.8626 Train Acc: 0.6924 Eval Loss: 0.8081 Eval Acc: 0.7185 (LR: 0.00010000)
[2025-05-26 23:41:51,571]: [LeNet5_relu6] Epoch: 129 Train Loss: 0.8613 Train Acc: 0.6956 Eval Loss: 0.7950 Eval Acc: 0.7227 (LR: 0.00010000)
[2025-05-26 23:42:22,380]: [LeNet5_relu6] Epoch: 130 Train Loss: 0.8644 Train Acc: 0.6940 Eval Loss: 0.8018 Eval Acc: 0.7187 (LR: 0.00010000)
[2025-05-26 23:42:53,001]: [LeNet5_relu6] Epoch: 131 Train Loss: 0.8626 Train Acc: 0.6940 Eval Loss: 0.7945 Eval Acc: 0.7219 (LR: 0.00010000)
[2025-05-26 23:43:23,387]: [LeNet5_relu6] Epoch: 132 Train Loss: 0.8614 Train Acc: 0.6940 Eval Loss: 0.8003 Eval Acc: 0.7185 (LR: 0.00010000)
[2025-05-26 23:43:54,181]: [LeNet5_relu6] Epoch: 133 Train Loss: 0.8625 Train Acc: 0.6954 Eval Loss: 0.7998 Eval Acc: 0.7191 (LR: 0.00010000)
[2025-05-26 23:44:24,986]: [LeNet5_relu6] Epoch: 134 Train Loss: 0.8657 Train Acc: 0.6938 Eval Loss: 0.7944 Eval Acc: 0.7217 (LR: 0.00010000)
[2025-05-26 23:44:55,315]: [LeNet5_relu6] Epoch: 135 Train Loss: 0.8595 Train Acc: 0.6945 Eval Loss: 0.7969 Eval Acc: 0.7208 (LR: 0.00010000)
[2025-05-26 23:45:25,742]: [LeNet5_relu6] Epoch: 136 Train Loss: 0.8596 Train Acc: 0.6960 Eval Loss: 0.7993 Eval Acc: 0.7226 (LR: 0.00010000)
[2025-05-26 23:45:55,760]: [LeNet5_relu6] Epoch: 137 Train Loss: 0.8598 Train Acc: 0.6968 Eval Loss: 0.7946 Eval Acc: 0.7226 (LR: 0.00010000)
[2025-05-26 23:46:26,467]: [LeNet5_relu6] Epoch: 138 Train Loss: 0.8622 Train Acc: 0.6950 Eval Loss: 0.7966 Eval Acc: 0.7240 (LR: 0.00010000)
[2025-05-26 23:46:56,761]: [LeNet5_relu6] Epoch: 139 Train Loss: 0.8628 Train Acc: 0.6948 Eval Loss: 0.7944 Eval Acc: 0.7220 (LR: 0.00010000)
[2025-05-26 23:47:27,097]: [LeNet5_relu6] Epoch: 140 Train Loss: 0.8640 Train Acc: 0.6930 Eval Loss: 0.7967 Eval Acc: 0.7235 (LR: 0.00010000)
[2025-05-26 23:47:58,776]: [LeNet5_relu6] Epoch: 141 Train Loss: 0.8659 Train Acc: 0.6937 Eval Loss: 0.7975 Eval Acc: 0.7217 (LR: 0.00010000)
[2025-05-26 23:48:29,541]: [LeNet5_relu6] Epoch: 142 Train Loss: 0.8634 Train Acc: 0.6958 Eval Loss: 0.7992 Eval Acc: 0.7197 (LR: 0.00010000)
[2025-05-26 23:49:00,292]: [LeNet5_relu6] Epoch: 143 Train Loss: 0.8606 Train Acc: 0.6961 Eval Loss: 0.7948 Eval Acc: 0.7241 (LR: 0.00010000)
[2025-05-26 23:49:30,733]: [LeNet5_relu6] Epoch: 144 Train Loss: 0.8593 Train Acc: 0.6958 Eval Loss: 0.7923 Eval Acc: 0.7254 (LR: 0.00010000)
[2025-05-26 23:50:00,870]: [LeNet5_relu6] Epoch: 145 Train Loss: 0.8594 Train Acc: 0.6965 Eval Loss: 0.7936 Eval Acc: 0.7199 (LR: 0.00010000)
[2025-05-26 23:50:31,559]: [LeNet5_relu6] Epoch: 146 Train Loss: 0.8556 Train Acc: 0.6985 Eval Loss: 0.7920 Eval Acc: 0.7206 (LR: 0.00010000)
[2025-05-26 23:51:00,373]: [LeNet5_relu6] Epoch: 147 Train Loss: 0.8569 Train Acc: 0.6960 Eval Loss: 0.7974 Eval Acc: 0.7239 (LR: 0.00010000)
[2025-05-26 23:51:30,066]: [LeNet5_relu6] Epoch: 148 Train Loss: 0.8575 Train Acc: 0.6971 Eval Loss: 0.7968 Eval Acc: 0.7214 (LR: 0.00010000)
[2025-05-26 23:52:00,892]: [LeNet5_relu6] Epoch: 149 Train Loss: 0.8579 Train Acc: 0.6977 Eval Loss: 0.7907 Eval Acc: 0.7239 (LR: 0.00010000)
[2025-05-26 23:52:31,386]: [LeNet5_relu6] Epoch: 150 Train Loss: 0.8580 Train Acc: 0.6983 Eval Loss: 0.7910 Eval Acc: 0.7232 (LR: 0.00010000)
[2025-05-26 23:53:01,604]: [LeNet5_relu6] Epoch: 151 Train Loss: 0.8563 Train Acc: 0.6973 Eval Loss: 0.7978 Eval Acc: 0.7204 (LR: 0.00010000)
[2025-05-26 23:53:32,357]: [LeNet5_relu6] Epoch: 152 Train Loss: 0.8543 Train Acc: 0.6987 Eval Loss: 0.7993 Eval Acc: 0.7172 (LR: 0.00010000)
[2025-05-26 23:54:02,872]: [LeNet5_relu6] Epoch: 153 Train Loss: 0.8552 Train Acc: 0.6981 Eval Loss: 0.7977 Eval Acc: 0.7196 (LR: 0.00010000)
[2025-05-26 23:54:33,611]: [LeNet5_relu6] Epoch: 154 Train Loss: 0.8591 Train Acc: 0.6965 Eval Loss: 0.7903 Eval Acc: 0.7245 (LR: 0.00010000)
[2025-05-26 23:55:04,274]: [LeNet5_relu6] Epoch: 155 Train Loss: 0.8559 Train Acc: 0.6972 Eval Loss: 0.7885 Eval Acc: 0.7243 (LR: 0.00010000)
[2025-05-26 23:55:34,705]: [LeNet5_relu6] Epoch: 156 Train Loss: 0.8588 Train Acc: 0.6968 Eval Loss: 0.7958 Eval Acc: 0.7190 (LR: 0.00010000)
[2025-05-26 23:56:05,251]: [LeNet5_relu6] Epoch: 157 Train Loss: 0.8575 Train Acc: 0.6958 Eval Loss: 0.7982 Eval Acc: 0.7217 (LR: 0.00010000)
[2025-05-26 23:56:36,193]: [LeNet5_relu6] Epoch: 158 Train Loss: 0.8539 Train Acc: 0.6981 Eval Loss: 0.8040 Eval Acc: 0.7171 (LR: 0.00010000)
[2025-05-26 23:57:06,722]: [LeNet5_relu6] Epoch: 159 Train Loss: 0.8587 Train Acc: 0.6959 Eval Loss: 0.7958 Eval Acc: 0.7219 (LR: 0.00010000)
[2025-05-26 23:57:37,501]: [LeNet5_relu6] Epoch: 160 Train Loss: 0.8564 Train Acc: 0.6986 Eval Loss: 0.7926 Eval Acc: 0.7208 (LR: 0.00010000)
[2025-05-26 23:58:08,025]: [LeNet5_relu6] Epoch: 161 Train Loss: 0.8570 Train Acc: 0.6975 Eval Loss: 0.7936 Eval Acc: 0.7187 (LR: 0.00010000)
[2025-05-26 23:58:38,922]: [LeNet5_relu6] Epoch: 162 Train Loss: 0.8559 Train Acc: 0.6960 Eval Loss: 0.7892 Eval Acc: 0.7211 (LR: 0.00010000)
[2025-05-26 23:59:09,643]: [LeNet5_relu6] Epoch: 163 Train Loss: 0.8556 Train Acc: 0.6995 Eval Loss: 0.7933 Eval Acc: 0.7241 (LR: 0.00010000)
[2025-05-26 23:59:40,491]: [LeNet5_relu6] Epoch: 164 Train Loss: 0.8542 Train Acc: 0.6982 Eval Loss: 0.7923 Eval Acc: 0.7222 (LR: 0.00010000)
[2025-05-27 00:00:11,621]: [LeNet5_relu6] Epoch: 165 Train Loss: 0.8547 Train Acc: 0.6986 Eval Loss: 0.7930 Eval Acc: 0.7218 (LR: 0.00010000)
[2025-05-27 00:00:42,381]: [LeNet5_relu6] Epoch: 166 Train Loss: 0.8563 Train Acc: 0.6956 Eval Loss: 0.7908 Eval Acc: 0.7227 (LR: 0.00001000)
[2025-05-27 00:01:12,935]: [LeNet5_relu6] Epoch: 167 Train Loss: 0.8459 Train Acc: 0.7010 Eval Loss: 0.7879 Eval Acc: 0.7234 (LR: 0.00001000)
[2025-05-27 00:01:43,582]: [LeNet5_relu6] Epoch: 168 Train Loss: 0.8452 Train Acc: 0.7004 Eval Loss: 0.7892 Eval Acc: 0.7226 (LR: 0.00001000)
[2025-05-27 00:02:14,274]: [LeNet5_relu6] Epoch: 169 Train Loss: 0.8439 Train Acc: 0.6982 Eval Loss: 0.7889 Eval Acc: 0.7245 (LR: 0.00001000)
[2025-05-27 00:02:44,863]: [LeNet5_relu6] Epoch: 170 Train Loss: 0.8425 Train Acc: 0.7021 Eval Loss: 0.7893 Eval Acc: 0.7234 (LR: 0.00001000)
[2025-05-27 00:03:15,502]: [LeNet5_relu6] Epoch: 171 Train Loss: 0.8429 Train Acc: 0.7030 Eval Loss: 0.7882 Eval Acc: 0.7251 (LR: 0.00001000)
[2025-05-27 00:03:47,347]: [LeNet5_relu6] Epoch: 172 Train Loss: 0.8447 Train Acc: 0.7016 Eval Loss: 0.7862 Eval Acc: 0.7253 (LR: 0.00001000)
[2025-05-27 00:04:18,452]: [LeNet5_relu6] Epoch: 173 Train Loss: 0.8430 Train Acc: 0.7015 Eval Loss: 0.7869 Eval Acc: 0.7239 (LR: 0.00001000)
[2025-05-27 00:04:50,935]: [LeNet5_relu6] Epoch: 174 Train Loss: 0.8471 Train Acc: 0.7004 Eval Loss: 0.7873 Eval Acc: 0.7232 (LR: 0.00001000)
[2025-05-27 00:05:22,089]: [LeNet5_relu6] Epoch: 175 Train Loss: 0.8439 Train Acc: 0.7021 Eval Loss: 0.7862 Eval Acc: 0.7249 (LR: 0.00001000)
[2025-05-27 00:05:52,768]: [LeNet5_relu6] Epoch: 176 Train Loss: 0.8485 Train Acc: 0.6999 Eval Loss: 0.7885 Eval Acc: 0.7248 (LR: 0.00001000)
[2025-05-27 00:06:23,014]: [LeNet5_relu6] Epoch: 177 Train Loss: 0.8404 Train Acc: 0.7002 Eval Loss: 0.7853 Eval Acc: 0.7263 (LR: 0.00001000)
[2025-05-27 00:06:54,476]: [LeNet5_relu6] Epoch: 178 Train Loss: 0.8456 Train Acc: 0.7036 Eval Loss: 0.7860 Eval Acc: 0.7260 (LR: 0.00001000)
[2025-05-27 00:07:26,936]: [LeNet5_relu6] Epoch: 179 Train Loss: 0.8436 Train Acc: 0.7024 Eval Loss: 0.7864 Eval Acc: 0.7258 (LR: 0.00001000)
[2025-05-27 00:08:04,963]: [LeNet5_relu6] Epoch: 180 Train Loss: 0.8482 Train Acc: 0.6995 Eval Loss: 0.7861 Eval Acc: 0.7246 (LR: 0.00001000)
[2025-05-27 00:08:36,035]: [LeNet5_relu6] Epoch: 181 Train Loss: 0.8424 Train Acc: 0.7023 Eval Loss: 0.7864 Eval Acc: 0.7248 (LR: 0.00001000)
[2025-05-27 00:09:07,282]: [LeNet5_relu6] Epoch: 182 Train Loss: 0.8451 Train Acc: 0.6995 Eval Loss: 0.7869 Eval Acc: 0.7250 (LR: 0.00001000)
[2025-05-27 00:09:42,589]: [LeNet5_relu6] Epoch: 183 Train Loss: 0.8435 Train Acc: 0.7009 Eval Loss: 0.7859 Eval Acc: 0.7265 (LR: 0.00001000)
[2025-05-27 00:10:18,852]: [LeNet5_relu6] Epoch: 184 Train Loss: 0.8424 Train Acc: 0.7014 Eval Loss: 0.7873 Eval Acc: 0.7235 (LR: 0.00001000)
[2025-05-27 00:10:53,893]: [LeNet5_relu6] Epoch: 185 Train Loss: 0.8490 Train Acc: 0.6993 Eval Loss: 0.7862 Eval Acc: 0.7255 (LR: 0.00001000)
[2025-05-27 00:11:24,941]: [LeNet5_relu6] Epoch: 186 Train Loss: 0.8481 Train Acc: 0.7005 Eval Loss: 0.7869 Eval Acc: 0.7252 (LR: 0.00001000)
[2025-05-27 00:11:59,605]: [LeNet5_relu6] Epoch: 187 Train Loss: 0.8453 Train Acc: 0.6999 Eval Loss: 0.7850 Eval Acc: 0.7259 (LR: 0.00001000)
[2025-05-27 00:12:28,723]: [LeNet5_relu6] Epoch: 188 Train Loss: 0.8470 Train Acc: 0.7014 Eval Loss: 0.7866 Eval Acc: 0.7253 (LR: 0.00001000)
[2025-05-27 00:13:01,638]: [LeNet5_relu6] Epoch: 189 Train Loss: 0.8452 Train Acc: 0.7004 Eval Loss: 0.7866 Eval Acc: 0.7244 (LR: 0.00001000)
[2025-05-27 00:13:35,573]: [LeNet5_relu6] Epoch: 190 Train Loss: 0.8442 Train Acc: 0.7028 Eval Loss: 0.7866 Eval Acc: 0.7249 (LR: 0.00001000)
[2025-05-27 00:14:07,917]: [LeNet5_relu6] Epoch: 191 Train Loss: 0.8401 Train Acc: 0.7009 Eval Loss: 0.7868 Eval Acc: 0.7242 (LR: 0.00001000)
[2025-05-27 00:14:40,659]: [LeNet5_relu6] Epoch: 192 Train Loss: 0.8493 Train Acc: 0.7000 Eval Loss: 0.7865 Eval Acc: 0.7261 (LR: 0.00001000)
[2025-05-27 00:15:14,752]: [LeNet5_relu6] Epoch: 193 Train Loss: 0.8443 Train Acc: 0.7003 Eval Loss: 0.7871 Eval Acc: 0.7242 (LR: 0.00001000)
[2025-05-27 00:15:48,309]: [LeNet5_relu6] Epoch: 194 Train Loss: 0.8475 Train Acc: 0.7008 Eval Loss: 0.7862 Eval Acc: 0.7256 (LR: 0.00001000)
[2025-05-27 00:16:21,238]: [LeNet5_relu6] Epoch: 195 Train Loss: 0.8412 Train Acc: 0.7033 Eval Loss: 0.7862 Eval Acc: 0.7252 (LR: 0.00001000)
[2025-05-27 00:16:53,678]: [LeNet5_relu6] Epoch: 196 Train Loss: 0.8453 Train Acc: 0.7027 Eval Loss: 0.7855 Eval Acc: 0.7244 (LR: 0.00001000)
[2025-05-27 00:17:25,860]: [LeNet5_relu6] Epoch: 197 Train Loss: 0.8399 Train Acc: 0.7024 Eval Loss: 0.7875 Eval Acc: 0.7259 (LR: 0.00001000)
[2025-05-27 00:17:58,749]: [LeNet5_relu6] Epoch: 198 Train Loss: 0.8490 Train Acc: 0.7020 Eval Loss: 0.7872 Eval Acc: 0.7250 (LR: 0.00000100)
[2025-05-27 00:18:32,749]: [LeNet5_relu6] Epoch: 199 Train Loss: 0.8420 Train Acc: 0.7020 Eval Loss: 0.7868 Eval Acc: 0.7258 (LR: 0.00000100)
[2025-05-27 00:19:06,245]: [LeNet5_relu6] Epoch: 200 Train Loss: 0.8481 Train Acc: 0.6985 Eval Loss: 0.7864 Eval Acc: 0.7252 (LR: 0.00000100)
[2025-05-27 00:19:06,246]: [LeNet5_relu6] Best Eval Accuracy: 0.7265
[2025-05-27 00:19:06,689]: 
Training of full-precision model finished!
[2025-05-27 00:19:06,689]: Model Architecture:
[2025-05-27 00:19:06,896]: LeNet5(
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (fc2): Sequential(
    (0): Linear(in_features=120, out_features=84, bias=True)
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 00:19:06,903]: 
Model Weights:
[2025-05-27 00:19:06,903]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 00:19:07,436]: Sample Values (25 elements): [-0.07612387835979462, -0.40454190969467163, 0.0891384556889534, 0.20209239423274994, -0.05354897677898407, 0.2827643156051636, 0.023292653262615204, 0.011160396970808506, -0.04793481156229973, -0.09974567592144012, -0.16613265872001648, 0.23279985785484314, 0.15351037681102753, -0.12287785857915878, -0.03716327250003815, 0.10775381326675415, 0.09384109079837799, 0.0778336450457573, 0.14933538436889648, -0.1407863199710846, 0.021844878792762756, 0.015699997544288635, 0.15824823081493378, -0.19339260458946228, -0.13278670608997345]
[2025-05-27 00:19:07,538]: Mean: -0.00108949
[2025-05-27 00:19:07,599]: Min: -0.45534581
[2025-05-27 00:19:07,607]: Max: 0.44357651
[2025-05-27 00:19:07,607]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 00:19:07,624]: Sample Values (25 elements): [0.04056185856461525, -0.1073058471083641, -0.3189436197280884, 0.16516613960266113, -0.4025636911392212, -0.1053767129778862, -0.0018963257316499949, -0.1940091997385025, -0.18930040299892426, 0.05937875807285309, -0.4759310781955719, 0.07621743530035019, 0.0077796741388738155, 0.12254193425178528, -0.26704874634742737, -0.009691554121673107, 0.3270849287509918, 0.004075444303452969, 0.08091292530298233, -0.02675909735262394, -0.020608361810445786, -0.050768934190273285, 0.10648538917303085, -0.019915955141186714, -0.011140108108520508]
[2025-05-27 00:19:07,624]: Mean: -0.00650459
[2025-05-27 00:19:07,625]: Min: -1.17364073
[2025-05-27 00:19:07,625]: Max: 0.50750858
[2025-05-27 00:19:07,625]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 00:19:07,626]: Sample Values (25 elements): [-0.06312824785709381, -0.07681690156459808, -3.4918537039629882e-06, -9.674431930761784e-05, 0.016681814566254616, 6.37848074802605e-07, 0.2523746192455292, -6.500163688555327e-12, -1.1855496495627449e-07, 0.06911011785268784, 0.13886752724647522, -9.070394764804892e-39, 0.04350786283612251, 0.11762160062789917, -0.020652633160352707, 4.911691247304916e-41, 0.00019574777979869395, 0.2099124938249588, 0.03181178867816925, -0.005540103185921907, -0.04442199692130089, 0.15202480554580688, 0.0005518670077435672, 0.10685022175312042, 1.560341355870598e-37]
[2025-05-27 00:19:07,627]: Mean: 0.00123235
[2025-05-27 00:19:07,627]: Min: -0.61355054
[2025-05-27 00:19:07,627]: Max: 0.46469495
[2025-05-27 00:19:07,627]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 00:19:07,660]: Sample Values (25 elements): [-2.028697565492621e-07, -0.016714638099074364, 0.1502959281206131, 0.027631716802716255, 0.12975195050239563, -0.013892452232539654, 0.41144508123397827, -0.07105760276317596, 0.11132749170064926, -0.028694242238998413, -0.11876889318227768, 1.1687694849389734e-28, 0.1271871030330658, -4.919118129165838e-41, -6.02122242067789e-19, 0.19775429368019104, -5.800394733379715e-41, 0.00028083371580578387, 4.483465682048049e-11, 0.08669577538967133, 0.007246704772114754, -0.03548706695437431, 2.2896946619024393e-26, 0.3699820041656494, 4.908748520529834e-41]
[2025-05-27 00:19:07,662]: Mean: -0.00349403
[2025-05-27 00:19:07,665]: Min: -0.50101507
[2025-05-27 00:19:07,665]: Max: 0.45377544
[2025-05-27 00:19:07,666]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 00:19:07,667]: Sample Values (25 elements): [5.692919557986897e-07, 0.0354795940220356, -0.06036394461989403, 0.16644980013370514, 0.011857042089104652, 0.07483707368373871, -0.07530029863119125, -0.09237822890281677, 0.18308790028095245, 3.5648411085276166e-06, -0.00023719588352832943, -0.038203295320272446, -0.19881442189216614, -2.7938169555596915e-09, 2.608328713904484e-06, -0.07023387402296066, -0.13061468303203583, -0.12547563016414642, -2.073481600402564e-26, -0.12980487942695618, 6.49356957094227e-12, -0.10048645734786987, 0.16392740607261658, -3.542650331075925e-12, 0.07008993625640869]
[2025-05-27 00:19:07,667]: Mean: -0.01410775
[2025-05-27 00:19:07,668]: Min: -0.42355654
[2025-05-27 00:19:07,668]: Max: 0.61453664
[2025-05-27 00:19:07,668]: Checkpoint of model at path [checkpoint/LeNet5_relu6.ckpt] will be used for QAT
[2025-05-27 02:04:40,504]: Checkpoint of model at path [checkpoint/LeNet5_relu6.ckpt] will be used for QAT
[2025-05-27 02:04:40,504]: 


QAT of LeNet5 with relu6 down to 4 bits...
[2025-05-27 02:04:40,749]: [LeNet5_relu6_quantized_4_bits] after configure_qat:
[2025-05-27 02:04:40,863]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 02:05:11,855]: [LeNet5_relu6_quantized_4_bits] Epoch: 001 Train Loss: 1.0197 Train Acc: 0.6382 Eval Loss: 0.9283 Eval Acc: 0.6662 (LR: 0.00100000)
[2025-05-27 02:05:42,751]: [LeNet5_relu6_quantized_4_bits] Epoch: 002 Train Loss: 1.0224 Train Acc: 0.6396 Eval Loss: 0.9600 Eval Acc: 0.6614 (LR: 0.00100000)
[2025-05-27 02:06:14,977]: [LeNet5_relu6_quantized_4_bits] Epoch: 003 Train Loss: 1.0343 Train Acc: 0.6325 Eval Loss: 0.9315 Eval Acc: 0.6677 (LR: 0.00100000)
[2025-05-27 02:06:47,593]: [LeNet5_relu6_quantized_4_bits] Epoch: 004 Train Loss: 1.0268 Train Acc: 0.6365 Eval Loss: 0.9286 Eval Acc: 0.6670 (LR: 0.00100000)
[2025-05-27 02:07:20,123]: [LeNet5_relu6_quantized_4_bits] Epoch: 005 Train Loss: 1.0298 Train Acc: 0.6357 Eval Loss: 0.9331 Eval Acc: 0.6730 (LR: 0.00100000)
[2025-05-27 02:07:52,196]: [LeNet5_relu6_quantized_4_bits] Epoch: 006 Train Loss: 1.0446 Train Acc: 0.6294 Eval Loss: 0.9268 Eval Acc: 0.6713 (LR: 0.00100000)
[2025-05-27 02:08:23,819]: [LeNet5_relu6_quantized_4_bits] Epoch: 007 Train Loss: 1.0438 Train Acc: 0.6286 Eval Loss: 0.9072 Eval Acc: 0.6817 (LR: 0.00100000)
[2025-05-27 02:08:55,920]: [LeNet5_relu6_quantized_4_bits] Epoch: 008 Train Loss: 1.0453 Train Acc: 0.6305 Eval Loss: 0.9208 Eval Acc: 0.6768 (LR: 0.00100000)
[2025-05-27 02:09:27,578]: [LeNet5_relu6_quantized_4_bits] Epoch: 009 Train Loss: 1.0534 Train Acc: 0.6252 Eval Loss: 0.9380 Eval Acc: 0.6700 (LR: 0.00100000)
[2025-05-27 02:09:59,183]: [LeNet5_relu6_quantized_4_bits] Epoch: 010 Train Loss: 1.0476 Train Acc: 0.6294 Eval Loss: 0.9833 Eval Acc: 0.6532 (LR: 0.00100000)
[2025-05-27 02:10:30,806]: [LeNet5_relu6_quantized_4_bits] Epoch: 011 Train Loss: 1.0478 Train Acc: 0.6299 Eval Loss: 0.9437 Eval Acc: 0.6636 (LR: 0.00100000)
[2025-05-27 02:11:02,633]: [LeNet5_relu6_quantized_4_bits] Epoch: 012 Train Loss: 1.0413 Train Acc: 0.6312 Eval Loss: 0.9424 Eval Acc: 0.6657 (LR: 0.00100000)
[2025-05-27 02:11:35,014]: [LeNet5_relu6_quantized_4_bits] Epoch: 013 Train Loss: 1.0466 Train Acc: 0.6304 Eval Loss: 0.9581 Eval Acc: 0.6654 (LR: 0.00010000)
[2025-05-27 02:12:07,277]: [LeNet5_relu6_quantized_4_bits] Epoch: 014 Train Loss: 0.9732 Train Acc: 0.6562 Eval Loss: 0.8787 Eval Acc: 0.6935 (LR: 0.00010000)
[2025-05-27 02:12:38,979]: [LeNet5_relu6_quantized_4_bits] Epoch: 015 Train Loss: 0.9602 Train Acc: 0.6623 Eval Loss: 0.8778 Eval Acc: 0.6933 (LR: 0.00010000)
[2025-05-27 02:13:10,917]: [LeNet5_relu6_quantized_4_bits] Epoch: 016 Train Loss: 0.9594 Train Acc: 0.6618 Eval Loss: 0.8853 Eval Acc: 0.6936 (LR: 0.00010000)
[2025-05-27 02:13:42,892]: [LeNet5_relu6_quantized_4_bits] Epoch: 017 Train Loss: 0.9577 Train Acc: 0.6627 Eval Loss: 0.8682 Eval Acc: 0.6940 (LR: 0.00010000)
[2025-05-27 02:14:14,751]: [LeNet5_relu6_quantized_4_bits] Epoch: 018 Train Loss: 0.9610 Train Acc: 0.6611 Eval Loss: 0.8651 Eval Acc: 0.6966 (LR: 0.00010000)
[2025-05-27 02:14:46,516]: [LeNet5_relu6_quantized_4_bits] Epoch: 019 Train Loss: 0.9564 Train Acc: 0.6614 Eval Loss: 0.8659 Eval Acc: 0.6965 (LR: 0.00010000)
[2025-05-27 02:15:18,373]: [LeNet5_relu6_quantized_4_bits] Epoch: 020 Train Loss: 0.9541 Train Acc: 0.6630 Eval Loss: 0.8816 Eval Acc: 0.6891 (LR: 0.00010000)
[2025-05-27 02:15:50,423]: [LeNet5_relu6_quantized_4_bits] Epoch: 021 Train Loss: 0.9538 Train Acc: 0.6625 Eval Loss: 0.8899 Eval Acc: 0.6904 (LR: 0.00010000)
[2025-05-27 02:16:22,381]: [LeNet5_relu6_quantized_4_bits] Epoch: 022 Train Loss: 0.9564 Train Acc: 0.6611 Eval Loss: 0.8894 Eval Acc: 0.6919 (LR: 0.00010000)
[2025-05-27 02:16:54,320]: [LeNet5_relu6_quantized_4_bits] Epoch: 023 Train Loss: 0.9529 Train Acc: 0.6640 Eval Loss: 0.9132 Eval Acc: 0.6778 (LR: 0.00010000)
[2025-05-27 02:17:26,103]: [LeNet5_relu6_quantized_4_bits] Epoch: 024 Train Loss: 0.9547 Train Acc: 0.6588 Eval Loss: 0.8740 Eval Acc: 0.6942 (LR: 0.00001000)
[2025-05-27 02:17:58,101]: [LeNet5_relu6_quantized_4_bits] Epoch: 025 Train Loss: 0.9295 Train Acc: 0.6718 Eval Loss: 0.8572 Eval Acc: 0.6985 (LR: 0.00001000)
[2025-05-27 02:18:29,798]: [LeNet5_relu6_quantized_4_bits] Epoch: 026 Train Loss: 0.9289 Train Acc: 0.6710 Eval Loss: 0.8521 Eval Acc: 0.6998 (LR: 0.00001000)
[2025-05-27 02:19:01,771]: [LeNet5_relu6_quantized_4_bits] Epoch: 027 Train Loss: 0.9302 Train Acc: 0.6707 Eval Loss: 0.8513 Eval Acc: 0.6996 (LR: 0.00001000)
[2025-05-27 02:19:33,610]: [LeNet5_relu6_quantized_4_bits] Epoch: 028 Train Loss: 0.9319 Train Acc: 0.6714 Eval Loss: 0.8529 Eval Acc: 0.7015 (LR: 0.00001000)
[2025-05-27 02:20:05,642]: [LeNet5_relu6_quantized_4_bits] Epoch: 029 Train Loss: 0.9331 Train Acc: 0.6686 Eval Loss: 0.8525 Eval Acc: 0.7022 (LR: 0.00001000)
[2025-05-27 02:20:37,061]: [LeNet5_relu6_quantized_4_bits] Epoch: 030 Train Loss: 0.9307 Train Acc: 0.6729 Eval Loss: 0.8479 Eval Acc: 0.7053 (LR: 0.00001000)
[2025-05-27 02:21:08,722]: [LeNet5_relu6_quantized_4_bits] Epoch: 031 Train Loss: 0.9290 Train Acc: 0.6716 Eval Loss: 0.8443 Eval Acc: 0.7024 (LR: 0.00001000)
[2025-05-27 02:21:40,303]: [LeNet5_relu6_quantized_4_bits] Epoch: 032 Train Loss: 0.9279 Train Acc: 0.6710 Eval Loss: 0.8587 Eval Acc: 0.7000 (LR: 0.00001000)
[2025-05-27 02:22:11,926]: [LeNet5_relu6_quantized_4_bits] Epoch: 033 Train Loss: 0.9250 Train Acc: 0.6720 Eval Loss: 0.8511 Eval Acc: 0.7010 (LR: 0.00001000)
[2025-05-27 02:22:43,554]: [LeNet5_relu6_quantized_4_bits] Epoch: 034 Train Loss: 0.9289 Train Acc: 0.6714 Eval Loss: 0.8504 Eval Acc: 0.7005 (LR: 0.00001000)
[2025-05-27 02:23:14,957]: [LeNet5_relu6_quantized_4_bits] Epoch: 035 Train Loss: 0.9279 Train Acc: 0.6710 Eval Loss: 0.8513 Eval Acc: 0.7019 (LR: 0.00001000)
[2025-05-27 02:23:46,592]: [LeNet5_relu6_quantized_4_bits] Epoch: 036 Train Loss: 0.9264 Train Acc: 0.6728 Eval Loss: 0.8484 Eval Acc: 0.7059 (LR: 0.00001000)
[2025-05-27 02:24:18,119]: [LeNet5_relu6_quantized_4_bits] Epoch: 037 Train Loss: 0.9326 Train Acc: 0.6696 Eval Loss: 0.8566 Eval Acc: 0.7003 (LR: 0.00000100)
[2025-05-27 02:24:49,962]: [LeNet5_relu6_quantized_4_bits] Epoch: 038 Train Loss: 0.9178 Train Acc: 0.6759 Eval Loss: 0.8482 Eval Acc: 0.7009 (LR: 0.00000100)
[2025-05-27 02:25:21,530]: [LeNet5_relu6_quantized_4_bits] Epoch: 039 Train Loss: 0.9182 Train Acc: 0.6735 Eval Loss: 0.8428 Eval Acc: 0.7034 (LR: 0.00000100)
[2025-05-27 02:25:53,012]: [LeNet5_relu6_quantized_4_bits] Epoch: 040 Train Loss: 0.9218 Train Acc: 0.6739 Eval Loss: 0.8497 Eval Acc: 0.7013 (LR: 0.00000100)
[2025-05-27 02:26:24,321]: [LeNet5_relu6_quantized_4_bits] Epoch: 041 Train Loss: 0.9217 Train Acc: 0.6740 Eval Loss: 0.8655 Eval Acc: 0.6989 (LR: 0.00000100)
[2025-05-27 02:26:55,491]: [LeNet5_relu6_quantized_4_bits] Epoch: 042 Train Loss: 0.9224 Train Acc: 0.6736 Eval Loss: 0.8444 Eval Acc: 0.7066 (LR: 0.00000100)
[2025-05-27 02:27:27,320]: [LeNet5_relu6_quantized_4_bits] Epoch: 043 Train Loss: 0.9224 Train Acc: 0.6744 Eval Loss: 0.8405 Eval Acc: 0.7026 (LR: 0.00000100)
[2025-05-27 02:27:58,590]: [LeNet5_relu6_quantized_4_bits] Epoch: 044 Train Loss: 0.9193 Train Acc: 0.6736 Eval Loss: 0.8487 Eval Acc: 0.6982 (LR: 0.00000100)
[2025-05-27 02:28:28,377]: [LeNet5_relu6_quantized_4_bits] Epoch: 045 Train Loss: 0.9170 Train Acc: 0.6761 Eval Loss: 0.8455 Eval Acc: 0.7062 (LR: 0.00000100)
[2025-05-27 02:28:57,975]: [LeNet5_relu6_quantized_4_bits] Epoch: 046 Train Loss: 0.9203 Train Acc: 0.6727 Eval Loss: 0.8436 Eval Acc: 0.7037 (LR: 0.00000100)
[2025-05-27 02:29:27,785]: [LeNet5_relu6_quantized_4_bits] Epoch: 047 Train Loss: 0.9207 Train Acc: 0.6743 Eval Loss: 0.8513 Eval Acc: 0.7040 (LR: 0.00000100)
[2025-05-27 02:29:57,401]: [LeNet5_relu6_quantized_4_bits] Epoch: 048 Train Loss: 0.9267 Train Acc: 0.6719 Eval Loss: 0.8459 Eval Acc: 0.7046 (LR: 0.00000100)
[2025-05-27 02:30:26,794]: [LeNet5_relu6_quantized_4_bits] Epoch: 049 Train Loss: 0.9219 Train Acc: 0.6723 Eval Loss: 0.8563 Eval Acc: 0.7002 (LR: 0.00000010)
[2025-05-27 02:30:56,367]: [LeNet5_relu6_quantized_4_bits] Epoch: 050 Train Loss: 0.9171 Train Acc: 0.6769 Eval Loss: 0.8456 Eval Acc: 0.7090 (LR: 0.00000010)
[2025-05-27 02:31:26,312]: [LeNet5_relu6_quantized_4_bits] Epoch: 051 Train Loss: 0.9199 Train Acc: 0.6758 Eval Loss: 0.8408 Eval Acc: 0.7045 (LR: 0.00000010)
[2025-05-27 02:31:56,298]: [LeNet5_relu6_quantized_4_bits] Epoch: 052 Train Loss: 0.9146 Train Acc: 0.6744 Eval Loss: 0.8484 Eval Acc: 0.7030 (LR: 0.00000010)
[2025-05-27 02:32:26,273]: [LeNet5_relu6_quantized_4_bits] Epoch: 053 Train Loss: 0.9181 Train Acc: 0.6763 Eval Loss: 0.8523 Eval Acc: 0.7033 (LR: 0.00000010)
[2025-05-27 02:32:56,431]: [LeNet5_relu6_quantized_4_bits] Epoch: 054 Train Loss: 0.9166 Train Acc: 0.6752 Eval Loss: 0.8396 Eval Acc: 0.7071 (LR: 0.00000010)
[2025-05-27 02:33:26,419]: [LeNet5_relu6_quantized_4_bits] Epoch: 055 Train Loss: 0.9177 Train Acc: 0.6769 Eval Loss: 0.8433 Eval Acc: 0.7031 (LR: 0.00000010)
[2025-05-27 02:33:56,166]: [LeNet5_relu6_quantized_4_bits] Epoch: 056 Train Loss: 0.9168 Train Acc: 0.6759 Eval Loss: 0.8501 Eval Acc: 0.7022 (LR: 0.00000010)
[2025-05-27 02:34:26,101]: [LeNet5_relu6_quantized_4_bits] Epoch: 057 Train Loss: 0.9137 Train Acc: 0.6774 Eval Loss: 0.8461 Eval Acc: 0.7046 (LR: 0.00000010)
[2025-05-27 02:34:55,893]: [LeNet5_relu6_quantized_4_bits] Epoch: 058 Train Loss: 0.9228 Train Acc: 0.6722 Eval Loss: 0.8552 Eval Acc: 0.6992 (LR: 0.00000010)
[2025-05-27 02:35:25,998]: [LeNet5_relu6_quantized_4_bits] Epoch: 059 Train Loss: 0.9202 Train Acc: 0.6740 Eval Loss: 0.8383 Eval Acc: 0.7061 (LR: 0.00000010)
[2025-05-27 02:35:56,205]: [LeNet5_relu6_quantized_4_bits] Epoch: 060 Train Loss: 0.9168 Train Acc: 0.6770 Eval Loss: 0.8486 Eval Acc: 0.7032 (LR: 0.00000010)
[2025-05-27 02:35:56,206]: [LeNet5_relu6_quantized_4_bits] Best Eval Accuracy: 0.7090
[2025-05-27 02:35:56,220]: 


Quantization of model down to 4 bits finished
[2025-05-27 02:35:56,221]: Model Architecture:
[2025-05-27 02:35:56,233]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1286], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3473045825958252, max_val=0.5820598602294922)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0856], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7731568813323975, max_val=0.5104501247406006)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0673], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5046368837356567, max_val=0.5051501989364624)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 02:35:56,233]: 
Model Weights:
[2025-05-27 02:35:56,233]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 02:35:56,234]: Sample Values (25 elements): [-0.04380813613533974, 0.04546135663986206, -0.10728415846824646, -0.01814514584839344, -0.03145481273531914, 0.06606514006853104, 0.33225217461586, 0.004513471852988005, -0.04147900268435478, -0.07560918480157852, -0.04660018905997276, 0.1299677938222885, 0.008980130776762962, -0.047513507306575775, 0.018520012497901917, 0.1838913857936859, 0.1855483055114746, -0.0921851247549057, -0.07729404419660568, -0.008034231141209602, -0.0993080586194992, 0.038909267634153366, -0.07824070751667023, -0.14391426742076874, 0.07357102632522583]
[2025-05-27 02:35:56,239]: Mean: -0.00110438
[2025-05-27 02:35:56,240]: Min: -0.46683696
[2025-05-27 02:35:56,240]: Max: 0.46609381
[2025-05-27 02:35:56,241]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 02:35:56,242]: Sample Values (25 elements): [0.0, 0.1286243051290512, -0.5144972205162048, 0.1286243051290512, 0.0, 0.0, 0.0, -0.2572486102581024, 0.1286243051290512, 0.1286243051290512, -0.1286243051290512, 0.38587290048599243, -0.1286243051290512, 0.0, 0.0, -0.1286243051290512, 0.0, 0.2572486102581024, 0.0, 0.1286243051290512, 0.1286243051290512, -0.1286243051290512, 0.0, 0.1286243051290512, 0.0]
[2025-05-27 02:35:56,242]: Mean: -0.01393430
[2025-05-27 02:35:56,242]: Min: -1.28624308
[2025-05-27 02:35:56,242]: Max: 0.64312154
[2025-05-27 02:35:56,244]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 02:35:56,245]: Sample Values (25 elements): [0.0, 0.08557380735874176, 0.0, 0.17114761471748352, 0.2567214369773865, 0.4278690218925476, 0.0, -0.08557380735874176, -0.08557380735874176, 0.0, -0.08557380735874176, -0.17114761471748352, 0.0, 0.0, 0.08557380735874176, 0.08557380735874176, 0.17114761471748352, 0.2567214369773865, 0.08557380735874176, 0.0, 0.0, 0.0, 0.08557380735874176, 0.0, 0.08557380735874176]
[2025-05-27 02:35:56,245]: Mean: -0.00078264
[2025-05-27 02:35:56,246]: Min: -0.77016425
[2025-05-27 02:35:56,246]: Max: 0.51344287
[2025-05-27 02:35:56,248]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 02:35:56,249]: Sample Values (25 elements): [-0.20195741951465607, 0.0, 0.0, -0.06731913983821869, 0.06731913983821869, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06731913983821869, 0.06731913983821869, 0.06731913983821869, 0.0, 0.0, 0.13463827967643738, 0.0, 0.0, -0.20195741951465607, 0.0, 0.0, 0.26927655935287476, 0.0, -0.33659571409225464]
[2025-05-27 02:35:56,249]: Mean: -0.00539622
[2025-05-27 02:35:56,249]: Min: -0.47123396
[2025-05-27 02:35:56,249]: Max: 0.53855312
[2025-05-27 02:35:56,249]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 02:35:56,250]: Sample Values (25 elements): [-0.12502960860729218, 0.16549423336982727, -0.130387544631958, -0.22054584324359894, -0.09529020637273788, -0.07713907957077026, -0.027516944333910942, -0.1898161768913269, -0.14020264148712158, -0.09909573197364807, 0.03846612572669983, 0.13830778002738953, -0.0507158525288105, -0.2036689817905426, 0.1583428531885147, -0.034555647522211075, -0.050811026245355606, 6.269829718928529e-41, 0.26566699147224426, 0.11341364681720734, 0.16193029284477234, 0.060717541724443436, 0.0809554010629654, -0.12047143280506134, -0.02452981099486351]
[2025-05-27 02:35:56,250]: Mean: -0.00711549
[2025-05-27 02:35:56,251]: Min: -0.33021298
[2025-05-27 02:35:56,251]: Max: 0.45477426
[2025-05-27 02:35:56,251]: 


QAT of LeNet5 with relu6 down to 3 bits...
[2025-05-27 02:35:56,278]: [LeNet5_relu6_quantized_3_bits] after configure_qat:
[2025-05-27 02:35:56,293]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 02:36:26,139]: [LeNet5_relu6_quantized_3_bits] Epoch: 001 Train Loss: 1.2647 Train Acc: 0.5527 Eval Loss: 1.1517 Eval Acc: 0.5848 (LR: 0.00100000)
[2025-05-27 02:36:56,019]: [LeNet5_relu6_quantized_3_bits] Epoch: 002 Train Loss: 1.2202 Train Acc: 0.5674 Eval Loss: 1.0625 Eval Acc: 0.6177 (LR: 0.00100000)
[2025-05-27 02:37:25,845]: [LeNet5_relu6_quantized_3_bits] Epoch: 003 Train Loss: 1.2112 Train Acc: 0.5700 Eval Loss: 1.0784 Eval Acc: 0.6124 (LR: 0.00100000)
[2025-05-27 02:37:55,982]: [LeNet5_relu6_quantized_3_bits] Epoch: 004 Train Loss: 1.2084 Train Acc: 0.5723 Eval Loss: 1.1194 Eval Acc: 0.5987 (LR: 0.00100000)
[2025-05-27 02:38:25,817]: [LeNet5_relu6_quantized_3_bits] Epoch: 005 Train Loss: 1.2093 Train Acc: 0.5709 Eval Loss: 1.0936 Eval Acc: 0.6195 (LR: 0.00100000)
[2025-05-27 02:38:55,749]: [LeNet5_relu6_quantized_3_bits] Epoch: 006 Train Loss: 1.2087 Train Acc: 0.5713 Eval Loss: 1.1015 Eval Acc: 0.6141 (LR: 0.00100000)
[2025-05-27 02:39:25,489]: [LeNet5_relu6_quantized_3_bits] Epoch: 007 Train Loss: 1.2120 Train Acc: 0.5705 Eval Loss: 1.0569 Eval Acc: 0.6297 (LR: 0.00100000)
[2025-05-27 02:39:55,057]: [LeNet5_relu6_quantized_3_bits] Epoch: 008 Train Loss: 1.2160 Train Acc: 0.5697 Eval Loss: 1.1043 Eval Acc: 0.6096 (LR: 0.00100000)
[2025-05-27 02:40:24,609]: [LeNet5_relu6_quantized_3_bits] Epoch: 009 Train Loss: 1.2269 Train Acc: 0.5647 Eval Loss: 1.1122 Eval Acc: 0.6154 (LR: 0.00100000)
[2025-05-27 02:40:54,187]: [LeNet5_relu6_quantized_3_bits] Epoch: 010 Train Loss: 1.2131 Train Acc: 0.5706 Eval Loss: 1.0993 Eval Acc: 0.6136 (LR: 0.00100000)
[2025-05-27 02:41:23,714]: [LeNet5_relu6_quantized_3_bits] Epoch: 011 Train Loss: 1.2146 Train Acc: 0.5691 Eval Loss: 1.1073 Eval Acc: 0.6088 (LR: 0.00100000)
[2025-05-27 02:41:53,456]: [LeNet5_relu6_quantized_3_bits] Epoch: 012 Train Loss: 1.2194 Train Acc: 0.5683 Eval Loss: 1.1316 Eval Acc: 0.6051 (LR: 0.00100000)
[2025-05-27 02:42:23,425]: [LeNet5_relu6_quantized_3_bits] Epoch: 013 Train Loss: 1.2117 Train Acc: 0.5697 Eval Loss: 1.0594 Eval Acc: 0.6275 (LR: 0.00010000)
[2025-05-27 02:42:53,462]: [LeNet5_relu6_quantized_3_bits] Epoch: 014 Train Loss: 1.1264 Train Acc: 0.6016 Eval Loss: 1.0073 Eval Acc: 0.6467 (LR: 0.00010000)
[2025-05-27 02:43:23,573]: [LeNet5_relu6_quantized_3_bits] Epoch: 015 Train Loss: 1.1168 Train Acc: 0.6034 Eval Loss: 1.0110 Eval Acc: 0.6457 (LR: 0.00010000)
[2025-05-27 02:43:53,722]: [LeNet5_relu6_quantized_3_bits] Epoch: 016 Train Loss: 1.1252 Train Acc: 0.6025 Eval Loss: 1.0072 Eval Acc: 0.6506 (LR: 0.00010000)
[2025-05-27 02:44:23,441]: [LeNet5_relu6_quantized_3_bits] Epoch: 017 Train Loss: 1.1205 Train Acc: 0.6032 Eval Loss: 1.0507 Eval Acc: 0.6336 (LR: 0.00010000)
[2025-05-27 02:44:53,061]: [LeNet5_relu6_quantized_3_bits] Epoch: 018 Train Loss: 1.1179 Train Acc: 0.6058 Eval Loss: 1.0117 Eval Acc: 0.6451 (LR: 0.00010000)
[2025-05-27 02:45:22,744]: [LeNet5_relu6_quantized_3_bits] Epoch: 019 Train Loss: 1.1127 Train Acc: 0.6080 Eval Loss: 1.0387 Eval Acc: 0.6376 (LR: 0.00010000)
[2025-05-27 02:45:52,366]: [LeNet5_relu6_quantized_3_bits] Epoch: 020 Train Loss: 1.1160 Train Acc: 0.6066 Eval Loss: 1.0023 Eval Acc: 0.6467 (LR: 0.00010000)
[2025-05-27 02:46:21,894]: [LeNet5_relu6_quantized_3_bits] Epoch: 021 Train Loss: 1.1256 Train Acc: 0.6028 Eval Loss: 1.0582 Eval Acc: 0.6310 (LR: 0.00010000)
[2025-05-27 02:46:51,197]: [LeNet5_relu6_quantized_3_bits] Epoch: 022 Train Loss: 1.1227 Train Acc: 0.6018 Eval Loss: 1.0070 Eval Acc: 0.6492 (LR: 0.00010000)
[2025-05-27 02:47:20,558]: [LeNet5_relu6_quantized_3_bits] Epoch: 023 Train Loss: 1.1250 Train Acc: 0.6018 Eval Loss: 1.0148 Eval Acc: 0.6380 (LR: 0.00010000)
[2025-05-27 02:47:50,012]: [LeNet5_relu6_quantized_3_bits] Epoch: 024 Train Loss: 1.1257 Train Acc: 0.6045 Eval Loss: 1.0495 Eval Acc: 0.6334 (LR: 0.00010000)
[2025-05-27 02:48:19,634]: [LeNet5_relu6_quantized_3_bits] Epoch: 025 Train Loss: 1.1255 Train Acc: 0.6009 Eval Loss: 1.0103 Eval Acc: 0.6449 (LR: 0.00010000)
[2025-05-27 02:48:49,622]: [LeNet5_relu6_quantized_3_bits] Epoch: 026 Train Loss: 1.1391 Train Acc: 0.5956 Eval Loss: 1.0232 Eval Acc: 0.6407 (LR: 0.00001000)
[2025-05-27 02:49:19,291]: [LeNet5_relu6_quantized_3_bits] Epoch: 027 Train Loss: 1.0866 Train Acc: 0.6153 Eval Loss: 0.9938 Eval Acc: 0.6522 (LR: 0.00001000)
[2025-05-27 02:49:49,272]: [LeNet5_relu6_quantized_3_bits] Epoch: 028 Train Loss: 1.0835 Train Acc: 0.6181 Eval Loss: 0.9863 Eval Acc: 0.6580 (LR: 0.00001000)
[2025-05-27 02:50:19,349]: [LeNet5_relu6_quantized_3_bits] Epoch: 029 Train Loss: 1.0868 Train Acc: 0.6137 Eval Loss: 1.0107 Eval Acc: 0.6487 (LR: 0.00001000)
[2025-05-27 02:50:49,085]: [LeNet5_relu6_quantized_3_bits] Epoch: 030 Train Loss: 1.0884 Train Acc: 0.6159 Eval Loss: 1.0070 Eval Acc: 0.6487 (LR: 0.00001000)
[2025-05-27 02:51:18,727]: [LeNet5_relu6_quantized_3_bits] Epoch: 031 Train Loss: 1.0844 Train Acc: 0.6180 Eval Loss: 0.9839 Eval Acc: 0.6555 (LR: 0.00001000)
[2025-05-27 02:51:48,293]: [LeNet5_relu6_quantized_3_bits] Epoch: 032 Train Loss: 1.0904 Train Acc: 0.6135 Eval Loss: 0.9854 Eval Acc: 0.6502 (LR: 0.00001000)
[2025-05-27 02:52:18,039]: [LeNet5_relu6_quantized_3_bits] Epoch: 033 Train Loss: 1.0934 Train Acc: 0.6121 Eval Loss: 0.9891 Eval Acc: 0.6572 (LR: 0.00001000)
[2025-05-27 02:52:47,590]: [LeNet5_relu6_quantized_3_bits] Epoch: 034 Train Loss: 1.0976 Train Acc: 0.6106 Eval Loss: 1.0146 Eval Acc: 0.6425 (LR: 0.00001000)
[2025-05-27 02:53:17,831]: [LeNet5_relu6_quantized_3_bits] Epoch: 035 Train Loss: 1.0978 Train Acc: 0.6127 Eval Loss: 0.9925 Eval Acc: 0.6543 (LR: 0.00001000)
[2025-05-27 02:53:47,768]: [LeNet5_relu6_quantized_3_bits] Epoch: 036 Train Loss: 1.0956 Train Acc: 0.6129 Eval Loss: 0.9982 Eval Acc: 0.6555 (LR: 0.00001000)
[2025-05-27 02:54:17,679]: [LeNet5_relu6_quantized_3_bits] Epoch: 037 Train Loss: 1.1028 Train Acc: 0.6128 Eval Loss: 1.0015 Eval Acc: 0.6473 (LR: 0.00000100)
[2025-05-27 02:54:47,841]: [LeNet5_relu6_quantized_3_bits] Epoch: 038 Train Loss: 1.0671 Train Acc: 0.6227 Eval Loss: 1.0004 Eval Acc: 0.6548 (LR: 0.00000100)
[2025-05-27 02:55:18,217]: [LeNet5_relu6_quantized_3_bits] Epoch: 039 Train Loss: 1.0692 Train Acc: 0.6215 Eval Loss: 0.9685 Eval Acc: 0.6622 (LR: 0.00000100)
[2025-05-27 02:55:48,400]: [LeNet5_relu6_quantized_3_bits] Epoch: 040 Train Loss: 1.0735 Train Acc: 0.6199 Eval Loss: 0.9839 Eval Acc: 0.6555 (LR: 0.00000100)
[2025-05-27 02:56:18,480]: [LeNet5_relu6_quantized_3_bits] Epoch: 041 Train Loss: 1.0730 Train Acc: 0.6209 Eval Loss: 0.9741 Eval Acc: 0.6612 (LR: 0.00000100)
[2025-05-27 02:56:48,377]: [LeNet5_relu6_quantized_3_bits] Epoch: 042 Train Loss: 1.0802 Train Acc: 0.6202 Eval Loss: 0.9871 Eval Acc: 0.6526 (LR: 0.00000100)
[2025-05-27 02:57:18,509]: [LeNet5_relu6_quantized_3_bits] Epoch: 043 Train Loss: 1.0756 Train Acc: 0.6236 Eval Loss: 0.9860 Eval Acc: 0.6583 (LR: 0.00000100)
[2025-05-27 02:57:47,630]: [LeNet5_relu6_quantized_3_bits] Epoch: 044 Train Loss: 1.0724 Train Acc: 0.6188 Eval Loss: 0.9576 Eval Acc: 0.6667 (LR: 0.00000100)
[2025-05-27 02:58:16,239]: [LeNet5_relu6_quantized_3_bits] Epoch: 045 Train Loss: 1.0788 Train Acc: 0.6199 Eval Loss: 0.9792 Eval Acc: 0.6562 (LR: 0.00000100)
[2025-05-27 02:58:44,871]: [LeNet5_relu6_quantized_3_bits] Epoch: 046 Train Loss: 1.0818 Train Acc: 0.6185 Eval Loss: 0.9855 Eval Acc: 0.6520 (LR: 0.00000100)
[2025-05-27 02:59:13,361]: [LeNet5_relu6_quantized_3_bits] Epoch: 047 Train Loss: 1.0828 Train Acc: 0.6189 Eval Loss: 0.9622 Eval Acc: 0.6613 (LR: 0.00000100)
[2025-05-27 02:59:42,111]: [LeNet5_relu6_quantized_3_bits] Epoch: 048 Train Loss: 1.0880 Train Acc: 0.6143 Eval Loss: 0.9904 Eval Acc: 0.6527 (LR: 0.00000100)
[2025-05-27 03:00:10,747]: [LeNet5_relu6_quantized_3_bits] Epoch: 049 Train Loss: 1.0780 Train Acc: 0.6190 Eval Loss: 0.9801 Eval Acc: 0.6528 (LR: 0.00000100)
[2025-05-27 03:00:39,206]: [LeNet5_relu6_quantized_3_bits] Epoch: 050 Train Loss: 1.0817 Train Acc: 0.6174 Eval Loss: 0.9796 Eval Acc: 0.6587 (LR: 0.00000010)
[2025-05-27 03:01:07,294]: [LeNet5_relu6_quantized_3_bits] Epoch: 051 Train Loss: 1.0605 Train Acc: 0.6270 Eval Loss: 0.9944 Eval Acc: 0.6495 (LR: 0.00000010)
[2025-05-27 03:01:35,265]: [LeNet5_relu6_quantized_3_bits] Epoch: 052 Train Loss: 1.0611 Train Acc: 0.6254 Eval Loss: 0.9763 Eval Acc: 0.6576 (LR: 0.00000010)
[2025-05-27 03:02:03,406]: [LeNet5_relu6_quantized_3_bits] Epoch: 053 Train Loss: 1.0645 Train Acc: 0.6239 Eval Loss: 0.9790 Eval Acc: 0.6613 (LR: 0.00000010)
[2025-05-27 03:02:31,738]: [LeNet5_relu6_quantized_3_bits] Epoch: 054 Train Loss: 1.0722 Train Acc: 0.6207 Eval Loss: 0.9859 Eval Acc: 0.6542 (LR: 0.00000010)
[2025-05-27 03:02:59,810]: [LeNet5_relu6_quantized_3_bits] Epoch: 055 Train Loss: 1.0728 Train Acc: 0.6199 Eval Loss: 0.9828 Eval Acc: 0.6553 (LR: 0.00000010)
[2025-05-27 03:03:27,802]: [LeNet5_relu6_quantized_3_bits] Epoch: 056 Train Loss: 1.0746 Train Acc: 0.6205 Eval Loss: 0.9592 Eval Acc: 0.6631 (LR: 0.00000010)
[2025-05-27 03:03:55,903]: [LeNet5_relu6_quantized_3_bits] Epoch: 057 Train Loss: 1.0785 Train Acc: 0.6181 Eval Loss: 0.9880 Eval Acc: 0.6545 (LR: 0.00000010)
[2025-05-27 03:04:23,708]: [LeNet5_relu6_quantized_3_bits] Epoch: 058 Train Loss: 1.0741 Train Acc: 0.6188 Eval Loss: 0.9605 Eval Acc: 0.6649 (LR: 0.00000010)
[2025-05-27 03:04:51,513]: [LeNet5_relu6_quantized_3_bits] Epoch: 059 Train Loss: 1.0734 Train Acc: 0.6201 Eval Loss: 0.9923 Eval Acc: 0.6535 (LR: 0.00000010)
[2025-05-27 03:05:19,349]: [LeNet5_relu6_quantized_3_bits] Epoch: 060 Train Loss: 1.0720 Train Acc: 0.6211 Eval Loss: 0.9889 Eval Acc: 0.6544 (LR: 0.00000010)
[2025-05-27 03:05:19,350]: [LeNet5_relu6_quantized_3_bits] Best Eval Accuracy: 0.6667
[2025-05-27 03:05:19,374]: 


Quantization of model down to 3 bits finished
[2025-05-27 03:05:19,374]: Model Architecture:
[2025-05-27 03:05:19,387]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3059], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4264686107635498, max_val=0.714978814125061)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1961], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8708930015563965, max_val=0.5021524429321289)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1466], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.513175368309021, max_val=0.5132873058319092)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 03:05:19,388]: 
Model Weights:
[2025-05-27 03:05:19,388]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 03:05:19,388]: Sample Values (25 elements): [-0.06863460689783096, 0.11270282417535782, -0.14128749072551727, -0.024357913061976433, 0.0347580723464489, 0.26531851291656494, 0.07994739711284637, -0.06681191921234131, -0.01172387134283781, 0.009178503416478634, 0.0508234016597271, 0.22771641612052917, -0.17133496701717377, 0.1951093226671219, 0.05078886076807976, 0.22275255620479584, -0.23875285685062408, 0.021543974056839943, -0.01673847623169422, 0.07256050407886505, 0.2476864904165268, 0.17503505945205688, -0.04011593759059906, -0.0016180871753022075, -0.06050318852066994]
[2025-05-27 03:05:19,388]: Mean: -0.00114315
[2025-05-27 03:05:19,389]: Min: -0.49149311
[2025-05-27 03:05:19,389]: Max: 0.48280391
[2025-05-27 03:05:19,390]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 03:05:19,390]: Sample Values (25 elements): [0.0, 0.0, 0.3059210777282715, 0.3059210777282715, -0.3059210777282715, 0.0, 0.0, -0.3059210777282715, 0.0, -0.3059210777282715, -0.3059210777282715, 0.0, 0.0, 0.0, 0.3059210777282715, 0.0, -0.3059210777282715, 0.3059210777282715, 0.0, 0.0, -0.3059210777282715, 0.3059210777282715, 0.0, 0.0, 0.0]
[2025-05-27 03:05:19,390]: Mean: -0.02587583
[2025-05-27 03:05:19,391]: Min: -1.52960539
[2025-05-27 03:05:19,391]: Max: 0.61184216
[2025-05-27 03:05:19,392]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 03:05:19,392]: Sample Values (25 elements): [0.0, 0.19614936411380768, 0.0, 0.0, 0.0, 0.0, -0.19614936411380768, -0.19614936411380768, 0.0, -0.19614936411380768, -0.19614936411380768, -0.19614936411380768, 0.0, 0.0, 0.0, 0.0, 0.0, -0.19614936411380768, 0.0, 0.0, 0.0, 0.19614936411380768, 0.0, 0.0, 0.0]
[2025-05-27 03:05:19,393]: Mean: -0.00388212
[2025-05-27 03:05:19,393]: Min: -0.78459746
[2025-05-27 03:05:19,393]: Max: 0.58844811
[2025-05-27 03:05:19,394]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 03:05:19,395]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.14663752913475037, 0.0, 0.0, 0.14663752913475037, 0.0, 0.14663752913475037, 0.0, -0.14663752913475037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14663752913475037, 0.14663752913475037]
[2025-05-27 03:05:19,395]: Mean: -0.00565893
[2025-05-27 03:05:19,395]: Min: -0.43991259
[2025-05-27 03:05:19,395]: Max: 0.58655012
[2025-05-27 03:05:19,395]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 03:05:19,396]: Sample Values (25 elements): [0.08925125747919083, -0.026102721691131592, -0.07198303937911987, -0.11711495369672775, 5.807261095854907e-41, -6.098450916741604e-41, 0.010684317909181118, -2.7898163024387334e-16, -0.13756102323532104, -0.04326978698372841, -0.009394305758178234, 6.152120647925244e-41, 0.08654050529003143, 0.048891402781009674, -0.043855488300323486, 0.10890134423971176, -0.07746919244527817, -0.05469067022204399, -0.07443892955780029, -0.0017676962306722999, -0.12275926023721695, 0.07617322355508804, 0.076612189412117, 0.0018884247401729226, 0.16977597773075104]
[2025-05-27 03:05:19,396]: Mean: 0.00038389
[2025-05-27 03:05:19,396]: Min: -0.19859228
[2025-05-27 03:05:19,396]: Max: 0.29892918
[2025-05-27 03:05:19,396]: 


QAT of LeNet5 with relu6 down to 2 bits...
[2025-05-27 03:05:19,414]: [LeNet5_relu6_quantized_2_bits] after configure_qat:
[2025-05-27 03:05:19,427]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 03:05:47,065]: [LeNet5_relu6_quantized_2_bits] Epoch: 001 Train Loss: 2.1139 Train Acc: 0.2663 Eval Loss: 1.7148 Eval Acc: 0.3760 (LR: 0.00100000)
[2025-05-27 03:06:14,538]: [LeNet5_relu6_quantized_2_bits] Epoch: 002 Train Loss: 1.7406 Train Acc: 0.3627 Eval Loss: 1.5746 Eval Acc: 0.4254 (LR: 0.00100000)
[2025-05-27 03:06:42,191]: [LeNet5_relu6_quantized_2_bits] Epoch: 003 Train Loss: 1.6436 Train Acc: 0.3991 Eval Loss: 1.4602 Eval Acc: 0.4714 (LR: 0.00100000)
[2025-05-27 03:07:09,719]: [LeNet5_relu6_quantized_2_bits] Epoch: 004 Train Loss: 1.5849 Train Acc: 0.4254 Eval Loss: 1.4953 Eval Acc: 0.4555 (LR: 0.00100000)
[2025-05-27 03:07:37,224]: [LeNet5_relu6_quantized_2_bits] Epoch: 005 Train Loss: 1.5575 Train Acc: 0.4349 Eval Loss: 1.4976 Eval Acc: 0.4454 (LR: 0.00100000)
[2025-05-27 03:08:04,765]: [LeNet5_relu6_quantized_2_bits] Epoch: 006 Train Loss: 1.5288 Train Acc: 0.4517 Eval Loss: 1.4090 Eval Acc: 0.4962 (LR: 0.00100000)
[2025-05-27 03:08:32,451]: [LeNet5_relu6_quantized_2_bits] Epoch: 007 Train Loss: 1.5061 Train Acc: 0.4557 Eval Loss: 1.3862 Eval Acc: 0.5042 (LR: 0.00100000)
[2025-05-27 03:09:00,459]: [LeNet5_relu6_quantized_2_bits] Epoch: 008 Train Loss: 1.5009 Train Acc: 0.4580 Eval Loss: 1.4010 Eval Acc: 0.4987 (LR: 0.00100000)
[2025-05-27 03:09:28,032]: [LeNet5_relu6_quantized_2_bits] Epoch: 009 Train Loss: 1.4943 Train Acc: 0.4625 Eval Loss: 1.3217 Eval Acc: 0.5319 (LR: 0.00100000)
[2025-05-27 03:09:56,015]: [LeNet5_relu6_quantized_2_bits] Epoch: 010 Train Loss: 1.4823 Train Acc: 0.4662 Eval Loss: 1.3299 Eval Acc: 0.5229 (LR: 0.00100000)
[2025-05-27 03:10:23,813]: [LeNet5_relu6_quantized_2_bits] Epoch: 011 Train Loss: 1.4756 Train Acc: 0.4693 Eval Loss: 1.3445 Eval Acc: 0.5212 (LR: 0.00100000)
[2025-05-27 03:10:51,715]: [LeNet5_relu6_quantized_2_bits] Epoch: 012 Train Loss: 1.4686 Train Acc: 0.4713 Eval Loss: 1.3317 Eval Acc: 0.5241 (LR: 0.00100000)
[2025-05-27 03:11:19,308]: [LeNet5_relu6_quantized_2_bits] Epoch: 013 Train Loss: 1.4726 Train Acc: 0.4718 Eval Loss: 1.3930 Eval Acc: 0.5027 (LR: 0.00100000)
[2025-05-27 03:11:46,840]: [LeNet5_relu6_quantized_2_bits] Epoch: 014 Train Loss: 1.4609 Train Acc: 0.4770 Eval Loss: 1.3660 Eval Acc: 0.5108 (LR: 0.00100000)
[2025-05-27 03:12:14,419]: [LeNet5_relu6_quantized_2_bits] Epoch: 015 Train Loss: 1.4667 Train Acc: 0.4749 Eval Loss: 1.3512 Eval Acc: 0.5163 (LR: 0.00010000)
[2025-05-27 03:12:42,043]: [LeNet5_relu6_quantized_2_bits] Epoch: 016 Train Loss: 1.4078 Train Acc: 0.4946 Eval Loss: 1.3012 Eval Acc: 0.5342 (LR: 0.00010000)
[2025-05-27 03:13:09,617]: [LeNet5_relu6_quantized_2_bits] Epoch: 017 Train Loss: 1.3984 Train Acc: 0.5022 Eval Loss: 1.2924 Eval Acc: 0.5410 (LR: 0.00010000)
[2025-05-27 03:13:37,215]: [LeNet5_relu6_quantized_2_bits] Epoch: 018 Train Loss: 1.4087 Train Acc: 0.4970 Eval Loss: 1.2761 Eval Acc: 0.5472 (LR: 0.00010000)
[2025-05-27 03:14:04,668]: [LeNet5_relu6_quantized_2_bits] Epoch: 019 Train Loss: 1.3955 Train Acc: 0.4995 Eval Loss: 1.2873 Eval Acc: 0.5444 (LR: 0.00010000)
[2025-05-27 03:14:32,127]: [LeNet5_relu6_quantized_2_bits] Epoch: 020 Train Loss: 1.4060 Train Acc: 0.4967 Eval Loss: 1.2558 Eval Acc: 0.5572 (LR: 0.00010000)
[2025-05-27 03:14:59,644]: [LeNet5_relu6_quantized_2_bits] Epoch: 021 Train Loss: 1.3956 Train Acc: 0.5020 Eval Loss: 1.2818 Eval Acc: 0.5453 (LR: 0.00010000)
[2025-05-27 03:15:27,583]: [LeNet5_relu6_quantized_2_bits] Epoch: 022 Train Loss: 1.4005 Train Acc: 0.4973 Eval Loss: 1.2995 Eval Acc: 0.5386 (LR: 0.00010000)
[2025-05-27 03:15:55,120]: [LeNet5_relu6_quantized_2_bits] Epoch: 023 Train Loss: 1.3991 Train Acc: 0.4990 Eval Loss: 1.2856 Eval Acc: 0.5424 (LR: 0.00010000)
[2025-05-27 03:16:22,557]: [LeNet5_relu6_quantized_2_bits] Epoch: 024 Train Loss: 1.4036 Train Acc: 0.4960 Eval Loss: 1.3098 Eval Acc: 0.5333 (LR: 0.00010000)
[2025-05-27 03:16:49,967]: [LeNet5_relu6_quantized_2_bits] Epoch: 025 Train Loss: 1.4098 Train Acc: 0.4949 Eval Loss: 1.2951 Eval Acc: 0.5362 (LR: 0.00010000)
[2025-05-27 03:17:17,335]: [LeNet5_relu6_quantized_2_bits] Epoch: 026 Train Loss: 1.4090 Train Acc: 0.4941 Eval Loss: 1.2666 Eval Acc: 0.5470 (LR: 0.00001000)
[2025-05-27 03:17:45,008]: [LeNet5_relu6_quantized_2_bits] Epoch: 027 Train Loss: 1.3663 Train Acc: 0.5102 Eval Loss: 1.2401 Eval Acc: 0.5623 (LR: 0.00001000)
[2025-05-27 03:18:12,609]: [LeNet5_relu6_quantized_2_bits] Epoch: 028 Train Loss: 1.3732 Train Acc: 0.5070 Eval Loss: 1.2580 Eval Acc: 0.5518 (LR: 0.00001000)
[2025-05-27 03:18:40,255]: [LeNet5_relu6_quantized_2_bits] Epoch: 029 Train Loss: 1.3786 Train Acc: 0.5072 Eval Loss: 1.2784 Eval Acc: 0.5438 (LR: 0.00001000)
[2025-05-27 03:19:07,689]: [LeNet5_relu6_quantized_2_bits] Epoch: 030 Train Loss: 1.3799 Train Acc: 0.5067 Eval Loss: 1.2655 Eval Acc: 0.5457 (LR: 0.00001000)
[2025-05-27 03:19:35,070]: [LeNet5_relu6_quantized_2_bits] Epoch: 031 Train Loss: 1.3795 Train Acc: 0.5070 Eval Loss: 1.2467 Eval Acc: 0.5584 (LR: 0.00001000)
[2025-05-27 03:20:02,354]: [LeNet5_relu6_quantized_2_bits] Epoch: 032 Train Loss: 1.3779 Train Acc: 0.5054 Eval Loss: 1.2596 Eval Acc: 0.5529 (LR: 0.00001000)
[2025-05-27 03:20:29,691]: [LeNet5_relu6_quantized_2_bits] Epoch: 033 Train Loss: 1.3782 Train Acc: 0.5067 Eval Loss: 1.2719 Eval Acc: 0.5457 (LR: 0.00000100)
[2025-05-27 03:20:57,100]: [LeNet5_relu6_quantized_2_bits] Epoch: 034 Train Loss: 1.3591 Train Acc: 0.5142 Eval Loss: 1.3108 Eval Acc: 0.5377 (LR: 0.00000100)
[2025-05-27 03:21:24,529]: [LeNet5_relu6_quantized_2_bits] Epoch: 035 Train Loss: 1.3616 Train Acc: 0.5158 Eval Loss: 1.2505 Eval Acc: 0.5591 (LR: 0.00000100)
[2025-05-27 03:21:51,771]: [LeNet5_relu6_quantized_2_bits] Epoch: 036 Train Loss: 1.3657 Train Acc: 0.5144 Eval Loss: 1.2457 Eval Acc: 0.5544 (LR: 0.00000100)
[2025-05-27 03:22:19,368]: [LeNet5_relu6_quantized_2_bits] Epoch: 037 Train Loss: 1.3608 Train Acc: 0.5110 Eval Loss: 1.2393 Eval Acc: 0.5596 (LR: 0.00000100)
[2025-05-27 03:22:46,893]: [LeNet5_relu6_quantized_2_bits] Epoch: 038 Train Loss: 1.3609 Train Acc: 0.5146 Eval Loss: 1.2560 Eval Acc: 0.5531 (LR: 0.00000100)
[2025-05-27 03:23:14,591]: [LeNet5_relu6_quantized_2_bits] Epoch: 039 Train Loss: 1.3676 Train Acc: 0.5097 Eval Loss: 1.2604 Eval Acc: 0.5522 (LR: 0.00000100)
[2025-05-27 03:23:42,087]: [LeNet5_relu6_quantized_2_bits] Epoch: 040 Train Loss: 1.3611 Train Acc: 0.5124 Eval Loss: 1.2593 Eval Acc: 0.5515 (LR: 0.00000100)
[2025-05-27 03:24:09,542]: [LeNet5_relu6_quantized_2_bits] Epoch: 041 Train Loss: 1.3657 Train Acc: 0.5119 Eval Loss: 1.2746 Eval Acc: 0.5429 (LR: 0.00000100)
[2025-05-27 03:24:36,904]: [LeNet5_relu6_quantized_2_bits] Epoch: 042 Train Loss: 1.3657 Train Acc: 0.5128 Eval Loss: 1.2881 Eval Acc: 0.5478 (LR: 0.00000100)
[2025-05-27 03:25:04,507]: [LeNet5_relu6_quantized_2_bits] Epoch: 043 Train Loss: 1.3628 Train Acc: 0.5112 Eval Loss: 1.2664 Eval Acc: 0.5463 (LR: 0.00000010)
[2025-05-27 03:25:32,127]: [LeNet5_relu6_quantized_2_bits] Epoch: 044 Train Loss: 1.3471 Train Acc: 0.5178 Eval Loss: 1.2434 Eval Acc: 0.5602 (LR: 0.00000010)
[2025-05-27 03:25:59,607]: [LeNet5_relu6_quantized_2_bits] Epoch: 045 Train Loss: 1.3427 Train Acc: 0.5207 Eval Loss: 1.2382 Eval Acc: 0.5635 (LR: 0.00000010)
[2025-05-27 03:26:27,216]: [LeNet5_relu6_quantized_2_bits] Epoch: 046 Train Loss: 1.3546 Train Acc: 0.5167 Eval Loss: 1.2404 Eval Acc: 0.5641 (LR: 0.00000010)
[2025-05-27 03:26:54,802]: [LeNet5_relu6_quantized_2_bits] Epoch: 047 Train Loss: 1.3506 Train Acc: 0.5168 Eval Loss: 1.2557 Eval Acc: 0.5535 (LR: 0.00000010)
[2025-05-27 03:27:22,550]: [LeNet5_relu6_quantized_2_bits] Epoch: 048 Train Loss: 1.3560 Train Acc: 0.5133 Eval Loss: 1.2991 Eval Acc: 0.5353 (LR: 0.00000010)
[2025-05-27 03:27:50,185]: [LeNet5_relu6_quantized_2_bits] Epoch: 049 Train Loss: 1.3581 Train Acc: 0.5150 Eval Loss: 1.2904 Eval Acc: 0.5349 (LR: 0.00000010)
[2025-05-27 03:28:17,628]: [LeNet5_relu6_quantized_2_bits] Epoch: 050 Train Loss: 1.3594 Train Acc: 0.5144 Eval Loss: 1.2570 Eval Acc: 0.5570 (LR: 0.00000010)
[2025-05-27 03:28:45,253]: [LeNet5_relu6_quantized_2_bits] Epoch: 051 Train Loss: 1.3592 Train Acc: 0.5136 Eval Loss: 1.2528 Eval Acc: 0.5546 (LR: 0.00000010)
[2025-05-27 03:29:12,729]: [LeNet5_relu6_quantized_2_bits] Epoch: 052 Train Loss: 1.3658 Train Acc: 0.5131 Eval Loss: 1.2597 Eval Acc: 0.5481 (LR: 0.00000010)
[2025-05-27 03:29:40,317]: [LeNet5_relu6_quantized_2_bits] Epoch: 053 Train Loss: 1.3652 Train Acc: 0.5120 Eval Loss: 1.2700 Eval Acc: 0.5469 (LR: 0.00000010)
[2025-05-27 03:30:07,770]: [LeNet5_relu6_quantized_2_bits] Epoch: 054 Train Loss: 1.3648 Train Acc: 0.5110 Eval Loss: 1.2500 Eval Acc: 0.5552 (LR: 0.00000010)
[2025-05-27 03:30:35,324]: [LeNet5_relu6_quantized_2_bits] Epoch: 055 Train Loss: 1.3615 Train Acc: 0.5136 Eval Loss: 1.2570 Eval Acc: 0.5477 (LR: 0.00000010)
[2025-05-27 03:31:02,960]: [LeNet5_relu6_quantized_2_bits] Epoch: 056 Train Loss: 1.3665 Train Acc: 0.5118 Eval Loss: 1.3052 Eval Acc: 0.5375 (LR: 0.00000010)
[2025-05-27 03:31:30,568]: [LeNet5_relu6_quantized_2_bits] Epoch: 057 Train Loss: 1.3619 Train Acc: 0.5140 Eval Loss: 1.2509 Eval Acc: 0.5559 (LR: 0.00000010)
[2025-05-27 03:31:58,138]: [LeNet5_relu6_quantized_2_bits] Epoch: 058 Train Loss: 1.3562 Train Acc: 0.5155 Eval Loss: 1.2497 Eval Acc: 0.5594 (LR: 0.00000010)
[2025-05-27 03:32:25,717]: [LeNet5_relu6_quantized_2_bits] Epoch: 059 Train Loss: 1.3654 Train Acc: 0.5114 Eval Loss: 1.2575 Eval Acc: 0.5486 (LR: 0.00000010)
[2025-05-27 03:32:53,234]: [LeNet5_relu6_quantized_2_bits] Epoch: 060 Train Loss: 1.3640 Train Acc: 0.5115 Eval Loss: 1.2476 Eval Acc: 0.5525 (LR: 0.00000010)
[2025-05-27 03:32:53,235]: [LeNet5_relu6_quantized_2_bits] Best Eval Accuracy: 0.5641
[2025-05-27 03:32:53,265]: 


Quantization of model down to 2 bits finished
[2025-05-27 03:32:53,265]: Model Architecture:
[2025-05-27 03:32:53,284]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5914], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0350563526153564, max_val=0.7391337156295776)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4942], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.747650146484375, max_val=0.7350740432739258)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3394], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5105822086334229, max_val=0.507660984992981)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): ReLU6(
        inplace=True
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 03:32:53,285]: 
Model Weights:
[2025-05-27 03:32:53,285]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 03:32:53,285]: Sample Values (25 elements): [0.08120407909154892, 0.014127125032246113, 0.07173098623752594, -0.0636255219578743, -0.034014616161584854, -0.12469982355833054, 0.3106776773929596, -0.36369067430496216, -0.2904360592365265, 0.08035106956958771, 0.16052238643169403, -0.21236790716648102, 0.05204148218035698, 0.02733907848596573, 0.030072612687945366, 0.1453765630722046, -0.34046798944473267, -0.21502768993377686, 0.037616729736328125, -0.2520052492618561, -0.05246927589178085, -0.494917094707489, 0.018056007102131844, 0.1381700485944748, -0.052989352494478226]
[2025-05-27 03:32:53,285]: Mean: -0.00110701
[2025-05-27 03:32:53,286]: Min: -0.53121984
[2025-05-27 03:32:53,286]: Max: 0.57341093
[2025-05-27 03:32:53,291]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 03:32:53,291]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.591396689414978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.591396689414978, -0.591396689414978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 03:32:53,291]: Mean: -0.03203399
[2025-05-27 03:32:53,292]: Min: -1.18279338
[2025-05-27 03:32:53,292]: Max: 0.59139669
[2025-05-27 03:32:53,294]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 03:32:53,295]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4942414164543152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 03:32:53,295]: Mean: -0.00361414
[2025-05-27 03:32:53,295]: Min: -0.98848283
[2025-05-27 03:32:53,296]: Max: 0.49424142
[2025-05-27 03:32:53,297]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 03:32:53,299]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33941441774368286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 03:32:53,299]: Mean: -0.00353557
[2025-05-27 03:32:53,299]: Min: -0.67882884
[2025-05-27 03:32:53,300]: Max: 0.33941442
[2025-05-27 03:32:53,300]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 03:32:53,300]: Sample Values (25 elements): [-5.065133429148484e-41, 0.01840204745531082, 4.971246432038721e-41, 5.402846359050765e-41, -5.248703527975035e-41, 0.07280903309583664, 0.04246993362903595, -0.04421002045273781, -0.048544175922870636, -0.05304991081357002, -0.04169594869017601, 0.08156755566596985, 0.013636071234941483, -5.275888718182936e-41, 0.028657913208007812, -0.11550231277942657, -5.113618356014122e-41, -0.06167740747332573, 0.07703058421611786, 5.34329117431696e-41, 0.07000795751810074, -0.024470916017889977, 0.06282278150320053, 5.000113180403812e-41, 6.263383745992635e-41]
[2025-05-27 03:32:53,301]: Mean: 0.00331992
[2025-05-27 03:32:53,301]: Min: -0.17355487
[2025-05-27 03:32:53,301]: Max: 0.15151624
