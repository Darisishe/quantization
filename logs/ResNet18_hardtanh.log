[2025-05-26 15:48:50,142]: 
Training ResNet18 with hardtanh
[2025-05-26 15:50:32,665]: [ResNet18_hardtanh] Epoch: 001 Train Loss: 1.8369 Train Acc: 0.3267 Eval Loss: 1.5796 Eval Acc: 0.4170 (LR: 0.00100000)
[2025-05-26 15:52:14,205]: [ResNet18_hardtanh] Epoch: 002 Train Loss: 1.4848 Train Acc: 0.4571 Eval Loss: 1.3388 Eval Acc: 0.5106 (LR: 0.00100000)
[2025-05-26 15:53:55,527]: [ResNet18_hardtanh] Epoch: 003 Train Loss: 1.2437 Train Acc: 0.5523 Eval Loss: 1.1946 Eval Acc: 0.5776 (LR: 0.00100000)
[2025-05-26 15:55:36,681]: [ResNet18_hardtanh] Epoch: 004 Train Loss: 1.1030 Train Acc: 0.6018 Eval Loss: 1.0994 Eval Acc: 0.6128 (LR: 0.00100000)
[2025-05-26 15:57:17,564]: [ResNet18_hardtanh] Epoch: 005 Train Loss: 1.0265 Train Acc: 0.6331 Eval Loss: 1.0593 Eval Acc: 0.6231 (LR: 0.00100000)
[2025-05-26 15:58:58,685]: [ResNet18_hardtanh] Epoch: 006 Train Loss: 0.9677 Train Acc: 0.6533 Eval Loss: 1.0127 Eval Acc: 0.6491 (LR: 0.00100000)
[2025-05-26 16:00:39,808]: [ResNet18_hardtanh] Epoch: 007 Train Loss: 0.9145 Train Acc: 0.6727 Eval Loss: 0.9573 Eval Acc: 0.6677 (LR: 0.00100000)
[2025-05-26 16:02:20,668]: [ResNet18_hardtanh] Epoch: 008 Train Loss: 0.8613 Train Acc: 0.6899 Eval Loss: 0.9366 Eval Acc: 0.6737 (LR: 0.00100000)
[2025-05-26 16:04:01,565]: [ResNet18_hardtanh] Epoch: 009 Train Loss: 0.8231 Train Acc: 0.7085 Eval Loss: 0.8484 Eval Acc: 0.7042 (LR: 0.00100000)
[2025-05-26 16:05:42,280]: [ResNet18_hardtanh] Epoch: 010 Train Loss: 0.7842 Train Acc: 0.7210 Eval Loss: 0.8321 Eval Acc: 0.7123 (LR: 0.00100000)
[2025-05-26 16:07:22,933]: [ResNet18_hardtanh] Epoch: 011 Train Loss: 0.7441 Train Acc: 0.7382 Eval Loss: 0.8170 Eval Acc: 0.7231 (LR: 0.00100000)
[2025-05-26 16:09:03,592]: [ResNet18_hardtanh] Epoch: 012 Train Loss: 0.7110 Train Acc: 0.7496 Eval Loss: 0.8303 Eval Acc: 0.7185 (LR: 0.00100000)
[2025-05-26 16:10:44,358]: [ResNet18_hardtanh] Epoch: 013 Train Loss: 0.6736 Train Acc: 0.7638 Eval Loss: 0.8688 Eval Acc: 0.7043 (LR: 0.00100000)
[2025-05-26 16:12:24,976]: [ResNet18_hardtanh] Epoch: 014 Train Loss: 0.6405 Train Acc: 0.7772 Eval Loss: 0.6528 Eval Acc: 0.7763 (LR: 0.00100000)
[2025-05-26 16:14:05,459]: [ResNet18_hardtanh] Epoch: 015 Train Loss: 0.6186 Train Acc: 0.7850 Eval Loss: 0.7838 Eval Acc: 0.7384 (LR: 0.00100000)
[2025-05-26 16:15:45,798]: [ResNet18_hardtanh] Epoch: 016 Train Loss: 0.5935 Train Acc: 0.7942 Eval Loss: 0.6455 Eval Acc: 0.7836 (LR: 0.00100000)
[2025-05-26 16:17:26,196]: [ResNet18_hardtanh] Epoch: 017 Train Loss: 0.5645 Train Acc: 0.8047 Eval Loss: 0.6824 Eval Acc: 0.7778 (LR: 0.00100000)
[2025-05-26 16:19:06,718]: [ResNet18_hardtanh] Epoch: 018 Train Loss: 0.5546 Train Acc: 0.8083 Eval Loss: 0.6734 Eval Acc: 0.7776 (LR: 0.00100000)
[2025-05-26 16:20:47,341]: [ResNet18_hardtanh] Epoch: 019 Train Loss: 0.5300 Train Acc: 0.8140 Eval Loss: 0.6707 Eval Acc: 0.7745 (LR: 0.00100000)
[2025-05-26 16:22:28,921]: [ResNet18_hardtanh] Epoch: 020 Train Loss: 0.5120 Train Acc: 0.8197 Eval Loss: 0.6562 Eval Acc: 0.7811 (LR: 0.00100000)
[2025-05-26 16:24:09,422]: [ResNet18_hardtanh] Epoch: 021 Train Loss: 0.5006 Train Acc: 0.8261 Eval Loss: 0.8727 Eval Acc: 0.7374 (LR: 0.00100000)
[2025-05-26 16:25:50,440]: [ResNet18_hardtanh] Epoch: 022 Train Loss: 0.4887 Train Acc: 0.8300 Eval Loss: 0.6360 Eval Acc: 0.7873 (LR: 0.00100000)
[2025-05-26 16:27:32,103]: [ResNet18_hardtanh] Epoch: 023 Train Loss: 0.4732 Train Acc: 0.8372 Eval Loss: 0.6441 Eval Acc: 0.7938 (LR: 0.00100000)
[2025-05-26 16:29:14,880]: [ResNet18_hardtanh] Epoch: 024 Train Loss: 0.4625 Train Acc: 0.8408 Eval Loss: 0.7160 Eval Acc: 0.7752 (LR: 0.00100000)
[2025-05-26 16:30:58,781]: [ResNet18_hardtanh] Epoch: 025 Train Loss: 0.4476 Train Acc: 0.8453 Eval Loss: 0.8630 Eval Acc: 0.7373 (LR: 0.00100000)
[2025-05-26 16:32:43,173]: [ResNet18_hardtanh] Epoch: 026 Train Loss: 0.4389 Train Acc: 0.8477 Eval Loss: 0.5408 Eval Acc: 0.8191 (LR: 0.00100000)
[2025-05-26 16:34:27,057]: [ResNet18_hardtanh] Epoch: 027 Train Loss: 0.4345 Train Acc: 0.8494 Eval Loss: 0.5573 Eval Acc: 0.8221 (LR: 0.00100000)
[2025-05-26 16:36:11,668]: [ResNet18_hardtanh] Epoch: 028 Train Loss: 0.4209 Train Acc: 0.8543 Eval Loss: 0.5552 Eval Acc: 0.8199 (LR: 0.00100000)
[2025-05-26 16:37:59,699]: [ResNet18_hardtanh] Epoch: 029 Train Loss: 0.4083 Train Acc: 0.8591 Eval Loss: 0.7673 Eval Acc: 0.7659 (LR: 0.00100000)
[2025-05-26 16:39:46,770]: [ResNet18_hardtanh] Epoch: 030 Train Loss: 0.4012 Train Acc: 0.8619 Eval Loss: 0.4751 Eval Acc: 0.8484 (LR: 0.00100000)
[2025-05-26 16:41:33,236]: [ResNet18_hardtanh] Epoch: 031 Train Loss: 0.3964 Train Acc: 0.8628 Eval Loss: 0.4878 Eval Acc: 0.8369 (LR: 0.00100000)
[2025-05-26 16:43:19,705]: [ResNet18_hardtanh] Epoch: 032 Train Loss: 0.3845 Train Acc: 0.8667 Eval Loss: 0.5127 Eval Acc: 0.8281 (LR: 0.00100000)
[2025-05-26 16:45:04,509]: [ResNet18_hardtanh] Epoch: 033 Train Loss: 0.3857 Train Acc: 0.8671 Eval Loss: 0.6189 Eval Acc: 0.7957 (LR: 0.00100000)
[2025-05-26 16:46:47,881]: [ResNet18_hardtanh] Epoch: 034 Train Loss: 0.3727 Train Acc: 0.8718 Eval Loss: 0.4601 Eval Acc: 0.8429 (LR: 0.00100000)
[2025-05-26 16:48:30,375]: [ResNet18_hardtanh] Epoch: 035 Train Loss: 0.3688 Train Acc: 0.8726 Eval Loss: 0.5573 Eval Acc: 0.8196 (LR: 0.00100000)
[2025-05-26 16:50:12,106]: [ResNet18_hardtanh] Epoch: 036 Train Loss: 0.3636 Train Acc: 0.8735 Eval Loss: 0.5974 Eval Acc: 0.8104 (LR: 0.00100000)
[2025-05-26 16:51:53,375]: [ResNet18_hardtanh] Epoch: 037 Train Loss: 0.3584 Train Acc: 0.8752 Eval Loss: 0.6455 Eval Acc: 0.8216 (LR: 0.00100000)
[2025-05-26 16:53:33,948]: [ResNet18_hardtanh] Epoch: 038 Train Loss: 0.3539 Train Acc: 0.8758 Eval Loss: 0.6372 Eval Acc: 0.8098 (LR: 0.00100000)
[2025-05-26 16:55:16,354]: [ResNet18_hardtanh] Epoch: 039 Train Loss: 0.3511 Train Acc: 0.8778 Eval Loss: 0.5612 Eval Acc: 0.8206 (LR: 0.00100000)
[2025-05-26 16:57:00,682]: [ResNet18_hardtanh] Epoch: 040 Train Loss: 0.3427 Train Acc: 0.8818 Eval Loss: 0.5346 Eval Acc: 0.8315 (LR: 0.00010000)
[2025-05-26 16:58:46,962]: [ResNet18_hardtanh] Epoch: 041 Train Loss: 0.2458 Train Acc: 0.9150 Eval Loss: 0.3236 Eval Acc: 0.8966 (LR: 0.00010000)
[2025-05-26 17:00:34,460]: [ResNet18_hardtanh] Epoch: 042 Train Loss: 0.2224 Train Acc: 0.9238 Eval Loss: 0.3134 Eval Acc: 0.8981 (LR: 0.00010000)
[2025-05-26 17:02:17,681]: [ResNet18_hardtanh] Epoch: 043 Train Loss: 0.2059 Train Acc: 0.9289 Eval Loss: 0.3227 Eval Acc: 0.8963 (LR: 0.00010000)
[2025-05-26 17:03:58,859]: [ResNet18_hardtanh] Epoch: 044 Train Loss: 0.2019 Train Acc: 0.9300 Eval Loss: 0.3089 Eval Acc: 0.9001 (LR: 0.00010000)
[2025-05-26 17:05:39,601]: [ResNet18_hardtanh] Epoch: 045 Train Loss: 0.1963 Train Acc: 0.9317 Eval Loss: 0.3165 Eval Acc: 0.8982 (LR: 0.00010000)
[2025-05-26 17:07:22,029]: [ResNet18_hardtanh] Epoch: 046 Train Loss: 0.1892 Train Acc: 0.9337 Eval Loss: 0.3163 Eval Acc: 0.8995 (LR: 0.00010000)
[2025-05-26 17:09:07,524]: [ResNet18_hardtanh] Epoch: 047 Train Loss: 0.1841 Train Acc: 0.9355 Eval Loss: 0.3100 Eval Acc: 0.9045 (LR: 0.00010000)
[2025-05-26 17:10:53,060]: [ResNet18_hardtanh] Epoch: 048 Train Loss: 0.1798 Train Acc: 0.9377 Eval Loss: 0.3108 Eval Acc: 0.9039 (LR: 0.00010000)
[2025-05-26 17:12:37,600]: [ResNet18_hardtanh] Epoch: 049 Train Loss: 0.1764 Train Acc: 0.9389 Eval Loss: 0.3123 Eval Acc: 0.9029 (LR: 0.00010000)
[2025-05-26 17:14:19,088]: [ResNet18_hardtanh] Epoch: 050 Train Loss: 0.1690 Train Acc: 0.9407 Eval Loss: 0.3132 Eval Acc: 0.9036 (LR: 0.00001000)
[2025-05-26 17:15:59,860]: [ResNet18_hardtanh] Epoch: 051 Train Loss: 0.1594 Train Acc: 0.9449 Eval Loss: 0.3058 Eval Acc: 0.9059 (LR: 0.00001000)
[2025-05-26 17:17:42,288]: [ResNet18_hardtanh] Epoch: 052 Train Loss: 0.1565 Train Acc: 0.9470 Eval Loss: 0.3076 Eval Acc: 0.9061 (LR: 0.00001000)
[2025-05-26 17:19:26,345]: [ResNet18_hardtanh] Epoch: 053 Train Loss: 0.1529 Train Acc: 0.9482 Eval Loss: 0.3048 Eval Acc: 0.9058 (LR: 0.00001000)
[2025-05-26 17:21:10,988]: [ResNet18_hardtanh] Epoch: 054 Train Loss: 0.1533 Train Acc: 0.9464 Eval Loss: 0.3043 Eval Acc: 0.9065 (LR: 0.00001000)
[2025-05-26 17:22:57,076]: [ResNet18_hardtanh] Epoch: 055 Train Loss: 0.1528 Train Acc: 0.9475 Eval Loss: 0.3088 Eval Acc: 0.9069 (LR: 0.00001000)
[2025-05-26 17:24:39,279]: [ResNet18_hardtanh] Epoch: 056 Train Loss: 0.1498 Train Acc: 0.9481 Eval Loss: 0.3048 Eval Acc: 0.9073 (LR: 0.00001000)
[2025-05-26 17:26:20,147]: [ResNet18_hardtanh] Epoch: 057 Train Loss: 0.1516 Train Acc: 0.9470 Eval Loss: 0.3041 Eval Acc: 0.9075 (LR: 0.00001000)
[2025-05-26 17:28:02,682]: [ResNet18_hardtanh] Epoch: 058 Train Loss: 0.1491 Train Acc: 0.9488 Eval Loss: 0.3077 Eval Acc: 0.9051 (LR: 0.00001000)
[2025-05-26 17:29:46,030]: [ResNet18_hardtanh] Epoch: 059 Train Loss: 0.1488 Train Acc: 0.9488 Eval Loss: 0.3059 Eval Acc: 0.9067 (LR: 0.00001000)
[2025-05-26 17:31:31,819]: [ResNet18_hardtanh] Epoch: 060 Train Loss: 0.1523 Train Acc: 0.9463 Eval Loss: 0.3074 Eval Acc: 0.9062 (LR: 0.00001000)
[2025-05-26 17:33:19,145]: [ResNet18_hardtanh] Epoch: 061 Train Loss: 0.1466 Train Acc: 0.9498 Eval Loss: 0.3098 Eval Acc: 0.9060 (LR: 0.00001000)
[2025-05-26 17:35:02,019]: [ResNet18_hardtanh] Epoch: 062 Train Loss: 0.1480 Train Acc: 0.9485 Eval Loss: 0.3093 Eval Acc: 0.9052 (LR: 0.00001000)
[2025-05-26 17:36:42,996]: [ResNet18_hardtanh] Epoch: 063 Train Loss: 0.1492 Train Acc: 0.9477 Eval Loss: 0.3069 Eval Acc: 0.9068 (LR: 0.00000100)
[2025-05-26 17:38:23,669]: [ResNet18_hardtanh] Epoch: 064 Train Loss: 0.1459 Train Acc: 0.9489 Eval Loss: 0.3071 Eval Acc: 0.9072 (LR: 0.00000100)
[2025-05-26 17:40:05,986]: [ResNet18_hardtanh] Epoch: 065 Train Loss: 0.1486 Train Acc: 0.9484 Eval Loss: 0.3065 Eval Acc: 0.9076 (LR: 0.00000100)
[2025-05-26 17:41:50,468]: [ResNet18_hardtanh] Epoch: 066 Train Loss: 0.1457 Train Acc: 0.9497 Eval Loss: 0.3056 Eval Acc: 0.9068 (LR: 0.00000100)
[2025-05-26 17:43:34,965]: [ResNet18_hardtanh] Epoch: 067 Train Loss: 0.1459 Train Acc: 0.9511 Eval Loss: 0.3080 Eval Acc: 0.9059 (LR: 0.00000100)
[2025-05-26 17:45:20,600]: [ResNet18_hardtanh] Epoch: 068 Train Loss: 0.1444 Train Acc: 0.9495 Eval Loss: 0.3073 Eval Acc: 0.9078 (LR: 0.00000100)
[2025-05-26 17:47:02,602]: [ResNet18_hardtanh] Epoch: 069 Train Loss: 0.1457 Train Acc: 0.9501 Eval Loss: 0.3072 Eval Acc: 0.9074 (LR: 0.00000010)
[2025-05-26 17:48:43,479]: [ResNet18_hardtanh] Epoch: 070 Train Loss: 0.1465 Train Acc: 0.9497 Eval Loss: 0.3070 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-05-26 17:50:25,659]: [ResNet18_hardtanh] Epoch: 071 Train Loss: 0.1447 Train Acc: 0.9502 Eval Loss: 0.3097 Eval Acc: 0.9064 (LR: 0.00000010)
[2025-05-26 17:52:09,694]: [ResNet18_hardtanh] Epoch: 072 Train Loss: 0.1471 Train Acc: 0.9489 Eval Loss: 0.3060 Eval Acc: 0.9069 (LR: 0.00000010)
[2025-05-26 17:53:54,920]: [ResNet18_hardtanh] Epoch: 073 Train Loss: 0.1453 Train Acc: 0.9499 Eval Loss: 0.3073 Eval Acc: 0.9066 (LR: 0.00000010)
[2025-05-26 17:55:40,992]: [ResNet18_hardtanh] Epoch: 074 Train Loss: 0.1468 Train Acc: 0.9490 Eval Loss: 0.3056 Eval Acc: 0.9066 (LR: 0.00000010)
[2025-05-26 17:57:23,226]: [ResNet18_hardtanh] Epoch: 075 Train Loss: 0.1442 Train Acc: 0.9504 Eval Loss: 0.3066 Eval Acc: 0.9072 (LR: 0.00000010)
[2025-05-26 17:59:04,438]: [ResNet18_hardtanh] Epoch: 076 Train Loss: 0.1453 Train Acc: 0.9491 Eval Loss: 0.3056 Eval Acc: 0.9073 (LR: 0.00000010)
[2025-05-26 18:00:47,391]: [ResNet18_hardtanh] Epoch: 077 Train Loss: 0.1451 Train Acc: 0.9488 Eval Loss: 0.3053 Eval Acc: 0.9072 (LR: 0.00000010)
[2025-05-26 18:02:31,219]: [ResNet18_hardtanh] Epoch: 078 Train Loss: 0.1434 Train Acc: 0.9504 Eval Loss: 0.3076 Eval Acc: 0.9069 (LR: 0.00000010)
[2025-05-26 18:04:16,422]: [ResNet18_hardtanh] Epoch: 079 Train Loss: 0.1475 Train Acc: 0.9493 Eval Loss: 0.3072 Eval Acc: 0.9079 (LR: 0.00000010)
[2025-05-26 18:06:04,039]: [ResNet18_hardtanh] Epoch: 080 Train Loss: 0.1435 Train Acc: 0.9496 Eval Loss: 0.3081 Eval Acc: 0.9063 (LR: 0.00000010)
[2025-05-26 18:07:47,472]: [ResNet18_hardtanh] Epoch: 081 Train Loss: 0.1451 Train Acc: 0.9508 Eval Loss: 0.3069 Eval Acc: 0.9062 (LR: 0.00000010)
[2025-05-26 18:09:28,808]: [ResNet18_hardtanh] Epoch: 082 Train Loss: 0.1471 Train Acc: 0.9493 Eval Loss: 0.3077 Eval Acc: 0.9073 (LR: 0.00000010)
[2025-05-26 18:11:09,859]: [ResNet18_hardtanh] Epoch: 083 Train Loss: 0.1482 Train Acc: 0.9482 Eval Loss: 0.3074 Eval Acc: 0.9069 (LR: 0.00000010)
[2025-05-26 18:12:52,184]: [ResNet18_hardtanh] Epoch: 084 Train Loss: 0.1451 Train Acc: 0.9499 Eval Loss: 0.3062 Eval Acc: 0.9073 (LR: 0.00000010)
[2025-05-26 18:14:38,280]: [ResNet18_hardtanh] Epoch: 085 Train Loss: 0.1467 Train Acc: 0.9489 Eval Loss: 0.3069 Eval Acc: 0.9074 (LR: 0.00000010)
[2025-05-26 18:16:24,158]: [ResNet18_hardtanh] Epoch: 086 Train Loss: 0.1425 Train Acc: 0.9503 Eval Loss: 0.3078 Eval Acc: 0.9079 (LR: 0.00000010)
[2025-05-26 18:18:08,677]: [ResNet18_hardtanh] Epoch: 087 Train Loss: 0.1458 Train Acc: 0.9497 Eval Loss: 0.3079 Eval Acc: 0.9067 (LR: 0.00000010)
[2025-05-26 18:19:49,948]: [ResNet18_hardtanh] Epoch: 088 Train Loss: 0.1447 Train Acc: 0.9499 Eval Loss: 0.3075 Eval Acc: 0.9067 (LR: 0.00000010)
[2025-05-26 18:21:31,068]: [ResNet18_hardtanh] Epoch: 089 Train Loss: 0.1432 Train Acc: 0.9506 Eval Loss: 0.3065 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-05-26 18:23:13,381]: [ResNet18_hardtanh] Epoch: 090 Train Loss: 0.1424 Train Acc: 0.9514 Eval Loss: 0.3071 Eval Acc: 0.9057 (LR: 0.00000010)
[2025-05-26 18:24:57,705]: [ResNet18_hardtanh] Epoch: 091 Train Loss: 0.1442 Train Acc: 0.9490 Eval Loss: 0.3064 Eval Acc: 0.9068 (LR: 0.00000010)
[2025-05-26 18:26:42,786]: [ResNet18_hardtanh] Epoch: 092 Train Loss: 0.1450 Train Acc: 0.9494 Eval Loss: 0.3080 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-05-26 18:28:27,588]: [ResNet18_hardtanh] Epoch: 093 Train Loss: 0.1452 Train Acc: 0.9506 Eval Loss: 0.3090 Eval Acc: 0.9064 (LR: 0.00000010)
[2025-05-26 18:30:09,110]: [ResNet18_hardtanh] Epoch: 094 Train Loss: 0.1437 Train Acc: 0.9509 Eval Loss: 0.3069 Eval Acc: 0.9078 (LR: 0.00000010)
[2025-05-26 18:31:49,934]: [ResNet18_hardtanh] Epoch: 095 Train Loss: 0.1436 Train Acc: 0.9503 Eval Loss: 0.3064 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-05-26 18:33:32,300]: [ResNet18_hardtanh] Epoch: 096 Train Loss: 0.1445 Train Acc: 0.9509 Eval Loss: 0.3083 Eval Acc: 0.9066 (LR: 0.00000010)
[2025-05-26 18:35:16,288]: [ResNet18_hardtanh] Epoch: 097 Train Loss: 0.1447 Train Acc: 0.9498 Eval Loss: 0.3037 Eval Acc: 0.9078 (LR: 0.00000010)
[2025-05-26 18:37:01,648]: [ResNet18_hardtanh] Epoch: 098 Train Loss: 0.1452 Train Acc: 0.9509 Eval Loss: 0.3064 Eval Acc: 0.9072 (LR: 0.00000010)
[2025-05-26 18:38:48,655]: [ResNet18_hardtanh] Epoch: 099 Train Loss: 0.1470 Train Acc: 0.9492 Eval Loss: 0.3051 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-05-26 18:40:31,940]: [ResNet18_hardtanh] Epoch: 100 Train Loss: 0.1444 Train Acc: 0.9499 Eval Loss: 0.3067 Eval Acc: 0.9059 (LR: 0.00000010)
[2025-05-26 18:40:31,941]: [ResNet18_hardtanh] Best Eval Accuracy: 0.9080
[2025-05-26 18:40:32,014]: 
Training of full-precision model finished!
[2025-05-26 18:40:32,014]: Model Architecture:
[2025-05-26 18:40:32,015]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-26 18:40:32,015]: 
Model Weights:
[2025-05-26 18:40:32,015]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-26 18:40:32,030]: Sample Values (25 elements): [0.3323904573917389, 0.1282762736082077, 0.10147632658481598, -0.036635007709264755, -0.0028297423850744963, -0.2919318377971649, -0.011911693960428238, 0.12211421877145767, 0.06686883419752121, 0.1834312379360199, 0.10329876840114594, -0.002985709346830845, 0.2575394809246063, 0.036120280623435974, 0.12148523330688477, 0.016251150518655777, -0.04340691864490509, 0.22480323910713196, 0.07014197111129761, -0.023724514991044998, 0.04311172291636467, -0.04441818222403526, 0.16489477455615997, 0.05481807515025139, -0.18545374274253845]
[2025-05-26 18:40:32,031]: Mean: -0.00002830
[2025-05-26 18:40:32,031]: Min: -0.38869253
[2025-05-26 18:40:32,031]: Max: 0.36930123
[2025-05-26 18:40:32,031]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-26 18:40:32,032]: Sample Values (25 elements): [1.1092348098754883, 0.5597167015075684, 0.7397830486297607, 0.9308966398239136, 0.6173413395881653, 0.9316685795783997, 0.7912076115608215, 0.7988898754119873, 1.006753921508789, 0.7514317631721497, 1.0937577486038208, 0.8903842568397522, 0.650066614151001, 0.44108232855796814, 0.7700077295303345, 0.9899574518203735, 0.5279788374900818, 0.6055505275726318, 0.8042585849761963, 0.7286974787712097, 0.6736760139465332, 0.8600450754165649, 0.43048182129859924, 0.7612162232398987, 0.6767230033874512]
[2025-05-26 18:40:32,032]: Mean: 0.68684626
[2025-05-26 18:40:32,032]: Min: 0.15673158
[2025-05-26 18:40:32,032]: Max: 1.10923481
[2025-05-26 18:40:32,032]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:40:32,039]: Sample Values (25 elements): [-0.045817386358976364, 0.014273157343268394, 0.09657832980155945, -0.013685483485460281, 0.004417099058628082, -0.08465222269296646, 0.051963698118925095, 0.09823541343212128, -0.0024015423841774464, -0.004659306723624468, -0.09817741066217422, -0.043803948909044266, -0.011166544631123543, -0.06712587922811508, 0.002503827679902315, -0.006709014531224966, 0.007612219545990229, -0.02188648283481598, -0.004017579834908247, 0.057432401925325394, -0.06722892075777054, -0.09393433481454849, -0.07252953201532364, 0.00764518091455102, 0.026300586760044098]
[2025-05-26 18:40:32,041]: Mean: 0.00023673
[2025-05-26 18:40:32,043]: Min: -0.31243229
[2025-05-26 18:40:32,044]: Max: 0.32518181
[2025-05-26 18:40:32,044]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 18:40:32,047]: Sample Values (25 elements): [0.1830960214138031, 0.4001959264278412, 0.561764121055603, 1.0632517337799072, 1.2686078548431396, 0.8711012005805969, 0.002421660115942359, 0.8025587797164917, 0.7622976303100586, 0.7149173617362976, 0.9558274745941162, 0.7191362977027893, 0.638062596321106, 0.9547895193099976, 0.8695837259292603, 0.8839401602745056, 0.9478245973587036, 0.631584107875824, 0.737319827079773, 0.9245995879173279, 0.9300441741943359, 0.4707174003124237, 0.5140123963356018, 0.7135992646217346, 0.6810270547866821]
[2025-05-26 18:40:32,052]: Mean: 0.76401162
[2025-05-26 18:40:32,060]: Min: 0.00242166
[2025-05-26 18:40:32,076]: Max: 1.30668426
[2025-05-26 18:40:32,077]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:40:32,077]: Sample Values (25 elements): [0.05178731307387352, 0.025695675984025, -0.02833298221230507, -0.002895707031711936, -0.057648688554763794, 0.0021315489429980516, -0.03757551312446594, 0.04551997035741806, 0.029868442565202713, -0.02431887574493885, -0.06426618248224258, 0.01761697791516781, -0.005435287486761808, 0.05604282021522522, -0.026042794808745384, -0.012764650397002697, 0.00832682941108942, 0.02874092385172844, -0.05350197106599808, -0.094095379114151, -4.202801846986404e-06, -0.00784759595990181, -0.04985574632883072, -0.006920735351741314, -0.07897336781024933]
[2025-05-26 18:40:32,078]: Mean: 0.00022883
[2025-05-26 18:40:32,078]: Min: -0.34240001
[2025-05-26 18:40:32,078]: Max: 0.51354599
[2025-05-26 18:40:32,078]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 18:40:32,078]: Sample Values (25 elements): [0.6700879335403442, 0.9146189093589783, 0.8600916862487793, 1.0089643001556396, 0.8215913772583008, 0.7976464629173279, 0.8381012082099915, 0.9175088405609131, 0.8968488574028015, 0.9161233305931091, 0.7946454286575317, 0.9708231091499329, 1.232285976409912, 0.9888650178909302, 1.4313994646072388, 0.7668187618255615, 1.0304064750671387, 0.6663153767585754, 0.9536491632461548, 0.900346040725708, 0.7934236526489258, 0.8611686825752258, 0.6966736912727356, 0.7741481065750122, 0.6952664852142334]
[2025-05-26 18:40:32,079]: Mean: 0.84467673
[2025-05-26 18:40:32,079]: Min: 0.61582071
[2025-05-26 18:40:32,081]: Max: 1.43139946
[2025-05-26 18:40:32,081]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:40:32,087]: Sample Values (25 elements): [0.11071570962667465, 0.11725866049528122, -0.010202324017882347, 0.07660873979330063, 0.029157515615224838, -0.08381132781505585, -0.12327597290277481, 0.011011416092514992, -0.10948891937732697, -0.01358597632497549, 0.030099453404545784, -0.051349736750125885, 0.02325393818318844, -0.03722783923149109, 0.020912669599056244, 0.022732000797986984, 0.008123726584017277, -0.07922924309968948, 0.02713443711400032, -0.00994190014898777, -0.09705990552902222, -0.07416588068008423, -0.07235145568847656, -0.009223474189639091, 0.05177905037999153]
[2025-05-26 18:40:32,089]: Mean: -0.00045756
[2025-05-26 18:40:32,090]: Min: -0.40245414
[2025-05-26 18:40:32,091]: Max: 0.34125990
[2025-05-26 18:40:32,091]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 18:40:32,098]: Sample Values (25 elements): [1.0179855823516846, 1.0261037349700928, 0.7718960046768188, 0.6364375948905945, 0.9955512881278992, 0.3444730341434479, 0.3443450331687927, 1.3934167623519897, 1.1293091773986816, 0.6247215270996094, 0.6733980178833008, 0.8317740559577942, 1.1039084196090698, 0.6667993068695068, 0.5102972984313965, 0.8522990942001343, 0.7539190053939819, 1.2056089639663696, 1.0807929039001465, 0.8315871357917786, 0.8593430519104004, -0.0006068561342544854, 1.2133253812789917, 1.2941999435424805, 1.0135077238082886]
[2025-05-26 18:40:32,106]: Mean: 0.86531806
[2025-05-26 18:40:32,122]: Min: -0.00060686
[2025-05-26 18:40:32,123]: Max: 1.50295520
[2025-05-26 18:40:32,123]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 18:40:32,124]: Sample Values (25 elements): [-0.016362935304641724, -0.019045179709792137, 0.04165014624595642, -0.021309996023774147, -0.07921162247657776, 0.05146156996488571, -0.07607487589120865, -0.06463928520679474, 0.022289898246526718, -0.05624350905418396, -0.04382888972759247, 0.013329294510185719, 0.015456833876669407, -0.05201150104403496, -0.05746307969093323, -0.01591774821281433, -0.08170382678508759, -0.0004555034975055605, -0.03721876069903374, -0.056796807795763016, -0.015668092295527458, -0.0021747818682342768, -0.03301163390278816, 0.032851990312337875, 0.060656625777482986]
[2025-05-26 18:40:32,124]: Mean: -0.00011170
[2025-05-26 18:40:32,124]: Min: -0.38130468
[2025-05-26 18:40:32,124]: Max: 0.45894843
[2025-05-26 18:40:32,124]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 18:40:32,125]: Sample Values (25 elements): [0.9131428599357605, 0.6359900236129761, 1.3354800939559937, 0.7229998707771301, 0.7007138133049011, 0.7405120134353638, 1.0905494689941406, 0.856229305267334, 0.9883158206939697, 0.7761650085449219, 0.9223922491073608, 0.7159035205841064, 1.2432125806808472, 0.6507336497306824, 0.7795131206512451, 1.0540066957473755, 1.0065586566925049, 0.5378159880638123, 0.821008026599884, 0.5832272171974182, 0.9344832301139832, 0.6994795203208923, 0.749255895614624, 1.0091469287872314, 0.7620473504066467]
[2025-05-26 18:40:32,126]: Mean: 0.84055907
[2025-05-26 18:40:32,131]: Min: 0.51434463
[2025-05-26 18:40:32,133]: Max: 1.37841272
[2025-05-26 18:40:32,133]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-26 18:40:32,136]: Sample Values (25 elements): [1.4869639413736175e-20, -7.379815084803107e-18, 0.024713991209864616, -0.07390688359737396, 0.03171428665518761, 0.008303740993142128, 0.002175377681851387, -0.032641712576150894, 3.0408040994044094e-13, -0.017051029950380325, -0.05017036944627762, -0.029783450067043304, -0.019805315881967545, -7.369432856591618e-21, 0.06987404078245163, 1.6175340325044513e-15, 0.05570470541715622, -5.4678205609275005e-21, 0.055456336587667465, -6.665658583851914e-31, -0.021908704191446304, -0.059798698872327805, -0.05313221365213394, 0.011034199967980385, -0.030538080260157585]
[2025-05-26 18:40:32,137]: Mean: 0.00085572
[2025-05-26 18:40:32,139]: Min: -0.38932452
[2025-05-26 18:40:32,144]: Max: 0.34105432
[2025-05-26 18:40:32,144]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-26 18:40:32,168]: Sample Values (25 elements): [-6.583827598660719e-07, 0.6496527791023254, 0.9250109791755676, 0.195288747549057, 8.807381277620152e-07, 0.7665858864784241, 0.746040403842926, 0.7013153433799744, 0.8516510128974915, 1.1530567407608032, 0.5679075121879578, 9.56232662496781e-12, -1.0354670898493623e-08, 0.7700081467628479, 1.3868766757241247e-07, 3.708437290583788e-08, 0.004130979999899864, 0.2151082456111908, 0.9429202675819397, 0.11011290550231934, 0.8058263659477234, -1.5209542425509426e-07, 0.9447070956230164, 3.2947672368166536e-10, 0.9457846879959106]
[2025-05-26 18:40:32,169]: Mean: 0.45618039
[2025-05-26 18:40:32,169]: Min: -0.00001958
[2025-05-26 18:40:32,169]: Max: 1.15571630
[2025-05-26 18:40:32,170]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 18:40:32,177]: Sample Values (25 elements): [0.05368604511022568, -4.791731660464158e-19, -0.0329023152589798, 0.019051607698202133, -0.004526724107563496, 2.675540713426017e-07, -4.255897985237919e-15, -0.006372914649546146, 0.006977763958275318, -7.814216634871585e-11, -0.00955265387892723, 0.0011196635896340013, -0.12701982259750366, 0.01649710163474083, -0.0039135911501944065, 9.379397852171678e-06, 5.310586459472688e-08, 0.057127952575683594, 2.570367685855486e-32, 0.11195642501115799, 0.05211161822080612, 9.166921444148102e-08, -2.0779671039417025e-10, 0.028858115896582603, 0.002374624367803335]
[2025-05-26 18:40:32,179]: Mean: 0.00023235
[2025-05-26 18:40:32,180]: Min: -0.26107457
[2025-05-26 18:40:32,182]: Max: 0.30772209
[2025-05-26 18:40:32,182]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-26 18:40:32,185]: Sample Values (25 elements): [0.6567193865776062, 0.8539546132087708, 0.7517873644828796, 0.29677140712738037, 0.5150371193885803, 0.5063471794128418, 0.6151834726333618, 0.7350451946258545, 0.3408115804195404, 0.7193969488143921, 0.43860918283462524, 0.3398553133010864, 0.6160038709640503, 0.5969164967536926, 0.29822149872779846, 0.9287289381027222, 0.7408102750778198, 0.4051145315170288, 0.6256313323974609, 0.4660979211330414, 0.6747519969940186, 0.6904412508010864, 0.6648345589637756, 0.4618261754512787, 0.41863352060317993]
[2025-05-26 18:40:32,190]: Mean: 0.61838740
[2025-05-26 18:40:32,198]: Min: 0.00002434
[2025-05-26 18:40:32,214]: Max: 0.99275023
[2025-05-26 18:40:32,214]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-26 18:40:32,215]: Sample Values (25 elements): [-0.12479719519615173, -0.07169444859027863, 0.11225637793540955, 0.034997716546058655, 0.07849406450986862, 0.049742765724658966, 0.019367340952157974, 0.04546438157558441, -0.09163431078195572, -0.01345357671380043, 0.024577582255005836, 0.02970379777252674, 0.013840868137776852, -0.023260856047272682, 0.040109626948833466, -0.05006300285458565, -0.08411742001771927, 0.04062673822045326, -0.05336809158325195, 0.11479328572750092, 0.09688977152109146, 0.15430665016174316, 0.04697747901082039, 0.02443712204694748, 0.059892065823078156]
[2025-05-26 18:40:32,215]: Mean: -0.00153678
[2025-05-26 18:40:32,216]: Min: -0.43829608
[2025-05-26 18:40:32,216]: Max: 0.39578027
[2025-05-26 18:40:32,216]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-26 18:40:32,216]: Sample Values (25 elements): [0.47712817788124084, 0.47742193937301636, 0.3340543508529663, 0.7179672718048096, 0.5332371592521667, 0.6391617059707642, 0.49530866742134094, 0.22998811304569244, 0.5508807897567749, 0.5660889744758606, 0.6673650741577148, 0.5484766364097595, 0.2957254946231842, 0.20995022356510162, 0.4704273045063019, 0.6366546154022217, 0.4722452461719513, 0.5439478754997253, 0.42759615182876587, 0.5134763717651367, 0.5457920432090759, 0.7814029455184937, 0.6918928623199463, 0.6728680729866028, 0.7421880960464478]
[2025-05-26 18:40:32,216]: Mean: 0.47148079
[2025-05-26 18:40:32,216]: Min: 0.00600266
[2025-05-26 18:40:32,218]: Max: 0.78282541
[2025-05-26 18:40:32,218]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 18:40:32,225]: Sample Values (25 elements): [3.680889776069394e-37, -0.0751272514462471, -0.13565003871917725, 0.058491360396146774, -2.3132567045462265e-25, 3.166757737083593e-28, 0.07723221927881241, 0.007695432752370834, 0.005000700708478689, -0.0021803989075124264, 0.022816650569438934, -0.0943073183298111, 0.0001907164987642318, -2.669401703742943e-21, 0.015824593603610992, 4.311161450178832e-22, -3.318765141784779e-21, -0.00029803343932144344, -0.002115346025675535, -0.06242549046874046, -1.1844874039934616e-11, -9.69130293126887e-33, 0.00796444434672594, 0.011740075424313545, 0.014963684603571892]
[2025-05-26 18:40:32,226]: Mean: -0.00004608
[2025-05-26 18:40:32,228]: Min: -0.29564521
[2025-05-26 18:40:32,229]: Max: 0.25050282
[2025-05-26 18:40:32,229]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-26 18:40:32,236]: Sample Values (25 elements): [0.7808207869529724, 0.6015551686286926, -1.8707408955265237e-08, 0.9344279170036316, 0.8687005043029785, 0.596226155757904, 0.6272522211074829, 0.00036365570849739015, 0.6892985701560974, 0.6168178915977478, 0.7817493081092834, 0.708783745765686, 0.9036879539489746, 0.7956631779670715, 0.7397732734680176, 0.6819066405296326, 0.6025592684745789, 0.6529404520988464, 0.7033069729804993, 1.153111219406128, 0.6208116412162781, 0.8658621907234192, 1.0310759544372559, 0.796686053276062, 0.02094132825732231]
[2025-05-26 18:40:32,244]: Mean: 0.40057808
[2025-05-26 18:40:32,256]: Min: -0.00001315
[2025-05-26 18:40:32,261]: Max: 1.18291295
[2025-05-26 18:40:32,261]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 18:40:32,263]: Sample Values (25 elements): [7.265405459534213e-19, -0.029477151110768318, -0.02331375889480114, -2.3706084370437683e-32, 2.1063688987466023e-12, 0.03579793870449066, -0.07610931992530823, -0.0787980705499649, 2.280720582904684e-18, -0.030474981293082237, 0.0013986160047352314, -0.09154864400625229, 0.0432610884308815, 0.012677722610533237, -0.04152187332510948, -8.723744353297148e-39, 0.020778153091669083, 1.477613750466844e-05, -4.391422407934442e-05, -0.01173806469887495, -2.627036687008954e-15, -5.768472586533888e-14, 5.153837340686576e-20, -0.0005549247143790126, -1.0862810030963033e-15]
[2025-05-26 18:40:32,265]: Mean: 0.00010731
[2025-05-26 18:40:32,271]: Min: -0.26221123
[2025-05-26 18:40:32,272]: Max: 0.23583552
[2025-05-26 18:40:32,273]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-26 18:40:32,275]: Sample Values (25 elements): [0.6479041576385498, 0.761871874332428, 0.5644283890724182, 0.4080999493598938, 0.7665439248085022, 0.42021647095680237, 0.3672936260700226, 0.7424198389053345, 0.6535537242889404, 0.4739481508731842, 0.7946134805679321, 0.22213797271251678, 0.09423863887786865, 0.4125884771347046, 0.5856773853302002, 0.8725820779800415, 0.6104941368103027, 0.5647820830345154, 0.6990258693695068, 0.562686562538147, 0.3920272886753082, 0.8191205263137817, 0.2631184756755829, 0.6485220789909363, 0.7066184878349304]
[2025-05-26 18:40:32,277]: Mean: 0.55013949
[2025-05-26 18:40:32,282]: Min: -0.00031137
[2025-05-26 18:40:32,290]: Max: 0.90389919
[2025-05-26 18:40:32,290]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-26 18:40:32,307]: Sample Values (25 elements): [-4.0843240248222834e-12, -1.0875257744932197e-20, -1.363450774808858e-13, 0.05597490072250366, 1.2472123657065673e-21, -1.554960080991649e-22, -1.4756491140133221e-08, 0.0002050927869277075, 0.003329237923026085, 2.2247576721000604e-20, -8.591858204454184e-06, -1.5479146895813756e-05, 1.796262404828302e-18, -0.0016986140981316566, 0.04536427929997444, 5.169546017506841e-20, 0.06587465107440948, -2.050448422163953e-10, 1.3403833169156273e-18, -3.569990026577689e-24, -1.4615358348481905e-13, -2.5368441952688427e-15, 3.309167142509419e-16, 1.1912598940182093e-19, 7.567583429491419e-18]
[2025-05-26 18:40:32,308]: Mean: -0.00009713
[2025-05-26 18:40:32,308]: Min: -0.31023049
[2025-05-26 18:40:32,308]: Max: 0.30434388
[2025-05-26 18:40:32,308]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-26 18:40:32,315]: Sample Values (25 elements): [-0.02046799287199974, 8.461354809696786e-06, -7.623516466992442e-07, 1.1580804084587726e-06, -4.260257355781505e-06, 3.144939455523854e-07, -1.7477930214226944e-07, 8.613767022325192e-06, -2.1496171029866673e-06, 0.8423187136650085, 0.2733684480190277, -1.5938834394546575e-06, 2.2190114634668134e-07, -4.996129518985981e-07, -4.211213308735751e-06, -3.5656157706398517e-06, -3.3224350204363873e-07, 1.2576591871038545e-05, 1.1365697272225361e-09, 1.500367052820195e-10, 9.399038845003815e-07, -1.9546624230315501e-07, 0.7018256187438965, -1.0492783530935412e-06, 0.7258846759796143]
[2025-05-26 18:40:32,317]: Mean: 0.16579585
[2025-05-26 18:40:32,319]: Min: -0.03348336
[2025-05-26 18:40:32,320]: Max: 1.07092845
[2025-05-26 18:40:32,320]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 18:40:32,344]: Sample Values (25 elements): [-1.2634325172185834e-20, 8.172391940598474e-20, -4.3507542013543343e-10, -0.0016683775465935469, 2.8133417470654898e-11, -0.020791081711649895, 1.1439436203852793e-11, -2.2998908333349277e-10, 0.0004476029716897756, -8.136745464517554e-21, -0.01227506808936596, -3.7152864670691296e-22, -0.0009883982129395008, 0.07682029902935028, 4.2821850232410696e-11, -2.9307681590898253e-21, -4.535731165655754e-12, 9.85032150591012e-17, 8.623579281219484e-13, 1.2819377559480927e-07, 1.231796526171224e-19, 3.8108471933639976e-09, -4.496344995201203e-21, 0.000866241694893688, -1.848645383976011e-15]
[2025-05-26 18:40:32,353]: Mean: 0.00008071
[2025-05-26 18:40:32,353]: Min: -0.27117524
[2025-05-26 18:40:32,354]: Max: 0.29356852
[2025-05-26 18:40:32,354]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-26 18:40:32,354]: Sample Values (25 elements): [0.5562607049942017, 0.6542362570762634, 0.08617281913757324, 0.534457266330719, 0.5264449119567871, 0.5044412016868591, -3.164687996104476e-06, 0.6360830068588257, 0.1852039396762848, 0.35163769125938416, 0.4142875373363495, 0.4996086061000824, 0.5140303373336792, 0.5817722082138062, 0.39675331115722656, 0.45752429962158203, 0.5489242672920227, 0.3445991277694702, 0.4492417275905609, 0.5049329996109009, 0.49027910828590393, 0.32972946763038635, 0.3117177188396454, 0.4379543960094452, 0.8249363899230957]
[2025-05-26 18:40:32,354]: Mean: 0.41441143
[2025-05-26 18:40:32,354]: Min: -0.01105363
[2025-05-26 18:40:32,354]: Max: 0.90154690
[2025-05-26 18:40:32,354]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-26 18:40:32,361]: Sample Values (25 elements): [-0.09819677472114563, 0.0605718269944191, 0.08049050718545914, 0.04341793432831764, -0.06794502586126328, -0.0296489205211401, -0.03304330259561539, -0.08059201389551163, 0.16799531877040863, 0.07342340797185898, 0.03835124149918556, 0.027532478794455528, 0.00048029067693278193, -0.05002889409661293, 0.07802075892686844, -0.059989284723997116, 0.0003837765543721616, 0.05493277311325073, -0.003744530025869608, -0.04890149459242821, 0.015499879606068134, 0.11216820031404495, -0.09418565779924393, -0.06515511870384216, 0.016394317150115967]
[2025-05-26 18:40:32,363]: Mean: -0.00017059
[2025-05-26 18:40:32,365]: Min: -0.37375593
[2025-05-26 18:40:32,366]: Max: 0.32935059
[2025-05-26 18:40:32,366]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-26 18:40:32,369]: Sample Values (25 elements): [0.6087071895599365, 0.49060168862342834, 0.36433130502700806, 0.3054773807525635, 0.6468968391418457, 0.34829258918762207, 1.5863433873164468e-05, 0.25011515617370605, 0.296057790517807, 0.7312095761299133, 0.5366881489753723, 0.14022336900234222, 0.42645445466041565, 0.4329693019390106, 0.2780941128730774, 0.37550845742225647, 0.4903121590614319, 0.25948405265808105, 0.3446683883666992, -5.183412667975063e-07, 0.6303819417953491, 0.46013519167900085, 0.31312206387519836, 0.7458508610725403, 0.2660561501979828]
[2025-05-26 18:40:32,374]: Mean: 0.36659089
[2025-05-26 18:40:32,382]: Min: -0.01865628
[2025-05-26 18:40:32,397]: Max: 0.74585086
[2025-05-26 18:40:32,397]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 18:40:32,408]: Sample Values (25 elements): [-3.732994553328828e-18, 8.811134932784626e-08, 6.02544652627407e-13, -1.233024061373072e-15, 7.321723534552003e-19, -0.03764110058546066, -4.4500582147173617e-13, -2.0075967277488616e-10, -7.359681440666233e-12, 3.8938456950986244e-13, 0.03971584886312485, -2.087441208927475e-14, 0.0030819284729659557, -1.308687358505032e-19, -0.03480379283428192, 0.08850981295108795, 2.882472351455626e-18, -0.008950535207986832, -1.5924700447200735e-10, -3.692744027228301e-12, 2.4208742649411974e-12, 1.7421493865284253e-19, -1.9484718834947906e-19, 8.706928927848523e-11, -1.1960643108811079e-20]
[2025-05-26 18:40:32,410]: Mean: 0.00001494
[2025-05-26 18:40:32,411]: Min: -0.27282965
[2025-05-26 18:40:32,412]: Max: 0.30176902
[2025-05-26 18:40:32,413]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-26 18:40:32,418]: Sample Values (25 elements): [-1.237808788800976e-07, -1.7636445193147665e-08, 2.9321203243171112e-08, 5.0543986773732286e-09, 9.775365583664097e-08, 3.381012803060912e-08, -1.0387294357983023e-09, 2.675681969321886e-07, -1.2466921361919958e-07, 0.8591556549072266, -1.5750430293337558e-06, -1.6945924130595813e-07, 2.2376814001745515e-07, -1.268340525939493e-07, -1.357054690842574e-09, -1.238842006756613e-08, -1.6511419786979786e-08, 1.2584283126670925e-07, 0.8475847244262695, 5.205530442253803e-07, 0.6997649073600769, 7.138532396311348e-07, 0.01274350006133318, 2.5642711420914566e-07, 0.5516757965087891]
[2025-05-26 18:40:32,422]: Mean: 0.12154363
[2025-05-26 18:40:32,435]: Min: -0.00003996
[2025-05-26 18:40:32,445]: Max: 1.08025503
[2025-05-26 18:40:32,445]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 18:40:32,456]: Sample Values (25 elements): [1.9341776444359304e-11, 2.9733270068277307e-08, -0.09515894949436188, -1.9747787849411225e-09, 8.706992815632475e-08, 3.3641562567473724e-16, -8.143677732164178e-09, 0.06339526176452637, -2.1167585045889226e-11, -8.361213841244757e-15, 4.1500413132042335e-13, -1.6988215304977738e-12, 0.014773114584386349, -2.9510371456349932e-18, 1.3069520170424197e-14, 4.3622373074869714e-11, -2.1294058183407572e-10, -0.027137640863656998, 4.26197260594563e-07, -2.0717098989281233e-26, -7.719375694215103e-10, -0.03899100795388222, 0.07189731299877167, 2.7246288585762793e-14, -3.4759890278623917e-13]
[2025-05-26 18:40:32,457]: Mean: 0.00002916
[2025-05-26 18:40:32,458]: Min: -0.29918453
[2025-05-26 18:40:32,460]: Max: 0.32840532
[2025-05-26 18:40:32,460]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-26 18:40:32,468]: Sample Values (25 elements): [0.34996452927589417, 0.4317888617515564, 0.6275193095207214, 0.8805549740791321, 0.37055355310440063, 0.541621744632721, 0.31980249285697937, 0.3811255693435669, 0.48384568095207214, -0.0001873804139904678, 0.4262039363384247, 0.7204241752624512, 0.33142808079719543, 0.2749680280685425, 0.528582751750946, 0.9210579991340637, 0.7090230584144592, 0.48213642835617065, 0.34214848279953003, 0.605297327041626, 0.005934420507401228, 0.9306917786598206, -0.00011724073556251824, 0.7016308307647705, 1.1757151696656365e-06]
[2025-05-26 18:40:32,481]: Mean: 0.48672470
[2025-05-26 18:40:32,490]: Min: -0.00261518
[2025-05-26 18:40:32,491]: Max: 1.06434107
[2025-05-26 18:40:32,491]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-26 18:40:32,513]: Sample Values (25 elements): [7.522608230781103e-11, 5.19802633789368e-05, -3.8087178674600305e-14, -4.699023026477126e-22, -1.0887683387137679e-19, 2.0465612905584505e-31, 4.298990444700491e-20, 4.138549523737425e-19, -8.652742386604944e-22, -2.3863504097543634e-19, -4.1901316042370137e-32, -4.846881142966941e-12, 0.04318734258413315, -9.491308518934798e-30, 4.1198719147529486e-20, -5.408039187136511e-33, -2.3625110889868277e-23, 1.1839236188637692e-11, 8.914290879799768e-33, -9.065385554924214e-22, -2.9446868800724236e-18, 9.723933271395693e-18, -3.4187654929240807e-09, -1.2000320182004032e-10, 1.5116332932887823e-26]
[2025-05-26 18:40:32,524]: Mean: 0.00000443
[2025-05-26 18:40:32,536]: Min: -0.30613258
[2025-05-26 18:40:32,536]: Max: 0.34249261
[2025-05-26 18:40:32,536]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-26 18:40:32,537]: Sample Values (25 elements): [0.027242664247751236, 8.410242120782119e-12, 5.988721909488959e-07, -5.248131742519035e-07, -2.941273578471737e-06, 0.5981804132461548, -1.0280169959742125e-07, -2.485122195139411e-06, 2.8005905150507715e-08, 0.00024767473223619163, -6.530432932697755e-14, 9.635081177350457e-09, 0.1338881254196167, 1.3133451659541606e-08, 1.4106667549640406e-06, 1.5758319094061335e-08, -5.0143427188231726e-08, -5.439627415171344e-09, 0.0007229867042042315, 2.6777122741350468e-09, 0.00020037632202729583, -1.1742221772692574e-07, -3.8424764170486014e-07, 0.9613821506500244, -1.0014327145313473e-08]
[2025-05-26 18:40:32,538]: Mean: 0.05535659
[2025-05-26 18:40:32,538]: Min: -0.02481945
[2025-05-26 18:40:32,538]: Max: 0.96138215
[2025-05-26 18:40:32,538]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 18:40:32,582]: Sample Values (25 elements): [9.68216116483487e-28, 2.399904962967829e-27, -4.948965786455956e-41, 1.506626929262798e-20, 8.967085408198006e-23, 7.849644735212144e-14, -0.00010841486800927669, 0.0013655191287398338, 1.5528616058497514e-14, -1.758579358380488e-20, -1.1291514581392116e-15, 3.424443648820457e-27, -4.353740223450586e-05, 0.00014402395754586905, 5.035884394209779e-25, 1.6282313517663296e-37, -3.974203490372639e-22, -3.176932593948814e-15, 1.5354827382907388e-06, 5.0927530251298615e-19, -3.6192503535037026e-17, -7.662810273464937e-13, 0.07137888669967651, 0.0003313160268589854, 2.631163340538478e-07]
[2025-05-26 18:40:32,582]: Mean: -0.00001027
[2025-05-26 18:40:32,582]: Min: -0.20115378
[2025-05-26 18:40:32,583]: Max: 0.25376654
[2025-05-26 18:40:32,583]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-26 18:40:32,583]: Sample Values (25 elements): [0.6299381256103516, -2.6851820577666137e-16, 0.5144379734992981, 0.4904193878173828, 0.22749537229537964, 0.7132615447044373, -8.200998991014785e-09, 0.02112501673400402, 0.48485225439071655, 0.43548867106437683, 0.019752807915210724, 0.1640089899301529, 0.43698641657829285, 0.09796850383281708, -3.0551573776274946e-14, 1.4938010550746839e-10, -5.206572382121522e-07, 4.756061287025659e-07, 0.6527153849601746, 2.16111709056957e-18, 0.06305036693811417, 0.45780664682388306, 0.1453769952058792, -3.3745766090670587e-21, 0.03302242234349251]
[2025-05-26 18:40:32,583]: Mean: 0.27709204
[2025-05-26 18:40:32,583]: Min: -0.01869849
[2025-05-26 18:40:32,585]: Max: 0.87002254
[2025-05-26 18:40:32,585]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-26 18:40:32,592]: Sample Values (25 elements): [0.009275108575820923, 4.302932939026505e-05, -2.8043286874890327e-05, 0.031248975545167923, -0.0558398999273777, 4.373658011279369e-13, -0.0003315176290925592, -3.0736077826007486e-34, 1.4397527224249695e-14, -0.0010544786928221583, -5.845571558272047e-14, -0.002811941783875227, 0.016358470544219017, 7.860684547722485e-09, -0.04374680295586586, 0.00022729241754859686, -0.005742650479078293, 0.001097909756936133, 0.0017884504050016403, 0.0008886204450391233, 1.1185681354965713e-17, 0.05957838147878647, 0.0030348566360771656, 2.907461658706612e-20, 0.05375487357378006]
[2025-05-26 18:40:32,593]: Mean: -0.00004411
[2025-05-26 18:40:32,602]: Min: -0.19934332
[2025-05-26 18:40:32,610]: Max: 0.21171546
[2025-05-26 18:40:32,610]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-26 18:40:32,627]: Sample Values (25 elements): [0.3483664095401764, 0.41085782647132874, 0.47456094622612, 1.2765033001709957e-14, 1.206134675157955e-05, -1.6817687760594812e-15, -3.75103617312561e-16, 0.5001317858695984, 0.3755328357219696, 0.0798301100730896, 1.3058870536042377e-05, 0.4018378257751465, 0.3162819445133209, 0.28061145544052124, 0.3332381248474121, 0.03901244327425957, 0.05718540400266647, 0.35805830359458923, 2.211697215344713e-14, 0.5435851216316223, 0.36945968866348267, 0.3948357403278351, 0.07634461671113968, 0.3854089081287384, 6.637392652919516e-05]
[2025-05-26 18:40:32,628]: Mean: 0.19983560
[2025-05-26 18:40:32,628]: Min: -0.00290157
[2025-05-26 18:40:32,628]: Max: 0.57647210
[2025-05-26 18:40:32,628]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 18:40:32,680]: Sample Values (25 elements): [-9.59888494022663e-20, -1.7743925007442016e-31, 1.4863743800895068e-34, 1.8957480848122317e-37, 2.0842097081201414e-25, -0.012011466547846794, 4.532611236827194e-31, -1.1761096703931915e-29, 0.0016314198728650808, 4.905245274369022e-41, -6.476376908674614e-39, -9.756038452372748e-23, -4.053625030533791e-31, -6.03558620521972e-17, -1.0478690006080924e-33, 9.912121121161666e-39, -4.822194328926982e-32, 1.1635275703255222e-14, -4.738899680848478e-33, 2.7000218810610575e-41, 4.573261632831886e-35, 1.6885646495114046e-42, -3.6786235687976087e-25, 5.189036056793338e-21, -1.6394170486019867e-38]
[2025-05-26 18:40:32,682]: Mean: -0.00000064
[2025-05-26 18:40:32,684]: Min: -0.12050257
[2025-05-26 18:40:32,685]: Max: 0.12439884
[2025-05-26 18:40:32,685]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-26 18:40:32,688]: Sample Values (25 elements): [-2.298846908388441e-07, 6.243525945137662e-07, -5.082902134745382e-07, 2.21669349542708e-09, 3.1825027235754533e-07, 3.2143263606343453e-09, -1.0640870868883212e-06, -7.004282451816835e-07, 7.267832558000009e-08, 2.851932556779957e-08, 2.9292441467987373e-06, -2.956101923246024e-07, 2.8184082623283757e-08, 1.4636128753409139e-06, -1.7048969880306686e-07, 3.832745232301704e-10, -1.9627459365177468e-16, -4.2270960420864867e-07, -1.7956027775767325e-08, 5.121526669427112e-07, -9.805628451431403e-07, 1.5486533300190786e-07, -8.354795255627323e-08, -3.471616327743732e-10, -8.218302127716015e-08]
[2025-05-26 18:40:32,693]: Mean: 0.01727849
[2025-05-26 18:40:32,701]: Min: -0.03914265
[2025-05-26 18:40:32,715]: Max: 0.89090860
[2025-05-26 18:40:32,715]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 18:40:32,763]: Sample Values (25 elements): [-4.561560258456307e-28, -1.2955986523463675e-29, -4.991285000078566e-41, -1.0785520697040307e-12, -7.2688404702870945e-31, 2.6876636948592902e-21, 1.2660750490785726e-29, -9.846573984194408e-39, 1.0846020052098389e-34, 1.723597111119525e-42, -1.4786603081815496e-20, 0.0006527943769469857, -1.0664966816665366e-18, -4.00466899225238e-12, 5.831431812806353e-38, 9.12257269725103e-10, 1.0936606658977028e-31, -0.012312667444348335, 9.585671145728725e-10, -3.01664909128865e-26, 8.554073820334917e-29, -5.971229826381496e-34, 1.6182617113250016e-26, 1.0712389047743217e-26, 9.72126379394922e-10]
[2025-05-26 18:40:32,764]: Mean: -0.00000039
[2025-05-26 18:40:32,764]: Min: -0.11675987
[2025-05-26 18:40:32,764]: Max: 0.12844880
[2025-05-26 18:40:32,764]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-26 18:40:32,772]: Sample Values (25 elements): [0.07873398810625076, 0.04512479528784752, 0.30836987495422363, 0.0898154005408287, 0.1393849402666092, 0.18573883175849915, 1.2938446971796913e-12, -3.1442073944454307e-10, 2.504481776859687e-14, 0.011003649793565273, 0.35700181126594543, 0.22218400239944458, 0.24077248573303223, 0.11756674200296402, 0.3063531517982483, 0.1760910004377365, -0.017205683514475822, 0.06486969441175461, 0.202941432595253, 0.33635377883911133, 0.11548181623220444, 0.006099773570895195, 0.24178360402584076, -1.9264585315436483e-13, 1.0274486150763096e-08]
[2025-05-26 18:40:32,774]: Mean: 0.10049023
[2025-05-26 18:40:32,776]: Min: -0.02497864
[2025-05-26 18:40:32,777]: Max: 0.47699705
[2025-05-26 18:40:32,777]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-26 18:40:32,782]: Sample Values (25 elements): [0.017927227541804314, -0.0008318267646245658, -0.20037789642810822, 0.0009129292448051274, 0.021980045363307, 0.07182154804468155, -0.034435372799634933, 0.024073168635368347, 0.09644406288862228, -0.00024915917310863733, -0.26413393020629883, -0.00012416379468049854, -0.07114514708518982, -0.038498297333717346, 0.001523130340501666, -0.0003623889933805913, 0.14599336683750153, -3.694166662171483e-05, -0.08156394958496094, 0.09400773793458939, 0.09255508333444595, -0.0052400436252355576, 0.00021968220244161785, 0.08866699784994125, -0.04631316289305687]
[2025-05-26 18:40:32,786]: Mean: 0.00021368
[2025-05-26 18:40:32,800]: Min: -0.48745778
[2025-05-26 18:40:32,809]: Max: 0.47411236
[2025-05-26 18:40:32,809]: Checkpoint of model at path [checkpoint/ResNet18_hardtanh.ckpt] will be used for QAT
[2025-05-28 04:15:24,664]: Checkpoint of model at path [checkpoint/ResNet18_hardtanh.ckpt] will be used for QAT
[2025-05-28 04:15:24,665]: 


QAT of ResNet18 with hardtanh down to 4 bits...
[2025-05-28 04:15:25,008]: [ResNet18_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-28 04:15:25,196]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-28 04:17:17,465]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.3250 Train Acc: 0.8866 Eval Loss: 0.6332 Eval Acc: 0.8110 (LR: 0.00100000)
[2025-05-28 04:19:22,553]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.3281 Train Acc: 0.8866 Eval Loss: 0.4473 Eval Acc: 0.8572 (LR: 0.00100000)
[2025-05-28 04:21:05,690]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.3304 Train Acc: 0.8836 Eval Loss: 0.5333 Eval Acc: 0.8300 (LR: 0.00100000)
[2025-05-28 04:22:50,315]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.3322 Train Acc: 0.8859 Eval Loss: 0.5636 Eval Acc: 0.8206 (LR: 0.00100000)
[2025-05-28 04:24:36,475]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.3301 Train Acc: 0.8841 Eval Loss: 0.7696 Eval Acc: 0.7806 (LR: 0.00100000)
[2025-05-28 04:26:19,662]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.3273 Train Acc: 0.8861 Eval Loss: 0.5783 Eval Acc: 0.8148 (LR: 0.00100000)
[2025-05-28 04:28:04,184]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.3239 Train Acc: 0.8872 Eval Loss: 0.5485 Eval Acc: 0.8280 (LR: 0.00100000)
[2025-05-28 04:29:50,095]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.3224 Train Acc: 0.8873 Eval Loss: 0.6939 Eval Acc: 0.7995 (LR: 0.00010000)
[2025-05-28 04:31:36,203]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.2178 Train Acc: 0.9248 Eval Loss: 0.3195 Eval Acc: 0.8956 (LR: 0.00010000)
[2025-05-28 04:33:19,747]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.1921 Train Acc: 0.9335 Eval Loss: 0.3214 Eval Acc: 0.9003 (LR: 0.00010000)
[2025-05-28 04:35:02,252]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.1835 Train Acc: 0.9363 Eval Loss: 0.3148 Eval Acc: 0.9010 (LR: 0.00010000)
[2025-05-28 04:36:44,151]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.1746 Train Acc: 0.9396 Eval Loss: 0.3074 Eval Acc: 0.9040 (LR: 0.00010000)
[2025-05-28 04:38:28,553]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.1701 Train Acc: 0.9410 Eval Loss: 0.3025 Eval Acc: 0.9050 (LR: 0.00010000)
[2025-05-28 04:40:12,741]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.1678 Train Acc: 0.9423 Eval Loss: 0.3160 Eval Acc: 0.8990 (LR: 0.00010000)
[2025-05-28 04:41:59,258]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.1619 Train Acc: 0.9436 Eval Loss: 0.3176 Eval Acc: 0.9041 (LR: 0.00010000)
[2025-05-28 04:43:43,074]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.1579 Train Acc: 0.9445 Eval Loss: 0.3086 Eval Acc: 0.9065 (LR: 0.00010000)
[2025-05-28 04:45:29,796]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.1566 Train Acc: 0.9458 Eval Loss: 0.3201 Eval Acc: 0.9027 (LR: 0.00010000)
[2025-05-28 04:47:15,283]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.1493 Train Acc: 0.9475 Eval Loss: 0.3260 Eval Acc: 0.9022 (LR: 0.00010000)
[2025-05-28 04:48:57,969]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.1507 Train Acc: 0.9488 Eval Loss: 0.3025 Eval Acc: 0.9069 (LR: 0.00001000)
[2025-05-28 04:50:41,917]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.1379 Train Acc: 0.9524 Eval Loss: 0.2961 Eval Acc: 0.9100 (LR: 0.00001000)
[2025-05-28 04:52:24,834]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.1328 Train Acc: 0.9538 Eval Loss: 0.3001 Eval Acc: 0.9076 (LR: 0.00001000)
[2025-05-28 04:54:07,333]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.1304 Train Acc: 0.9549 Eval Loss: 0.2958 Eval Acc: 0.9093 (LR: 0.00001000)
[2025-05-28 04:55:49,830]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.1317 Train Acc: 0.9540 Eval Loss: 0.2993 Eval Acc: 0.9099 (LR: 0.00001000)
[2025-05-28 04:57:32,423]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.1294 Train Acc: 0.9550 Eval Loss: 0.3026 Eval Acc: 0.9086 (LR: 0.00001000)
[2025-05-28 04:59:20,916]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.1261 Train Acc: 0.9561 Eval Loss: 0.3012 Eval Acc: 0.9087 (LR: 0.00001000)
[2025-05-28 05:01:03,703]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.1288 Train Acc: 0.9557 Eval Loss: 0.3002 Eval Acc: 0.9083 (LR: 0.00001000)
[2025-05-28 05:02:51,265]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.1253 Train Acc: 0.9565 Eval Loss: 0.2999 Eval Acc: 0.9095 (LR: 0.00001000)
[2025-05-28 05:04:36,700]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.1264 Train Acc: 0.9559 Eval Loss: 0.3025 Eval Acc: 0.9079 (LR: 0.00000100)
[2025-05-28 05:06:20,243]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.1244 Train Acc: 0.9561 Eval Loss: 0.3024 Eval Acc: 0.9088 (LR: 0.00000100)
[2025-05-28 05:08:03,866]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.1247 Train Acc: 0.9570 Eval Loss: 0.3027 Eval Acc: 0.9069 (LR: 0.00000100)
[2025-05-28 05:09:46,267]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.1257 Train Acc: 0.9565 Eval Loss: 0.2976 Eval Acc: 0.9097 (LR: 0.00000100)
[2025-05-28 05:11:28,130]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.1229 Train Acc: 0.9579 Eval Loss: 0.3007 Eval Acc: 0.9096 (LR: 0.00000100)
[2025-05-28 05:13:13,639]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.1220 Train Acc: 0.9574 Eval Loss: 0.3001 Eval Acc: 0.9090 (LR: 0.00000100)
[2025-05-28 05:14:58,361]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.1226 Train Acc: 0.9571 Eval Loss: 0.3011 Eval Acc: 0.9092 (LR: 0.00000010)
[2025-05-28 05:16:43,310]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.1251 Train Acc: 0.9564 Eval Loss: 0.3041 Eval Acc: 0.9087 (LR: 0.00000010)
[2025-05-28 05:18:29,058]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.1236 Train Acc: 0.9575 Eval Loss: 0.2988 Eval Acc: 0.9092 (LR: 0.00000010)
[2025-05-28 05:20:16,753]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.1205 Train Acc: 0.9583 Eval Loss: 0.3009 Eval Acc: 0.9098 (LR: 0.00000010)
[2025-05-28 05:21:59,486]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.1225 Train Acc: 0.9581 Eval Loss: 0.3005 Eval Acc: 0.9102 (LR: 0.00000010)
[2025-05-28 05:23:45,101]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.1225 Train Acc: 0.9565 Eval Loss: 0.3005 Eval Acc: 0.9090 (LR: 0.00000010)
[2025-05-28 05:25:32,388]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.1219 Train Acc: 0.9574 Eval Loss: 0.3018 Eval Acc: 0.9105 (LR: 0.00000010)
[2025-05-28 05:27:16,232]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.1230 Train Acc: 0.9573 Eval Loss: 0.3032 Eval Acc: 0.9088 (LR: 0.00000010)
[2025-05-28 05:28:58,581]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.1228 Train Acc: 0.9576 Eval Loss: 0.3026 Eval Acc: 0.9100 (LR: 0.00000010)
[2025-05-28 05:30:51,542]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.1219 Train Acc: 0.9571 Eval Loss: 0.3007 Eval Acc: 0.9089 (LR: 0.00000010)
[2025-05-28 05:32:44,018]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.1245 Train Acc: 0.9565 Eval Loss: 0.3011 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-05-28 05:34:35,995]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.1246 Train Acc: 0.9562 Eval Loss: 0.3002 Eval Acc: 0.9092 (LR: 0.00000010)
[2025-05-28 05:36:24,606]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.1222 Train Acc: 0.9578 Eval Loss: 0.2981 Eval Acc: 0.9104 (LR: 0.00000010)
[2025-05-28 05:38:07,868]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.1231 Train Acc: 0.9573 Eval Loss: 0.2995 Eval Acc: 0.9091 (LR: 0.00000010)
[2025-05-28 05:39:50,208]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.1208 Train Acc: 0.9583 Eval Loss: 0.3027 Eval Acc: 0.9088 (LR: 0.00000010)
[2025-05-28 05:41:32,141]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.1252 Train Acc: 0.9562 Eval Loss: 0.3012 Eval Acc: 0.9082 (LR: 0.00000010)
[2025-05-28 05:43:17,168]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.1220 Train Acc: 0.9582 Eval Loss: 0.3022 Eval Acc: 0.9090 (LR: 0.00000010)
[2025-05-28 05:45:01,167]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.1226 Train Acc: 0.9567 Eval Loss: 0.3013 Eval Acc: 0.9112 (LR: 0.00000010)
[2025-05-28 05:46:42,502]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.1199 Train Acc: 0.9571 Eval Loss: 0.3038 Eval Acc: 0.9094 (LR: 0.00000010)
[2025-05-28 05:48:33,173]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.1216 Train Acc: 0.9583 Eval Loss: 0.3013 Eval Acc: 0.9091 (LR: 0.00000010)
[2025-05-28 05:50:16,723]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.1205 Train Acc: 0.9584 Eval Loss: 0.3013 Eval Acc: 0.9099 (LR: 0.00000010)
[2025-05-28 05:52:00,828]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.1249 Train Acc: 0.9563 Eval Loss: 0.3013 Eval Acc: 0.9101 (LR: 0.00000010)
[2025-05-28 05:53:49,074]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.1215 Train Acc: 0.9575 Eval Loss: 0.3009 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-05-28 05:55:36,483]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.1204 Train Acc: 0.9592 Eval Loss: 0.3003 Eval Acc: 0.9105 (LR: 0.00000010)
[2025-05-28 05:57:26,232]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.1205 Train Acc: 0.9582 Eval Loss: 0.3007 Eval Acc: 0.9102 (LR: 0.00000010)
[2025-05-28 05:59:13,455]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.1232 Train Acc: 0.9571 Eval Loss: 0.2996 Eval Acc: 0.9101 (LR: 0.00000010)
[2025-05-28 06:01:01,669]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.1219 Train Acc: 0.9579 Eval Loss: 0.3018 Eval Acc: 0.9090 (LR: 0.00000010)
[2025-05-28 06:01:01,669]: [ResNet18_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.9112
[2025-05-28 06:01:02,582]: 


Quantization of model down to 4 bits finished
[2025-05-28 06:01:02,582]: Model Architecture:
[2025-05-28 06:01:04,124]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0435], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3193168640136719, max_val=0.3329331874847412)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0624], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40145254135131836, max_val=0.5352388620376587)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0510], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42534148693084717, max_val=0.34008264541625977)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0603], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4077046513557434, max_val=0.4975290894508362)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0540], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42496365308761597, max_val=0.385409951210022)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0407], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3080921769142151, max_val=0.30254268646240234)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0583], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4416200518608093, max_val=0.4330728352069855)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0367], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3019666075706482, max_val=0.24923068284988403)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0388], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2683406472206116, max_val=0.31368139386177063)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36428648233413696, max_val=0.30689889192581177)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0403], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30308300256729126, max_val=0.30074578523635864)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0490], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38199949264526367, max_val=0.35315877199172974)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2879815101623535, max_val=0.30487924814224243)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0420], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27725428342819214, max_val=0.3530806303024292)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0435], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3103839159011841, max_val=0.34251266717910767)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23291878402233124, max_val=0.23858238756656647)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0281], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20090040564537048, max_val=0.22074872255325317)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0159], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1177038848400116, max_val=0.121260866522789)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0155], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11202560365200043, max_val=0.1199989914894104)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-28 06:01:04,125]: 
Model Weights:
[2025-05-28 06:01:04,125]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-28 06:01:04,253]: Sample Values (25 elements): [-0.11360298842191696, -0.22123651206493378, -0.01897837407886982, 0.08958249539136887, -0.15218636393547058, -0.17823848128318787, 0.27738410234451294, 0.0940067395567894, -0.024116449058055878, 0.18586386740207672, 0.013464892283082008, 0.040512096136808395, -0.11790932714939117, 0.15292516350746155, 0.2227291613817215, 0.34155112504959106, 0.05669747292995453, 0.08247839659452438, -0.14633719623088837, 0.22064778208732605, -0.2359391748905182, 0.011176427826285362, 0.2120201736688614, -0.2107783704996109, -0.062461983412504196]
[2025-05-28 06:01:04,652]: Mean: 0.00016158
[2025-05-28 06:01:04,695]: Min: -0.40130302
[2025-05-28 06:01:04,710]: Max: 0.39871827
[2025-05-28 06:01:04,710]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-28 06:01:04,710]: Sample Values (25 elements): [0.5333779454231262, 0.9554306268692017, 0.4795011579990387, 0.5874430537223816, 0.1422833800315857, 0.977977454662323, 0.9307688474655151, 0.8963167071342468, 0.38231346011161804, 0.6254401206970215, 0.7254646420478821, 0.46185770630836487, 0.6955063343048096, 0.6632235050201416, 0.7770550847053528, 0.6245912909507751, 0.7530983090400696, 0.6156014800071716, 0.5882807374000549, 0.7452345490455627, 0.06088153272867203, 0.7879883646965027, 0.8664770722389221, 0.5319924354553223, 0.6764979362487793]
[2025-05-28 06:01:04,711]: Mean: 0.66506410
[2025-05-28 06:01:04,711]: Min: 0.06088153
[2025-05-28 06:01:04,711]: Max: 1.15568900
[2025-05-28 06:01:04,712]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 06:01:04,713]: Sample Values (25 elements): [0.04348333925008774, -0.04348333925008774, 0.04348333925008774, -0.04348333925008774, 0.0, 0.0, 0.08696667850017548, 0.0, 0.0, 0.0, -0.04348333925008774, -0.13045001029968262, 0.04348333925008774, 0.13045001029968262, -0.04348333925008774, 0.0, 0.0, 0.13045001029968262, 0.0, -0.04348333925008774, 0.0, 0.0, 0.04348333925008774, 0.08696667850017548, 0.0]
[2025-05-28 06:01:04,713]: Mean: 0.00017457
[2025-05-28 06:01:04,713]: Min: -0.30438337
[2025-05-28 06:01:04,714]: Max: 0.34786671
[2025-05-28 06:01:04,714]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-28 06:01:04,714]: Sample Values (25 elements): [0.746319055557251, 1.1787712574005127, -6.038335212622069e-41, 0.6860098242759705, 1.1148693561553955, 0.3025232255458832, 0.8847107291221619, 1.0145214796066284, 0.389119029045105, 0.6999797224998474, 0.8913193941116333, 0.6233378052711487, 0.029667388647794724, 0.5560519695281982, 0.6148762702941895, 1.1073319911956787, 0.4146445393562317, 1.3405139446258545, 0.20873378217220306, 0.15939028561115265, 1.3323307037353516, 0.3535582423210144, 0.3093624711036682, 1.0651129484176636, 0.13081508874893188]
[2025-05-28 06:01:04,714]: Mean: 0.72175139
[2025-05-28 06:01:04,715]: Min: -0.00328125
[2025-05-28 06:01:04,715]: Max: 1.34051394
[2025-05-28 06:01:04,717]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 06:01:04,717]: Sample Values (25 elements): [-0.062446095049381256, -0.12489219009876251, 0.062446095049381256, 0.062446095049381256, -0.12489219009876251, -0.12489219009876251, -0.062446095049381256, 0.0, 0.0, -0.12489219009876251, 0.0, -0.062446095049381256, 0.0, 0.0, 0.062446095049381256, -0.12489219009876251, -0.062446095049381256, 0.062446095049381256, -0.062446095049381256, 0.0, 0.0, -0.062446095049381256, 0.062446095049381256, 0.062446095049381256, 0.0]
[2025-05-28 06:01:04,718]: Mean: 0.00014399
[2025-05-28 06:01:04,718]: Min: -0.37467659
[2025-05-28 06:01:04,718]: Max: 0.56201488
[2025-05-28 06:01:04,718]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-28 06:01:04,719]: Sample Values (25 elements): [0.5894607305526733, 0.891975998878479, 0.7186546921730042, 0.7446969151496887, 0.8662378787994385, 0.657559335231781, 1.0165777206420898, 0.8601712584495544, 0.7483921647071838, 0.9172747135162354, 0.6912571787834167, 0.7774277925491333, 0.7730426788330078, 0.6858436465263367, 0.8331055641174316, 0.6718250513076782, 0.6043271422386169, 0.9647961258888245, 0.9487749934196472, 0.663985550403595, 0.8113324642181396, 1.1909000873565674, 0.8007485270500183, 0.985156238079071, 0.7574809193611145]
[2025-05-28 06:01:04,719]: Mean: 0.81957424
[2025-05-28 06:01:04,719]: Min: 0.58946073
[2025-05-28 06:01:04,719]: Max: 1.39272308
[2025-05-28 06:01:04,721]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 06:01:04,722]: Sample Values (25 elements): [0.0, 0.0, 0.05102827772498131, -0.05102827772498131, 0.05102827772498131, 0.0, -0.05102827772498131, 0.0, 0.05102827772498131, 0.05102827772498131, -0.05102827772498131, -0.05102827772498131, -0.05102827772498131, 0.0, -0.05102827772498131, 0.05102827772498131, -0.05102827772498131, 0.0, 0.10205655544996262, 0.0, 0.0, 0.0, -0.05102827772498131, 0.0, -0.10205655544996262]
[2025-05-28 06:01:04,722]: Mean: -0.00037513
[2025-05-28 06:01:04,722]: Min: -0.40822622
[2025-05-28 06:01:04,723]: Max: 0.35719794
[2025-05-28 06:01:04,723]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-28 06:01:04,723]: Sample Values (25 elements): [1.1391950845718384, 1.2465763092041016, 0.018872128799557686, 0.929749071598053, 1.0221940279006958, 1.0046626329421997, 0.9979493618011475, 0.7330544590950012, 0.8183729648590088, 1.2750968933105469, 0.9018235802650452, 0.8939094543457031, 0.9160423874855042, -5.307558063476677e-41, 1.0832873582839966, 0.4848410189151764, 0.3215903341770172, 0.670651376247406, 0.9992750287055969, 0.13425996899604797, 1.110251784324646, 0.12303169816732407, 0.6356086730957031, 1.1518827676773071, 0.8378799557685852]
[2025-05-28 06:01:04,723]: Mean: 0.83063591
[2025-05-28 06:01:04,724]: Min: -0.00000000
[2025-05-28 06:01:04,724]: Max: 1.46967435
[2025-05-28 06:01:04,725]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 06:01:04,726]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.12069784104824066, 0.24139568209648132, 0.06034892052412033, 0.06034892052412033, -0.06034892052412033, 0.06034892052412033, 0.0, 0.06034892052412033, 0.0, 0.0, 0.06034892052412033, 0.06034892052412033, 0.0, 0.0, 0.06034892052412033, 0.0, 0.12069784104824066, 0.0, -0.06034892052412033, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,726]: Mean: 0.00005402
[2025-05-28 06:01:04,726]: Min: -0.42244244
[2025-05-28 06:01:04,727]: Max: 0.48279136
[2025-05-28 06:01:04,727]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-28 06:01:04,727]: Sample Values (25 elements): [0.6884169578552246, 0.5847735404968262, 1.0801119804382324, 1.2552767992019653, 0.6661437749862671, 0.8661004900932312, 0.970655083656311, 0.7376490235328674, 0.44198593497276306, 0.7524603009223938, 1.004936933517456, 0.5934494733810425, 0.6727085113525391, 0.8164229989051819, 1.2515720129013062, 0.7911002039909363, 0.6604712009429932, 0.5840532779693604, 0.683678925037384, 1.064316987991333, 0.7026392221450806, 0.6158813834190369, 0.8402794599533081, 0.779661238193512, 0.7844383716583252]
[2025-05-28 06:01:04,727]: Mean: 0.81624049
[2025-05-28 06:01:04,727]: Min: 0.44198593
[2025-05-28 06:01:04,727]: Max: 1.33466840
[2025-05-28 06:01:04,729]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-28 06:01:04,730]: Sample Values (25 elements): [0.21609963476657867, -0.05402490869164467, 0.0, -0.10804981738328934, 0.0, 0.0, -0.05402490869164467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05402490869164467, 0.0, 0.0, 0.0, 0.05402490869164467, -0.10804981738328934, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,730]: Mean: 0.00112112
[2025-05-28 06:01:04,730]: Min: -0.43219927
[2025-05-28 06:01:04,730]: Max: 0.37817436
[2025-05-28 06:01:04,730]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-28 06:01:04,731]: Sample Values (25 elements): [0.42457306385040283, 0.9192491173744202, 0.6891462206840515, 0.7135441303253174, 0.8086935877799988, 0.309196799993515, 0.7570192217826843, 5.995035090074432e-41, 0.5447691679000854, 6.052067937572452e-41, 0.9735389351844788, 0.4167878031730652, 0.6665889024734497, 5.818611613415938e-41, 0.8301411271095276, 0.8880507946014404, 0.8320254683494568, 0.4500519335269928, 0.8438218235969543, 0.17868463695049286, -5.638264501057334e-41, 4.884434019913897e-05, 0.8964701890945435, 0.3270038962364197, 6.138528052821294e-41]
[2025-05-28 06:01:04,731]: Mean: 0.42136580
[2025-05-28 06:01:04,731]: Min: -0.00186212
[2025-05-28 06:01:04,731]: Max: 1.11909258
[2025-05-28 06:01:04,732]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 06:01:04,734]: Sample Values (25 elements): [-0.1628359705209732, -0.0814179852604866, 0.0, 0.0407089926302433, 0.0, 0.0, 0.0814179852604866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0814179852604866, 0.0, 0.0, 0.0, 0.0814179852604866, 0.0, 0.0, 0.0, 0.0407089926302433, 0.0407089926302433, -0.0814179852604866, 0.0]
[2025-05-28 06:01:04,734]: Mean: 0.00019298
[2025-05-28 06:01:04,734]: Min: -0.32567194
[2025-05-28 06:01:04,735]: Max: 0.28496295
[2025-05-28 06:01:04,735]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-28 06:01:04,735]: Sample Values (25 elements): [0.7383211851119995, 0.8449389934539795, 0.5694233775138855, 0.7220644950866699, 0.701085090637207, -5.820293171573128e-41, 0.4058094918727875, 0.5236083269119263, 0.7133069634437561, 0.7008362412452698, 0.5720161199569702, 0.5964669585227966, 0.34473422169685364, 0.8657450675964355, 0.6919770240783691, 0.7675673365592957, 0.6830138564109802, 0.6945655941963196, 0.4087360203266144, 0.5706648230552673, 0.7906365990638733, 0.7242327928543091, 0.467756450176239, 0.7742618322372437, 0.5673940181732178]
[2025-05-28 06:01:04,735]: Mean: 0.59960186
[2025-05-28 06:01:04,736]: Min: -0.00000000
[2025-05-28 06:01:04,736]: Max: 0.93071002
[2025-05-28 06:01:04,737]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-28 06:01:04,738]: Sample Values (25 elements): [-0.05831286311149597, 0.05831286311149597, 0.05831286311149597, -0.11662572622299194, 0.11662572622299194, 0.0, 0.0, -0.05831286311149597, -0.2332514524459839, 0.0, 0.17493858933448792, 0.0, 0.17493858933448792, 0.0, -0.11662572622299194, 0.11662572622299194, -0.05831286311149597, -0.05831286311149597, 0.11662572622299194, 0.17493858933448792, 0.0, 0.05831286311149597, 0.05831286311149597, -0.05831286311149597, 0.11662572622299194]
[2025-05-28 06:01:04,738]: Mean: -0.00138094
[2025-05-28 06:01:04,738]: Min: -0.46650290
[2025-05-28 06:01:04,738]: Max: 0.40819004
[2025-05-28 06:01:04,738]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-28 06:01:04,738]: Sample Values (25 elements): [0.5379847884178162, 0.6995915770530701, 0.04733307659626007, 0.15744176506996155, 0.49002599716186523, 0.38714465498924255, 0.5873764753341675, 0.5009759664535522, 0.23152869939804077, 0.3148152232170105, 5.000953959482407e-41, 0.4922839105129242, 0.18800145387649536, 0.4138198792934418, 0.37696099281311035, 0.5555939078330994, 0.23498912155628204, 0.5424740314483643, 0.45742687582969666, 0.5017515420913696, 0.42127498984336853, 0.540360152721405, 0.45233598351478577, 0.6012305617332458, 0.3620954155921936]
[2025-05-28 06:01:04,739]: Mean: 0.41676170
[2025-05-28 06:01:04,739]: Min: -0.00845403
[2025-05-28 06:01:04,739]: Max: 0.73075110
[2025-05-28 06:01:04,740]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 06:01:04,742]: Sample Values (25 elements): [0.0, -0.03674648702144623, 0.0, 0.0, 0.0, 0.0, -0.03674648702144623, -0.03674648702144623, 0.0, 0.0, 0.0, -0.03674648702144623, 0.0, 0.07349297404289246, 0.0, 0.0, 0.0, 0.03674648702144623, 0.0, -0.03674648702144623, 0.0, -0.03674648702144623, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,742]: Mean: -0.00009993
[2025-05-28 06:01:04,742]: Min: -0.29397190
[2025-05-28 06:01:04,742]: Max: 0.25722539
[2025-05-28 06:01:04,742]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-28 06:01:04,743]: Sample Values (25 elements): [0.881603479385376, -5.372157922682051e-41, 0.8395978808403015, 0.8029593825340271, 5.298870012997863e-41, 1.0254759788513184, -5.414757395997526e-41, 0.7505155205726624, -5.565256851066011e-41, 0.3605170249938965, 1.0178555250167847, 1.1833394765853882, 0.8728441596031189, -5.553766203658547e-41, 0.9598360061645508, 5.206944833738155e-41, 0.6794639825820923, 1.001252293586731, 4.95877487570623e-41, -5.109134200928283e-41, 6.227370375459487e-41, 0.932765543460846, 0.6105582118034363, -6.088501697644898e-41, 4.947284228298767e-41]
[2025-05-28 06:01:04,743]: Mean: 0.37962782
[2025-05-28 06:01:04,743]: Min: -0.00000000
[2025-05-28 06:01:04,743]: Max: 1.18333948
[2025-05-28 06:01:04,744]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 06:01:04,746]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.07760294526815414, 0.0, 0.07760294526815414, 0.0, 0.0, 0.0, 0.03880147263407707, 0.0, 0.0, 0.0, -0.03880147263407707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03880147263407707]
[2025-05-28 06:01:04,746]: Mean: 0.00026288
[2025-05-28 06:01:04,746]: Min: -0.27161032
[2025-05-28 06:01:04,746]: Max: 0.31041178
[2025-05-28 06:01:04,746]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-28 06:01:04,747]: Sample Values (25 elements): [0.019277554005384445, 0.7360081672668457, 0.5562089085578918, 0.7221455574035645, 0.7594987154006958, 0.583135187625885, 0.27516376972198486, 0.4144195318222046, 0.414966881275177, 0.35322853922843933, -0.0012359559768810868, 0.5153361558914185, 0.6805006265640259, 0.5504758358001709, 0.6762850284576416, 0.6254170536994934, 0.7248936891555786, 0.5213872790336609, 0.6838034987449646, 0.7882983088493347, 0.33690333366394043, 0.6962078213691711, 0.5290381908416748, 0.667788028717041, 0.609123945236206]
[2025-05-28 06:01:04,747]: Mean: 0.53130031
[2025-05-28 06:01:04,747]: Min: -0.00123596
[2025-05-28 06:01:04,747]: Max: 0.89160520
[2025-05-28 06:01:04,748]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-28 06:01:04,751]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04474569484591484, 0.0, 0.0, 0.0, 0.0, -0.13423708081245422, 0.0, 0.0, -0.04474569484591484, 0.0, 0.04474569484591484, 0.0, 0.0, 0.0, -0.08949138969182968]
[2025-05-28 06:01:04,752]: Mean: -0.00009104
[2025-05-28 06:01:04,752]: Min: -0.35796556
[2025-05-28 06:01:04,752]: Max: 0.31321988
[2025-05-28 06:01:04,752]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-28 06:01:04,753]: Sample Values (25 elements): [5.291723390829807e-41, -5.229926128553082e-41, -5.890217964942936e-41, 0.6250612139701843, 0.6586928367614746, 5.082649659952544e-41, 0.8893255591392517, 6.114565849081339e-41, 6.286785430346859e-41, -4.907207092219077e-41, 6.22302635022008e-41, 0.6237608194351196, -5.97793924880967e-41, -5.998818595928109e-41, -5.453012844073593e-41, -5.789744865050847e-41, 0.010390684939920902, 0.721348226070404, -6.070284817608675e-41, -5.119083420024989e-41, 5.520975819593347e-41, 0.5315330624580383, 5.350858186024314e-41, -6.159687659632598e-41, 0.8601617217063904]
[2025-05-28 06:01:04,753]: Mean: 0.15893725
[2025-05-28 06:01:04,753]: Min: -0.00000000
[2025-05-28 06:01:04,753]: Max: 1.10213447
[2025-05-28 06:01:04,755]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 06:01:04,762]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04025525599718094, 0.0, 0.0, 0.04025525599718094, 0.12076576799154282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04025525599718094, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,762]: Mean: 0.00006204
[2025-05-28 06:01:04,762]: Min: -0.32204205
[2025-05-28 06:01:04,763]: Max: 0.28178680
[2025-05-28 06:01:04,763]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-28 06:01:04,763]: Sample Values (25 elements): [0.5433048605918884, 0.45433393120765686, 0.4890486001968384, 0.4159785509109497, 0.380557656288147, 0.29255571961402893, 0.5936140418052673, 0.4232863485813141, -6.165292853489898e-41, 0.5882737636566162, 6.034271447075527e-41, 0.30020275712013245, 0.6332972049713135, 0.005502503365278244, 0.4813940227031708, 0.6033891439437866, 0.4991477131843567, 5.415177785536823e-41, 0.26841557025909424, 0.6143753528594971, 0.35788941383361816, 0.3251788020133972, -0.003939847927540541, 0.4533681571483612, 0.5458011031150818]
[2025-05-28 06:01:04,763]: Mean: 0.40550342
[2025-05-28 06:01:04,763]: Min: -0.00411514
[2025-05-28 06:01:04,764]: Max: 0.90040863
[2025-05-28 06:01:04,765]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-28 06:01:04,765]: Sample Values (25 elements): [0.0, -0.04901055246591568, 0.0, -0.04901055246591568, -0.09802110493183136, -0.04901055246591568, -0.14703166484832764, 0.0, 0.0, -0.04901055246591568, -0.04901055246591568, 0.0, -0.19604220986366272, 0.0, -0.04901055246591568, 0.04901055246591568, 0.04901055246591568, 0.0, 0.0, -0.04901055246591568, 0.09802110493183136, 0.0, 0.09802110493183136, -0.09802110493183136, 0.0]
[2025-05-28 06:01:04,765]: Mean: -0.00013910
[2025-05-28 06:01:04,766]: Min: -0.39208442
[2025-05-28 06:01:04,766]: Max: 0.34307387
[2025-05-28 06:01:04,766]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-28 06:01:04,766]: Sample Values (25 elements): [0.06854504346847534, 0.22964134812355042, 0.19955970346927643, 0.3334588408470154, 0.12669384479522705, 5.277009756954396e-41, 0.31757670640945435, 0.5054419040679932, 0.5639845728874207, -5.64877423953977e-41, 0.2930104732513428, 5.777273308718356e-41, 0.3463270962238312, 0.2558569014072418, 0.3659658133983612, 0.260247141122818, 0.3120105266571045, 0.22587330639362335, 0.4364466965198517, 0.3745153248310089, 0.2753516137599945, 0.33945849537849426, 0.3341582715511322, 0.00029498254298232496, 0.4226646423339844]
[2025-05-28 06:01:04,766]: Mean: 0.32918125
[2025-05-28 06:01:04,767]: Min: -0.00035991
[2025-05-28 06:01:04,767]: Max: 0.73842204
[2025-05-28 06:01:04,768]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 06:01:04,774]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.1185721606016159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039524052292108536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,774]: Mean: 0.00001139
[2025-05-28 06:01:04,774]: Min: -0.27666837
[2025-05-28 06:01:04,774]: Max: 0.31619242
[2025-05-28 06:01:04,774]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-28 06:01:04,775]: Sample Values (25 elements): [-5.471650113649113e-41, -6.004423789785409e-41, -5.067095246998539e-41, -6.048144301872343e-41, -5.885313420317799e-41, -5.969391328177288e-41, -5.044954731262206e-41, -5.653118264779177e-41, 5.991671973760053e-41, 5.811184731555016e-41, -5.478376346277872e-41, -5.878867447381905e-41, 6.292811013743456e-41, -6.098030527202306e-41, 0.8541359305381775, 5.704966307959195e-41, -5.734113316017151e-41, -6.108540265684743e-41, 0.7658697962760925, 5.005438114568247e-41, -5.07228005131654e-41, 5.930295101022626e-41, 0.8556153774261475, -5.679602805754916e-41, -5.233149115021029e-41]
[2025-05-28 06:01:04,775]: Mean: 0.11718131
[2025-05-28 06:01:04,775]: Min: -0.00043385
[2025-05-28 06:01:04,775]: Max: 1.09275413
[2025-05-28 06:01:04,777]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 06:01:04,784]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.042022328823804855, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08404465764760971, 0.0, 0.0, 0.0, -0.08404465764760971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,785]: Mean: 0.00001183
[2025-05-28 06:01:04,785]: Min: -0.29415631
[2025-05-28 06:01:04,785]: Max: 0.33617863
[2025-05-28 06:01:04,785]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-28 06:01:04,786]: Sample Values (25 elements): [0.9091225266456604, 0.6935693621635437, 0.5299599766731262, 0.3568466007709503, 0.8947520852088928, 0.800254225730896, 0.22437597811222076, 0.21913546323776245, 0.8911917209625244, 0.549031674861908, 0.3324733376502991, 0.5218856930732727, 0.6002379655838013, 0.5932044982910156, 0.4596579670906067, 0.5330914258956909, 0.4916330873966217, 0.7629035711288452, 0.40876734256744385, 0.444934606552124, 0.6632891893386841, -7.255221862578765e-05, 0.2526935935020447, 0.7686519026756287, 0.8767259120941162]
[2025-05-28 06:01:04,786]: Mean: 0.47154376
[2025-05-28 06:01:04,786]: Min: -0.00249106
[2025-05-28 06:01:04,786]: Max: 1.07671285
[2025-05-28 06:01:04,788]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-28 06:01:04,801]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.04352644085884094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04352644085884094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,802]: Mean: -0.00000018
[2025-05-28 06:01:04,802]: Min: -0.30468509
[2025-05-28 06:01:04,802]: Max: 0.34821153
[2025-05-28 06:01:04,802]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-28 06:01:04,803]: Sample Values (25 elements): [6.110782343227662e-41, 5.144026532689971e-41, -5.499255693396312e-41, -5.504720757407179e-41, 5.873402383371038e-41, 5.088114723963411e-41, -6.075609751773109e-41, -4.946863838759469e-41, 5.641487487525281e-41, 5.621869309024734e-41, -4.878958407061873e-06, 6.151560128539514e-41, -5.152854713015217e-41, 4.976571366203155e-41, -5.260194175382498e-41, 5.810484082322854e-41, -5.558670748283684e-41, -5.637563851825172e-41, 1.0009856224060059, -5.131274716664615e-41, -5.129593158507425e-41, -5.291022741597644e-41, 0.7422640323638916, -5.951174448141066e-41, -5.176676786908739e-41]
[2025-05-28 06:01:04,803]: Mean: 0.04859439
[2025-05-28 06:01:04,803]: Min: -0.00000488
[2025-05-28 06:01:04,804]: Max: 1.00098562
[2025-05-28 06:01:04,806]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 06:01:04,908]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031433410942554474, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,909]: Mean: -0.00000887
[2025-05-28 06:01:04,909]: Min: -0.22003388
[2025-05-28 06:01:04,909]: Max: 0.25146729
[2025-05-28 06:01:04,909]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-28 06:01:04,910]: Sample Values (25 elements): [6.060195468665536e-41, 0.44179242849349976, 5.435496613269533e-41, -5.501637900785664e-41, 5.805713954032399e-05, 0.3751707375049591, 0.6231081485748291, -5.799413824454688e-41, 0.43956318497657776, 0.3226988911628723, 5.800394733379715e-41, 0.7319955825805664, 0.49925175309181213, -5.004176945950354e-41, -5.186065486619715e-41, 0.5333632826805115, 5.031362136158256e-41, -5.744623054499588e-41, 0.5516490936279297, 0.6542286276817322, 0.28956982493400574, 0.5162651538848877, 6.001901452549624e-41, 0.6468055248260498, 0.2048014998435974]
[2025-05-28 06:01:04,910]: Mean: 0.24812075
[2025-05-28 06:01:04,910]: Min: -0.00867942
[2025-05-28 06:01:04,910]: Max: 0.92185891
[2025-05-28 06:01:04,912]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-28 06:01:04,913]: Sample Values (25 elements): [0.0, 0.0, 0.08432982861995697, 0.0, 0.0, 0.0, -0.056219883263111115, 0.0, 0.028109941631555557, 0.0, 0.0, -0.056219883263111115, 0.0, -0.028109941631555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028109941631555557]
[2025-05-28 06:01:04,913]: Mean: -0.00003775
[2025-05-28 06:01:04,913]: Min: -0.19676960
[2025-05-28 06:01:04,914]: Max: 0.22487953
[2025-05-28 06:01:04,914]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-28 06:01:04,914]: Sample Values (25 elements): [0.3272274434566498, 0.35641929507255554, 0.4224790334701538, -4.0267931922244316e-07, 1.7817929176544567e-07, -8.886099385563284e-05, 0.44242429733276367, -5.297608844379971e-41, 5.112777576935528e-41, 1.824937712058272e-08, 0.027782028540968895, 4.08716687161359e-06, 0.49753478169441223, 3.109040846993594e-08, -6.279358548485938e-41, 6.218542195134241e-41, 9.750939170771744e-06, -8.273238449874043e-08, 0.2442592829465866, 0.24733394384384155, 0.3757443130016327, 0.39380863308906555, 0.30586010217666626, 0.34047791361808777, 5.479637514895765e-41]
[2025-05-28 06:01:04,914]: Mean: 0.15449908
[2025-05-28 06:01:04,914]: Min: -0.01909514
[2025-05-28 06:01:04,914]: Max: 0.59741437
[2025-05-28 06:01:04,916]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 06:01:04,955]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:04,955]: Mean: -0.00000233
[2025-05-28 06:01:04,956]: Min: -0.11151689
[2025-05-28 06:01:04,956]: Max: 0.12744787
[2025-05-28 06:01:04,956]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-28 06:01:04,958]: Sample Values (25 elements): [-5.780636425032735e-41, -5.758075519757106e-41, -5.68660929807654e-41, 6.114145459542042e-41, -5.376221688228593e-41, 5.745183573885318e-41, 6.068042740065755e-41, -6.248529982270792e-41, -6.042258848322179e-41, -6.247969462885062e-41, 5.008240711496896e-41, 5.487484786295984e-41, 5.066814987305674e-41, 5.551944515654925e-41, -5.133516794207535e-41, -5.401304930740007e-41, 6.074909102540947e-41, -5.431993367108721e-41, -5.855746022720546e-41, -4.935513321198438e-41, 5.594403859123967e-41, -5.975276781727452e-41, -5.890358094789369e-41, 5.962244706009232e-41, -5.763540583767973e-41]
[2025-05-28 06:01:04,958]: Mean: 0.01067354
[2025-05-28 06:01:04,959]: Min: -0.00006274
[2025-05-28 06:01:04,959]: Max: 0.82839787
[2025-05-28 06:01:04,960]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 06:01:05,011]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 06:01:05,011]: Mean: -0.00000107
[2025-05-28 06:01:05,011]: Min: -0.10827814
[2025-05-28 06:01:05,012]: Max: 0.12374645
[2025-05-28 06:01:05,012]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-28 06:01:05,012]: Sample Values (25 elements): [0.05095083266496658, -0.016142986714839935, -5.579129705862827e-41, 0.04669986665248871, -5.374119740532106e-41, 4.987221234532024e-41, 0.07307092845439911, -5.46184102439884e-41, -4.380846689855389e-07, 0.003845229046419263, 0.14527425169944763, -1.792090552044101e-05, 0.13320361077785492, -6.085839230562681e-41, 2.0668338038376532e-05, 0.003954644780606031, -5.694876959016057e-41, 0.17641425132751465, 0.15484069287776947, -5.76508201207873e-41, 0.17733542621135712, 0.1734890192747116, -4.98301733913905e-41, -0.0033271010033786297, 0.09195911884307861]
[2025-05-28 06:01:05,012]: Mean: 0.06561043
[2025-05-28 06:01:05,012]: Min: -0.04303562
[2025-05-28 06:01:05,013]: Max: 0.42568353
[2025-05-28 06:01:05,013]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-28 06:01:05,013]: Sample Values (25 elements): [-2.7184796635992825e-05, -5.97569717126675e-41, -0.25552716851234436, -6.072106505612297e-41, -5.016087982897115e-41, 3.570822002529894e-07, 0.006570266559720039, 4.914213584540701e-41, 6.183369603679688e-41, 5.685207999612215e-41, 0.002604004228487611, -0.06357019394636154, 0.054814212024211884, -0.017232824116945267, -0.11364664882421494, 0.2541317343711853, 0.22672124207019806, -0.007026778534054756, -0.21382778882980347, 0.08030861616134644, -5.401865450125737e-41, -0.03716885671019554, 0.3383390009403229, -0.2522786259651184, 5.660825406332964e-41]
[2025-05-28 06:01:05,013]: Mean: 0.00002030
[2025-05-28 06:01:05,013]: Min: -0.52136391
[2025-05-28 06:01:05,014]: Max: 0.51030201
[2025-05-28 06:01:05,014]: 


QAT of ResNet18 with hardtanh down to 3 bits...
[2025-05-28 06:01:06,410]: [ResNet18_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-28 06:01:06,542]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-28 06:02:53,529]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.3573 Train Acc: 0.8768 Eval Loss: 0.7462 Eval Acc: 0.7767 (LR: 0.00100000)
[2025-05-28 06:04:39,257]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.3591 Train Acc: 0.8752 Eval Loss: 0.5419 Eval Acc: 0.8344 (LR: 0.00100000)
[2025-05-28 06:06:26,282]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.3631 Train Acc: 0.8742 Eval Loss: 0.5640 Eval Acc: 0.8268 (LR: 0.00100000)
[2025-05-28 06:08:11,224]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.3588 Train Acc: 0.8748 Eval Loss: 0.7103 Eval Acc: 0.7969 (LR: 0.00100000)
[2025-05-28 06:09:55,777]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.3635 Train Acc: 0.8734 Eval Loss: 0.9785 Eval Acc: 0.7296 (LR: 0.00100000)
[2025-05-28 06:11:39,437]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.3654 Train Acc: 0.8735 Eval Loss: 0.8253 Eval Acc: 0.7551 (LR: 0.00100000)
[2025-05-28 06:13:23,013]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.3644 Train Acc: 0.8723 Eval Loss: 0.5809 Eval Acc: 0.8112 (LR: 0.00100000)
[2025-05-28 06:15:06,966]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.3580 Train Acc: 0.8761 Eval Loss: 0.6445 Eval Acc: 0.8060 (LR: 0.00010000)
[2025-05-28 06:16:51,895]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.2600 Train Acc: 0.9104 Eval Loss: 0.3339 Eval Acc: 0.8917 (LR: 0.00010000)
[2025-05-28 06:18:35,459]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.2276 Train Acc: 0.9219 Eval Loss: 0.3363 Eval Acc: 0.8927 (LR: 0.00010000)
[2025-05-28 06:20:19,058]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.2173 Train Acc: 0.9248 Eval Loss: 0.3341 Eval Acc: 0.8913 (LR: 0.00010000)
[2025-05-28 06:22:02,389]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.2103 Train Acc: 0.9264 Eval Loss: 0.3265 Eval Acc: 0.8960 (LR: 0.00010000)
[2025-05-28 06:23:45,935]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.2028 Train Acc: 0.9294 Eval Loss: 0.3240 Eval Acc: 0.8968 (LR: 0.00010000)
[2025-05-28 06:25:29,769]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.2041 Train Acc: 0.9282 Eval Loss: 0.3369 Eval Acc: 0.8959 (LR: 0.00010000)
[2025-05-28 06:27:13,008]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.1960 Train Acc: 0.9318 Eval Loss: 0.3338 Eval Acc: 0.8951 (LR: 0.00010000)
[2025-05-28 06:28:57,758]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.1919 Train Acc: 0.9329 Eval Loss: 0.3383 Eval Acc: 0.8958 (LR: 0.00010000)
[2025-05-28 06:30:41,454]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.1893 Train Acc: 0.9348 Eval Loss: 0.3260 Eval Acc: 0.9002 (LR: 0.00010000)
[2025-05-28 06:32:24,994]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.1874 Train Acc: 0.9344 Eval Loss: 0.3456 Eval Acc: 0.8954 (LR: 0.00010000)
[2025-05-28 06:34:06,012]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.1848 Train Acc: 0.9347 Eval Loss: 0.3326 Eval Acc: 0.9002 (LR: 0.00001000)
[2025-05-28 06:35:48,102]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.1681 Train Acc: 0.9411 Eval Loss: 0.2993 Eval Acc: 0.9070 (LR: 0.00001000)
[2025-05-28 06:37:31,710]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.1620 Train Acc: 0.9421 Eval Loss: 0.3000 Eval Acc: 0.9072 (LR: 0.00001000)
[2025-05-28 06:39:21,587]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.1631 Train Acc: 0.9436 Eval Loss: 0.3026 Eval Acc: 0.9061 (LR: 0.00001000)
[2025-05-28 06:41:09,517]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.1598 Train Acc: 0.9453 Eval Loss: 0.3014 Eval Acc: 0.9084 (LR: 0.00001000)
[2025-05-28 06:42:55,716]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.1572 Train Acc: 0.9461 Eval Loss: 0.3016 Eval Acc: 0.9065 (LR: 0.00001000)
[2025-05-28 06:44:39,373]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.1571 Train Acc: 0.9454 Eval Loss: 0.3102 Eval Acc: 0.9053 (LR: 0.00001000)
[2025-05-28 06:46:23,370]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.1577 Train Acc: 0.9445 Eval Loss: 0.3084 Eval Acc: 0.9050 (LR: 0.00000100)
[2025-05-28 06:48:09,607]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.1579 Train Acc: 0.9450 Eval Loss: 0.3097 Eval Acc: 0.9041 (LR: 0.00000100)
[2025-05-28 06:49:57,403]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.1550 Train Acc: 0.9455 Eval Loss: 0.3061 Eval Acc: 0.9075 (LR: 0.00000100)
[2025-05-28 06:51:44,632]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.1539 Train Acc: 0.9471 Eval Loss: 0.3081 Eval Acc: 0.9054 (LR: 0.00000100)
[2025-05-28 06:53:26,949]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.1532 Train Acc: 0.9470 Eval Loss: 0.3065 Eval Acc: 0.9061 (LR: 0.00000100)
[2025-05-28 06:55:08,288]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.1550 Train Acc: 0.9465 Eval Loss: 0.3040 Eval Acc: 0.9073 (LR: 0.00000100)
[2025-05-28 06:56:49,640]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.1528 Train Acc: 0.9459 Eval Loss: 0.3072 Eval Acc: 0.9061 (LR: 0.00000010)
[2025-05-28 06:58:52,803]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.1536 Train Acc: 0.9462 Eval Loss: 0.3070 Eval Acc: 0.9079 (LR: 0.00000010)
[2025-05-28 07:00:36,132]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.1521 Train Acc: 0.9472 Eval Loss: 0.3081 Eval Acc: 0.9063 (LR: 0.00000010)
[2025-05-28 07:02:20,493]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.1543 Train Acc: 0.9461 Eval Loss: 0.3081 Eval Acc: 0.9081 (LR: 0.00000010)
[2025-05-28 07:04:06,185]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.1506 Train Acc: 0.9479 Eval Loss: 0.3052 Eval Acc: 0.9075 (LR: 0.00000010)
[2025-05-28 07:05:51,847]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.1553 Train Acc: 0.9465 Eval Loss: 0.3062 Eval Acc: 0.9069 (LR: 0.00000010)
[2025-05-28 07:07:38,713]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.1538 Train Acc: 0.9466 Eval Loss: 0.3115 Eval Acc: 0.9058 (LR: 0.00000010)
[2025-05-28 07:09:24,294]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.1576 Train Acc: 0.9445 Eval Loss: 0.3077 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-05-28 07:11:11,387]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.1536 Train Acc: 0.9468 Eval Loss: 0.3025 Eval Acc: 0.9062 (LR: 0.00000010)
[2025-05-28 07:12:58,347]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.1542 Train Acc: 0.9464 Eval Loss: 0.3090 Eval Acc: 0.9051 (LR: 0.00000010)
[2025-05-28 07:14:46,305]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.1551 Train Acc: 0.9463 Eval Loss: 0.3080 Eval Acc: 0.9062 (LR: 0.00000010)
[2025-05-28 07:16:37,203]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.1520 Train Acc: 0.9464 Eval Loss: 0.3027 Eval Acc: 0.9082 (LR: 0.00000010)
[2025-05-28 07:18:22,413]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.1496 Train Acc: 0.9473 Eval Loss: 0.3052 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-05-28 07:20:05,529]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.1554 Train Acc: 0.9450 Eval Loss: 0.3021 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-05-28 07:21:48,633]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.1518 Train Acc: 0.9473 Eval Loss: 0.3045 Eval Acc: 0.9114 (LR: 0.00000010)
[2025-05-28 07:23:32,221]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.1571 Train Acc: 0.9455 Eval Loss: 0.3051 Eval Acc: 0.9071 (LR: 0.00000010)
[2025-05-28 07:25:15,547]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.1551 Train Acc: 0.9458 Eval Loss: 0.3053 Eval Acc: 0.9065 (LR: 0.00000010)
[2025-05-28 07:26:58,709]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.1486 Train Acc: 0.9492 Eval Loss: 0.3078 Eval Acc: 0.9059 (LR: 0.00000010)
[2025-05-28 07:28:41,937]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.1518 Train Acc: 0.9474 Eval Loss: 0.3078 Eval Acc: 0.9074 (LR: 0.00000010)
[2025-05-28 07:30:25,245]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.1496 Train Acc: 0.9492 Eval Loss: 0.3100 Eval Acc: 0.9061 (LR: 0.00000010)
[2025-05-28 07:32:08,193]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.1523 Train Acc: 0.9464 Eval Loss: 0.3053 Eval Acc: 0.9064 (LR: 0.00000010)
[2025-05-28 07:34:09,244]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.1535 Train Acc: 0.9468 Eval Loss: 0.3077 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-05-28 07:36:10,124]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.1488 Train Acc: 0.9481 Eval Loss: 0.3064 Eval Acc: 0.9058 (LR: 0.00000010)
[2025-05-28 07:38:10,975]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.1516 Train Acc: 0.9483 Eval Loss: 0.3064 Eval Acc: 0.9071 (LR: 0.00000010)
[2025-05-28 07:40:07,071]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.1544 Train Acc: 0.9463 Eval Loss: 0.3062 Eval Acc: 0.9049 (LR: 0.00000010)
[2025-05-28 07:41:59,627]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.1527 Train Acc: 0.9470 Eval Loss: 0.3055 Eval Acc: 0.9073 (LR: 0.00000010)
[2025-05-28 07:43:55,029]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.1542 Train Acc: 0.9460 Eval Loss: 0.3086 Eval Acc: 0.9058 (LR: 0.00000010)
[2025-05-28 07:45:47,377]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.1515 Train Acc: 0.9473 Eval Loss: 0.3079 Eval Acc: 0.9063 (LR: 0.00000010)
[2025-05-28 07:47:38,113]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.1526 Train Acc: 0.9465 Eval Loss: 0.3090 Eval Acc: 0.9060 (LR: 0.00000010)
[2025-05-28 07:47:38,114]: [ResNet18_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.9114
[2025-05-28 07:47:38,270]: 


Quantization of model down to 3 bits finished
[2025-05-28 07:47:38,270]: Model Architecture:
[2025-05-28 07:47:38,568]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0918], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31193268299102783, max_val=0.3304944634437561)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1381], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3987715244293213, max_val=0.568264365196228)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1136], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4415527582168579, max_val=0.35330212116241455)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1324], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43685054779052734, max_val=0.4896620810031891)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1183], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.452688068151474, max_val=0.3753957152366638)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0876], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30965250730514526, max_val=0.30327367782592773)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1292], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.452336847782135, max_val=0.4524087905883789)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0816], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33660465478897095, max_val=0.23424667119979858)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0739], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2595546245574951, max_val=0.25768816471099854)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0967], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31462717056274414, max_val=0.3625372648239136)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0866], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31336671113967896, max_val=0.29291635751724243)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1010], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3741002678871155, max_val=0.3330754041671753)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0946], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32424676418304443, max_val=0.33764857053756714)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2927584648132324, max_val=0.31810152530670166)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0975], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.317361056804657, max_val=0.36486202478408813)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0678], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22567874193191528, max_val=0.24910908937454224)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0564], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18684785068035126, max_val=0.2078070193529129)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0375], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1346110850572586, max_val=0.12772347033023834)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0368], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12628282606601715, max_val=0.1315307915210724)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-28 07:47:38,580]: 
Model Weights:
[2025-05-28 07:47:38,580]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-28 07:47:38,581]: Sample Values (25 elements): [-0.16300362348556519, -0.2206682413816452, 0.2501036822795868, -0.20386859774589539, 0.3005724847316742, 0.16907455027103424, 0.16995061933994293, -0.10589058697223663, -0.2246706485748291, 0.06834849715232849, -0.21237441897392273, -0.15829123556613922, 0.21548530459403992, 0.09198037534952164, 0.02578701451420784, 0.24476513266563416, -0.2184528261423111, -0.1330202966928482, -0.09542160481214523, -0.20491570234298706, -0.17855705320835114, -0.11051107943058014, 0.11916612833738327, 0.25328245759010315, -0.16156552731990814]
[2025-05-28 07:47:38,581]: Mean: 0.00071029
[2025-05-28 07:47:38,581]: Min: -0.39022973
[2025-05-28 07:47:38,581]: Max: 0.39670190
[2025-05-28 07:47:38,581]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-28 07:47:38,582]: Sample Values (25 elements): [0.5313996076583862, 0.9845708608627319, 0.39338740706443787, 0.7985325455665588, 0.5538054704666138, 0.7761862874031067, 0.6244977116584778, 0.6193541288375854, 0.6027842164039612, 0.7813385725021362, 0.48794686794281006, 0.5587682723999023, 0.7957625985145569, 0.10855551809072495, 0.8304810523986816, 0.43848544359207153, 0.6880653500556946, 0.7222769856452942, 0.7720328569412231, 0.7977461218833923, 0.7616997361183167, 0.9729580879211426, 0.5979328751564026, 0.4501365125179291, 0.6149678230285645]
[2025-05-28 07:47:38,582]: Mean: 0.69976008
[2025-05-28 07:47:38,582]: Min: 0.10855552
[2025-05-28 07:47:38,582]: Max: 1.16070998
[2025-05-28 07:47:38,583]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 07:47:38,584]: Sample Values (25 elements): [0.0, 0.0917753130197525, -0.0917753130197525, -0.0917753130197525, 0.0, 0.0917753130197525, 0.0917753130197525, 0.0, 0.0, 0.0, 0.0, 0.0917753130197525, 0.0917753130197525, 0.0, 0.0, -0.0917753130197525, 0.0, 0.0, 0.0917753130197525, 0.0, 0.0, 0.0917753130197525, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,584]: Mean: 0.00028381
[2025-05-28 07:47:38,584]: Min: -0.27532595
[2025-05-28 07:47:38,584]: Max: 0.36710125
[2025-05-28 07:47:38,584]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-28 07:47:38,585]: Sample Values (25 elements): [0.6030267477035522, 0.5766050815582275, 1.1693860292434692, 0.8823171854019165, 1.1562021970748901, 0.9253166317939758, 0.07169505953788757, 0.9079060554504395, 0.6295517086982727, 0.7730987668037415, 0.7232202291488647, 1.249656319618225, 0.589133620262146, 0.7140313386917114, 0.2642514705657959, 0.37878531217575073, 0.2554114758968353, 0.969363272190094, 1.1599438190460205, -6.038335212622069e-41, 0.8725886344909668, 0.6864659190177917, 0.024796735495328903, 0.1665615290403366, 0.2964864671230316]
[2025-05-28 07:47:38,585]: Mean: 0.71541905
[2025-05-28 07:47:38,585]: Min: -0.00000000
[2025-05-28 07:47:38,585]: Max: 1.27223051
[2025-05-28 07:47:38,586]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 07:47:38,587]: Sample Values (25 elements): [0.0, 0.0, -0.1381479948759079, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1381479948759079, 0.0, 0.0, 0.0, 0.1381479948759079, 0.0, 0.0, 0.1381479948759079, 0.0, 0.0, 0.1381479948759079, 0.1381479948759079, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,587]: Mean: 0.00046469
[2025-05-28 07:47:38,587]: Min: -0.41444397
[2025-05-28 07:47:38,587]: Max: 0.55259198
[2025-05-28 07:47:38,587]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-28 07:47:38,587]: Sample Values (25 elements): [0.7845008373260498, 0.6175798177719116, 0.8053246736526489, 0.727500855922699, 0.9334096312522888, 0.759011447429657, 0.5999857783317566, 0.8362450003623962, 0.7290146946907043, 0.6681350469589233, 0.6633358001708984, 0.8477109670639038, 0.7794140577316284, 0.8702854514122009, 1.3675581216812134, 0.6704517602920532, 0.9087432026863098, 0.8338797688484192, 0.8304121494293213, 0.604366660118103, 0.7920950055122375, 0.88150954246521, 0.7877928018569946, 0.8919444680213928, 0.7349822521209717]
[2025-05-28 07:47:38,587]: Mean: 0.81626093
[2025-05-28 07:47:38,588]: Min: 0.59998578
[2025-05-28 07:47:38,588]: Max: 1.36755812
[2025-05-28 07:47:38,589]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 07:47:38,589]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.11355070024728775, 0.0, -0.11355070024728775, -0.11355070024728775, -0.11355070024728775, 0.0, 0.0, 0.0, 0.0, 0.2271014004945755, 0.0, 0.0, 0.11355070024728775, 0.0, 0.11355070024728775, 0.0, 0.0, -0.11355070024728775, 0.0, 0.0, 0.11355070024728775, 0.0]
[2025-05-28 07:47:38,589]: Mean: -0.00067766
[2025-05-28 07:47:38,590]: Min: -0.45420280
[2025-05-28 07:47:38,590]: Max: 0.34065211
[2025-05-28 07:47:38,590]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-28 07:47:38,590]: Sample Values (25 elements): [1.2837327718734741, 0.5745976567268372, 0.7388598322868347, 0.854493260383606, 0.6555368304252625, 1.098724365234375, 0.06540754437446594, 0.8082315921783447, 0.7780954241752625, 1.37909734249115, 0.6596123576164246, 1.4642764329910278, 1.039023756980896, 0.9331352710723877, 1.1054471731185913, 1.1517691612243652, 0.7013761401176453, 0.7622255682945251, -5.307558063476677e-41, 0.6071534752845764, 0.9193399548530579, 0.5184261202812195, 0.8944700956344604, 0.23250168561935425, 0.7650139927864075]
[2025-05-28 07:47:38,590]: Mean: 0.81612319
[2025-05-28 07:47:38,590]: Min: -0.00000000
[2025-05-28 07:47:38,591]: Max: 1.46427643
[2025-05-28 07:47:38,591]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 07:47:38,592]: Sample Values (25 elements): [0.0, -0.13235895335674286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13235895335674286, 0.0, 0.13235895335674286, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13235895335674286, 0.13235895335674286, 0.0, 0.0, -0.13235895335674286, 0.0, 0.0, -0.13235895335674286]
[2025-05-28 07:47:38,592]: Mean: -0.00037700
[2025-05-28 07:47:38,592]: Min: -0.39707685
[2025-05-28 07:47:38,593]: Max: 0.52943581
[2025-05-28 07:47:38,593]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-28 07:47:38,593]: Sample Values (25 elements): [0.706590473651886, 0.4937956929206848, 0.7956472039222717, 1.032373070716858, 0.8908030986785889, 0.8723204135894775, 1.0927923917770386, 0.9752766489982605, 0.6508669257164001, 0.4963151216506958, 0.772440493106842, 0.6087691783905029, 0.8950896263122559, 0.6496678590774536, 0.7604016661643982, 0.8472849726676941, 0.6944406032562256, 1.3276290893554688, 1.0710963010787964, 0.8142030835151672, 0.6224935054779053, 0.6911383867263794, 0.7574720978736877, 0.8046880960464478, 0.6852539777755737]
[2025-05-28 07:47:38,593]: Mean: 0.82295346
[2025-05-28 07:47:38,593]: Min: 0.46862146
[2025-05-28 07:47:38,593]: Max: 1.38881063
[2025-05-28 07:47:38,594]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-28 07:47:38,595]: Sample Values (25 elements): [-0.11829768121242523, 0.0, 0.11829768121242523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11829768121242523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11829768121242523, 0.0, 0.0]
[2025-05-28 07:47:38,596]: Mean: 0.00080547
[2025-05-28 07:47:38,596]: Min: -0.47319072
[2025-05-28 07:47:38,596]: Max: 0.35489303
[2025-05-28 07:47:38,596]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-28 07:47:38,597]: Sample Values (25 elements): [0.2637399137020111, 0.17562955617904663, 0.5685171484947205, 0.7702033519744873, 0.7089888453483582, 0.7260605096817017, 5.344131953395555e-41, 0.9693520665168762, 0.8544550538063049, 5.589219054805965e-41, 0.919517457485199, 0.01751967892050743, 0.8271812796592712, 0.47476935386657715, 0.6049767136573792, 0.5498131513595581, 0.21481174230575562, 0.9810164570808411, 0.4159334897994995, 6.291830104818429e-41, 5.105070435381741e-41, 6.138528052821294e-41, 0.7499252557754517, 0.4208519756793976, 6.235357776706139e-41]
[2025-05-28 07:47:38,597]: Mean: 0.42635691
[2025-05-28 07:47:38,597]: Min: -0.00000000
[2025-05-28 07:47:38,597]: Max: 1.10733235
[2025-05-28 07:47:38,598]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 07:47:38,600]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08756088465452194, 0.0, -0.08756088465452194, 0.0, -0.08756088465452194, 0.0, 0.0, 0.0, 0.0, 0.08756088465452194, -0.08756088465452194, 0.0, 0.0, -0.17512176930904388]
[2025-05-28 07:47:38,600]: Mean: 0.00025653
[2025-05-28 07:47:38,600]: Min: -0.35024354
[2025-05-28 07:47:38,601]: Max: 0.26268265
[2025-05-28 07:47:38,601]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-28 07:47:38,601]: Sample Values (25 elements): [0.4675004482269287, 0.6927111148834229, 0.46781080961227417, 0.7531294226646423, 0.4010729491710663, 0.6383394002914429, 0.5899609327316284, 0.45130857825279236, 0.6629676818847656, 0.914030134677887, 0.19684457778930664, -5.424706615094232e-41, 0.7160667777061462, 0.6469711065292358, 0.9328888058662415, 0.5389367341995239, 0.5846663117408752, 0.7149648070335388, 0.8048099875450134, 0.7111073136329651, 0.6042857766151428, 0.48858100175857544, 0.7992806434631348, 0.6311066746711731, 0.6595744490623474]
[2025-05-28 07:47:38,601]: Mean: 0.60544312
[2025-05-28 07:47:38,601]: Min: -0.00000000
[2025-05-28 07:47:38,601]: Max: 1.00070381
[2025-05-28 07:47:38,602]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-28 07:47:38,603]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.12924937903881073, 0.0, 0.0, 0.0, -0.12924937903881073, -0.12924937903881073, 0.0, -0.12924937903881073, 0.0, -0.12924937903881073, -0.12924937903881073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,603]: Mean: -0.00171975
[2025-05-28 07:47:38,603]: Min: -0.38774812
[2025-05-28 07:47:38,603]: Max: 0.51699752
[2025-05-28 07:47:38,603]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-28 07:47:38,603]: Sample Values (25 elements): [0.22296492755413055, 0.2213236689567566, 0.47734585404396057, 0.20386014878749847, 0.7077499628067017, 5.619627231481814e-41, 0.4259549677371979, 0.47942572832107544, 0.2498023509979248, 0.3804718255996704, 0.5297441482543945, 0.35931622982025146, 0.6273027658462524, 0.5235220789909363, 0.6845808625221252, 0.2577110826969147, 0.6275993585586548, 0.1654343158006668, 0.5696350932121277, 0.2719893753528595, 0.33039069175720215, 0.5049415230751038, 0.32977232336997986, 0.6254997253417969, 0.41477060317993164]
[2025-05-28 07:47:38,604]: Mean: 0.40045291
[2025-05-28 07:47:38,604]: Min: 0.00000000
[2025-05-28 07:47:38,604]: Max: 0.70774996
[2025-05-28 07:47:38,605]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 07:47:38,607]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.08155019581317902, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08155019581317902, 0.0, 0.0, -0.08155019581317902, 0.0, 0.0, -0.08155019581317902, -0.08155019581317902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,607]: Mean: -0.00018582
[2025-05-28 07:47:38,608]: Min: -0.32620078
[2025-05-28 07:47:38,608]: Max: 0.24465059
[2025-05-28 07:47:38,608]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-28 07:47:38,608]: Sample Values (25 elements): [-5.953836915223283e-41, 0.7567654848098755, 0.8897693157196045, 5.655220212475664e-41, -5.248843657821467e-41, 5.863172904581467e-41, 5.075362907938055e-41, -5.845096154391677e-41, 6.113865199849177e-41, 1.0031596422195435, 5.15131328470446e-41, 0.7281715273857117, 0.7832790613174438, -5.05868745621259e-41, 0.7328816056251526, -5.734673835402881e-41, 6.142311558674971e-41, 0.8737071752548218, -6.088501697644898e-41, 6.013812489496385e-41, 5.293545078833429e-41, 0.8997618556022644, -5.740419159106613e-41, 5.328717670287982e-41, -6.015914437192872e-41]
[2025-05-28 07:47:38,608]: Mean: 0.38105506
[2025-05-28 07:47:38,609]: Min: -0.00000000
[2025-05-28 07:47:38,609]: Max: 1.16775787
[2025-05-28 07:47:38,610]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 07:47:38,611]: Sample Values (25 elements): [0.07389183342456818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07389183342456818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,612]: Mean: 0.00026709
[2025-05-28 07:47:38,612]: Min: -0.29556733
[2025-05-28 07:47:38,612]: Max: 0.22167550
[2025-05-28 07:47:38,612]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-28 07:47:38,612]: Sample Values (25 elements): [0.7140260934829712, 0.40381312370300293, 0.6388547420501709, 0.6699925661087036, 0.733787477016449, -6.187853758765527e-41, 0.514090359210968, 0.7779096961021423, 0.8047956824302673, 0.08352077007293701, 0.9235683083534241, 0.5502484440803528, 0.7676011919975281, 0.6609218716621399, 0.6823245286941528, 0.6693218946456909, 0.4336220324039459, 0.5664215087890625, 0.3941769003868103, 0.47819364070892334, 0.4976987838745117, 0.7321376204490662, 0.8270055651664734, 0.38752055168151855, 0.5504019260406494]
[2025-05-28 07:47:38,612]: Mean: 0.53721809
[2025-05-28 07:47:38,613]: Min: -0.00000000
[2025-05-28 07:47:38,613]: Max: 0.92356831
[2025-05-28 07:47:38,614]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-28 07:47:38,617]: Sample Values (25 elements): [0.19347555935382843, 0.09673777967691422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09673777967691422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09673777967691422, 0.0, 0.0, 0.0, 0.0, 0.09673777967691422, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,617]: Mean: -0.00009873
[2025-05-28 07:47:38,617]: Min: -0.29021335
[2025-05-28 07:47:38,617]: Max: 0.38695112
[2025-05-28 07:47:38,617]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-28 07:47:38,618]: Sample Values (25 elements): [0.6238119006156921, 6.036513524618447e-41, -5.20722509343102e-41, 5.304054817315865e-41, 5.488325565374579e-41, 5.405929215672279e-41, -5.18970886262696e-41, 0.986912727355957, -5.594964378509697e-41, 5.84411524546665e-41, 5.553205684272818e-41, -5.890217964942936e-41, 0.04407777264714241, 6.22302635022008e-41, -6.013812489496385e-41, 0.7877832055091858, -5.873262253524606e-41, 5.885033160624934e-41, 6.286785430346859e-41, 6.296174130057836e-41, -5.387011686403894e-41, -6.177624279975956e-41, 1.04712975025177, -5.119083420024989e-41, -5.340208317695445e-41]
[2025-05-28 07:47:38,618]: Mean: 0.16102806
[2025-05-28 07:47:38,618]: Min: -0.00000000
[2025-05-28 07:47:38,619]: Max: 1.10217071
[2025-05-28 07:47:38,620]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 07:47:38,628]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,628]: Mean: 0.00008957
[2025-05-28 07:47:38,628]: Min: -0.34644750
[2025-05-28 07:47:38,629]: Max: 0.25983563
[2025-05-28 07:47:38,629]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-28 07:47:38,629]: Sample Values (25 elements): [0.6479368805885315, 0.21036753058433533, 0.0641745775938034, 0.5590859055519104, 0.5601877570152283, 0.5859994888305664, 0.7035453915596008, 0.2218960076570511, 6.071265726533702e-41, 0.6282081604003906, 0.5029787421226501, 0.628846287727356, 0.28318190574645996, 0.5936980843544006, 0.5366420745849609, 0.6825941801071167, 0.5064447522163391, 0.4327291250228882, 0.47890275716781616, 0.40012091398239136, 0.4878411591053009, 0.438297301530838, 0.24906505644321442, 4.957934096627635e-41, 0.6432282328605652]
[2025-05-28 07:47:38,629]: Mean: 0.41023946
[2025-05-28 07:47:38,629]: Min: -0.00656256
[2025-05-28 07:47:38,629]: Max: 0.92876393
[2025-05-28 07:47:38,630]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-28 07:47:38,631]: Sample Values (25 elements): [0.10102509707212448, 0.0, 0.0, 0.0, 0.0, 0.10102509707212448, 0.10102509707212448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10102509707212448, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,631]: Mean: -0.00020348
[2025-05-28 07:47:38,631]: Min: -0.40410039
[2025-05-28 07:47:38,631]: Max: 0.30307528
[2025-05-28 07:47:38,632]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-28 07:47:38,632]: Sample Values (25 elements): [0.39272427558898926, 0.1700192540884018, 0.30455437302589417, 0.19093286991119385, 0.29077744483947754, 0.3159950077533722, 6.26590608322842e-41, 0.423662006855011, 0.4130499064922333, 0.1916516274213791, 0.3207584023475647, 0.41726571321487427, 0.6846891045570374, 0.6861256957054138, 0.5944229960441589, 0.26235583424568176, 0.3153655529022217, 0.5568490028381348, 0.5023095011711121, 0.40514588356018066, 0.31445321440696716, 0.3821795880794525, 0.23651020228862762, 0.5001916885375977, 0.42828211188316345]
[2025-05-28 07:47:38,632]: Mean: 0.32525775
[2025-05-28 07:47:38,632]: Min: -0.00000000
[2025-05-28 07:47:38,632]: Max: 0.72090364
[2025-05-28 07:47:38,633]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 07:47:38,639]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,639]: Mean: 0.00000882
[2025-05-28 07:47:38,640]: Min: -0.28366944
[2025-05-28 07:47:38,640]: Max: 0.37822592
[2025-05-28 07:47:38,640]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-28 07:47:38,640]: Sample Values (25 elements): [5.995735739306595e-41, 0.7548621892929077, 5.048598107269451e-41, 6.048985080950938e-41, 6.124234808485181e-41, -6.153381816543137e-41, 0.6912322640419006, 4.97629110651029e-41, -5.933798347183438e-41, -5.952715876451823e-41, 6.134464287274752e-41, 0.8768221139907837, 0.4057641625404358, 4.974609548353101e-41, 5.704966307959195e-41, -5.786101489043602e-41, 0.616421639919281, -5.400744411354277e-41, -6.12815844418529e-41, 5.674698261129779e-41, -5.80459862877269e-41, 5.886154199396394e-41, 5.165466399194141e-41, 5.535689453468757e-41, -5.433394665573046e-41]
[2025-05-28 07:47:38,641]: Mean: 0.11820009
[2025-05-28 07:47:38,641]: Min: -0.00000000
[2025-05-28 07:47:38,641]: Max: 1.06325030
[2025-05-28 07:47:38,642]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 07:47:38,648]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08726571500301361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,649]: Mean: 0.00000888
[2025-05-28 07:47:38,649]: Min: -0.26179713
[2025-05-28 07:47:38,649]: Max: 0.34906286
[2025-05-28 07:47:38,649]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-28 07:47:38,650]: Sample Values (25 elements): [0.515576958656311, 0.8823320865631104, 0.06907172501087189, 0.5100632309913635, 0.5399393439292908, 1.0681984424591064, 0.5100275874137878, 0.926470935344696, 0.581976056098938, 0.6081564426422119, 0.429913192987442, 0.6191996932029724, -6.141751039289241e-41, 0.5802600383758545, 0.7325395345687866, 0.6130412817001343, 0.6763969659805298, 0.4041135311126709, 0.4068361222743988, 0.6913177967071533, 0.9910071492195129, 0.7243950963020325, 0.9463894367218018, 0.47000399231910706, 0.5380452275276184]
[2025-05-28 07:47:38,650]: Mean: 0.47436136
[2025-05-28 07:47:38,650]: Min: -0.00000000
[2025-05-28 07:47:38,650]: Max: 1.06819844
[2025-05-28 07:47:38,651]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-28 07:47:38,670]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09746044129133224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09746044129133224, 0.0, 0.0, -0.09746044129133224, 0.0]
[2025-05-28 07:47:38,670]: Mean: -0.00000711
[2025-05-28 07:47:38,670]: Min: -0.29238132
[2025-05-28 07:47:38,670]: Max: 0.38984177
[2025-05-28 07:47:38,670]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-28 07:47:38,671]: Sample Values (25 elements): [5.234830673178219e-41, -5.93898315150144e-41, 6.231434141006029e-41, -5.291022741597644e-41, -5.624812035799816e-41, 0.4394550323486328, -5.506542445410801e-41, -6.036233264925582e-41, 6.044921315404396e-41, 6.29757542852216e-41, 5.062470962066267e-41, 0.3689751625061035, -5.987608208213511e-41, -4.983157468985482e-41, 6.20382856125883e-41, -6.136706364817671e-41, 0.7006068229675293, -5.330959747830902e-41, -5.799694084147553e-41, -5.132535885282507e-41, 5.674558131283347e-41, -5.976958339884642e-41, 0.7783645987510681, -5.070878752852216e-41, 6.189395187076285e-41]
[2025-05-28 07:47:38,671]: Mean: 0.05056453
[2025-05-28 07:47:38,671]: Min: -0.00000000
[2025-05-28 07:47:38,671]: Max: 0.95446086
[2025-05-28 07:47:38,672]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 07:47:38,719]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,719]: Mean: -0.00000658
[2025-05-28 07:47:38,720]: Min: -0.20348051
[2025-05-28 07:47:38,720]: Max: 0.27130735
[2025-05-28 07:47:38,720]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-28 07:47:38,720]: Sample Values (25 elements): [0.6980903744697571, 6.164452074411303e-41, 0.49975329637527466, 5.560772695980172e-41, 0.3633245527744293, -5.813987328483666e-41, 5.123287315417964e-41, 0.08390259742736816, 6.052208067418885e-41, 5.830102260823401e-41, 0.2767035663127899, 0.5322349071502686, 0.8935631513595581, 5.183262889691066e-41, -5.235951711949679e-41, 6.227230245613055e-41, 0.5292968153953552, 0.3207171559333801, 5.465764660098949e-41, 5.996156128845892e-41, 0.43959230184555054, 5.711272151048657e-41, 0.22133874893188477, 0.41134652495384216, 0.8385515809059143]
[2025-05-28 07:47:38,720]: Mean: 0.24828222
[2025-05-28 07:47:38,721]: Min: -0.00000000
[2025-05-28 07:47:38,721]: Max: 0.89356315
[2025-05-28 07:47:38,722]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-28 07:47:38,723]: Sample Values (25 elements): [0.05637926608324051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05637926608324051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05637926608324051, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,723]: Mean: -0.00009334
[2025-05-28 07:47:38,723]: Min: -0.16913781
[2025-05-28 07:47:38,724]: Max: 0.22551706
[2025-05-28 07:47:38,724]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-28 07:47:38,724]: Sample Values (25 elements): [0.40234115719795227, 0.01875022053718567, 0.3502180278301239, 0.31213051080703735, 0.3865993022918701, 5.477955956738575e-41, 0.35121309757232666, 0.3314081132411957, 0.3493025600910187, 5.783579151807817e-41, 0.4123954772949219, 5.879007577228338e-41, 0.42419883608818054, -6.146515454067945e-41, -5.576607122748101e-07, 0.26006370782852173, 6.181127526136768e-41, -5.560352306440874e-41, 0.1277908980846405, -5.490427513071066e-41, 0.29973411560058594, 1.727251798349077e-15, 5.934919385954898e-41, 0.34376275539398193, 0.39313650131225586]
[2025-05-28 07:47:38,724]: Mean: 0.16324452
[2025-05-28 07:47:38,724]: Min: -0.00056654
[2025-05-28 07:47:38,725]: Max: 0.59936017
[2025-05-28 07:47:38,726]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 07:47:38,772]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03747636824846268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03747636824846268]
[2025-05-28 07:47:38,773]: Mean: -0.00000048
[2025-05-28 07:47:38,774]: Min: -0.14990547
[2025-05-28 07:47:38,774]: Max: 0.11242910
[2025-05-28 07:47:38,774]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-28 07:47:38,774]: Sample Values (25 elements): [-5.520275170361184e-41, -6.077011050237434e-41, -5.365711949746157e-41, 5.065693948534214e-41, 5.220817688534971e-41, -5.763540583767973e-41, 3.901878820045113e-09, -5.006419023493274e-41, -5.172613021362197e-41, -5.758075519757106e-41, -6.298276077754323e-41, 5.271824952636394e-41, 6.060475728358401e-41, 6.265625823535555e-41, -5.656621510939989e-41, -5.975276781727452e-41, 5.965187432784314e-41, 5.809643303244259e-41, 5.598327494824077e-41, -5.558951007976549e-41, 5.066955117152106e-41, 5.496172836774798e-41, 5.063451870991294e-41, 5.456656220080838e-41, -5.485663098292361e-41]
[2025-05-28 07:47:38,775]: Mean: 0.01050062
[2025-05-28 07:47:38,775]: Min: -0.00000000
[2025-05-28 07:47:38,775]: Max: 0.88443536
[2025-05-28 07:47:38,776]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 07:47:38,819]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 07:47:38,820]: Mean: -0.00000342
[2025-05-28 07:47:38,820]: Min: -0.11049154
[2025-05-28 07:47:38,820]: Max: 0.14732206
[2025-05-28 07:47:38,820]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-28 07:47:38,822]: Sample Values (25 elements): [-4.917016181469351e-41, -5.377062467307188e-41, 0.16988039016723633, 0.1742297261953354, 0.00019617268117144704, 0.05339273810386658, 0.0843963474035263, 6.048424561565208e-41, 0.06526906043291092, 5.044254082030044e-41, 0.09575993567705154, -5.083070049491841e-41, -5.759757077914296e-41, 0.20704330503940582, 0.2423550933599472, 0.1907781958580017, 4.909869559301294e-41, -5.896523808032398e-41, 0.06353143602609634, 5.474172450884898e-41, 4.917997090394378e-41, 5.464783751173922e-41, 0.14287692308425903, -5.575766589548447e-41, -5.415177785536823e-41]
[2025-05-28 07:47:38,822]: Mean: 0.06655078
[2025-05-28 07:47:38,822]: Min: -0.02828848
[2025-05-28 07:47:38,823]: Max: 0.43870512
[2025-05-28 07:47:38,823]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-28 07:47:38,834]: Sample Values (25 elements): [5.552645164887088e-41, 0.004620962776243687, 5.486644007217389e-41, 0.03024117276072502, -4.908047871297672e-41, -5.017629411207872e-41, -0.23147298395633698, 4.953449941541796e-41, -6.087660918566303e-41, 0.03823155537247658, 5.242677944578438e-41, 5.450070117298511e-41, 0.15900886058807373, -5.758636039142836e-41, -0.08428651839494705, 0.06652922928333282, -0.06660038232803345, 5.258232357532444e-41, 0.20014052093029022, -0.029438301920890808, 6.084718191791221e-41, 0.1482819765806198, -5.044113952183612e-41, 5.474032321038465e-41, -4.240829198920437e-08]
[2025-05-28 07:47:38,834]: Mean: -0.00000818
[2025-05-28 07:47:38,834]: Min: -0.50507146
[2025-05-28 07:47:38,834]: Max: 0.50067955
[2025-05-28 07:47:38,834]: 


QAT of ResNet18 with hardtanh down to 2 bits...
[2025-05-28 07:47:39,204]: [ResNet18_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-28 07:47:39,279]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-28 07:49:27,952]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 0.7023 Train Acc: 0.7567 Eval Loss: 0.7040 Eval Acc: 0.7621 (LR: 0.00100000)
[2025-05-28 07:51:17,526]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.5965 Train Acc: 0.7922 Eval Loss: 0.8854 Eval Acc: 0.7040 (LR: 0.00100000)
[2025-05-28 07:53:09,171]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.5785 Train Acc: 0.7990 Eval Loss: 0.9254 Eval Acc: 0.7094 (LR: 0.00100000)
[2025-05-28 07:54:57,175]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.5624 Train Acc: 0.8051 Eval Loss: 0.9653 Eval Acc: 0.7018 (LR: 0.00100000)
[2025-05-28 07:56:44,821]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.5683 Train Acc: 0.8013 Eval Loss: 0.7903 Eval Acc: 0.7502 (LR: 0.00100000)
[2025-05-28 07:58:30,226]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.5592 Train Acc: 0.8060 Eval Loss: 0.5944 Eval Acc: 0.7970 (LR: 0.00100000)
[2025-05-28 08:00:17,543]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.5621 Train Acc: 0.8048 Eval Loss: 0.9866 Eval Acc: 0.6899 (LR: 0.00100000)
[2025-05-28 08:02:08,554]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.5600 Train Acc: 0.8062 Eval Loss: 0.7252 Eval Acc: 0.7644 (LR: 0.00100000)
[2025-05-28 08:03:54,882]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.5589 Train Acc: 0.8052 Eval Loss: 0.6548 Eval Acc: 0.7802 (LR: 0.00100000)
[2025-05-28 08:05:42,902]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.5490 Train Acc: 0.8092 Eval Loss: 0.7521 Eval Acc: 0.7493 (LR: 0.00100000)
[2025-05-28 08:07:28,141]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.5464 Train Acc: 0.8113 Eval Loss: 0.8722 Eval Acc: 0.7287 (LR: 0.00100000)
[2025-05-28 08:09:12,444]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.5509 Train Acc: 0.8073 Eval Loss: 0.7347 Eval Acc: 0.7628 (LR: 0.00010000)
[2025-05-28 08:10:56,892]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.4471 Train Acc: 0.8446 Eval Loss: 0.4517 Eval Acc: 0.8455 (LR: 0.00010000)
[2025-05-28 08:12:41,227]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.4134 Train Acc: 0.8572 Eval Loss: 0.4683 Eval Acc: 0.8432 (LR: 0.00010000)
[2025-05-28 08:14:26,680]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.4091 Train Acc: 0.8571 Eval Loss: 0.4410 Eval Acc: 0.8538 (LR: 0.00010000)
[2025-05-28 08:16:15,455]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.4060 Train Acc: 0.8596 Eval Loss: 0.4580 Eval Acc: 0.8469 (LR: 0.00010000)
[2025-05-28 08:18:01,026]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.3989 Train Acc: 0.8619 Eval Loss: 0.4799 Eval Acc: 0.8410 (LR: 0.00010000)
[2025-05-28 08:19:46,584]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.3999 Train Acc: 0.8622 Eval Loss: 0.5026 Eval Acc: 0.8345 (LR: 0.00010000)
[2025-05-28 08:21:32,183]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.3931 Train Acc: 0.8638 Eval Loss: 0.4649 Eval Acc: 0.8442 (LR: 0.00010000)
[2025-05-28 08:23:18,285]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.3949 Train Acc: 0.8612 Eval Loss: 0.4253 Eval Acc: 0.8598 (LR: 0.00010000)
[2025-05-28 08:25:03,709]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.3905 Train Acc: 0.8645 Eval Loss: 0.5078 Eval Acc: 0.8344 (LR: 0.00010000)
[2025-05-28 08:26:49,298]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.3906 Train Acc: 0.8643 Eval Loss: 0.4725 Eval Acc: 0.8414 (LR: 0.00010000)
[2025-05-28 08:28:34,672]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.3859 Train Acc: 0.8663 Eval Loss: 0.4771 Eval Acc: 0.8433 (LR: 0.00010000)
[2025-05-28 08:30:22,102]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.3848 Train Acc: 0.8659 Eval Loss: 0.4634 Eval Acc: 0.8480 (LR: 0.00010000)
[2025-05-28 08:32:23,164]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.3851 Train Acc: 0.8655 Eval Loss: 0.4385 Eval Acc: 0.8548 (LR: 0.00010000)
[2025-05-28 08:34:24,039]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.3893 Train Acc: 0.8633 Eval Loss: 0.4274 Eval Acc: 0.8569 (LR: 0.00001000)
[2025-05-28 08:36:24,480]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.3571 Train Acc: 0.8756 Eval Loss: 0.3919 Eval Acc: 0.8662 (LR: 0.00001000)
[2025-05-28 08:38:25,164]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.3488 Train Acc: 0.8779 Eval Loss: 0.3944 Eval Acc: 0.8674 (LR: 0.00001000)
[2025-05-28 08:40:26,017]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.3467 Train Acc: 0.8791 Eval Loss: 0.3842 Eval Acc: 0.8687 (LR: 0.00001000)
[2025-05-28 08:42:26,694]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.3457 Train Acc: 0.8800 Eval Loss: 0.3885 Eval Acc: 0.8727 (LR: 0.00001000)
[2025-05-28 08:44:27,361]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.3428 Train Acc: 0.8808 Eval Loss: 0.3932 Eval Acc: 0.8704 (LR: 0.00001000)
[2025-05-28 08:46:27,822]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.3449 Train Acc: 0.8792 Eval Loss: 0.3889 Eval Acc: 0.8704 (LR: 0.00001000)
[2025-05-28 08:48:28,327]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.3481 Train Acc: 0.8785 Eval Loss: 0.3894 Eval Acc: 0.8708 (LR: 0.00001000)
[2025-05-28 08:50:28,770]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.3429 Train Acc: 0.8811 Eval Loss: 0.3876 Eval Acc: 0.8685 (LR: 0.00001000)
[2025-05-28 08:52:29,250]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.3437 Train Acc: 0.8829 Eval Loss: 0.3922 Eval Acc: 0.8685 (LR: 0.00000100)
[2025-05-28 08:54:29,745]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.3365 Train Acc: 0.8819 Eval Loss: 0.3781 Eval Acc: 0.8741 (LR: 0.00000100)
[2025-05-28 08:56:30,421]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.3364 Train Acc: 0.8817 Eval Loss: 0.3840 Eval Acc: 0.8699 (LR: 0.00000100)
[2025-05-28 08:58:30,894]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.3327 Train Acc: 0.8851 Eval Loss: 0.3860 Eval Acc: 0.8692 (LR: 0.00000100)
[2025-05-28 09:00:31,526]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.3330 Train Acc: 0.8830 Eval Loss: 0.3908 Eval Acc: 0.8682 (LR: 0.00000100)
[2025-05-28 09:02:33,139]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.3324 Train Acc: 0.8854 Eval Loss: 0.3796 Eval Acc: 0.8736 (LR: 0.00000100)
[2025-05-28 09:04:33,865]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.3303 Train Acc: 0.8848 Eval Loss: 0.3800 Eval Acc: 0.8721 (LR: 0.00000100)
[2025-05-28 09:06:34,548]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.3329 Train Acc: 0.8829 Eval Loss: 0.3854 Eval Acc: 0.8706 (LR: 0.00000010)
[2025-05-28 09:08:35,407]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.3298 Train Acc: 0.8851 Eval Loss: 0.3799 Eval Acc: 0.8716 (LR: 0.00000010)
[2025-05-28 09:10:36,267]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.3300 Train Acc: 0.8861 Eval Loss: 0.3760 Eval Acc: 0.8723 (LR: 0.00000010)
[2025-05-28 09:12:37,110]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.3293 Train Acc: 0.8869 Eval Loss: 0.3779 Eval Acc: 0.8717 (LR: 0.00000010)
[2025-05-28 09:14:37,774]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.3252 Train Acc: 0.8860 Eval Loss: 0.3728 Eval Acc: 0.8765 (LR: 0.00000010)
[2025-05-28 09:16:38,695]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.3272 Train Acc: 0.8871 Eval Loss: 0.3869 Eval Acc: 0.8708 (LR: 0.00000010)
[2025-05-28 09:18:39,724]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.3256 Train Acc: 0.8866 Eval Loss: 0.3848 Eval Acc: 0.8725 (LR: 0.00000010)
[2025-05-28 09:20:40,366]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.3257 Train Acc: 0.8870 Eval Loss: 0.3848 Eval Acc: 0.8734 (LR: 0.00000010)
[2025-05-28 09:22:41,067]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.3286 Train Acc: 0.8872 Eval Loss: 0.3868 Eval Acc: 0.8705 (LR: 0.00000010)
[2025-05-28 09:24:41,857]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.3302 Train Acc: 0.8852 Eval Loss: 0.3806 Eval Acc: 0.8742 (LR: 0.00000010)
[2025-05-28 09:26:42,674]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.3296 Train Acc: 0.8841 Eval Loss: 0.3708 Eval Acc: 0.8720 (LR: 0.00000010)
[2025-05-28 09:28:43,355]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.3251 Train Acc: 0.8863 Eval Loss: 0.3774 Eval Acc: 0.8723 (LR: 0.00000010)
[2025-05-28 09:30:44,021]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.3249 Train Acc: 0.8866 Eval Loss: 0.3740 Eval Acc: 0.8719 (LR: 0.00000010)
[2025-05-28 09:32:44,809]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.3261 Train Acc: 0.8869 Eval Loss: 0.3868 Eval Acc: 0.8733 (LR: 0.00000010)
[2025-05-28 09:34:45,458]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.3297 Train Acc: 0.8859 Eval Loss: 0.3736 Eval Acc: 0.8762 (LR: 0.00000010)
[2025-05-28 09:36:45,909]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.3293 Train Acc: 0.8861 Eval Loss: 0.3824 Eval Acc: 0.8708 (LR: 0.00000010)
[2025-05-28 09:38:46,553]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.3283 Train Acc: 0.8846 Eval Loss: 0.3747 Eval Acc: 0.8731 (LR: 0.00000010)
[2025-05-28 09:40:47,456]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.3286 Train Acc: 0.8844 Eval Loss: 0.3758 Eval Acc: 0.8764 (LR: 0.00000010)
[2025-05-28 09:42:49,177]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.3273 Train Acc: 0.8866 Eval Loss: 0.3824 Eval Acc: 0.8693 (LR: 0.00000010)
[2025-05-28 09:42:49,177]: [ResNet18_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8765
[2025-05-28 09:42:49,267]: 


Quantization of model down to 2 bits finished
[2025-05-28 09:42:49,267]: Model Architecture:
[2025-05-28 09:42:49,343]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2275], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.34320271015167236, max_val=0.3393867611885071)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3136], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4369867444038391, max_val=0.5037182569503784)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2829], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4569806456565857, max_val=0.39177945256233215)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2766], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36045610904693604, max_val=0.4692118465900421)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2762], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4150557518005371, max_val=0.41353583335876465)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2068], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32812151312828064, max_val=0.2923020124435425)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3110], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46941202878952026, max_val=0.46367496252059937)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1844], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28491514921188354, max_val=0.26842331886291504)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2093], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780129909515381, max_val=0.3498813509941101)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2134], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3447765111923218, max_val=0.2954750657081604)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2286], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3456273078918457, max_val=0.34005236625671387)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2350], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35437244176864624, max_val=0.35051286220550537)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2331], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3417888283729553, max_val=0.3576042056083679)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2345], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32786381244659424, max_val=0.3755451440811157)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2082], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.29956281185150146, max_val=0.3251591920852661)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1581], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2453591227531433, max_val=0.22891271114349365)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1643], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24276725947856903, max_val=0.250000536441803)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1299], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2052176296710968, max_val=0.18452194333076477)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1136], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17282436788082123, max_val=0.16797205805778503)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-28 09:42:49,343]: 
Model Weights:
[2025-05-28 09:42:49,343]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-28 09:42:49,343]: Sample Values (25 elements): [0.19315025210380554, 0.11552823334932327, -0.14690326154232025, 0.32765933871269226, 0.1047569140791893, 0.13571299612522125, 0.19246742129325867, -0.034584831446409225, 0.06201865151524544, -0.150435671210289, 0.07974467426538467, 0.021077876910567284, 0.07464288175106049, -0.046968135982751846, -0.2001332938671112, 0.24259327352046967, -0.02221561223268509, 0.1952015459537506, 0.10113324970006943, -0.026314130052924156, -0.08434145152568817, 0.20144318044185638, 0.1331997811794281, -0.035265833139419556, -0.04526117816567421]
[2025-05-28 09:42:49,343]: Mean: 0.00097167
[2025-05-28 09:42:49,344]: Min: -0.39814466
[2025-05-28 09:42:49,344]: Max: 0.41885892
[2025-05-28 09:42:49,344]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-28 09:42:49,344]: Sample Values (25 elements): [0.7738675475120544, 0.6415092349052429, 0.879527747631073, 0.8818599581718445, 1.4265379905700684, 0.4899277687072754, 1.1781079769134521, 0.7537177801132202, 0.8641365170478821, 0.6931663155555725, 0.6862491369247437, 0.6438368558883667, 0.5894482135772705, 0.8000963926315308, 0.8128919005393982, 0.053948599845170975, 1.1331366300582886, 0.23871061205863953, 1.0208394527435303, 0.7071752548217773, 0.7985022664070129, 0.8301647305488586, 0.8373886346817017, 0.5420330762863159, 1.1205123662948608]
[2025-05-28 09:42:49,344]: Mean: 0.83584738
[2025-05-28 09:42:49,344]: Min: 0.05394860
[2025-05-28 09:42:49,345]: Max: 1.42653799
[2025-05-28 09:42:49,346]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 09:42:49,346]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22752982378005981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,346]: Mean: -0.00008641
[2025-05-28 09:42:49,346]: Min: -0.45505965
[2025-05-28 09:42:49,347]: Max: 0.22752982
[2025-05-28 09:42:49,347]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-28 09:42:49,347]: Sample Values (25 elements): [0.6225382089614868, 0.09174739569425583, 0.410506933927536, 0.6098417043685913, 0.650538444519043, 0.6404901146888733, 0.7177920341491699, -6.038335212622069e-41, 0.9388648271560669, 0.7981679439544678, 0.8300635814666748, 0.6408818364143372, 0.3299635648727417, 0.35885125398635864, 0.79241544008255, 0.7648330330848694, 0.6556183695793152, 1.0861518383026123, 0.3356032073497772, 0.7814871668815613, 0.5240007042884827, 0.9608882665634155, 0.5850790739059448, 0.6224105358123779, -4.951768383384606e-41]
[2025-05-28 09:42:49,347]: Mean: 0.64102584
[2025-05-28 09:42:49,347]: Min: -0.00000000
[2025-05-28 09:42:49,347]: Max: 1.09288025
[2025-05-28 09:42:49,348]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 09:42:49,349]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,349]: Mean: 0.00028921
[2025-05-28 09:42:49,349]: Min: -0.31356832
[2025-05-28 09:42:49,349]: Max: 0.62713665
[2025-05-28 09:42:49,349]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-28 09:42:49,350]: Sample Values (25 elements): [0.8060102462768555, 0.7789250612258911, 0.63496333360672, 0.7737603783607483, 0.8087859749794006, 0.8107431530952454, 0.8234456181526184, 0.4724402129650116, 0.7564471960067749, 0.7250633835792542, 0.6839600801467896, 0.7631109952926636, 0.8591216206550598, 0.921604335308075, 0.3908284902572632, 0.7832987308502197, 0.6717574000358582, 0.9775352478027344, 0.728179931640625, 0.6782460808753967, 0.5605543255805969, 0.9459596872329712, 0.9040527939796448, 0.9287840127944946, 0.5652368664741516]
[2025-05-28 09:42:49,350]: Mean: 0.75597376
[2025-05-28 09:42:49,350]: Min: 0.39082849
[2025-05-28 09:42:49,350]: Max: 1.14315581
[2025-05-28 09:42:49,351]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 09:42:49,352]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2829200029373169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,352]: Mean: -0.00009210
[2025-05-28 09:42:49,352]: Min: -0.56584001
[2025-05-28 09:42:49,352]: Max: 0.28292000
[2025-05-28 09:42:49,352]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-28 09:42:49,352]: Sample Values (25 elements): [0.6051819920539856, 1.2088830471038818, 0.31505337357521057, 0.7842584252357483, 0.10469937324523926, 1.0535153150558472, -4.936634359969898e-41, 0.737297773361206, 0.04974053427577019, 0.634708821773529, 0.0454251766204834, 0.7372720241546631, 0.32346558570861816, 0.633575439453125, 0.8928267359733582, 0.6796631813049316, 0.6585118770599365, 0.6283898949623108, 0.8893254399299622, 0.83966064453125, 1.2666093111038208, 0.87157142162323, 0.5498129725456238, 1.3440687656402588, 0.7458606362342834]
[2025-05-28 09:42:49,352]: Mean: 0.68269414
[2025-05-28 09:42:49,353]: Min: -0.00000000
[2025-05-28 09:42:49,353]: Max: 1.34406877
[2025-05-28 09:42:49,354]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 09:42:49,354]: Sample Values (25 elements): [0.0, 0.27655598521232605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27655598521232605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.27655598521232605, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,354]: Mean: -0.00009753
[2025-05-28 09:42:49,355]: Min: -0.27655599
[2025-05-28 09:42:49,355]: Max: 0.55311197
[2025-05-28 09:42:49,355]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-28 09:42:49,355]: Sample Values (25 elements): [0.9458106756210327, 0.843816339969635, 0.43031394481658936, 0.7160031795501709, 1.1411534547805786, 0.470287561416626, 0.3093291223049164, 0.8357511758804321, 0.7250801920890808, 0.42805033922195435, 0.8558439016342163, 0.773634135723114, 0.526547908782959, 0.8104841113090515, 0.9196105599403381, 0.6364923715591431, 0.8294427990913391, 1.2758057117462158, 0.6628808379173279, 0.573320209980011, 0.8500916361808777, 0.6954671144485474, 0.8822724223136902, 0.37060993909835815, 1.200509786605835]
[2025-05-28 09:42:49,355]: Mean: 0.77408469
[2025-05-28 09:42:49,355]: Min: 0.30932912
[2025-05-28 09:42:49,355]: Max: 1.37600112
[2025-05-28 09:42:49,356]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-28 09:42:49,357]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,357]: Mean: 0.00083165
[2025-05-28 09:42:49,358]: Min: -0.55239439
[2025-05-28 09:42:49,358]: Max: 0.27619720
[2025-05-28 09:42:49,358]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-28 09:42:49,358]: Sample Values (25 elements): [5.105070435381741e-41, 0.2716974914073944, 0.5358312129974365, -5.073541219934433e-41, 0.7182418704032898, 0.5209905505180359, 6.235357776706139e-41, 5.510185821418046e-41, 5.49799452477842e-41, 0.7765607237815857, 0.15561608970165253, 4.92262137532665e-41, 0.7464953064918518, 0.7612658739089966, 0.7118192911148071, 0.2775397002696991, -5.18914834324123e-41, 0.8546342849731445, 0.5788843035697937, 0.8991091847419739, 0.5592623353004456, 0.774036705493927, 0.6710384488105774, 3.4146550031977085e-09, 0.6080370545387268]
[2025-05-28 09:42:49,358]: Mean: 0.40697712
[2025-05-28 09:42:49,358]: Min: -0.00000000
[2025-05-28 09:42:49,359]: Max: 1.04581404
[2025-05-28 09:42:49,359]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 09:42:49,361]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20680785179138184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20680785179138184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,361]: Mean: 0.00017391
[2025-05-28 09:42:49,361]: Min: -0.41361570
[2025-05-28 09:42:49,361]: Max: 0.20680785
[2025-05-28 09:42:49,362]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-28 09:42:49,362]: Sample Values (25 elements): [0.6507073640823364, 0.7193461060523987, 0.40836361050605774, 0.07434602826833725, 0.38019031286239624, 0.25567564368247986, 0.8171784281730652, 0.8330379128456116, 0.6075495481491089, 0.5461761355400085, 0.6406948566436768, 0.5393123030662537, 0.6509108543395996, 0.4819932281970978, 0.6164173483848572, 0.5830366015434265, 0.6577915549278259, 0.7201338410377502, 0.5664948225021362, 0.6184957027435303, 0.9148518443107605, 0.8415484428405762, 0.5725569128990173, 0.779511034488678, 0.651837170124054]
[2025-05-28 09:42:49,362]: Mean: 0.58195412
[2025-05-28 09:42:49,362]: Min: -0.00000000
[2025-05-28 09:42:49,362]: Max: 0.95222563
[2025-05-28 09:42:49,363]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-28 09:42:49,364]: Sample Values (25 elements): [0.31102901697158813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31102901697158813, 0.31102901697158813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31102901697158813]
[2025-05-28 09:42:49,364]: Mean: -0.00091122
[2025-05-28 09:42:49,364]: Min: -0.62205803
[2025-05-28 09:42:49,364]: Max: 0.31102902
[2025-05-28 09:42:49,364]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-28 09:42:49,364]: Sample Values (25 elements): [0.21126142144203186, 0.3395765423774719, 0.2710643410682678, 0.4761098325252533, 0.2082260400056839, 0.43730610609054565, 0.495711088180542, 0.4853999614715576, 0.22126856446266174, 0.4633611738681793, 0.3278927803039551, 0.4837448000907898, 0.2615935802459717, 0.4493550658226013, 0.3629302680492401, 0.4075774550437927, 0.2963217794895172, 0.3680365979671478, 5.000953959482407e-41, 0.4303892254829407, 0.30867666006088257, 0.4462743401527405, 0.5079924464225769, 0.31804561614990234, 0.2662230432033539]
[2025-05-28 09:42:49,365]: Mean: 0.31497151
[2025-05-28 09:42:49,365]: Min: 0.00000000
[2025-05-28 09:42:49,365]: Max: 0.64255774
[2025-05-28 09:42:49,366]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 09:42:49,367]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18444615602493286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,368]: Mean: -0.00018763
[2025-05-28 09:42:49,368]: Min: -0.36889231
[2025-05-28 09:42:49,368]: Max: 0.18444616
[2025-05-28 09:42:49,368]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-28 09:42:49,368]: Sample Values (25 elements): [0.8883066177368164, 0.7731746435165405, -5.55334581411925e-41, 0.681114137172699, 0.6437734961509705, -5.024495773683064e-41, 0.8768134117126465, 0.7835512757301331, 5.151033025011595e-41, 0.006657043006271124, -5.651436706621987e-41, 0.785609781742096, -5.109134200928283e-41, 0.640953779220581, 5.121886016953639e-41, 5.651716966314852e-41, 4.979934482517535e-41, -5.793948760443821e-41, 5.654519563243502e-41, 5.375941428535728e-41, 1.1020101308822632, 0.5111859440803528, 1.0749106407165527, -5.251085735364387e-41, 6.184911031990445e-41]
[2025-05-28 09:42:49,368]: Mean: 0.35414159
[2025-05-28 09:42:49,369]: Min: -0.00000000
[2025-05-28 09:42:49,369]: Max: 1.10201013
[2025-05-28 09:42:49,370]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-28 09:42:49,371]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20929811894893646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,371]: Mean: -0.00016891
[2025-05-28 09:42:49,372]: Min: -0.20929812
[2025-05-28 09:42:49,372]: Max: 0.41859624
[2025-05-28 09:42:49,372]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-28 09:42:49,372]: Sample Values (25 elements): [0.7856897711753845, 0.5074864625930786, 0.38496848940849304, 0.6235687136650085, -4.963399160638502e-41, 0.5880321860313416, 0.402091920375824, 0.6830610036849976, 0.4563182294368744, 0.35488852858543396, 0.6424848437309265, 0.7712689638137817, 0.6518324017524719, 0.7193970680236816, 0.5776118040084839, 0.7946887612342834, 0.5225505828857422, 0.4389762580394745, 0.30965229868888855, 0.15199661254882812, 0.7715861201286316, 0.8478666543960571, 0.754470944404602, 0.6125380992889404, 0.10077711939811707]
[2025-05-28 09:42:49,372]: Mean: 0.54603612
[2025-05-28 09:42:49,372]: Min: -0.00000000
[2025-05-28 09:42:49,372]: Max: 0.94113708
[2025-05-28 09:42:49,373]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-28 09:42:49,376]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.2134172022342682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2134172022342682, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,377]: Mean: -0.00004921
[2025-05-28 09:42:49,377]: Min: -0.42683440
[2025-05-28 09:42:49,377]: Max: 0.21341720
[2025-05-28 09:42:49,377]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-28 09:42:49,377]: Sample Values (25 elements): [0.7163099646568298, -5.706787995962818e-41, 6.213217260969806e-41, -5.45679634992727e-41, -6.298976726986485e-41, -5.314844815491166e-41, -5.073401090088e-41, 6.30262010299373e-41, -5.387011686403894e-41, -6.063978974519213e-41, 5.490147253378201e-41, 0.652805507183075, 5.638965150289496e-41, 5.526020494064916e-41, 0.41062575578689575, -6.156604803011084e-41, -5.05812693682686e-41, 5.549842567958438e-41, 5.53639010270092e-41, 5.250385086132225e-41, 0.6174902319908142, -6.286925560193292e-41, 5.521676468825509e-41, 1.0076175928115845, 5.658443198943611e-41]
[2025-05-28 09:42:49,377]: Mean: 0.16531202
[2025-05-28 09:42:49,378]: Min: -0.00000000
[2025-05-28 09:42:49,378]: Max: 1.07090735
[2025-05-28 09:42:49,379]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 09:42:49,385]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22855989634990692, 0.0, 0.0]
[2025-05-28 09:42:49,385]: Mean: 0.00009765
[2025-05-28 09:42:49,385]: Min: -0.45711979
[2025-05-28 09:42:49,385]: Max: 0.22855990
[2025-05-28 09:42:49,385]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-28 09:42:49,386]: Sample Values (25 elements): [0.5335788726806641, 0.43752989172935486, 0.5901469588279724, 0.26456114649772644, 0.5261427760124207, 5.53302698638654e-41, 0.4842908978462219, 5.036406810629825e-41, 0.34226155281066895, 0.08117277175188065, 0.4650479257106781, 0.08702585846185684, 0.7217879891395569, 0.5237792730331421, 0.571436882019043, -5.82155434019102e-41, 0.27092739939689636, 5.205403405427398e-41, 0.3163817524909973, 0.05867619067430496, 0.5731905102729797, 0.4257776141166687, 0.5916736721992493, 5.105490824921039e-41, 6.085278711176951e-41]
[2025-05-28 09:42:49,386]: Mean: 0.40108672
[2025-05-28 09:42:49,386]: Min: -0.00000000
[2025-05-28 09:42:49,386]: Max: 0.85903835
[2025-05-28 09:42:49,387]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-28 09:42:49,388]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.23496177792549133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23496177792549133, 0.0, 0.0, 0.0, 0.0, -0.23496177792549133, 0.0]
[2025-05-28 09:42:49,388]: Mean: 0.00000717
[2025-05-28 09:42:49,388]: Min: -0.46992356
[2025-05-28 09:42:49,388]: Max: 0.23496178
[2025-05-28 09:42:49,388]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-28 09:42:49,388]: Sample Values (25 elements): [0.2300781011581421, 0.5688052177429199, 0.11317696422338486, 0.6459055542945862, 0.41590645909309387, 0.3732896149158478, 0.3661438226699829, 0.5751244425773621, 0.1391766518354416, 0.13975651562213898, 0.24977748095989227, 0.3225015699863434, 0.3856765627861023, 0.4908543825149536, 0.6316930055618286, 0.06317749619483948, 0.40952786803245544, 0.18187053501605988, 0.3077217638492584, 0.42829620838165283, 5.664328652493776e-41, 0.4200930595397949, 5.974295872802425e-41, 0.4167840778827667, 5.702163711030546e-41]
[2025-05-28 09:42:49,389]: Mean: 0.28260091
[2025-05-28 09:42:49,389]: Min: -0.00000000
[2025-05-28 09:42:49,389]: Max: 0.64590555
[2025-05-28 09:42:49,390]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 09:42:49,396]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,396]: Mean: -0.00002727
[2025-05-28 09:42:49,396]: Min: -0.23313102
[2025-05-28 09:42:49,397]: Max: 0.46626204
[2025-05-28 09:42:49,397]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-28 09:42:49,397]: Sample Values (25 elements): [0.9149028658866882, 5.072560311009405e-41, 5.789884994897279e-41, 5.118522900639259e-41, -5.618646322556787e-41, 5.421483628626285e-41, -5.024495773683064e-41, -5.315405334876896e-41, 5.204842886041668e-41, -5.880408875692662e-41, -5.005858504107544e-41, -5.850000699016814e-41, -5.773770062557544e-41, -6.116807926624259e-41, 5.846777712548867e-41, 5.825337846044697e-41, -5.63237904750717e-41, 6.199764795712288e-41, -5.435076223730235e-41, 5.407470643983037e-41, 5.370336234678429e-41, 5.079566803331029e-41, -4.949386175995254e-41, 5.959442109080582e-41, 0.17795655131340027]
[2025-05-28 09:42:49,397]: Mean: 0.11853279
[2025-05-28 09:42:49,397]: Min: -0.00000000
[2025-05-28 09:42:49,397]: Max: 1.02714169
[2025-05-28 09:42:49,398]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-28 09:42:49,407]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,407]: Mean: 0.00003538
[2025-05-28 09:42:49,407]: Min: -0.23446965
[2025-05-28 09:42:49,407]: Max: 0.46893930
[2025-05-28 09:42:49,407]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-28 09:42:49,407]: Sample Values (25 elements): [0.6721100211143494, 0.5433713793754578, 0.6694760322570801, 0.40108799934387207, 0.47588050365448, 0.6391551494598389, 0.5609599351882935, 0.5607242584228516, 0.21255072951316833, 0.5364824533462524, 0.7536911964416504, 0.30114665627479553, 0.1840958446264267, 0.7071024775505066, 4.993667207467918e-41, 0.8908868432044983, 0.21015068888664246, 0.40539923310279846, 0.45285049080848694, -6.245166865956412e-41, 0.3493121266365051, 0.21878895163536072, -5.602251130524186e-41, 0.429187148809433, 4.910149818994159e-41]
[2025-05-28 09:42:49,408]: Mean: 0.44678956
[2025-05-28 09:42:49,408]: Min: -0.00000000
[2025-05-28 09:42:49,408]: Max: 1.05354702
[2025-05-28 09:42:49,409]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-28 09:42:49,422]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20824067294597626, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,423]: Mean: -0.00000000
[2025-05-28 09:42:49,423]: Min: -0.20824067
[2025-05-28 09:42:49,423]: Max: 0.41648135
[2025-05-28 09:42:49,423]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-28 09:42:49,423]: Sample Values (25 elements): [5.5945439889704e-41, -6.183649863372553e-41, -5.288080014822562e-41, -5.146268610232891e-41, 0.5265491604804993, -6.14903779130373e-41, 0.8192107081413269, 5.806420316776312e-41, 5.047477068497991e-41, 5.255710020296659e-41, -5.562314124290929e-41, -4.953449941541796e-41, 5.885593680010664e-41, 5.167288087197763e-41, 5.373419091299944e-41, 6.158987010400436e-41, -5.49911556354988e-41, -4.911410987612051e-41, 6.261001538603283e-41, -6.000360024238867e-41, -5.152854713015217e-41, 5.747425651428237e-41, 6.095227930273657e-41, 5.148931077315108e-41, 5.766903700082352e-41]
[2025-05-28 09:42:49,424]: Mean: 0.05626196
[2025-05-28 09:42:49,424]: Min: -0.00000000
[2025-05-28 09:42:49,424]: Max: 0.95575792
[2025-05-28 09:42:49,425]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 09:42:49,472]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,472]: Mean: -0.00001320
[2025-05-28 09:42:49,472]: Min: -0.31618124
[2025-05-28 09:42:49,473]: Max: 0.15809062
[2025-05-28 09:42:49,473]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-28 09:42:49,474]: Sample Values (25 elements): [5.031362136158256e-41, 0.3182961046695709, 0.4040490686893463, 5.831083169748429e-41, 0.5830035209655762, 5.761578765917918e-41, 0.45165109634399414, 5.269022355707745e-41, 5.438299210198183e-41, -5.022253696140144e-41, 0.7613754868507385, -4.948265137223794e-41, -4.970125393267261e-41, 6.169076359343575e-41, 0.4507446587085724, 0.533031165599823, 0.283092200756073, 5.03892914786561e-41, 0.5262697339057922, 0.4598085582256317, 0.4522167146205902, 5.646812421689715e-41, 0.8295478820800781, 0.2804616093635559, 0.3039661645889282]
[2025-05-28 09:42:49,474]: Mean: 0.21954739
[2025-05-28 09:42:49,474]: Min: -0.00000000
[2025-05-28 09:42:49,475]: Max: 1.03656268
[2025-05-28 09:42:49,476]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-28 09:42:49,477]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,477]: Mean: -0.00000251
[2025-05-28 09:42:49,477]: Min: -0.16425595
[2025-05-28 09:42:49,478]: Max: 0.32851189
[2025-05-28 09:42:49,478]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-28 09:42:49,478]: Sample Values (25 elements): [0.2964445948600769, -5.631678398275007e-41, -6.037354303697042e-41, 6.202987782180235e-41, 0.5760117769241333, 0.29747387766838074, 5.192791719248475e-41, 5.570721915076878e-41, 0.4404294788837433, 0.42538416385650635, -6.238720893020518e-41, -4.933971892887681e-41, 0.46798673272132874, 0.4260050356388092, 4.910990598072754e-41, 0.22653603553771973, 0.48658066987991333, 0.33204975724220276, -4.936354100277033e-41, 5.472490892727708e-41, 0.35705918073654175, 0.25097888708114624, 0.24602237343788147, -5.51887387189686e-41, 4.916595791930053e-41]
[2025-05-28 09:42:49,478]: Mean: 0.17924151
[2025-05-28 09:42:49,478]: Min: -0.00000000
[2025-05-28 09:42:49,478]: Max: 0.68865532
[2025-05-28 09:42:49,479]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 09:42:49,524]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,525]: Mean: -0.00000127
[2025-05-28 09:42:49,525]: Min: -0.25982639
[2025-05-28 09:42:49,525]: Max: 0.12991320
[2025-05-28 09:42:49,525]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-28 09:42:49,526]: Sample Values (25 elements): [-5.360667275274588e-41, 6.191637264619204e-41, -6.138528052821294e-41, 5.986347039595619e-41, 5.131134586818183e-41, -6.069584168376513e-41, 6.300518155297243e-41, 5.015947853050683e-41, 5.165326269347708e-41, 5.185224707541121e-41, 5.192651589402042e-41, 6.200465444944451e-41, 5.689832284544487e-41, 6.000640283931732e-41, 5.87802666830331e-41, -5.242537814732006e-41, 6.04912521079737e-41, 5.962805225394962e-41, -4.993527077621486e-41, -5.797031617065336e-41, -5.847758621473894e-41, -4.97685162589602e-41, 5.539893348861732e-41, -5.414337006458228e-41, 5.884893030778502e-41]
[2025-05-28 09:42:49,526]: Mean: 0.00988681
[2025-05-28 09:42:49,526]: Min: -0.00000000
[2025-05-28 09:42:49,526]: Max: 1.10125422
[2025-05-28 09:42:49,527]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-28 09:42:49,570]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 09:42:49,570]: Mean: -0.00000125
[2025-05-28 09:42:49,571]: Min: -0.22719762
[2025-05-28 09:42:49,571]: Max: 0.11359881
[2025-05-28 09:42:49,571]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-28 09:42:49,571]: Sample Values (25 elements): [-5.318628321344843e-41, -5.22754392116373e-41, -6.191076745233474e-41, 4.99829149240019e-41, 0.44259217381477356, 6.119330263860044e-41, -5.152294193629487e-41, 0.07830559462308884, 0.3588007986545563, -5.007540062264734e-41, 5.139822637296997e-41, 0.16585221886634827, 0.3516455292701721, -6.085839230562681e-41, -6.205930508955317e-41, 0.35603225231170654, 5.982563533741942e-41, 5.279391964343748e-41, 0.11624641716480255, 5.51551075558248e-41, -5.330118968752307e-41, -4.974189158813803e-41, 0.41991350054740906, 0.302309513092041, 0.24571368098258972]
[2025-05-28 09:42:49,571]: Mean: 0.07479916
[2025-05-28 09:42:49,572]: Min: -0.00000000
[2025-05-28 09:42:49,572]: Max: 0.46987179
[2025-05-28 09:42:49,572]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-28 09:42:49,572]: Sample Values (25 elements): [-5.17275315120863e-41, -4.90566566390832e-41, -0.19902169704437256, -0.1088007315993309, -0.04033076763153076, 5.185645097080418e-41, 0.05452448129653931, 5.378183506078648e-41, 6.191357004926339e-41, 0.1805640310049057, -5.520695559900482e-41, -6.181547915676066e-41, -0.12895730137825012, -0.27840423583984375, -4.912952415922809e-41, 6.14959831068946e-41, -4.909589299608429e-41, 0.027102282270789146, -0.03585068881511688, 5.307277803783812e-41, -5.366973118364049e-41, 0.062115512788295746, -0.033638980239629745, -0.07552413642406464, -5.393737919032653e-41]
[2025-05-28 09:42:49,572]: Mean: 0.00008035
[2025-05-28 09:42:49,572]: Min: -0.47723231
[2025-05-28 09:42:49,573]: Max: 0.46870917
