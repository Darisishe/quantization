[2025-05-14 15:32:43,820]: 
Training ResNet18 with hardtanh
[2025-05-14 15:34:01,725]: [ResNet18_hardtanh] Epoch: 001 Train Loss: 1.8738 Train Acc: 0.3135 Eval Loss: 1.6776 Eval Acc: 0.4024 (LR: 0.001000)
[2025-05-14 15:35:19,631]: [ResNet18_hardtanh] Epoch: 002 Train Loss: 1.6165 Train Acc: 0.4137 Eval Loss: 1.5598 Eval Acc: 0.4359 (LR: 0.001000)
[2025-05-14 15:36:37,533]: [ResNet18_hardtanh] Epoch: 003 Train Loss: 1.4585 Train Acc: 0.4703 Eval Loss: 1.3767 Eval Acc: 0.4998 (LR: 0.001000)
[2025-05-14 15:37:55,402]: [ResNet18_hardtanh] Epoch: 004 Train Loss: 1.3485 Train Acc: 0.5100 Eval Loss: 1.3713 Eval Acc: 0.5070 (LR: 0.001000)
[2025-05-14 15:39:13,277]: [ResNet18_hardtanh] Epoch: 005 Train Loss: 1.2513 Train Acc: 0.5519 Eval Loss: 1.1728 Eval Acc: 0.5831 (LR: 0.001000)
[2025-05-14 15:40:31,189]: [ResNet18_hardtanh] Epoch: 006 Train Loss: 1.1773 Train Acc: 0.5765 Eval Loss: 1.1553 Eval Acc: 0.5873 (LR: 0.001000)
[2025-05-14 15:41:49,072]: [ResNet18_hardtanh] Epoch: 007 Train Loss: 1.1146 Train Acc: 0.6048 Eval Loss: 1.1383 Eval Acc: 0.5912 (LR: 0.001000)
[2025-05-14 15:43:06,957]: [ResNet18_hardtanh] Epoch: 008 Train Loss: 1.0630 Train Acc: 0.6213 Eval Loss: 1.0348 Eval Acc: 0.6304 (LR: 0.001000)
[2025-05-14 15:44:24,824]: [ResNet18_hardtanh] Epoch: 009 Train Loss: 1.0172 Train Acc: 0.6388 Eval Loss: 1.0274 Eval Acc: 0.6315 (LR: 0.001000)
[2025-05-14 15:45:42,684]: [ResNet18_hardtanh] Epoch: 010 Train Loss: 0.9736 Train Acc: 0.6556 Eval Loss: 0.9936 Eval Acc: 0.6496 (LR: 0.001000)
[2025-05-14 15:47:00,566]: [ResNet18_hardtanh] Epoch: 011 Train Loss: 0.9307 Train Acc: 0.6706 Eval Loss: 1.0160 Eval Acc: 0.6464 (LR: 0.001000)
[2025-05-14 15:48:18,264]: [ResNet18_hardtanh] Epoch: 012 Train Loss: 0.8991 Train Acc: 0.6815 Eval Loss: 0.9256 Eval Acc: 0.6719 (LR: 0.001000)
[2025-05-14 15:49:36,139]: [ResNet18_hardtanh] Epoch: 013 Train Loss: 0.8696 Train Acc: 0.6943 Eval Loss: 0.8683 Eval Acc: 0.6989 (LR: 0.001000)
[2025-05-14 15:50:54,015]: [ResNet18_hardtanh] Epoch: 014 Train Loss: 0.8392 Train Acc: 0.7043 Eval Loss: 0.8767 Eval Acc: 0.6922 (LR: 0.001000)
[2025-05-14 15:52:11,734]: [ResNet18_hardtanh] Epoch: 015 Train Loss: 0.8131 Train Acc: 0.7126 Eval Loss: 0.8818 Eval Acc: 0.6967 (LR: 0.001000)
[2025-05-14 15:53:29,429]: [ResNet18_hardtanh] Epoch: 016 Train Loss: 0.7900 Train Acc: 0.7218 Eval Loss: 0.8321 Eval Acc: 0.7102 (LR: 0.001000)
[2025-05-14 15:54:47,354]: [ResNet18_hardtanh] Epoch: 017 Train Loss: 0.7641 Train Acc: 0.7302 Eval Loss: 0.8091 Eval Acc: 0.7229 (LR: 0.001000)
[2025-05-14 15:56:05,209]: [ResNet18_hardtanh] Epoch: 018 Train Loss: 0.7470 Train Acc: 0.7393 Eval Loss: 0.8663 Eval Acc: 0.7027 (LR: 0.001000)
[2025-05-14 15:57:22,892]: [ResNet18_hardtanh] Epoch: 019 Train Loss: 0.7246 Train Acc: 0.7430 Eval Loss: 0.7656 Eval Acc: 0.7381 (LR: 0.001000)
[2025-05-14 15:58:41,956]: [ResNet18_hardtanh] Epoch: 020 Train Loss: 0.7036 Train Acc: 0.7520 Eval Loss: 0.7371 Eval Acc: 0.7514 (LR: 0.001000)
[2025-05-14 15:59:59,664]: [ResNet18_hardtanh] Epoch: 021 Train Loss: 0.6859 Train Acc: 0.7595 Eval Loss: 0.7683 Eval Acc: 0.7378 (LR: 0.001000)
[2025-05-14 16:01:17,544]: [ResNet18_hardtanh] Epoch: 022 Train Loss: 0.6666 Train Acc: 0.7660 Eval Loss: 0.8183 Eval Acc: 0.7262 (LR: 0.001000)
[2025-05-14 16:02:35,243]: [ResNet18_hardtanh] Epoch: 023 Train Loss: 0.6528 Train Acc: 0.7704 Eval Loss: 0.7137 Eval Acc: 0.7520 (LR: 0.001000)
[2025-05-14 16:03:53,110]: [ResNet18_hardtanh] Epoch: 024 Train Loss: 0.6334 Train Acc: 0.7756 Eval Loss: 0.7057 Eval Acc: 0.7595 (LR: 0.001000)
[2025-05-14 16:05:11,012]: [ResNet18_hardtanh] Epoch: 025 Train Loss: 0.6228 Train Acc: 0.7817 Eval Loss: 0.7280 Eval Acc: 0.7442 (LR: 0.001000)
[2025-05-14 16:06:28,710]: [ResNet18_hardtanh] Epoch: 026 Train Loss: 0.6064 Train Acc: 0.7861 Eval Loss: 0.7097 Eval Acc: 0.7608 (LR: 0.001000)
[2025-05-14 16:07:46,557]: [ResNet18_hardtanh] Epoch: 027 Train Loss: 0.5905 Train Acc: 0.7939 Eval Loss: 0.7240 Eval Acc: 0.7539 (LR: 0.001000)
[2025-05-14 16:09:04,257]: [ResNet18_hardtanh] Epoch: 028 Train Loss: 0.5781 Train Acc: 0.7957 Eval Loss: 0.6646 Eval Acc: 0.7802 (LR: 0.001000)
[2025-05-14 16:10:22,161]: [ResNet18_hardtanh] Epoch: 029 Train Loss: 0.5711 Train Acc: 0.8000 Eval Loss: 0.7150 Eval Acc: 0.7618 (LR: 0.001000)
[2025-05-14 16:11:39,998]: [ResNet18_hardtanh] Epoch: 030 Train Loss: 0.5539 Train Acc: 0.8053 Eval Loss: 0.6774 Eval Acc: 0.7736 (LR: 0.001000)
[2025-05-14 16:12:57,709]: [ResNet18_hardtanh] Epoch: 031 Train Loss: 0.5461 Train Acc: 0.8078 Eval Loss: 0.7078 Eval Acc: 0.7648 (LR: 0.001000)
[2025-05-14 16:14:15,568]: [ResNet18_hardtanh] Epoch: 032 Train Loss: 0.5329 Train Acc: 0.8135 Eval Loss: 0.6708 Eval Acc: 0.7766 (LR: 0.001000)
[2025-05-14 16:15:33,475]: [ResNet18_hardtanh] Epoch: 033 Train Loss: 0.5287 Train Acc: 0.8138 Eval Loss: 0.6846 Eval Acc: 0.7731 (LR: 0.001000)
[2025-05-14 16:16:51,379]: [ResNet18_hardtanh] Epoch: 034 Train Loss: 0.5132 Train Acc: 0.8198 Eval Loss: 0.6518 Eval Acc: 0.7827 (LR: 0.001000)
[2025-05-14 16:18:09,238]: [ResNet18_hardtanh] Epoch: 035 Train Loss: 0.4959 Train Acc: 0.8262 Eval Loss: 0.6750 Eval Acc: 0.7741 (LR: 0.001000)
[2025-05-14 16:19:26,952]: [ResNet18_hardtanh] Epoch: 036 Train Loss: 0.4979 Train Acc: 0.8247 Eval Loss: 0.6274 Eval Acc: 0.7900 (LR: 0.001000)
[2025-05-14 16:20:44,813]: [ResNet18_hardtanh] Epoch: 037 Train Loss: 0.4777 Train Acc: 0.8339 Eval Loss: 0.6127 Eval Acc: 0.7999 (LR: 0.001000)
[2025-05-14 16:22:02,693]: [ResNet18_hardtanh] Epoch: 038 Train Loss: 0.4736 Train Acc: 0.8330 Eval Loss: 0.6827 Eval Acc: 0.7766 (LR: 0.001000)
[2025-05-14 16:23:20,406]: [ResNet18_hardtanh] Epoch: 039 Train Loss: 0.4579 Train Acc: 0.8380 Eval Loss: 0.6235 Eval Acc: 0.7983 (LR: 0.001000)
[2025-05-14 16:24:39,592]: [ResNet18_hardtanh] Epoch: 040 Train Loss: 0.4512 Train Acc: 0.8415 Eval Loss: 0.6602 Eval Acc: 0.7842 (LR: 0.001000)
[2025-05-14 16:25:57,365]: [ResNet18_hardtanh] Epoch: 041 Train Loss: 0.4492 Train Acc: 0.8417 Eval Loss: 0.6128 Eval Acc: 0.8032 (LR: 0.001000)
[2025-05-14 16:27:15,256]: [ResNet18_hardtanh] Epoch: 042 Train Loss: 0.4355 Train Acc: 0.8467 Eval Loss: 0.6083 Eval Acc: 0.8044 (LR: 0.001000)
[2025-05-14 16:28:33,302]: [ResNet18_hardtanh] Epoch: 043 Train Loss: 0.4282 Train Acc: 0.8479 Eval Loss: 0.6272 Eval Acc: 0.8010 (LR: 0.001000)
[2025-05-14 16:29:51,198]: [ResNet18_hardtanh] Epoch: 044 Train Loss: 0.4204 Train Acc: 0.8522 Eval Loss: 0.5985 Eval Acc: 0.8052 (LR: 0.001000)
[2025-05-14 16:31:09,068]: [ResNet18_hardtanh] Epoch: 045 Train Loss: 0.4121 Train Acc: 0.8554 Eval Loss: 0.6001 Eval Acc: 0.8059 (LR: 0.001000)
[2025-05-14 16:32:26,971]: [ResNet18_hardtanh] Epoch: 046 Train Loss: 0.4074 Train Acc: 0.8564 Eval Loss: 0.6429 Eval Acc: 0.7955 (LR: 0.001000)
[2025-05-14 16:33:44,681]: [ResNet18_hardtanh] Epoch: 047 Train Loss: 0.3989 Train Acc: 0.8588 Eval Loss: 0.6057 Eval Acc: 0.8048 (LR: 0.001000)
[2025-05-14 16:35:02,553]: [ResNet18_hardtanh] Epoch: 048 Train Loss: 0.3879 Train Acc: 0.8627 Eval Loss: 0.5900 Eval Acc: 0.8087 (LR: 0.001000)
[2025-05-14 16:36:20,451]: [ResNet18_hardtanh] Epoch: 049 Train Loss: 0.3821 Train Acc: 0.8643 Eval Loss: 0.6014 Eval Acc: 0.8020 (LR: 0.001000)
[2025-05-14 16:37:38,155]: [ResNet18_hardtanh] Epoch: 050 Train Loss: 0.3736 Train Acc: 0.8680 Eval Loss: 0.5947 Eval Acc: 0.8103 (LR: 0.001000)
[2025-05-14 16:38:56,068]: [ResNet18_hardtanh] Epoch: 051 Train Loss: 0.3705 Train Acc: 0.8704 Eval Loss: 0.6093 Eval Acc: 0.8061 (LR: 0.001000)
[2025-05-14 16:40:13,742]: [ResNet18_hardtanh] Epoch: 052 Train Loss: 0.3629 Train Acc: 0.8722 Eval Loss: 0.5969 Eval Acc: 0.8056 (LR: 0.001000)
[2025-05-14 16:41:31,612]: [ResNet18_hardtanh] Epoch: 053 Train Loss: 0.3535 Train Acc: 0.8759 Eval Loss: 0.6184 Eval Acc: 0.8041 (LR: 0.001000)
[2025-05-14 16:42:49,515]: [ResNet18_hardtanh] Epoch: 054 Train Loss: 0.3506 Train Acc: 0.8757 Eval Loss: 0.5528 Eval Acc: 0.8262 (LR: 0.001000)
[2025-05-14 16:44:07,433]: [ResNet18_hardtanh] Epoch: 055 Train Loss: 0.3424 Train Acc: 0.8784 Eval Loss: 0.5842 Eval Acc: 0.8172 (LR: 0.001000)
[2025-05-14 16:45:25,104]: [ResNet18_hardtanh] Epoch: 056 Train Loss: 0.3336 Train Acc: 0.8818 Eval Loss: 0.5776 Eval Acc: 0.8229 (LR: 0.001000)
[2025-05-14 16:46:42,991]: [ResNet18_hardtanh] Epoch: 057 Train Loss: 0.3346 Train Acc: 0.8811 Eval Loss: 0.5658 Eval Acc: 0.8253 (LR: 0.001000)
[2025-05-14 16:48:00,712]: [ResNet18_hardtanh] Epoch: 058 Train Loss: 0.3262 Train Acc: 0.8843 Eval Loss: 0.6246 Eval Acc: 0.8034 (LR: 0.001000)
[2025-05-14 16:49:18,582]: [ResNet18_hardtanh] Epoch: 059 Train Loss: 0.3255 Train Acc: 0.8865 Eval Loss: 0.5637 Eval Acc: 0.8262 (LR: 0.001000)
[2025-05-14 16:50:37,735]: [ResNet18_hardtanh] Epoch: 060 Train Loss: 0.3155 Train Acc: 0.8870 Eval Loss: 0.5676 Eval Acc: 0.8240 (LR: 0.001000)
[2025-05-14 16:51:55,507]: [ResNet18_hardtanh] Epoch: 061 Train Loss: 0.3063 Train Acc: 0.8914 Eval Loss: 0.5732 Eval Acc: 0.8270 (LR: 0.001000)
[2025-05-14 16:53:14,901]: [ResNet18_hardtanh] Epoch: 062 Train Loss: 0.3085 Train Acc: 0.8903 Eval Loss: 0.6272 Eval Acc: 0.8067 (LR: 0.001000)
[2025-05-14 16:54:33,022]: [ResNet18_hardtanh] Epoch: 063 Train Loss: 0.3027 Train Acc: 0.8937 Eval Loss: 0.5400 Eval Acc: 0.8306 (LR: 0.001000)
[2025-05-14 16:55:50,883]: [ResNet18_hardtanh] Epoch: 064 Train Loss: 0.2955 Train Acc: 0.8957 Eval Loss: 0.5384 Eval Acc: 0.8361 (LR: 0.001000)
[2025-05-14 16:57:08,753]: [ResNet18_hardtanh] Epoch: 065 Train Loss: 0.2897 Train Acc: 0.8967 Eval Loss: 0.6084 Eval Acc: 0.8203 (LR: 0.001000)
[2025-05-14 16:58:26,676]: [ResNet18_hardtanh] Epoch: 066 Train Loss: 0.2876 Train Acc: 0.8982 Eval Loss: 0.5915 Eval Acc: 0.8237 (LR: 0.001000)
[2025-05-14 16:59:44,344]: [ResNet18_hardtanh] Epoch: 067 Train Loss: 0.2804 Train Acc: 0.9005 Eval Loss: 0.5782 Eval Acc: 0.8272 (LR: 0.001000)
[2025-05-14 17:01:02,259]: [ResNet18_hardtanh] Epoch: 068 Train Loss: 0.2759 Train Acc: 0.9017 Eval Loss: 0.5596 Eval Acc: 0.8257 (LR: 0.001000)
[2025-05-14 17:02:20,134]: [ResNet18_hardtanh] Epoch: 069 Train Loss: 0.2721 Train Acc: 0.9037 Eval Loss: 0.5564 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-14 17:03:37,791]: [ResNet18_hardtanh] Epoch: 070 Train Loss: 0.2711 Train Acc: 0.9026 Eval Loss: 0.5738 Eval Acc: 0.8297 (LR: 0.000100)
[2025-05-14 17:04:55,688]: [ResNet18_hardtanh] Epoch: 071 Train Loss: 0.1948 Train Acc: 0.9354 Eval Loss: 0.4758 Eval Acc: 0.8557 (LR: 0.000100)
[2025-05-14 17:06:13,971]: [ResNet18_hardtanh] Epoch: 072 Train Loss: 0.1774 Train Acc: 0.9404 Eval Loss: 0.4689 Eval Acc: 0.8600 (LR: 0.000100)
[2025-05-14 17:07:31,876]: [ResNet18_hardtanh] Epoch: 073 Train Loss: 0.1719 Train Acc: 0.9427 Eval Loss: 0.4751 Eval Acc: 0.8582 (LR: 0.000100)
[2025-05-14 17:08:49,590]: [ResNet18_hardtanh] Epoch: 074 Train Loss: 0.1689 Train Acc: 0.9422 Eval Loss: 0.4775 Eval Acc: 0.8596 (LR: 0.000100)
[2025-05-14 17:10:07,256]: [ResNet18_hardtanh] Epoch: 075 Train Loss: 0.1657 Train Acc: 0.9436 Eval Loss: 0.4749 Eval Acc: 0.8592 (LR: 0.000100)
[2025-05-14 17:11:24,933]: [ResNet18_hardtanh] Epoch: 076 Train Loss: 0.1639 Train Acc: 0.9442 Eval Loss: 0.4819 Eval Acc: 0.8588 (LR: 0.000100)
[2025-05-14 17:12:42,625]: [ResNet18_hardtanh] Epoch: 077 Train Loss: 0.1615 Train Acc: 0.9448 Eval Loss: 0.4784 Eval Acc: 0.8621 (LR: 0.000100)
[2025-05-14 17:14:00,484]: [ResNet18_hardtanh] Epoch: 078 Train Loss: 0.1594 Train Acc: 0.9450 Eval Loss: 0.4775 Eval Acc: 0.8598 (LR: 0.000100)
[2025-05-14 17:15:18,180]: [ResNet18_hardtanh] Epoch: 079 Train Loss: 0.1573 Train Acc: 0.9463 Eval Loss: 0.4831 Eval Acc: 0.8616 (LR: 0.000100)
[2025-05-14 17:16:37,166]: [ResNet18_hardtanh] Epoch: 080 Train Loss: 0.1519 Train Acc: 0.9485 Eval Loss: 0.4852 Eval Acc: 0.8621 (LR: 0.000100)
[2025-05-14 17:17:55,154]: [ResNet18_hardtanh] Epoch: 081 Train Loss: 0.1526 Train Acc: 0.9483 Eval Loss: 0.4847 Eval Acc: 0.8601 (LR: 0.000100)
[2025-05-14 17:19:13,053]: [ResNet18_hardtanh] Epoch: 082 Train Loss: 0.1519 Train Acc: 0.9480 Eval Loss: 0.4839 Eval Acc: 0.8598 (LR: 0.000100)
[2025-05-14 17:20:30,921]: [ResNet18_hardtanh] Epoch: 083 Train Loss: 0.1524 Train Acc: 0.9485 Eval Loss: 0.4842 Eval Acc: 0.8601 (LR: 0.000100)
[2025-05-14 17:21:48,788]: [ResNet18_hardtanh] Epoch: 084 Train Loss: 0.1492 Train Acc: 0.9500 Eval Loss: 0.4875 Eval Acc: 0.8600 (LR: 0.000100)
[2025-05-14 17:23:06,488]: [ResNet18_hardtanh] Epoch: 085 Train Loss: 0.1476 Train Acc: 0.9489 Eval Loss: 0.4945 Eval Acc: 0.8568 (LR: 0.000100)
[2025-05-14 17:24:24,193]: [ResNet18_hardtanh] Epoch: 086 Train Loss: 0.1468 Train Acc: 0.9498 Eval Loss: 0.4903 Eval Acc: 0.8610 (LR: 0.000100)
[2025-05-14 17:25:42,092]: [ResNet18_hardtanh] Epoch: 087 Train Loss: 0.1416 Train Acc: 0.9519 Eval Loss: 0.4953 Eval Acc: 0.8606 (LR: 0.000100)
[2025-05-14 17:26:59,952]: [ResNet18_hardtanh] Epoch: 088 Train Loss: 0.1462 Train Acc: 0.9493 Eval Loss: 0.4990 Eval Acc: 0.8593 (LR: 0.000100)
[2025-05-14 17:28:17,831]: [ResNet18_hardtanh] Epoch: 089 Train Loss: 0.1430 Train Acc: 0.9510 Eval Loss: 0.4982 Eval Acc: 0.8608 (LR: 0.000100)
[2025-05-14 17:29:35,526]: [ResNet18_hardtanh] Epoch: 090 Train Loss: 0.1425 Train Acc: 0.9520 Eval Loss: 0.4925 Eval Acc: 0.8596 (LR: 0.000100)
[2025-05-14 17:30:53,236]: [ResNet18_hardtanh] Epoch: 091 Train Loss: 0.1378 Train Acc: 0.9532 Eval Loss: 0.4966 Eval Acc: 0.8601 (LR: 0.000100)
[2025-05-14 17:32:10,918]: [ResNet18_hardtanh] Epoch: 092 Train Loss: 0.1404 Train Acc: 0.9518 Eval Loss: 0.5020 Eval Acc: 0.8600 (LR: 0.000100)
[2025-05-14 17:33:28,605]: [ResNet18_hardtanh] Epoch: 093 Train Loss: 0.1360 Train Acc: 0.9531 Eval Loss: 0.4981 Eval Acc: 0.8631 (LR: 0.000100)
[2025-05-14 17:34:46,492]: [ResNet18_hardtanh] Epoch: 094 Train Loss: 0.1368 Train Acc: 0.9539 Eval Loss: 0.5013 Eval Acc: 0.8602 (LR: 0.000100)
[2025-05-14 17:36:04,376]: [ResNet18_hardtanh] Epoch: 095 Train Loss: 0.1351 Train Acc: 0.9537 Eval Loss: 0.5027 Eval Acc: 0.8623 (LR: 0.000100)
[2025-05-14 17:37:22,049]: [ResNet18_hardtanh] Epoch: 096 Train Loss: 0.1320 Train Acc: 0.9558 Eval Loss: 0.5104 Eval Acc: 0.8582 (LR: 0.000100)
[2025-05-14 17:38:39,733]: [ResNet18_hardtanh] Epoch: 097 Train Loss: 0.1337 Train Acc: 0.9542 Eval Loss: 0.5037 Eval Acc: 0.8594 (LR: 0.000100)
[2025-05-14 17:39:57,422]: [ResNet18_hardtanh] Epoch: 098 Train Loss: 0.1309 Train Acc: 0.9556 Eval Loss: 0.5046 Eval Acc: 0.8598 (LR: 0.000100)
[2025-05-14 17:41:15,292]: [ResNet18_hardtanh] Epoch: 099 Train Loss: 0.1346 Train Acc: 0.9541 Eval Loss: 0.5107 Eval Acc: 0.8597 (LR: 0.000100)
[2025-05-14 17:42:34,528]: [ResNet18_hardtanh] Epoch: 100 Train Loss: 0.1296 Train Acc: 0.9556 Eval Loss: 0.5092 Eval Acc: 0.8601 (LR: 0.000010)
[2025-05-14 17:43:52,294]: [ResNet18_hardtanh] Epoch: 101 Train Loss: 0.1251 Train Acc: 0.9582 Eval Loss: 0.4995 Eval Acc: 0.8628 (LR: 0.000010)
[2025-05-14 17:45:10,170]: [ResNet18_hardtanh] Epoch: 102 Train Loss: 0.1254 Train Acc: 0.9576 Eval Loss: 0.4992 Eval Acc: 0.8624 (LR: 0.000010)
[2025-05-14 17:46:28,048]: [ResNet18_hardtanh] Epoch: 103 Train Loss: 0.1232 Train Acc: 0.9588 Eval Loss: 0.5012 Eval Acc: 0.8604 (LR: 0.000010)
[2025-05-14 17:47:45,930]: [ResNet18_hardtanh] Epoch: 104 Train Loss: 0.1196 Train Acc: 0.9606 Eval Loss: 0.5003 Eval Acc: 0.8620 (LR: 0.000010)
[2025-05-14 17:49:03,633]: [ResNet18_hardtanh] Epoch: 105 Train Loss: 0.1237 Train Acc: 0.9575 Eval Loss: 0.5016 Eval Acc: 0.8612 (LR: 0.000010)
[2025-05-14 17:50:21,478]: [ResNet18_hardtanh] Epoch: 106 Train Loss: 0.1224 Train Acc: 0.9583 Eval Loss: 0.5000 Eval Acc: 0.8611 (LR: 0.000010)
[2025-05-14 17:51:39,349]: [ResNet18_hardtanh] Epoch: 107 Train Loss: 0.1192 Train Acc: 0.9595 Eval Loss: 0.5020 Eval Acc: 0.8624 (LR: 0.000010)
[2025-05-14 17:52:57,023]: [ResNet18_hardtanh] Epoch: 108 Train Loss: 0.1201 Train Acc: 0.9593 Eval Loss: 0.5037 Eval Acc: 0.8605 (LR: 0.000010)
[2025-05-14 17:54:14,733]: [ResNet18_hardtanh] Epoch: 109 Train Loss: 0.1183 Train Acc: 0.9607 Eval Loss: 0.4999 Eval Acc: 0.8618 (LR: 0.000010)
[2025-05-14 17:55:32,426]: [ResNet18_hardtanh] Epoch: 110 Train Loss: 0.1200 Train Acc: 0.9591 Eval Loss: 0.5016 Eval Acc: 0.8622 (LR: 0.000010)
[2025-05-14 17:56:50,282]: [ResNet18_hardtanh] Epoch: 111 Train Loss: 0.1220 Train Acc: 0.9586 Eval Loss: 0.5016 Eval Acc: 0.8598 (LR: 0.000010)
[2025-05-14 17:58:07,961]: [ResNet18_hardtanh] Epoch: 112 Train Loss: 0.1211 Train Acc: 0.9589 Eval Loss: 0.5051 Eval Acc: 0.8615 (LR: 0.000010)
[2025-05-14 17:59:25,657]: [ResNet18_hardtanh] Epoch: 113 Train Loss: 0.1206 Train Acc: 0.9600 Eval Loss: 0.5004 Eval Acc: 0.8616 (LR: 0.000010)
[2025-05-14 18:00:43,513]: [ResNet18_hardtanh] Epoch: 114 Train Loss: 0.1202 Train Acc: 0.9603 Eval Loss: 0.5011 Eval Acc: 0.8619 (LR: 0.000010)
[2025-05-14 18:02:01,391]: [ResNet18_hardtanh] Epoch: 115 Train Loss: 0.1213 Train Acc: 0.9579 Eval Loss: 0.5003 Eval Acc: 0.8624 (LR: 0.000010)
[2025-05-14 18:03:19,077]: [ResNet18_hardtanh] Epoch: 116 Train Loss: 0.1201 Train Acc: 0.9592 Eval Loss: 0.5012 Eval Acc: 0.8618 (LR: 0.000010)
[2025-05-14 18:04:36,758]: [ResNet18_hardtanh] Epoch: 117 Train Loss: 0.1175 Train Acc: 0.9613 Eval Loss: 0.5032 Eval Acc: 0.8604 (LR: 0.000010)
[2025-05-14 18:05:54,447]: [ResNet18_hardtanh] Epoch: 118 Train Loss: 0.1190 Train Acc: 0.9605 Eval Loss: 0.4999 Eval Acc: 0.8608 (LR: 0.000010)
[2025-05-14 18:07:12,139]: [ResNet18_hardtanh] Epoch: 119 Train Loss: 0.1183 Train Acc: 0.9612 Eval Loss: 0.5030 Eval Acc: 0.8617 (LR: 0.000010)
[2025-05-14 18:08:30,999]: [ResNet18_hardtanh] Epoch: 120 Train Loss: 0.1209 Train Acc: 0.9593 Eval Loss: 0.5030 Eval Acc: 0.8611 (LR: 0.000010)
[2025-05-14 18:08:30,999]: [ResNet18_hardtanh] Best Eval Accuracy: 0.8631
[2025-05-14 18:08:31,069]: 
Training of full-precision model finished!
[2025-05-14 18:08:31,069]: Model Architecture:
[2025-05-14 18:08:31,071]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 18:08:31,071]: 
Model Weights:
[2025-05-14 18:08:31,071]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 18:08:31,071]: Sample Values (25 elements): [0.12244075536727905, -0.07923845946788788, 0.05405743047595024, -0.07260098308324814, 0.17504693567752838, 0.07564760744571686, -0.11249810457229614, -0.158054918050766, -0.10750751942396164, -0.13341772556304932, 0.18460622429847717, -0.11818941682577133, 0.09816816449165344, -0.11227861046791077, 0.0089710783213377, -0.20263485610485077, -0.2532412111759186, -0.12204882502555847, -0.01477768737822771, -0.07251150906085968, -0.12101754546165466, 0.12670937180519104, 0.09260855615139008, 0.18858905136585236, -0.09056898951530457]
[2025-05-14 18:08:31,071]: Mean: 0.00105399
[2025-05-14 18:08:31,072]: Min: -0.30904421
[2025-05-14 18:08:31,072]: Max: 0.31748727
[2025-05-14 18:08:31,072]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 18:08:31,072]: Sample Values (25 elements): [1.011521816253662, 0.9818001389503479, 1.0236272811889648, 0.9984755516052246, 0.9223999977111816, 0.9508986473083496, 0.9053573608398438, 0.8984652161598206, 0.9719494581222534, 1.008193850517273, 0.971282422542572, 0.9940415024757385, 1.022526741027832, 1.0193675756454468, 0.9397844076156616, 0.8856474757194519, 0.9252510070800781, 0.9567548632621765, 0.9315366148948669, 0.9833722114562988, 0.9707927107810974, 1.034468650817871, 0.7949405312538147, 0.7802690267562866, 0.9182456135749817]
[2025-05-14 18:08:31,072]: Mean: 0.95324552
[2025-05-14 18:08:31,072]: Min: 0.78026903
[2025-05-14 18:08:31,072]: Max: 1.06347823
[2025-05-14 18:08:31,072]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 18:08:31,073]: Sample Values (25 elements): [0.017058616504073143, 0.025564584881067276, -0.0068153818137943745, 0.01202247105538845, -0.01862511783838272, 0.0263905618339777, 0.007711680140346289, 0.020472588017582893, 0.040246184915304184, -0.04272419959306717, -0.0006306309951469302, -0.0006136785377748311, 0.0023013760801404715, -0.0018308539874851704, -0.04349144920706749, -0.01985763944685459, 0.0026460106018930674, 0.0020779999904334545, 0.011347395367920399, 0.023215848952531815, 0.009025814943015575, -0.049318548291921616, -0.010962378233671188, 0.00022186075511854142, 0.0034910908434540033]
[2025-05-14 18:08:31,073]: Mean: 0.00012218
[2025-05-14 18:08:31,073]: Min: -0.13882534
[2025-05-14 18:08:31,073]: Max: 0.11163857
[2025-05-14 18:08:31,074]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 18:08:31,074]: Sample Values (25 elements): [0.9647525548934937, 0.9940128922462463, 1.041968822479248, 1.0065640211105347, 0.9774360060691833, 1.0029535293579102, 0.9593233466148376, 1.033879041671753, 1.0048824548721313, 0.9871872663497925, 0.9638587236404419, 0.9956811666488647, 0.9737743139266968, 0.984257698059082, 1.0130726099014282, 1.0171607732772827, 0.9650837182998657, 0.9659273028373718, 1.0071440935134888, 1.0268831253051758, 0.9369386434555054, 1.0300467014312744, 0.9717790484428406, 1.00571608543396, 0.9836971759796143]
[2025-05-14 18:08:31,074]: Mean: 0.98822141
[2025-05-14 18:08:31,074]: Min: 0.93402761
[2025-05-14 18:08:31,074]: Max: 1.04899871
[2025-05-14 18:08:31,074]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 18:08:31,075]: Sample Values (25 elements): [0.003991385456174612, -0.0005841798265464604, -0.034217800945043564, 0.03951738774776459, 0.010176747106015682, -0.035690248012542725, 0.05049203708767891, 0.03631165251135826, -0.012351022101938725, -0.031755879521369934, -0.054242003709077835, -0.031868189573287964, -0.025674670934677124, -0.05904603376984596, -0.014588365331292152, 0.05040481314063072, -0.0026764862705022097, 0.04250571131706238, 0.02459748275578022, 0.01342200580984354, -0.014001516625285149, -0.04284263029694557, 0.015963522717356682, 0.04840724542737007, -0.021919777616858482]
[2025-05-14 18:08:31,075]: Mean: 0.00003174
[2025-05-14 18:08:31,075]: Min: -0.15285333
[2025-05-14 18:08:31,075]: Max: 0.20040962
[2025-05-14 18:08:31,075]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 18:08:31,076]: Sample Values (25 elements): [0.9331601858139038, 0.9883568286895752, 0.961325466632843, 0.9518932700157166, 0.9743335247039795, 0.9770399332046509, 0.9809946417808533, 0.9935973286628723, 0.9642473459243774, 1.0030791759490967, 0.9835357069969177, 0.9766168594360352, 0.9554489254951477, 0.9247676134109497, 0.958726704120636, 1.0099252462387085, 0.9783354997634888, 1.0178539752960205, 1.0005606412887573, 0.984998345375061, 0.9560309052467346, 0.9594601988792419, 0.9743660688400269, 0.9899336695671082, 0.9845559000968933]
[2025-05-14 18:08:31,076]: Mean: 0.97687602
[2025-05-14 18:08:31,076]: Min: 0.91621202
[2025-05-14 18:08:31,076]: Max: 1.20159566
[2025-05-14 18:08:31,076]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 18:08:31,077]: Sample Values (25 elements): [0.006858478765934706, 0.035784900188446045, 0.011846962384879589, -0.013922127895057201, -0.03707106411457062, -0.03537744656205177, -0.015413417480885983, 0.014201043173670769, 5.1224094931967556e-05, 0.013797186315059662, -0.03469458222389221, -0.0185723714530468, -0.026883594691753387, -0.0247370433062315, -0.016887405887246132, 0.031318794935941696, 0.026120446622371674, -0.01749967411160469, -0.0007197043159976602, -0.030712606385350227, 0.00998852588236332, -0.014293596148490906, -0.04551798477768898, 0.08702369034290314, 0.0032706030178815126]
[2025-05-14 18:08:31,077]: Mean: 0.00001892
[2025-05-14 18:08:31,077]: Min: -0.15631312
[2025-05-14 18:08:31,077]: Max: 0.13782541
[2025-05-14 18:08:31,077]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 18:08:31,077]: Sample Values (25 elements): [0.9783212542533875, 0.9732270240783691, 1.0178799629211426, 0.9824021458625793, 1.005446434020996, 0.9984573721885681, 0.9822247624397278, 0.9904103875160217, 0.9867183566093445, 0.9837809801101685, 0.9686156511306763, 0.972989022731781, 0.969768226146698, 0.9732410311698914, 0.9808968901634216, 0.9706020951271057, 0.9787508845329285, 0.9759622812271118, 0.977176308631897, 1.00676429271698, 0.99485182762146, 1.0292038917541504, 1.020701289176941, 1.0277156829833984, 0.9858629703521729]
[2025-05-14 18:08:31,077]: Mean: 0.99166650
[2025-05-14 18:08:31,078]: Min: 0.96047378
[2025-05-14 18:08:31,078]: Max: 1.02976120
[2025-05-14 18:08:31,078]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 18:08:31,078]: Sample Values (25 elements): [-0.004425818100571632, 0.022191278636455536, -0.027023233473300934, -0.019519202411174774, 0.04935278370976448, -0.014972291886806488, -0.015539133921265602, -0.003984592854976654, -0.005306591745465994, -0.02566644549369812, 0.03320423886179924, -0.03099941648542881, -0.01495200116187334, 0.016286015510559082, 0.025914663448929787, 0.03762347251176834, 0.026400327682495117, -0.011058424599468708, 0.013239195570349693, -0.04208593815565109, 0.0494830347597599, -0.0177846010774374, 0.02821328304708004, -0.004257228225469589, -0.02172805182635784]
[2025-05-14 18:08:31,079]: Mean: 0.00030383
[2025-05-14 18:08:31,079]: Min: -0.09022436
[2025-05-14 18:08:31,079]: Max: 0.09454152
[2025-05-14 18:08:31,079]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 18:08:31,079]: Sample Values (25 elements): [0.9815165996551514, 0.9668540358543396, 0.9657927751541138, 0.9802298545837402, 1.0094633102416992, 0.9528451561927795, 0.9775987267494202, 0.985417902469635, 0.9776379466056824, 0.9776591658592224, 0.9993749856948853, 0.9729406833648682, 0.9316712021827698, 1.006013035774231, 0.9654839634895325, 0.9544923305511475, 0.9716911911964417, 0.9727659821510315, 1.0156725645065308, 0.965820848941803, 0.9814695119857788, 1.0313962697982788, 0.976771891117096, 0.9608666896820068, 0.9544922113418579]
[2025-05-14 18:08:31,079]: Mean: 0.97643948
[2025-05-14 18:08:31,079]: Min: 0.92109442
[2025-05-14 18:08:31,080]: Max: 1.06168497
[2025-05-14 18:08:31,080]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 18:08:31,080]: Sample Values (25 elements): [0.05422600358724594, 0.0188466664403677, 0.030779032036662102, 0.010418770834803581, 0.00439561577513814, 0.013868242502212524, -0.061230920255184174, 0.03884383663535118, -0.010884054005146027, -0.006020586472004652, -0.027528034523129463, 0.0017894896445795894, 0.008977876044809818, 0.0035529816523194313, -0.044735461473464966, 0.039854325354099274, -0.0013704932061955333, -0.0013578681973740458, 0.036167774349451065, -0.0012555693974718451, -0.025604253634810448, 0.03193022683262825, -0.034710343927145004, 0.011628515087068081, 0.020713696256279945]
[2025-05-14 18:08:31,081]: Mean: 0.00011318
[2025-05-14 18:08:31,081]: Min: -0.10206186
[2025-05-14 18:08:31,081]: Max: 0.09933089
[2025-05-14 18:08:31,081]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 18:08:31,081]: Sample Values (25 elements): [0.9847158789634705, 0.9776371717453003, 0.9774965643882751, 0.9724792838096619, 0.9813037514686584, 0.9797443747520447, 0.9698755145072937, 0.9859701991081238, 0.9761144518852234, 0.981044590473175, 0.9771405458450317, 0.9856393337249756, 0.9799435138702393, 0.9759543538093567, 0.9860261082649231, 0.9770810604095459, 0.9914838075637817, 0.9743310213088989, 0.9805319309234619, 0.9734082221984863, 0.968371570110321, 1.0111950635910034, 0.9776210188865662, 0.9791985750198364, 0.9855526089668274]
[2025-05-14 18:08:31,081]: Mean: 0.97848350
[2025-05-14 18:08:31,082]: Min: 0.96349198
[2025-05-14 18:08:31,082]: Max: 1.01119506
[2025-05-14 18:08:31,082]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 18:08:31,083]: Sample Values (25 elements): [-0.020872149616479874, -0.019538583233952522, 0.02533317543566227, 0.022441815584897995, -0.005794476717710495, -0.029660511761903763, -0.0026412163861095905, 0.017569810152053833, -0.01643027924001217, -0.00626505957916379, -0.00031670142197981477, 0.014103913679718971, 0.01581362448632717, -0.007030322216451168, 0.01710701920092106, 0.01767084002494812, -0.014635617844760418, 0.0180061012506485, 0.004259116947650909, 0.012626076117157936, -0.01646379567682743, 0.0065194591879844666, -0.00317227840423584, 0.0303745549172163, -0.02413634955883026]
[2025-05-14 18:08:31,083]: Mean: -0.00003179
[2025-05-14 18:08:31,083]: Min: -0.05795630
[2025-05-14 18:08:31,084]: Max: 0.06237352
[2025-05-14 18:08:31,084]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 18:08:31,084]: Sample Values (25 elements): [0.9767539501190186, 0.9737284183502197, 0.967080295085907, 0.9698566794395447, 0.98643559217453, 0.9767470359802246, 0.9861368536949158, 0.9821881651878357, 0.980063796043396, 1.0033246278762817, 0.9842740893363953, 0.9723328351974487, 0.9811860918998718, 0.9841740727424622, 0.9699673056602478, 0.9727931022644043, 0.964870810508728, 0.9833604097366333, 0.9922394752502441, 0.969689667224884, 0.9726153612136841, 0.9720881581306458, 0.9728373289108276, 0.9809094071388245, 0.9729366302490234]
[2025-05-14 18:08:31,084]: Mean: 0.97859311
[2025-05-14 18:08:31,084]: Min: 0.96125817
[2025-05-14 18:08:31,084]: Max: 1.00332463
[2025-05-14 18:08:31,084]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 18:08:31,085]: Sample Values (25 elements): [-0.011455988511443138, 0.11699657887220383, 0.08487751334905624, 0.019909508526325226, 0.05500004068017006, 0.018891962245106697, -0.001960115972906351, 0.026707574725151062, 0.09197072684764862, 0.04569154232740402, 0.03108745813369751, -0.11049190908670425, 0.050494495779275894, -0.11813685297966003, 0.12455812096595764, 0.03890262916684151, -0.06391457468271255, 0.049939222633838654, 0.05819631740450859, -0.04503459483385086, 0.04211890697479248, 0.11855559796094894, -0.029627665877342224, -0.03240656852722168, 0.09193479269742966]
[2025-05-14 18:08:31,085]: Mean: 0.00003528
[2025-05-14 18:08:31,085]: Min: -0.15892208
[2025-05-14 18:08:31,085]: Max: 0.15358226
[2025-05-14 18:08:31,085]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 18:08:31,085]: Sample Values (25 elements): [0.9529309868812561, 0.9516997337341309, 0.9557505249977112, 0.9489067792892456, 0.9529125690460205, 0.9583756327629089, 0.9500588774681091, 0.9454331398010254, 0.9622609615325928, 0.9512598514556885, 0.9804151654243469, 0.9523318409919739, 0.9546133875846863, 0.9344433546066284, 0.9509376287460327, 0.9515079259872437, 0.9570992588996887, 0.9519205093383789, 0.9525030255317688, 0.9628386497497559, 0.9530236124992371, 0.9590575695037842, 0.9459134340286255, 0.9575823545455933, 0.961249828338623]
[2025-05-14 18:08:31,086]: Mean: 0.95634663
[2025-05-14 18:08:31,086]: Min: 0.93444335
[2025-05-14 18:08:31,086]: Max: 0.98050588
[2025-05-14 18:08:31,086]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 18:08:31,087]: Sample Values (25 elements): [0.007670183200389147, -0.02281820960342884, -0.004228533711284399, 0.025655582547187805, -0.002317576203495264, 0.036025092005729675, -0.010216767899692059, -0.02094685100018978, -0.016804436221718788, 0.01848655380308628, -0.02082855813205242, -0.02623022347688675, -0.023998670279979706, 0.01863422431051731, -0.006983182393014431, -0.028491493314504623, -0.02563081681728363, 0.0237810667604208, -0.028793001547455788, -0.002493154490366578, 0.002634092466905713, -0.02739565260708332, 0.023460695520043373, 0.008349220268428326, -0.034532006829977036]
[2025-05-14 18:08:31,087]: Mean: -0.00004401
[2025-05-14 18:08:31,088]: Min: -0.05551529
[2025-05-14 18:08:31,088]: Max: 0.05575678
[2025-05-14 18:08:31,088]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 18:08:31,088]: Sample Values (25 elements): [0.9727641940116882, 0.9929174780845642, 0.9795395731925964, 0.9729169011116028, 0.9756609797477722, 0.98741614818573, 0.9781745672225952, 0.9872016906738281, 1.008483648300171, 0.9791789650917053, 0.97209632396698, 0.975537896156311, 0.9840260148048401, 0.9817091226577759, 0.9672577381134033, 0.995051383972168, 0.9890813231468201, 0.983240008354187, 0.9887505173683167, 0.9847435355186462, 0.9871140122413635, 0.9846720695495605, 0.9868074655532837, 0.9822815656661987, 0.980384111404419]
[2025-05-14 18:08:31,088]: Mean: 0.98276514
[2025-05-14 18:08:31,088]: Min: 0.96720934
[2025-05-14 18:08:31,088]: Max: 1.01173127
[2025-05-14 18:08:31,089]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 18:08:31,090]: Sample Values (25 elements): [-0.01376910787075758, 0.019250717014074326, 0.026512959972023964, -0.0009709951700642705, 0.0025399522855877876, 0.00718105910345912, -0.02682696282863617, 0.011297491379082203, 0.0075719403102993965, 0.036959607154130936, -0.02338012494146824, -0.02337191440165043, 0.008995972573757172, 0.001091157435439527, -0.0015553546836599708, 0.03631095588207245, -0.027104942128062248, 0.02573339454829693, -0.024847187101840973, -0.029499558731913567, 0.005622556898742914, -0.0047661433927714825, 0.02616691216826439, -0.0014908539596945047, -0.019415510818362236]
[2025-05-14 18:08:31,090]: Mean: 0.00003898
[2025-05-14 18:08:31,090]: Min: -0.05312140
[2025-05-14 18:08:31,090]: Max: 0.05800406
[2025-05-14 18:08:31,090]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 18:08:31,091]: Sample Values (25 elements): [0.9748619794845581, 0.9968019127845764, 0.9866193532943726, 0.9726540446281433, 0.9808273911476135, 0.9738097190856934, 0.9839666485786438, 0.988657534122467, 0.9891555905342102, 0.9918534755706787, 0.9839439392089844, 0.9876813292503357, 0.9877557754516602, 0.9804116487503052, 0.9937319755554199, 0.9859408140182495, 0.9844098091125488, 0.9915269017219543, 0.983669102191925, 0.9880419373512268, 1.0041154623031616, 0.9976788759231567, 0.9872189164161682, 0.9800169467926025, 0.9798241257667542]
[2025-05-14 18:08:31,091]: Mean: 0.98540342
[2025-05-14 18:08:31,091]: Min: 0.96443969
[2025-05-14 18:08:31,091]: Max: 1.00661790
[2025-05-14 18:08:31,091]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 18:08:31,094]: Sample Values (25 elements): [-0.002687145723029971, 0.020788520574569702, -0.017231103032827377, -0.0023848165292292833, 0.00745824771001935, 0.006898118648678064, -0.01014345046132803, -0.02170504815876484, -0.007398058660328388, 0.02711327001452446, 0.005972330924123526, -0.025879181921482086, -0.000544230337254703, -0.01498827338218689, 0.014942510984838009, 0.006236251909285784, -0.030575918033719063, -0.012129983864724636, 0.028233597055077553, 0.005977353546768427, -0.022929342463612556, 0.028612520545721054, -0.021124061197042465, 0.008509821258485317, 0.016167961061000824]
[2025-05-14 18:08:31,094]: Mean: 0.00007485
[2025-05-14 18:08:31,094]: Min: -0.04892978
[2025-05-14 18:08:31,094]: Max: 0.04759765
[2025-05-14 18:08:31,094]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 18:08:31,094]: Sample Values (25 elements): [0.9745302796363831, 0.9768102765083313, 0.9716473817825317, 0.9744697213172913, 0.9693368077278137, 0.9823831915855408, 0.9728789925575256, 0.9707342982292175, 0.9806610941886902, 0.9749568104743958, 0.9792900085449219, 0.9767083525657654, 0.9768304824829102, 0.9711235761642456, 0.9800853729248047, 0.9782111644744873, 0.9762850999832153, 0.9766151905059814, 0.9717453122138977, 0.9757435917854309, 0.9778997898101807, 0.9760795831680298, 0.9724628329277039, 0.981447696685791, 0.9743913412094116]
[2025-05-14 18:08:31,095]: Mean: 0.97584546
[2025-05-14 18:08:31,095]: Min: 0.96834081
[2025-05-14 18:08:31,095]: Max: 0.98892510
[2025-05-14 18:08:31,095]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 18:08:31,100]: Sample Values (25 elements): [-0.00841362401843071, -0.014105834066867828, -0.0014790863497182727, 0.013328115455806255, 0.0044580488465726376, -0.012994642369449139, -0.00043934540008194745, -0.0015518125146627426, -0.012286973185837269, 0.006408030167222023, 0.002919635036960244, 0.008338148705661297, 0.007332902867347002, 0.004125587176531553, -0.0006164081860333681, -0.017685316503047943, 0.0005215840064920485, -0.008729211986064911, 0.023865381255745888, 0.012315000407397747, -0.010795786045491695, -0.003237247932702303, -0.018118523061275482, -0.007615744136273861, 4.399282261147164e-05]
[2025-05-14 18:08:31,101]: Mean: 0.00002545
[2025-05-14 18:08:31,101]: Min: -0.04113847
[2025-05-14 18:08:31,101]: Max: 0.03868077
[2025-05-14 18:08:31,101]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 18:08:31,101]: Sample Values (25 elements): [0.9771546721458435, 0.9710015058517456, 0.9774596691131592, 0.9790444374084473, 0.9817039966583252, 0.9827800989151001, 0.9764202833175659, 0.9735651612281799, 0.9777446985244751, 0.9720884561538696, 0.9847182035446167, 0.977978527545929, 0.9765542149543762, 0.9796184301376343, 0.9820762276649475, 0.9815949201583862, 0.9795732498168945, 0.9837539792060852, 0.9776774644851685, 0.9872565865516663, 0.9838873744010925, 0.9741910696029663, 0.9773111343383789, 0.9833383560180664, 0.9746209979057312]
[2025-05-14 18:08:31,101]: Mean: 0.97882342
[2025-05-14 18:08:31,102]: Min: 0.96789998
[2025-05-14 18:08:31,102]: Max: 0.98972017
[2025-05-14 18:08:31,102]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 18:08:31,102]: Sample Values (25 elements): [-0.06877598166465759, 0.016011865809559822, -0.04544270411133766, 0.048046596348285675, -0.0009073223918676376, -0.046735990792512894, 0.010471121408045292, -0.06340843439102173, 0.00016854975547175854, 0.06857750564813614, -0.04819485545158386, -0.014413869008421898, 0.049478691071271896, 0.072707898914814, -0.020421896129846573, 0.014975869096815586, -0.056228503584861755, 0.002927012974396348, 0.05841771513223648, 0.06886418908834457, -0.06513968110084534, -0.0285319946706295, -0.05461423099040985, 0.049831219017505646, -0.04704127088189125]
[2025-05-14 18:08:31,102]: Mean: -0.00036823
[2025-05-14 18:08:31,103]: Min: -0.11560193
[2025-05-14 18:08:31,103]: Max: 0.11103886
[2025-05-14 18:08:31,103]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 18:08:31,103]: Sample Values (25 elements): [0.9663532376289368, 0.9599459171295166, 0.9627709984779358, 0.957445502281189, 0.9640868306159973, 0.9707514643669128, 0.9537968635559082, 0.9623382687568665, 0.9570835828781128, 0.9556987881660461, 0.9638221859931946, 0.9637953042984009, 0.9565226435661316, 0.9556943774223328, 0.9594509601593018, 0.9619660377502441, 0.9609836935997009, 0.9613013863563538, 0.9595547914505005, 0.9623010158538818, 0.9617748856544495, 0.9606989026069641, 0.9605625867843628, 0.96452796459198, 0.9633282423019409]
[2025-05-14 18:08:31,103]: Mean: 0.96127743
[2025-05-14 18:08:31,103]: Min: 0.94832379
[2025-05-14 18:08:31,103]: Max: 0.97544366
[2025-05-14 18:08:31,103]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 18:08:31,109]: Sample Values (25 elements): [0.0007390916580334306, -0.018992621451616287, 0.005448003299534321, -0.0022206087596714497, 0.0021816985681653023, -0.010097882710397243, -0.002789414254948497, -0.0004840517067350447, 0.012041836977005005, -0.019743289798498154, 0.007068675942718983, -0.007070649415254593, -0.003813238348811865, -0.0012233867309987545, -0.012435464188456535, -0.005189670715481043, -0.013685127720236778, -0.01047995314002037, -0.006547822616994381, -0.0017968742176890373, 0.011565185151994228, 0.008286552503705025, 0.02270221710205078, -0.007847392931580544, -0.018091650679707527]
[2025-05-14 18:08:31,109]: Mean: -0.00004050
[2025-05-14 18:08:31,109]: Min: -0.04066700
[2025-05-14 18:08:31,109]: Max: 0.04277448
[2025-05-14 18:08:31,109]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 18:08:31,110]: Sample Values (25 elements): [0.9773427844047546, 0.9810834527015686, 0.9828541278839111, 0.9797810912132263, 0.9788634777069092, 0.9833064079284668, 0.9823980331420898, 0.9822221398353577, 0.980874240398407, 0.9791034460067749, 0.9849027395248413, 0.9908654689788818, 0.9777781963348389, 0.9758967757225037, 0.9796956777572632, 0.9772754907608032, 0.9816178679466248, 0.9760652780532837, 0.9834493398666382, 0.9830821752548218, 0.9788047671318054, 0.979928731918335, 0.9797170162200928, 0.9772049784660339, 0.9835583567619324]
[2025-05-14 18:08:31,110]: Mean: 0.98047942
[2025-05-14 18:08:31,110]: Min: 0.96878070
[2025-05-14 18:08:31,110]: Max: 1.00435734
[2025-05-14 18:08:31,110]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 18:08:31,116]: Sample Values (25 elements): [-0.014231668785214424, 0.0030707521364092827, -0.016910327598452568, 0.027283307164907455, 0.005163629539310932, 0.007672686129808426, -0.005883034318685532, 0.0004273590457160026, 0.011114556342363358, -0.007266480941325426, 0.02047378569841385, 0.004686873406171799, -0.00958928745239973, -0.011022684164345264, -0.004357357043772936, -0.005896837916225195, -0.018212376162409782, -0.005984760820865631, -0.012618237175047398, -0.006465335842221975, 0.006867137271910906, -0.009675254113972187, -0.010646597482264042, 0.009748706594109535, 0.009519947692751884]
[2025-05-14 18:08:31,116]: Mean: 0.00000114
[2025-05-14 18:08:31,116]: Min: -0.04025758
[2025-05-14 18:08:31,116]: Max: 0.03743161
[2025-05-14 18:08:31,116]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 18:08:31,116]: Sample Values (25 elements): [0.9853010773658752, 0.9764313101768494, 0.9893364310264587, 0.9833013415336609, 0.9782126545906067, 0.984383225440979, 0.9824661016464233, 0.9806084632873535, 0.9866549968719482, 0.9811892509460449, 0.9871872067451477, 0.9765735864639282, 0.9839487075805664, 0.9803919196128845, 0.9776646494865417, 0.990757405757904, 0.9837334752082825, 0.9794759154319763, 0.973327100276947, 0.9976722598075867, 0.9870492815971375, 0.985049843788147, 0.9866772294044495, 0.9815400838851929, 0.9853118062019348]
[2025-05-14 18:08:31,117]: Mean: 0.98265350
[2025-05-14 18:08:31,117]: Min: 0.97122908
[2025-05-14 18:08:31,117]: Max: 0.99951410
[2025-05-14 18:08:31,117]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 18:08:31,128]: Sample Values (25 elements): [-0.01140607986599207, -0.009689665399491787, 0.01729251816868782, -0.00021060454309917986, -0.004620438441634178, -0.007227750960737467, -0.013775058090686798, 0.013861147686839104, -0.0007711346843279898, -0.008076195605099201, -0.012043187394738197, -0.013268311507999897, 0.0011300573823973536, 0.005724451970309019, -0.00859943125396967, 0.010447206906974316, -0.008471203967928886, -0.015811314806342125, 0.0027463827282190323, -0.014327538199722767, 0.018150346353650093, 0.01187791209667921, -0.01036533061414957, -0.013420353643596172, 0.008509326726198196]
[2025-05-14 18:08:31,129]: Mean: -0.00000764
[2025-05-14 18:08:31,129]: Min: -0.03417838
[2025-05-14 18:08:31,129]: Max: 0.03372144
[2025-05-14 18:08:31,129]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 18:08:31,129]: Sample Values (25 elements): [0.9737920761108398, 0.9720100164413452, 0.9770139455795288, 0.971623420715332, 0.9725411534309387, 0.9707983732223511, 0.9763274192810059, 0.9731229543685913, 0.9761943817138672, 0.9745691418647766, 0.974784791469574, 0.9732690453529358, 0.9723593592643738, 0.9772830605506897, 0.9756301641464233, 0.9737758040428162, 0.9706777334213257, 0.9762053489685059, 0.9704115986824036, 0.977260410785675, 0.9725047945976257, 0.9737079739570618, 0.9768006205558777, 0.9794939160346985, 0.9731329083442688]
[2025-05-14 18:08:31,130]: Mean: 0.97464281
[2025-05-14 18:08:31,130]: Min: 0.96973068
[2025-05-14 18:08:31,130]: Max: 0.98255104
[2025-05-14 18:08:31,130]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 18:08:31,167]: Sample Values (25 elements): [-0.008035645820200443, -0.003967175260186195, -0.01286726538091898, 0.005443742033094168, -0.017419038340449333, -0.0074668456800282, 0.015175669454038143, 0.004705536179244518, -0.011356120929121971, 0.012652212753891945, -0.007283665705472231, 0.0072766379453241825, -0.004427563399076462, 0.00950413104146719, -0.011223984882235527, 0.012107688933610916, 0.001059080590493977, -0.0044739446602761745, -0.0005573928938247263, -0.00785053800791502, 0.0007782012689858675, 0.011520398780703545, 0.005658045876771212, 0.0033702868968248367, -0.0019091286230832338]
[2025-05-14 18:08:31,167]: Mean: 0.00000195
[2025-05-14 18:08:31,168]: Min: -0.02715929
[2025-05-14 18:08:31,168]: Max: 0.02623817
[2025-05-14 18:08:31,168]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 18:08:31,168]: Sample Values (25 elements): [0.9747087359428406, 0.9757881760597229, 0.9774237871170044, 0.9742703437805176, 0.9775570631027222, 0.9790842533111572, 0.975735604763031, 0.9761542081832886, 0.9774543642997742, 0.9724889993667603, 0.9775896668434143, 0.975792407989502, 0.9785819053649902, 0.98197340965271, 0.9746928215026855, 0.9770116209983826, 0.9759058952331543, 0.9766724705696106, 0.9775791168212891, 0.984760046005249, 0.9784976243972778, 0.9761909246444702, 0.9730947613716125, 0.9739514589309692, 0.9748749732971191]
[2025-05-14 18:08:31,168]: Mean: 0.97662795
[2025-05-14 18:08:31,169]: Min: 0.97076726
[2025-05-14 18:08:31,169]: Max: 0.98804772
[2025-05-14 18:08:31,169]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 18:08:31,170]: Sample Values (25 elements): [-0.05127666890621185, 0.027065077796578407, 0.007677736226469278, -0.05133106932044029, 0.007424425333738327, 0.004410407971590757, 0.012719210237264633, 0.012387057766318321, -0.0499737411737442, -0.02047228254377842, 0.04519610106945038, 0.04181840643286705, -0.05766451731324196, -0.018746981397271156, 0.014206687919795513, 0.04772739112377167, -0.01617891527712345, -0.035559799522161484, 0.03751187399029732, 0.06257989257574081, 0.0514976941049099, -0.0029041075613349676, -0.05614171177148819, 0.0158629659563303, 0.006793098524212837]
[2025-05-14 18:08:31,170]: Mean: -0.00006007
[2025-05-14 18:08:31,170]: Min: -0.07531282
[2025-05-14 18:08:31,170]: Max: 0.07441591
[2025-05-14 18:08:31,171]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 18:08:31,171]: Sample Values (25 elements): [0.9702202081680298, 0.9670171737670898, 0.9702105522155762, 0.9696203470230103, 0.9702473878860474, 0.9683789014816284, 0.9666222333908081, 0.9692934155464172, 0.9685249924659729, 0.9675604104995728, 0.9690413475036621, 0.9673401713371277, 0.9664537310600281, 0.9672561287879944, 0.9666707515716553, 0.9700736999511719, 0.9682819843292236, 0.9683773517608643, 0.9683470726013184, 0.9686667323112488, 0.9669291973114014, 0.9681838154792786, 0.9696328639984131, 0.9683622717857361, 0.9697049260139465]
[2025-05-14 18:08:31,171]: Mean: 0.96812063
[2025-05-14 18:08:31,171]: Min: 0.96283609
[2025-05-14 18:08:31,171]: Max: 0.97573501
[2025-05-14 18:08:31,171]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 18:08:31,209]: Sample Values (25 elements): [-0.006140163168311119, -0.010079849511384964, -0.010600537993013859, 0.006765819154679775, -0.005569629371166229, 0.0046333675272762775, 0.01102486252784729, -0.0016397949075326324, -0.009079115465283394, 0.0060538724064826965, 5.7995886891148984e-05, -0.0038193040527403355, -0.012515082955360413, 0.007662810850888491, -0.00010012617713073269, -0.009169704280793667, 0.004098110366612673, -0.008428752422332764, -0.006869127042591572, 0.0009264166583307087, 0.008574338629841805, -0.003798744408413768, 0.011986100114881992, -0.0025826003402471542, -0.0057181487791240215]
[2025-05-14 18:08:31,209]: Mean: -0.00000403
[2025-05-14 18:08:31,209]: Min: -0.02499244
[2025-05-14 18:08:31,209]: Max: 0.02773419
[2025-05-14 18:08:31,209]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 18:08:31,210]: Sample Values (25 elements): [0.9745692014694214, 0.9738889336585999, 0.9778240919113159, 0.978461742401123, 0.9737633466720581, 0.9770907759666443, 0.9780935645103455, 0.9787527322769165, 0.9731263518333435, 0.9724968075752258, 0.9819352030754089, 0.9763340353965759, 0.9721226692199707, 0.9716918468475342, 0.9745492935180664, 0.9741784334182739, 0.975936233997345, 0.9745421409606934, 0.9765311479568481, 0.9727240204811096, 0.9798012375831604, 0.9835298657417297, 0.9736023545265198, 0.97465580701828, 0.9757163524627686]
[2025-05-14 18:08:31,210]: Mean: 0.97540152
[2025-05-14 18:08:31,210]: Min: 0.96971703
[2025-05-14 18:08:31,210]: Max: 0.98744607
[2025-05-14 18:08:31,210]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 18:08:31,249]: Sample Values (25 elements): [0.010166521184146404, 0.006204153876751661, 0.005358510185033083, -0.01366175152361393, 0.0046479469165205956, 0.004369392525404692, 0.014487006701529026, 0.010665449313819408, -0.013360941782593727, 0.003026895457878709, -0.014264187775552273, -0.0021022979635745287, 0.0030622854828834534, -5.8865949540631846e-05, 0.01491792406886816, -0.004265987779945135, -0.00011280647595413029, 0.013470619916915894, -0.006039582192897797, -0.0038455824833363295, 0.003653097664937377, 0.008089338429272175, 0.0029393667355179787, -0.006192052271217108, -0.008085137233138084]
[2025-05-14 18:08:31,249]: Mean: -0.00000390
[2025-05-14 18:08:31,249]: Min: -0.02205843
[2025-05-14 18:08:31,250]: Max: 0.02157601
[2025-05-14 18:08:31,250]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 18:08:31,250]: Sample Values (25 elements): [0.9781715869903564, 0.9784566164016724, 0.9796180725097656, 0.97617107629776, 0.9751098155975342, 0.9770143628120422, 0.9781215786933899, 0.979077935218811, 0.9793249368667603, 0.9767360091209412, 0.9813533425331116, 0.9798092246055603, 0.9802891612052917, 0.9800176620483398, 0.9777868390083313, 0.9783573746681213, 0.9824284911155701, 0.9812118411064148, 0.9805900454521179, 0.9769485592842102, 0.9760338068008423, 0.9783036708831787, 0.9788541793823242, 0.9791852235794067, 0.9779258370399475]
[2025-05-14 18:08:31,250]: Mean: 0.97917110
[2025-05-14 18:08:31,250]: Min: 0.97326905
[2025-05-14 18:08:31,250]: Max: 0.98563963
[2025-05-14 18:08:31,251]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 18:08:31,251]: Sample Values (25 elements): [0.08264361321926117, 0.0005457382067106664, -0.08819033950567245, -0.07264001667499542, 0.0815388634800911, -0.06276707351207733, 0.04262511059641838, -0.06315658986568451, 0.002352264244109392, 0.03585571423172951, -0.025612814351916313, 0.07376305758953094, 0.016590630635619164, -0.022751260548830032, -0.01094128843396902, 0.028446704149246216, 0.03056555800139904, -0.051871467381715775, -0.009713899344205856, 0.0495094358921051, -0.05573489889502525, 0.08359234780073166, -0.07686546444892883, 0.08399098366498947, 0.09054394066333771]
[2025-05-14 18:08:31,251]: Mean: 0.00007920
[2025-05-14 18:08:31,251]: Min: -0.14287896
[2025-05-14 18:08:31,251]: Max: 0.13702309
[2025-05-14 18:08:31,251]: 


QAT of ResNet18 with hardtanh down to 4 bits...
[2025-05-14 18:08:31,470]: [ResNet18_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-14 18:08:31,515]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 18:10:09,385]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.3612 Train Acc: 0.8715 Eval Loss: 0.6215 Eval Acc: 0.8154 (LR: 0.001000)
[2025-05-14 18:11:47,428]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.3185 Train Acc: 0.8864 Eval Loss: 0.6130 Eval Acc: 0.8171 (LR: 0.001000)
[2025-05-14 18:13:25,709]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.3123 Train Acc: 0.8890 Eval Loss: 0.6952 Eval Acc: 0.7995 (LR: 0.001000)
[2025-05-14 18:15:03,764]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.3062 Train Acc: 0.8903 Eval Loss: 0.6094 Eval Acc: 0.8197 (LR: 0.001000)
[2025-05-14 18:16:41,811]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.2979 Train Acc: 0.8947 Eval Loss: 0.5785 Eval Acc: 0.8246 (LR: 0.001000)
[2025-05-14 18:18:19,875]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.2920 Train Acc: 0.8951 Eval Loss: 0.6565 Eval Acc: 0.8068 (LR: 0.001000)
[2025-05-14 18:19:57,730]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.2836 Train Acc: 0.8989 Eval Loss: 0.5456 Eval Acc: 0.8351 (LR: 0.001000)
[2025-05-14 18:21:35,602]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.2799 Train Acc: 0.9009 Eval Loss: 0.6242 Eval Acc: 0.8166 (LR: 0.001000)
[2025-05-14 18:23:13,460]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.2732 Train Acc: 0.9022 Eval Loss: 0.6269 Eval Acc: 0.8186 (LR: 0.001000)
[2025-05-14 18:24:51,298]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.2706 Train Acc: 0.9026 Eval Loss: 0.5567 Eval Acc: 0.8333 (LR: 0.001000)
[2025-05-14 18:26:29,136]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.2612 Train Acc: 0.9079 Eval Loss: 0.5560 Eval Acc: 0.8347 (LR: 0.001000)
[2025-05-14 18:28:06,975]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.2583 Train Acc: 0.9070 Eval Loss: 0.5644 Eval Acc: 0.8356 (LR: 0.001000)
[2025-05-14 18:29:45,048]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.2576 Train Acc: 0.9073 Eval Loss: 0.5535 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-14 18:31:22,919]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.2468 Train Acc: 0.9124 Eval Loss: 0.6188 Eval Acc: 0.8245 (LR: 0.001000)
[2025-05-14 18:33:00,750]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.2478 Train Acc: 0.9116 Eval Loss: 0.5663 Eval Acc: 0.8353 (LR: 0.001000)
[2025-05-14 18:34:38,588]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.2451 Train Acc: 0.9131 Eval Loss: 0.6219 Eval Acc: 0.8223 (LR: 0.001000)
[2025-05-14 18:36:16,422]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.2354 Train Acc: 0.9171 Eval Loss: 0.5213 Eval Acc: 0.8471 (LR: 0.001000)
[2025-05-14 18:37:54,263]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.2381 Train Acc: 0.9143 Eval Loss: 0.5686 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-14 18:39:32,140]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.2252 Train Acc: 0.9204 Eval Loss: 0.6169 Eval Acc: 0.8201 (LR: 0.001000)
[2025-05-14 18:41:11,694]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.2271 Train Acc: 0.9188 Eval Loss: 0.5839 Eval Acc: 0.8311 (LR: 0.001000)
[2025-05-14 18:42:49,452]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.2221 Train Acc: 0.9209 Eval Loss: 0.5930 Eval Acc: 0.8310 (LR: 0.001000)
[2025-05-14 18:44:27,286]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.2180 Train Acc: 0.9231 Eval Loss: 0.5659 Eval Acc: 0.8382 (LR: 0.001000)
[2025-05-14 18:46:05,132]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.2183 Train Acc: 0.9211 Eval Loss: 0.5619 Eval Acc: 0.8419 (LR: 0.001000)
[2025-05-14 18:47:43,004]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.2092 Train Acc: 0.9240 Eval Loss: 0.5996 Eval Acc: 0.8326 (LR: 0.001000)
[2025-05-14 18:49:20,861]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.2087 Train Acc: 0.9255 Eval Loss: 0.5571 Eval Acc: 0.8410 (LR: 0.001000)
[2025-05-14 18:50:58,720]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.2104 Train Acc: 0.9247 Eval Loss: 0.5727 Eval Acc: 0.8402 (LR: 0.001000)
[2025-05-14 18:52:36,856]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.2000 Train Acc: 0.9294 Eval Loss: 0.5635 Eval Acc: 0.8394 (LR: 0.001000)
[2025-05-14 18:54:27,869]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.2007 Train Acc: 0.9285 Eval Loss: 0.5930 Eval Acc: 0.8427 (LR: 0.001000)
[2025-05-14 18:56:05,898]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.1962 Train Acc: 0.9299 Eval Loss: 0.5787 Eval Acc: 0.8391 (LR: 0.001000)
[2025-05-14 18:57:43,922]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.1977 Train Acc: 0.9293 Eval Loss: 0.5787 Eval Acc: 0.8445 (LR: 0.000250)
[2025-05-14 18:59:21,959]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.1216 Train Acc: 0.9604 Eval Loss: 0.4762 Eval Acc: 0.8668 (LR: 0.000250)
[2025-05-14 19:01:00,036]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.1081 Train Acc: 0.9643 Eval Loss: 0.4729 Eval Acc: 0.8674 (LR: 0.000250)
[2025-05-14 19:02:38,094]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.1034 Train Acc: 0.9657 Eval Loss: 0.4871 Eval Acc: 0.8674 (LR: 0.000250)
[2025-05-14 19:04:29,232]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.1005 Train Acc: 0.9661 Eval Loss: 0.4943 Eval Acc: 0.8675 (LR: 0.000250)
[2025-05-14 19:06:07,264]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.0998 Train Acc: 0.9667 Eval Loss: 0.4815 Eval Acc: 0.8705 (LR: 0.000250)
[2025-05-14 19:07:45,013]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.0941 Train Acc: 0.9686 Eval Loss: 0.4942 Eval Acc: 0.8669 (LR: 0.000250)
[2025-05-14 19:09:22,797]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.0918 Train Acc: 0.9690 Eval Loss: 0.5038 Eval Acc: 0.8650 (LR: 0.000250)
[2025-05-14 19:11:00,801]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.0900 Train Acc: 0.9699 Eval Loss: 0.4989 Eval Acc: 0.8649 (LR: 0.000250)
[2025-05-14 19:12:38,661]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.0883 Train Acc: 0.9699 Eval Loss: 0.5172 Eval Acc: 0.8638 (LR: 0.000250)
[2025-05-14 19:14:18,337]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.0875 Train Acc: 0.9703 Eval Loss: 0.5228 Eval Acc: 0.8632 (LR: 0.000250)
[2025-05-14 19:15:56,236]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.0886 Train Acc: 0.9696 Eval Loss: 0.5059 Eval Acc: 0.8684 (LR: 0.000250)
[2025-05-14 19:17:34,065]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.0853 Train Acc: 0.9712 Eval Loss: 0.5293 Eval Acc: 0.8625 (LR: 0.000250)
[2025-05-14 19:19:12,056]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.0850 Train Acc: 0.9712 Eval Loss: 0.5071 Eval Acc: 0.8681 (LR: 0.000250)
[2025-05-14 19:20:50,029]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.0830 Train Acc: 0.9723 Eval Loss: 0.5145 Eval Acc: 0.8654 (LR: 0.000250)
[2025-05-14 19:22:28,048]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.0833 Train Acc: 0.9721 Eval Loss: 0.5130 Eval Acc: 0.8694 (LR: 0.000063)
[2025-05-14 19:24:06,006]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.0688 Train Acc: 0.9778 Eval Loss: 0.5023 Eval Acc: 0.8701 (LR: 0.000063)
[2025-05-14 19:25:44,055]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.0656 Train Acc: 0.9791 Eval Loss: 0.4968 Eval Acc: 0.8704 (LR: 0.000063)
[2025-05-14 19:27:22,052]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.0658 Train Acc: 0.9793 Eval Loss: 0.4968 Eval Acc: 0.8709 (LR: 0.000063)
[2025-05-14 19:29:00,310]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.0657 Train Acc: 0.9796 Eval Loss: 0.4981 Eval Acc: 0.8727 (LR: 0.000063)
[2025-05-14 19:30:38,472]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.0640 Train Acc: 0.9793 Eval Loss: 0.4992 Eval Acc: 0.8740 (LR: 0.000063)
[2025-05-14 19:32:16,670]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.0629 Train Acc: 0.9799 Eval Loss: 0.5037 Eval Acc: 0.8711 (LR: 0.000063)
[2025-05-14 19:33:54,674]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.0623 Train Acc: 0.9804 Eval Loss: 0.5084 Eval Acc: 0.8699 (LR: 0.000063)
[2025-05-14 19:35:32,671]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.0622 Train Acc: 0.9798 Eval Loss: 0.5050 Eval Acc: 0.8702 (LR: 0.000063)
[2025-05-14 19:37:10,488]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.0615 Train Acc: 0.9803 Eval Loss: 0.5015 Eval Acc: 0.8721 (LR: 0.000063)
[2025-05-14 19:38:48,312]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.0614 Train Acc: 0.9804 Eval Loss: 0.5041 Eval Acc: 0.8710 (LR: 0.000063)
[2025-05-14 19:40:26,313]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.0608 Train Acc: 0.9808 Eval Loss: 0.5106 Eval Acc: 0.8695 (LR: 0.000063)
[2025-05-14 19:42:04,319]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.0589 Train Acc: 0.9816 Eval Loss: 0.5116 Eval Acc: 0.8689 (LR: 0.000063)
[2025-05-14 19:43:41,908]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.0611 Train Acc: 0.9805 Eval Loss: 0.5049 Eval Acc: 0.8706 (LR: 0.000063)
[2025-05-14 19:45:19,710]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.0592 Train Acc: 0.9818 Eval Loss: 0.5054 Eval Acc: 0.8705 (LR: 0.000063)
[2025-05-14 19:46:59,123]: [ResNet18_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.0580 Train Acc: 0.9821 Eval Loss: 0.5068 Eval Acc: 0.8692 (LR: 0.000063)
[2025-05-14 19:46:59,123]: [ResNet18_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8740
[2025-05-14 19:46:59,200]: 


Quantization of model down to 4 bits finished
[2025-05-14 19:46:59,200]: Model Architecture:
[2025-05-14 19:46:59,249]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0218], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1798500269651413, max_val=0.14738152921199799)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0305], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20320646464824677, max_val=0.2546539306640625)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21113938093185425, max_val=0.17315058410167694)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11698731034994125, max_val=0.11733811348676682)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0159], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12028631567955017, max_val=0.1182887926697731)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07476264238357544, max_val=0.0744197741150856)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0237], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1781294047832489, max_val=0.17726826667785645)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06503809988498688, max_val=0.06350355595350266)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0084], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06234206631779671, max_val=0.063868947327137)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0076], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0537896566092968, max_val=0.06033336743712425)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0063], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.049188729375600815, max_val=0.04604092612862587)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0166], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12737718224525452, max_val=0.12112191319465637)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05052576959133148, max_val=0.05248511955142021)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0060], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04683930426836014, max_val=0.04290188476443291)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0051], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03813626617193222, max_val=0.03843626379966736)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.029984090477228165, max_val=0.028896328061819077)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0106], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08076779544353485, max_val=0.07868325710296631)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02753552794456482, max_val=0.031984660774469376)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.022790666669607162, max_val=0.02309805154800415)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 19:46:59,249]: 
Model Weights:
[2025-05-14 19:46:59,249]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 19:46:59,249]: Sample Values (25 elements): [0.2946646809577942, -0.18314340710639954, 0.0985790267586708, 0.04386530816555023, 0.05407191812992096, -0.16015967726707458, 0.08986859768629074, -0.06998397409915924, -0.17558899521827698, -0.1611175388097763, -0.16820380091667175, -0.27342426776885986, -0.14548148214817047, 0.19151031970977783, -0.1446538269519806, -0.17420606315135956, -0.019295668229460716, 0.18152743577957153, -0.020840058103203773, -0.22265475988388062, -0.14983505010604858, 0.1344822645187378, 0.19571305811405182, -0.12041592597961426, 0.12868772447109222]
[2025-05-14 19:46:59,250]: Mean: 0.00067041
[2025-05-14 19:46:59,250]: Min: -0.37338424
[2025-05-14 19:46:59,250]: Max: 0.37583983
[2025-05-14 19:46:59,250]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 19:46:59,250]: Sample Values (25 elements): [0.9935181140899658, 0.9625198245048523, 0.8438217043876648, 1.051175832748413, 0.9775729179382324, 0.9688712358474731, 0.9150222539901733, 0.8507025241851807, 0.8974310159683228, 1.0289027690887451, 0.960620105266571, 0.959358274936676, 0.8787046670913696, 0.8958011865615845, 0.9661245942115784, 1.046305537223816, 0.8985477089881897, 0.7915657162666321, 0.9979164004325867, 0.8990873098373413, 0.9993521571159363, 0.8604260683059692, 0.8914842009544373, 0.8259276151657104, 0.9615530371665955]
[2025-05-14 19:46:59,250]: Mean: 0.91968107
[2025-05-14 19:46:59,251]: Min: 0.70279866
[2025-05-14 19:46:59,251]: Max: 1.09346962
[2025-05-14 19:46:59,252]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 19:46:59,252]: Sample Values (25 elements): [0.021815428510308266, 0.0, 0.021815428510308266, -0.021815428510308266, 0.06544628739356995, 0.021815428510308266, 0.021815428510308266, -0.021815428510308266, 0.0, 0.04363085702061653, 0.021815428510308266, 0.06544628739356995, 0.06544628739356995, -0.04363085702061653, -0.021815428510308266, -0.04363085702061653, 0.021815428510308266, 0.0, 0.0, -0.04363085702061653, 0.0, 0.0, 0.0, -0.04363085702061653, 0.0]
[2025-05-14 19:46:59,253]: Mean: 0.00010356
[2025-05-14 19:46:59,253]: Min: -0.17452343
[2025-05-14 19:46:59,253]: Max: 0.15270799
[2025-05-14 19:46:59,253]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 19:46:59,253]: Sample Values (25 elements): [0.9907060265541077, 0.9330039620399475, 0.9967333674430847, 1.0288515090942383, 0.9651532769203186, 0.9851900935173035, 1.010618805885315, 0.9450433254241943, 0.9258914589881897, 0.9305642247200012, 1.0024480819702148, 0.912222683429718, 0.9571610689163208, 1.038770318031311, 0.9967730045318604, 1.0003650188446045, 0.9736388921737671, 0.9550283551216125, 0.9514644145965576, 0.9774523973464966, 1.0159804821014404, 0.9620944261550903, 0.9406251907348633, 0.9179814457893372, 1.0048884153366089]
[2025-05-14 19:46:59,253]: Mean: 0.97822869
[2025-05-14 19:46:59,254]: Min: 0.89863575
[2025-05-14 19:46:59,254]: Max: 1.07727575
[2025-05-14 19:46:59,255]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 19:46:59,255]: Sample Values (25 elements): [-0.0305239986628294, -0.0305239986628294, 0.0610479973256588, 0.0, -0.0305239986628294, -0.0305239986628294, 0.0305239986628294, 0.0305239986628294, 0.0305239986628294, 0.0, 0.0305239986628294, 0.0, 0.0, 0.0, -0.0305239986628294, 0.0610479973256588, 0.0, 0.0305239986628294, 0.0, 0.0305239986628294, -0.0305239986628294, 0.0305239986628294, 0.0, -0.0305239986628294, 0.0305239986628294]
[2025-05-14 19:46:59,255]: Mean: 0.00008197
[2025-05-14 19:46:59,256]: Min: -0.21366799
[2025-05-14 19:46:59,256]: Max: 0.24419199
[2025-05-14 19:46:59,256]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 19:46:59,256]: Sample Values (25 elements): [0.9786561131477356, 0.9203317761421204, 0.959189236164093, 0.8968362808227539, 0.9050443172454834, 0.9194969534873962, 0.9159339666366577, 0.9797869920730591, 0.9707475304603577, 0.8700902462005615, 0.9659534692764282, 0.9455647468566895, 0.9226173162460327, 1.2358191013336182, 0.9084969758987427, 0.908236026763916, 0.9464578628540039, 0.9814799427986145, 0.9324665665626526, 0.9332431554794312, 0.9592841863632202, 0.9666273593902588, 0.9006121754646301, 0.9227176308631897, 0.9227016568183899]
[2025-05-14 19:46:59,256]: Mean: 0.95442784
[2025-05-14 19:46:59,256]: Min: 0.86294436
[2025-05-14 19:46:59,257]: Max: 1.23581910
[2025-05-14 19:46:59,258]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 19:46:59,258]: Sample Values (25 elements): [0.0, 0.0, -0.025619326159358025, 0.05123865231871605, -0.05123865231871605, 0.0, -0.05123865231871605, -0.12809662520885468, -0.05123865231871605, 0.0, 0.0, 0.05123865231871605, 0.0, 0.025619326159358025, 0.05123865231871605, 0.025619326159358025, -0.025619326159358025, 0.025619326159358025, 0.05123865231871605, 0.0, 0.025619326159358025, 0.025619326159358025, -0.05123865231871605, 0.05123865231871605, 0.0]
[2025-05-14 19:46:59,258]: Mean: -0.00000695
[2025-05-14 19:46:59,258]: Min: -0.20495461
[2025-05-14 19:46:59,259]: Max: 0.17933528
[2025-05-14 19:46:59,259]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 19:46:59,259]: Sample Values (25 elements): [0.9597380757331848, 1.0435229539871216, 0.9616777300834656, 0.9939773678779602, 0.9761987328529358, 1.023197054862976, 0.9869876503944397, 1.011611819267273, 0.9857795834541321, 0.9670512676239014, 0.9550565481185913, 0.9507636427879333, 1.0176647901535034, 0.983030378818512, 1.002314567565918, 0.975286066532135, 0.9428585767745972, 0.9742758274078369, 0.9863885045051575, 0.9737790822982788, 1.0206615924835205, 0.944214940071106, 0.9563363194465637, 1.0127265453338623, 1.0437626838684082]
[2025-05-14 19:46:59,259]: Mean: 0.98905247
[2025-05-14 19:46:59,259]: Min: 0.94248384
[2025-05-14 19:46:59,259]: Max: 1.04434156
[2025-05-14 19:46:59,260]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 19:46:59,261]: Sample Values (25 elements): [0.0, -0.031243350356817245, -0.015621675178408623, 0.04686502367258072, 0.04686502367258072, 0.031243350356817245, 0.031243350356817245, -0.031243350356817245, -0.04686502367258072, -0.04686502367258072, -0.031243350356817245, 0.0, 0.015621675178408623, 0.015621675178408623, -0.015621675178408623, -0.015621675178408623, 0.015621675178408623, -0.015621675178408623, -0.015621675178408623, 0.031243350356817245, -0.015621675178408623, 0.0, 0.015621675178408623, 0.0, 0.015621675178408623]
[2025-05-14 19:46:59,261]: Mean: 0.00028689
[2025-05-14 19:46:59,261]: Min: -0.10935172
[2025-05-14 19:46:59,261]: Max: 0.12497340
[2025-05-14 19:46:59,261]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 19:46:59,262]: Sample Values (25 elements): [0.9470430612564087, 0.9431220889091492, 0.940980851650238, 0.9520111680030823, 0.9262587428092957, 0.9577264785766602, 0.9452112913131714, 0.9501662254333496, 0.88636714220047, 0.9133992791175842, 0.9420201182365417, 0.9408937692642212, 0.9486871361732483, 0.95066237449646, 0.9756547212600708, 0.9587375521659851, 0.9680428504943848, 0.9922903776168823, 0.9282829165458679, 1.093923807144165, 0.8858413696289062, 0.9372678995132446, 0.9661059379577637, 0.9811319708824158, 0.9537053108215332]
[2025-05-14 19:46:59,262]: Mean: 0.95917654
[2025-05-14 19:46:59,262]: Min: 0.88584137
[2025-05-14 19:46:59,262]: Max: 1.09393752
[2025-05-14 19:46:59,263]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 19:46:59,264]: Sample Values (25 elements): [0.0, -0.03181001543998718, -0.01590500771999359, 0.047715023159980774, -0.03181001543998718, -0.03181001543998718, -0.047715023159980774, 0.03181001543998718, 0.0, -0.01590500771999359, -0.01590500771999359, 0.01590500771999359, -0.01590500771999359, 0.0, 0.03181001543998718, 0.047715023159980774, 0.0, 0.0, -0.01590500771999359, 0.01590500771999359, -0.06362003087997437, 0.0, 0.01590500771999359, 0.0, -0.01590500771999359]
[2025-05-14 19:46:59,264]: Mean: 0.00009233
[2025-05-14 19:46:59,264]: Min: -0.12724006
[2025-05-14 19:46:59,264]: Max: 0.11133505
[2025-05-14 19:46:59,265]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 19:46:59,265]: Sample Values (25 elements): [0.9739527702331543, 0.9584909677505493, 0.9691635370254517, 0.9579727649688721, 0.9910252094268799, 0.9698628187179565, 0.977003812789917, 0.965869128704071, 1.021337628364563, 0.9633766412734985, 0.9802240133285522, 0.9663976430892944, 0.9729483723640442, 0.9793056845664978, 0.9718927145004272, 0.9709426164627075, 0.9644649028778076, 0.9784826636314392, 0.9906547665596008, 0.9479566216468811, 0.9646758437156677, 0.9773343205451965, 0.9779504537582397, 0.9734686017036438, 0.9648945331573486]
[2025-05-14 19:46:59,265]: Mean: 0.97045648
[2025-05-14 19:46:59,265]: Min: 0.94795662
[2025-05-14 19:46:59,265]: Max: 1.02133763
[2025-05-14 19:46:59,266]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 19:46:59,268]: Sample Values (25 elements): [-0.029836442321538925, -0.019890962168574333, -0.019890962168574333, -0.019890962168574333, 0.0, -0.009945481084287167, -0.009945481084287167, 0.019890962168574333, 0.0, 0.019890962168574333, 0.0, 0.0, 0.009945481084287167, 0.009945481084287167, -0.029836442321538925, -0.029836442321538925, 0.009945481084287167, -0.009945481084287167, 0.019890962168574333, -0.04972740635275841, 0.009945481084287167, 0.029836442321538925, 0.009945481084287167, -0.019890962168574333, -0.019890962168574333]
[2025-05-14 19:46:59,268]: Mean: -0.00000938
[2025-05-14 19:46:59,268]: Min: -0.07956385
[2025-05-14 19:46:59,268]: Max: 0.06961837
[2025-05-14 19:46:59,268]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 19:46:59,269]: Sample Values (25 elements): [0.9782677292823792, 0.9582773447036743, 0.9734297394752502, 0.9675830602645874, 0.9710839986801147, 0.9807102084159851, 0.9649791717529297, 0.96864914894104, 0.9915313720703125, 0.978359043598175, 0.9643207788467407, 0.9720813632011414, 0.979595959186554, 0.971410870552063, 0.9527660012245178, 0.9724129438400269, 0.9592471122741699, 0.9830448627471924, 0.9656471610069275, 0.9698290228843689, 0.9556903839111328, 0.9777873754501343, 0.9792174100875854, 0.9664084911346436, 0.9640172719955444]
[2025-05-14 19:46:59,269]: Mean: 0.96896052
[2025-05-14 19:46:59,269]: Min: 0.94119811
[2025-05-14 19:46:59,269]: Max: 0.99438202
[2025-05-14 19:46:59,270]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 19:46:59,270]: Sample Values (25 elements): [-0.04738636314868927, -0.09477272629737854, 0.09477272629737854, 0.023693181574344635, -0.023693181574344635, 0.0710795447230339, 0.0710795447230339, 0.0710795447230339, 0.1421590894460678, 0.023693181574344635, 0.0, -0.11846590787172318, 0.023693181574344635, -0.09477272629737854, -0.04738636314868927, 0.023693181574344635, -0.11846590787172318, 0.0710795447230339, 0.0710795447230339, 0.11846590787172318, 0.0, 0.0, -0.04738636314868927, 0.0710795447230339, -0.04738636314868927]
[2025-05-14 19:46:59,270]: Mean: 0.00013015
[2025-05-14 19:46:59,271]: Min: -0.18954545
[2025-05-14 19:46:59,271]: Max: 0.16585228
[2025-05-14 19:46:59,271]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 19:46:59,271]: Sample Values (25 elements): [0.9339513778686523, 0.9162030816078186, 0.9210695624351501, 0.9362245202064514, 0.9212125539779663, 0.9830597639083862, 0.9179701805114746, 0.9364674091339111, 0.9413264989852905, 0.9209285378456116, 0.9273863434791565, 0.9181952476501465, 0.9394331574440002, 0.9454265832901001, 0.9230294823646545, 0.9363128542900085, 0.9529404640197754, 0.948878824710846, 0.9309356808662415, 0.934020459651947, 0.9518856406211853, 0.9166825413703918, 0.936765730381012, 0.9235772490501404, 0.9347224831581116]
[2025-05-14 19:46:59,271]: Mean: 0.93484986
[2025-05-14 19:46:59,271]: Min: 0.90539235
[2025-05-14 19:46:59,272]: Max: 0.98305976
[2025-05-14 19:46:59,273]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 19:46:59,274]: Sample Values (25 elements): [-0.01713888719677925, -0.01713888719677925, 0.01713888719677925, 0.04284721612930298, 0.01713888719677925, -0.04284721612930298, 0.008569443598389626, -0.01713888719677925, -0.01713888719677925, 0.01713888719677925, -0.01713888719677925, 0.008569443598389626, 0.01713888719677925, -0.025708330795168877, 0.0, 0.008569443598389626, -0.01713888719677925, -0.01713888719677925, -0.008569443598389626, -0.01713888719677925, 0.01713888719677925, 0.01713888719677925, 0.01713888719677925, 0.008569443598389626, -0.008569443598389626]
[2025-05-14 19:46:59,274]: Mean: -0.00004707
[2025-05-14 19:46:59,274]: Min: -0.06855555
[2025-05-14 19:46:59,275]: Max: 0.05998611
[2025-05-14 19:46:59,275]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 19:46:59,275]: Sample Values (25 elements): [0.9834827780723572, 0.9946056604385376, 0.9997904300689697, 0.9975936412811279, 0.9889693856239319, 1.0264321565628052, 0.9732173681259155, 0.9735583066940308, 0.9974074959754944, 0.9906094670295715, 0.9710990786552429, 0.9813441634178162, 0.984257161617279, 0.9600375890731812, 1.021773338317871, 0.9672971963882446, 0.9607211947441101, 0.9748378992080688, 0.9716697931289673, 0.9939613342285156, 0.9715551733970642, 0.9658728241920471, 0.9898198843002319, 0.9706580638885498, 0.9668604731559753]
[2025-05-14 19:46:59,275]: Mean: 0.97997695
[2025-05-14 19:46:59,275]: Min: 0.95708913
[2025-05-14 19:46:59,275]: Max: 1.02643216
[2025-05-14 19:46:59,276]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 19:46:59,278]: Sample Values (25 elements): [0.008414057083427906, -0.016828114166855812, -0.042070284485816956, 0.025242172181606293, -0.016828114166855812, 0.016828114166855812, 0.008414057083427906, -0.025242172181606293, -0.008414057083427906, 0.016828114166855812, 0.016828114166855812, 0.016828114166855812, 0.008414057083427906, -0.033656228333711624, 0.025242172181606293, 0.016828114166855812, -0.008414057083427906, -0.008414057083427906, -0.008414057083427906, 0.008414057083427906, -0.033656228333711624, 0.016828114166855812, 0.008414057083427906, 0.008414057083427906, -0.016828114166855812]
[2025-05-14 19:46:59,278]: Mean: 0.00003703
[2025-05-14 19:46:59,278]: Min: -0.05889840
[2025-05-14 19:46:59,278]: Max: 0.06731246
[2025-05-14 19:46:59,278]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 19:46:59,279]: Sample Values (25 elements): [0.9836263656616211, 0.9803259372711182, 0.9717842936515808, 0.9723137021064758, 0.9852084517478943, 0.9678208231925964, 0.9664677977561951, 0.969204306602478, 0.9741867780685425, 0.9768896698951721, 0.9786384105682373, 0.97551029920578, 0.9957389235496521, 0.9799859523773193, 0.9869493246078491, 0.9771700501441956, 0.9633164405822754, 0.9779515862464905, 0.967234194278717, 0.9794144630432129, 0.9701191186904907, 0.982090175151825, 0.9804893732070923, 0.9638490676879883, 0.9801545739173889]
[2025-05-14 19:46:59,279]: Mean: 0.97733778
[2025-05-14 19:46:59,279]: Min: 0.94852114
[2025-05-14 19:46:59,279]: Max: 1.01082635
[2025-05-14 19:46:59,280]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 19:46:59,283]: Sample Values (25 elements): [0.0, 0.030432814732193947, 0.015216407366096973, 0.022824611514806747, -0.030432814732193947, -0.015216407366096973, -0.007608203683048487, -0.015216407366096973, 0.007608203683048487, 0.015216407366096973, 0.0, 0.007608203683048487, -0.030432814732193947, -0.015216407366096973, -0.015216407366096973, 0.0, -0.007608203683048487, -0.015216407366096973, 0.015216407366096973, -0.015216407366096973, 0.007608203683048487, 0.007608203683048487, -0.015216407366096973, 0.022824611514806747, 0.015216407366096973]
[2025-05-14 19:46:59,283]: Mean: 0.00008900
[2025-05-14 19:46:59,283]: Min: -0.05325742
[2025-05-14 19:46:59,283]: Max: 0.06086563
[2025-05-14 19:46:59,283]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 19:46:59,284]: Sample Values (25 elements): [0.9633448719978333, 0.9729054570198059, 0.9722821712493896, 0.9684855341911316, 0.9699177145957947, 0.9617188572883606, 0.9579982161521912, 0.9759702086448669, 0.956236720085144, 0.9721909165382385, 0.9641081690788269, 0.9650938510894775, 0.9626886248588562, 0.9648363590240479, 0.9722456336021423, 0.9737889170646667, 0.9697020649909973, 0.9607022404670715, 0.9673517942428589, 0.9699711799621582, 0.9625893235206604, 0.9886044859886169, 0.9747786521911621, 0.9612481594085693, 0.9628400802612305]
[2025-05-14 19:46:59,284]: Mean: 0.96661162
[2025-05-14 19:46:59,284]: Min: 0.95583379
[2025-05-14 19:46:59,284]: Max: 0.98860449
[2025-05-14 19:46:59,285]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 19:46:59,291]: Sample Values (25 elements): [-0.019045965746045113, 0.01269731018692255, 0.01269731018692255, 0.01269731018692255, 0.0, 0.006348655093461275, -0.006348655093461275, 0.006348655093461275, 0.006348655093461275, 0.006348655093461275, 0.01269731018692255, 0.006348655093461275, 0.01269731018692255, -0.01269731018692255, 0.0, 0.019045965746045113, -0.006348655093461275, 0.0, 0.019045965746045113, 0.006348655093461275, 0.01269731018692255, -0.01269731018692255, -0.0253946203738451, -0.019045965746045113, 0.01269731018692255]
[2025-05-14 19:46:59,291]: Mean: 0.00002936
[2025-05-14 19:46:59,291]: Min: -0.05078924
[2025-05-14 19:46:59,291]: Max: 0.04444059
[2025-05-14 19:46:59,291]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 19:46:59,292]: Sample Values (25 elements): [0.9717494249343872, 0.9779557585716248, 0.9746411442756653, 0.9638665914535522, 0.9722433686256409, 0.9676983952522278, 0.9763504862785339, 0.9618650078773499, 0.9681944847106934, 0.973434329032898, 0.9647021293640137, 0.9696976542472839, 0.9743696451187134, 0.9809589385986328, 0.9767718315124512, 0.9755412936210632, 0.9723420739173889, 0.9732280373573303, 0.9679328203201294, 0.9655452370643616, 0.9731754660606384, 0.9682494401931763, 0.9657532572746277, 0.9727093577384949, 0.9730164408683777]
[2025-05-14 19:46:59,292]: Mean: 0.96987498
[2025-05-14 19:46:59,292]: Min: 0.95731032
[2025-05-14 19:46:59,292]: Max: 0.98556101
[2025-05-14 19:46:59,293]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 19:46:59,294]: Sample Values (25 elements): [-0.08283297717571259, 0.09939956665039062, 0.08283297717571259, 0.06626638025045395, 0.0, -0.016566595062613487, 0.06626638025045395, -0.06626638025045395, -0.04969978332519531, 0.06626638025045395, 0.033133190125226974, 0.06626638025045395, 0.016566595062613487, 0.016566595062613487, -0.033133190125226974, -0.04969978332519531, -0.08283297717571259, -0.04969978332519531, -0.06626638025045395, 0.0, 0.06626638025045395, 0.08283297717571259, 0.016566595062613487, 0.016566595062613487, -0.016566595062613487]
[2025-05-14 19:46:59,294]: Mean: -0.00037615
[2025-05-14 19:46:59,294]: Min: -0.13253276
[2025-05-14 19:46:59,294]: Max: 0.11596616
[2025-05-14 19:46:59,294]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 19:46:59,294]: Sample Values (25 elements): [0.9396376013755798, 0.9374939799308777, 0.9357686638832092, 0.9468479156494141, 0.9420442581176758, 0.9394626617431641, 0.9472251534461975, 0.9487987756729126, 0.9442842602729797, 0.9302533864974976, 0.9391076564788818, 0.9445497393608093, 0.9509935975074768, 0.9476115107536316, 0.9382949471473694, 0.9453332424163818, 0.9397732615470886, 0.9509132504463196, 0.9551295042037964, 0.9424217343330383, 0.9330106973648071, 0.935871422290802, 0.9441047310829163, 0.9366323351860046, 0.9415956735610962]
[2025-05-14 19:46:59,295]: Mean: 0.94244814
[2025-05-14 19:46:59,295]: Min: 0.92700970
[2025-05-14 19:46:59,295]: Max: 0.96475941
[2025-05-14 19:46:59,296]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 19:46:59,302]: Sample Values (25 elements): [0.013734837993979454, -0.020602256059646606, 0.0, 0.0, 0.013734837993979454, -0.006867418996989727, 0.013734837993979454, -0.013734837993979454, -0.027469675987958908, -0.027469675987958908, -0.006867418996989727, 0.013734837993979454, 0.006867418996989727, 0.013734837993979454, 0.020602256059646606, -0.006867418996989727, 0.0, 0.006867418996989727, 0.013734837993979454, 0.006867418996989727, 0.0, -0.006867418996989727, 0.006867418996989727, -0.006867418996989727, -0.027469675987958908]
[2025-05-14 19:46:59,302]: Mean: -0.00004029
[2025-05-14 19:46:59,302]: Min: -0.04807193
[2025-05-14 19:46:59,302]: Max: 0.05493935
[2025-05-14 19:46:59,302]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 19:46:59,302]: Sample Values (25 elements): [0.9765376448631287, 0.9783191084861755, 0.9675084352493286, 0.9786210656166077, 0.9807834029197693, 0.9665504693984985, 0.9928827285766602, 0.9783955812454224, 0.9908517003059387, 0.9690253138542175, 0.9772776961326599, 0.9716317057609558, 0.98247891664505, 0.9727005362510681, 0.9740402102470398, 0.9711745381355286, 0.9701662659645081, 0.9721236824989319, 0.971665620803833, 0.9706448316574097, 0.9737234115600586, 0.9833377599716187, 0.9721513986587524, 0.9765112400054932, 0.9658170938491821]
[2025-05-14 19:46:59,303]: Mean: 0.97553658
[2025-05-14 19:46:59,303]: Min: 0.95804089
[2025-05-14 19:46:59,303]: Max: 1.00913286
[2025-05-14 19:46:59,304]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 19:46:59,310]: Sample Values (25 elements): [0.005982762668281794, 0.011965525336563587, -0.005982762668281794, 0.011965525336563587, 0.011965525336563587, -0.023931050673127174, 0.0, 0.0, -0.011965525336563587, 0.017948288470506668, 0.005982762668281794, 0.0, -0.023931050673127174, 0.017948288470506668, 0.011965525336563587, -0.017948288470506668, 0.0, 0.0, 0.005982762668281794, -0.005982762668281794, -0.011965525336563587, 0.011965525336563587, -0.011965525336563587, 0.005982762668281794, -0.017948288470506668]
[2025-05-14 19:46:59,310]: Mean: -0.00000598
[2025-05-14 19:46:59,310]: Min: -0.04786210
[2025-05-14 19:46:59,310]: Max: 0.04187934
[2025-05-14 19:46:59,310]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 19:46:59,311]: Sample Values (25 elements): [0.978972852230072, 0.9801953434944153, 0.9897328615188599, 0.9849233627319336, 0.9688686728477478, 0.9861511588096619, 0.9674166440963745, 0.9731572866439819, 0.9708166122436523, 0.9668184518814087, 0.9722946286201477, 0.9711176156997681, 0.9693835973739624, 0.9982231259346008, 0.9735543727874756, 0.9850340485572815, 0.9743216037750244, 0.9881948828697205, 0.978722333908081, 0.9744752049446106, 0.9684480428695679, 0.9717018008232117, 0.9670252799987793, 0.9709010720252991, 0.9855362772941589]
[2025-05-14 19:46:59,311]: Mean: 0.97526854
[2025-05-14 19:46:59,311]: Min: 0.95721012
[2025-05-14 19:46:59,311]: Max: 0.99822313
[2025-05-14 19:46:59,312]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 19:46:59,325]: Sample Values (25 elements): [0.015314526855945587, 0.010209684260189533, -0.005104842130094767, -0.010209684260189533, -0.020419368520379066, 0.015314526855945587, -0.015314526855945587, -0.015314526855945587, -0.005104842130094767, 0.005104842130094767, -0.005104842130094767, 0.015314526855945587, -0.015314526855945587, -0.005104842130094767, 0.015314526855945587, -0.005104842130094767, -0.010209684260189533, 0.0, 0.010209684260189533, -0.010209684260189533, 0.005104842130094767, -0.020419368520379066, 0.0, 0.015314526855945587, -0.005104842130094767]
[2025-05-14 19:46:59,325]: Mean: -0.00000925
[2025-05-14 19:46:59,325]: Min: -0.03573389
[2025-05-14 19:46:59,326]: Max: 0.04083874
[2025-05-14 19:46:59,326]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 19:46:59,326]: Sample Values (25 elements): [0.9663744568824768, 0.9666284322738647, 0.9636667966842651, 0.9624918699264526, 0.9663090705871582, 0.9617104530334473, 0.9592747092247009, 0.9634559750556946, 0.9598440527915955, 0.9664774537086487, 0.9631944894790649, 0.9619719386100769, 0.9663475751876831, 0.9662375450134277, 0.9633108377456665, 0.9694802165031433, 0.9630411863327026, 0.9612895846366882, 0.9645048379898071, 0.9621344208717346, 0.9692372679710388, 0.9612815380096436, 0.9614369869232178, 0.9599701166152954, 0.959615170955658]
[2025-05-14 19:46:59,326]: Mean: 0.96386296
[2025-05-14 19:46:59,326]: Min: 0.95688838
[2025-05-14 19:46:59,327]: Max: 0.97647017
[2025-05-14 19:46:59,328]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 19:46:59,369]: Sample Values (25 elements): [-0.011776089668273926, -0.003925363067537546, 0.011776089668273926, 0.003925363067537546, -0.003925363067537546, 0.015701452270150185, -0.003925363067537546, 0.0, -0.003925363067537546, 0.0, -0.003925363067537546, -0.011776089668273926, -0.011776089668273926, 0.0, 0.003925363067537546, -0.015701452270150185, -0.007850726135075092, 0.003925363067537546, 0.007850726135075092, 0.007850726135075092, 0.007850726135075092, -0.019626814872026443, -0.003925363067537546, 0.0, -0.007850726135075092]
[2025-05-14 19:46:59,369]: Mean: -0.00000080
[2025-05-14 19:46:59,370]: Min: -0.03140290
[2025-05-14 19:46:59,370]: Max: 0.02747754
[2025-05-14 19:46:59,370]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 19:46:59,370]: Sample Values (25 elements): [0.9663383960723877, 0.9657894372940063, 0.9694682955741882, 0.9641016721725464, 0.9673990607261658, 0.9667977690696716, 0.9610474109649658, 0.9665478467941284, 0.9694540500640869, 0.9652860164642334, 0.9661694169044495, 0.9767497181892395, 0.9680800437927246, 0.9648823738098145, 0.9656448364257812, 0.9668899774551392, 0.9643399715423584, 0.9647578001022339, 0.9599416255950928, 0.9631288051605225, 0.9658359289169312, 0.9681198596954346, 0.9638829231262207, 0.9634829163551331, 0.9612162113189697]
[2025-05-14 19:46:59,370]: Mean: 0.96618146
[2025-05-14 19:46:59,371]: Min: 0.95889574
[2025-05-14 19:46:59,371]: Max: 0.98060286
[2025-05-14 19:46:59,372]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 19:46:59,373]: Sample Values (25 elements): [-0.010630069300532341, 0.031890206038951874, 0.010630069300532341, -0.042520277202129364, 0.053150348365306854, 0.042520277202129364, -0.06378041207790375, 0.053150348365306854, 0.0, -0.031890206038951874, 0.031890206038951874, 0.053150348365306854, 0.0, -0.010630069300532341, 0.010630069300532341, -0.042520277202129364, 0.0, -0.021260138601064682, 0.010630069300532341, -0.053150348365306854, 0.042520277202129364, 0.021260138601064682, 0.053150348365306854, 0.053150348365306854, 0.042520277202129364]
[2025-05-14 19:46:59,373]: Mean: -0.00005280
[2025-05-14 19:46:59,373]: Min: -0.08504055
[2025-05-14 19:46:59,373]: Max: 0.07441048
[2025-05-14 19:46:59,374]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 19:46:59,374]: Sample Values (25 elements): [0.9537609219551086, 0.9509302377700806, 0.9522489309310913, 0.9569706916809082, 0.9493874311447144, 0.9539238810539246, 0.9525429606437683, 0.954961359500885, 0.9525561928749084, 0.9552351832389832, 0.952002763748169, 0.9525858759880066, 0.9520972967147827, 0.9510220289230347, 0.9537649154663086, 0.9528939127922058, 0.9523557424545288, 0.9495581984519958, 0.949529767036438, 0.9522094130516052, 0.9504625797271729, 0.9464155435562134, 0.9516146779060364, 0.9555221199989319, 0.9511033296585083]
[2025-05-14 19:46:59,374]: Mean: 0.95310807
[2025-05-14 19:46:59,374]: Min: 0.94360942
[2025-05-14 19:46:59,374]: Max: 0.96326196
[2025-05-14 19:46:59,375]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 19:46:59,415]: Sample Values (25 elements): [0.0, -0.007936017587780952, -0.011904026381671429, 0.003968008793890476, 0.003968008793890476, 0.0, 0.003968008793890476, 0.0, 0.003968008793890476, 0.003968008793890476, -0.011904026381671429, 0.011904026381671429, -0.015872035175561905, 0.0, -0.007936017587780952, 0.007936017587780952, 0.011904026381671429, -0.003968008793890476, 0.003968008793890476, 0.007936017587780952, 0.0, 0.011904026381671429, -0.003968008793890476, 0.003968008793890476, 0.0]
[2025-05-14 19:46:59,415]: Mean: -0.00000347
[2025-05-14 19:46:59,416]: Min: -0.02777606
[2025-05-14 19:46:59,416]: Max: 0.03174407
[2025-05-14 19:46:59,416]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 19:46:59,417]: Sample Values (25 elements): [0.9685918688774109, 0.9752134680747986, 0.9681535363197327, 0.9641643166542053, 0.9604507088661194, 0.9738296866416931, 0.9626681208610535, 0.9714040160179138, 0.9652591943740845, 0.9753446578979492, 0.9718470573425293, 0.9614160656929016, 0.9644037485122681, 0.9696630239486694, 0.9666957259178162, 0.9640563130378723, 0.966334342956543, 0.9641532301902771, 0.9591934680938721, 0.9663304686546326, 0.9640603065490723, 0.9663590788841248, 0.9585462808609009, 0.9642791748046875, 0.964397668838501]
[2025-05-14 19:46:59,417]: Mean: 0.96504879
[2025-05-14 19:46:59,418]: Min: 0.95609075
[2025-05-14 19:46:59,418]: Max: 0.98407739
[2025-05-14 19:46:59,419]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 19:46:59,461]: Sample Values (25 elements): [-0.003059250768274069, -0.003059250768274069, 0.0, 0.015296254307031631, -0.012237003073096275, -0.012237003073096275, 0.003059250768274069, -0.012237003073096275, 0.00917775183916092, 0.015296254307031631, 0.003059250768274069, -0.00917775183916092, 0.015296254307031631, -0.006118501536548138, -0.006118501536548138, -0.006118501536548138, -0.00917775183916092, 0.0, 0.0, -0.003059250768274069, 0.00917775183916092, -0.00917775183916092, -0.015296254307031631, -0.012237003073096275, -0.00917775183916092]
[2025-05-14 19:46:59,461]: Mean: -0.00000425
[2025-05-14 19:46:59,461]: Min: -0.02141475
[2025-05-14 19:46:59,462]: Max: 0.02447401
[2025-05-14 19:46:59,462]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 19:46:59,462]: Sample Values (25 elements): [0.9668952822685242, 0.967458188533783, 0.9693410396575928, 0.9639870524406433, 0.9659299850463867, 0.9688154458999634, 0.9663927555084229, 0.9653987884521484, 0.9713184237480164, 0.9659140706062317, 0.964805006980896, 0.9662215113639832, 0.9682083129882812, 0.9661159515380859, 0.9641003608703613, 0.9668446183204651, 0.9702183604240417, 0.9716396927833557, 0.9647378921508789, 0.9637100100517273, 0.9689512252807617, 0.9705759882926941, 0.9701530337333679, 0.967655599117279, 0.9699068069458008]
[2025-05-14 19:46:59,462]: Mean: 0.96907401
[2025-05-14 19:46:59,462]: Min: 0.96133810
[2025-05-14 19:46:59,463]: Max: 0.97769201
[2025-05-14 19:46:59,463]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 19:46:59,463]: Sample Values (25 elements): [-0.03159749135375023, 0.013955213129520416, -0.05043313652276993, -0.12446247786283493, -0.027780985459685326, -0.04043377935886383, -0.06964840739965439, -0.021836016327142715, 0.07911961525678635, 0.08962202817201614, -0.012923888862133026, 0.04533997178077698, -0.04867078363895416, 0.06615766137838364, -0.08176013827323914, -0.013503639958798885, 0.04730047285556793, 0.00723835127428174, 0.04998668655753136, -0.056028515100479126, 0.03941121697425842, -0.05550612509250641, -0.07203195989131927, -0.10668110102415085, -0.005828251130878925]
[2025-05-14 19:46:59,463]: Mean: 0.00007814
[2025-05-14 19:46:59,463]: Min: -0.16103691
[2025-05-14 19:46:59,463]: Max: 0.15487213
[2025-05-14 19:46:59,463]: 


QAT of ResNet18 with hardtanh down to 3 bits...
[2025-05-14 19:46:59,687]: [ResNet18_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-14 19:46:59,754]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 19:48:37,552]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.5540 Train Acc: 0.8070 Eval Loss: 0.7252 Eval Acc: 0.7717 (LR: 0.001000)
[2025-05-14 19:50:15,610]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.4406 Train Acc: 0.8441 Eval Loss: 0.6824 Eval Acc: 0.7859 (LR: 0.001000)
[2025-05-14 19:51:53,551]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.4160 Train Acc: 0.8531 Eval Loss: 0.6989 Eval Acc: 0.7879 (LR: 0.001000)
[2025-05-14 19:53:31,534]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.3979 Train Acc: 0.8581 Eval Loss: 0.6256 Eval Acc: 0.8064 (LR: 0.001000)
[2025-05-14 19:55:09,534]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.3955 Train Acc: 0.8599 Eval Loss: 0.6057 Eval Acc: 0.8135 (LR: 0.001000)
[2025-05-14 19:56:47,558]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.3718 Train Acc: 0.8674 Eval Loss: 0.5873 Eval Acc: 0.8151 (LR: 0.001000)
[2025-05-14 19:58:25,540]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.3561 Train Acc: 0.8737 Eval Loss: 0.5774 Eval Acc: 0.8172 (LR: 0.001000)
[2025-05-14 20:00:03,569]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.3548 Train Acc: 0.8740 Eval Loss: 0.6452 Eval Acc: 0.7975 (LR: 0.001000)
[2025-05-14 20:01:41,319]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.3534 Train Acc: 0.8750 Eval Loss: 0.6260 Eval Acc: 0.8057 (LR: 0.001000)
[2025-05-14 20:03:19,098]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.3370 Train Acc: 0.8807 Eval Loss: 0.6096 Eval Acc: 0.8122 (LR: 0.001000)
[2025-05-14 20:04:56,876]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.3280 Train Acc: 0.8835 Eval Loss: 0.5511 Eval Acc: 0.8316 (LR: 0.001000)
[2025-05-14 20:06:35,127]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.3246 Train Acc: 0.8846 Eval Loss: 0.5948 Eval Acc: 0.8175 (LR: 0.001000)
[2025-05-14 20:08:13,611]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.3148 Train Acc: 0.8878 Eval Loss: 0.5479 Eval Acc: 0.8326 (LR: 0.001000)
[2025-05-14 20:09:51,527]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.3101 Train Acc: 0.8899 Eval Loss: 0.5725 Eval Acc: 0.8233 (LR: 0.001000)
[2025-05-14 20:11:29,389]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.3084 Train Acc: 0.8913 Eval Loss: 0.6348 Eval Acc: 0.8107 (LR: 0.001000)
[2025-05-14 20:13:07,036]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.2947 Train Acc: 0.8955 Eval Loss: 0.5773 Eval Acc: 0.8223 (LR: 0.001000)
[2025-05-14 20:14:44,692]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.2915 Train Acc: 0.8958 Eval Loss: 0.5125 Eval Acc: 0.8407 (LR: 0.001000)
[2025-05-14 20:16:22,350]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.2838 Train Acc: 0.8988 Eval Loss: 0.5726 Eval Acc: 0.8337 (LR: 0.001000)
[2025-05-14 20:17:59,937]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.2853 Train Acc: 0.8985 Eval Loss: 0.5465 Eval Acc: 0.8383 (LR: 0.001000)
[2025-05-14 20:19:39,337]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.2825 Train Acc: 0.8995 Eval Loss: 0.6733 Eval Acc: 0.8066 (LR: 0.001000)
[2025-05-14 20:21:17,369]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.2728 Train Acc: 0.9029 Eval Loss: 0.5564 Eval Acc: 0.8362 (LR: 0.001000)
[2025-05-14 20:22:55,421]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.2736 Train Acc: 0.9039 Eval Loss: 0.5487 Eval Acc: 0.8384 (LR: 0.001000)
[2025-05-14 20:24:33,226]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.2664 Train Acc: 0.9048 Eval Loss: 0.5531 Eval Acc: 0.8367 (LR: 0.001000)
[2025-05-14 20:26:11,122]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.2576 Train Acc: 0.9097 Eval Loss: 0.5710 Eval Acc: 0.8292 (LR: 0.001000)
[2025-05-14 20:27:48,886]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.2548 Train Acc: 0.9092 Eval Loss: 0.5351 Eval Acc: 0.8406 (LR: 0.001000)
[2025-05-14 20:29:26,894]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.2538 Train Acc: 0.9099 Eval Loss: 0.5522 Eval Acc: 0.8398 (LR: 0.001000)
[2025-05-14 20:31:04,946]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.2492 Train Acc: 0.9122 Eval Loss: 0.5335 Eval Acc: 0.8445 (LR: 0.001000)
[2025-05-14 20:32:43,009]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.2397 Train Acc: 0.9134 Eval Loss: 0.5347 Eval Acc: 0.8388 (LR: 0.001000)
[2025-05-14 20:34:21,018]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.2434 Train Acc: 0.9137 Eval Loss: 0.5204 Eval Acc: 0.8435 (LR: 0.001000)
[2025-05-14 20:35:59,045]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.2362 Train Acc: 0.9167 Eval Loss: 0.5463 Eval Acc: 0.8400 (LR: 0.000250)
[2025-05-14 20:37:36,876]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.1600 Train Acc: 0.9451 Eval Loss: 0.4352 Eval Acc: 0.8716 (LR: 0.000250)
[2025-05-14 20:39:14,506]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.1443 Train Acc: 0.9503 Eval Loss: 0.4416 Eval Acc: 0.8694 (LR: 0.000250)
[2025-05-14 20:40:52,273]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.1380 Train Acc: 0.9527 Eval Loss: 0.4468 Eval Acc: 0.8714 (LR: 0.000250)
[2025-05-14 20:42:30,093]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.1318 Train Acc: 0.9551 Eval Loss: 0.4489 Eval Acc: 0.8740 (LR: 0.000250)
[2025-05-14 20:44:07,945]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.1303 Train Acc: 0.9550 Eval Loss: 0.4585 Eval Acc: 0.8686 (LR: 0.000250)
[2025-05-14 20:45:45,692]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.1276 Train Acc: 0.9560 Eval Loss: 0.4601 Eval Acc: 0.8696 (LR: 0.000250)
[2025-05-14 20:47:23,512]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.1253 Train Acc: 0.9582 Eval Loss: 0.4648 Eval Acc: 0.8700 (LR: 0.000250)
[2025-05-14 20:49:01,113]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.1238 Train Acc: 0.9570 Eval Loss: 0.4705 Eval Acc: 0.8716 (LR: 0.000250)
[2025-05-14 20:50:39,123]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.1208 Train Acc: 0.9579 Eval Loss: 0.4846 Eval Acc: 0.8676 (LR: 0.000250)
[2025-05-14 20:52:18,381]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.1217 Train Acc: 0.9582 Eval Loss: 0.4812 Eval Acc: 0.8672 (LR: 0.000250)
[2025-05-14 20:53:56,478]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.1166 Train Acc: 0.9602 Eval Loss: 0.4787 Eval Acc: 0.8688 (LR: 0.000250)
[2025-05-14 20:55:34,657]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.1215 Train Acc: 0.9582 Eval Loss: 0.4808 Eval Acc: 0.8638 (LR: 0.000250)
[2025-05-14 20:57:12,542]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.1165 Train Acc: 0.9604 Eval Loss: 0.4782 Eval Acc: 0.8709 (LR: 0.000250)
[2025-05-14 20:58:50,530]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.1137 Train Acc: 0.9612 Eval Loss: 0.4674 Eval Acc: 0.8740 (LR: 0.000250)
[2025-05-14 21:00:28,481]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.1120 Train Acc: 0.9617 Eval Loss: 0.4846 Eval Acc: 0.8699 (LR: 0.000063)
[2025-05-14 21:02:06,384]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.0966 Train Acc: 0.9678 Eval Loss: 0.4626 Eval Acc: 0.8734 (LR: 0.000063)
[2025-05-14 21:03:44,367]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.0937 Train Acc: 0.9692 Eval Loss: 0.4600 Eval Acc: 0.8754 (LR: 0.000063)
[2025-05-14 21:05:22,248]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.0914 Train Acc: 0.9699 Eval Loss: 0.4669 Eval Acc: 0.8740 (LR: 0.000063)
[2025-05-14 21:07:00,227]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.0892 Train Acc: 0.9695 Eval Loss: 0.4614 Eval Acc: 0.8755 (LR: 0.000063)
[2025-05-14 21:08:37,951]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.0870 Train Acc: 0.9717 Eval Loss: 0.4593 Eval Acc: 0.8772 (LR: 0.000063)
[2025-05-14 21:10:15,731]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.0904 Train Acc: 0.9699 Eval Loss: 0.4611 Eval Acc: 0.8734 (LR: 0.000063)
[2025-05-14 21:11:53,499]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.0882 Train Acc: 0.9702 Eval Loss: 0.4629 Eval Acc: 0.8733 (LR: 0.000063)
[2025-05-14 21:13:31,070]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.0841 Train Acc: 0.9725 Eval Loss: 0.4643 Eval Acc: 0.8746 (LR: 0.000063)
[2025-05-14 21:15:08,689]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.0848 Train Acc: 0.9720 Eval Loss: 0.4696 Eval Acc: 0.8743 (LR: 0.000063)
[2025-05-14 21:16:46,299]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.0842 Train Acc: 0.9727 Eval Loss: 0.4623 Eval Acc: 0.8768 (LR: 0.000063)
[2025-05-14 21:18:23,897]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.0852 Train Acc: 0.9718 Eval Loss: 0.4740 Eval Acc: 0.8744 (LR: 0.000063)
[2025-05-14 21:20:01,402]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.0850 Train Acc: 0.9713 Eval Loss: 0.4693 Eval Acc: 0.8732 (LR: 0.000063)
[2025-05-14 21:21:39,160]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.0847 Train Acc: 0.9704 Eval Loss: 0.4582 Eval Acc: 0.8786 (LR: 0.000063)
[2025-05-14 21:23:16,981]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.0834 Train Acc: 0.9729 Eval Loss: 0.4647 Eval Acc: 0.8781 (LR: 0.000063)
[2025-05-14 21:24:56,567]: [ResNet18_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.0851 Train Acc: 0.9708 Eval Loss: 0.4639 Eval Acc: 0.8786 (LR: 0.000063)
[2025-05-14 21:24:56,567]: [ResNet18_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8786
[2025-05-14 21:24:56,648]: 


Quantization of model down to 3 bits finished
[2025-05-14 21:24:56,648]: Model Architecture:
[2025-05-14 21:24:56,696]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0569], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20404557883739471, max_val=0.1941293329000473)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0853], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25715744495391846, max_val=0.340192973613739)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27148014307022095, max_val=0.21209360659122467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0393], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13693681359291077, max_val=0.1381910741329193)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0414], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14691869914531708, max_val=0.1429276019334793)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0235], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08487351983785629, max_val=0.0795707181096077)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0582], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20166972279548645, max_val=0.2058340609073639)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0211], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07353638112545013, max_val=0.07389526069164276)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07178936898708344, max_val=0.07385001331567764)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0168], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05900431051850319, max_val=0.0585673563182354)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04983510076999664, max_val=0.04908011481165886)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0362], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12752129137516022, max_val=0.1259303092956543)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.053909316658973694, max_val=0.05389269068837166)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0136], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04915300011634827, max_val=0.04592721909284592)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03998070955276489, max_val=0.03994113579392433)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0090], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03181061893701553, max_val=0.03142015263438225)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0228], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08021019399166107, max_val=0.07948664575815201)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.027720661833882332, max_val=0.033053550869226456)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023465391248464584, max_val=0.022853389382362366)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 21:24:56,697]: 
Model Weights:
[2025-05-14 21:24:56,697]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 21:24:56,697]: Sample Values (25 elements): [-0.3449997007846832, -0.06945706158876419, -0.05980606749653816, 0.2588343620300293, -0.2507544457912445, 0.06870774924755096, 0.14522573351860046, 0.19106100499629974, 0.17224718630313873, -0.10548046976327896, 0.025787411257624626, 0.11411649733781815, -0.2316972315311432, 0.08250612020492554, -0.16446208953857422, -0.14952538907527924, 0.09656409919261932, -0.06991855800151825, 0.2467566281557083, -0.3346400856971741, -0.2786877453327179, -0.1647958904504776, 0.22298820316791534, -0.057911984622478485, 0.10012128204107285]
[2025-05-14 21:24:56,697]: Mean: 0.00002871
[2025-05-14 21:24:56,697]: Min: -0.46837938
[2025-05-14 21:24:56,697]: Max: 0.47796413
[2025-05-14 21:24:56,698]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 21:24:56,698]: Sample Values (25 elements): [0.8978815078735352, 0.8775630593299866, 0.9520918726921082, 0.8463379144668579, 0.8403129577636719, 0.7910000681877136, 0.8616670966148376, 0.9812371730804443, 0.6790739893913269, 0.6292619109153748, 0.7476857900619507, 1.1572530269622803, 0.909209132194519, 0.7995758652687073, 0.9970542788505554, 0.9977034330368042, 0.9022567272186279, 1.0242220163345337, 0.9573681950569153, 0.9103987812995911, 0.977227509021759, 1.0003622770309448, 0.7983391284942627, 0.8734188079833984, 0.7237544655799866]
[2025-05-14 21:24:56,698]: Mean: 0.88448071
[2025-05-14 21:24:56,698]: Min: 0.62926191
[2025-05-14 21:24:56,698]: Max: 1.15725303
[2025-05-14 21:24:56,699]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 21:24:56,700]: Sample Values (25 elements): [0.0, 0.0, 0.05688214674592018, -0.05688214674592018, 0.0, 0.0, 0.05688214674592018, -0.11376429349184036, 0.0, -0.05688214674592018, 0.0, 0.0, 0.0, -0.11376429349184036, 0.05688214674592018, 0.0, 0.05688214674592018, 0.0, -0.05688214674592018, 0.0, 0.0, 0.0, 0.05688214674592018, 0.05688214674592018, -0.05688214674592018]
[2025-05-14 21:24:56,700]: Mean: 0.00007869
[2025-05-14 21:24:56,700]: Min: -0.22752859
[2025-05-14 21:24:56,700]: Max: 0.17064644
[2025-05-14 21:24:56,700]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 21:24:56,701]: Sample Values (25 elements): [0.9981754422187805, 0.9410948157310486, 0.9887917637825012, 0.9817086458206177, 0.9437546133995056, 0.8388321399688721, 0.937321126461029, 1.014896273612976, 0.9462704658508301, 0.9402579069137573, 0.8580017685890198, 0.8411547541618347, 1.0177531242370605, 0.9587322473526001, 1.0158063173294067, 0.9420409798622131, 1.0113693475723267, 0.9389800429344177, 0.9754522442817688, 0.8765165209770203, 1.0077086687088013, 0.9206419587135315, 1.0803791284561157, 0.9994329214096069, 0.9538655877113342]
[2025-05-14 21:24:56,701]: Mean: 0.96597570
[2025-05-14 21:24:56,701]: Min: 0.83883214
[2025-05-14 21:24:56,701]: Max: 1.11277878
[2025-05-14 21:24:56,702]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 21:24:56,703]: Sample Values (25 elements): [-0.08533590286970139, 0.0, 0.0, 0.08533590286970139, 0.0, 0.0, 0.0, -0.08533590286970139, -0.08533590286970139, -0.08533590286970139, 0.0, 0.0, 0.0, -0.08533590286970139, 0.08533590286970139, 0.08533590286970139, 0.0, 0.0, 0.0, -0.08533590286970139, 0.0, 0.08533590286970139, 0.0, 0.0, 0.0]
[2025-05-14 21:24:56,703]: Mean: 0.00012732
[2025-05-14 21:24:56,703]: Min: -0.25600770
[2025-05-14 21:24:56,703]: Max: 0.34134361
[2025-05-14 21:24:56,703]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 21:24:56,703]: Sample Values (25 elements): [0.865266740322113, 0.9411532878875732, 0.9508230090141296, 0.9390780925750732, 0.9540467858314514, 0.9610503911972046, 0.965994119644165, 0.9247695207595825, 0.9167401790618896, 0.8280443549156189, 0.8567439913749695, 0.8774297833442688, 0.9718498587608337, 0.9609054327011108, 0.9581686854362488, 0.8596293926239014, 0.8641762733459473, 1.0540757179260254, 0.9813019633293152, 0.9298085570335388, 0.9763891696929932, 1.1227853298187256, 0.843230664730072, 0.877019464969635, 0.8363232612609863]
[2025-05-14 21:24:56,704]: Mean: 0.93078732
[2025-05-14 21:24:56,704]: Min: 0.79376495
[2025-05-14 21:24:56,704]: Max: 1.33602083
[2025-05-14 21:24:56,705]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 21:24:56,705]: Sample Values (25 elements): [0.0, -0.06908199191093445, 0.06908199191093445, 0.0, 0.0, 0.0, -0.06908199191093445, -0.06908199191093445, 0.0, 0.0, 0.0, 0.06908199191093445, 0.0, 0.0, 0.0, -0.06908199191093445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1381639838218689, 0.0, 0.0, 0.0]
[2025-05-14 21:24:56,706]: Mean: -0.00004872
[2025-05-14 21:24:56,706]: Min: -0.27632797
[2025-05-14 21:24:56,706]: Max: 0.20724598
[2025-05-14 21:24:56,706]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 21:24:56,706]: Sample Values (25 elements): [0.9569505453109741, 0.9425060153007507, 1.0350744724273682, 0.9872832894325256, 0.9986563920974731, 0.9440559148788452, 0.9741020202636719, 0.9687884449958801, 1.0914939641952515, 0.9716567993164062, 0.9659779071807861, 0.9962902069091797, 0.9813365340232849, 1.0239430665969849, 0.9377518892288208, 1.089582085609436, 1.0373116731643677, 0.9424546360969543, 1.0150766372680664, 0.9980536699295044, 0.9778344631195068, 0.9973065257072449, 1.0283164978027344, 0.993607223033905, 0.9512312412261963]
[2025-05-14 21:24:56,706]: Mean: 0.99522483
[2025-05-14 21:24:56,706]: Min: 0.93071955
[2025-05-14 21:24:56,707]: Max: 1.09149396
[2025-05-14 21:24:56,708]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 21:24:56,708]: Sample Values (25 elements): [0.039303943514823914, 0.0, 0.039303943514823914, -0.039303943514823914, -0.039303943514823914, 0.0, -0.039303943514823914, 0.039303943514823914, 0.039303943514823914, -0.039303943514823914, 0.039303943514823914, 0.0, 0.0, -0.039303943514823914, 0.0, 0.039303943514823914, -0.039303943514823914, 0.07860788702964783, 0.0, 0.0, -0.039303943514823914, 0.039303943514823914, 0.0, 0.0, 0.0]
[2025-05-14 21:24:56,708]: Mean: 0.00039982
[2025-05-14 21:24:56,708]: Min: -0.11791183
[2025-05-14 21:24:56,709]: Max: 0.15721577
[2025-05-14 21:24:56,709]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 21:24:56,709]: Sample Values (25 elements): [0.9342671036720276, 0.9368994832038879, 0.9799907207489014, 0.9714816808700562, 0.9402313828468323, 0.9389744997024536, 0.9366008043289185, 0.9197571873664856, 0.9246034622192383, 0.9833282232284546, 0.977391242980957, 0.9472367763519287, 1.028687834739685, 1.0082541704177856, 0.8809752464294434, 0.9762781858444214, 0.8961735367774963, 0.9105607867240906, 0.953377902507782, 0.92295902967453, 1.1773127317428589, 0.9015046954154968, 0.8869771361351013, 1.02060067653656, 0.9433128833770752]
[2025-05-14 21:24:56,709]: Mean: 0.94569969
[2025-05-14 21:24:56,709]: Min: 0.82806206
[2025-05-14 21:24:56,709]: Max: 1.17731273
[2025-05-14 21:24:56,710]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 21:24:56,711]: Sample Values (25 elements): [0.0, 0.0, 0.041406672447919846, -0.041406672447919846, 0.041406672447919846, 0.041406672447919846, -0.041406672447919846, 0.0, 0.041406672447919846, -0.041406672447919846, 0.041406672447919846, 0.041406672447919846, -0.08281334489583969, 0.0, 0.0, 0.0, 0.041406672447919846, 0.0, 0.041406672447919846, 0.0, 0.041406672447919846, 0.0, 0.0, -0.041406672447919846, -0.041406672447919846]
[2025-05-14 21:24:56,711]: Mean: 0.00014490
[2025-05-14 21:24:56,711]: Min: -0.16562669
[2025-05-14 21:24:56,712]: Max: 0.12422001
[2025-05-14 21:24:56,712]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 21:24:56,712]: Sample Values (25 elements): [0.9617403149604797, 0.9621061086654663, 0.9581065773963928, 0.966552197933197, 0.9833998680114746, 0.9539632201194763, 0.9812513589859009, 0.9721565842628479, 0.985219419002533, 0.9512230753898621, 0.9630895256996155, 0.9862651824951172, 0.9873744249343872, 0.9623631238937378, 0.9590170979499817, 0.995823860168457, 0.967812180519104, 0.9599111080169678, 0.9663720726966858, 0.9627132415771484, 0.9797059297561646, 0.9829908013343811, 0.9850117564201355, 0.9476761221885681, 0.9997127056121826]
[2025-05-14 21:24:56,712]: Mean: 0.97217011
[2025-05-14 21:24:56,712]: Min: 0.94368052
[2025-05-14 21:24:56,712]: Max: 1.04948223
[2025-05-14 21:24:56,713]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 21:24:56,715]: Sample Values (25 elements): [-0.023492036387324333, -0.023492036387324333, -0.023492036387324333, 0.0, -0.046984072774648666, -0.023492036387324333, 0.0, 0.023492036387324333, 0.023492036387324333, -0.023492036387324333, 0.046984072774648666, -0.023492036387324333, -0.023492036387324333, 0.0, 0.046984072774648666, 0.0, 0.023492036387324333, -0.023492036387324333, 0.023492036387324333, 0.0, 0.046984072774648666, 0.023492036387324333, 0.023492036387324333, 0.0, -0.023492036387324333]
[2025-05-14 21:24:56,715]: Mean: -0.00001195
[2025-05-14 21:24:56,715]: Min: -0.09396815
[2025-05-14 21:24:56,715]: Max: 0.07047611
[2025-05-14 21:24:56,715]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 21:24:56,715]: Sample Values (25 elements): [0.9836825728416443, 0.9792990684509277, 0.9972587823867798, 0.9899297952651978, 0.9631330966949463, 0.9559181928634644, 0.9623733758926392, 0.991108238697052, 0.9964271187782288, 0.9573251605033875, 0.9727206230163574, 0.9681487679481506, 0.9640628099441528, 0.9549117684364319, 0.9720761775970459, 0.9861517548561096, 0.9500510692596436, 0.9590346813201904, 0.9777524471282959, 0.9447915554046631, 0.9669370055198669, 0.9650766253471375, 0.9477567672729492, 0.9629130363464355, 0.9411319494247437]
[2025-05-14 21:24:56,716]: Mean: 0.96868694
[2025-05-14 21:24:56,716]: Min: 0.92749542
[2025-05-14 21:24:56,716]: Max: 1.01864135
[2025-05-14 21:24:56,717]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 21:24:56,717]: Sample Values (25 elements): [0.11642993241548538, -0.11642993241548538, 0.05821496620774269, 0.0, -0.11642993241548538, 0.11642993241548538, 0.0, 0.0, 0.05821496620774269, 0.05821496620774269, 0.11642993241548538, 0.0, -0.11642993241548538, -0.05821496620774269, 0.11642993241548538, -0.05821496620774269, -0.05821496620774269, 0.05821496620774269, 0.11642993241548538, -0.11642993241548538, -0.11642993241548538, 0.05821496620774269, 0.0, 0.11642993241548538, -0.11642993241548538]
[2025-05-14 21:24:56,717]: Mean: -0.00009949
[2025-05-14 21:24:56,718]: Min: -0.17464490
[2025-05-14 21:24:56,718]: Max: 0.23285986
[2025-05-14 21:24:56,718]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 21:24:56,718]: Sample Values (25 elements): [0.9048740267753601, 0.9224570393562317, 0.9313417077064514, 0.9018902778625488, 0.9159584641456604, 0.9092490673065186, 0.9419322609901428, 0.9331110715866089, 0.9340156316757202, 0.9440405964851379, 0.9197763204574585, 0.9578025937080383, 0.925971508026123, 0.9340387582778931, 0.9141793847084045, 0.9454742074012756, 0.9184387922286987, 0.9358927011489868, 0.9316526055335999, 0.9429886937141418, 0.9433904886245728, 0.9222773909568787, 0.938606321811676, 0.9431501030921936, 0.9264663457870483]
[2025-05-14 21:24:56,718]: Mean: 0.93017840
[2025-05-14 21:24:56,718]: Min: 0.89154607
[2025-05-14 21:24:56,718]: Max: 0.99852353
[2025-05-14 21:24:56,719]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 21:24:56,721]: Sample Values (25 elements): [-0.021061651408672333, 0.0, 0.021061651408672333, 0.0, 0.0, 0.021061651408672333, 0.0, 0.0, -0.042123302817344666, -0.021061651408672333, 0.0, 0.0, -0.042123302817344666, 0.0, 0.021061651408672333, -0.021061651408672333, 0.021061651408672333, 0.042123302817344666, 0.021061651408672333, 0.042123302817344666, 0.021061651408672333, -0.021061651408672333, -0.021061651408672333, 0.021061651408672333, -0.021061651408672333]
[2025-05-14 21:24:56,721]: Mean: -0.00010641
[2025-05-14 21:24:56,721]: Min: -0.06318495
[2025-05-14 21:24:56,721]: Max: 0.08424661
[2025-05-14 21:24:56,721]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 21:24:56,722]: Sample Values (25 elements): [0.9740463495254517, 0.9561100602149963, 0.963068425655365, 0.9845585227012634, 0.988969624042511, 0.9821468591690063, 0.9877256155014038, 0.9731948375701904, 0.9669469594955444, 0.9689654111862183, 1.0001401901245117, 0.9713492393493652, 0.9804655909538269, 0.9701865911483765, 1.0179749727249146, 0.9964688420295715, 1.0093690156936646, 1.0048555135726929, 1.05518639087677, 0.9891900420188904, 1.026820421218872, 0.9803594946861267, 1.0005594491958618, 0.9935969710350037, 1.0001952648162842]
[2025-05-14 21:24:56,722]: Mean: 0.98619348
[2025-05-14 21:24:56,722]: Min: 0.95611006
[2025-05-14 21:24:56,722]: Max: 1.05518639
[2025-05-14 21:24:56,723]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 21:24:56,725]: Sample Values (25 elements): [-0.02080567553639412, 0.0, 0.0, -0.02080567553639412, -0.02080567553639412, 0.0, 0.02080567553639412, 0.0, 0.0, 0.02080567553639412, 0.0, 0.0, 0.04161135107278824, -0.04161135107278824, 0.02080567553639412, 0.02080567553639412, 0.06241702660918236, -0.02080567553639412, -0.02080567553639412, 0.0, 0.0, 0.0, -0.02080567553639412, -0.02080567553639412, -0.02080567553639412]
[2025-05-14 21:24:56,725]: Mean: 0.00011401
[2025-05-14 21:24:56,725]: Min: -0.06241703
[2025-05-14 21:24:56,725]: Max: 0.08322270
[2025-05-14 21:24:56,725]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 21:24:56,725]: Sample Values (25 elements): [0.979775607585907, 0.9693406224250793, 0.9660118818283081, 0.9615758061408997, 0.9631507992744446, 0.9945999383926392, 0.9802277088165283, 0.9722613096237183, 0.9720569849014282, 0.9742723107337952, 0.9727201461791992, 0.9822348356246948, 0.9866270422935486, 1.0178148746490479, 1.0083993673324585, 0.967358410358429, 0.9687277674674988, 0.9838030338287354, 0.9718074798583984, 0.9957221746444702, 0.9689578413963318, 0.9352412819862366, 0.9811417460441589, 0.9669708609580994, 0.9651822447776794]
[2025-05-14 21:24:56,725]: Mean: 0.97565567
[2025-05-14 21:24:56,726]: Min: 0.93524128
[2025-05-14 21:24:56,726]: Max: 1.01781487
[2025-05-14 21:24:56,727]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 21:24:56,730]: Sample Values (25 elements): [0.016795914620161057, -0.016795914620161057, 0.0, 0.0, 0.0, -0.03359182924032211, -0.016795914620161057, 0.0, -0.016795914620161057, -0.016795914620161057, 0.016795914620161057, -0.03359182924032211, 0.03359182924032211, 0.0, 0.03359182924032211, 0.0, 0.016795914620161057, 0.0, -0.016795914620161057, 0.0, 0.0, -0.016795914620161057, 0.016795914620161057, 0.016795914620161057, 0.016795914620161057]
[2025-05-14 21:24:56,730]: Mean: 0.00010399
[2025-05-14 21:24:56,730]: Min: -0.06718366
[2025-05-14 21:24:56,730]: Max: 0.05038774
[2025-05-14 21:24:56,730]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 21:24:56,731]: Sample Values (25 elements): [0.9601835608482361, 0.9613677859306335, 0.9605144262313843, 0.9702363014221191, 0.9606745839118958, 0.9708757996559143, 0.9620318412780762, 0.9714760780334473, 0.9661731719970703, 0.9640201330184937, 0.9591619372367859, 0.96212238073349, 0.9640909433364868, 0.972255289554596, 0.9643294215202332, 0.9698147773742676, 0.979487955570221, 0.9699240326881409, 0.9685153365135193, 0.9620336294174194, 0.9675832390785217, 0.9806569814682007, 0.9726629853248596, 0.9627957344055176, 0.9627497792243958]
[2025-05-14 21:24:56,731]: Mean: 0.96769702
[2025-05-14 21:24:56,731]: Min: 0.95169818
[2025-05-14 21:24:56,731]: Max: 0.99658602
[2025-05-14 21:24:56,732]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 21:24:56,738]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.014130750671029091, 0.014130750671029091, 0.014130750671029091, 0.0, 0.0, -0.014130750671029091, 0.0, 0.0, 0.028261501342058182, 0.0, -0.014130750671029091, -0.014130750671029091, -0.028261501342058182, 0.014130750671029091, 0.0, -0.014130750671029091, -0.028261501342058182, 0.0, 0.014130750671029091, -0.014130750671029091, 0.0]
[2025-05-14 21:24:56,738]: Mean: 0.00002295
[2025-05-14 21:24:56,738]: Min: -0.05652300
[2025-05-14 21:24:56,738]: Max: 0.04239225
[2025-05-14 21:24:56,738]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 21:24:56,739]: Sample Values (25 elements): [0.9726738333702087, 0.9764474034309387, 0.9734094738960266, 0.962302565574646, 0.968913733959198, 0.9628233313560486, 0.9820739030838013, 0.9747799634933472, 0.9729520678520203, 0.9596455097198486, 0.9711788296699524, 0.9679122567176819, 0.9654732942581177, 0.9735741019248962, 0.9626811146736145, 0.9751935005187988, 0.9553143978118896, 0.9608129262924194, 0.9671218991279602, 0.9721013307571411, 0.9676981568336487, 0.962773323059082, 0.9761974811553955, 0.9711213111877441, 0.9589129686355591]
[2025-05-14 21:24:56,739]: Mean: 0.97086489
[2025-05-14 21:24:56,739]: Min: 0.95531440
[2025-05-14 21:24:56,739]: Max: 0.99159878
[2025-05-14 21:24:56,740]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 21:24:56,741]: Sample Values (25 elements): [-0.07241471111774445, 0.0, 0.03620735555887222, 0.0, 0.0, -0.03620735555887222, 0.0, 0.03620735555887222, -0.07241471111774445, -0.03620735555887222, -0.03620735555887222, 0.07241471111774445, 0.07241471111774445, 0.03620735555887222, 0.0, 0.03620735555887222, -0.07241471111774445, -0.07241471111774445, -0.07241471111774445, 0.07241471111774445, 0.07241471111774445, 0.0, 0.07241471111774445, 0.03620735555887222, -0.07241471111774445]
[2025-05-14 21:24:56,741]: Mean: -0.00032486
[2025-05-14 21:24:56,741]: Min: -0.14482942
[2025-05-14 21:24:56,741]: Max: 0.10862207
[2025-05-14 21:24:56,741]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 21:24:56,741]: Sample Values (25 elements): [0.9371020197868347, 0.9320731163024902, 0.9415435791015625, 0.9373275637626648, 0.9322172403335571, 0.9338355660438538, 0.9438599944114685, 0.9437112808227539, 0.9436485171318054, 0.9532138109207153, 0.9481990933418274, 0.9456982612609863, 0.9394739866256714, 0.9236924052238464, 0.9383194446563721, 0.9412789940834045, 0.9336075782775879, 0.9395654201507568, 0.9477141499519348, 0.9300498366355896, 0.9422941207885742, 0.9401986002922058, 0.9355565905570984, 0.9411442279815674, 0.940873384475708]
[2025-05-14 21:24:56,742]: Mean: 0.93911791
[2025-05-14 21:24:56,742]: Min: 0.91687804
[2025-05-14 21:24:56,742]: Max: 0.96362597
[2025-05-14 21:24:56,743]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 21:24:56,749]: Sample Values (25 elements): [0.0, -0.015400337055325508, -0.015400337055325508, -0.015400337055325508, 0.015400337055325508, 0.015400337055325508, 0.015400337055325508, 0.0, -0.030800674110651016, 0.0, -0.015400337055325508, 0.0, 0.015400337055325508, 0.0, -0.015400337055325508, 0.030800674110651016, 0.015400337055325508, -0.015400337055325508, 0.0, 0.015400337055325508, 0.0, 0.0, 0.030800674110651016, 0.015400337055325508, 0.0]
[2025-05-14 21:24:56,749]: Mean: -0.00005238
[2025-05-14 21:24:56,749]: Min: -0.06160135
[2025-05-14 21:24:56,749]: Max: 0.04620101
[2025-05-14 21:24:56,749]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 21:24:56,750]: Sample Values (25 elements): [0.969079852104187, 0.9633470177650452, 0.9814149737358093, 0.9620132446289062, 0.9764524698257446, 0.9785822629928589, 0.9774668216705322, 0.9845498204231262, 0.9834355115890503, 0.984759509563446, 0.9824625253677368, 0.9746180176734924, 0.9728870987892151, 0.9815352559089661, 0.9772611260414124, 0.9808615446090698, 0.9863888025283813, 0.974303662776947, 0.9825929403305054, 0.9866712093353271, 0.9765660166740417, 0.9753531217575073, 0.9736747145652771, 0.9684456586837769, 0.9811298847198486]
[2025-05-14 21:24:56,750]: Mean: 0.97739536
[2025-05-14 21:24:56,750]: Min: 0.95591503
[2025-05-14 21:24:56,750]: Max: 1.01532316
[2025-05-14 21:24:56,751]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 21:24:56,757]: Sample Values (25 elements): [0.04074866697192192, -0.01358288899064064, -0.01358288899064064, 0.0, 0.01358288899064064, 0.0, 0.01358288899064064, -0.02716577798128128, -0.01358288899064064, 0.0, 0.0, -0.01358288899064064, -0.01358288899064064, -0.01358288899064064, -0.02716577798128128, 0.01358288899064064, 0.02716577798128128, -0.01358288899064064, 0.0, 0.01358288899064064, 0.0, 0.0, 0.01358288899064064, -0.01358288899064064, 0.01358288899064064]
[2025-05-14 21:24:56,757]: Mean: 0.00000732
[2025-05-14 21:24:56,757]: Min: -0.05433156
[2025-05-14 21:24:56,757]: Max: 0.04074867
[2025-05-14 21:24:56,757]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 21:24:56,758]: Sample Values (25 elements): [0.9668735861778259, 0.9755339622497559, 0.9868152141571045, 0.9562888145446777, 0.9722365736961365, 0.9626211524009705, 0.9678424000740051, 0.9744691252708435, 0.9650264382362366, 0.9676976203918457, 0.9758164286613464, 0.9705061912536621, 0.9840391874313354, 0.9754610657691956, 0.9805399179458618, 0.9819430112838745, 0.9664376974105835, 0.9685046672821045, 0.9774863719940186, 0.9868623614311218, 0.963239312171936, 0.9730779528617859, 0.978374183177948, 0.9736766219139099, 0.9924421906471252]
[2025-05-14 21:24:56,758]: Mean: 0.97520792
[2025-05-14 21:24:56,758]: Min: 0.95596641
[2025-05-14 21:24:56,758]: Max: 1.00000322
[2025-05-14 21:24:56,759]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 21:24:56,771]: Sample Values (25 elements): [0.0, -0.02283485420048237, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, -0.011417427100241184, 0.011417427100241184, -0.011417427100241184, 0.011417427100241184, 0.011417427100241184, 0.02283485420048237, 0.0, 0.02283485420048237, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, 0.011417427100241184, -0.011417427100241184, 0.011417427100241184, -0.02283485420048237, -0.02283485420048237]
[2025-05-14 21:24:56,772]: Mean: -0.00000900
[2025-05-14 21:24:56,772]: Min: -0.04566971
[2025-05-14 21:24:56,772]: Max: 0.03425228
[2025-05-14 21:24:56,772]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 21:24:56,772]: Sample Values (25 elements): [0.9592804312705994, 0.9596277475357056, 0.9595707058906555, 0.9617976546287537, 0.9600037336349487, 0.9613901972770691, 0.9590504765510559, 0.9611088037490845, 0.9615990519523621, 0.9641271233558655, 0.9656967520713806, 0.9600940346717834, 0.960796058177948, 0.9641270041465759, 0.966603696346283, 0.9621121287345886, 0.9700059294700623, 0.9576483368873596, 0.9636795520782471, 0.9607109427452087, 0.958963930606842, 0.9610031843185425, 0.9625319242477417, 0.9639779329299927, 0.9596050977706909]
[2025-05-14 21:24:56,773]: Mean: 0.96374661
[2025-05-14 21:24:56,773]: Min: 0.95649576
[2025-05-14 21:24:56,773]: Max: 0.97979307
[2025-05-14 21:24:56,774]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 21:24:56,811]: Sample Values (25 elements): [0.018065977841615677, 0.0, -0.009032988920807838, -0.009032988920807838, -0.009032988920807838, -0.009032988920807838, 0.009032988920807838, 0.0, -0.009032988920807838, 0.009032988920807838, 0.018065977841615677, -0.009032988920807838, -0.009032988920807838, 0.0, 0.018065977841615677, 0.0, 0.009032988920807838, 0.0, 0.009032988920807838, 0.009032988920807838, 0.009032988920807838, 0.0, 0.0, 0.009032988920807838, 0.009032988920807838]
[2025-05-14 21:24:56,811]: Mean: 0.00000391
[2025-05-14 21:24:56,812]: Min: -0.03613196
[2025-05-14 21:24:56,812]: Max: 0.02709897
[2025-05-14 21:24:56,812]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 21:24:56,812]: Sample Values (25 elements): [0.9596897959709167, 0.9643356800079346, 0.9638500809669495, 0.9617134928703308, 0.9632561206817627, 0.9689882397651672, 0.9678840637207031, 0.9739519953727722, 0.96370929479599, 0.9646039009094238, 0.9640110731124878, 0.9669179916381836, 0.9678359627723694, 0.9626935720443726, 0.9626393914222717, 0.9660612344741821, 0.9625763893127441, 0.9634864926338196, 0.9635018110275269, 0.9647905230522156, 0.9726395010948181, 0.9787954688072205, 0.9641311168670654, 0.966198205947876, 0.9645952582359314]
[2025-05-14 21:24:56,812]: Mean: 0.96632308
[2025-05-14 21:24:56,813]: Min: 0.95795119
[2025-05-14 21:24:56,813]: Max: 0.98118746
[2025-05-14 21:24:56,814]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 21:24:56,815]: Sample Values (25 elements): [-0.022813815623521805, 0.04562763124704361, -0.04562763124704361, 0.0, -0.022813815623521805, -0.022813815623521805, 0.022813815623521805, 0.022813815623521805, 0.0, -0.022813815623521805, 0.04562763124704361, -0.04562763124704361, 0.022813815623521805, -0.06844145059585571, 0.022813815623521805, -0.04562763124704361, 0.022813815623521805, 0.04562763124704361, 0.06844145059585571, -0.022813815623521805, 0.04562763124704361, 0.0, -0.04562763124704361, -0.04562763124704361, 0.022813815623521805]
[2025-05-14 21:24:56,815]: Mean: -0.00003777
[2025-05-14 21:24:56,815]: Min: -0.09125526
[2025-05-14 21:24:56,816]: Max: 0.06844145
[2025-05-14 21:24:56,816]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 21:24:56,816]: Sample Values (25 elements): [0.9545817375183105, 0.9587756991386414, 0.9527444839477539, 0.9500705003738403, 0.9553394913673401, 0.9511435627937317, 0.9570472240447998, 0.9458714723587036, 0.9565923810005188, 0.9484495520591736, 0.9560421705245972, 0.9554055333137512, 0.9502089619636536, 0.9535641670227051, 0.9510580897331238, 0.9546253681182861, 0.9527254700660706, 0.948920488357544, 0.9516986012458801, 0.9540038108825684, 0.954159677028656, 0.9528226852416992, 0.9470646381378174, 0.9516587853431702, 0.9525521993637085]
[2025-05-14 21:24:56,816]: Mean: 0.95221496
[2025-05-14 21:24:56,816]: Min: 0.94183797
[2025-05-14 21:24:56,816]: Max: 0.96258181
[2025-05-14 21:24:56,817]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 21:24:56,855]: Sample Values (25 elements): [-0.008682029321789742, 0.008682029321789742, 0.008682029321789742, 0.008682029321789742, 0.008682029321789742, 0.0, 0.008682029321789742, -0.008682029321789742, -0.008682029321789742, 0.0, 0.008682029321789742, 0.0, -0.017364058643579483, -0.008682029321789742, -0.008682029321789742, 0.0, -0.008682029321789742, 0.017364058643579483, 0.008682029321789742, 0.0, 0.017364058643579483, 0.008682029321789742, -0.008682029321789742, 0.008682029321789742, 0.0]
[2025-05-14 21:24:56,856]: Mean: -0.00000106
[2025-05-14 21:24:56,856]: Min: -0.02604609
[2025-05-14 21:24:56,856]: Max: 0.03472812
[2025-05-14 21:24:56,856]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 21:24:56,857]: Sample Values (25 elements): [0.9728909134864807, 0.9618025422096252, 0.9643093347549438, 0.9647932648658752, 0.962548553943634, 0.963395357131958, 0.9610773324966431, 0.9681177139282227, 0.9700543284416199, 0.9663404822349548, 0.9652456641197205, 0.9677522778511047, 0.9637647271156311, 0.9673082232475281, 0.9635730385780334, 0.962845504283905, 0.9638276696205139, 0.9673449993133545, 0.9604929089546204, 0.963169515132904, 0.9631069898605347, 0.9668993353843689, 0.9600822329521179, 0.9708393216133118, 0.9619389772415161]
[2025-05-14 21:24:56,858]: Mean: 0.96515328
[2025-05-14 21:24:56,858]: Min: 0.95656931
[2025-05-14 21:24:56,858]: Max: 0.98473024
[2025-05-14 21:24:56,859]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 21:24:56,894]: Sample Values (25 elements): [-0.013233963400125504, 0.019850945100188255, 0.006616981700062752, 0.013233963400125504, 0.006616981700062752, 0.006616981700062752, -0.013233963400125504, 0.006616981700062752, 0.0, 0.0, 0.013233963400125504, 0.013233963400125504, -0.013233963400125504, 0.0, 0.0, 0.013233963400125504, 0.006616981700062752, -0.006616981700062752, -0.006616981700062752, -0.006616981700062752, -0.006616981700062752, 0.006616981700062752, -0.006616981700062752, 0.006616981700062752, 0.0]
[2025-05-14 21:24:56,894]: Mean: -0.00000231
[2025-05-14 21:24:56,894]: Min: -0.02646793
[2025-05-14 21:24:56,894]: Max: 0.01985095
[2025-05-14 21:24:56,895]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 21:24:56,895]: Sample Values (25 elements): [0.9676121473312378, 0.9687439203262329, 0.9678446650505066, 0.9653578996658325, 0.962410032749176, 0.9702409505844116, 0.9651849865913391, 0.9730138182640076, 0.9718456268310547, 0.9695270657539368, 0.9669351577758789, 0.9690313339233398, 0.9688036441802979, 0.9684085845947266, 0.968410074710846, 0.9673409461975098, 0.9682730436325073, 0.9661539793014526, 0.9676870107650757, 0.9665189385414124, 0.9694628119468689, 0.9687293767929077, 0.9648128747940063, 0.9685211777687073, 0.9680656790733337]
[2025-05-14 21:24:56,895]: Mean: 0.96893680
[2025-05-14 21:24:56,895]: Min: 0.96120787
[2025-05-14 21:24:56,895]: Max: 0.97769767
[2025-05-14 21:24:56,895]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 21:24:56,896]: Sample Values (25 elements): [0.06656613200902939, 0.06533286720514297, -0.0967651903629303, -0.04278178885579109, -0.029457950964570045, -0.06152018532156944, -0.05668215453624725, 0.04119560495018959, -0.028436027467250824, -0.014798831194639206, -0.1104712188243866, 0.03824182599782944, 0.04145149514079094, 0.06873360276222229, -0.10321486741304398, -0.023592950776219368, 0.01334990095347166, -0.045784275978803635, 0.07696650922298431, 0.023137692362070084, -0.045142266899347305, 0.02077171579003334, -0.07421641051769257, -0.024447903037071228, -0.011910401284694672]
[2025-05-14 21:24:56,896]: Mean: 0.00007815
[2025-05-14 21:24:56,896]: Min: -0.16168694
[2025-05-14 21:24:56,896]: Max: 0.15323071
[2025-05-14 21:24:56,896]: 


QAT of ResNet18 with hardtanh down to 2 bits...
[2025-05-14 21:24:57,107]: [ResNet18_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-14 21:24:57,152]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 21:26:35,049]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 0.8441 Train Acc: 0.7183 Eval Loss: 0.9156 Eval Acc: 0.7061 (LR: 0.001000)
[2025-05-14 21:28:12,873]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.5924 Train Acc: 0.7907 Eval Loss: 0.7718 Eval Acc: 0.7413 (LR: 0.001000)
[2025-05-14 21:29:50,966]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.5469 Train Acc: 0.8056 Eval Loss: 0.7132 Eval Acc: 0.7679 (LR: 0.001000)
[2025-05-14 21:31:29,083]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.5199 Train Acc: 0.8157 Eval Loss: 0.7394 Eval Acc: 0.7621 (LR: 0.001000)
[2025-05-14 21:33:06,960]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.5028 Train Acc: 0.8223 Eval Loss: 0.6860 Eval Acc: 0.7722 (LR: 0.001000)
[2025-05-14 21:34:44,927]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.4876 Train Acc: 0.8284 Eval Loss: 0.6520 Eval Acc: 0.7910 (LR: 0.001000)
[2025-05-14 21:36:22,842]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.4749 Train Acc: 0.8323 Eval Loss: 0.6802 Eval Acc: 0.7818 (LR: 0.001000)
[2025-05-14 21:38:01,128]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.4723 Train Acc: 0.8326 Eval Loss: 0.6860 Eval Acc: 0.7809 (LR: 0.001000)
[2025-05-14 21:39:39,075]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.4607 Train Acc: 0.8379 Eval Loss: 0.6345 Eval Acc: 0.7951 (LR: 0.001000)
[2025-05-14 21:41:16,998]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.4509 Train Acc: 0.8411 Eval Loss: 0.7191 Eval Acc: 0.7749 (LR: 0.001000)
[2025-05-14 21:42:54,704]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.4480 Train Acc: 0.8428 Eval Loss: 0.6633 Eval Acc: 0.7856 (LR: 0.001000)
[2025-05-14 21:44:32,371]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.4385 Train Acc: 0.8461 Eval Loss: 0.6922 Eval Acc: 0.7806 (LR: 0.001000)
[2025-05-14 21:46:11,031]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.4296 Train Acc: 0.8475 Eval Loss: 0.5852 Eval Acc: 0.8090 (LR: 0.001000)
[2025-05-14 21:47:54,267]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.4272 Train Acc: 0.8474 Eval Loss: 0.6643 Eval Acc: 0.7904 (LR: 0.001000)
[2025-05-14 21:49:39,229]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.4132 Train Acc: 0.8553 Eval Loss: 0.5990 Eval Acc: 0.8062 (LR: 0.001000)
[2025-05-14 21:51:24,936]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.4162 Train Acc: 0.8524 Eval Loss: 0.5783 Eval Acc: 0.8128 (LR: 0.001000)
[2025-05-14 21:53:10,248]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.4037 Train Acc: 0.8579 Eval Loss: 0.6439 Eval Acc: 0.7991 (LR: 0.001000)
[2025-05-14 21:54:57,675]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.4098 Train Acc: 0.8553 Eval Loss: 0.5953 Eval Acc: 0.8047 (LR: 0.001000)
[2025-05-14 21:56:42,821]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.4025 Train Acc: 0.8566 Eval Loss: 0.6106 Eval Acc: 0.8024 (LR: 0.001000)
[2025-05-14 21:58:29,592]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.4001 Train Acc: 0.8576 Eval Loss: 0.6117 Eval Acc: 0.8036 (LR: 0.001000)
[2025-05-14 22:00:14,308]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.3901 Train Acc: 0.8620 Eval Loss: 0.6341 Eval Acc: 0.8026 (LR: 0.001000)
[2025-05-14 22:01:58,999]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.3904 Train Acc: 0.8619 Eval Loss: 0.6184 Eval Acc: 0.8074 (LR: 0.001000)
[2025-05-14 22:03:50,818]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.3837 Train Acc: 0.8622 Eval Loss: 0.5740 Eval Acc: 0.8179 (LR: 0.001000)
[2025-05-14 22:05:42,437]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.3787 Train Acc: 0.8651 Eval Loss: 0.6361 Eval Acc: 0.8011 (LR: 0.001000)
[2025-05-14 22:07:28,279]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.3748 Train Acc: 0.8666 Eval Loss: 0.6027 Eval Acc: 0.8138 (LR: 0.001000)
[2025-05-14 22:09:14,010]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.3701 Train Acc: 0.8674 Eval Loss: 0.5611 Eval Acc: 0.8212 (LR: 0.001000)
[2025-05-14 22:10:59,771]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.3701 Train Acc: 0.8702 Eval Loss: 0.5991 Eval Acc: 0.8124 (LR: 0.001000)
[2025-05-14 22:12:45,495]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.3664 Train Acc: 0.8693 Eval Loss: 0.6822 Eval Acc: 0.7874 (LR: 0.001000)
[2025-05-14 22:14:31,447]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.3651 Train Acc: 0.8708 Eval Loss: 0.6172 Eval Acc: 0.8090 (LR: 0.001000)
[2025-05-14 22:16:17,174]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.3580 Train Acc: 0.8728 Eval Loss: 0.6090 Eval Acc: 0.8135 (LR: 0.000250)
[2025-05-14 22:18:03,155]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.2869 Train Acc: 0.8997 Eval Loss: 0.5008 Eval Acc: 0.8412 (LR: 0.000250)
[2025-05-14 22:19:43,775]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.2739 Train Acc: 0.9028 Eval Loss: 0.5031 Eval Acc: 0.8419 (LR: 0.000250)
[2025-05-14 22:21:34,033]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.2744 Train Acc: 0.9037 Eval Loss: 0.5276 Eval Acc: 0.8359 (LR: 0.000250)
[2025-05-14 22:23:30,555]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.2725 Train Acc: 0.9047 Eval Loss: 0.5201 Eval Acc: 0.8394 (LR: 0.000250)
[2025-05-14 22:25:23,534]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.2668 Train Acc: 0.9063 Eval Loss: 0.4980 Eval Acc: 0.8432 (LR: 0.000250)
[2025-05-14 22:27:27,893]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.2673 Train Acc: 0.9043 Eval Loss: 0.5203 Eval Acc: 0.8387 (LR: 0.000250)
[2025-05-14 22:35:16,378]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.2650 Train Acc: 0.9065 Eval Loss: 0.5083 Eval Acc: 0.8395 (LR: 0.000250)
[2025-05-14 22:43:38,329]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.2650 Train Acc: 0.9064 Eval Loss: 0.5376 Eval Acc: 0.8341 (LR: 0.000250)
[2025-05-14 22:49:03,597]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.2612 Train Acc: 0.9074 Eval Loss: 0.5075 Eval Acc: 0.8457 (LR: 0.000250)
[2025-05-14 22:52:27,677]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.2602 Train Acc: 0.9072 Eval Loss: 0.5255 Eval Acc: 0.8394 (LR: 0.000250)
[2025-05-14 22:55:51,260]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.2601 Train Acc: 0.9064 Eval Loss: 0.5145 Eval Acc: 0.8433 (LR: 0.000250)
[2025-05-14 22:59:27,759]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.2579 Train Acc: 0.9094 Eval Loss: 0.5235 Eval Acc: 0.8383 (LR: 0.000250)
[2025-05-14 23:03:39,357]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.2573 Train Acc: 0.9086 Eval Loss: 0.5310 Eval Acc: 0.8368 (LR: 0.000250)
[2025-05-14 23:06:51,962]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.2581 Train Acc: 0.9063 Eval Loss: 0.5427 Eval Acc: 0.8362 (LR: 0.000250)
[2025-05-14 23:09:59,279]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.2553 Train Acc: 0.9093 Eval Loss: 0.5392 Eval Acc: 0.8395 (LR: 0.000063)
[2025-05-14 23:13:42,046]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.2287 Train Acc: 0.9186 Eval Loss: 0.4898 Eval Acc: 0.8503 (LR: 0.000063)
[2025-05-14 23:17:22,614]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.2223 Train Acc: 0.9230 Eval Loss: 0.4880 Eval Acc: 0.8506 (LR: 0.000063)
[2025-05-14 23:20:38,492]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.2253 Train Acc: 0.9206 Eval Loss: 0.4863 Eval Acc: 0.8514 (LR: 0.000063)
[2025-05-14 23:22:44,587]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.2223 Train Acc: 0.9204 Eval Loss: 0.4873 Eval Acc: 0.8510 (LR: 0.000063)
[2025-05-14 23:24:32,360]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.2185 Train Acc: 0.9221 Eval Loss: 0.4992 Eval Acc: 0.8518 (LR: 0.000063)
[2025-05-14 23:26:17,541]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.2197 Train Acc: 0.9228 Eval Loss: 0.4897 Eval Acc: 0.8515 (LR: 0.000063)
[2025-05-14 23:28:04,677]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.2210 Train Acc: 0.9223 Eval Loss: 0.5088 Eval Acc: 0.8482 (LR: 0.000063)
[2025-05-14 23:30:23,080]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.2249 Train Acc: 0.9193 Eval Loss: 0.4930 Eval Acc: 0.8510 (LR: 0.000063)
[2025-05-14 23:32:25,953]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.2234 Train Acc: 0.9205 Eval Loss: 0.4870 Eval Acc: 0.8495 (LR: 0.000063)
[2025-05-14 23:34:26,172]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.2190 Train Acc: 0.9221 Eval Loss: 0.4903 Eval Acc: 0.8503 (LR: 0.000063)
[2025-05-14 23:36:27,747]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.2245 Train Acc: 0.9207 Eval Loss: 0.4928 Eval Acc: 0.8485 (LR: 0.000063)
[2025-05-14 23:38:42,171]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.2204 Train Acc: 0.9214 Eval Loss: 0.4912 Eval Acc: 0.8532 (LR: 0.000063)
[2025-05-14 23:40:40,917]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.2193 Train Acc: 0.9226 Eval Loss: 0.4961 Eval Acc: 0.8554 (LR: 0.000063)
[2025-05-14 23:42:39,958]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.2194 Train Acc: 0.9215 Eval Loss: 0.4994 Eval Acc: 0.8481 (LR: 0.000063)
[2025-05-14 23:44:55,373]: [ResNet18_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.2185 Train Acc: 0.9230 Eval Loss: 0.4892 Eval Acc: 0.8546 (LR: 0.000063)
[2025-05-14 23:44:55,374]: [ResNet18_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8554
[2025-05-14 23:44:55,497]: 


Quantization of model down to 2 bits finished
[2025-05-14 23:44:55,498]: Model Architecture:
[2025-05-14 23:44:55,607]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0982], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15529567003250122, max_val=0.1392369270324707)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1737], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2276861071586609, max_val=0.2933092415332794)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1400], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2147780805826187, max_val=0.2051156610250473)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0859], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13088899850845337, max_val=0.12676818668842316)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0813], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12392327189445496, max_val=0.1201096624135971)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0465], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07156065851449966, max_val=0.0679602399468422)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1265], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1912968009710312, max_val=0.18834935128688812)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0416], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06235048547387123, max_val=0.06251037120819092)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0413], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06066633015871048, max_val=0.06327147036790848)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0357], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.052923932671546936, max_val=0.05405412241816521)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04829651489853859, max_val=0.04587901383638382)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0825], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12539838254451752, max_val=0.12199102342128754)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0309], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.046208806335926056, max_val=0.046564530581235886)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0306], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04801931977272034, max_val=0.04384055733680725)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0250], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03759786859154701, max_val=0.037520136684179306)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0196], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.030148353427648544, max_val=0.028627216815948486)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0547], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08202394843101501, max_val=0.08202797919511795)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0193], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.027242083102464676, max_val=0.03078373521566391)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023189343512058258, max_val=0.023089855909347534)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 23:44:55,608]: 
Model Weights:
[2025-05-14 23:44:55,608]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 23:44:55,611]: Sample Values (25 elements): [0.12123935669660568, -0.1870378702878952, -0.18556110560894012, -0.09651748836040497, -0.04286074638366699, 0.18324951827526093, 0.0903608649969101, 0.3047840893268585, -0.08624497801065445, -0.1261342167854309, -0.2509456276893616, 0.2668454349040985, 0.14294375479221344, -0.13773605227470398, -0.062352828681468964, -0.21078196167945862, -0.1319303959608078, 0.1677420735359192, 0.12334509938955307, 0.167509987950325, 0.3684741258621216, 0.05008433014154434, -0.011092768050730228, -0.15287567675113678, 0.010467220097780228]
[2025-05-14 23:44:55,612]: Mean: -0.00029078
[2025-05-14 23:44:55,612]: Min: -0.34812924
[2025-05-14 23:44:55,613]: Max: 0.36847413
[2025-05-14 23:44:55,613]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 23:44:55,613]: Sample Values (25 elements): [1.0151041746139526, 0.8505665063858032, 0.9503559470176697, 0.8833630681037903, 0.9471638798713684, 1.0847526788711548, 1.1237868070602417, 0.9380156397819519, 1.0438650846481323, 1.0085331201553345, 0.9773942232131958, 0.9872605800628662, 0.8905830979347229, 1.0704832077026367, 0.9822801947593689, 1.0175392627716064, 0.8890984058380127, 1.0322693586349487, 0.9032474160194397, 0.8057085275650024, 0.9324895739555359, 0.9785148501396179, 0.9890714287757874, 0.9375219941139221, 1.0352036952972412]
[2025-05-14 23:44:55,614]: Mean: 0.96807915
[2025-05-14 23:44:55,614]: Min: 0.79796284
[2025-05-14 23:44:55,614]: Max: 1.13669503
[2025-05-14 23:44:55,615]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 23:44:55,616]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09817773103713989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,616]: Mean: 0.00042079
[2025-05-14 23:44:55,616]: Min: -0.19635546
[2025-05-14 23:44:55,616]: Max: 0.09817773
[2025-05-14 23:44:55,616]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 23:44:55,617]: Sample Values (25 elements): [0.934580385684967, 0.9980866312980652, 0.9903193116188049, 0.9885470271110535, 1.0713353157043457, 0.9989323019981384, 0.9789871573448181, 1.0090956687927246, 1.021738886833191, 0.9957689046859741, 0.9827261567115784, 0.9677140712738037, 1.0357823371887207, 0.9671019911766052, 1.0092949867248535, 1.0221503973007202, 0.9649242162704468, 0.9736137986183167, 0.9625401496887207, 0.9904475808143616, 1.0066990852355957, 1.0193413496017456, 1.0156255960464478, 0.9749237298965454, 0.9358482360839844]
[2025-05-14 23:44:55,617]: Mean: 0.98171502
[2025-05-14 23:44:55,617]: Min: 0.89732575
[2025-05-14 23:44:55,617]: Max: 1.07461059
[2025-05-14 23:44:55,619]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 23:44:55,620]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,620]: Mean: -0.00031563
[2025-05-14 23:44:55,620]: Min: -0.17366521
[2025-05-14 23:44:55,621]: Max: 0.34733042
[2025-05-14 23:44:55,621]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 23:44:55,621]: Sample Values (25 elements): [0.9525411128997803, 1.0345478057861328, 0.944381833076477, 0.9178577065467834, 0.9408940672874451, 1.0149112939834595, 0.9884693026542664, 0.9080352783203125, 0.9426116347312927, 0.8901048898696899, 1.0762393474578857, 0.9214383363723755, 0.9190274477005005, 0.9170712232589722, 0.9257797002792358, 0.9048390984535217, 0.9419460892677307, 0.9797326922416687, 0.9618494510650635, 0.9202411770820618, 0.9532886743545532, 0.9243147969245911, 0.9670878648757935, 0.9214858412742615, 0.9551371335983276]
[2025-05-14 23:44:55,621]: Mean: 0.95327628
[2025-05-14 23:44:55,621]: Min: 0.84993434
[2025-05-14 23:44:55,622]: Max: 1.49357319
[2025-05-14 23:44:55,623]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 23:44:55,623]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,624]: Mean: 0.00002278
[2025-05-14 23:44:55,624]: Min: -0.27992937
[2025-05-14 23:44:55,624]: Max: 0.13996468
[2025-05-14 23:44:55,624]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 23:44:55,624]: Sample Values (25 elements): [0.9778814911842346, 0.9712455868721008, 1.0133342742919922, 0.9626222848892212, 1.0344969034194946, 1.064772605895996, 0.9610090255737305, 1.0191999673843384, 0.9671868085861206, 1.036032795906067, 0.9914055466651917, 0.9439668655395508, 0.9627623558044434, 0.9935247898101807, 0.990627110004425, 1.021727204322815, 0.9529455900192261, 0.9764075875282288, 1.0085691213607788, 0.9609778523445129, 1.039136290550232, 0.9630542993545532, 1.002263069152832, 1.0220496654510498, 0.969703197479248]
[2025-05-14 23:44:55,625]: Mean: 0.98923641
[2025-05-14 23:44:55,625]: Min: 0.93871051
[2025-05-14 23:44:55,625]: Max: 1.06477261
[2025-05-14 23:44:55,626]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 23:44:55,627]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08588562905788422, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,627]: Mean: 0.00019337
[2025-05-14 23:44:55,627]: Min: -0.17177126
[2025-05-14 23:44:55,628]: Max: 0.08588563
[2025-05-14 23:44:55,628]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 23:44:55,628]: Sample Values (25 elements): [1.039631724357605, 0.9707444906234741, 0.9641069173812866, 0.9410228729248047, 0.9525404572486877, 0.972145140171051, 0.9308561086654663, 0.9945099353790283, 0.9844502806663513, 0.9796279668807983, 0.9645251631736755, 0.9663572311401367, 0.9727258086204529, 0.9220944046974182, 1.0371861457824707, 0.9496757984161377, 0.9765711426734924, 0.9439792037010193, 0.9839068651199341, 0.9738258123397827, 0.9334385395050049, 0.975806474685669, 0.9731011390686035, 0.9716142416000366, 1.1978145837783813]
[2025-05-14 23:44:55,628]: Mean: 0.97890770
[2025-05-14 23:44:55,628]: Min: 0.91364586
[2025-05-14 23:44:55,629]: Max: 1.19781458
[2025-05-14 23:44:55,630]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 23:44:55,631]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08134442567825317, 0.0, 0.0, 0.0, 0.08134442567825317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,631]: Mean: 0.00021625
[2025-05-14 23:44:55,631]: Min: -0.16268885
[2025-05-14 23:44:55,631]: Max: 0.08134443
[2025-05-14 23:44:55,631]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 23:44:55,632]: Sample Values (25 elements): [0.9890760183334351, 0.9579563736915588, 0.9898868203163147, 0.973377525806427, 0.9591285586357117, 0.9835158586502075, 0.9639377593994141, 0.9656649231910706, 0.9695143699645996, 0.9709456562995911, 0.9878324866294861, 0.9638954401016235, 0.971259355545044, 0.9768257141113281, 0.9536352157592773, 0.9825229644775391, 0.9616984724998474, 0.985727846622467, 0.9773846864700317, 0.9858096837997437, 0.9633418321609497, 0.9817696213722229, 0.9506790041923523, 0.9799972772598267, 0.960004448890686]
[2025-05-14 23:44:55,632]: Mean: 0.97464633
[2025-05-14 23:44:55,632]: Min: 0.95067900
[2025-05-14 23:44:55,632]: Max: 1.02605057
[2025-05-14 23:44:55,635]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 23:44:55,637]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04650702700018883, 0.0, -0.04650702700018883, -0.04650702700018883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04650702700018883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04650702700018883, 0.0]
[2025-05-14 23:44:55,637]: Mean: 0.00000978
[2025-05-14 23:44:55,637]: Min: -0.09301405
[2025-05-14 23:44:55,637]: Max: 0.04650703
[2025-05-14 23:44:55,638]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 23:44:55,638]: Sample Values (25 elements): [0.9628031253814697, 0.9958413243293762, 0.9898180961608887, 0.9763697981834412, 0.9746691584587097, 0.9852973818778992, 1.0106662511825562, 0.9693506956100464, 0.979752779006958, 0.9491546154022217, 0.9773144125938416, 0.9885357022285461, 0.9920579195022583, 0.9887171387672424, 0.9592070579528809, 0.9897529482841492, 0.9875385761260986, 0.9785200357437134, 0.9691355228424072, 0.9784119725227356, 0.9791401028633118, 0.9582785964012146, 0.9872835874557495, 0.978986918926239, 0.9840391278266907]
[2025-05-14 23:44:55,638]: Mean: 0.97766149
[2025-05-14 23:44:55,638]: Min: 0.94915462
[2025-05-14 23:44:55,638]: Max: 1.02128315
[2025-05-14 23:44:55,639]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 23:44:55,640]: Sample Values (25 elements): [0.0, 0.0, 0.12654872238636017, 0.0, 0.0, 0.12654872238636017, 0.0, 0.0, 0.0, -0.12654872238636017, 0.12654872238636017, 0.0, 0.0, 0.0, 0.12654872238636017, 0.0, 0.0, -0.12654872238636017, 0.12654872238636017, 0.0, 0.12654872238636017, 0.12654872238636017, 0.0, -0.12654872238636017, 0.0]
[2025-05-14 23:44:55,641]: Mean: 0.00050978
[2025-05-14 23:44:55,641]: Min: -0.25309744
[2025-05-14 23:44:55,641]: Max: 0.12654872
[2025-05-14 23:44:55,641]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 23:44:55,642]: Sample Values (25 elements): [0.961083173751831, 0.913889467716217, 0.9038363695144653, 0.9367958307266235, 0.9405221939086914, 0.9262077212333679, 0.9362469911575317, 0.9216121435165405, 0.9237200021743774, 0.9316053986549377, 0.9250277280807495, 0.9401403665542603, 0.9278096556663513, 0.9161204695701599, 0.9146832227706909, 0.9601582884788513, 0.9469039440155029, 0.9402649998664856, 0.9258752465248108, 0.9435545802116394, 0.9324501752853394, 0.9319174289703369, 0.9199938178062439, 0.9413398504257202, 0.9478527307510376]
[2025-05-14 23:44:55,642]: Mean: 0.93183839
[2025-05-14 23:44:55,642]: Min: 0.89892876
[2025-05-14 23:44:55,642]: Max: 0.97544497
[2025-05-14 23:44:55,643]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 23:44:55,645]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.04162025824189186, 0.0, 0.04162025824189186, 0.0, 0.0, 0.04162025824189186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04162025824189186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,645]: Mean: 0.00007085
[2025-05-14 23:44:55,645]: Min: -0.04162026
[2025-05-14 23:44:55,645]: Max: 0.08324052
[2025-05-14 23:44:55,645]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 23:44:55,646]: Sample Values (25 elements): [0.9775859713554382, 0.9744116067886353, 0.9963436722755432, 1.010162353515625, 0.9899284839630127, 0.9758349061012268, 0.9785714745521545, 0.9622654318809509, 0.9734170436859131, 0.9671189188957214, 0.9853708744049072, 0.9761765599250793, 0.9655930995941162, 1.0088130235671997, 0.9756302237510681, 0.994623601436615, 0.9698122143745422, 0.987068772315979, 0.9755434393882751, 0.9900661110877991, 0.9672359228134155, 0.9795670509338379, 0.9801917672157288, 0.9713346362113953, 0.9737558364868164]
[2025-05-14 23:44:55,646]: Mean: 0.98294711
[2025-05-14 23:44:55,646]: Min: 0.95717472
[2025-05-14 23:44:55,646]: Max: 1.03557825
[2025-05-14 23:44:55,648]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 23:44:55,650]: Sample Values (25 elements): [0.0, 0.041312649846076965, -0.041312649846076965, 0.0, -0.041312649846076965, 0.0, 0.041312649846076965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041312649846076965, 0.0, 0.0, -0.041312649846076965, 0.0, 0.0, 0.0, 0.0, -0.041312649846076965, 0.0]
[2025-05-14 23:44:55,650]: Mean: 0.00016922
[2025-05-14 23:44:55,650]: Min: -0.04131265
[2025-05-14 23:44:55,650]: Max: 0.08262530
[2025-05-14 23:44:55,650]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 23:44:55,650]: Sample Values (25 elements): [0.9959187507629395, 0.9929918646812439, 1.0155357122421265, 0.9676335453987122, 0.9858973622322083, 0.9829580187797546, 0.9692120552062988, 0.9898448586463928, 0.9858062267303467, 0.9800524115562439, 0.9743620157241821, 0.9971135854721069, 0.9989693760871887, 0.9793453216552734, 0.9866672158241272, 0.9937018752098083, 0.9818907380104065, 0.9680413603782654, 0.9802078604698181, 0.9831287264823914, 0.9781427979469299, 0.986743688583374, 0.9993470311164856, 0.9784444570541382, 1.020054578781128]
[2025-05-14 23:44:55,651]: Mean: 0.98716986
[2025-05-14 23:44:55,651]: Min: 0.95697570
[2025-05-14 23:44:55,651]: Max: 1.02005458
[2025-05-14 23:44:55,652]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 23:44:55,655]: Sample Values (25 elements): [-0.03565935790538788, 0.0, -0.03565935790538788, 0.03565935790538788, 0.0, 0.0, 0.0, -0.03565935790538788, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03565935790538788, 0.03565935790538788, 0.0, 0.0, 0.0, 0.0, -0.03565935790538788, 0.0, -0.03565935790538788, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,656]: Mean: 0.00017085
[2025-05-14 23:44:55,656]: Min: -0.03565936
[2025-05-14 23:44:55,656]: Max: 0.07131872
[2025-05-14 23:44:55,656]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 23:44:55,658]: Sample Values (25 elements): [0.9683607816696167, 0.9640401601791382, 0.9658901691436768, 0.9609547853469849, 0.9604065418243408, 0.9606238603591919, 0.9670347571372986, 0.9678020477294922, 0.9745249152183533, 0.971274733543396, 0.967290461063385, 0.9680431485176086, 0.9653952717781067, 0.9677692651748657, 0.9726910591125488, 0.9667825102806091, 0.9725711345672607, 0.9743481874465942, 0.9660412669181824, 0.9641022682189941, 0.9718976616859436, 0.96778404712677, 0.9779399633407593, 0.9651117324829102, 0.9806295037269592]
[2025-05-14 23:44:55,658]: Mean: 0.96862364
[2025-05-14 23:44:55,658]: Min: 0.95649970
[2025-05-14 23:44:55,658]: Max: 0.98962206
[2025-05-14 23:44:55,659]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 23:44:55,670]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.03139181062579155, 0.0, 0.03139181062579155, 0.0, -0.03139181062579155, 0.0, 0.0, 0.03139181062579155, 0.0, 0.0, 0.0, -0.03139181062579155, 0.0, -0.03139181062579155, 0.0, 0.03139181062579155, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,671]: Mean: 0.00025834
[2025-05-14 23:44:55,671]: Min: -0.06278362
[2025-05-14 23:44:55,671]: Max: 0.03139181
[2025-05-14 23:44:55,671]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 23:44:55,672]: Sample Values (25 elements): [0.9799106121063232, 0.9786810278892517, 0.9814064502716064, 0.9776613712310791, 0.9790198802947998, 0.9687124490737915, 0.9785234928131104, 0.9774132966995239, 0.9747559428215027, 0.9698734879493713, 0.9742005467414856, 0.9802455902099609, 0.9735547304153442, 0.9743096828460693, 0.9790133833885193, 0.9789771437644958, 0.9738633632659912, 0.969142735004425, 0.9872084259986877, 0.9700393080711365, 0.9843053817749023, 0.9630772471427917, 0.9673678874969482, 0.9772020578384399, 0.9766458868980408]
[2025-05-14 23:44:55,672]: Mean: 0.97554755
[2025-05-14 23:44:55,672]: Min: 0.95865381
[2025-05-14 23:44:55,672]: Max: 0.99647528
[2025-05-14 23:44:55,673]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 23:44:55,674]: Sample Values (25 elements): [0.0, 0.0, 0.08246314525604248, 0.08246314525604248, 0.0, -0.08246314525604248, 0.0, -0.08246314525604248, 0.0, -0.08246314525604248, 0.08246314525604248, 0.0, 0.0, 0.08246314525604248, 0.0, 0.0, -0.08246314525604248, 0.0, 0.0, 0.0, 0.0, -0.08246314525604248, 0.0, -0.08246314525604248, -0.08246314525604248]
[2025-05-14 23:44:55,674]: Mean: -0.00006543
[2025-05-14 23:44:55,674]: Min: -0.16492629
[2025-05-14 23:44:55,674]: Max: 0.08246315
[2025-05-14 23:44:55,674]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 23:44:55,675]: Sample Values (25 elements): [0.9370768070220947, 0.9493823647499084, 0.9401583671569824, 0.9317890405654907, 0.9435868859291077, 0.9427099227905273, 0.9339849948883057, 0.9534051418304443, 0.9327898025512695, 0.9379158020019531, 0.9327040314674377, 0.9425868988037109, 0.9477105736732483, 0.9406276345252991, 0.9450320601463318, 0.9372951984405518, 0.9512536525726318, 0.9382045269012451, 0.9462037086486816, 0.9431446194648743, 0.9464387893676758, 0.9371248483657837, 0.9247502088546753, 0.9417151808738708, 0.9442988038063049]
[2025-05-14 23:44:55,675]: Mean: 0.94100964
[2025-05-14 23:44:55,675]: Min: 0.92325407
[2025-05-14 23:44:55,676]: Max: 0.96620387
[2025-05-14 23:44:55,677]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 23:44:55,683]: Sample Values (25 elements): [0.0, -0.030924368649721146, 0.0, 0.0, 0.030924368649721146, -0.030924368649721146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030924368649721146, 0.0, 0.030924368649721146, 0.0, 0.0, 0.0, 0.030924368649721146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:55,684]: Mean: 0.00007466
[2025-05-14 23:44:55,684]: Min: -0.03092437
[2025-05-14 23:44:55,684]: Max: 0.06184874
[2025-05-14 23:44:55,684]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 23:44:55,685]: Sample Values (25 elements): [0.9721183776855469, 0.9697554111480713, 0.9736292958259583, 0.9780343174934387, 0.9867067933082581, 0.964909553527832, 0.9790824055671692, 0.9819822907447815, 0.974200963973999, 0.984148383140564, 0.9823726415634155, 0.9795635342597961, 0.9839840531349182, 0.980022668838501, 0.9667922854423523, 0.9995609521865845, 0.9841499924659729, 0.9782451391220093, 0.9787581562995911, 0.9764536023139954, 0.9682059288024902, 0.9993383884429932, 0.9795722365379333, 0.978128969669342, 0.9816060066223145]
[2025-05-14 23:44:55,685]: Mean: 0.97778189
[2025-05-14 23:44:55,685]: Min: 0.95912707
[2025-05-14 23:44:55,685]: Max: 1.02216828
[2025-05-14 23:44:55,686]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 23:44:55,693]: Sample Values (25 elements): [0.0, 0.030620014294981956, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030620014294981956, 0.0, 0.030620014294981956, 0.0, 0.0, -0.030620014294981956, 0.0, 0.0, 0.0, 0.0, 0.030620014294981956, 0.0, 0.0, -0.030620014294981956, 0.0]
[2025-05-14 23:44:55,693]: Mean: 0.00010440
[2025-05-14 23:44:55,693]: Min: -0.06124003
[2025-05-14 23:44:55,694]: Max: 0.03062001
[2025-05-14 23:44:55,694]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 23:44:55,694]: Sample Values (25 elements): [0.9783447980880737, 0.972652018070221, 0.9836336970329285, 0.9791058301925659, 0.9796470403671265, 0.9885737895965576, 0.9851320385932922, 0.9781520366668701, 0.9772064089775085, 0.9797562956809998, 0.9752025008201599, 0.9754132032394409, 0.975521445274353, 0.9709604978561401, 0.9771652817726135, 0.9701332449913025, 0.9996719360351562, 0.9850379228591919, 0.9720056653022766, 0.9714081287384033, 0.9736378192901611, 0.9893840551376343, 0.9899488091468811, 0.9835197925567627, 0.9824538230895996]
[2025-05-14 23:44:55,694]: Mean: 0.98054206
[2025-05-14 23:44:55,694]: Min: 0.96148926
[2025-05-14 23:44:55,695]: Max: 1.00793874
[2025-05-14 23:44:55,696]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 23:44:55,712]: Sample Values (25 elements): [-0.025039367377758026, 0.0, 0.025039367377758026, 0.0, 0.0, -0.025039367377758026, -0.025039367377758026, 0.0, -0.025039367377758026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025039367377758026, 0.0, -0.025039367377758026, 0.0, 0.0, 0.0, 0.0, -0.025039367377758026, -0.025039367377758026]
[2025-05-14 23:44:55,713]: Mean: 0.00006351
[2025-05-14 23:44:55,713]: Min: -0.05007873
[2025-05-14 23:44:55,713]: Max: 0.02503937
[2025-05-14 23:44:55,713]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 23:44:55,714]: Sample Values (25 elements): [0.9672570824623108, 0.9699767231941223, 0.9648725986480713, 0.9670994877815247, 0.9635729193687439, 0.9654455780982971, 0.9650449156761169, 0.9713135361671448, 0.9606650471687317, 0.9671440720558167, 0.9741438627243042, 0.9602690935134888, 0.965929388999939, 0.9678922295570374, 0.9635933637619019, 0.9670992493629456, 0.963847279548645, 0.9819896221160889, 0.962521493434906, 0.9628353118896484, 0.9697766304016113, 0.9611285328865051, 0.9656161665916443, 0.9641873836517334, 0.9670774936676025]
[2025-05-14 23:44:55,714]: Mean: 0.96581912
[2025-05-14 23:44:55,714]: Min: 0.95760763
[2025-05-14 23:44:55,714]: Max: 0.98449379
[2025-05-14 23:44:55,715]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 23:44:55,781]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019591867923736572, -0.019591867923736572, 0.019591867923736572, 0.019591867923736572, 0.0, 0.0, 0.0, -0.019591867923736572, 0.0, 0.0, -0.019591867923736572, 0.0, 0.0, 0.0, -0.019591867923736572, 0.0]
[2025-05-14 23:44:55,781]: Mean: 0.00005617
[2025-05-14 23:44:55,781]: Min: -0.03918374
[2025-05-14 23:44:55,782]: Max: 0.01959187
[2025-05-14 23:44:55,782]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 23:44:55,782]: Sample Values (25 elements): [0.9673031568527222, 0.9662565588951111, 0.968386173248291, 0.9752276539802551, 0.9649509787559509, 0.9654752016067505, 0.968710720539093, 0.9657561182975769, 0.9676256775856018, 0.9612392783164978, 0.9689050912857056, 0.9676911234855652, 0.9761224985122681, 0.9793854355812073, 0.9687637686729431, 0.9726259708404541, 0.9689077734947205, 0.968852162361145, 0.9740834832191467, 0.9709494113922119, 0.9658188819885254, 0.9686288237571716, 0.9716041088104248, 0.9667870402336121, 0.9631796479225159]
[2025-05-14 23:44:55,782]: Mean: 0.96917218
[2025-05-14 23:44:55,783]: Min: 0.96022522
[2025-05-14 23:44:55,783]: Max: 0.98783582
[2025-05-14 23:44:55,784]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 23:44:55,785]: Sample Values (25 elements): [-0.05468396097421646, 0.0, 0.0, 0.05468396097421646, 0.05468396097421646, -0.05468396097421646, 0.0, 0.05468396097421646, 0.0, 0.0, 0.05468396097421646, 0.0, -0.05468396097421646, 0.0, -0.05468396097421646, 0.0, 0.05468396097421646, 0.0, 0.0, 0.0, -0.05468396097421646, 0.0, 0.0, -0.05468396097421646, -0.05468396097421646]
[2025-05-14 23:44:55,786]: Mean: 0.00002378
[2025-05-14 23:44:55,786]: Min: -0.05468396
[2025-05-14 23:44:55,787]: Max: 0.05468396
[2025-05-14 23:44:55,787]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 23:44:55,787]: Sample Values (25 elements): [0.9558005332946777, 0.9537578821182251, 0.9549545049667358, 0.9551517367362976, 0.9503307342529297, 0.9542959332466125, 0.9500842094421387, 0.9529455900192261, 0.956454336643219, 0.9555519819259644, 0.9520587921142578, 0.9537238478660583, 0.9517623782157898, 0.9547191858291626, 0.9503389000892639, 0.9539080858230591, 0.9535382390022278, 0.9528033137321472, 0.95543372631073, 0.9517516493797302, 0.9531264305114746, 0.949531614780426, 0.956605851650238, 0.9483327269554138, 0.9545300006866455]
[2025-05-14 23:44:55,788]: Mean: 0.95263588
[2025-05-14 23:44:55,788]: Min: 0.94459242
[2025-05-14 23:44:55,788]: Max: 0.96389514
[2025-05-14 23:44:55,790]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 23:44:55,914]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.01934196427464485, 0.0, -0.01934196427464485, -0.01934196427464485, 0.0, 0.0, 0.0, 0.0, 0.01934196427464485, 0.0, 0.0, 0.0, 0.01934196427464485, 0.01934196427464485, -0.01934196427464485, 0.01934196427464485, -0.01934196427464485]
[2025-05-14 23:44:55,914]: Mean: 0.00001055
[2025-05-14 23:44:55,915]: Min: -0.01934196
[2025-05-14 23:44:55,915]: Max: 0.03868393
[2025-05-14 23:44:55,915]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 23:44:55,917]: Sample Values (25 elements): [0.9635598659515381, 0.9672218561172485, 0.9641404151916504, 0.9718096256256104, 0.9643325209617615, 0.9659274220466614, 0.967119038105011, 0.9621356725692749, 0.9668573141098022, 0.9685789942741394, 0.9611932635307312, 0.9650719165802002, 0.9634548425674438, 0.9649502635002136, 0.9679644107818604, 0.9623602032661438, 0.9587021470069885, 0.9627436399459839, 0.9583329558372498, 0.9688501358032227, 0.9678610563278198, 0.9616365432739258, 0.9646651148796082, 0.9628334045410156, 0.970021665096283]
[2025-05-14 23:44:55,917]: Mean: 0.96665448
[2025-05-14 23:44:55,918]: Min: 0.95660758
[2025-05-14 23:44:55,918]: Max: 0.98573440
[2025-05-14 23:44:55,921]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 23:44:56,018]: Sample Values (25 elements): [0.0, 0.015426382422447205, 0.0, 0.0, 0.0, 0.0, 0.015426382422447205, -0.015426382422447205, 0.015426382422447205, 0.0, 0.015426382422447205, 0.0, 0.015426382422447205, 0.0, -0.015426382422447205, -0.015426382422447205, 0.0, -0.015426382422447205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 23:44:56,019]: Mean: -0.00000909
[2025-05-14 23:44:56,019]: Min: -0.03085276
[2025-05-14 23:44:56,019]: Max: 0.01542638
[2025-05-14 23:44:56,019]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 23:44:56,020]: Sample Values (25 elements): [0.9721002578735352, 0.9681587815284729, 0.9690377712249756, 0.9672184586524963, 0.969607412815094, 0.9664967656135559, 0.9718754887580872, 0.9694315791130066, 0.9747060537338257, 0.9684008955955505, 0.9646892547607422, 0.972548246383667, 0.9727840423583984, 0.965369462966919, 0.9671401977539062, 0.9695571660995483, 0.967695951461792, 0.9677532911300659, 0.9659819006919861, 0.9740453362464905, 0.965484082698822, 0.9674538373947144, 0.9686434268951416, 0.964769184589386, 0.9669421315193176]
[2025-05-14 23:44:56,020]: Mean: 0.96803069
[2025-05-14 23:44:56,020]: Min: 0.96074533
[2025-05-14 23:44:56,020]: Max: 0.97610128
[2025-05-14 23:44:56,020]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 23:44:56,021]: Sample Values (25 elements): [-0.07539083063602448, -0.11081951856613159, 0.031159404665231705, -0.018925126641988754, -0.022280678153038025, -0.038784209638834, -0.047369930893182755, 0.1362401694059372, -0.13004283607006073, 0.07678426802158356, -0.01537680346518755, 0.03376729041337967, 0.048345793038606644, 0.0985991358757019, 0.03445792198181152, -0.1268896907567978, -0.045039255172014236, -0.08199644088745117, -0.06941204518079758, -0.08951088786125183, 0.09371040016412735, 0.02866855449974537, -0.054734934121370316, -0.059593986719846725, -0.08610019832849503]
[2025-05-14 23:44:56,021]: Mean: 0.00007816
[2025-05-14 23:44:56,021]: Min: -0.16125686
[2025-05-14 23:44:56,021]: Max: 0.15324235
