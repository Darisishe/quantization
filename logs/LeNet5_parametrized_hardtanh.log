[2025-05-20 07:06:55,006]: Checkpoint of model at path [checkpoint/LeNet5_hardtanh.ckpt] will be used for QAT
[2025-05-20 07:06:55,007]: 


QAT of LeNet5 with parametrized_hardtanh down to 2 bits...
[2025-05-20 07:06:55,156]: [LeNet5_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-20 07:06:55,275]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 07:07:23,365]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.6374 Train Acc: 0.4149 Eval Loss: 1.4261 Eval Acc: 0.4894 (LR: 0.010000)
[2025-05-20 07:07:49,767]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 1.5414 Train Acc: 0.4462 Eval Loss: 1.4290 Eval Acc: 0.4875 (LR: 0.010000)
[2025-05-20 07:08:16,430]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 1.5056 Train Acc: 0.4603 Eval Loss: 1.4187 Eval Acc: 0.4925 (LR: 0.010000)
[2025-05-20 07:08:43,123]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 1.4867 Train Acc: 0.4685 Eval Loss: 1.3557 Eval Acc: 0.5093 (LR: 0.010000)
[2025-05-20 07:09:10,022]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 1.4800 Train Acc: 0.4699 Eval Loss: 1.4032 Eval Acc: 0.4980 (LR: 0.010000)
[2025-05-20 07:09:36,422]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 1.4766 Train Acc: 0.4739 Eval Loss: 1.4068 Eval Acc: 0.4925 (LR: 0.010000)
[2025-05-20 07:10:04,196]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 1.4691 Train Acc: 0.4760 Eval Loss: 1.3256 Eval Acc: 0.5301 (LR: 0.010000)
[2025-05-20 07:10:31,155]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 1.4701 Train Acc: 0.4731 Eval Loss: 1.3965 Eval Acc: 0.5060 (LR: 0.010000)
[2025-05-20 07:10:57,420]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 1.4677 Train Acc: 0.4763 Eval Loss: 1.3772 Eval Acc: 0.5125 (LR: 0.010000)
[2025-05-20 07:11:23,801]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 1.4723 Train Acc: 0.4763 Eval Loss: 1.3595 Eval Acc: 0.5184 (LR: 0.010000)
[2025-05-20 07:11:51,466]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 1.4651 Train Acc: 0.4788 Eval Loss: 1.3821 Eval Acc: 0.5050 (LR: 0.010000)
[2025-05-20 07:12:19,211]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 1.4628 Train Acc: 0.4782 Eval Loss: 1.3241 Eval Acc: 0.5233 (LR: 0.010000)
[2025-05-20 07:12:47,044]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 1.4652 Train Acc: 0.4763 Eval Loss: 1.3526 Eval Acc: 0.5114 (LR: 0.010000)
[2025-05-20 07:13:13,368]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 1.4716 Train Acc: 0.4771 Eval Loss: 1.3098 Eval Acc: 0.5232 (LR: 0.010000)
[2025-05-20 07:13:39,866]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 1.4638 Train Acc: 0.4774 Eval Loss: 1.3333 Eval Acc: 0.5179 (LR: 0.001000)
[2025-05-20 07:14:06,109]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 1.3660 Train Acc: 0.5120 Eval Loss: 1.2685 Eval Acc: 0.5396 (LR: 0.001000)
[2025-05-20 07:14:31,918]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 1.3544 Train Acc: 0.5194 Eval Loss: 1.2394 Eval Acc: 0.5584 (LR: 0.001000)
[2025-05-20 07:14:58,153]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 1.3520 Train Acc: 0.5206 Eval Loss: 1.2915 Eval Acc: 0.5440 (LR: 0.001000)
[2025-05-20 07:15:25,857]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 1.3527 Train Acc: 0.5206 Eval Loss: 1.2412 Eval Acc: 0.5567 (LR: 0.001000)
[2025-05-20 07:15:52,106]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 1.3489 Train Acc: 0.5192 Eval Loss: 1.2795 Eval Acc: 0.5454 (LR: 0.001000)
[2025-05-20 07:16:19,257]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 1.3530 Train Acc: 0.5211 Eval Loss: 1.2878 Eval Acc: 0.5431 (LR: 0.001000)
[2025-05-20 07:16:45,558]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 1.3607 Train Acc: 0.5184 Eval Loss: 1.2510 Eval Acc: 0.5605 (LR: 0.001000)
[2025-05-20 07:17:12,651]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 1.3533 Train Acc: 0.5170 Eval Loss: 1.3083 Eval Acc: 0.5382 (LR: 0.001000)
[2025-05-20 07:17:40,090]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 1.3544 Train Acc: 0.5177 Eval Loss: 1.2411 Eval Acc: 0.5579 (LR: 0.001000)
[2025-05-20 07:18:07,648]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 1.3597 Train Acc: 0.5174 Eval Loss: 1.2683 Eval Acc: 0.5501 (LR: 0.001000)
[2025-05-20 07:18:35,977]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 1.3518 Train Acc: 0.5177 Eval Loss: 1.3210 Eval Acc: 0.5372 (LR: 0.001000)
[2025-05-20 07:19:03,184]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 1.3577 Train Acc: 0.5181 Eval Loss: 1.2875 Eval Acc: 0.5522 (LR: 0.001000)
[2025-05-20 07:19:31,350]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 1.3506 Train Acc: 0.5206 Eval Loss: 1.2632 Eval Acc: 0.5503 (LR: 0.001000)
[2025-05-20 07:19:58,724]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 1.3570 Train Acc: 0.5173 Eval Loss: 1.2399 Eval Acc: 0.5543 (LR: 0.001000)
[2025-05-20 07:20:26,541]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 1.3627 Train Acc: 0.5153 Eval Loss: 1.2784 Eval Acc: 0.5471 (LR: 0.000100)
[2025-05-20 07:20:54,170]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 1.3298 Train Acc: 0.5275 Eval Loss: 1.2132 Eval Acc: 0.5706 (LR: 0.000100)
[2025-05-20 07:21:21,771]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 1.3203 Train Acc: 0.5299 Eval Loss: 1.2345 Eval Acc: 0.5743 (LR: 0.000100)
[2025-05-20 07:21:49,284]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 1.3250 Train Acc: 0.5288 Eval Loss: 1.2711 Eval Acc: 0.5563 (LR: 0.000100)
[2025-05-20 07:22:16,437]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 1.3260 Train Acc: 0.5276 Eval Loss: 1.2768 Eval Acc: 0.5524 (LR: 0.000100)
[2025-05-20 07:22:44,389]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 1.3312 Train Acc: 0.5251 Eval Loss: 1.2537 Eval Acc: 0.5623 (LR: 0.000100)
[2025-05-20 07:23:12,391]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 1.3282 Train Acc: 0.5288 Eval Loss: 1.2416 Eval Acc: 0.5708 (LR: 0.000100)
[2025-05-20 07:23:40,768]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 1.3370 Train Acc: 0.5240 Eval Loss: 1.2422 Eval Acc: 0.5687 (LR: 0.000100)
[2025-05-20 07:24:08,668]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 1.3337 Train Acc: 0.5311 Eval Loss: 1.2509 Eval Acc: 0.5609 (LR: 0.000100)
[2025-05-20 07:24:36,087]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 1.3317 Train Acc: 0.5282 Eval Loss: 1.1992 Eval Acc: 0.5798 (LR: 0.000100)
[2025-05-20 07:25:03,065]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 1.3253 Train Acc: 0.5291 Eval Loss: 1.2572 Eval Acc: 0.5601 (LR: 0.000100)
[2025-05-20 07:25:31,280]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 1.3337 Train Acc: 0.5288 Eval Loss: 1.2655 Eval Acc: 0.5567 (LR: 0.000100)
[2025-05-20 07:25:58,683]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 1.3284 Train Acc: 0.5297 Eval Loss: 1.2389 Eval Acc: 0.5625 (LR: 0.000100)
[2025-05-20 07:26:26,138]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 1.3305 Train Acc: 0.5290 Eval Loss: 1.2702 Eval Acc: 0.5501 (LR: 0.000100)
[2025-05-20 07:26:53,761]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 1.3280 Train Acc: 0.5291 Eval Loss: 1.2446 Eval Acc: 0.5626 (LR: 0.000100)
[2025-05-20 07:27:21,218]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 1.3381 Train Acc: 0.5245 Eval Loss: 1.2635 Eval Acc: 0.5501 (LR: 0.000010)
[2025-05-20 07:27:48,354]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 1.3198 Train Acc: 0.5314 Eval Loss: 1.2440 Eval Acc: 0.5624 (LR: 0.000010)
[2025-05-20 07:28:16,863]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 1.3189 Train Acc: 0.5308 Eval Loss: 1.2375 Eval Acc: 0.5640 (LR: 0.000010)
[2025-05-20 07:28:44,490]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 1.3277 Train Acc: 0.5289 Eval Loss: 1.2256 Eval Acc: 0.5609 (LR: 0.000010)
[2025-05-20 07:29:11,759]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 1.3222 Train Acc: 0.5330 Eval Loss: 1.2387 Eval Acc: 0.5645 (LR: 0.000010)
[2025-05-20 07:29:38,603]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 1.3203 Train Acc: 0.5332 Eval Loss: 1.2246 Eval Acc: 0.5603 (LR: 0.000010)
[2025-05-20 07:30:05,288]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 1.3221 Train Acc: 0.5309 Eval Loss: 1.2334 Eval Acc: 0.5656 (LR: 0.000010)
[2025-05-20 07:30:31,822]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 1.3234 Train Acc: 0.5305 Eval Loss: 1.2546 Eval Acc: 0.5586 (LR: 0.000010)
[2025-05-20 07:30:58,525]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 1.3226 Train Acc: 0.5330 Eval Loss: 1.2426 Eval Acc: 0.5617 (LR: 0.000010)
[2025-05-20 07:31:25,071]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 1.3252 Train Acc: 0.5316 Eval Loss: 1.2501 Eval Acc: 0.5646 (LR: 0.000010)
[2025-05-20 07:31:51,712]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 1.3253 Train Acc: 0.5293 Eval Loss: 1.2423 Eval Acc: 0.5586 (LR: 0.000010)
[2025-05-20 07:32:18,209]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 1.3219 Train Acc: 0.5299 Eval Loss: 1.2278 Eval Acc: 0.5609 (LR: 0.000010)
[2025-05-20 07:32:44,738]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 1.3176 Train Acc: 0.5331 Eval Loss: 1.2223 Eval Acc: 0.5655 (LR: 0.000010)
[2025-05-20 07:33:11,099]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 1.3247 Train Acc: 0.5310 Eval Loss: 1.2365 Eval Acc: 0.5640 (LR: 0.000010)
[2025-05-20 07:33:38,210]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 1.3209 Train Acc: 0.5343 Eval Loss: 1.2347 Eval Acc: 0.5617 (LR: 0.000010)
[2025-05-20 07:34:05,744]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 1.3224 Train Acc: 0.5334 Eval Loss: 1.2266 Eval Acc: 0.5690 (LR: 0.000010)
[2025-05-20 07:34:05,744]: [LeNet5_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.5798
[2025-05-20 07:34:05,760]: 


Quantization of model down to 2 bits finished
[2025-05-20 07:34:05,760]: Model Architecture:
[2025-05-20 07:34:05,770]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7376], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7574928402900696, max_val=1.455202341079712)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8166], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2374], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33576104044914246, max_val=0.37639641761779785)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.4174], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2368], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3390965759754181, max_val=0.3713706135749817)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0542], device='cuda:0'), zero_point=tensor([-1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 07:34:05,771]: 
Model Weights:
[2025-05-20 07:34:05,771]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-20 07:34:05,771]: Sample Values (25 elements): [-0.10975149273872375, -1.5132825374603271, -0.43473753333091736, 1.5076911449432373, 0.24667565524578094, 0.2988737225532532, 0.02852397970855236, 0.16638053953647614, -0.3918912410736084, 0.27899813652038574, -0.284097820520401, 0.2303980588912964, 0.28543326258659363, 0.2824496924877167, -0.07434410601854324, -0.00014624465256929398, -0.23427779972553253, -0.043486665934324265, -0.2561836242675781, 0.09104090183973312, -0.18465618789196014, 0.1397283375263214, 0.13897813856601715, 0.18244245648384094, -0.11251212656497955]
[2025-05-20 07:34:05,776]: Mean: 0.00285556
[2025-05-20 07:34:05,776]: Min: -1.51328254
[2025-05-20 07:34:05,776]: Max: 1.50769114
[2025-05-20 07:34:05,777]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-20 07:34:05,778]: Sample Values (25 elements): [0.7375650405883789, 0.0, 0.0, 0.0, 0.7375650405883789, 0.0, -0.7375650405883789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7375650405883789, 0.0, 0.0, 0.0, 0.0, -0.7375650405883789, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 07:34:05,778]: Mean: 0.03196115
[2025-05-20 07:34:05,778]: Min: -0.73756504
[2025-05-20 07:34:05,778]: Max: 1.47513008
[2025-05-20 07:34:05,779]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-20 07:34:05,780]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2373858392238617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 07:34:05,780]: Mean: 0.00016320
[2025-05-20 07:34:05,780]: Min: -0.23738584
[2025-05-20 07:34:05,780]: Max: 0.47477168
[2025-05-20 07:34:05,781]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-20 07:34:05,782]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.23682239651679993, 0.0, 0.0, 0.23682239651679993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 07:34:05,782]: Mean: 0.00180906
[2025-05-20 07:34:05,782]: Min: -0.23682240
[2025-05-20 07:34:05,782]: Max: 0.47364479
[2025-05-20 07:34:05,782]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-20 07:34:05,782]: Sample Values (25 elements): [0.011275339871644974, 0.04454835504293442, -0.04201582446694374, -0.00617924565449357, -0.03310361132025719, -0.03729429468512535, -0.0890921875834465, 0.08139067888259888, -0.06452959775924683, 0.1131523922085762, 0.07353410124778748, -0.05051405727863312, -0.0389259047806263, 0.02439240552484989, 0.039281249046325684, -0.024196356534957886, -0.039214976131916046, 0.015344132669270039, -0.0263984352350235, -0.03856774419546127, 0.040645670145750046, 0.02259160950779915, -0.03375736251473427, -0.060041241347789764, 0.07836567610502243]
[2025-05-20 07:34:05,783]: Mean: 0.00240611
[2025-05-20 07:34:05,783]: Min: -0.11719830
[2025-05-20 07:34:05,783]: Max: 0.12158856
[2025-05-20 07:34:05,783]: 


QAT of LeNet5 with parametrized_hardtanh down to 3 bits...
[2025-05-20 07:34:05,797]: [LeNet5_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-20 07:34:05,802]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 07:34:31,827]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 1.4484 Train Acc: 0.4832 Eval Loss: 1.2921 Eval Acc: 0.5338 (LR: 0.010000)
[2025-05-20 07:34:58,487]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 1.3930 Train Acc: 0.5018 Eval Loss: 1.2608 Eval Acc: 0.5430 (LR: 0.010000)
[2025-05-20 07:35:25,323]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 1.3641 Train Acc: 0.5103 Eval Loss: 1.2074 Eval Acc: 0.5687 (LR: 0.010000)
[2025-05-20 07:35:53,157]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 1.3617 Train Acc: 0.5122 Eval Loss: 1.2503 Eval Acc: 0.5516 (LR: 0.010000)
[2025-05-20 07:36:20,297]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 1.3602 Train Acc: 0.5171 Eval Loss: 1.2207 Eval Acc: 0.5620 (LR: 0.010000)
[2025-05-20 07:36:46,608]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 1.3449 Train Acc: 0.5217 Eval Loss: 1.2289 Eval Acc: 0.5585 (LR: 0.010000)
[2025-05-20 07:37:13,127]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 1.3434 Train Acc: 0.5195 Eval Loss: 1.2289 Eval Acc: 0.5601 (LR: 0.010000)
[2025-05-20 07:37:39,339]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 1.3401 Train Acc: 0.5240 Eval Loss: 1.2095 Eval Acc: 0.5695 (LR: 0.010000)
[2025-05-20 07:38:05,500]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 1.3375 Train Acc: 0.5250 Eval Loss: 1.2117 Eval Acc: 0.5675 (LR: 0.010000)
[2025-05-20 07:38:32,802]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 1.3310 Train Acc: 0.5247 Eval Loss: 1.2148 Eval Acc: 0.5583 (LR: 0.010000)
[2025-05-20 07:38:59,686]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 1.3173 Train Acc: 0.5303 Eval Loss: 1.2219 Eval Acc: 0.5630 (LR: 0.010000)
[2025-05-20 07:39:26,242]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 1.3200 Train Acc: 0.5296 Eval Loss: 1.2182 Eval Acc: 0.5561 (LR: 0.010000)
[2025-05-20 07:39:52,838]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 1.3230 Train Acc: 0.5277 Eval Loss: 1.2504 Eval Acc: 0.5471 (LR: 0.010000)
[2025-05-20 07:40:19,211]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 1.3211 Train Acc: 0.5304 Eval Loss: 1.1781 Eval Acc: 0.5775 (LR: 0.010000)
[2025-05-20 07:40:45,834]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 1.3198 Train Acc: 0.5293 Eval Loss: 1.2746 Eval Acc: 0.5424 (LR: 0.001000)
[2025-05-20 07:41:12,231]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 1.2491 Train Acc: 0.5543 Eval Loss: 1.1437 Eval Acc: 0.5912 (LR: 0.001000)
[2025-05-20 07:41:38,743]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 1.2333 Train Acc: 0.5627 Eval Loss: 1.1395 Eval Acc: 0.5937 (LR: 0.001000)
[2025-05-20 07:42:06,107]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 1.2276 Train Acc: 0.5643 Eval Loss: 1.1392 Eval Acc: 0.5931 (LR: 0.001000)
[2025-05-20 07:42:34,340]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 1.2197 Train Acc: 0.5635 Eval Loss: 1.1312 Eval Acc: 0.5968 (LR: 0.001000)
[2025-05-20 07:43:01,458]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 1.2254 Train Acc: 0.5631 Eval Loss: 1.1294 Eval Acc: 0.5933 (LR: 0.001000)
[2025-05-20 07:43:27,468]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 1.2187 Train Acc: 0.5649 Eval Loss: 1.1293 Eval Acc: 0.5953 (LR: 0.001000)
[2025-05-20 07:43:53,794]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 1.2198 Train Acc: 0.5653 Eval Loss: 1.1251 Eval Acc: 0.5970 (LR: 0.001000)
[2025-05-20 07:44:19,908]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 1.2130 Train Acc: 0.5676 Eval Loss: 1.1361 Eval Acc: 0.5961 (LR: 0.001000)
[2025-05-20 07:44:45,982]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 1.2155 Train Acc: 0.5685 Eval Loss: 1.1202 Eval Acc: 0.6001 (LR: 0.001000)
[2025-05-20 07:45:12,117]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 1.2077 Train Acc: 0.5703 Eval Loss: 1.1299 Eval Acc: 0.5911 (LR: 0.001000)
[2025-05-20 07:45:38,233]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 1.2082 Train Acc: 0.5682 Eval Loss: 1.1228 Eval Acc: 0.5986 (LR: 0.001000)
[2025-05-20 07:46:04,496]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 1.2097 Train Acc: 0.5672 Eval Loss: 1.1190 Eval Acc: 0.5998 (LR: 0.001000)
[2025-05-20 07:46:30,578]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 1.2059 Train Acc: 0.5708 Eval Loss: 1.1108 Eval Acc: 0.6099 (LR: 0.001000)
[2025-05-20 07:46:56,913]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 1.2000 Train Acc: 0.5733 Eval Loss: 1.1520 Eval Acc: 0.5881 (LR: 0.001000)
[2025-05-20 07:47:23,198]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 1.2057 Train Acc: 0.5672 Eval Loss: 1.1350 Eval Acc: 0.6007 (LR: 0.000100)
[2025-05-20 07:47:49,183]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 1.1915 Train Acc: 0.5764 Eval Loss: 1.1051 Eval Acc: 0.6022 (LR: 0.000100)
[2025-05-20 07:48:15,270]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 1.1863 Train Acc: 0.5776 Eval Loss: 1.1189 Eval Acc: 0.5979 (LR: 0.000100)
[2025-05-20 07:48:41,577]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 1.1867 Train Acc: 0.5777 Eval Loss: 1.1086 Eval Acc: 0.6062 (LR: 0.000100)
[2025-05-20 07:49:07,908]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 1.1849 Train Acc: 0.5814 Eval Loss: 1.1027 Eval Acc: 0.6079 (LR: 0.000100)
[2025-05-20 07:49:33,992]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 1.1894 Train Acc: 0.5769 Eval Loss: 1.1082 Eval Acc: 0.6087 (LR: 0.000100)
[2025-05-20 07:49:59,874]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 1.1814 Train Acc: 0.5826 Eval Loss: 1.1025 Eval Acc: 0.6061 (LR: 0.000100)
[2025-05-20 07:50:25,559]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 1.1837 Train Acc: 0.5770 Eval Loss: 1.1029 Eval Acc: 0.6044 (LR: 0.000100)
[2025-05-20 07:50:51,533]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 1.1880 Train Acc: 0.5788 Eval Loss: 1.1077 Eval Acc: 0.6035 (LR: 0.000100)
[2025-05-20 07:51:17,607]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 1.1816 Train Acc: 0.5780 Eval Loss: 1.1064 Eval Acc: 0.6087 (LR: 0.000100)
[2025-05-20 07:51:43,587]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 1.1869 Train Acc: 0.5759 Eval Loss: 1.1108 Eval Acc: 0.6032 (LR: 0.000100)
[2025-05-20 07:52:09,582]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 1.1836 Train Acc: 0.5792 Eval Loss: 1.0970 Eval Acc: 0.6134 (LR: 0.000100)
[2025-05-20 07:52:35,689]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 1.1862 Train Acc: 0.5775 Eval Loss: 1.0986 Eval Acc: 0.6056 (LR: 0.000100)
[2025-05-20 07:53:01,794]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 1.1828 Train Acc: 0.5799 Eval Loss: 1.1143 Eval Acc: 0.6019 (LR: 0.000100)
[2025-05-20 07:53:27,888]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 1.1830 Train Acc: 0.5787 Eval Loss: 1.1132 Eval Acc: 0.6030 (LR: 0.000100)
[2025-05-20 07:53:54,205]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 1.1849 Train Acc: 0.5791 Eval Loss: 1.1008 Eval Acc: 0.6064 (LR: 0.000010)
[2025-05-20 07:54:20,481]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 1.1788 Train Acc: 0.5786 Eval Loss: 1.1033 Eval Acc: 0.6044 (LR: 0.000010)
[2025-05-20 07:54:46,230]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 1.1774 Train Acc: 0.5807 Eval Loss: 1.1072 Eval Acc: 0.6046 (LR: 0.000010)
[2025-05-20 07:55:12,265]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 1.1743 Train Acc: 0.5785 Eval Loss: 1.0977 Eval Acc: 0.6072 (LR: 0.000010)
[2025-05-20 07:55:37,951]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 1.1779 Train Acc: 0.5836 Eval Loss: 1.0986 Eval Acc: 0.6070 (LR: 0.000010)
[2025-05-20 07:56:03,814]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 1.1770 Train Acc: 0.5822 Eval Loss: 1.1018 Eval Acc: 0.6079 (LR: 0.000010)
[2025-05-20 07:56:29,582]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 1.1800 Train Acc: 0.5812 Eval Loss: 1.1076 Eval Acc: 0.6011 (LR: 0.000010)
[2025-05-20 07:56:55,758]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 1.1790 Train Acc: 0.5791 Eval Loss: 1.0916 Eval Acc: 0.6101 (LR: 0.000010)
[2025-05-20 07:57:21,623]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 1.1803 Train Acc: 0.5814 Eval Loss: 1.1015 Eval Acc: 0.6038 (LR: 0.000010)
[2025-05-20 07:57:47,548]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 1.1759 Train Acc: 0.5812 Eval Loss: 1.0896 Eval Acc: 0.6097 (LR: 0.000010)
[2025-05-20 07:58:13,782]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 1.1748 Train Acc: 0.5831 Eval Loss: 1.1005 Eval Acc: 0.6064 (LR: 0.000010)
[2025-05-20 07:58:40,139]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 1.1770 Train Acc: 0.5811 Eval Loss: 1.1047 Eval Acc: 0.6020 (LR: 0.000010)
[2025-05-20 07:59:06,184]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 1.1806 Train Acc: 0.5799 Eval Loss: 1.0939 Eval Acc: 0.6080 (LR: 0.000010)
[2025-05-20 07:59:31,878]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 1.1785 Train Acc: 0.5805 Eval Loss: 1.1075 Eval Acc: 0.6072 (LR: 0.000010)
[2025-05-20 07:59:57,792]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 1.1752 Train Acc: 0.5818 Eval Loss: 1.1098 Eval Acc: 0.6051 (LR: 0.000010)
[2025-05-20 08:00:23,374]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 1.1791 Train Acc: 0.5821 Eval Loss: 1.1022 Eval Acc: 0.6052 (LR: 0.000010)
[2025-05-20 08:00:23,374]: [LeNet5_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.6134
[2025-05-20 08:00:23,389]: 


Quantization of model down to 3 bits finished
[2025-05-20 08:00:23,389]: Model Architecture:
[2025-05-20 08:00:23,398]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2571], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6898381114006042, max_val=1.110115885734558)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1216], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0890], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2815427780151367, max_val=0.3411194682121277)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2529], device='cuda:0'), zero_point=tensor([-1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1123], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3383742570877075, max_val=0.4480079412460327)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2949], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 08:00:23,398]: 
Model Weights:
[2025-05-20 08:00:23,398]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-20 08:00:23,399]: Sample Values (25 elements): [0.35580146312713623, -0.08524900674819946, -0.20669080317020416, 0.220163032412529, 0.03590827435255051, 0.35332873463630676, -0.5277733206748962, 0.13397014141082764, 0.014325677417218685, -0.1338411271572113, 0.2658200263977051, -0.4008631408214569, 0.12943203747272491, -0.271085262298584, -0.2293298989534378, -0.5721782445907593, -0.2836325466632843, -1.3128209114074707, 0.7964075207710266, 0.2514510154724121, 0.8247323036193848, -0.6517800688743591, 0.037732966244220734, 0.2808925211429596, 0.37776410579681396]
[2025-05-20 08:00:23,399]: Mean: 0.00346258
[2025-05-20 08:00:23,399]: Min: -1.67032182
[2025-05-20 08:00:23,399]: Max: 1.47979581
[2025-05-20 08:00:23,400]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-20 08:00:23,400]: Sample Values (25 elements): [0.5142725706100464, 0.2571362853050232, -0.2571362853050232, -0.2571362853050232, -0.2571362853050232, -0.2571362853050232, 0.0, -0.2571362853050232, -0.2571362853050232, -0.2571362853050232, 0.2571362853050232, 0.2571362853050232, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2571362853050232, -0.5142725706100464, 0.2571362853050232, -0.2571362853050232, -0.2571362853050232, -0.2571362853050232, 0.2571362853050232, -0.2571362853050232]
[2025-05-20 08:00:23,401]: Mean: -0.00557129
[2025-05-20 08:00:23,401]: Min: -0.77140886
[2025-05-20 08:00:23,401]: Max: 1.02854514
[2025-05-20 08:00:23,402]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-20 08:00:23,402]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.08895174413919449, 0.0, 0.0, 0.0, 0.0, 0.08895174413919449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08895174413919449, 0.0, 0.0, -0.08895174413919449, 0.0, 0.0, -0.08895174413919449, 0.0, -0.08895174413919449, 0.08895174413919449]
[2025-05-20 08:00:23,403]: Mean: 0.00021867
[2025-05-20 08:00:23,403]: Min: -0.26685524
[2025-05-20 08:00:23,403]: Max: 0.35580698
[2025-05-20 08:00:23,404]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-20 08:00:23,404]: Sample Values (25 elements): [0.0, 0.11234031617641449, -0.11234031617641449, 0.0, 0.0, 0.0, 0.0, 0.11234031617641449, -0.11234031617641449, -0.11234031617641449, 0.11234031617641449, 0.0, -0.11234031617641449, 0.0, 0.0, 0.0, 0.0, 0.11234031617641449, 0.0, 0.11234031617641449, -0.11234031617641449, 0.11234031617641449, -0.11234031617641449, -0.11234031617641449, 0.0]
[2025-05-20 08:00:23,404]: Mean: -0.00028977
[2025-05-20 08:00:23,404]: Min: -0.33702093
[2025-05-20 08:00:23,405]: Max: 0.44936126
[2025-05-20 08:00:23,405]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-20 08:00:23,405]: Sample Values (25 elements): [0.32802459597587585, -0.15169550478458405, -0.023459138348698616, 0.1628870815038681, -0.05043313279747963, -0.1899571418762207, -0.3591305613517761, -0.02741708979010582, 0.06117760017514229, -0.06833546608686447, 0.06781750917434692, -0.11519316583871841, -0.27640870213508606, -0.22601978480815887, -0.03502294421195984, 0.015886932611465454, -0.16763469576835632, -0.09009228646755219, 0.03746325522661209, 0.08206656575202942, -0.10779048502445221, 0.26810258626937866, 0.015649087727069855, -0.05107024312019348, -0.07053763419389725]
[2025-05-20 08:00:23,405]: Mean: 0.00240609
[2025-05-20 08:00:23,405]: Min: -0.51683515
[2025-05-20 08:00:23,405]: Max: 0.45349148
[2025-05-20 08:00:23,405]: 


QAT of LeNet5 with parametrized_hardtanh down to 4 bits...
[2025-05-20 08:00:23,420]: [LeNet5_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-20 08:00:23,425]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 08:00:49,496]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 1.3410 Train Acc: 0.5233 Eval Loss: 1.1669 Eval Acc: 0.5842 (LR: 0.010000)
[2025-05-20 08:01:15,327]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 1.3188 Train Acc: 0.5299 Eval Loss: 1.2463 Eval Acc: 0.5573 (LR: 0.010000)
[2025-05-20 08:01:41,258]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 1.3049 Train Acc: 0.5353 Eval Loss: 1.1916 Eval Acc: 0.5721 (LR: 0.010000)
[2025-05-20 08:02:06,893]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 1.2919 Train Acc: 0.5387 Eval Loss: 1.1670 Eval Acc: 0.5822 (LR: 0.010000)
[2025-05-20 08:02:32,548]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 1.2868 Train Acc: 0.5393 Eval Loss: 1.2167 Eval Acc: 0.5711 (LR: 0.010000)
[2025-05-20 08:02:58,031]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 1.2749 Train Acc: 0.5466 Eval Loss: 1.1384 Eval Acc: 0.5890 (LR: 0.010000)
[2025-05-20 08:03:23,583]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 1.2783 Train Acc: 0.5452 Eval Loss: 1.1495 Eval Acc: 0.5895 (LR: 0.010000)
[2025-05-20 08:03:51,571]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 1.2722 Train Acc: 0.5468 Eval Loss: 1.1888 Eval Acc: 0.5703 (LR: 0.010000)
[2025-05-20 08:04:18,710]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 1.2665 Train Acc: 0.5493 Eval Loss: 1.1755 Eval Acc: 0.5844 (LR: 0.010000)
[2025-05-20 08:04:45,992]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 1.2732 Train Acc: 0.5435 Eval Loss: 1.1696 Eval Acc: 0.5802 (LR: 0.010000)
[2025-05-20 08:05:13,957]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 1.2788 Train Acc: 0.5433 Eval Loss: 1.1615 Eval Acc: 0.5812 (LR: 0.010000)
[2025-05-20 08:05:41,814]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 1.2636 Train Acc: 0.5485 Eval Loss: 1.1944 Eval Acc: 0.5746 (LR: 0.010000)
[2025-05-20 08:06:09,383]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 1.2632 Train Acc: 0.5507 Eval Loss: 1.1595 Eval Acc: 0.5862 (LR: 0.010000)
[2025-05-20 08:06:37,232]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 1.2788 Train Acc: 0.5434 Eval Loss: 1.2127 Eval Acc: 0.5646 (LR: 0.010000)
[2025-05-20 08:07:04,481]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 1.2656 Train Acc: 0.5475 Eval Loss: 1.1407 Eval Acc: 0.5939 (LR: 0.001000)
[2025-05-20 08:07:35,811]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 1.2015 Train Acc: 0.5702 Eval Loss: 1.0935 Eval Acc: 0.6091 (LR: 0.001000)
[2025-05-20 08:08:04,226]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 1.1820 Train Acc: 0.5806 Eval Loss: 1.1062 Eval Acc: 0.6047 (LR: 0.001000)
[2025-05-20 08:08:31,496]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 1.1781 Train Acc: 0.5787 Eval Loss: 1.0865 Eval Acc: 0.6123 (LR: 0.001000)
[2025-05-20 08:08:58,115]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 1.1710 Train Acc: 0.5809 Eval Loss: 1.0900 Eval Acc: 0.6068 (LR: 0.001000)
[2025-05-20 08:09:26,649]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 1.1667 Train Acc: 0.5834 Eval Loss: 1.0893 Eval Acc: 0.6113 (LR: 0.001000)
[2025-05-20 08:09:54,351]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 1.1691 Train Acc: 0.5829 Eval Loss: 1.0744 Eval Acc: 0.6155 (LR: 0.001000)
[2025-05-20 08:10:21,659]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 1.1711 Train Acc: 0.5809 Eval Loss: 1.0804 Eval Acc: 0.6145 (LR: 0.001000)
[2025-05-20 08:10:48,312]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 1.1684 Train Acc: 0.5860 Eval Loss: 1.0766 Eval Acc: 0.6136 (LR: 0.001000)
[2025-05-20 08:11:16,109]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 1.1662 Train Acc: 0.5843 Eval Loss: 1.0824 Eval Acc: 0.6130 (LR: 0.001000)
[2025-05-20 08:11:43,498]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 1.1652 Train Acc: 0.5851 Eval Loss: 1.0755 Eval Acc: 0.6160 (LR: 0.001000)
[2025-05-20 08:12:10,894]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 1.1658 Train Acc: 0.5832 Eval Loss: 1.0729 Eval Acc: 0.6189 (LR: 0.001000)
[2025-05-20 08:12:37,697]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 1.1599 Train Acc: 0.5875 Eval Loss: 1.0730 Eval Acc: 0.6179 (LR: 0.001000)
[2025-05-20 08:13:05,198]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 1.1570 Train Acc: 0.5880 Eval Loss: 1.0701 Eval Acc: 0.6188 (LR: 0.001000)
[2025-05-20 08:13:32,930]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 1.1595 Train Acc: 0.5874 Eval Loss: 1.0818 Eval Acc: 0.6136 (LR: 0.001000)
[2025-05-20 08:14:00,925]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 1.1534 Train Acc: 0.5869 Eval Loss: 1.0719 Eval Acc: 0.6172 (LR: 0.000100)
[2025-05-20 08:14:27,837]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 1.1411 Train Acc: 0.5953 Eval Loss: 1.0691 Eval Acc: 0.6181 (LR: 0.000100)
[2025-05-20 08:14:56,052]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 1.1471 Train Acc: 0.5910 Eval Loss: 1.0628 Eval Acc: 0.6226 (LR: 0.000100)
[2025-05-20 08:15:22,608]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 1.1412 Train Acc: 0.5919 Eval Loss: 1.0611 Eval Acc: 0.6223 (LR: 0.000100)
[2025-05-20 08:15:48,869]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 1.1458 Train Acc: 0.5909 Eval Loss: 1.0642 Eval Acc: 0.6206 (LR: 0.000100)
[2025-05-20 08:16:14,885]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 1.1380 Train Acc: 0.5945 Eval Loss: 1.0616 Eval Acc: 0.6203 (LR: 0.000100)
[2025-05-20 08:16:41,038]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 1.1406 Train Acc: 0.5916 Eval Loss: 1.0583 Eval Acc: 0.6199 (LR: 0.000100)
[2025-05-20 08:17:07,329]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 1.1419 Train Acc: 0.5936 Eval Loss: 1.0620 Eval Acc: 0.6180 (LR: 0.000100)
[2025-05-20 08:17:33,675]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 1.1421 Train Acc: 0.5929 Eval Loss: 1.0618 Eval Acc: 0.6202 (LR: 0.000100)
[2025-05-20 08:17:59,775]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 1.1383 Train Acc: 0.5950 Eval Loss: 1.0595 Eval Acc: 0.6192 (LR: 0.000100)
[2025-05-20 08:18:25,965]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 1.1401 Train Acc: 0.5937 Eval Loss: 1.0624 Eval Acc: 0.6227 (LR: 0.000100)
[2025-05-20 08:18:53,106]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 1.1419 Train Acc: 0.5921 Eval Loss: 1.0635 Eval Acc: 0.6177 (LR: 0.000100)
[2025-05-20 08:19:19,654]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 1.1390 Train Acc: 0.5946 Eval Loss: 1.0533 Eval Acc: 0.6218 (LR: 0.000100)
[2025-05-20 08:19:45,502]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 1.1447 Train Acc: 0.5919 Eval Loss: 1.0735 Eval Acc: 0.6205 (LR: 0.000100)
[2025-05-20 08:20:11,331]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 1.1389 Train Acc: 0.5950 Eval Loss: 1.0577 Eval Acc: 0.6206 (LR: 0.000100)
[2025-05-20 08:20:37,363]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 1.1414 Train Acc: 0.5900 Eval Loss: 1.0571 Eval Acc: 0.6241 (LR: 0.000010)
[2025-05-20 08:21:03,499]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 1.1380 Train Acc: 0.5949 Eval Loss: 1.0565 Eval Acc: 0.6238 (LR: 0.000010)
[2025-05-20 08:21:30,621]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 1.1405 Train Acc: 0.5954 Eval Loss: 1.0651 Eval Acc: 0.6236 (LR: 0.000010)
[2025-05-20 08:21:57,140]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 1.1384 Train Acc: 0.5961 Eval Loss: 1.0741 Eval Acc: 0.6198 (LR: 0.000010)
[2025-05-20 08:22:23,341]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 1.1375 Train Acc: 0.5931 Eval Loss: 1.0605 Eval Acc: 0.6231 (LR: 0.000010)
[2025-05-20 08:22:49,447]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 1.1396 Train Acc: 0.5951 Eval Loss: 1.0604 Eval Acc: 0.6186 (LR: 0.000010)
[2025-05-20 08:23:15,338]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 1.1379 Train Acc: 0.5930 Eval Loss: 1.0633 Eval Acc: 0.6160 (LR: 0.000010)
[2025-05-20 08:23:41,519]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 1.1367 Train Acc: 0.5951 Eval Loss: 1.0585 Eval Acc: 0.6204 (LR: 0.000010)
[2025-05-20 08:24:08,222]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 1.1365 Train Acc: 0.5941 Eval Loss: 1.0546 Eval Acc: 0.6208 (LR: 0.000010)
[2025-05-20 08:24:36,805]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 1.1379 Train Acc: 0.5957 Eval Loss: 1.0688 Eval Acc: 0.6183 (LR: 0.000010)
[2025-05-20 08:25:17,750]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 1.1311 Train Acc: 0.5966 Eval Loss: 1.0539 Eval Acc: 0.6210 (LR: 0.000010)
[2025-05-20 08:25:50,448]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 1.1353 Train Acc: 0.5952 Eval Loss: 1.0553 Eval Acc: 0.6224 (LR: 0.000010)
[2025-05-20 08:26:18,444]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 1.1390 Train Acc: 0.5947 Eval Loss: 1.0606 Eval Acc: 0.6210 (LR: 0.000010)
[2025-05-20 08:26:46,935]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 1.1403 Train Acc: 0.5946 Eval Loss: 1.0548 Eval Acc: 0.6213 (LR: 0.000010)
[2025-05-20 08:27:14,197]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 1.1404 Train Acc: 0.5958 Eval Loss: 1.0638 Eval Acc: 0.6188 (LR: 0.000010)
[2025-05-20 08:27:40,788]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 1.1418 Train Acc: 0.5943 Eval Loss: 1.0551 Eval Acc: 0.6237 (LR: 0.000010)
[2025-05-20 08:27:40,788]: [LeNet5_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.6241
[2025-05-20 08:27:40,805]: 


Quantization of model down to 4 bits finished
[2025-05-20 08:27:40,806]: Model Architecture:
[2025-05-20 08:27:40,815]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0938], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5889698266983032, max_val=0.8175003528594971)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0280], device='cuda:0'), zero_point=tensor([-1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0409], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27272793650627136, max_val=0.3414594829082489)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0669], device='cuda:0'), zero_point=tensor([-1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0478], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31134340167045593, max_val=0.4063735008239746)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1011], device='cuda:0'), zero_point=tensor([-1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 08:27:40,835]: 
Model Weights:
[2025-05-20 08:27:40,835]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-20 08:27:40,836]: Sample Values (25 elements): [-0.076711006462574, -0.1189478412270546, 0.48056232929229736, -0.04783125966787338, 0.37202396988868713, -0.17613109946250916, 0.03533606976270676, 0.09166412055492401, -0.9831998348236084, 0.1813254952430725, -0.2440425008535385, 0.056449856609106064, 0.4185645878314972, 0.8129957318305969, -0.3983668088912964, -0.14371483027935028, 0.11512379348278046, -0.2731126844882965, -0.34357866644859314, -0.1283772736787796, -0.11519918590784073, -0.1609465777873993, 0.10850869864225388, 0.042904745787382126, -0.07508952170610428]
[2025-05-20 08:27:40,836]: Mean: 0.00386212
[2025-05-20 08:27:40,836]: Min: -1.62632596
[2025-05-20 08:27:40,836]: Max: 1.46192896
[2025-05-20 08:27:40,837]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-20 08:27:40,838]: Sample Values (25 elements): [-0.09376468509435654, -0.18752937018871307, -0.09376468509435654, 0.0, 0.0, 0.0, -0.18752937018871307, -0.18752937018871307, 0.0, -0.18752937018871307, 0.18752937018871307, 0.09376468509435654, -0.09376468509435654, -0.09376468509435654, -0.18752937018871307, 0.0, 0.0, 0.0, 0.0, 0.37505874037742615, -0.18752937018871307, 0.0, 0.0, -0.09376468509435654, 0.281294047832489]
[2025-05-20 08:27:40,838]: Mean: -0.00355524
[2025-05-20 08:27:40,838]: Min: -0.56258810
[2025-05-20 08:27:40,838]: Max: 0.84388214
[2025-05-20 08:27:40,839]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-20 08:27:40,840]: Sample Values (25 elements): [0.12283748388290405, 0.0, -0.04094582796096802, 0.0, 0.0, 0.0, -0.04094582796096802, 0.04094582796096802, -0.04094582796096802, 0.08189165592193604, -0.04094582796096802, -0.04094582796096802, 0.0, -0.04094582796096802, -0.04094582796096802, -0.08189165592193604, 0.04094582796096802, 0.0, -0.04094582796096802, 0.0, 0.08189165592193604, 0.04094582796096802, 0.0, -0.08189165592193604, 0.0]
[2025-05-20 08:27:40,840]: Mean: 0.00021582
[2025-05-20 08:27:40,840]: Min: -0.28662080
[2025-05-20 08:27:40,840]: Max: 0.32756662
[2025-05-20 08:27:40,841]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-20 08:27:40,842]: Sample Values (25 elements): [-0.09569558501243591, -0.09569558501243591, 0.0, -0.09569558501243591, 0.09569558501243591, 0.0, 0.0, -0.09569558501243591, 0.04784779250621796, -0.04784779250621796, -0.04784779250621796, 0.14354337751865387, -0.14354337751865387, -0.14354337751865387, 0.0, 0.0, 0.04784779250621796, 0.0, 0.04784779250621796, 0.0, 0.09569558501243591, 0.0, 0.14354337751865387, 0.04784779250621796, 0.04784779250621796]
[2025-05-20 08:27:40,842]: Mean: 0.00060759
[2025-05-20 08:27:40,842]: Min: -0.33493453
[2025-05-20 08:27:40,842]: Max: 0.38278234
[2025-05-20 08:27:40,842]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-20 08:27:40,843]: Sample Values (25 elements): [-0.05433163419365883, 0.032190777361392975, 0.17023910582065582, -0.04492827132344246, 0.2189161330461502, -0.3121594488620758, 0.24967695772647858, 0.1862446665763855, -0.15386343002319336, -0.2992435395717621, -0.3101019263267517, 0.24891291558742523, 0.1370180994272232, -0.006665895693004131, -0.28487709164619446, 0.3294990360736847, 0.05489879846572876, 0.08305229246616364, -0.24439194798469543, -0.0584474541246891, 0.06973320245742798, 0.029491277411580086, 0.01778259314596653, -0.10965971648693085, -0.27640458941459656]
[2025-05-20 08:27:40,843]: Mean: 0.00240611
[2025-05-20 08:27:40,843]: Min: -0.64775044
[2025-05-20 08:27:40,843]: Max: 0.60693276
