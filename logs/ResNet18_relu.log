[2025-05-10 06:02:10,735]: 
Training ResNet18 with relu
[2025-05-10 06:03:31,203]: [ResNet18_relu] Epoch: 001 Train Loss: 1.6330 Train Acc: 0.3959 Eval Loss: 1.3469 Eval Acc: 0.5084 (LR: 0.001000)
[2025-05-10 06:03:31,272]: [ResNet18_relu] Best Eval Accuracy: 0.5084
[2025-05-10 06:03:31,323]: 
Training of full-precision model finished!
[2025-05-10 06:03:31,323]: Model Architecture:
[2025-05-10 06:03:31,324]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU(inplace=True)
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:03:31,324]: 
Model Weights:
[2025-05-10 06:03:31,324]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-10 06:03:31,334]: Sample Values (25 elements): [0.1707119643688202, 0.09532660245895386, 0.14682964980602264, 0.011680243536829948, 0.10983077436685562, 0.17410463094711304, 0.016625890508294106, 0.12680093944072723, -0.013935036957263947, -0.15872742235660553, 0.18554316461086273, -0.13698706030845642, -0.12310115993022919, -0.17974328994750977, -0.1313801407814026, -0.08864136040210724, -0.03797673434019089, 0.01028952281922102, 0.15397579967975616, -0.1782023161649704, 0.1217101439833641, 0.046868979930877686, 0.08586513996124268, -0.10816322267055511, -0.08702438324689865]
[2025-05-10 06:03:31,338]: Mean: -0.00039062
[2025-05-10 06:03:31,343]: Min: -0.19809438
[2025-05-10 06:03:31,344]: Max: 0.19930728
[2025-05-10 06:03:31,344]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-10 06:03:31,344]: Sample Values (25 elements): [0.9992721080780029, 0.9990614056587219, 1.0029650926589966, 0.9985373020172119, 0.9981951713562012, 0.9957051277160645, 1.0004581212997437, 0.9975287914276123, 0.9976796507835388, 1.00035560131073, 0.9983553290367126, 0.9988635778427124, 1.0018144845962524, 0.9991541504859924, 0.9989514350891113, 0.9998626112937927, 0.9984991550445557, 1.0029510259628296, 0.9992618560791016, 1.0012415647506714, 0.9998747110366821, 0.9995085000991821, 0.997134268283844, 0.9981340169906616, 1.001197099685669]
[2025-05-10 06:03:31,344]: Mean: 0.99961793
[2025-05-10 06:03:31,344]: Min: 0.99570513
[2025-05-10 06:03:31,345]: Max: 1.00364554
[2025-05-10 06:03:31,345]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:03:31,345]: Sample Values (25 elements): [-0.031129738315939903, -0.03688962012529373, 0.034134261310100555, -8.483271085424349e-05, 0.003721330314874649, 0.005824586842209101, 0.025074321776628494, 0.00017252590623684227, -0.00936376303434372, -0.0225038081407547, -0.0057180034928023815, -0.0062441593036055565, 0.030913908034563065, -0.017250539734959602, -0.02928391844034195, 0.01891760341823101, -0.03529363498091698, -0.024800116196274757, -0.01149544958025217, -0.012032417580485344, -0.013573621399700642, 0.03438625484704971, 0.03887895494699478, 0.013140669092535973, 0.007874692790210247]
[2025-05-10 06:03:31,345]: Mean: -0.00020518
[2025-05-10 06:03:31,346]: Min: -0.04852938
[2025-05-10 06:03:31,346]: Max: 0.04878559
[2025-05-10 06:03:31,346]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:03:31,346]: Sample Values (25 elements): [0.99835604429245, 0.9973924160003662, 1.0016095638275146, 0.9999033808708191, 0.9995735287666321, 0.9995388984680176, 1.0000373125076294, 0.9983437657356262, 1.0004651546478271, 0.9990336894989014, 0.9987834095954895, 1.0017021894454956, 0.9989947080612183, 1.0049861669540405, 1.000763177871704, 1.0008904933929443, 1.0001804828643799, 0.9988929033279419, 0.9988200068473816, 0.9985148906707764, 0.9972295761108398, 0.9979499578475952, 1.0002195835113525, 0.9981303811073303, 0.9989986419677734]
[2025-05-10 06:03:31,346]: Mean: 0.99961782
[2025-05-10 06:03:31,346]: Min: 0.99722958
[2025-05-10 06:03:31,346]: Max: 1.00498617
[2025-05-10 06:03:31,346]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:03:31,347]: Sample Values (25 elements): [-0.03192330151796341, 0.01168691087514162, -0.024348817765712738, -0.03009374998509884, -0.03902190923690796, -0.010210794396698475, 0.00990688893944025, 0.02879134938120842, -0.03275042027235031, 0.042779821902513504, -0.020396685227751732, -0.02937319688498974, 0.02042379230260849, -0.002635270357131958, 0.01638401485979557, 0.004367968998849392, -0.03162279725074768, 0.009861155413091183, -0.017911605536937714, -0.03370901942253113, 0.028253793716430664, -0.0047210222110152245, 0.0034320184495300055, 0.03615749627351761, -0.016492297872900963]
[2025-05-10 06:03:31,347]: Mean: -0.00031756
[2025-05-10 06:03:31,347]: Min: -0.04769985
[2025-05-10 06:03:31,347]: Max: 0.04826184
[2025-05-10 06:03:31,347]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:03:31,348]: Sample Values (25 elements): [0.996356725692749, 0.997615396976471, 0.998250424861908, 1.000918984413147, 0.9987779855728149, 0.9996107220649719, 0.9984003901481628, 1.003362774848938, 1.0013892650604248, 0.998678982257843, 1.0001895427703857, 0.9985989332199097, 1.000374436378479, 1.0002877712249756, 0.9990999102592468, 0.9981445670127869, 1.0018997192382812, 1.004314661026001, 0.9996552467346191, 0.9992407560348511, 1.0009363889694214, 1.0005419254302979, 0.9984880685806274, 0.9992898106575012, 0.9984581470489502]
[2025-05-10 06:03:31,348]: Mean: 0.99945050
[2025-05-10 06:03:31,348]: Min: 0.99635673
[2025-05-10 06:03:31,348]: Max: 1.00431466
[2025-05-10 06:03:31,348]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:03:31,349]: Sample Values (25 elements): [-0.034028854221105576, 0.03561690077185631, 0.008867128752171993, -0.0303537305444479, 0.00011398432252462953, 0.0028969815466552973, 0.01337269227951765, -0.03284650668501854, 0.0242893286049366, 0.03701167553663254, -0.03500716760754585, 0.030458839610219002, -0.037276800721883774, 0.036952778697013855, 0.019972633570432663, -0.021093878895044327, -0.0057701654732227325, -0.03186282142996788, -0.011160289868712425, -0.010104593820869923, 0.021422985941171646, -0.015502162277698517, -0.03943335637450218, -0.042634956538677216, -0.014105197973549366]
[2025-05-10 06:03:31,349]: Mean: -0.00004134
[2025-05-10 06:03:31,349]: Min: -0.04784477
[2025-05-10 06:03:31,349]: Max: 0.04709016
[2025-05-10 06:03:31,349]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:03:31,349]: Sample Values (25 elements): [1.0007678270339966, 0.9997422695159912, 0.9987083673477173, 1.000806212425232, 0.9988890290260315, 0.9994888305664062, 0.9991604089736938, 1.0021398067474365, 1.0012363195419312, 1.000863790512085, 0.998779833316803, 0.9979372620582581, 0.9988223910331726, 0.9993652701377869, 0.9979091286659241, 0.9987896084785461, 1.0013140439987183, 1.0009797811508179, 0.9990717768669128, 0.9991635084152222, 0.9985208511352539, 0.9999136924743652, 0.9995504021644592, 0.9997069835662842, 0.9986775517463684]
[2025-05-10 06:03:31,350]: Mean: 0.99961805
[2025-05-10 06:03:31,350]: Min: 0.99790913
[2025-05-10 06:03:31,350]: Max: 1.00213981
[2025-05-10 06:03:31,350]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:03:31,350]: Sample Values (25 elements): [0.020153403282165527, 0.004786448087543249, -0.012369371019303799, 0.01438841037452221, -0.027409367263317108, -0.012712369672954082, 0.03617274388670921, 0.022861065343022346, -0.01698106713593006, 0.020017346367239952, 0.00043206970440223813, 0.027179457247257233, -0.004812347237020731, -0.0028817332349717617, 0.008724267594516277, -0.0007108432473614812, 0.0007068846607580781, -0.013289159163832664, 0.034549519419670105, -0.022706318646669388, 0.03119242936372757, 0.02274361439049244, -0.004042595159262419, -0.0289475005120039, -0.002737509785220027]
[2025-05-10 06:03:31,351]: Mean: -0.00009095
[2025-05-10 06:03:31,351]: Min: -0.04687442
[2025-05-10 06:03:31,351]: Max: 0.04718294
[2025-05-10 06:03:31,351]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:03:31,351]: Sample Values (25 elements): [1.0008658170700073, 1.000259518623352, 1.0002169609069824, 0.9996026754379272, 0.9993839263916016, 0.9992229342460632, 0.999726414680481, 0.999933123588562, 0.9995356798171997, 1.0001872777938843, 0.9999175667762756, 0.9987117648124695, 0.9994822144508362, 1.0004725456237793, 1.0003095865249634, 1.0005810260772705, 0.9986804127693176, 1.0024141073226929, 1.0003306865692139, 0.9994398951530457, 0.9998517036437988, 0.9998534917831421, 0.9997825622558594, 1.00113046169281, 0.999886155128479]
[2025-05-10 06:03:31,351]: Mean: 0.99978447
[2025-05-10 06:03:31,352]: Min: 0.99783671
[2025-05-10 06:03:31,352]: Max: 1.00241411
[2025-05-10 06:03:31,352]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-10 06:03:31,353]: Sample Values (25 elements): [-0.016126999631524086, 0.0002261274348711595, 0.033283255994319916, -0.030138304457068443, 0.022819070145487785, -0.002546584000810981, 0.02413376048207283, -0.012346893548965454, 0.008474613539874554, 0.0011521235574036837, -0.03047638200223446, -0.022057509049773216, 0.022024769335985184, -0.037569597363471985, 0.0013350113295018673, 0.01185669843107462, 0.008312322199344635, 0.03918444737792015, 0.0048245619982481, 0.026490768417716026, 0.026504455134272575, 0.00837850570678711, 0.025241240859031677, -0.003754138480871916, 0.036747414618730545]
[2025-05-10 06:03:31,353]: Mean: -0.00006153
[2025-05-10 06:03:31,353]: Min: -0.04505280
[2025-05-10 06:03:31,353]: Max: 0.04632866
[2025-05-10 06:03:31,353]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:03:31,353]: Sample Values (25 elements): [1.0006499290466309, 0.9999013543128967, 0.9996948838233948, 0.9993136525154114, 1.0000463724136353, 0.9998367428779602, 0.9985939860343933, 0.9987477660179138, 0.9999615550041199, 0.9997197389602661, 1.000275731086731, 0.9998670220375061, 0.9998420476913452, 0.9998918771743774, 0.999940812587738, 0.9990875720977783, 0.9998310208320618, 0.9984001517295837, 0.9997255206108093, 1.0003689527511597, 0.998987078666687, 0.9996985197067261, 0.9994620680809021, 0.9996190071105957, 0.9993920922279358]
[2025-05-10 06:03:31,353]: Mean: 0.99961811
[2025-05-10 06:03:31,354]: Min: 0.99821186
[2025-05-10 06:03:31,354]: Max: 1.00128341
[2025-05-10 06:03:31,354]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:03:31,355]: Sample Values (25 elements): [0.01717500388622284, -0.017228219658136368, 0.005369082093238831, 0.008819914422929287, 0.029776250943541527, -0.008340273052453995, 0.0075567737221717834, -0.026758234947919846, 0.028029585257172585, 0.007185520604252815, 0.009685320779681206, -0.023630432784557343, -0.011312981136143208, -0.008398439735174179, 0.0009793988429009914, -0.0025211265310645103, 0.016819993034005165, -0.02110910788178444, 0.0029955299105495214, -0.025924131274223328, -0.024258434772491455, -0.01347951591014862, -0.019861333072185516, 0.0043475087732076645, -0.0199953094124794]
[2025-05-10 06:03:31,355]: Mean: -0.00012852
[2025-05-10 06:03:31,356]: Min: -0.03319154
[2025-05-10 06:03:31,356]: Max: 0.03322957
[2025-05-10 06:03:31,356]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:03:31,356]: Sample Values (25 elements): [0.9998266696929932, 0.9986153841018677, 1.0006978511810303, 0.9993153214454651, 0.9990804195404053, 0.9995378255844116, 0.9989016056060791, 0.9996890425682068, 0.9997839331626892, 0.9994102120399475, 0.9990432262420654, 1.0003846883773804, 1.000097632408142, 1.0013751983642578, 1.0003682374954224, 0.9989835023880005, 0.9993956685066223, 0.9988903999328613, 0.9998238682746887, 1.0001574754714966, 0.9993597865104675, 0.9987282752990723, 0.999076247215271, 0.9987807273864746, 1.0005443096160889]
[2025-05-10 06:03:31,356]: Mean: 0.99962705
[2025-05-10 06:03:31,356]: Min: 0.99795938
[2025-05-10 06:03:31,356]: Max: 1.00192285
[2025-05-10 06:03:31,357]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-10 06:03:31,357]: Sample Values (25 elements): [-0.08971239626407623, -0.10675496608018875, 0.039588332176208496, -0.055313143879175186, -0.05845818668603897, 0.08279500901699066, -0.09970857948064804, -0.029827319085597992, -0.05058297514915466, -0.017861461266875267, -0.0018543030600994825, -0.0850902646780014, -0.053364530205726624, 0.037808988243341446, -0.04188505932688713, 0.04272092133760452, -0.0015700034564360976, 0.07442370802164078, 0.00794689729809761, 0.08726092427968979, -0.012089258059859276, -0.09001070261001587, -0.0032083869446069, 0.001355790183879435, 0.09614608436822891]
[2025-05-10 06:03:31,357]: Mean: -0.00073749
[2025-05-10 06:03:31,357]: Min: -0.12728094
[2025-05-10 06:03:31,357]: Max: 0.12722857
[2025-05-10 06:03:31,357]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-10 06:03:31,358]: Sample Values (25 elements): [0.9997804760932922, 0.9998714327812195, 0.9989513158798218, 0.9996235966682434, 0.9997066259384155, 0.999286949634552, 0.999636709690094, 0.9995536208152771, 1.0000749826431274, 1.0005176067352295, 0.9995543360710144, 0.9996241927146912, 0.9997332692146301, 1.0008478164672852, 0.9982927441596985, 0.9986090064048767, 1.000018835067749, 0.9994540214538574, 0.9985600113868713, 0.9996697902679443, 1.0001895427703857, 1.0007309913635254, 0.9998570680618286, 0.9989622235298157, 0.9997085332870483]
[2025-05-10 06:03:31,358]: Mean: 0.99964476
[2025-05-10 06:03:31,358]: Min: 0.99799597
[2025-05-10 06:03:31,358]: Max: 1.00125241
[2025-05-10 06:03:31,358]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:03:31,359]: Sample Values (25 elements): [-0.007688603829592466, -0.015456312336027622, 0.01457518432289362, -0.01995197869837284, 6.131411646492779e-05, 0.007048489525914192, -0.01298594195395708, -0.01625930517911911, 0.0034854018595069647, 0.0018493710085749626, -0.015281186439096928, 0.025484144687652588, -0.027236981317400932, -0.017697246745228767, 0.020767342299222946, 0.006037852726876736, -0.029198763892054558, -0.015884047374129295, 0.013726700097322464, -0.012307288125157356, 0.011615418829023838, -0.020955784246325493, -0.00646298797801137, -0.0013661207631230354, -0.02681749127805233]
[2025-05-10 06:03:31,360]: Mean: 0.00000075
[2025-05-10 06:03:31,360]: Min: -0.03215057
[2025-05-10 06:03:31,360]: Max: 0.03260062
[2025-05-10 06:03:31,360]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:03:31,360]: Sample Values (25 elements): [0.9998666644096375, 0.9983429908752441, 0.9995055794715881, 0.999168336391449, 0.9991744160652161, 0.9994668960571289, 0.99894779920578, 0.9986193180084229, 0.9992475509643555, 0.9995269179344177, 0.9993447065353394, 0.9991142153739929, 1.0001640319824219, 0.9993600845336914, 0.9997164011001587, 0.9987636208534241, 0.9987435340881348, 1.0006786584854126, 0.9987398386001587, 0.9995789527893066, 0.9995563626289368, 0.9991243481636047, 1.0000578165054321, 0.9999189376831055, 0.999973475933075]
[2025-05-10 06:03:31,360]: Mean: 0.99961782
[2025-05-10 06:03:31,361]: Min: 0.99831676
[2025-05-10 06:03:31,361]: Max: 1.00162482
[2025-05-10 06:03:31,361]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:03:31,362]: Sample Values (25 elements): [0.00743776885792613, 0.015317520126700401, 0.012363760732114315, 0.014145035296678543, -0.0074072484858334064, 0.006131124682724476, 0.02220442332327366, 0.015680057927966118, 0.002874949248507619, -0.01119109708815813, 0.01660703495144844, -0.0021711220033466816, 0.018843477591872215, 0.0006529759848490357, 0.009713655337691307, 0.020986272022128105, 0.013823019340634346, -0.0051551335491240025, -0.005143872462213039, -0.014857924543321133, 0.022231869399547577, -0.02270057238638401, 0.003501526778563857, -0.00045735316234640777, 0.014948360621929169]
[2025-05-10 06:03:31,362]: Mean: -0.00004477
[2025-05-10 06:03:31,362]: Min: -0.03169364
[2025-05-10 06:03:31,363]: Max: 0.03233026
[2025-05-10 06:03:31,363]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:03:31,363]: Sample Values (25 elements): [1.0000858306884766, 0.9996510148048401, 0.9989535212516785, 1.00020170211792, 0.9991732239723206, 1.0003453493118286, 0.9995969533920288, 1.000045895576477, 0.9998314380645752, 1.0001890659332275, 0.9992214441299438, 0.9989376068115234, 0.9997389912605286, 0.9995055198669434, 0.9993312358856201, 0.9988501667976379, 0.9996206760406494, 1.00009024143219, 0.9994935989379883, 1.0002388954162598, 0.9998263716697693, 0.9998885989189148, 0.9993100166320801, 0.9990338087081909, 1.0000306367874146]
[2025-05-10 06:03:31,363]: Mean: 0.99958217
[2025-05-10 06:03:31,363]: Min: 0.99803650
[2025-05-10 06:03:31,363]: Max: 1.00063682
[2025-05-10 06:03:31,363]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-10 06:03:31,366]: Sample Values (25 elements): [0.00863939244300127, -0.014074844308197498, 0.008535263128578663, 0.02409488894045353, 0.02186596393585205, 0.020091449841856956, -0.0035797092132270336, -0.003567011561244726, -0.019881486892700195, -0.008648442104458809, 0.0166438277810812, 0.02554730512201786, 0.011045590974390507, -0.0018185512162745, 0.002352073322981596, 0.023255834355950356, 0.005796192679554224, -0.022476688027381897, -0.005860671401023865, -0.02599354460835457, -0.0213581845164299, 0.022557457908988, 0.02340118959546089, 0.017284713685512543, 0.012247716076672077]
[2025-05-10 06:03:31,366]: Mean: -0.00006885
[2025-05-10 06:03:31,366]: Min: -0.03139633
[2025-05-10 06:03:31,366]: Max: 0.03166387
[2025-05-10 06:03:31,366]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:03:31,367]: Sample Values (25 elements): [0.9993800520896912, 0.9993784427642822, 0.9994190335273743, 0.998729407787323, 0.9997974634170532, 0.9993324875831604, 0.9992570281028748, 0.9993065595626831, 0.9998581409454346, 0.9999296069145203, 0.999767541885376, 0.9998229742050171, 0.9996791481971741, 0.9991291761398315, 0.9996870160102844, 0.9995177388191223, 1.0000025033950806, 1.0000858306884766, 1.0001472234725952, 0.9998565912246704, 0.9995344877243042, 0.9997294545173645, 0.9993651509284973, 0.9995182156562805, 0.9997753500938416]
[2025-05-10 06:03:31,367]: Mean: 0.99961805
[2025-05-10 06:03:31,367]: Min: 0.99872941
[2025-05-10 06:03:31,367]: Max: 1.00087190
[2025-05-10 06:03:31,367]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:03:31,372]: Sample Values (25 elements): [0.011153919622302055, 0.0008686452638357878, -0.014684579335153103, -0.014482440426945686, 0.015012954361736774, -0.014879253692924976, -0.013970812782645226, 0.004700236488133669, -0.00026023545069620013, -0.0013664347352460027, -0.019292009994387627, 0.011647201143205166, 0.017734140157699585, 0.020859381183981895, 0.009037557058036327, -0.017311634495854378, 0.018907997757196426, -0.0046849325299263, -0.004019956570118666, -0.013702783733606339, -0.01775234006345272, -0.008029771037399769, -0.0031421156600117683, 0.013170785270631313, -0.011900399811565876]
[2025-05-10 06:03:31,373]: Mean: -0.00002502
[2025-05-10 06:03:31,373]: Min: -0.02262856
[2025-05-10 06:03:31,373]: Max: 0.02355556
[2025-05-10 06:03:31,373]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:03:31,373]: Sample Values (25 elements): [1.0001298189163208, 0.9996427893638611, 0.9993880987167358, 0.999882161617279, 1.0004260540008545, 0.9998172521591187, 1.000714659690857, 0.9995155930519104, 0.9991586804389954, 0.9993577599525452, 0.9999628067016602, 0.9997686147689819, 1.0011694431304932, 0.9993578195571899, 0.9996153116226196, 0.9991577863693237, 0.9989435076713562, 1.0006980895996094, 0.9991217851638794, 0.9996936321258545, 0.9993942975997925, 0.9995720386505127, 0.9993721842765808, 0.999487042427063, 0.9994549751281738]
[2025-05-10 06:03:31,373]: Mean: 0.99965054
[2025-05-10 06:03:31,374]: Min: 0.99884677
[2025-05-10 06:03:31,374]: Max: 1.00116944
[2025-05-10 06:03:31,374]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-10 06:03:31,374]: Sample Values (25 elements): [-0.06971026957035065, 0.011768635362386703, 0.02275617979466915, 0.002284035086631775, -0.04742838069796562, -0.06705112010240555, -0.003995633218437433, -0.06624717265367508, -0.0680779442191124, -0.0688033252954483, 0.05224759876728058, 0.0016017577145248652, 0.006121051497757435, 0.052819691598415375, 0.06382136046886444, 0.012372256256639957, 0.007432137615978718, 0.07947906851768494, -0.08067011833190918, -0.0697755292057991, 0.024333661422133446, -0.04255221039056778, -0.029982639476656914, -0.04096338525414467, -0.07811647653579712]
[2025-05-10 06:03:31,374]: Mean: 0.00023351
[2025-05-10 06:03:31,375]: Min: -0.08960994
[2025-05-10 06:03:31,375]: Max: 0.08921329
[2025-05-10 06:03:31,375]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-10 06:03:31,375]: Sample Values (25 elements): [1.0001996755599976, 0.9997469782829285, 0.9991154670715332, 0.9999568462371826, 0.99964439868927, 0.9993080496788025, 0.999959409236908, 0.9998288750648499, 1.0000042915344238, 0.9987632632255554, 0.9994488954544067, 0.9998242855072021, 0.9995760917663574, 1.0001347064971924, 1.0000154972076416, 1.0000367164611816, 0.9997816681861877, 0.9991122484207153, 0.9996131062507629, 0.9994344711303711, 0.9994708895683289, 0.998999834060669, 0.9993845820426941, 0.9993417263031006, 0.9998979568481445]
[2025-05-10 06:03:31,375]: Mean: 0.99954313
[2025-05-10 06:03:31,375]: Min: 0.99868584
[2025-05-10 06:03:31,375]: Max: 1.00070024
[2025-05-10 06:03:31,375]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:03:31,381]: Sample Values (25 elements): [0.001262366189621389, 0.0018343458650633693, 0.017597708851099014, -0.009552675299346447, 0.011091659776866436, -0.014257704839110374, 0.017687924206256866, 0.020205816254019737, 0.020008713006973267, 0.009837181307375431, 0.003162552835419774, -0.003464344423264265, 0.011396345682442188, -0.006217748858034611, -0.004600021056830883, 0.0072456831112504005, -0.015804139897227287, 0.020548798143863678, 0.00657142698764801, -0.007548561319708824, -0.019576242193579674, -0.006731576286256313, -0.009294122457504272, 0.00030512732337228954, -0.014371158555150032]
[2025-05-10 06:03:31,381]: Mean: -0.00003764
[2025-05-10 06:03:31,381]: Min: -0.02259484
[2025-05-10 06:03:31,381]: Max: 0.02413107
[2025-05-10 06:03:31,381]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:03:31,381]: Sample Values (25 elements): [0.9995988011360168, 0.9996700882911682, 0.9995896816253662, 0.9995432496070862, 0.9996093511581421, 0.9992291331291199, 1.0001345872879028, 0.9989426732063293, 0.9993607997894287, 0.9996244311332703, 0.9994054436683655, 0.9998264312744141, 0.9999848008155823, 0.9998477101325989, 0.999456524848938, 0.9997300505638123, 0.9994150996208191, 0.9993201494216919, 1.0000859498977661, 0.999110758304596, 0.9993270039558411, 0.9994794726371765, 0.9998447299003601, 0.9994729161262512, 0.9996375441551208]
[2025-05-10 06:03:31,382]: Mean: 0.99961805
[2025-05-10 06:03:31,382]: Min: 0.99890566
[2025-05-10 06:03:31,382]: Max: 1.00055158
[2025-05-10 06:03:31,382]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:03:31,388]: Sample Values (25 elements): [0.004049020819365978, -0.00439620204269886, -0.011679954826831818, -0.018017638474702835, 0.012534253299236298, -0.01920093223452568, -0.01415653433650732, 0.0046922978945076466, -0.01879946142435074, 0.008045562542974949, -0.0027014666702598333, 0.008109175600111485, -0.004010988399386406, -0.014413499273359776, 0.017148438841104507, -0.006843330804258585, -0.016143642365932465, -0.0005915689980611205, -0.004050310701131821, -0.01751156710088253, 0.00907913688570261, 0.0014488567830994725, -0.01673252135515213, -0.005913118831813335, -0.010452711954712868]
[2025-05-10 06:03:31,388]: Mean: -0.00002913
[2025-05-10 06:03:31,388]: Min: -0.02232006
[2025-05-10 06:03:31,388]: Max: 0.02315750
[2025-05-10 06:03:31,389]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:03:31,389]: Sample Values (25 elements): [1.0000426769256592, 0.9997962117195129, 0.9997426271438599, 0.9991683959960938, 0.9993767142295837, 0.9993746280670166, 0.9995433688163757, 0.9993987679481506, 0.999735414981842, 0.9999787211418152, 0.9996259808540344, 0.9995103478431702, 0.9992241263389587, 0.9999162554740906, 0.9996359944343567, 1.00033700466156, 0.9998291730880737, 0.9996878504753113, 0.9998195767402649, 0.9997885227203369, 0.9996604919433594, 0.9994864463806152, 0.9995068311691284, 0.9994469285011292, 0.9996592998504639]
[2025-05-10 06:03:31,389]: Mean: 0.99966037
[2025-05-10 06:03:31,389]: Min: 0.99887788
[2025-05-10 06:03:31,389]: Max: 1.00098085
[2025-05-10 06:03:31,389]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-10 06:03:31,401]: Sample Values (25 elements): [0.016411298885941505, 7.176734470704105e-06, -0.0036556427367031574, 0.010676787234842777, -0.0014281251933425665, 0.012292811647057533, -0.008494325913488865, 0.009044100530445576, 0.010701159946620464, -0.0013747968478128314, 0.017643075436353683, -0.02066611312329769, 0.01319280918687582, -0.020363256335258484, -0.008931636810302734, -0.011677686125040054, -0.01350343693047762, -0.011161211878061295, -0.0013913693837821484, 0.011475838720798492, -0.007944688200950623, -0.008450261317193508, 0.01829630881547928, -0.01622084528207779, -0.011303946375846863]
[2025-05-10 06:03:31,401]: Mean: -0.00000643
[2025-05-10 06:03:31,401]: Min: -0.02219126
[2025-05-10 06:03:31,401]: Max: 0.02231138
[2025-05-10 06:03:31,401]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:03:31,402]: Sample Values (25 elements): [0.9995343685150146, 0.9995416402816772, 0.9999672174453735, 0.9993467926979065, 0.9997434020042419, 0.9993858337402344, 0.9997197985649109, 0.999229907989502, 0.9998885989189148, 0.9995764493942261, 0.9998683333396912, 0.9993964433670044, 0.9997677803039551, 0.9995921850204468, 1.0, 0.9999268054962158, 0.999484658241272, 0.999503493309021, 0.9995964169502258, 0.999191164970398, 0.9995720386505127, 0.9998208284378052, 0.999697208404541, 0.9996435046195984, 0.999435305595398]
[2025-05-10 06:03:31,402]: Mean: 0.99961811
[2025-05-10 06:03:31,402]: Min: 0.99896032
[2025-05-10 06:03:31,402]: Max: 1.00029838
[2025-05-10 06:03:31,402]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:03:31,437]: Sample Values (25 elements): [0.004909214098006487, 0.007750540506094694, -0.009128266014158726, -0.00787376705557108, 0.006768825463950634, -0.011796947568655014, -0.009648924693465233, 0.01096657570451498, -0.005305478349328041, 0.004419844131916761, 0.00951481331139803, -0.001595893525518477, 0.011083926074206829, -0.014209837652742863, 0.011827195063233376, 0.008030331693589687, -0.013247683644294739, -0.010262265801429749, 0.005323625169694424, 0.0023679814767092466, -0.004302508197724819, 0.005035725422203541, 0.0015050899237394333, -0.010073477402329445, -0.0096426485106349]
[2025-05-10 06:03:31,437]: Mean: -0.00000762
[2025-05-10 06:03:31,437]: Min: -0.01598118
[2025-05-10 06:03:31,437]: Max: 0.01623497
[2025-05-10 06:03:31,437]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:03:31,438]: Sample Values (25 elements): [0.9998262524604797, 0.99950110912323, 0.9998452067375183, 0.9997574090957642, 0.9997671246528625, 0.9995698928833008, 0.9995095729827881, 0.999528169631958, 0.9995969533920288, 1.0001641511917114, 0.9997416734695435, 0.9998685121536255, 0.9998864531517029, 0.9997225403785706, 0.9999143481254578, 0.9996041059494019, 1.0000475645065308, 0.9997959136962891, 0.9995527863502502, 0.9996007084846497, 0.999729335308075, 0.9997366666793823, 0.9996274709701538, 0.9996058940887451, 0.9994743466377258]
[2025-05-10 06:03:31,438]: Mean: 0.99975681
[2025-05-10 06:03:31,438]: Min: 0.99922812
[2025-05-10 06:03:31,438]: Max: 1.00098336
[2025-05-10 06:03:31,438]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-10 06:03:31,439]: Sample Values (25 elements): [0.023593857884407043, 0.03306812047958374, 0.054527848958969116, -0.013594617135822773, -0.04321383684873581, 0.014578478410840034, 0.03638645261526108, -0.0454360656440258, -0.014469967223703861, -0.011446108110249043, -0.03383013606071472, 0.05735038220882416, 0.05486975237727165, 0.020690755918622017, 0.052822329103946686, -0.04461534693837166, -0.00508087407797575, -0.040470048785209656, 0.044792380183935165, -0.04357310011982918, 0.022301645949482918, -0.0171448215842247, 0.04291071742773056, -0.027641313150525093, 0.03984137251973152]
[2025-05-10 06:03:31,440]: Mean: 0.00013848
[2025-05-10 06:03:31,440]: Min: -0.06312924
[2025-05-10 06:03:31,440]: Max: 0.06344064
[2025-05-10 06:03:31,440]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-10 06:03:31,440]: Sample Values (25 elements): [0.9997413158416748, 1.0001256465911865, 0.9993557929992676, 0.9995663166046143, 0.9999784827232361, 0.9998117089271545, 0.9995783567428589, 0.9996476769447327, 0.9998353719711304, 1.0000277757644653, 0.999634325504303, 0.9994941353797913, 0.9996880888938904, 0.9993407726287842, 0.9993855357170105, 0.9995535612106323, 1.000211477279663, 0.9997537732124329, 0.9994992613792419, 0.999564528465271, 0.9993325471878052, 0.9998307824134827, 0.9994864463806152, 0.9995118379592896, 0.9995482563972473]
[2025-05-10 06:03:31,440]: Mean: 0.99964607
[2025-05-10 06:03:31,440]: Min: 0.99912244
[2025-05-10 06:03:31,441]: Max: 1.00025380
[2025-05-10 06:03:31,441]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:03:31,479]: Sample Values (25 elements): [0.00893942266702652, -0.0036542201414704323, 0.0025132091250270605, -0.01305348426103592, 0.004992733243852854, 0.011236485093832016, 0.013047710061073303, 0.0018903538584709167, -0.008380154147744179, -0.013931923545897007, -0.010334883816540241, 0.0016804971965029836, 0.008799267001450062, 0.0046928878873586655, 0.012944323010742664, 0.003358699381351471, 0.013524668291211128, -0.012447959743440151, -0.011350933462381363, -0.014047466218471527, -0.007971740327775478, 0.008902314119040966, 0.014237813651561737, -0.00247860886156559, 0.008044393733143806]
[2025-05-10 06:03:31,479]: Mean: -0.00000709
[2025-05-10 06:03:31,480]: Min: -0.01582149
[2025-05-10 06:03:31,480]: Max: 0.01609042
[2025-05-10 06:03:31,480]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:03:31,480]: Sample Values (25 elements): [0.9995759129524231, 0.9993718266487122, 0.9995809197425842, 0.9997062087059021, 0.9996793270111084, 0.9995844960212708, 0.9995774030685425, 0.9995266199111938, 0.9996981024742126, 0.9996044039726257, 0.9998557567596436, 0.9994521141052246, 0.999406099319458, 0.9994604587554932, 0.9994516968727112, 0.9995073676109314, 0.9999053478240967, 0.9999514222145081, 0.9997081160545349, 0.9994296431541443, 0.9996700882911682, 0.9999884366989136, 0.9996683597564697, 0.999692976474762, 0.9994274973869324]
[2025-05-10 06:03:31,480]: Mean: 0.99961805
[2025-05-10 06:03:31,481]: Min: 0.99914980
[2025-05-10 06:03:31,481]: Max: 1.00033653
[2025-05-10 06:03:31,481]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:03:31,522]: Sample Values (25 elements): [0.004658446181565523, -0.0024911058135330677, -0.008644865825772285, 0.00664502801373601, -0.008559777401387691, -0.0031298515386879444, -0.009899664670228958, 0.004767816048115492, -0.014751332812011242, -0.007895683869719505, -0.0029599859844893217, 0.008020957931876183, 0.0006628612172789872, 0.011612079106271267, 0.002419969765469432, -0.008137959986925125, -0.010477220639586449, -0.0055918931029737, 0.001004427787847817, 0.009171642363071442, -0.011629586108028889, 0.0005788562702946365, -0.013217293657362461, -0.005769144278019667, 0.005744395777583122]
[2025-05-10 06:03:31,522]: Mean: -0.00000551
[2025-05-10 06:03:31,523]: Min: -0.01610458
[2025-05-10 06:03:31,523]: Max: 0.01608781
[2025-05-10 06:03:31,523]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:03:31,523]: Sample Values (25 elements): [0.9997413754463196, 0.9996176362037659, 0.9994960427284241, 0.9998044967651367, 0.9995197653770447, 0.9998650550842285, 1.00010347366333, 0.9996643662452698, 0.9996123909950256, 1.0000004768371582, 1.000088095664978, 0.9999825954437256, 0.9997377395629883, 1.0001232624053955, 0.999699592590332, 0.9997509717941284, 1.0003865957260132, 0.9999828338623047, 0.9999743700027466, 1.000090479850769, 0.9996857047080994, 0.9997279047966003, 0.9999241232872009, 0.9996978640556335, 1.000011920928955]
[2025-05-10 06:03:31,523]: Mean: 0.99983442
[2025-05-10 06:03:31,523]: Min: 0.99935699
[2025-05-10 06:03:31,524]: Max: 1.00071526
[2025-05-10 06:03:31,524]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-10 06:03:31,524]: Sample Values (25 elements): [-0.028431875631213188, 0.045456413179636, -0.01940293237566948, 0.033534325659275055, -0.0193112101405859, 0.009789169766008854, -0.008408449590206146, 0.006408467888832092, -0.0038926205597817898, 0.02180006168782711, -0.01109369844198227, -0.005861650686711073, 0.013055806048214436, 0.02577514760196209, 0.022892670705914497, 0.004356931895017624, 0.014839728362858295, -0.015460632741451263, -0.027107996866106987, -0.00843525305390358, -0.039679478853940964, 0.012220547534525394, -0.004088328219950199, 0.049045175313949585, -0.005599761847406626]
[2025-05-10 06:03:31,524]: Mean: 0.00054613
[2025-05-10 06:03:31,524]: Min: -0.05405683
[2025-05-10 06:03:31,524]: Max: 0.05639288
[2025-05-10 06:03:31,524]: 


QAT of ResNet18 with relu down to 4 bits...
[2025-05-10 06:03:31,789]: [ResNet18_relu_quantized_4_bits] after configure_qat:
[2025-05-10 06:03:31,894]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:05:18,808]: [ResNet18_relu_quantized_4_bits] Epoch: 001 Train Loss: 1.2919 Train Acc: 0.5302 Eval Loss: 1.2075 Eval Acc: 0.5702 (LR: 0.001000)
[2025-05-10 06:05:18,885]: [ResNet18_relu_quantized_4_bits] Best Eval Accuracy: 0.5702
[2025-05-10 06:05:18,962]: 


Quantization of model down to 4 bits finished
[2025-05-10 06:05:18,962]: Model Architecture:
[2025-05-10 06:05:19,015]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6781], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.171995162963867)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.051307059824466705, max_val=0.05334344133734703)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6141], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.21117115020752)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.049845483154058456, max_val=0.05128290504217148)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.575060844421387)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0068], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05189254507422447, max_val=0.049608584493398666)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5534], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.301188468933105)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04870413616299629, max_val=0.050245873630046844)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1441], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.16077995300293)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0063], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.046755414456129074, max_val=0.04765046015381813)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5294], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.941162109375)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03401157632470131, max_val=0.03609250485897064)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0171], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12860584259033203, max_val=0.1281256377696991)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7191], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.78672981262207)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03356902301311493, max_val=0.03447048366069794)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4615], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.92250394821167)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03288659825921059, max_val=0.03371930867433548)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8793], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.189871788024902)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03223424404859543, max_val=0.03273386135697365)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4481], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.722232341766357)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023530365899205208, max_val=0.024820363149046898)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09007737785577774, max_val=0.08997806161642075)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6662], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.992871284484863)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02343245968222618, max_val=0.024835461750626564)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4530], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.794478416442871)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023098215460777283, max_val=0.024260591715574265)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8762], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.143153190612793)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0030], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.022575663402676582, max_val=0.022977977991104126)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4504], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.756060600280762)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0022], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.016447443515062332, max_val=0.01660957746207714)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0085], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06338714063167572, max_val=0.06381530314683914)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6947], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.420683860778809)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0022], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.016351811587810516, max_val=0.016686851158738136)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4541], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.811738967895508)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0022], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.01618906855583191, max_val=0.01637084223330021)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9350], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.024781227111816)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:05:19,015]: 
Model Weights:
[2025-05-10 06:05:19,015]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-10 06:05:19,016]: Sample Values (25 elements): [-0.0588592067360878, -0.07266097515821457, 0.09264461696147919, -0.1358761340379715, -0.11698794364929199, 0.1589888334274292, 0.10745560377836227, 0.010164225473999977, 0.029873082414269447, 0.11463106423616409, 0.011012197472155094, 0.06760086864233017, -0.09985732287168503, 0.08808835595846176, -0.17291274666786194, 0.09543858468532562, 0.07353483140468597, -0.12294113636016846, -0.10290693491697311, -0.19563047587871552, 0.06824317574501038, 0.12419481575489044, -0.03908694535493851, 0.013724833726882935, -0.044540803879499435]
[2025-05-10 06:05:19,016]: Mean: -0.00083574
[2025-05-10 06:05:19,016]: Min: -0.20039089
[2025-05-10 06:05:19,016]: Max: 0.21196982
[2025-05-10 06:05:19,016]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-10 06:05:19,017]: Sample Values (25 elements): [1.0019457340240479, 0.9980212450027466, 0.9984541535377502, 1.0049622058868408, 0.9965958595275879, 0.9944255948066711, 1.0036929845809937, 0.9962745904922485, 0.9994493722915649, 0.9944524168968201, 0.999397873878479, 1.0066052675247192, 0.9962982535362244, 1.0068613290786743, 0.9979516267776489, 1.0050824880599976, 1.001400113105774, 0.9970940351486206, 0.9962363839149475, 0.9997132420539856, 0.9983892440795898, 0.9992290139198303, 0.9998085498809814, 1.000016212463379, 0.9953888654708862]
[2025-05-10 06:05:19,017]: Mean: 0.99999505
[2025-05-10 06:05:19,017]: Min: 0.99442559
[2025-05-10 06:05:19,018]: Max: 1.01373947
[2025-05-10 06:05:19,019]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:05:19,020]: Sample Values (25 elements): [0.0, 0.03489295393228531, 0.006978590972721577, -0.013957181945443153, -0.03489295393228531, 0.0, -0.013957181945443153, -0.027914363890886307, -0.03489295393228531, 0.04187154769897461, 0.013957181945443153, -0.03489295393228531, 0.013957181945443153, -0.020935773849487305, -0.006978590972721577, 0.03489295393228531, -0.04187154769897461, 0.0, 0.006978590972721577, -0.03489295393228531, -0.03489295393228531, 0.006978590972721577, 0.013957181945443153, 0.020935773849487305, -0.03489295393228531]
[2025-05-10 06:05:19,020]: Mean: -0.00029551
[2025-05-10 06:05:19,020]: Min: -0.04885014
[2025-05-10 06:05:19,020]: Max: 0.05582873
[2025-05-10 06:05:19,020]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:05:19,020]: Sample Values (25 elements): [1.0070056915283203, 0.9990774989128113, 0.9993659853935242, 0.9977211952209473, 0.9978901147842407, 1.0032343864440918, 0.9993339776992798, 0.9972378015518188, 0.9963777661323547, 1.0024847984313965, 1.0005556344985962, 1.0004067420959473, 0.9980391263961792, 0.9973127841949463, 0.9978343844413757, 0.9983769059181213, 1.0020440816879272, 0.9969489574432373, 0.9979414343833923, 1.003562331199646, 0.9965094923973083, 0.9966378211975098, 1.001304030418396, 0.9990236163139343, 0.996361255645752]
[2025-05-10 06:05:19,021]: Mean: 0.99957639
[2025-05-10 06:05:19,021]: Min: 0.99583948
[2025-05-10 06:05:19,021]: Max: 1.00700569
[2025-05-10 06:05:19,022]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:05:19,022]: Sample Values (25 elements): [0.0, 0.026971012353897095, -0.02022825926542282, -0.04045651853084564, 0.013485506176948547, -0.04045651853084564, -0.026971012353897095, -0.03371376544237137, -0.006742753088474274, -0.026971012353897095, 0.02022825926542282, 0.026971012353897095, 0.04045651853084564, 0.02022825926542282, -0.006742753088474274, 0.006742753088474274, -0.013485506176948547, -0.026971012353897095, -0.026971012353897095, 0.02022825926542282, -0.006742753088474274, -0.04045651853084564, -0.006742753088474274, 0.03371376544237137, -0.02022825926542282]
[2025-05-10 06:05:19,023]: Mean: -0.00044172
[2025-05-10 06:05:19,023]: Min: -0.04719927
[2025-05-10 06:05:19,023]: Max: 0.05394202
[2025-05-10 06:05:19,023]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:05:19,024]: Sample Values (25 elements): [0.9987931847572327, 0.9991602301597595, 1.0016350746154785, 0.9990417957305908, 1.00086510181427, 0.9987922310829163, 1.001161813735962, 0.9982805848121643, 1.0006352663040161, 1.0024980306625366, 1.0024131536483765, 0.9987180829048157, 1.0016732215881348, 1.0025192499160767, 0.9986178874969482, 0.9987685084342957, 0.9982520341873169, 0.9994795322418213, 0.9968233108520508, 0.9970250129699707, 1.0089714527130127, 0.996567964553833, 0.9978795051574707, 0.9977714419364929, 1.000494122505188]
[2025-05-10 06:05:19,024]: Mean: 0.99959826
[2025-05-10 06:05:19,024]: Min: 0.99490845
[2025-05-10 06:05:19,024]: Max: 1.00897145
[2025-05-10 06:05:19,025]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:05:19,026]: Sample Values (25 elements): [-0.013535812497138977, 0.0067679062485694885, 0.04060743749141693, 0.03383953124284744, 0.03383953124284744, -0.027071624994277954, 0.04060743749141693, 0.027071624994277954, -0.04060743749141693, 0.03383953124284744, 0.013535812497138977, -0.027071624994277954, 0.027071624994277954, 0.013535812497138977, 0.0067679062485694885, -0.03383953124284744, -0.03383953124284744, -0.013535812497138977, 0.027071624994277954, -0.020303718745708466, -0.0067679062485694885, -0.013535812497138977, -0.020303718745708466, 0.027071624994277954, -0.020303718745708466]
[2025-05-10 06:05:19,026]: Mean: -0.00009124
[2025-05-10 06:05:19,026]: Min: -0.05414325
[2025-05-10 06:05:19,026]: Max: 0.04737534
[2025-05-10 06:05:19,026]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:05:19,027]: Sample Values (25 elements): [0.9977025389671326, 0.9982243180274963, 0.9990426898002625, 1.0007741451263428, 0.9974862337112427, 0.9983063340187073, 0.9985415935516357, 0.9995874762535095, 0.9981383681297302, 1.0010238885879517, 1.0010124444961548, 1.0012965202331543, 0.9976102113723755, 0.9992121458053589, 1.0013996362686157, 0.9988725185394287, 0.9996100068092346, 0.9974145889282227, 1.0014145374298096, 0.9981839656829834, 0.9969533681869507, 1.0026628971099854, 0.9978052973747253, 0.9984387755393982, 1.00123929977417]
[2025-05-10 06:05:19,027]: Mean: 0.99941474
[2025-05-10 06:05:19,027]: Min: 0.99635768
[2025-05-10 06:05:19,027]: Max: 1.00524068
[2025-05-10 06:05:19,028]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:05:19,029]: Sample Values (25 elements): [-0.026389282196760178, -0.026389282196760178, -0.026389282196760178, 0.026389282196760178, 0.0, 0.046181242913007736, -0.006597320549190044, -0.03958392143249512, -0.013194641098380089, -0.01979196071624756, 0.026389282196760178, 0.0329866036772728, -0.006597320549190044, 0.01979196071624756, -0.0329866036772728, -0.0329866036772728, 0.006597320549190044, 0.013194641098380089, 0.0, -0.01979196071624756, -0.01979196071624756, -0.0329866036772728, -0.0329866036772728, 0.01979196071624756, -0.03958392143249512]
[2025-05-10 06:05:19,029]: Mean: -0.00007033
[2025-05-10 06:05:19,029]: Min: -0.04618124
[2025-05-10 06:05:19,029]: Max: 0.05277856
[2025-05-10 06:05:19,029]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:05:19,030]: Sample Values (25 elements): [0.9992140531539917, 1.0013859272003174, 0.9989028573036194, 0.9991421103477478, 0.9981985688209534, 0.9994949102401733, 1.0004329681396484, 1.001071572303772, 1.0003647804260254, 0.9979762434959412, 1.001237392425537, 1.0001119375228882, 0.998036801815033, 0.9994198679924011, 1.0014885663986206, 0.9996029734611511, 0.9989226460456848, 1.0037236213684082, 1.0022870302200317, 0.9991118311882019, 1.002854824066162, 1.000266432762146, 1.001162052154541, 1.0029116868972778, 0.997982382774353]
[2025-05-10 06:05:19,031]: Mean: 0.99991339
[2025-05-10 06:05:19,031]: Min: 0.99661255
[2025-05-10 06:05:19,031]: Max: 1.00372362
[2025-05-10 06:05:19,033]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-10 06:05:19,033]: Sample Values (25 elements): [0.03147193789482117, 0.025177551433444023, 0.012588775716722012, 0.006294387858361006, 0.03147193789482117, -0.03147193789482117, 0.006294387858361006, -0.03776632621884346, 0.012588775716722012, 0.03147193789482117, -0.025177551433444023, -0.025177551433444023, -0.03776632621884346, 0.03147193789482117, -0.03147193789482117, -0.006294387858361006, 0.006294387858361006, 0.025177551433444023, -0.03147193789482117, 0.006294387858361006, 0.0, 0.006294387858361006, -0.006294387858361006, -0.03147193789482117, -0.03776632621884346]
[2025-05-10 06:05:19,034]: Mean: -0.00010603
[2025-05-10 06:05:19,034]: Min: -0.04406071
[2025-05-10 06:05:19,034]: Max: 0.05035510
[2025-05-10 06:05:19,034]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:05:19,034]: Sample Values (25 elements): [0.998471200466156, 0.9990044236183167, 0.999772310256958, 0.9981680512428284, 0.9985641837120056, 0.9983827471733093, 1.0001089572906494, 0.9997138977050781, 0.9998834133148193, 1.000762701034546, 0.9999019503593445, 0.9995821118354797, 1.0012249946594238, 0.9999088048934937, 0.9982977509498596, 1.0002601146697998, 0.9994603395462036, 0.9972411394119263, 0.9980258345603943, 0.9987820982933044, 0.999125599861145, 1.0010617971420288, 0.9999445080757141, 1.0003912448883057, 0.9989629983901978]
[2025-05-10 06:05:19,034]: Mean: 0.99932283
[2025-05-10 06:05:19,035]: Min: 0.99715310
[2025-05-10 06:05:19,035]: Max: 1.00337076
[2025-05-10 06:05:19,036]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:05:19,037]: Sample Values (25 elements): [0.004674362484365702, 0.009348724968731403, -0.004674362484365702, 0.014023087918758392, 0.0, 0.014023087918758392, -0.018697449937462807, -0.014023087918758392, -0.004674362484365702, 0.009348724968731403, 0.018697449937462807, 0.028046175837516785, -0.02337181195616722, 0.004674362484365702, -0.009348724968731403, 0.004674362484365702, -0.009348724968731403, -0.028046175837516785, 0.028046175837516785, -0.02337181195616722, 0.014023087918758392, 0.018697449937462807, 0.028046175837516785, -0.004674362484365702, -0.004674362484365702]
[2025-05-10 06:05:19,038]: Mean: -0.00014313
[2025-05-10 06:05:19,038]: Min: -0.03272054
[2025-05-10 06:05:19,038]: Max: 0.03739490
[2025-05-10 06:05:19,038]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:05:19,038]: Sample Values (25 elements): [0.9990381598472595, 0.9987128376960754, 1.0008255243301392, 0.9991537928581238, 0.9993791580200195, 0.9984687566757202, 0.9993450045585632, 0.9996782541275024, 0.9992928504943848, 1.0005316734313965, 0.9984073042869568, 0.9992245435714722, 0.9987193942070007, 0.9977598190307617, 0.9998645186424255, 1.0005855560302734, 1.0015393495559692, 0.9997183084487915, 0.9997497797012329, 1.000430703163147, 0.9992291927337646, 1.0007188320159912, 0.9983415007591248, 0.9990391135215759, 0.9989702701568604]
[2025-05-10 06:05:19,039]: Mean: 0.99940610
[2025-05-10 06:05:19,039]: Min: 0.99703538
[2025-05-10 06:05:19,039]: Max: 1.00290442
[2025-05-10 06:05:19,040]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-10 06:05:19,040]: Sample Values (25 elements): [0.03423192724585533, -0.08557981997728348, 0.11981174349784851, -0.05134788900613785, -0.017115963622927666, -0.06846385449171066, -0.1026957780122757, 0.0, -0.06846385449171066, 0.05134788900613785, -0.05134788900613785, -0.1026957780122757, -0.05134788900613785, -0.1026957780122757, 0.03423192724585533, -0.05134788900613785, -0.017115963622927666, -0.08557981997728348, -0.1026957780122757, -0.1026957780122757, 0.03423192724585533, -0.017115963622927666, 0.08557981997728348, -0.1026957780122757, 0.11981174349784851]
[2025-05-10 06:05:19,040]: Mean: -0.00083574
[2025-05-10 06:05:19,041]: Min: -0.13692771
[2025-05-10 06:05:19,041]: Max: 0.11981174
[2025-05-10 06:05:19,041]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-10 06:05:19,041]: Sample Values (25 elements): [0.9989867806434631, 1.0011284351348877, 0.9999322891235352, 0.9988132119178772, 0.9981692433357239, 0.9994379281997681, 0.9988131523132324, 1.0004241466522217, 0.9996371269226074, 0.9969640970230103, 0.9988836646080017, 0.9979322552680969, 0.9994804859161377, 0.9981886148452759, 1.001534342765808, 0.9993360042572021, 0.9995487928390503, 0.9994674324989319, 0.999708890914917, 0.9991010427474976, 0.9989203810691833, 0.9989025592803955, 0.9984720349311829, 0.9998255372047424, 0.999120831489563]
[2025-05-10 06:05:19,041]: Mean: 0.99913895
[2025-05-10 06:05:19,041]: Min: 0.99656695
[2025-05-10 06:05:19,041]: Max: 1.00153434
[2025-05-10 06:05:19,042]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:05:19,044]: Sample Values (25 elements): [-0.01814676821231842, -0.022683460265398026, 0.004536692053079605, 0.0, -0.02722015231847763, -0.004536692053079605, -0.022683460265398026, -0.022683460265398026, -0.02722015231847763, -0.004536692053079605, -0.00907338410615921, 0.013610076159238815, 0.0, 0.013610076159238815, 0.022683460265398026, 0.02722015231847763, 0.004536692053079605, 0.004536692053079605, 0.022683460265398026, -0.00907338410615921, 0.01814676821231842, -0.02722015231847763, -0.01814676821231842, 0.022683460265398026, -0.022683460265398026]
[2025-05-10 06:05:19,044]: Mean: -0.00001572
[2025-05-10 06:05:19,044]: Min: -0.03175684
[2025-05-10 06:05:19,045]: Max: 0.03629354
[2025-05-10 06:05:19,045]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:05:19,046]: Sample Values (25 elements): [0.99837726354599, 0.9986046552658081, 1.0000104904174805, 1.000061273574829, 0.9995025396347046, 0.9981162548065186, 0.9992374181747437, 0.9979544878005981, 0.9996156096458435, 0.9988479614257812, 0.9984161257743835, 0.9978733062744141, 0.9978023767471313, 0.9988307952880859, 0.9984796643257141, 0.9986664652824402, 0.9994297027587891, 0.99908447265625, 0.9986290335655212, 0.999819278717041, 1.000333309173584, 1.000072956085205, 0.9983003735542297, 1.0001881122589111, 0.9997085928916931]
[2025-05-10 06:05:19,046]: Mean: 0.99927956
[2025-05-10 06:05:19,046]: Min: 0.99742162
[2025-05-10 06:05:19,047]: Max: 1.00311816
[2025-05-10 06:05:19,048]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:05:19,050]: Sample Values (25 elements): [-0.026645595207810402, -0.017763730138540268, -0.017763730138540268, -0.004440932534635067, -0.017763730138540268, -0.026645595207810402, 0.02220466360449791, 0.02220466360449791, 0.02220466360449791, 0.02220466360449791, -0.004440932534635067, 0.0, 0.008881865069270134, 0.004440932534635067, -0.031086526811122894, 0.013322797603905201, 0.017763730138540268, -0.013322797603905201, 0.031086526811122894, 0.008881865069270134, -0.008881865069270134, 0.008881865069270134, 0.017763730138540268, 0.004440932534635067, 0.017763730138540268]
[2025-05-10 06:05:19,050]: Mean: -0.00005051
[2025-05-10 06:05:19,050]: Min: -0.03108653
[2025-05-10 06:05:19,050]: Max: 0.03552746
[2025-05-10 06:05:19,050]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:05:19,051]: Sample Values (25 elements): [0.9989051818847656, 1.000055193901062, 0.9999493956565857, 0.9987882375717163, 0.9985089898109436, 0.9993101954460144, 0.9986932873725891, 0.9989749789237976, 0.998876690864563, 0.9999546408653259, 0.9986444711685181, 1.0009384155273438, 0.9998385906219482, 0.9987856149673462, 1.000955581665039, 0.9990707039833069, 0.9987823963165283, 0.9994703531265259, 0.9988739490509033, 0.9988621473312378, 0.9994744062423706, 1.0002422332763672, 1.000357747077942, 1.0003690719604492, 0.9994082450866699]
[2025-05-10 06:05:19,051]: Mean: 0.99941772
[2025-05-10 06:05:19,051]: Min: 0.99736297
[2025-05-10 06:05:19,052]: Max: 1.00095558
[2025-05-10 06:05:19,054]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-10 06:05:19,058]: Sample Values (25 elements): [-0.025989700108766556, 0.017326466739177704, -0.02165808342397213, 0.004331616684794426, -0.02165808342397213, -0.030321316793560982, -0.017326466739177704, 0.008663233369588852, 0.008663233369588852, -0.008663233369588852, -0.017326466739177704, -0.012994850054383278, -0.017326466739177704, -0.012994850054383278, -0.008663233369588852, 0.017326466739177704, -0.030321316793560982, 0.004331616684794426, 0.008663233369588852, 0.004331616684794426, -0.012994850054383278, 0.017326466739177704, -0.008663233369588852, 0.017326466739177704, -0.008663233369588852]
[2025-05-10 06:05:19,059]: Mean: -0.00007858
[2025-05-10 06:05:19,059]: Min: -0.03032132
[2025-05-10 06:05:19,059]: Max: 0.03465293
[2025-05-10 06:05:19,059]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:05:19,060]: Sample Values (25 elements): [0.9990205764770508, 0.9992649555206299, 0.9996529817581177, 0.9993964433670044, 0.9987786412239075, 0.9989606142044067, 0.999100387096405, 1.0002801418304443, 0.9984853267669678, 0.9984939694404602, 0.999059796333313, 0.9998491406440735, 0.9991698265075684, 0.9991501569747925, 1.000014305114746, 0.9994919300079346, 0.9994242787361145, 0.9992561340332031, 0.9987213611602783, 1.000889778137207, 0.9986411929130554, 0.9994041323661804, 0.9994719624519348, 0.9998502731323242, 0.998660683631897]
[2025-05-10 06:05:19,060]: Mean: 0.99926347
[2025-05-10 06:05:19,060]: Min: 0.99813962
[2025-05-10 06:05:19,060]: Max: 1.00106227
[2025-05-10 06:05:19,061]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:05:19,071]: Sample Values (25 elements): [-0.016118425875902176, -0.016118425875902176, 0.016118425875902176, 0.0032236850820481777, -0.009671054780483246, -0.01934210956096649, 0.0, 0.01934210956096649, -0.0032236850820481777, 0.0032236850820481777, 0.016118425875902176, 0.0032236850820481777, 0.0, 0.009671054780483246, -0.01934210956096649, -0.009671054780483246, -0.0064473701640963554, 0.0064473701640963554, -0.0064473701640963554, 0.0, -0.022565795108675957, 0.0032236850820481777, 0.0032236850820481777, -0.012894740328192711, 0.0]
[2025-05-10 06:05:19,071]: Mean: -0.00003460
[2025-05-10 06:05:19,071]: Min: -0.02256580
[2025-05-10 06:05:19,071]: Max: 0.02578948
[2025-05-10 06:05:19,071]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:05:19,072]: Sample Values (25 elements): [0.9989278316497803, 1.0001827478408813, 0.9990582466125488, 0.9993932247161865, 0.9990912079811096, 1.000080943107605, 0.9994621276855469, 0.999323308467865, 0.9993717670440674, 0.9995466470718384, 0.9992060661315918, 0.9992420077323914, 1.0009740591049194, 0.999167799949646, 0.9983205199241638, 0.998603880405426, 0.9986952543258667, 0.9988190531730652, 0.9993952512741089, 1.000267505645752, 0.9994462132453918, 0.9992563724517822, 0.9993159174919128, 0.9998725652694702, 0.9991564154624939]
[2025-05-10 06:05:19,072]: Mean: 0.99938154
[2025-05-10 06:05:19,072]: Min: 0.99796295
[2025-05-10 06:05:19,072]: Max: 1.00237846
[2025-05-10 06:05:19,074]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-10 06:05:19,074]: Sample Values (25 elements): [-0.06001950800418854, 0.08402731269598007, -0.06001950800418854, -0.072023406624794, 0.08402731269598007, -0.06001950800418854, 0.024007802829146385, -0.06001950800418854, 0.072023406624794, -0.012003901414573193, 0.06001950800418854, -0.072023406624794, -0.012003901414573193, 0.08402731269598007, 0.012003901414573193, -0.06001950800418854, 0.06001950800418854, -0.04801560565829277, -0.024007802829146385, -0.036011703312397, -0.072023406624794, 0.012003901414573193, -0.06001950800418854, 0.036011703312397, 0.012003901414573193]
[2025-05-10 06:05:19,074]: Mean: 0.00021723
[2025-05-10 06:05:19,075]: Min: -0.09603121
[2025-05-10 06:05:19,075]: Max: 0.08402731
[2025-05-10 06:05:19,075]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-10 06:05:19,075]: Sample Values (25 elements): [0.9993365406990051, 0.998934805393219, 0.9995622038841248, 0.9990460276603699, 0.9992179274559021, 0.9991370439529419, 0.9984536170959473, 0.9990649819374084, 0.9991155862808228, 0.9987637400627136, 0.9990717172622681, 0.9985055923461914, 0.9987863898277283, 0.9981909394264221, 0.9978957772254944, 0.9987227916717529, 0.9984087944030762, 0.9998234510421753, 0.9991166591644287, 0.9994196891784668, 0.9993614554405212, 0.9993388056755066, 1.0010188817977905, 0.9987613558769226, 0.9992852210998535]
[2025-05-10 06:05:19,075]: Mean: 0.99900842
[2025-05-10 06:05:19,075]: Min: 0.99771720
[2025-05-10 06:05:19,075]: Max: 1.00101888
[2025-05-10 06:05:19,076]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:05:19,084]: Sample Values (25 elements): [0.003218053374439478, 0.006436106748878956, 0.009654160588979721, 0.003218053374439478, 0.012872213497757912, 0.012872213497757912, 0.009654160588979721, -0.016090266406536102, -0.009654160588979721, -0.019308321177959442, 0.019308321177959442, 0.016090266406536102, 0.006436106748878956, 0.0, 0.0, -0.019308321177959442, 0.016090266406536102, 0.019308321177959442, -0.016090266406536102, 0.009654160588979721, -0.006436106748878956, -0.012872213497757912, -0.009654160588979721, 0.012872213497757912, -0.006436106748878956]
[2025-05-10 06:05:19,084]: Mean: -0.00004621
[2025-05-10 06:05:19,084]: Min: -0.02252637
[2025-05-10 06:05:19,084]: Max: 0.02574443
[2025-05-10 06:05:19,084]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:05:19,085]: Sample Values (25 elements): [0.9994181394577026, 0.9993630647659302, 0.9996282458305359, 0.9991609454154968, 0.9988877773284912, 0.9982141852378845, 0.998780369758606, 0.9994478821754456, 0.9996907711029053, 0.9986312985420227, 0.9997047781944275, 0.999439537525177, 0.9992157816886902, 0.999252438545227, 0.9988177418708801, 1.0002167224884033, 0.9994601011276245, 0.9988346099853516, 0.9996883869171143, 0.999123215675354, 0.9984741806983948, 0.9994350671768188, 0.9992023706436157, 0.9989522695541382, 1.0003751516342163]
[2025-05-10 06:05:19,085]: Mean: 0.99925506
[2025-05-10 06:05:19,085]: Min: 0.99820995
[2025-05-10 06:05:19,085]: Max: 1.00067353
[2025-05-10 06:05:19,087]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:05:19,095]: Sample Values (25 elements): [-0.015788307413458824, 0.01894596964120865, -0.01894596964120865, 0.01894596964120865, -0.006315323058515787, -0.012630646117031574, -0.012630646117031574, 0.006315323058515787, 0.009472984820604324, -0.006315323058515787, 0.006315323058515787, 0.015788307413458824, 0.0, -0.006315323058515787, 0.01894596964120865, 0.009472984820604324, -0.015788307413458824, 0.01894596964120865, -0.009472984820604324, 0.012630646117031574, 0.012630646117031574, -0.01894596964120865, -0.006315323058515787, 0.009472984820604324, 0.015788307413458824]
[2025-05-10 06:05:19,095]: Mean: -0.00003433
[2025-05-10 06:05:19,096]: Min: -0.02210363
[2025-05-10 06:05:19,096]: Max: 0.02526129
[2025-05-10 06:05:19,096]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:05:19,096]: Sample Values (25 elements): [0.9988124966621399, 0.9994384050369263, 0.9987022280693054, 0.9989668726921082, 0.9989795088768005, 0.9991669058799744, 0.9989591836929321, 0.9994894862174988, 0.9992486834526062, 0.9986479878425598, 0.9992356896400452, 0.9991608262062073, 0.999296247959137, 0.9993287324905396, 0.9991805553436279, 1.0001447200775146, 0.9993957877159119, 0.9991511702537537, 0.9988872408866882, 0.9995033740997314, 0.9996101260185242, 0.9995350241661072, 0.9989709854125977, 0.998927891254425, 0.9993609189987183]
[2025-05-10 06:05:19,096]: Mean: 0.99940872
[2025-05-10 06:05:19,097]: Min: 0.99820364
[2025-05-10 06:05:19,097]: Max: 1.00149119
[2025-05-10 06:05:19,098]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-10 06:05:19,113]: Sample Values (25 elements): [-0.003037147456780076, 0.018222885206341743, 0.006074294913560152, -0.009111442603170872, -0.015185737051069736, -0.003037147456780076, -0.015185737051069736, -0.015185737051069736, 0.018222885206341743, 0.018222885206341743, 0.021260032430291176, 0.012148589827120304, 0.015185737051069736, 0.018222885206341743, 0.009111442603170872, -0.009111442603170872, -0.018222885206341743, 0.003037147456780076, -0.009111442603170872, -0.018222885206341743, -0.015185737051069736, 0.018222885206341743, 0.009111442603170872, -0.018222885206341743, -0.015185737051069736]
[2025-05-10 06:05:19,113]: Mean: -0.00001197
[2025-05-10 06:05:19,114]: Min: -0.02126003
[2025-05-10 06:05:19,114]: Max: 0.02429718
[2025-05-10 06:05:19,114]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:05:19,115]: Sample Values (25 elements): [0.9991493225097656, 0.999460756778717, 0.9994533061981201, 0.9997158646583557, 0.9998050928115845, 0.9992522597312927, 0.9989930391311646, 0.9998721480369568, 0.9994731545448303, 0.9991443753242493, 0.9987926483154297, 0.9991223216056824, 0.9997463822364807, 0.9996069073677063, 0.9988289475440979, 0.998892068862915, 0.9996130466461182, 0.9993684887886047, 0.999468207359314, 0.9995284080505371, 0.9994725584983826, 0.9992749691009521, 0.999016523361206, 0.9991596341133118, 0.999137818813324]
[2025-05-10 06:05:19,115]: Mean: 0.99924386
[2025-05-10 06:05:19,115]: Min: 0.99841207
[2025-05-10 06:05:19,116]: Max: 1.00035119
[2025-05-10 06:05:19,117]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:05:19,166]: Sample Values (25 elements): [-0.01322318147867918, 0.01322318147867918, 0.0, 0.004407727159559727, 0.00661159073933959, -0.01322318147867918, 0.004407727159559727, 0.01101931743323803, 0.01322318147867918, 0.004407727159559727, 0.008815454319119453, 0.00661159073933959, 0.004407727159559727, -0.0022038635797798634, 0.01101931743323803, 0.01101931743323803, 0.01322318147867918, 0.004407727159559727, 0.008815454319119453, 0.01322318147867918, -0.01101931743323803, 0.01101931743323803, 0.01322318147867918, 0.008815454319119453, -0.004407727159559727]
[2025-05-10 06:05:19,167]: Mean: -0.00000906
[2025-05-10 06:05:19,167]: Min: -0.01542705
[2025-05-10 06:05:19,167]: Max: 0.01763091
[2025-05-10 06:05:19,167]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:05:19,168]: Sample Values (25 elements): [0.9995670914649963, 0.9992743730545044, 0.9991926550865173, 0.9994288682937622, 0.9995189905166626, 1.0014280080795288, 0.9997841119766235, 0.999306857585907, 0.9998636245727539, 0.9993254542350769, 0.9993414878845215, 0.9991176724433899, 0.9993402361869812, 0.9994988441467285, 0.9991334080696106, 0.9995209574699402, 0.9996215105056763, 0.9990665912628174, 0.9996997117996216, 1.0001220703125, 0.9990956783294678, 0.9995235204696655, 0.9995751976966858, 0.9994247555732727, 0.9989709854125977]
[2025-05-10 06:05:19,168]: Mean: 0.99948508
[2025-05-10 06:05:19,168]: Min: 0.99868757
[2025-05-10 06:05:19,168]: Max: 1.00142801
[2025-05-10 06:05:19,170]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-10 06:05:19,171]: Sample Values (25 elements): [0.01696041040122509, 0.05936143547296524, 0.03392082080245018, 0.05936143547296524, 0.008480205200612545, 0.05088122934103012, 0.01696041040122509, 0.05088122934103012, 0.008480205200612545, 0.01696041040122509, 0.03392082080245018, 0.05936143547296524, 0.05936143547296524, 0.01696041040122509, 0.05088122934103012, -0.02544061467051506, -0.03392082080245018, -0.01696041040122509, -0.05088122934103012, -0.0424010269343853, 0.008480205200612545, -0.03392082080245018, 0.008480205200612545, 0.05936143547296524, 0.02544061467051506]
[2025-05-10 06:05:19,171]: Mean: 0.00013212
[2025-05-10 06:05:19,172]: Min: -0.05936144
[2025-05-10 06:05:19,172]: Max: 0.06784164
[2025-05-10 06:05:19,172]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-10 06:05:19,172]: Sample Values (25 elements): [0.9995636343955994, 0.9993743896484375, 0.9992791414260864, 0.9988574981689453, 0.9996253252029419, 0.9993340373039246, 0.9993636012077332, 0.9993886947631836, 0.9986465573310852, 0.9993740916252136, 0.9993366003036499, 0.9989878535270691, 0.9992396831512451, 0.9995115399360657, 0.9994110465049744, 0.9992360472679138, 0.9997867941856384, 0.9992784857749939, 0.9992305040359497, 0.999410092830658, 0.9986767768859863, 0.9988049268722534, 0.9996318817138672, 0.9991863369941711, 0.9989525079727173]
[2025-05-10 06:05:19,172]: Mean: 0.99925202
[2025-05-10 06:05:19,172]: Min: 0.99861181
[2025-05-10 06:05:19,172]: Max: 1.00010216
[2025-05-10 06:05:19,174]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:05:19,223]: Sample Values (25 elements): [-0.004405445419251919, -0.004405445419251919, -0.013216336257755756, -0.0022027227096259594, 0.015419058501720428, 0.006608168128877878, -0.006608168128877878, -0.013216336257755756, -0.0022027227096259594, 0.006608168128877878, -0.013216336257755756, 0.011013614013791084, -0.008810890838503838, -0.004405445419251919, 0.006608168128877878, -0.0022027227096259594, -0.006608168128877878, -0.0022027227096259594, -0.011013614013791084, 0.0022027227096259594, 0.011013614013791084, 0.004405445419251919, 0.0022027227096259594, 0.011013614013791084, 0.013216336257755756]
[2025-05-10 06:05:19,223]: Mean: -0.00000899
[2025-05-10 06:05:19,223]: Min: -0.01541906
[2025-05-10 06:05:19,224]: Max: 0.01762178
[2025-05-10 06:05:19,224]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:05:19,224]: Sample Values (25 elements): [0.9992568492889404, 0.999311089515686, 0.9997304677963257, 0.9989467859268188, 0.9993037581443787, 0.9997341632843018, 0.9998244047164917, 0.9996866583824158, 0.9990863800048828, 0.9991248846054077, 0.9993530511856079, 0.9989715218544006, 0.9993149042129517, 1.0000765323638916, 0.9994927644729614, 0.9993749260902405, 0.9986923933029175, 0.998723566532135, 0.999125063419342, 0.9989193677902222, 0.9990471601486206, 0.9995220899581909, 0.9991927146911621, 0.9994369745254517, 0.9990640878677368]
[2025-05-10 06:05:19,224]: Mean: 0.99924207
[2025-05-10 06:05:19,224]: Min: 0.99860501
[2025-05-10 06:05:19,224]: Max: 1.00035465
[2025-05-10 06:05:19,226]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:05:19,272]: Sample Values (25 elements): [-0.002170776017010212, 0.013024656102061272, 0.013024656102061272, -0.002170776017010212, 0.013024656102061272, 0.0, 0.015195432119071484, 0.002170776017010212, 0.013024656102061272, -0.01085388008505106, -0.006512328051030636, -0.013024656102061272, 0.002170776017010212, 0.002170776017010212, -0.01085388008505106, 0.01085388008505106, -0.01085388008505106, 0.0, -0.01085388008505106, 0.013024656102061272, 0.002170776017010212, 0.006512328051030636, -0.004341552034020424, 0.002170776017010212, 0.002170776017010212]
[2025-05-10 06:05:19,272]: Mean: -0.00000205
[2025-05-10 06:05:19,272]: Min: -0.01519543
[2025-05-10 06:05:19,272]: Max: 0.01736621
[2025-05-10 06:05:19,273]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:05:19,273]: Sample Values (25 elements): [0.9995948672294617, 1.0003401041030884, 0.9997550249099731, 0.9992263317108154, 0.9995379447937012, 0.9996654391288757, 0.9992253184318542, 0.9997878074645996, 1.0002503395080566, 0.9995355606079102, 0.9998752474784851, 0.9995182752609253, 0.9996922016143799, 0.9995908737182617, 0.9995054006576538, 0.9993525147438049, 0.9994263648986816, 0.999498724937439, 0.999961256980896, 0.9996894598007202, 0.9997916221618652, 1.000022530555725, 1.0003911256790161, 0.999423623085022, 0.9992228746414185]
[2025-05-10 06:05:19,273]: Mean: 0.99961138
[2025-05-10 06:05:19,274]: Min: 0.99896657
[2025-05-10 06:05:19,274]: Max: 1.00054574
[2025-05-10 06:05:19,274]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-10 06:05:19,274]: Sample Values (25 elements): [0.001983718480914831, -0.04796178266406059, 0.03395475074648857, 0.007604994811117649, -0.005764184053987265, 0.04850757494568825, 0.012733303010463715, -0.034489549696445465, -0.03176575154066086, -0.017914149910211563, -0.02781587652862072, 0.017814235761761665, 0.0032030304428189993, -0.02630644105374813, -0.01938442327082157, -0.024264317005872726, 0.01372733898460865, 0.043689776211977005, 0.0032933452166616917, -0.03571447357535362, -0.031184077262878418, 0.02580880932509899, -0.012539196759462357, 0.029068317264318466, 0.011622639372944832]
[2025-05-10 06:05:19,275]: Mean: 0.00054592
[2025-05-10 06:05:19,275]: Min: -0.05860392
[2025-05-10 06:05:19,275]: Max: 0.06085042
[2025-05-10 06:05:19,275]: 


QAT of ResNet18 with relu down to 3 bits...
[2025-05-10 06:05:19,509]: [ResNet18_relu_quantized_3_bits] after configure_qat:
[2025-05-10 06:05:19,560]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:07:10,751]: [ResNet18_relu_quantized_3_bits] Epoch: 001 Train Loss: 1.4222 Train Acc: 0.4799 Eval Loss: 1.2832 Eval Acc: 0.5279 (LR: 0.001000)
[2025-05-10 06:07:10,826]: [ResNet18_relu_quantized_3_bits] Best Eval Accuracy: 0.5279
[2025-05-10 06:07:10,911]: 


Quantization of model down to 3 bits finished
[2025-05-10 06:07:10,911]: Model Architecture:
[2025-05-10 06:07:10,959]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4233], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.962807655334473)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05115395039319992, max_val=0.05306566134095192)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2372], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.66043758392334)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04909602180123329, max_val=0.049802035093307495)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8248], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.773477554321289)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05017566308379173, max_val=0.048285238444805145)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1105], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.773366928100586)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.047921475023031235, max_val=0.04970220476388931)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1715], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.20024299621582)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0136], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04635632410645485, max_val=0.04853583499789238)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1194], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.835951328277588)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0100], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03411868214607239, max_val=0.03576965257525444)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1282763034105301, max_val=0.12770599126815796)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4602], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.22143268585205)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0097], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.033468395471572876, max_val=0.034480076283216476)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9455], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.618553638458252)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0095], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.032992105931043625, max_val=0.03372432291507721)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8747], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.122868537902832)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0093], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.032219987362623215, max_val=0.032883644104003906)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9656], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.75909948348999)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.024047156795859337, max_val=0.02561904676258564)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09001927077770233, max_val=0.08976344764232635)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3912], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.738668441772461)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023344427347183228, max_val=0.025507662445306778)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9552], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.686702251434326)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0068], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02312484197318554, max_val=0.024613497778773308)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9058], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.340754508972168)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.022616729140281677, max_val=0.023398132994771004)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9275], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.492185592651367)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.016345391049981117, max_val=0.016989409923553467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0182], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06358448415994644, max_val=0.06381405889987946)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4612], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.228196144104004)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.01640930213034153, max_val=0.017003139480948448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9572], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.70023250579834)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.016315123066306114, max_val=0.016274766996502876)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9376], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.563254356384277)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:07:10,960]: 
Model Weights:
[2025-05-10 06:07:10,960]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-10 06:07:10,960]: Sample Values (25 elements): [-0.055307965725660324, -0.11716042459011078, -0.1548079401254654, 0.07029550522565842, 0.07854829728603363, 0.17775234580039978, -0.10568062216043472, -0.12409164756536484, 0.19081223011016846, 0.1807066798210144, -0.10900717228651047, 0.0039764391258358955, 0.061616361141204834, 0.006132602691650391, 0.0858832374215126, 0.17207640409469604, -0.06829756498336792, 0.16345126926898956, 0.11658759415149689, -0.026960117742419243, 0.14947625994682312, -0.022644437849521637, 0.14065001904964447, 0.018417688086628914, -0.17656484246253967]
[2025-05-10 06:07:10,960]: Mean: -0.00078691
[2025-05-10 06:07:10,960]: Min: -0.20408684
[2025-05-10 06:07:10,961]: Max: 0.21005282
[2025-05-10 06:07:10,961]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-10 06:07:10,961]: Sample Values (25 elements): [1.0073063373565674, 1.000543236732483, 0.9970152974128723, 1.00736403465271, 0.9981235265731812, 0.9992175102233887, 1.006280779838562, 1.0006526708602905, 1.0038182735443115, 1.0027492046356201, 0.9979155659675598, 1.000304102897644, 1.005006194114685, 0.9983897805213928, 1.008933424949646, 0.9952829480171204, 1.004428744316101, 0.9988604784011841, 1.0101839303970337, 1.0063989162445068, 1.0034517049789429, 0.9975201487541199, 0.9973517656326294, 1.0052121877670288, 0.9994941353797913]
[2025-05-10 06:07:10,961]: Mean: 1.00160933
[2025-05-10 06:07:10,961]: Min: 0.99528295
[2025-05-10 06:07:10,961]: Max: 1.01120901
[2025-05-10 06:07:10,962]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:07:10,963]: Sample Values (25 elements): [-0.04467480629682541, 0.0, -0.014891602098941803, 0.0, 0.04467480629682541, 0.014891602098941803, 0.0, -0.014891602098941803, -0.029783204197883606, 0.029783204197883606, 0.029783204197883606, -0.029783204197883606, -0.014891602098941803, -0.029783204197883606, 0.0, 0.0, 0.014891602098941803, -0.014891602098941803, 0.014891602098941803, 0.029783204197883606, -0.04467480629682541, 0.029783204197883606, -0.014891602098941803, 0.0, 0.0]
[2025-05-10 06:07:10,963]: Mean: -0.00029085
[2025-05-10 06:07:10,963]: Min: -0.04467481
[2025-05-10 06:07:10,963]: Max: 0.05956641
[2025-05-10 06:07:10,963]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:07:10,964]: Sample Values (25 elements): [1.0009632110595703, 0.9970812797546387, 1.0001736879348755, 0.9997164011001587, 1.0000008344650269, 1.0006741285324097, 1.0012848377227783, 0.9962674379348755, 0.9973572492599487, 0.9965999126434326, 0.9991891980171204, 0.9987161159515381, 0.9987692832946777, 1.0003820657730103, 0.9997678399085999, 0.9982078075408936, 1.0000969171524048, 1.001137137413025, 1.0047330856323242, 1.0010652542114258, 0.9998181462287903, 1.0002201795578003, 1.0004225969314575, 0.9987169504165649, 1.0003772974014282]
[2025-05-10 06:07:10,964]: Mean: 0.99998575
[2025-05-10 06:07:10,964]: Min: 0.99626744
[2025-05-10 06:07:10,964]: Max: 1.00917959
[2025-05-10 06:07:10,965]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:07:10,966]: Sample Values (25 elements): [0.014129005372524261, 0.042387016117572784, 0.0, 0.028258010745048523, -0.042387016117572784, 0.0, 0.014129005372524261, 0.028258010745048523, 0.014129005372524261, 0.028258010745048523, 0.0, 0.0, -0.042387016117572784, -0.014129005372524261, -0.028258010745048523, 0.014129005372524261, 0.028258010745048523, -0.014129005372524261, -0.014129005372524261, 0.014129005372524261, 0.0, 0.014129005372524261, 0.0, 0.028258010745048523, 0.0]
[2025-05-10 06:07:10,966]: Mean: -0.00036526
[2025-05-10 06:07:10,966]: Min: -0.04238702
[2025-05-10 06:07:10,966]: Max: 0.05651602
[2025-05-10 06:07:10,966]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:07:10,966]: Sample Values (25 elements): [1.0024337768554688, 1.0028140544891357, 1.0005292892456055, 1.0026196241378784, 0.9988497495651245, 0.9983409643173218, 1.0018854141235352, 1.000414252281189, 0.9992446303367615, 0.9981252551078796, 1.0008089542388916, 1.001833438873291, 0.9981538653373718, 1.002998948097229, 0.9985714554786682, 0.9974480271339417, 0.9992954134941101, 0.9985166788101196, 1.0004866123199463, 1.0011595487594604, 1.0006033182144165, 1.0004984140396118, 0.9992023706436157, 1.0013387203216553, 1.0027295351028442]
[2025-05-10 06:07:10,967]: Mean: 1.00054908
[2025-05-10 06:07:10,967]: Min: 0.99591881
[2025-05-10 06:07:10,967]: Max: 1.00840580
[2025-05-10 06:07:10,968]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:07:10,968]: Sample Values (25 elements): [-0.014067184180021286, 0.0, -0.028134368360042572, 0.04220155254006386, 0.014067184180021286, 0.014067184180021286, 0.04220155254006386, -0.028134368360042572, -0.014067184180021286, 0.0, 0.014067184180021286, -0.04220155254006386, 0.014067184180021286, 0.014067184180021286, 0.0, 0.04220155254006386, -0.028134368360042572, -0.028134368360042572, 0.04220155254006386, -0.04220155254006386, 0.04220155254006386, -0.04220155254006386, 0.014067184180021286, -0.04220155254006386, 0.0]
[2025-05-10 06:07:10,968]: Mean: -0.00013051
[2025-05-10 06:07:10,969]: Min: -0.05626874
[2025-05-10 06:07:10,969]: Max: 0.04220155
[2025-05-10 06:07:10,969]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:07:10,969]: Sample Values (25 elements): [0.9968850612640381, 0.9999454617500305, 0.997882604598999, 1.0017101764678955, 0.9967560768127441, 0.9960469007492065, 1.0003842115402222, 0.9980108141899109, 0.9968895316123962, 0.9994879961013794, 0.9991347789764404, 0.9999294877052307, 1.0008169412612915, 0.9988810420036316, 1.0044609308242798, 1.000863790512085, 1.0014005899429321, 0.9995322823524475, 0.9984999299049377, 1.002628207206726, 0.9986964464187622, 0.998820424079895, 0.9988693594932556, 1.0020521879196167, 0.9997240304946899]
[2025-05-10 06:07:10,969]: Mean: 0.99965519
[2025-05-10 06:07:10,969]: Min: 0.99604690
[2025-05-10 06:07:10,969]: Max: 1.00446093
[2025-05-10 06:07:10,970]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:07:10,971]: Sample Values (25 elements): [-0.04184219613671303, 0.0, 0.013947398401796818, 0.013947398401796818, 0.027894796803593636, 0.013947398401796818, -0.027894796803593636, 0.013947398401796818, 0.04184219613671303, 0.013947398401796818, 0.013947398401796818, -0.027894796803593636, 0.027894796803593636, -0.027894796803593636, -0.027894796803593636, -0.027894796803593636, 0.013947398401796818, -0.027894796803593636, -0.013947398401796818, 0.0, -0.013947398401796818, 0.04184219613671303, 0.013947398401796818, 0.013947398401796818, 0.027894796803593636]
[2025-05-10 06:07:10,971]: Mean: -0.00003065
[2025-05-10 06:07:10,971]: Min: -0.04184220
[2025-05-10 06:07:10,971]: Max: 0.05578959
[2025-05-10 06:07:10,971]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:07:10,972]: Sample Values (25 elements): [1.0016114711761475, 1.0002169609069824, 1.0033457279205322, 0.9988152384757996, 1.000415325164795, 1.0012413263320923, 1.0023781061172485, 1.000227689743042, 0.9986234903335571, 1.0003405809402466, 0.9993279576301575, 1.0008424520492554, 1.0002323389053345, 1.0010364055633545, 0.9988901615142822, 0.9999728202819824, 1.0018253326416016, 1.000450611114502, 1.0009857416152954, 0.9997985363006592, 0.9979714155197144, 1.0023611783981323, 1.0006862878799438, 1.0017050504684448, 1.002026081085205]
[2025-05-10 06:07:10,972]: Mean: 1.00055659
[2025-05-10 06:07:10,972]: Min: 0.99797142
[2025-05-10 06:07:10,972]: Max: 1.00419569
[2025-05-10 06:07:10,973]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-10 06:07:10,974]: Sample Values (25 elements): [0.02711564488708973, 0.0, -0.013557822443544865, 0.02711564488708973, 0.0, -0.02711564488708973, 0.013557822443544865, -0.04067346826195717, 0.013557822443544865, -0.04067346826195717, -0.04067346826195717, 0.02711564488708973, 0.0, -0.013557822443544865, 0.013557822443544865, -0.013557822443544865, -0.013557822443544865, 0.04067346826195717, 0.0, 0.02711564488708973, 0.013557822443544865, 0.013557822443544865, -0.02711564488708973, -0.04067346826195717, 0.02711564488708973]
[2025-05-10 06:07:10,974]: Mean: -0.00006363
[2025-05-10 06:07:10,974]: Min: -0.04067347
[2025-05-10 06:07:10,974]: Max: 0.05423129
[2025-05-10 06:07:10,974]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:07:10,975]: Sample Values (25 elements): [0.9989244341850281, 0.9987848401069641, 1.0004136562347412, 1.0004990100860596, 0.9985513687133789, 0.9994099736213684, 0.9998278021812439, 1.000298261642456, 0.9996117949485779, 0.99910968542099, 0.9982892274856567, 0.99808269739151, 0.9986386299133301, 1.000505805015564, 0.9992406368255615, 0.998511016368866, 1.000074028968811, 0.9983069896697998, 0.999833881855011, 0.9998830556869507, 0.9977845549583435, 1.0004478693008423, 1.0011837482452393, 0.9990246891975403, 0.9998155832290649]
[2025-05-10 06:07:10,975]: Mean: 0.99948907
[2025-05-10 06:07:10,975]: Min: 0.99751675
[2025-05-10 06:07:10,975]: Max: 1.00265634
[2025-05-10 06:07:10,976]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:07:10,978]: Sample Values (25 elements): [0.019969850778579712, 0.0, 0.029954776167869568, 0.009984925389289856, 0.019969850778579712, 0.019969850778579712, 0.0, 0.009984925389289856, 0.029954776167869568, -0.009984925389289856, -0.029954776167869568, -0.019969850778579712, -0.009984925389289856, -0.019969850778579712, -0.019969850778579712, -0.019969850778579712, 0.029954776167869568, 0.009984925389289856, -0.009984925389289856, 0.0, 0.0, -0.009984925389289856, -0.019969850778579712, -0.019969850778579712, -0.019969850778579712]
[2025-05-10 06:07:10,978]: Mean: -0.00012595
[2025-05-10 06:07:10,978]: Min: -0.02995478
[2025-05-10 06:07:10,978]: Max: 0.03993970
[2025-05-10 06:07:10,978]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:07:10,979]: Sample Values (25 elements): [0.9995734095573425, 1.0001201629638672, 0.9979783296585083, 0.9977152943611145, 0.9997369050979614, 0.9994534850120544, 0.9984309077262878, 0.999633252620697, 1.0010663270950317, 1.0017011165618896, 0.9982731938362122, 0.9986816048622131, 0.9993054270744324, 1.0007449388504028, 1.0013893842697144, 1.003401517868042, 0.9975584149360657, 0.9995288848876953, 1.0017420053482056, 0.9999058246612549, 0.998579204082489, 0.9992712736129761, 0.9999575614929199, 0.9995214939117432, 1.0013229846954346]
[2025-05-10 06:07:10,979]: Mean: 0.99959004
[2025-05-10 06:07:10,979]: Min: 0.99719089
[2025-05-10 06:07:10,979]: Max: 1.00396478
[2025-05-10 06:07:10,980]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-10 06:07:10,981]: Sample Values (25 elements): [0.10970854759216309, 0.10970854759216309, -0.10970854759216309, -0.036569517105817795, 0.0, 0.036569517105817795, 0.0, 0.0, 0.036569517105817795, 0.10970854759216309, -0.10970854759216309, -0.036569517105817795, -0.036569517105817795, 0.0, 0.036569517105817795, 0.07313903421163559, 0.10970854759216309, 0.10970854759216309, 0.036569517105817795, -0.07313903421163559, 0.0, 0.07313903421163559, -0.10970854759216309, -0.036569517105817795, 0.10970854759216309]
[2025-05-10 06:07:10,981]: Mean: -0.00068746
[2025-05-10 06:07:10,981]: Min: -0.14627807
[2025-05-10 06:07:10,981]: Max: 0.10970855
[2025-05-10 06:07:10,981]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-10 06:07:10,981]: Sample Values (25 elements): [0.9982707500457764, 0.9997843503952026, 0.999398410320282, 1.0002269744873047, 0.9991576075553894, 0.9997825026512146, 0.9985879063606262, 0.9997806549072266, 1.000192642211914, 0.9989694356918335, 1.0018298625946045, 0.9985058903694153, 0.9994901418685913, 0.9997673034667969, 1.000088095664978, 0.9986712336540222, 1.001201868057251, 0.9976900815963745, 0.9985074996948242, 0.9995089173316956, 1.0007202625274658, 0.9976568222045898, 0.9993410706520081, 0.9991291761398315, 0.9985628128051758]
[2025-05-10 06:07:10,982]: Mean: 0.99928939
[2025-05-10 06:07:10,982]: Min: 0.99639982
[2025-05-10 06:07:10,982]: Max: 1.00182986
[2025-05-10 06:07:10,983]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:07:10,984]: Sample Values (25 elements): [-0.019413970410823822, -0.009706985205411911, -0.009706985205411911, 0.0, -0.019413970410823822, 0.0, 0.019413970410823822, 0.019413970410823822, -0.009706985205411911, -0.029120955616235733, -0.009706985205411911, 0.0, 0.009706985205411911, 0.019413970410823822, -0.009706985205411911, 0.009706985205411911, -0.009706985205411911, -0.009706985205411911, 0.0, 0.0, 0.0, 0.029120955616235733, -0.009706985205411911, -0.019413970410823822, -0.029120955616235733]
[2025-05-10 06:07:10,984]: Mean: -0.00001119
[2025-05-10 06:07:10,984]: Min: -0.02912096
[2025-05-10 06:07:10,985]: Max: 0.03882794
[2025-05-10 06:07:10,985]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:07:10,985]: Sample Values (25 elements): [0.9992930889129639, 0.9987871646881104, 0.9999549984931946, 0.9994454979896545, 0.9982426762580872, 0.9990786910057068, 0.9989912509918213, 0.9990372657775879, 0.997904360294342, 1.0011069774627686, 1.000216007232666, 0.9989349246025085, 1.0002779960632324, 0.9998981356620789, 1.0027999877929688, 0.9990350008010864, 0.9991098046302795, 1.0004862546920776, 0.9997814893722534, 0.9999408721923828, 0.9979739189147949, 0.9996111392974854, 1.0000966787338257, 0.9985529780387878, 0.9983521103858948]
[2025-05-10 06:07:10,985]: Mean: 0.99938607
[2025-05-10 06:07:10,985]: Min: 0.99758536
[2025-05-10 06:07:10,985]: Max: 1.00339377
[2025-05-10 06:07:10,986]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:07:10,988]: Sample Values (25 elements): [0.0, 0.009531754069030285, -0.01906350813806057, 0.01906350813806057, -0.01906350813806057, 0.02859526127576828, -0.01906350813806057, -0.009531754069030285, -0.02859526127576828, 0.009531754069030285, -0.009531754069030285, 0.009531754069030285, 0.0, -0.01906350813806057, 0.01906350813806057, 0.0, -0.02859526127576828, 0.009531754069030285, -0.009531754069030285, 0.02859526127576828, 0.02859526127576828, -0.01906350813806057, 0.01906350813806057, -0.02859526127576828, 0.01906350813806057]
[2025-05-10 06:07:10,988]: Mean: -0.00002831
[2025-05-10 06:07:10,988]: Min: -0.02859526
[2025-05-10 06:07:10,988]: Max: 0.03812702
[2025-05-10 06:07:10,988]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:07:10,988]: Sample Values (25 elements): [1.0005671977996826, 0.9987828731536865, 0.9998891353607178, 0.999524712562561, 1.0009286403656006, 0.9991916418075562, 0.9995619654655457, 0.9998478889465332, 0.9982842803001404, 0.9994305372238159, 0.9994223117828369, 0.9992491602897644, 0.9981147646903992, 0.9996099472045898, 1.0001068115234375, 0.9988804459571838, 1.0008033514022827, 1.000264286994934, 0.9988827109336853, 0.9988086819648743, 0.9989767670631409, 0.9994283318519592, 0.9993187785148621, 1.0001003742218018, 0.9994847178459167]
[2025-05-10 06:07:10,989]: Mean: 0.99960476
[2025-05-10 06:07:10,989]: Min: 0.99724931
[2025-05-10 06:07:10,989]: Max: 1.00126672
[2025-05-10 06:07:10,990]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-10 06:07:10,992]: Sample Values (25 elements): [-0.009301390498876572, 0.027904171496629715, 0.009301390498876572, 0.027904171496629715, 0.027904171496629715, -0.027904171496629715, 0.009301390498876572, -0.009301390498876572, -0.018602780997753143, 0.009301390498876572, 0.0, 0.0, 0.018602780997753143, -0.018602780997753143, 0.009301390498876572, -0.009301390498876572, 0.0, -0.018602780997753143, 0.0, -0.009301390498876572, -0.018602780997753143, -0.018602780997753143, 0.018602780997753143, 0.009301390498876572, -0.018602780997753143]
[2025-05-10 06:07:10,993]: Mean: -0.00006216
[2025-05-10 06:07:10,993]: Min: -0.02790417
[2025-05-10 06:07:10,993]: Max: 0.03720556
[2025-05-10 06:07:10,993]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:07:10,993]: Sample Values (25 elements): [0.9992657899856567, 0.998393177986145, 0.9993304014205933, 0.999396800994873, 0.9994717836380005, 0.9992026090621948, 0.999040424823761, 0.9987466931343079, 0.99906325340271, 0.9985056519508362, 0.9994497895240784, 0.9989638924598694, 0.9988123774528503, 0.9989989995956421, 0.9992628693580627, 0.9989951252937317, 0.9993300437927246, 0.9993469715118408, 0.9987084269523621, 0.9987812042236328, 0.9992209076881409, 0.9993647336959839, 0.9984377026557922, 0.9991843700408936, 0.9993994235992432]
[2025-05-10 06:07:10,993]: Mean: 0.99932694
[2025-05-10 06:07:10,994]: Min: 0.99791580
[2025-05-10 06:07:10,994]: Max: 1.00195801
[2025-05-10 06:07:10,995]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:07:11,000]: Sample Values (25 elements): [-0.014192971400916576, 0.021289456635713577, -0.007096485700458288, -0.014192971400916576, -0.007096485700458288, 0.014192971400916576, 0.007096485700458288, -0.007096485700458288, -0.014192971400916576, -0.007096485700458288, -0.007096485700458288, 0.0, 0.0, 0.007096485700458288, 0.014192971400916576, 0.007096485700458288, -0.014192971400916576, 0.007096485700458288, 0.0, -0.007096485700458288, 0.007096485700458288, 0.014192971400916576, 0.0, 0.021289456635713577, -0.014192971400916576]
[2025-05-10 06:07:11,000]: Mean: -0.00002895
[2025-05-10 06:07:11,000]: Min: -0.02128946
[2025-05-10 06:07:11,001]: Max: 0.02838594
[2025-05-10 06:07:11,001]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:07:11,001]: Sample Values (25 elements): [0.9990115165710449, 1.000283122062683, 0.9990594983100891, 0.9989252090454102, 0.9998264312744141, 0.9994609355926514, 0.9996861815452576, 0.9992727637290955, 0.9988178610801697, 0.9990023374557495, 0.9994765520095825, 0.9992858171463013, 0.9990488886833191, 0.999030590057373, 0.9988554120063782, 0.999655544757843, 1.0008819103240967, 0.9988460540771484, 0.9984155893325806, 1.0032742023468018, 0.998918354511261, 0.9989578127861023, 1.0000159740447998, 1.0001463890075684, 0.9998868107795715]
[2025-05-10 06:07:11,001]: Mean: 0.99946833
[2025-05-10 06:07:11,001]: Min: 0.99785548
[2025-05-10 06:07:11,002]: Max: 1.00327420
[2025-05-10 06:07:11,003]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-10 06:07:11,004]: Sample Values (25 elements): [-0.05136822536587715, -0.025684112682938576, 0.05136822536587715, 0.0, 0.0, 0.07705233991146088, 0.07705233991146088, -0.07705233991146088, 0.07705233991146088, -0.025684112682938576, 0.05136822536587715, -0.025684112682938576, -0.07705233991146088, -0.025684112682938576, -0.05136822536587715, -0.05136822536587715, -0.07705233991146088, 0.025684112682938576, 0.07705233991146088, 0.05136822536587715, -0.025684112682938576, 0.05136822536587715, 0.0, 0.07705233991146088, 0.025684112682938576]
[2025-05-10 06:07:11,004]: Mean: 0.00022809
[2025-05-10 06:07:11,004]: Min: -0.10273645
[2025-05-10 06:07:11,004]: Max: 0.07705234
[2025-05-10 06:07:11,004]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-10 06:07:11,004]: Sample Values (25 elements): [0.999232828617096, 0.9984737634658813, 0.9988775849342346, 0.99965900182724, 0.999668300151825, 0.9986456036567688, 0.9991668462753296, 0.9992797374725342, 0.9998883008956909, 0.9995168447494507, 0.9988592863082886, 0.999849259853363, 0.9994951486587524, 0.9980787038803101, 0.9993091821670532, 0.9997411966323853, 0.9993284344673157, 0.9989300966262817, 0.9984351396560669, 0.9982729554176331, 1.0000272989273071, 0.9995066523551941, 0.9987319111824036, 0.9998815655708313, 0.9998331665992737]
[2025-05-10 06:07:11,005]: Mean: 0.99909151
[2025-05-10 06:07:11,005]: Min: 0.99803007
[2025-05-10 06:07:11,005]: Max: 1.00104702
[2025-05-10 06:07:11,006]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:07:11,014]: Sample Values (25 elements): [0.013958794996142387, -0.020938191562891006, 0.0, 0.013958794996142387, 0.0, 0.020938191562891006, 0.013958794996142387, -0.020938191562891006, -0.006979397498071194, 0.013958794996142387, 0.006979397498071194, 0.006979397498071194, -0.013958794996142387, -0.013958794996142387, -0.013958794996142387, 0.006979397498071194, -0.020938191562891006, -0.013958794996142387, 0.013958794996142387, 0.0, 0.0, 0.020938191562891006, 0.0, -0.013958794996142387, 0.020938191562891006]
[2025-05-10 06:07:11,014]: Mean: -0.00004084
[2025-05-10 06:07:11,014]: Min: -0.02093819
[2025-05-10 06:07:11,014]: Max: 0.02791759
[2025-05-10 06:07:11,014]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:07:11,015]: Sample Values (25 elements): [0.999221920967102, 0.9997389316558838, 0.9997695684432983, 0.9997687339782715, 0.9995297193527222, 0.9998773336410522, 0.9991604089736938, 0.9990763068199158, 0.999467670917511, 0.9991056323051453, 0.9988354444503784, 0.9987573623657227, 0.9999322891235352, 0.9984104037284851, 0.9996573328971863, 0.9993445873260498, 0.9995575547218323, 1.0001884698867798, 1.0002769231796265, 0.998759388923645, 0.9991533756256104, 0.9995363354682922, 0.9991023540496826, 0.9989259839057922, 0.998370349407196]
[2025-05-10 06:07:11,015]: Mean: 0.99930078
[2025-05-10 06:07:11,015]: Min: 0.99810714
[2025-05-10 06:07:11,015]: Max: 1.00082409
[2025-05-10 06:07:11,017]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:07:11,024]: Sample Values (25 elements): [0.013640078715980053, 0.020460117608308792, 0.013640078715980053, 0.013640078715980053, -0.0068200393579900265, -0.013640078715980053, 0.0, 0.0, 0.0068200393579900265, -0.020460117608308792, -0.0068200393579900265, 0.0, 0.020460117608308792, 0.0, 0.020460117608308792, 0.0, 0.013640078715980053, 0.020460117608308792, 0.0068200393579900265, 0.0, 0.013640078715980053, 0.013640078715980053, 0.020460117608308792, 0.013640078715980053, 0.0]
[2025-05-10 06:07:11,025]: Mean: -0.00002997
[2025-05-10 06:07:11,025]: Min: -0.02046012
[2025-05-10 06:07:11,025]: Max: 0.02728016
[2025-05-10 06:07:11,025]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:07:11,025]: Sample Values (25 elements): [0.998913049697876, 0.9995726346969604, 1.0001128911972046, 0.9995028972625732, 0.9989240765571594, 0.998902440071106, 0.9990612268447876, 0.9996196031570435, 0.9992488622665405, 1.0000640153884888, 0.9990596175193787, 1.0001773834228516, 0.9991475939750671, 0.9992492198944092, 0.998725950717926, 0.9998975992202759, 0.9989464282989502, 0.9990198612213135, 0.9996026754379272, 0.9994360208511353, 0.9995608925819397, 0.999470055103302, 1.0004088878631592, 0.9995826482772827, 0.999322772026062]
[2025-05-10 06:07:11,026]: Mean: 0.99946940
[2025-05-10 06:07:11,026]: Min: 0.99829745
[2025-05-10 06:07:11,026]: Max: 1.00168896
[2025-05-10 06:07:11,027]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-10 06:07:11,041]: Sample Values (25 elements): [0.0, 0.01972273364663124, 0.01972273364663124, -0.013148488476872444, 0.006574244238436222, 0.0, 0.01972273364663124, 0.013148488476872444, 0.01972273364663124, -0.013148488476872444, -0.013148488476872444, 0.013148488476872444, 0.01972273364663124, 0.013148488476872444, -0.006574244238436222, -0.006574244238436222, 0.0, -0.006574244238436222, 0.0, 0.013148488476872444, 0.013148488476872444, 0.0, 0.006574244238436222, -0.006574244238436222, 0.013148488476872444]
[2025-05-10 06:07:11,041]: Mean: -0.00000093
[2025-05-10 06:07:11,042]: Min: -0.01972273
[2025-05-10 06:07:11,042]: Max: 0.02629698
[2025-05-10 06:07:11,042]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:07:11,042]: Sample Values (25 elements): [0.9989016056060791, 0.9997528195381165, 0.9988504648208618, 0.9995014071464539, 0.9989603161811829, 0.9990493059158325, 0.9989390969276428, 0.9991255402565002, 0.9993117451667786, 0.9994871616363525, 0.9990214109420776, 0.9994903206825256, 0.9991198182106018, 0.9993999004364014, 0.9993078112602234, 0.9993519186973572, 0.999100387096405, 0.9994285106658936, 0.9996216297149658, 0.9992375373840332, 0.9994340538978577, 0.9994158148765564, 0.999238908290863, 0.9986955523490906, 0.9993482828140259]
[2025-05-10 06:07:11,042]: Mean: 0.99927020
[2025-05-10 06:07:11,042]: Min: 0.99833798
[2025-05-10 06:07:11,043]: Max: 1.00016963
[2025-05-10 06:07:11,044]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:07:11,082]: Sample Values (25 elements): [0.0, 0.0, 0.009524444118142128, -0.004762222059071064, 0.004762222059071064, 0.0, -0.004762222059071064, -0.004762222059071064, 0.009524444118142128, -0.004762222059071064, -0.014286666177213192, -0.004762222059071064, 0.009524444118142128, 0.004762222059071064, 0.0, -0.004762222059071064, 0.004762222059071064, -0.009524444118142128, 0.014286666177213192, -0.009524444118142128, 0.004762222059071064, -0.004762222059071064, 0.014286666177213192, 0.014286666177213192, -0.009524444118142128]
[2025-05-10 06:07:11,082]: Mean: -0.00000687
[2025-05-10 06:07:11,082]: Min: -0.01428667
[2025-05-10 06:07:11,082]: Max: 0.01904889
[2025-05-10 06:07:11,083]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:07:11,084]: Sample Values (25 elements): [0.9991729259490967, 0.9994370937347412, 0.999660313129425, 0.9990131855010986, 1.0000050067901611, 0.9993287920951843, 0.9992212653160095, 0.9991644620895386, 0.9997146725654602, 0.9995673894882202, 0.999115526676178, 0.9990901350975037, 0.9996836185455322, 0.9990291595458984, 0.99921053647995, 0.9989001750946045, 0.9995598793029785, 0.9995136857032776, 0.9991739392280579, 0.9995616674423218, 0.9998088479042053, 0.9997761845588684, 0.9992992877960205, 0.9997042417526245, 0.9997073411941528]
[2025-05-10 06:07:11,084]: Mean: 0.99951416
[2025-05-10 06:07:11,084]: Min: 0.99869591
[2025-05-10 06:07:11,084]: Max: 1.00147796
[2025-05-10 06:07:11,085]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-10 06:07:11,087]: Sample Values (25 elements): [0.01819988340139389, -0.01819988340139389, -0.01819988340139389, -0.03639976680278778, -0.01819988340139389, 0.0, -0.03639976680278778, -0.01819988340139389, -0.03639976680278778, 0.05459965020418167, 0.05459965020418167, -0.03639976680278778, 0.0, 0.03639976680278778, 0.05459965020418167, 0.0, -0.03639976680278778, 0.0, 0.03639976680278778, 0.03639976680278778, 0.0, 0.01819988340139389, 0.03639976680278778, -0.05459965020418167, 0.01819988340139389]
[2025-05-10 06:07:11,087]: Mean: 0.00015371
[2025-05-10 06:07:11,087]: Min: -0.05459965
[2025-05-10 06:07:11,087]: Max: 0.07279953
[2025-05-10 06:07:11,087]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-10 06:07:11,088]: Sample Values (25 elements): [0.9994144439697266, 0.9988957047462463, 0.9993066787719727, 0.9991384148597717, 0.9998443722724915, 0.9995648264884949, 0.9992928504943848, 0.9996057748794556, 0.9991732239723206, 0.999046266078949, 0.9988847374916077, 0.9997606873512268, 0.9988151788711548, 0.9994034767150879, 0.9987213015556335, 0.9991551637649536, 0.9994907379150391, 0.9991832971572876, 0.9993863701820374, 0.9994770884513855, 0.9993088245391846, 0.9998922944068909, 0.999452531337738, 0.9990766644477844, 0.9990603923797607]
[2025-05-10 06:07:11,088]: Mean: 0.99926752
[2025-05-10 06:07:11,088]: Min: 0.99846917
[2025-05-10 06:07:11,088]: Max: 1.00033343
[2025-05-10 06:07:11,089]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:07:11,135]: Sample Values (25 elements): [-0.004773726686835289, 0.009547453373670578, 0.0, 0.0, 0.009547453373670578, 0.004773726686835289, -0.009547453373670578, -0.004773726686835289, 0.009547453373670578, 0.004773726686835289, 0.014321180060505867, -0.009547453373670578, -0.014321180060505867, 0.0, 0.009547453373670578, 0.014321180060505867, 0.0, 0.0, -0.014321180060505867, 0.0, 0.009547453373670578, 0.009547453373670578, -0.009547453373670578, 0.0, 0.004773726686835289]
[2025-05-10 06:07:11,135]: Mean: -0.00000592
[2025-05-10 06:07:11,135]: Min: -0.01432118
[2025-05-10 06:07:11,136]: Max: 0.01909491
[2025-05-10 06:07:11,136]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:07:11,136]: Sample Values (25 elements): [0.9993801116943359, 0.9988561868667603, 0.9994487166404724, 0.9995020627975464, 0.998891294002533, 0.9991260766983032, 0.9993695020675659, 0.9992225766181946, 0.9989802241325378, 0.9994036555290222, 0.9987306594848633, 0.999737560749054, 0.9995877742767334, 0.9993735551834106, 0.9988948106765747, 0.9989448189735413, 0.9993938207626343, 0.9993684887886047, 0.9990514516830444, 0.9995341300964355, 0.9993072748184204, 0.9992246031761169, 0.9996941089630127, 0.999201774597168, 0.9989771842956543]
[2025-05-10 06:07:11,136]: Mean: 0.99926376
[2025-05-10 06:07:11,136]: Min: 0.99860150
[2025-05-10 06:07:11,136]: Max: 1.00058854
[2025-05-10 06:07:11,137]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:07:11,172]: Sample Values (25 elements): [0.004655842669308186, -0.004655842669308186, -0.013967528007924557, 0.004655842669308186, 0.013967528007924557, 0.009311685338616371, 0.004655842669308186, 0.0, -0.013967528007924557, -0.004655842669308186, 0.009311685338616371, 0.0, -0.013967528007924557, 0.0, 0.004655842669308186, 0.004655842669308186, 0.004655842669308186, 0.004655842669308186, 0.009311685338616371, 0.0, 0.0, -0.009311685338616371, -0.013967528007924557, -0.004655842669308186, -0.013967528007924557]
[2025-05-10 06:07:11,172]: Mean: -0.00000230
[2025-05-10 06:07:11,172]: Min: -0.01862337
[2025-05-10 06:07:11,173]: Max: 0.01396753
[2025-05-10 06:07:11,173]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:07:11,173]: Sample Values (25 elements): [0.9999266862869263, 0.9993054866790771, 0.9997040629386902, 0.9994947910308838, 0.9994861483573914, 0.9994168877601624, 0.9996002912521362, 0.9993838667869568, 0.9993817210197449, 0.9991918206214905, 0.9991426467895508, 0.9999940991401672, 0.9990819096565247, 0.9994372725486755, 0.9995784759521484, 0.9996903538703918, 0.9995003938674927, 0.9995112419128418, 0.9995236396789551, 0.9995384216308594, 1.00007164478302, 0.9994907379150391, 0.9989078640937805, 0.9996566772460938, 0.9995108842849731]
[2025-05-10 06:07:11,173]: Mean: 0.99962789
[2025-05-10 06:07:11,173]: Min: 0.99890786
[2025-05-10 06:07:11,174]: Max: 1.00073731
[2025-05-10 06:07:11,174]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-10 06:07:11,174]: Sample Values (25 elements): [-0.0127709424123168, 0.016620885580778122, -0.01056914683431387, -0.03241826593875885, 0.05615084990859032, 0.05654999986290932, -0.031833384186029434, 0.034056127071380615, -0.01739128679037094, -0.022153113037347794, 0.03628723695874214, -0.034768108278512955, 0.03325548395514488, 0.038905736058950424, 0.01911448873579502, -0.04767924174666405, 0.047972671687603, -0.02207084372639656, -0.02052772231400013, -0.033576223999261856, -0.04172014445066452, -0.014011776074767113, -0.00547085190191865, -0.01648044027388096, 0.018885735422372818]
[2025-05-10 06:07:11,174]: Mean: 0.00054592
[2025-05-10 06:07:11,174]: Min: -0.05778515
[2025-05-10 06:07:11,174]: Max: 0.06092221
[2025-05-10 06:07:11,174]: 


QAT of ResNet18 with relu down to 2 bits...
[2025-05-10 06:07:11,393]: [ResNet18_relu_quantized_2_bits] after configure_qat:
[2025-05-10 06:07:11,436]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:08:53,803]: [ResNet18_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.1533 Train Acc: 0.1977 Eval Loss: 2.0425 Eval Acc: 0.2421 (LR: 0.001000)
[2025-05-10 06:08:53,876]: [ResNet18_relu_quantized_2_bits] Best Eval Accuracy: 0.2421
[2025-05-10 06:08:53,953]: 


Quantization of model down to 2 bits finished
[2025-05-10 06:08:53,953]: Model Architecture:
[2025-05-10 06:08:54,000]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.9376], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.81289005279541)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0339], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04955301433801651, max_val=0.05220884457230568)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8492], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.54757022857666)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0330], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.048752814531326294, max_val=0.0503852516412735)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.0904], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.271307945251465)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0325], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04928174242377281, max_val=0.048346053808927536)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8498], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.549476623535156)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0317], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.047325026243925095, max_val=0.04780097305774689)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.3530], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.0590877532959)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0329], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04745883122086525, max_val=0.05119267851114273)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7572], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.271739959716797)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0229], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03378972038626671, max_val=0.03490563482046127)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0855], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12848757207393646, max_val=0.12786982953548431)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.0728], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.218493461608887)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0223], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03296540677547455, max_val=0.03379201143980026)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7284], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.185088157653809)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.032494254410266876, max_val=0.033163100481033325)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.4283], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.285032272338867)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.032071467489004135, max_val=0.03352190554141998)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7891], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.367390632629395)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0160], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02368052676320076, max_val=0.024319691583514214)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0602], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09051616489887238, max_val=0.08996029943227768)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.0499], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.149727821350098)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0161], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023119894787669182, max_val=0.025032758712768555)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.41109848022461)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0160], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.023144906386733055, max_val=0.02492447756230831)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.4746], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.423921585083008)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0155], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02274792641401291, max_val=0.023629646748304367)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7140], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.141961097717285)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0111], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.01641971990466118, max_val=0.016763076186180115)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0425], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06372293084859848, max_val=0.06391453742980957)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.9786], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=11.935726165771484)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.016521774232387543, max_val=0.01698814518749714)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5947], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.784238338470459)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.016960911452770233, max_val=0.01652451418340206)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.2648], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.794434547424316)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-10 06:08:54,000]: 
Model Weights:
[2025-05-10 06:08:54,001]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-10 06:08:54,001]: Sample Values (25 elements): [0.18416140973567963, -0.04975583404302597, -0.1283283233642578, 0.1565677672624588, 0.011318881064653397, -0.12902431190013885, -0.10787168890237808, 0.013542470522224903, -0.19701021909713745, -0.09891442209482193, 0.1740482747554779, 0.019441477954387665, 0.14944988489151, 0.10962183773517609, 0.11861840635538101, -0.03784506767988205, -0.018247194588184357, 0.07989528030157089, -0.18271245062351227, -0.14604026079177856, 0.19186334311962128, 0.14089404046535492, -0.0705021545290947, -0.08295456320047379, 0.10311052948236465]
[2025-05-10 06:08:54,001]: Mean: -0.00006754
[2025-05-10 06:08:54,001]: Min: -0.21773282
[2025-05-10 06:08:54,001]: Max: 0.23682967
[2025-05-10 06:08:54,001]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-10 06:08:54,002]: Sample Values (25 elements): [0.9976230263710022, 1.0050687789916992, 0.9970292448997498, 0.9946284294128418, 1.00529146194458, 1.0024020671844482, 0.999347984790802, 1.0113271474838257, 0.9993358254432678, 0.9964284300804138, 1.001902461051941, 0.9999247789382935, 1.0131008625030518, 1.0053855180740356, 1.0108089447021484, 1.0050346851348877, 0.9981098175048828, 0.9981018900871277, 0.9977363348007202, 1.012770652770996, 0.9971808195114136, 1.0092856884002686, 1.0335594415664673, 1.001300573348999, 1.0076764822006226]
[2025-05-10 06:08:54,002]: Mean: 1.00665402
[2025-05-10 06:08:54,002]: Min: 0.99462843
[2025-05-10 06:08:54,002]: Max: 1.07276368
[2025-05-10 06:08:54,003]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:08:54,004]: Sample Values (25 elements): [0.03393861651420593, 0.0, 0.03393861651420593, -0.03393861651420593, 0.0, 0.03393861651420593, -0.03393861651420593, 0.03393861651420593, 0.03393861651420593, 0.03393861651420593, -0.03393861651420593, 0.0, 0.0, -0.03393861651420593, -0.03393861651420593, -0.03393861651420593, 0.03393861651420593, 0.0, 0.03393861651420593, 0.0, 0.0, -0.03393861651420593, 0.0, 0.0, 0.03393861651420593]
[2025-05-10 06:08:54,004]: Mean: -0.00044559
[2025-05-10 06:08:54,004]: Min: -0.03393862
[2025-05-10 06:08:54,004]: Max: 0.06787723
[2025-05-10 06:08:54,004]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:08:54,004]: Sample Values (25 elements): [0.9974321126937866, 1.0013377666473389, 0.9973286986351013, 1.0011804103851318, 1.0002987384796143, 1.000326156616211, 0.9982948899269104, 1.0008573532104492, 0.9988082647323608, 0.9965916275978088, 0.9993434548377991, 0.9992941617965698, 0.9996256232261658, 1.0031899213790894, 1.00259268283844, 0.9992201328277588, 1.0012112855911255, 0.9983410835266113, 0.9987239241600037, 1.001137614250183, 0.9987810254096985, 0.9988569617271423, 1.0101617574691772, 0.9978811740875244, 1.001648187637329]
[2025-05-10 06:08:54,005]: Mean: 1.00066447
[2025-05-10 06:08:54,005]: Min: 0.99659163
[2025-05-10 06:08:54,005]: Max: 1.01070714
[2025-05-10 06:08:54,006]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:08:54,006]: Sample Values (25 elements): [-0.03305430710315704, 0.0, 0.0, 0.0, -0.03305430710315704, -0.03305430710315704, 0.03305430710315704, 0.0, -0.03305430710315704, 0.0, 0.0, -0.03305430710315704, 0.0, 0.03305430710315704, -0.03305430710315704, 0.03305430710315704, 0.0, 0.03305430710315704, -0.03305430710315704, 0.0, 0.03305430710315704, -0.03305430710315704, -0.03305430710315704, 0.03305430710315704, 0.0]
[2025-05-10 06:08:54,007]: Mean: -0.00031293
[2025-05-10 06:08:54,007]: Min: -0.03305431
[2025-05-10 06:08:54,007]: Max: 0.06610861
[2025-05-10 06:08:54,007]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:08:54,007]: Sample Values (25 elements): [0.9992193579673767, 0.9989748597145081, 1.0029834508895874, 1.0008184909820557, 0.9986036419868469, 1.0002952814102173, 1.001812219619751, 0.9980244636535645, 1.0031341314315796, 1.0023384094238281, 1.0070805549621582, 0.9995884299278259, 1.0000360012054443, 1.0009496212005615, 1.0079339742660522, 1.0008304119110107, 0.9968647956848145, 1.0002235174179077, 1.0036507844924927, 0.9986665844917297, 1.0002988576889038, 0.9974146485328674, 0.9979557991027832, 0.9988807439804077, 0.9988675713539124]
[2025-05-10 06:08:54,007]: Mean: 1.00030184
[2025-05-10 06:08:54,007]: Min: 0.99676138
[2025-05-10 06:08:54,008]: Max: 1.00793397
[2025-05-10 06:08:54,009]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:08:54,009]: Sample Values (25 elements): [0.032550640404224396, 0.0, 0.032550640404224396, 0.0, -0.032550640404224396, -0.032550640404224396, -0.032550640404224396, -0.032550640404224396, -0.032550640404224396, 0.032550640404224396, 0.0, -0.032550640404224396, 0.0, 0.0, 0.0, -0.032550640404224396, -0.032550640404224396, 0.032550640404224396, -0.032550640404224396, 0.032550640404224396, 0.032550640404224396, -0.032550640404224396, 0.0, 0.0, 0.0]
[2025-05-10 06:08:54,009]: Mean: -0.00018190
[2025-05-10 06:08:54,009]: Min: -0.06510128
[2025-05-10 06:08:54,010]: Max: 0.03255064
[2025-05-10 06:08:54,010]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-10 06:08:54,010]: Sample Values (25 elements): [1.0007576942443848, 1.0012078285217285, 1.0012731552124023, 0.9995058178901672, 0.9991494417190552, 0.9990116357803345, 1.0039923191070557, 0.997844934463501, 1.00008225440979, 1.001652479171753, 0.9973209500312805, 0.9980534911155701, 1.0028650760650635, 0.9993719458580017, 1.0015074014663696, 0.9990098476409912, 1.000052809715271, 1.0004916191101074, 0.9997221827507019, 0.998929500579834, 0.9982382655143738, 1.000918984413147, 1.0037838220596313, 0.9990293979644775, 1.0026369094848633]
[2025-05-10 06:08:54,010]: Mean: 1.00013399
[2025-05-10 06:08:54,010]: Min: 0.99686253
[2025-05-10 06:08:54,010]: Max: 1.00443006
[2025-05-10 06:08:54,011]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-10 06:08:54,012]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.03171440586447716, 0.03171440586447716, 0.03171440586447716, 0.0, -0.03171440586447716, -0.03171440586447716, 0.03171440586447716, -0.03171440586447716, -0.03171440586447716, 0.03171440586447716, 0.0, 0.03171440586447716, 0.03171440586447716, 0.0, 0.0, 0.03171440586447716, 0.03171440586447716, 0.03171440586447716, 0.03171440586447716, 0.03171440586447716, 0.03171440586447716]
[2025-05-10 06:08:54,012]: Mean: -0.00016948
[2025-05-10 06:08:54,012]: Min: -0.03171441
[2025-05-10 06:08:54,012]: Max: 0.06342881
[2025-05-10 06:08:54,012]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-10 06:08:54,013]: Sample Values (25 elements): [0.9998908042907715, 0.9984362125396729, 0.997529923915863, 1.0005879402160645, 1.0004866123199463, 0.99813312292099, 0.9995352625846863, 1.0001966953277588, 0.9988212585449219, 0.9994198083877563, 0.9984098672866821, 0.9988818168640137, 0.9989201426506042, 1.0059351921081543, 1.003623127937317, 1.0021721124649048, 1.000190258026123, 0.9994990825653076, 1.0006521940231323, 0.9997979402542114, 1.0008896589279175, 1.0018194913864136, 0.9994140267372131, 1.0003219842910767, 1.000004529953003]
[2025-05-10 06:08:54,013]: Mean: 1.00018132
[2025-05-10 06:08:54,013]: Min: 0.99752992
[2025-05-10 06:08:54,013]: Max: 1.00593519
[2025-05-10 06:08:54,014]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-10 06:08:54,015]: Sample Values (25 elements): [-0.03289999067783356, 0.03289999067783356, -0.03289999067783356, 0.03289999067783356, 0.0, 0.03289999067783356, 0.03289999067783356, 0.03289999067783356, 0.03289999067783356, 0.03289999067783356, -0.03289999067783356, 0.0, 0.0, 0.0, -0.03289999067783356, 0.0, -0.03289999067783356, 0.03289999067783356, 0.0, 0.0, 0.0, 0.0, -0.03289999067783356, -0.03289999067783356, 0.0]
[2025-05-10 06:08:54,015]: Mean: -0.00012182
[2025-05-10 06:08:54,015]: Min: -0.03289999
[2025-05-10 06:08:54,015]: Max: 0.06579998
[2025-05-10 06:08:54,015]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:08:54,016]: Sample Values (25 elements): [0.999158501625061, 1.000676155090332, 1.0016250610351562, 0.9980430603027344, 0.9993835687637329, 1.0000272989273071, 0.9990612268447876, 0.9979795813560486, 0.9987685680389404, 0.9989215135574341, 0.998526930809021, 0.9991702437400818, 1.0011889934539795, 0.9979875087738037, 0.999286949634552, 0.9980886578559875, 1.0012595653533936, 1.0001870393753052, 1.0000721216201782, 1.0012518167495728, 0.9991419315338135, 0.9989888668060303, 0.9993624687194824, 1.0013842582702637, 1.0013409852981567]
[2025-05-10 06:08:54,016]: Mean: 0.99986798
[2025-05-10 06:08:54,016]: Min: 0.99758142
[2025-05-10 06:08:54,016]: Max: 1.00830936
[2025-05-10 06:08:54,017]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:08:54,018]: Sample Values (25 elements): [0.0, -0.022903209552168846, 0.022903209552168846, -0.022903209552168846, 0.0, -0.022903209552168846, 0.0, -0.022903209552168846, 0.022903209552168846, -0.022903209552168846, 0.0, -0.022903209552168846, -0.022903209552168846, 0.022903209552168846, -0.022903209552168846, 0.022903209552168846, 0.022903209552168846, 0.0, -0.022903209552168846, 0.022903209552168846, 0.0, 0.0, -0.022903209552168846, 0.0, 0.022903209552168846]
[2025-05-10 06:08:54,018]: Mean: -0.00016604
[2025-05-10 06:08:54,019]: Min: -0.02290321
[2025-05-10 06:08:54,019]: Max: 0.04580642
[2025-05-10 06:08:54,019]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:08:54,019]: Sample Values (25 elements): [0.9985658526420593, 0.9981045126914978, 0.999585747718811, 0.9986029863357544, 1.0006593465805054, 0.9992950558662415, 0.9991384148597717, 0.9982115030288696, 0.99871826171875, 1.0008840560913086, 0.9987757802009583, 0.9984521865844727, 1.0003691911697388, 0.999798059463501, 0.9996848702430725, 1.0000110864639282, 0.9980648756027222, 1.0005253553390503, 1.003013014793396, 1.000646948814392, 0.9999666810035706, 1.0003278255462646, 1.0013225078582764, 1.0000423192977905, 0.9985964894294739]
[2025-05-10 06:08:54,019]: Mean: 0.99990082
[2025-05-10 06:08:54,019]: Min: 0.99745756
[2025-05-10 06:08:54,020]: Max: 1.00560927
[2025-05-10 06:08:54,020]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-10 06:08:54,021]: Sample Values (25 elements): [0.0, 0.08545567840337753, -0.08545567840337753, 0.0, 0.08545567840337753, -0.08545567840337753, -0.08545567840337753, 0.0, 0.08545567840337753, -0.08545567840337753, 0.0, 0.0, -0.08545567840337753, -0.08545567840337753, -0.08545567840337753, 0.08545567840337753, 0.08545567840337753, 0.08545567840337753, -0.08545567840337753, -0.08545567840337753, 0.08545567840337753, 0.0, -0.08545567840337753, 0.08545567840337753, 0.0]
[2025-05-10 06:08:54,021]: Mean: -0.00129352
[2025-05-10 06:08:54,021]: Min: -0.17091136
[2025-05-10 06:08:54,021]: Max: 0.08545568
[2025-05-10 06:08:54,021]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-10 06:08:54,022]: Sample Values (25 elements): [0.9995446801185608, 0.9989132285118103, 0.9996702671051025, 0.9990935325622559, 1.0004899501800537, 1.0005130767822266, 0.9997982382774353, 0.9983817934989929, 1.0002291202545166, 0.9999477863311768, 0.9984727501869202, 0.9993045926094055, 0.9994029402732849, 0.9995878338813782, 0.9973887205123901, 0.999536395072937, 0.9981910586357117, 0.9991637468338013, 0.9992452263832092, 1.0022460222244263, 0.9989396333694458, 0.9974800944328308, 0.9985809922218323, 1.000579595565796, 0.9985144138336182]
[2025-05-10 06:08:54,022]: Mean: 0.99962598
[2025-05-10 06:08:54,022]: Min: 0.99700814
[2025-05-10 06:08:54,022]: Max: 1.00268078
[2025-05-10 06:08:54,023]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:08:54,024]: Sample Values (25 elements): [0.022256314754486084, -0.022256314754486084, -0.022256314754486084, -0.022256314754486084, -0.022256314754486084, -0.022256314754486084, 0.022256314754486084, 0.022256314754486084, 0.022256314754486084, 0.0, 0.022256314754486084, 0.0, 0.0, 0.022256314754486084, -0.022256314754486084, -0.022256314754486084, -0.022256314754486084, 0.0, 0.0, 0.022256314754486084, 0.0, -0.022256314754486084, 0.0, -0.022256314754486084, 0.0]
[2025-05-10 06:08:54,025]: Mean: -0.00003170
[2025-05-10 06:08:54,025]: Min: -0.02225631
[2025-05-10 06:08:54,025]: Max: 0.04451263
[2025-05-10 06:08:54,025]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-10 06:08:54,025]: Sample Values (25 elements): [0.9990453124046326, 1.0014039278030396, 1.000065565109253, 0.9986223578453064, 1.0026280879974365, 0.9986536502838135, 0.9988133311271667, 0.9990091323852539, 0.998134434223175, 0.9990615248680115, 1.000142216682434, 0.99955815076828, 0.9993250966072083, 1.0001429319381714, 0.9998288154602051, 1.0005717277526855, 0.9997876286506653, 0.9986576437950134, 0.9994423985481262, 0.9991327524185181, 1.0024943351745605, 1.000652551651001, 1.0007997751235962, 0.9989113211631775, 1.000272512435913]
[2025-05-10 06:08:54,025]: Mean: 0.99955070
[2025-05-10 06:08:54,025]: Min: 0.99766392
[2025-05-10 06:08:54,026]: Max: 1.00593221
[2025-05-10 06:08:54,027]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-10 06:08:54,028]: Sample Values (25 elements): [0.021887673065066338, 0.021887673065066338, -0.021887673065066338, 0.021887673065066338, -0.021887673065066338, 0.021887673065066338, 0.021887673065066338, -0.021887673065066338, -0.021887673065066338, 0.021887673065066338, -0.021887673065066338, 0.0, -0.021887673065066338, 0.0, 0.021887673065066338, 0.0, 0.0, 0.0, 0.0, 0.021887673065066338, 0.0, -0.021887673065066338, 0.021887673065066338, 0.0, -0.021887673065066338]
[2025-05-10 06:08:54,028]: Mean: -0.00001989
[2025-05-10 06:08:54,028]: Min: -0.02188767
[2025-05-10 06:08:54,029]: Max: 0.04377535
[2025-05-10 06:08:54,029]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-10 06:08:54,029]: Sample Values (25 elements): [0.9987650513648987, 1.000092625617981, 0.9994729161262512, 0.9997972846031189, 0.9987353682518005, 0.9999220371246338, 0.9990424513816833, 0.9998278617858887, 0.9987154603004456, 0.9989467263221741, 1.0011143684387207, 1.000496506690979, 0.9997787475585938, 0.9986816048622131, 1.0018399953842163, 0.999738335609436, 0.9996161460876465, 0.9996276497840881, 0.9983581900596619, 0.9994719624519348, 1.000108003616333, 1.0007483959197998, 1.0011639595031738, 0.9985471963882446, 0.9997339844703674]
[2025-05-10 06:08:54,029]: Mean: 0.99967819
[2025-05-10 06:08:54,029]: Min: 0.99794102
[2025-05-10 06:08:54,029]: Max: 1.00213349
[2025-05-10 06:08:54,030]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-10 06:08:54,033]: Sample Values (25 elements): [-0.021869990974664688, -0.021869990974664688, -0.021869990974664688, -0.021869990974664688, 0.021869990974664688, -0.021869990974664688, 0.0, -0.021869990974664688, -0.021869990974664688, 0.021869990974664688, 0.0, -0.021869990974664688, -0.021869990974664688, -0.021869990974664688, 0.021869990974664688, -0.021869990974664688, -0.021869990974664688, 0.021869990974664688, 0.021869990974664688, 0.021869990974664688, 0.0, 0.0, -0.021869990974664688, 0.021869990974664688, -0.021869990974664688]
[2025-05-10 06:08:54,034]: Mean: -0.00003916
[2025-05-10 06:08:54,034]: Min: -0.02186999
[2025-05-10 06:08:54,034]: Max: 0.04373998
[2025-05-10 06:08:54,034]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:08:54,034]: Sample Values (25 elements): [0.9995489716529846, 0.9985383749008179, 0.9996672868728638, 0.9996986985206604, 1.000040888786316, 1.0001733303070068, 0.999830424785614, 1.0004009008407593, 0.9994411468505859, 0.9998699426651001, 0.9991763830184937, 0.9987248182296753, 0.9991754293441772, 0.9991747736930847, 0.9992089867591858, 0.9988651275634766, 0.9990429282188416, 0.9990442991256714, 1.0013666152954102, 0.9988003373146057, 0.9997822642326355, 1.0005396604537964, 1.0002106428146362, 0.9989044070243835, 0.9994580149650574]
[2025-05-10 06:08:54,034]: Mean: 0.99946356
[2025-05-10 06:08:54,035]: Min: 0.99817973
[2025-05-10 06:08:54,035]: Max: 1.00214434
[2025-05-10 06:08:54,036]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:08:54,042]: Sample Values (25 elements): [0.01600203663110733, -0.01600203663110733, 0.0, 0.0, -0.01600203663110733, 0.0, 0.01600203663110733, 0.0, 0.01600203663110733, 0.01600203663110733, 0.0, -0.01600203663110733, 0.01600203663110733, 0.01600203663110733, 0.0, 0.0, -0.01600203663110733, 0.01600203663110733, -0.01600203663110733, -0.01600203663110733, -0.01600203663110733, 0.0, 0.0, 0.01600203663110733, 0.0]
[2025-05-10 06:08:54,042]: Mean: -0.00004259
[2025-05-10 06:08:54,042]: Min: -0.01600204
[2025-05-10 06:08:54,042]: Max: 0.03200407
[2025-05-10 06:08:54,042]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:08:54,043]: Sample Values (25 elements): [0.9990181922912598, 0.9990338087081909, 1.000109314918518, 0.9996466040611267, 0.9985167384147644, 1.0005468130111694, 0.9992491602897644, 1.0010344982147217, 0.9994176030158997, 0.9986416101455688, 0.9993466138839722, 1.0001276731491089, 1.0010586977005005, 0.9992534518241882, 0.9989191293716431, 0.9995809197425842, 0.9987530708312988, 1.0004671812057495, 0.9997811317443848, 0.9980316162109375, 0.9990842342376709, 1.0006771087646484, 0.9995596408843994, 1.0002226829528809, 1.0006301403045654]
[2025-05-10 06:08:54,043]: Mean: 0.99962711
[2025-05-10 06:08:54,043]: Min: 0.99785388
[2025-05-10 06:08:54,043]: Max: 1.00346398
[2025-05-10 06:08:54,044]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-10 06:08:54,045]: Sample Values (25 elements): [0.060161080211400986, -0.060161080211400986, 0.0, -0.060161080211400986, -0.060161080211400986, 0.060161080211400986, 0.060161080211400986, 0.060161080211400986, -0.060161080211400986, 0.060161080211400986, -0.060161080211400986, -0.060161080211400986, -0.060161080211400986, -0.060161080211400986, 0.0, -0.060161080211400986, -0.060161080211400986, -0.060161080211400986, 0.0, 0.0, 0.0, -0.060161080211400986, -0.060161080211400986, -0.060161080211400986, 0.060161080211400986]
[2025-05-10 06:08:54,045]: Mean: 0.00011016
[2025-05-10 06:08:54,045]: Min: -0.12032216
[2025-05-10 06:08:54,045]: Max: 0.06016108
[2025-05-10 06:08:54,045]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-10 06:08:54,045]: Sample Values (25 elements): [1.001381516456604, 0.9991206526756287, 0.9994102716445923, 0.9992560744285583, 1.0007280111312866, 0.9993394017219543, 0.9996509552001953, 0.9992647171020508, 0.9993696212768555, 0.9995250701904297, 1.0002338886260986, 1.0002479553222656, 1.0001709461212158, 0.9987554550170898, 0.9987812042236328, 0.9988724589347839, 0.9987184405326843, 1.0004640817642212, 0.9999684691429138, 0.9986962080001831, 0.9990856647491455, 0.9989441633224487, 0.9994682669639587, 0.9995324611663818, 0.9993392825126648]
[2025-05-10 06:08:54,046]: Mean: 0.99940372
[2025-05-10 06:08:54,046]: Min: 0.99782509
[2025-05-10 06:08:54,046]: Max: 1.00207007
[2025-05-10 06:08:54,047]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:08:54,053]: Sample Values (25 elements): [0.016053151339292526, 0.0, 0.016053151339292526, -0.016053151339292526, -0.016053151339292526, 0.0, 0.016053151339292526, 0.0, 0.016053151339292526, 0.0, 0.016053151339292526, 0.0, 0.0, -0.016053151339292526, -0.016053151339292526, -0.016053151339292526, 0.0, -0.016053151339292526, -0.016053151339292526, 0.016053151339292526, -0.016053151339292526, 0.016053151339292526, -0.016053151339292526, 0.016053151339292526, 0.0]
[2025-05-10 06:08:54,054]: Mean: -0.00000659
[2025-05-10 06:08:54,054]: Min: -0.01605315
[2025-05-10 06:08:54,054]: Max: 0.03210630
[2025-05-10 06:08:54,054]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-10 06:08:54,054]: Sample Values (25 elements): [0.9997652173042297, 0.9990308880805969, 0.9991262555122375, 0.9989234209060669, 0.9990327954292297, 0.9989226460456848, 0.9988492727279663, 0.999955415725708, 1.0001074075698853, 1.0000033378601074, 0.9989252090454102, 0.9994555711746216, 0.9993436336517334, 0.9991781115531921, 0.9986180663108826, 0.9993804097175598, 0.9992552399635315, 0.999137282371521, 1.000626802444458, 0.998595654964447, 0.998572826385498, 0.9995192885398865, 0.9997820258140564, 0.9991297125816345, 0.9993295669555664]
[2025-05-10 06:08:54,054]: Mean: 0.99947196
[2025-05-10 06:08:54,055]: Min: 0.99804497
[2025-05-10 06:08:54,055]: Max: 1.00184000
[2025-05-10 06:08:54,056]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-10 06:08:54,061]: Sample Values (25 elements): [0.0, -0.016025949269533157, -0.016025949269533157, -0.016025949269533157, 0.016025949269533157, 0.0, 0.0, 0.0, -0.016025949269533157, 0.0, -0.016025949269533157, 0.0, 0.016025949269533157, -0.016025949269533157, 0.016025949269533157, -0.016025949269533157, -0.016025949269533157, -0.016025949269533157, -0.016025949269533157, 0.016025949269533157, 0.016025949269533157, 0.016025949269533157, -0.016025949269533157, 0.0, 0.016025949269533157]
[2025-05-10 06:08:54,062]: Mean: -0.00001426
[2025-05-10 06:08:54,062]: Min: -0.01602595
[2025-05-10 06:08:54,062]: Max: 0.03205190
[2025-05-10 06:08:54,062]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-10 06:08:54,062]: Sample Values (25 elements): [1.0002412796020508, 0.999067485332489, 0.9999337196350098, 0.9991812109947205, 0.9986476898193359, 0.999801754951477, 0.9992172122001648, 0.9991341829299927, 0.9990198612213135, 1.0006678104400635, 0.9993928670883179, 1.0010207891464233, 1.000247836112976, 0.9995153546333313, 0.9993172287940979, 0.9995973706245422, 1.0010687112808228, 0.9999666810035706, 0.998925507068634, 0.9998638033866882, 0.9995615482330322, 0.9995747208595276, 0.9994165897369385, 1.0006206035614014, 0.9996293783187866]
[2025-05-10 06:08:54,062]: Mean: 0.99980456
[2025-05-10 06:08:54,063]: Min: 0.99797714
[2025-05-10 06:08:54,063]: Max: 1.00362158
[2025-05-10 06:08:54,064]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-10 06:08:54,078]: Sample Values (25 elements): [0.0, -0.015461764298379421, 0.015461764298379421, 0.0, 0.015461764298379421, 0.015461764298379421, 0.0, 0.0, -0.015461764298379421, 0.015461764298379421, 0.0, -0.015461764298379421, 0.015461764298379421, 0.0, -0.015461764298379421, 0.0, 0.015461764298379421, 0.015461764298379421, -0.015461764298379421, 0.0, 0.015461764298379421, -0.015461764298379421, 0.015461764298379421, -0.015461764298379421, 0.015461764298379421]
[2025-05-10 06:08:54,079]: Mean: 0.00005412
[2025-05-10 06:08:54,079]: Min: -0.01546176
[2025-05-10 06:08:54,079]: Max: 0.03092353
[2025-05-10 06:08:54,079]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:08:54,079]: Sample Values (25 elements): [0.9989811182022095, 0.9998530745506287, 0.9992930293083191, 0.9989615082740784, 0.9997774958610535, 0.9989833235740662, 0.9985294938087463, 1.0001612901687622, 0.9992742538452148, 0.9992777705192566, 0.9991816282272339, 0.9990124106407166, 1.0009468793869019, 0.9996715784072876, 0.9989778399467468, 0.9990071058273315, 0.9993506073951721, 0.9990825057029724, 0.9998205900192261, 0.9993423819541931, 1.0000455379486084, 0.9994251132011414, 0.9995493292808533, 1.0007878541946411, 1.000044584274292]
[2025-05-10 06:08:54,080]: Mean: 0.99952716
[2025-05-10 06:08:54,080]: Min: 0.99843282
[2025-05-10 06:08:54,080]: Max: 1.00122559
[2025-05-10 06:08:54,081]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:08:54,123]: Sample Values (25 elements): [-0.01106236781924963, -0.01106236781924963, 0.01106236781924963, 0.01106236781924963, 0.01106236781924963, -0.01106236781924963, 0.01106236781924963, -0.01106236781924963, -0.01106236781924963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01106236781924963, 0.0, 0.0, 0.0, 0.01106236781924963, -0.01106236781924963, 0.0, 0.0, 0.0, -0.01106236781924963]
[2025-05-10 06:08:54,123]: Mean: -0.00015577
[2025-05-10 06:08:54,123]: Min: -0.01106237
[2025-05-10 06:08:54,123]: Max: 0.02212474
[2025-05-10 06:08:54,123]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:08:54,125]: Sample Values (25 elements): [1.001840353012085, 0.9994045495986938, 0.9998279809951782, 0.9996601939201355, 0.999151349067688, 1.000796914100647, 0.9999571442604065, 1.0054086446762085, 1.0052695274353027, 1.0037509202957153, 0.9992040395736694, 1.0038528442382812, 1.0013805627822876, 1.0027743577957153, 1.000239372253418, 1.0001405477523804, 0.9995197653770447, 1.0030568838119507, 0.9998403191566467, 1.0004839897155762, 1.000333309173584, 0.999616801738739, 0.9998971819877625, 0.999143123626709, 0.9988842010498047]
[2025-05-10 06:08:54,125]: Mean: 1.00028682
[2025-05-10 06:08:54,125]: Min: 0.99867004
[2025-05-10 06:08:54,125]: Max: 1.00788569
[2025-05-10 06:08:54,126]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-10 06:08:54,128]: Sample Values (25 elements): [-0.0425470769405365, 0.0, 0.0425470769405365, -0.0425470769405365, -0.0425470769405365, -0.0425470769405365, -0.0425470769405365, -0.0425470769405365, 0.0, -0.0425470769405365, -0.0425470769405365, -0.0425470769405365, 0.0425470769405365, -0.0425470769405365, -0.0425470769405365, -0.0425470769405365, 0.0, 0.0425470769405365, 0.0, -0.0425470769405365, 0.0425470769405365, -0.0425470769405365, 0.0425470769405365, 0.0, -0.0425470769405365]
[2025-05-10 06:08:54,128]: Mean: -0.00016230
[2025-05-10 06:08:54,128]: Min: -0.04254708
[2025-05-10 06:08:54,128]: Max: 0.08509415
[2025-05-10 06:08:54,128]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-10 06:08:54,128]: Sample Values (25 elements): [0.9997599720954895, 1.0006998777389526, 0.9990604519844055, 0.9990395307540894, 0.9991874694824219, 0.9994692206382751, 1.0007554292678833, 0.999022901058197, 0.9993743300437927, 0.9984392523765564, 0.9991961121559143, 0.9994451999664307, 0.9990517497062683, 0.9998629689216614, 0.9990682005882263, 1.0006873607635498, 0.9992806911468506, 0.9995083212852478, 0.9995872974395752, 0.9992596507072449, 0.9991607666015625, 0.99898362159729, 0.9998360872268677, 0.9989773631095886, 0.9996691346168518]
[2025-05-10 06:08:54,128]: Mean: 0.99952579
[2025-05-10 06:08:54,129]: Min: 0.99780124
[2025-05-10 06:08:54,129]: Max: 1.00327480
[2025-05-10 06:08:54,130]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:08:54,175]: Sample Values (25 elements): [-0.011171381920576096, 0.0, -0.011171381920576096, 0.0, 0.0, 0.0, -0.011171381920576096, 0.0, 0.0, 0.0, 0.011171381920576096, 0.011171381920576096, -0.011171381920576096, 0.0, 0.011171381920576096, 0.011171381920576096, 0.0, -0.011171381920576096, -0.011171381920576096, 0.0, 0.0, 0.011171381920576096, 0.0, 0.011171381920576096, 0.0]
[2025-05-10 06:08:54,176]: Mean: 0.00002173
[2025-05-10 06:08:54,176]: Min: -0.01117138
[2025-05-10 06:08:54,176]: Max: 0.02234276
[2025-05-10 06:08:54,176]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-10 06:08:54,176]: Sample Values (25 elements): [1.0009469985961914, 0.999864935874939, 0.9996092915534973, 0.9991286993026733, 1.0004892349243164, 1.000249981880188, 0.9993090629577637, 1.0002245903015137, 1.0011053085327148, 0.9992989301681519, 0.9993903040885925, 0.9988124370574951, 0.9999834895133972, 1.000137448310852, 1.0003252029418945, 0.9999876022338867, 0.9994418025016785, 0.9987330436706543, 0.9996798038482666, 1.0001758337020874, 0.999534547328949, 0.9999894499778748, 1.0001617670059204, 1.0002344846725464, 0.9999117851257324]
[2025-05-10 06:08:54,177]: Mean: 0.99970543
[2025-05-10 06:08:54,177]: Min: 0.99826020
[2025-05-10 06:08:54,177]: Max: 1.00242889
[2025-05-10 06:08:54,178]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-10 06:08:54,217]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.011163274757564068, 0.0, 0.0, 0.011163274757564068, -0.011163274757564068, 0.011163274757564068, -0.011163274757564068, -0.011163274757564068, 0.011163274757564068, 0.0, 0.011163274757564068, 0.0, 0.0, 0.0, 0.0, -0.011163274757564068, 0.0, 0.011163274757564068, -0.011163274757564068, 0.011163274757564068, -0.011163274757564068, 0.0]
[2025-05-10 06:08:54,217]: Mean: -0.00019507
[2025-05-10 06:08:54,217]: Min: -0.02232655
[2025-05-10 06:08:54,217]: Max: 0.01116327
[2025-05-10 06:08:54,217]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-10 06:08:54,218]: Sample Values (25 elements): [1.0024622678756714, 0.9994210004806519, 1.000399112701416, 0.9991762042045593, 1.0030527114868164, 1.000706672668457, 0.9995986223220825, 0.9996516704559326, 1.0005455017089844, 0.9995967745780945, 0.9994971752166748, 1.005094051361084, 1.0019268989562988, 0.9992926716804504, 0.999967634677887, 0.9999590516090393, 1.0013141632080078, 1.000406265258789, 1.000031590461731, 1.0057028532028198, 1.0053852796554565, 0.9995551109313965, 1.0027035474777222, 1.0031501054763794, 0.9991070032119751]
[2025-05-10 06:08:54,218]: Mean: 1.00065470
[2025-05-10 06:08:54,218]: Min: 0.99878162
[2025-05-10 06:08:54,218]: Max: 1.01112163
[2025-05-10 06:08:54,218]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-10 06:08:54,218]: Sample Values (25 elements): [-0.042286623269319534, 0.0387132354080677, 0.02848047763109207, 0.021404925733804703, 0.0003646580153144896, 0.00479620136320591, -0.017072034999728203, -0.02832455188035965, 0.0005999018321745098, 0.0022215056233108044, -0.03650779277086258, -0.010265583172440529, 0.00811379961669445, 0.010208790190517902, -0.023222891613841057, -0.021876076236367226, 0.024536646902561188, -0.019111856818199158, 0.030202427878975868, 0.02886119671165943, 0.02030017040669918, 0.005017437506467104, 0.03949838504195213, -0.041283536702394485, 0.00034860684536397457]
[2025-05-10 06:08:54,219]: Mean: 0.00054592
[2025-05-10 06:08:54,219]: Min: -0.06102353
[2025-05-10 06:08:54,219]: Max: 0.06343785
