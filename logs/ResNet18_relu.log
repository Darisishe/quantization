[2025-05-26 11:59:22,380]: 
Training ResNet18 with relu
[2025-05-26 12:00:43,619]: [ResNet18_relu] Epoch: 001 Train Loss: 1.5087 Train Acc: 0.4451 Eval Loss: 1.2843 Eval Acc: 0.5505 (LR: 0.00100000)
[2025-05-26 12:02:08,312]: [ResNet18_relu] Epoch: 002 Train Loss: 1.0735 Train Acc: 0.6155 Eval Loss: 1.2351 Eval Acc: 0.5821 (LR: 0.00100000)
[2025-05-26 12:03:32,378]: [ResNet18_relu] Epoch: 003 Train Loss: 0.8970 Train Acc: 0.6813 Eval Loss: 1.0484 Eval Acc: 0.6333 (LR: 0.00100000)
[2025-05-26 12:04:56,899]: [ResNet18_relu] Epoch: 004 Train Loss: 0.7747 Train Acc: 0.7288 Eval Loss: 0.7073 Eval Acc: 0.7571 (LR: 0.00100000)
[2025-05-26 12:06:20,729]: [ResNet18_relu] Epoch: 005 Train Loss: 0.6812 Train Acc: 0.7624 Eval Loss: 0.7122 Eval Acc: 0.7641 (LR: 0.00100000)
[2025-05-26 12:07:44,755]: [ResNet18_relu] Epoch: 006 Train Loss: 0.6123 Train Acc: 0.7864 Eval Loss: 0.6615 Eval Acc: 0.7768 (LR: 0.00100000)
[2025-05-26 12:09:11,283]: [ResNet18_relu] Epoch: 007 Train Loss: 0.5626 Train Acc: 0.8058 Eval Loss: 0.5528 Eval Acc: 0.8131 (LR: 0.00100000)
[2025-05-26 12:10:35,543]: [ResNet18_relu] Epoch: 008 Train Loss: 0.5207 Train Acc: 0.8194 Eval Loss: 0.6307 Eval Acc: 0.7983 (LR: 0.00100000)
[2025-05-26 12:11:59,493]: [ResNet18_relu] Epoch: 009 Train Loss: 0.4890 Train Acc: 0.8316 Eval Loss: 0.5891 Eval Acc: 0.8072 (LR: 0.00100000)
[2025-05-26 12:13:23,755]: [ResNet18_relu] Epoch: 010 Train Loss: 0.4537 Train Acc: 0.8431 Eval Loss: 0.5481 Eval Acc: 0.8191 (LR: 0.00100000)
[2025-05-26 12:14:47,594]: [ResNet18_relu] Epoch: 011 Train Loss: 0.4303 Train Acc: 0.8520 Eval Loss: 0.4596 Eval Acc: 0.8489 (LR: 0.00100000)
[2025-05-26 12:16:11,782]: [ResNet18_relu] Epoch: 012 Train Loss: 0.4079 Train Acc: 0.8584 Eval Loss: 0.4233 Eval Acc: 0.8600 (LR: 0.00100000)
[2025-05-26 12:17:36,561]: [ResNet18_relu] Epoch: 013 Train Loss: 0.3872 Train Acc: 0.8665 Eval Loss: 0.3699 Eval Acc: 0.8758 (LR: 0.00100000)
[2025-05-26 12:19:02,398]: [ResNet18_relu] Epoch: 014 Train Loss: 0.3698 Train Acc: 0.8713 Eval Loss: 0.3722 Eval Acc: 0.8744 (LR: 0.00100000)
[2025-05-26 12:20:28,708]: [ResNet18_relu] Epoch: 015 Train Loss: 0.3543 Train Acc: 0.8774 Eval Loss: 0.4524 Eval Acc: 0.8498 (LR: 0.00100000)
[2025-05-26 12:21:54,679]: [ResNet18_relu] Epoch: 016 Train Loss: 0.3437 Train Acc: 0.8808 Eval Loss: 0.4502 Eval Acc: 0.8599 (LR: 0.00100000)
[2025-05-26 12:23:20,237]: [ResNet18_relu] Epoch: 017 Train Loss: 0.3262 Train Acc: 0.8883 Eval Loss: 0.3925 Eval Acc: 0.8705 (LR: 0.00100000)
[2025-05-26 12:24:45,903]: [ResNet18_relu] Epoch: 018 Train Loss: 0.3167 Train Acc: 0.8907 Eval Loss: 0.3816 Eval Acc: 0.8787 (LR: 0.00100000)
[2025-05-26 12:26:11,125]: [ResNet18_relu] Epoch: 019 Train Loss: 0.3053 Train Acc: 0.8943 Eval Loss: 0.4458 Eval Acc: 0.8618 (LR: 0.00010000)
[2025-05-26 12:27:37,656]: [ResNet18_relu] Epoch: 020 Train Loss: 0.2112 Train Acc: 0.9284 Eval Loss: 0.2605 Eval Acc: 0.9175 (LR: 0.00010000)
[2025-05-26 12:29:04,077]: [ResNet18_relu] Epoch: 021 Train Loss: 0.1767 Train Acc: 0.9403 Eval Loss: 0.2561 Eval Acc: 0.9188 (LR: 0.00010000)
[2025-05-26 12:30:29,883]: [ResNet18_relu] Epoch: 022 Train Loss: 0.1624 Train Acc: 0.9441 Eval Loss: 0.2535 Eval Acc: 0.9225 (LR: 0.00010000)
[2025-05-26 12:31:52,507]: [ResNet18_relu] Epoch: 023 Train Loss: 0.1542 Train Acc: 0.9463 Eval Loss: 0.2536 Eval Acc: 0.9208 (LR: 0.00010000)
[2025-05-26 12:33:14,118]: [ResNet18_relu] Epoch: 024 Train Loss: 0.1481 Train Acc: 0.9480 Eval Loss: 0.2597 Eval Acc: 0.9213 (LR: 0.00010000)
[2025-05-26 12:34:35,412]: [ResNet18_relu] Epoch: 025 Train Loss: 0.1406 Train Acc: 0.9517 Eval Loss: 0.2516 Eval Acc: 0.9232 (LR: 0.00010000)
[2025-05-26 12:35:57,528]: [ResNet18_relu] Epoch: 026 Train Loss: 0.1300 Train Acc: 0.9542 Eval Loss: 0.2570 Eval Acc: 0.9231 (LR: 0.00010000)
[2025-05-26 12:37:20,814]: [ResNet18_relu] Epoch: 027 Train Loss: 0.1234 Train Acc: 0.9565 Eval Loss: 0.2581 Eval Acc: 0.9224 (LR: 0.00010000)
[2025-05-26 12:38:41,847]: [ResNet18_relu] Epoch: 028 Train Loss: 0.1208 Train Acc: 0.9580 Eval Loss: 0.2553 Eval Acc: 0.9238 (LR: 0.00010000)
[2025-05-26 12:40:02,401]: [ResNet18_relu] Epoch: 029 Train Loss: 0.1124 Train Acc: 0.9611 Eval Loss: 0.2664 Eval Acc: 0.9256 (LR: 0.00010000)
[2025-05-26 12:41:22,563]: [ResNet18_relu] Epoch: 030 Train Loss: 0.1084 Train Acc: 0.9627 Eval Loss: 0.2619 Eval Acc: 0.9266 (LR: 0.00010000)
[2025-05-26 12:42:42,970]: [ResNet18_relu] Epoch: 031 Train Loss: 0.1052 Train Acc: 0.9632 Eval Loss: 0.2557 Eval Acc: 0.9276 (LR: 0.00001000)
[2025-05-26 12:44:03,267]: [ResNet18_relu] Epoch: 032 Train Loss: 0.0914 Train Acc: 0.9685 Eval Loss: 0.2524 Eval Acc: 0.9270 (LR: 0.00001000)
[2025-05-26 12:45:23,588]: [ResNet18_relu] Epoch: 033 Train Loss: 0.0887 Train Acc: 0.9691 Eval Loss: 0.2529 Eval Acc: 0.9289 (LR: 0.00001000)
[2025-05-26 12:46:45,704]: [ResNet18_relu] Epoch: 034 Train Loss: 0.0851 Train Acc: 0.9703 Eval Loss: 0.2570 Eval Acc: 0.9291 (LR: 0.00001000)
[2025-05-26 12:48:10,127]: [ResNet18_relu] Epoch: 035 Train Loss: 0.0866 Train Acc: 0.9700 Eval Loss: 0.2576 Eval Acc: 0.9282 (LR: 0.00001000)
[2025-05-26 12:49:34,735]: [ResNet18_relu] Epoch: 036 Train Loss: 0.0828 Train Acc: 0.9710 Eval Loss: 0.2578 Eval Acc: 0.9291 (LR: 0.00001000)
[2025-05-26 12:50:59,010]: [ResNet18_relu] Epoch: 037 Train Loss: 0.0834 Train Acc: 0.9713 Eval Loss: 0.2595 Eval Acc: 0.9289 (LR: 0.00000100)
[2025-05-26 12:52:23,219]: [ResNet18_relu] Epoch: 038 Train Loss: 0.0809 Train Acc: 0.9712 Eval Loss: 0.2599 Eval Acc: 0.9283 (LR: 0.00000100)
[2025-05-26 12:53:47,227]: [ResNet18_relu] Epoch: 039 Train Loss: 0.0820 Train Acc: 0.9710 Eval Loss: 0.2608 Eval Acc: 0.9288 (LR: 0.00000100)
[2025-05-26 12:55:11,580]: [ResNet18_relu] Epoch: 040 Train Loss: 0.0803 Train Acc: 0.9719 Eval Loss: 0.2579 Eval Acc: 0.9284 (LR: 0.00000100)
[2025-05-26 12:56:35,778]: [ResNet18_relu] Epoch: 041 Train Loss: 0.0790 Train Acc: 0.9721 Eval Loss: 0.2603 Eval Acc: 0.9292 (LR: 0.00000100)
[2025-05-26 12:57:59,890]: [ResNet18_relu] Epoch: 042 Train Loss: 0.0812 Train Acc: 0.9714 Eval Loss: 0.2605 Eval Acc: 0.9278 (LR: 0.00000100)
[2025-05-26 12:59:24,009]: [ResNet18_relu] Epoch: 043 Train Loss: 0.0807 Train Acc: 0.9725 Eval Loss: 0.2601 Eval Acc: 0.9287 (LR: 0.00000010)
[2025-05-26 13:00:47,548]: [ResNet18_relu] Epoch: 044 Train Loss: 0.0802 Train Acc: 0.9727 Eval Loss: 0.2583 Eval Acc: 0.9277 (LR: 0.00000010)
[2025-05-26 13:02:10,715]: [ResNet18_relu] Epoch: 045 Train Loss: 0.0776 Train Acc: 0.9741 Eval Loss: 0.2589 Eval Acc: 0.9293 (LR: 0.00000010)
[2025-05-26 13:03:30,992]: [ResNet18_relu] Epoch: 046 Train Loss: 0.0782 Train Acc: 0.9728 Eval Loss: 0.2642 Eval Acc: 0.9273 (LR: 0.00000010)
[2025-05-26 13:04:51,027]: [ResNet18_relu] Epoch: 047 Train Loss: 0.0807 Train Acc: 0.9721 Eval Loss: 0.2593 Eval Acc: 0.9284 (LR: 0.00000010)
[2025-05-26 13:06:11,075]: [ResNet18_relu] Epoch: 048 Train Loss: 0.0788 Train Acc: 0.9724 Eval Loss: 0.2608 Eval Acc: 0.9278 (LR: 0.00000010)
[2025-05-26 13:07:31,300]: [ResNet18_relu] Epoch: 049 Train Loss: 0.0802 Train Acc: 0.9717 Eval Loss: 0.2596 Eval Acc: 0.9295 (LR: 0.00000010)
[2025-05-26 13:08:51,560]: [ResNet18_relu] Epoch: 050 Train Loss: 0.0812 Train Acc: 0.9716 Eval Loss: 0.2594 Eval Acc: 0.9294 (LR: 0.00000010)
[2025-05-26 13:10:11,855]: [ResNet18_relu] Epoch: 051 Train Loss: 0.0790 Train Acc: 0.9732 Eval Loss: 0.2612 Eval Acc: 0.9285 (LR: 0.00000010)
[2025-05-26 13:11:32,048]: [ResNet18_relu] Epoch: 052 Train Loss: 0.0791 Train Acc: 0.9717 Eval Loss: 0.2595 Eval Acc: 0.9281 (LR: 0.00000010)
[2025-05-26 13:12:52,491]: [ResNet18_relu] Epoch: 053 Train Loss: 0.0786 Train Acc: 0.9717 Eval Loss: 0.2573 Eval Acc: 0.9280 (LR: 0.00000010)
[2025-05-26 13:14:12,743]: [ResNet18_relu] Epoch: 054 Train Loss: 0.0800 Train Acc: 0.9720 Eval Loss: 0.2620 Eval Acc: 0.9290 (LR: 0.00000010)
[2025-05-26 13:15:33,188]: [ResNet18_relu] Epoch: 055 Train Loss: 0.0801 Train Acc: 0.9718 Eval Loss: 0.2592 Eval Acc: 0.9290 (LR: 0.00000010)
[2025-05-26 13:16:53,632]: [ResNet18_relu] Epoch: 056 Train Loss: 0.0785 Train Acc: 0.9723 Eval Loss: 0.2593 Eval Acc: 0.9301 (LR: 0.00000010)
[2025-05-26 13:18:14,025]: [ResNet18_relu] Epoch: 057 Train Loss: 0.0810 Train Acc: 0.9714 Eval Loss: 0.2610 Eval Acc: 0.9291 (LR: 0.00000010)
[2025-05-26 13:19:34,483]: [ResNet18_relu] Epoch: 058 Train Loss: 0.0793 Train Acc: 0.9723 Eval Loss: 0.2593 Eval Acc: 0.9283 (LR: 0.00000010)
[2025-05-26 13:20:54,905]: [ResNet18_relu] Epoch: 059 Train Loss: 0.0777 Train Acc: 0.9735 Eval Loss: 0.2588 Eval Acc: 0.9284 (LR: 0.00000010)
[2025-05-26 13:22:16,088]: [ResNet18_relu] Epoch: 060 Train Loss: 0.0791 Train Acc: 0.9729 Eval Loss: 0.2579 Eval Acc: 0.9287 (LR: 0.00000010)
[2025-05-26 13:23:36,618]: [ResNet18_relu] Epoch: 061 Train Loss: 0.0780 Train Acc: 0.9730 Eval Loss: 0.2582 Eval Acc: 0.9287 (LR: 0.00000010)
[2025-05-26 13:24:56,842]: [ResNet18_relu] Epoch: 062 Train Loss: 0.0786 Train Acc: 0.9734 Eval Loss: 0.2607 Eval Acc: 0.9290 (LR: 0.00000010)
[2025-05-26 13:26:17,053]: [ResNet18_relu] Epoch: 063 Train Loss: 0.0781 Train Acc: 0.9726 Eval Loss: 0.2598 Eval Acc: 0.9293 (LR: 0.00000010)
[2025-05-26 13:27:37,324]: [ResNet18_relu] Epoch: 064 Train Loss: 0.0799 Train Acc: 0.9723 Eval Loss: 0.2576 Eval Acc: 0.9298 (LR: 0.00000010)
[2025-05-26 13:28:57,583]: [ResNet18_relu] Epoch: 065 Train Loss: 0.0785 Train Acc: 0.9726 Eval Loss: 0.2602 Eval Acc: 0.9287 (LR: 0.00000010)
[2025-05-26 13:30:17,770]: [ResNet18_relu] Epoch: 066 Train Loss: 0.0796 Train Acc: 0.9719 Eval Loss: 0.2606 Eval Acc: 0.9284 (LR: 0.00000010)
[2025-05-26 13:31:37,950]: [ResNet18_relu] Epoch: 067 Train Loss: 0.0782 Train Acc: 0.9726 Eval Loss: 0.2592 Eval Acc: 0.9294 (LR: 0.00000010)
[2025-05-26 13:32:58,169]: [ResNet18_relu] Epoch: 068 Train Loss: 0.0806 Train Acc: 0.9723 Eval Loss: 0.2600 Eval Acc: 0.9280 (LR: 0.00000010)
[2025-05-26 13:34:18,461]: [ResNet18_relu] Epoch: 069 Train Loss: 0.0787 Train Acc: 0.9725 Eval Loss: 0.2607 Eval Acc: 0.9283 (LR: 0.00000010)
[2025-05-26 13:35:38,663]: [ResNet18_relu] Epoch: 070 Train Loss: 0.0803 Train Acc: 0.9725 Eval Loss: 0.2578 Eval Acc: 0.9283 (LR: 0.00000010)
[2025-05-26 13:36:58,903]: [ResNet18_relu] Epoch: 071 Train Loss: 0.0778 Train Acc: 0.9730 Eval Loss: 0.2570 Eval Acc: 0.9291 (LR: 0.00000010)
[2025-05-26 13:38:19,319]: [ResNet18_relu] Epoch: 072 Train Loss: 0.0780 Train Acc: 0.9725 Eval Loss: 0.2589 Eval Acc: 0.9288 (LR: 0.00000010)
[2025-05-26 13:39:39,730]: [ResNet18_relu] Epoch: 073 Train Loss: 0.0789 Train Acc: 0.9728 Eval Loss: 0.2584 Eval Acc: 0.9292 (LR: 0.00000010)
[2025-05-26 13:40:59,974]: [ResNet18_relu] Epoch: 074 Train Loss: 0.0796 Train Acc: 0.9726 Eval Loss: 0.2589 Eval Acc: 0.9293 (LR: 0.00000010)
[2025-05-26 13:42:20,398]: [ResNet18_relu] Epoch: 075 Train Loss: 0.0795 Train Acc: 0.9719 Eval Loss: 0.2608 Eval Acc: 0.9288 (LR: 0.00000010)
[2025-05-26 13:43:40,814]: [ResNet18_relu] Epoch: 076 Train Loss: 0.0819 Train Acc: 0.9719 Eval Loss: 0.2599 Eval Acc: 0.9281 (LR: 0.00000010)
[2025-05-26 13:43:40,815]: Early stopping was triggered!
[2025-05-26 13:43:40,815]: [ResNet18_relu] Best Eval Accuracy: 0.9301
[2025-05-26 13:43:40,890]: 
Training of full-precision model finished!
[2025-05-26 13:43:40,890]: Model Architecture:
[2025-05-26 13:43:40,891]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-26 13:43:40,891]: 
Model Weights:
[2025-05-26 13:43:40,891]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-26 13:43:40,901]: Sample Values (25 elements): [0.13734620809555054, 0.02443615533411503, -0.0360829122364521, 0.0057369498535990715, -0.042739760130643845, -0.159139484167099, 0.18164819478988647, -0.1496957242488861, 0.10078919678926468, 0.09672614932060242, 0.1291515976190567, -0.17062225937843323, -0.04769067466259003, 0.16614598035812378, 0.02987593598663807, 0.040160804986953735, 0.12804608047008514, -0.15550965070724487, -0.14739803969860077, -0.08652273565530777, -0.06265169382095337, -0.0561036579310894, 0.2052774280309677, 0.01188705675303936, -0.19159726798534393]
[2025-05-26 13:43:40,907]: Mean: -0.00199869
[2025-05-26 13:43:40,913]: Min: -0.38083518
[2025-05-26 13:43:40,913]: Max: 0.32666004
[2025-05-26 13:43:40,913]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-26 13:43:40,914]: Sample Values (25 elements): [0.7957434058189392, 0.7401281595230103, 0.8047277927398682, 0.5848978757858276, 0.9099540114402771, 0.6980249881744385, 0.7635841965675354, 0.7523672580718994, 0.7680569291114807, 1.0475881099700928, 0.7816541194915771, 0.7481595873832703, 0.7920162081718445, 0.6783275604248047, 0.7762406468391418, 0.6275534629821777, 0.772982120513916, 0.7355347871780396, 0.9611722230911255, 0.8455307483673096, 0.9599476456642151, 1.1494513750076294, 0.6495841145515442, 0.8073779940605164, 0.842253565788269]
[2025-05-26 13:43:40,914]: Mean: 0.77499902
[2025-05-26 13:43:40,914]: Min: 0.47415355
[2025-05-26 13:43:40,914]: Max: 1.14945138
[2025-05-26 13:43:40,914]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 13:43:40,915]: Sample Values (25 elements): [0.0049353549256920815, -0.011277003213763237, -0.17955748736858368, 0.04796183109283447, -0.031037813052535057, -0.05564181134104729, -0.030750062316656113, 0.039124056696891785, -0.021829167380928993, -0.09678375720977783, -0.018832029774785042, -0.012528492137789726, -0.055909592658281326, -0.049737315624952316, -0.07044868171215057, -0.04238658398389816, -0.0018447538604959846, 0.013992448337376118, 0.003703848924487829, 0.021049650385975838, -0.011447674594819546, 0.005940386094152927, 0.07386615872383118, -0.05079984292387962, -0.1452791690826416]
[2025-05-26 13:43:40,915]: Mean: -0.00803048
[2025-05-26 13:43:40,915]: Min: -0.34176204
[2025-05-26 13:43:40,915]: Max: 0.23574097
[2025-05-26 13:43:40,915]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 13:43:40,915]: Sample Values (25 elements): [0.6768607497215271, 0.6614948511123657, 0.6493422985076904, 0.9200568795204163, 0.7982571721076965, 0.7499873042106628, 1.064867615699768, 0.8370940089225769, 0.7685768008232117, 0.7612386345863342, 0.8257075548171997, 0.8962803483009338, 0.6385704278945923, 0.6550840139389038, 0.7847257852554321, 0.7313344478607178, 0.7981144785881042, 0.7477559447288513, 0.9590247273445129, 0.8999490737915039, 0.7898088693618774, 0.8020815253257751, 1.06967031955719, 0.9023401141166687, 0.7390881180763245]
[2025-05-26 13:43:40,916]: Mean: 0.79943818
[2025-05-26 13:43:40,916]: Min: 0.57656980
[2025-05-26 13:43:40,916]: Max: 1.18843937
[2025-05-26 13:43:40,916]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 13:43:40,917]: Sample Values (25 elements): [0.00896195787936449, 0.023544413968920708, -0.05741583928465843, 0.03070046752691269, -0.007286691106855869, 0.0011389617575332522, 0.018511010333895683, -0.017419230192899704, -0.0014656602870672941, 0.05994442105293274, 0.0855216309428215, 0.03490450978279114, -0.1235673651099205, -0.02306232787668705, 0.011044343933463097, 0.03592138737440109, -0.04051009938120842, -0.07722283154726028, -0.01719493232667446, -0.10729680210351944, -0.028140807524323463, -0.05218169093132019, -0.006278252229094505, 0.015480436384677887, -0.07491065561771393]
[2025-05-26 13:43:40,917]: Mean: -0.00285040
[2025-05-26 13:43:40,917]: Min: -0.42764384
[2025-05-26 13:43:40,917]: Max: 0.28315026
[2025-05-26 13:43:40,917]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 13:43:40,917]: Sample Values (25 elements): [0.8853140473365784, 0.7952280044555664, 0.9673526287078857, 0.6760751008987427, 0.832219660282135, 0.895904541015625, 0.8103848695755005, 1.1788012981414795, 0.9438294768333435, 0.659592866897583, 0.8779171705245972, 1.1265791654586792, 0.9605635404586792, 0.7103723883628845, 1.0611523389816284, 0.8150926232337952, 0.8981578946113586, 0.7944917678833008, 0.886605441570282, 1.0788685083389282, 0.9012466073036194, 0.7913094758987427, 0.973537266254425, 0.84727942943573, 0.693337619304657]
[2025-05-26 13:43:40,917]: Mean: 0.85811764
[2025-05-26 13:43:40,918]: Min: 0.56583732
[2025-05-26 13:43:40,918]: Max: 1.17880130
[2025-05-26 13:43:40,918]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 13:43:40,918]: Sample Values (25 elements): [-0.07262154668569565, -0.02421949990093708, -0.021983809769153595, 0.14643961191177368, 0.024673745036125183, 0.08190874755382538, 0.018271617591381073, 0.0808166116476059, -0.003233254887163639, -0.060697849839925766, -0.047491006553173065, 0.1287565529346466, 0.035485729575157166, -0.052330564707517624, 0.014882921241223812, -0.025038404390215874, -0.007405071519315243, -0.0027711116708815098, 0.016164129599928856, 0.02475016936659813, 0.01846778765320778, 0.018849922344088554, 0.0186572577804327, -0.09065079689025879, -0.06584052741527557]
[2025-05-26 13:43:40,919]: Mean: -0.00344160
[2025-05-26 13:43:40,919]: Min: -0.39013439
[2025-05-26 13:43:40,919]: Max: 0.30465293
[2025-05-26 13:43:40,919]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 13:43:40,919]: Sample Values (25 elements): [0.6678071618080139, 0.581537663936615, 0.6681610345840454, 0.7631539702415466, 0.7314407825469971, 0.5884544849395752, 0.8202848434448242, 0.8422996997833252, 0.680008053779602, 0.9924359321594238, 0.8087815046310425, 0.7021564245223999, 0.6742057204246521, 0.7724723219871521, 0.8179296255111694, 0.7463998794555664, 0.595409631729126, 0.7224991321563721, 0.7335777282714844, 0.9443453550338745, 0.9165864586830139, 0.6725141406059265, 0.9757667779922485, 0.5247750878334045, 0.6450541019439697]
[2025-05-26 13:43:40,919]: Mean: 0.73016363
[2025-05-26 13:43:40,919]: Min: 0.48459169
[2025-05-26 13:43:40,920]: Max: 1.12685013
[2025-05-26 13:43:40,920]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 13:43:40,920]: Sample Values (25 elements): [-0.02048306353390217, -0.019378196448087692, 0.06401150673627853, -0.02928033657371998, 0.06617449969053268, -0.009626733139157295, 0.06131001189351082, 0.05425170436501503, 0.1544387936592102, -0.05695342272520065, 0.012691045179963112, -0.24712984263896942, 0.04208851605653763, -0.016628652811050415, 0.0518607534468174, -0.1610216349363327, -0.0005160907749086618, -0.008292588405311108, -0.008353864774107933, 0.02366585098206997, 0.02516251802444458, 0.04336359724402428, -0.027734683826565742, -0.031582508236169815, -0.0357867032289505]
[2025-05-26 13:43:40,920]: Mean: -0.00610352
[2025-05-26 13:43:40,921]: Min: -0.32143977
[2025-05-26 13:43:40,921]: Max: 0.30160040
[2025-05-26 13:43:40,921]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 13:43:40,921]: Sample Values (25 elements): [0.8084309697151184, 0.7789302468299866, 0.8394782543182373, 0.717301607131958, 0.7803822159767151, 0.9206116795539856, 0.7228020429611206, 0.5789623856544495, 1.0431714057922363, 0.9529790282249451, 0.7110945582389832, 0.8560754060745239, 0.8678949475288391, 0.5867717862129211, 0.7107576727867126, 0.7894161343574524, 0.7605568766593933, 0.6022651791572571, 0.4774463474750519, 0.6772767305374146, 0.7581629753112793, 0.8298816084861755, 0.7198957204818726, 0.7389496564865112, 0.8781006932258606]
[2025-05-26 13:43:40,921]: Mean: 0.75957501
[2025-05-26 13:43:40,921]: Min: 0.47744635
[2025-05-26 13:43:40,921]: Max: 1.04317141
[2025-05-26 13:43:40,922]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-26 13:43:40,922]: Sample Values (25 elements): [-0.076136514544487, -0.0016571016749367118, 0.04103953763842583, -0.07595773041248322, 0.03843611851334572, -0.05002245306968689, -0.04317306727170944, 0.008387316018342972, -0.0580817274749279, -0.0846228301525116, -0.03998593986034393, -0.06462924182415009, 0.0013700058916583657, 0.05734001472592354, 0.008249325677752495, -0.07411700487136841, 0.009840437211096287, -0.03379106521606445, -0.034152813255786896, 0.03188249096274376, 0.003940309397876263, -0.06100821867585182, 0.07920151948928833, 0.016719847917556763, -0.007488118950277567]
[2025-05-26 13:43:40,923]: Mean: -0.00425692
[2025-05-26 13:43:40,923]: Min: -0.27423930
[2025-05-26 13:43:40,923]: Max: 0.27272117
[2025-05-26 13:43:40,923]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-26 13:43:40,923]: Sample Values (25 elements): [0.5260339975357056, 0.6149302124977112, 0.6113442778587341, 0.7965908050537109, 0.3343662917613983, 0.6067876219749451, 0.7210304141044617, 0.6427257061004639, 0.5854644179344177, 0.620244026184082, 0.49012595415115356, 0.6709000468254089, 0.8191613554954529, 0.7373537421226501, 0.5818749070167542, 0.6788644194602966, 0.5496026873588562, 0.4439626634120941, 0.7849194407463074, 0.6699057817459106, 0.6337175965309143, 0.5505855083465576, 0.7157068848609924, 0.6794725060462952, 0.7701014876365662]
[2025-05-26 13:43:40,923]: Mean: 0.62223601
[2025-05-26 13:43:40,923]: Min: 0.27447334
[2025-05-26 13:43:40,924]: Max: 0.97688138
[2025-05-26 13:43:40,924]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 13:43:40,925]: Sample Values (25 elements): [-0.00942230224609375, -0.017407793551683426, -0.10315326601266861, 0.06003054231405258, 0.0021681867074221373, 0.03238469362258911, -0.04267347976565361, 0.02547876350581646, 0.022322818636894226, 0.017163608223199844, -0.0431155189871788, -0.03220587596297264, -0.015417419373989105, 0.03102959878742695, 0.01451884862035513, -0.0051575591787695885, 0.07923702150583267, -0.0063943578861653805, 0.0031483666971325874, 0.052615560591220856, -0.007713804021477699, 0.01158057153224945, -0.07886573672294617, -0.007475760765373707, -0.023407427594065666]
[2025-05-26 13:43:40,925]: Mean: -0.00301202
[2025-05-26 13:43:40,925]: Min: -0.23693708
[2025-05-26 13:43:40,926]: Max: 0.26559815
[2025-05-26 13:43:40,926]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-26 13:43:40,926]: Sample Values (25 elements): [0.6888826489448547, 0.751641571521759, 0.8385370373725891, 0.701661229133606, 0.7378556132316589, 0.8184784054756165, 0.7121250629425049, 0.6272963285446167, 0.6604906320571899, 0.7918392419815063, 0.5824618339538574, 0.6570991277694702, 0.779251217842102, 0.7191250324249268, 0.737983226776123, 0.6745368242263794, 0.7886768579483032, 0.5259745121002197, 0.6908887624740601, 0.8233853578567505, 0.7481994032859802, 0.7481023073196411, 0.6170307993888855, 0.8055860996246338, 0.7553930282592773]
[2025-05-26 13:43:40,926]: Mean: 0.71809292
[2025-05-26 13:43:40,926]: Min: 0.35494167
[2025-05-26 13:43:40,926]: Max: 0.94195634
[2025-05-26 13:43:40,926]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-26 13:43:40,927]: Sample Values (25 elements): [0.02891414612531662, 0.03218747302889824, -0.04980583116412163, -0.052600663155317307, 0.12981300055980682, 0.05054020509123802, 0.09670262783765793, -0.027254579588770866, -0.07058554142713547, -0.05008105933666229, -0.07389075309038162, 0.04885035753250122, -0.020243890583515167, -0.02879492938518524, -0.15494008362293243, 0.14143742620944977, -0.07069508731365204, 0.13410988450050354, 0.021055392920970917, 0.10676473379135132, 0.06135629117488861, 0.07091573625802994, 0.031359486281871796, 0.031380221247673035, -0.1299799233675003]
[2025-05-26 13:43:40,927]: Mean: -0.00232151
[2025-05-26 13:43:40,927]: Min: -0.30640450
[2025-05-26 13:43:40,927]: Max: 0.31347016
[2025-05-26 13:43:40,927]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-26 13:43:40,928]: Sample Values (25 elements): [0.6280703544616699, 0.521562933921814, 0.7645924687385559, 0.6118974685668945, 0.4241940379142761, 0.5599567294120789, 0.9230182766914368, 0.7083680033683777, 0.7055563926696777, 0.6632717847824097, 0.707578182220459, 0.5671175718307495, 0.6446669101715088, 0.6617279648780823, 0.6882951259613037, 0.5847666263580322, 0.6692416667938232, 0.4703581631183624, 0.6299661993980408, 0.6764050722122192, 0.7701736688613892, 0.6321173906326294, 0.6181865334510803, 0.5743032693862915, 0.6875545978546143]
[2025-05-26 13:43:40,928]: Mean: 0.63848388
[2025-05-26 13:43:40,928]: Min: 0.37348488
[2025-05-26 13:43:40,928]: Max: 0.92301828
[2025-05-26 13:43:40,928]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 13:43:40,929]: Sample Values (25 elements): [-0.03849473223090172, 0.07916828244924545, 0.04884088784456253, -0.004784666001796722, 0.03665219247341156, -0.01951371505856514, 0.03761786222457886, 0.008707110770046711, 0.02803376130759716, 0.03423725441098213, 0.06507805734872818, -0.009659932926297188, -0.01107830461114645, 0.0040696910582482815, -0.09695564210414886, -0.10373866558074951, 0.01919451355934143, 0.01839114911854267, -0.00039047052268870175, -0.04986937716603279, -0.04373659938573837, 0.005053853616118431, 0.0511680543422699, 0.012849411927163601, -0.0020906387362629175]
[2025-05-26 13:43:40,930]: Mean: -0.00343783
[2025-05-26 13:43:40,930]: Min: -0.27692312
[2025-05-26 13:43:40,930]: Max: 0.24760669
[2025-05-26 13:43:40,930]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-26 13:43:40,930]: Sample Values (25 elements): [0.3807526230812073, 0.6605057120323181, 0.5566158294677734, 0.5418663024902344, 0.3647967576980591, 0.673087477684021, 0.5854989886283875, 0.669577419757843, 0.8775928616523743, 0.5814002156257629, 0.47760209441185, 0.5635027885437012, 0.43655651807785034, 0.3711455166339874, 0.6344248652458191, 0.6636917591094971, 0.3255322277545929, 0.6179013252258301, 0.4119107723236084, 0.44895225763320923, 0.8797288537025452, 0.7676777839660645, 0.6058274507522583, 0.6544612050056458, 0.5365463495254517]
[2025-05-26 13:43:40,930]: Mean: 0.57114524
[2025-05-26 13:43:40,931]: Min: 0.02159013
[2025-05-26 13:43:40,931]: Max: 0.97563994
[2025-05-26 13:43:40,931]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 13:43:40,932]: Sample Values (25 elements): [0.03708028420805931, -0.0335906520485878, -0.008177562616765499, -0.016272924840450287, -0.008258696645498276, 0.03145519644021988, 0.0014340083580464125, 0.002392264548689127, -0.01869187317788601, 0.07380080968141556, -0.020014800131320953, -0.016668852418661118, -0.011819620616734028, -0.0036065683234483004, -0.04956146702170372, -0.022342219948768616, -0.021928390488028526, -0.04193348065018654, -0.03536560758948326, 0.008786553516983986, -0.035724714398384094, 0.014352484606206417, -0.05411485210061073, -0.026638628914952278, 0.059344273060560226]
[2025-05-26 13:43:40,932]: Mean: -0.00232326
[2025-05-26 13:43:40,933]: Min: -0.25742197
[2025-05-26 13:43:40,933]: Max: 0.26996481
[2025-05-26 13:43:40,933]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-26 13:43:40,933]: Sample Values (25 elements): [0.6165195107460022, 0.5923470854759216, 0.7372320294380188, 0.5795955657958984, 0.4579009413719177, 0.5600190162658691, 0.6180949807167053, 0.524503231048584, 0.5149431824684143, 0.5002803802490234, 0.5456312298774719, 0.6188443303108215, 0.5537111163139343, 0.6327932476997375, 0.5286887884140015, 0.8734315037727356, 0.3600141704082489, 0.6505584120750427, 0.7143684029579163, 0.643094539642334, 0.7255391478538513, 0.7596729397773743, 0.4795994758605957, 0.5283063650131226, 0.5735177397727966]
[2025-05-26 13:43:40,933]: Mean: 0.58699042
[2025-05-26 13:43:40,933]: Min: 0.17222011
[2025-05-26 13:43:40,933]: Max: 0.87343150
[2025-05-26 13:43:40,934]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-26 13:43:40,937]: Sample Values (25 elements): [-0.014691388234496117, -0.011649365536868572, 0.04995276406407356, -0.037131886929273605, -0.03443477302789688, 0.0018592873821035028, 0.014739382080733776, 0.01360304094851017, 0.08869823813438416, -0.008948580361902714, 0.05778384581208229, 0.01871073804795742, 0.033251434564590454, 0.02646246924996376, -7.774015102768317e-05, -0.016252653673291206, 0.009977866895496845, 0.00016888721438590437, -0.004373620264232159, -0.023635942488908768, 0.02710612304508686, 0.0910143181681633, -0.03901245817542076, -0.0005932830972597003, 0.04324205219745636]
[2025-05-26 13:43:40,937]: Mean: -0.00144853
[2025-05-26 13:43:40,937]: Min: -0.20075673
[2025-05-26 13:43:40,937]: Max: 0.28674820
[2025-05-26 13:43:40,938]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-26 13:43:40,938]: Sample Values (25 elements): [0.48021966218948364, 0.3613570034503937, 0.2993831932544708, 0.6841925382614136, 0.225331112742424, 0.6187326908111572, 0.6559666395187378, 0.5957123637199402, 0.5300551652908325, 0.6868587732315063, 0.5697721242904663, 0.5779946446418762, 0.5885818004608154, 0.6721253991127014, 0.4310072362422943, 0.4194432497024536, 0.3173113465309143, 0.5630854368209839, 0.380130797624588, 0.6746448874473572, 0.31566452980041504, 0.34505581855773926, 0.007652898784726858, 0.5809935927391052, 0.24681396782398224]
[2025-05-26 13:43:40,938]: Mean: 0.44145367
[2025-05-26 13:43:40,938]: Min: 0.00000882
[2025-05-26 13:43:40,938]: Max: 0.94059807
[2025-05-26 13:43:40,938]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 13:43:40,946]: Sample Values (25 elements): [0.0020743089262396097, 0.001155794714577496, -2.5197625291184522e-05, -0.0016215738141909242, -0.005208177957683802, -0.00034281512489542365, -0.0033969078212976456, -0.021560359746217728, 0.02806679904460907, 0.006442931015044451, 0.009321428835391998, 0.046882353723049164, -0.0033869200851768255, -0.021675989031791687, 0.023828603327274323, 0.0012366254813969135, -0.032246340066194534, 0.00019920682825613767, 0.03012070246040821, 0.0016603120602667332, 0.004779336042702198, 0.04546709358692169, -5.6268760090461e-05, 0.021711962297558784, 0.001188172260299325]
[2025-05-26 13:43:40,946]: Mean: -0.00091278
[2025-05-26 13:43:40,946]: Min: -0.26201206
[2025-05-26 13:43:40,946]: Max: 0.25828311
[2025-05-26 13:43:40,946]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-26 13:43:40,947]: Sample Values (25 elements): [0.4250142276287079, 0.6692869067192078, 0.43693915009498596, 0.6911409497261047, 0.4642806649208069, 0.6570603251457214, 0.6964079737663269, 0.49432748556137085, 0.5327457189559937, 0.44147321581840515, 0.8424332737922668, 0.6506555080413818, 0.7346758246421814, 0.63511723279953, 0.34029752016067505, 0.30907827615737915, 0.5321067571640015, 0.3612280786037445, 0.5377780795097351, 0.43321093916893005, 0.47186022996902466, 0.7142971158027649, 0.4460260570049286, 0.7933163046836853, 0.42814764380455017]
[2025-05-26 13:43:40,947]: Mean: 0.52651507
[2025-05-26 13:43:40,947]: Min: 0.00649185
[2025-05-26 13:43:40,947]: Max: 0.98997021
[2025-05-26 13:43:40,947]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-26 13:43:40,948]: Sample Values (25 elements): [-0.02120203524827957, 0.021245764568448067, -0.016485340893268585, 0.013114172965288162, -0.13325276970863342, -0.04554467648267746, -0.008274275809526443, 0.014950978569686413, 0.032781243324279785, 0.05769895389676094, -0.004261111840605736, -0.03388169780373573, 0.020215364173054695, 0.08152202516794205, -0.02392537333071232, -0.058371298015117645, -0.00639758026227355, -0.11379490047693253, 0.009333883412182331, -0.015883654356002808, 0.015774346888065338, -0.02888363227248192, 0.15317434072494507, 0.014588644728064537, 0.04026900231838226]
[2025-05-26 13:43:40,948]: Mean: -0.00474498
[2025-05-26 13:43:40,948]: Min: -0.26860929
[2025-05-26 13:43:40,948]: Max: 0.26195970
[2025-05-26 13:43:40,948]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-26 13:43:40,948]: Sample Values (25 elements): [0.4976930320262909, 0.48000574111938477, 0.2460213601589203, 0.4287193715572357, 0.5473620295524597, 0.3041635751724243, 0.5442525744438171, 0.5418888926506042, 0.3314960300922394, 0.42334434390068054, 0.4547182321548462, 0.4974358081817627, 0.5892719030380249, 0.5339289903640747, 0.5426545143127441, 0.14419269561767578, 0.5634089708328247, 0.4460659325122833, 0.5365412831306458, 0.22435183823108673, 0.40674370527267456, 0.3492435812950134, 0.629489541053772, 0.2327909767627716, 0.13405141234397888]
[2025-05-26 13:43:40,949]: Mean: 0.40385309
[2025-05-26 13:43:40,949]: Min: 0.01484891
[2025-05-26 13:43:40,949]: Max: 0.71177512
[2025-05-26 13:43:40,949]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 13:43:40,955]: Sample Values (25 elements): [-0.030060820281505585, -0.047857616096735, -0.013715006411075592, -0.0054089766927063465, 2.937031455536473e-26, 6.516461814598529e-10, 3.846158078981386e-15, -2.078621070763188e-21, -0.048250485211610794, -2.1963154867900794e-18, -0.046776413917541504, -0.0003282625984866172, -0.006893504876643419, -0.030868589878082275, 5.712431084248237e-05, -0.006363377440720797, 4.910850468226321e-41, -4.929347607955409e-41, -0.020298967137932777, 0.011157343164086342, 0.0001510617439635098, -6.622588261961937e-05, -0.0001452591532142833, -9.543274577159498e-16, 4.045238736191785e-21]
[2025-05-26 13:43:40,955]: Mean: -0.00094199
[2025-05-26 13:43:40,956]: Min: -0.21968850
[2025-05-26 13:43:40,956]: Max: 0.23969011
[2025-05-26 13:43:40,956]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-26 13:43:40,956]: Sample Values (25 elements): [0.16660048067569733, 0.7418956756591797, 0.6500266194343567, 0.24352407455444336, 0.5452032089233398, 0.600836455821991, 0.42498305439949036, 3.694214205374191e-12, 4.344096851127688e-07, 0.23339274525642395, 0.13350501656532288, 0.01795695349574089, 0.3413551449775696, 0.11269547790288925, 0.43326810002326965, 0.283012330532074, 2.0695594127317918e-09, 0.00203090556897223, 0.7174052000045776, 0.2821650803089142, 0.426470011472702, 0.5832000970840454, 0.380874902009964, 4.284126475795347e-08, 0.4116322100162506]
[2025-05-26 13:43:40,956]: Mean: 0.24673678
[2025-05-26 13:43:40,956]: Min: -0.00109979
[2025-05-26 13:43:40,957]: Max: 0.76866895
[2025-05-26 13:43:40,957]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 13:43:40,965]: Sample Values (25 elements): [0.0463443286716938, -0.034517236053943634, -0.07020825147628784, 0.017239637672901154, -0.018973153084516525, -0.024716397747397423, 0.0006933245458640158, 0.00021739302610512823, 4.908328130990537e-41, 0.028913704678416252, -0.0006305098650045693, 0.051179178059101105, 0.004320900421589613, -0.0001967534626601264, -0.007902221754193306, -0.05150565505027771, -0.0030869918409734964, 6.1490985338248725e-28, -9.666291274420466e-21, 0.002081257291138172, 0.0034087728708982468, -0.014284889213740826, 0.0036730628926306963, 0.0010656911181285977, -1.2315835291829755e-22]
[2025-05-26 13:43:40,966]: Mean: -0.00095011
[2025-05-26 13:43:40,966]: Min: -0.20497115
[2025-05-26 13:43:40,966]: Max: 0.22851130
[2025-05-26 13:43:40,966]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-26 13:43:40,967]: Sample Values (25 elements): [0.20232929289340973, 0.21559664607048035, 0.31244683265686035, 0.1263296902179718, 0.5890283584594727, 0.4470645487308502, 0.626150369644165, 0.6220861077308655, 0.8886914253234863, 0.4478070139884949, 0.39774978160858154, 0.35063305497169495, 0.3772863745689392, 0.5093615055084229, 0.2753531038761139, 0.3651195466518402, 0.38748329877853394, 0.209723100066185, 0.45273488759994507, 0.18339280784130096, 0.351351797580719, 0.38038283586502075, 0.31913334131240845, 0.5586271286010742, 0.07353325188159943]
[2025-05-26 13:43:40,967]: Mean: 0.33959103
[2025-05-26 13:43:40,967]: Min: -0.00481670
[2025-05-26 13:43:40,967]: Max: 0.88869143
[2025-05-26 13:43:40,967]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-26 13:43:40,985]: Sample Values (25 elements): [-4.907487351911942e-41, 1.649544563442351e-11, -4.906786702679779e-41, -4.925143712562435e-41, 4.907627481758374e-41, 4.928506828876814e-41, -5.271560965525168e-10, 0.0015553488628938794, 2.979608950663145e-40, 4.910149818994159e-41, 0.007617326453328133, 4.914774103926431e-41, 4.956672928009743e-41, -0.008496051654219627, -0.04958885535597801, 4.918277350087243e-41, -0.024006018415093422, -0.015689866617321968, -2.8585087373761943e-41, 4.920239167937298e-41, -1.0757865196514512e-17, -0.00915638916194439, -4.91813722024081e-41, -4.910710338379889e-41, -4.905525534061887e-41]
[2025-05-26 13:43:40,985]: Mean: -0.00019576
[2025-05-26 13:43:40,985]: Min: -0.20600615
[2025-05-26 13:43:40,986]: Max: 0.30988187
[2025-05-26 13:43:40,986]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-26 13:43:40,986]: Sample Values (25 elements): [9.839281744916661e-08, 0.3724546730518341, 0.33545801043510437, 0.02838977612555027, 2.0562063607777042e-13, 0.0004387338412925601, 0.3841738700866699, 0.23577862977981567, 1.3847879074446907e-15, 0.8614513874053955, 0.6848709583282471, 5.584664397595418e-10, 1.0771211700344452e-10, 2.0525243085559898e-15, 4.768859612758192e-11, 1.479176698661322e-12, 0.36163008213043213, 1.6865478458250038e-10, 6.932779741056199e-22, 3.798401792314927e-18, 5.299061496132247e-14, 2.8136699636860385e-09, 0.4346318542957306, 0.4676377475261688, 0.4092004597187042]
[2025-05-26 13:43:40,986]: Mean: 0.10854502
[2025-05-26 13:43:40,986]: Min: -0.00005790
[2025-05-26 13:43:40,986]: Max: 0.94753927
[2025-05-26 13:43:40,987]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 13:43:41,055]: Sample Values (25 elements): [-0.005693298298865557, 4.925704231948164e-41, -4.918837869472973e-41, -8.347605762537569e-05, -0.00012250914005562663, -4.679272870526121e-36, 4.9638195501778e-41, -4.911130727919186e-41, 4.951768383384606e-41, 0.01191527210175991, -0.00921675842255354, 5.1278247198454475e-39, -0.0001138362058554776, 4.919118129165838e-41, -4.940417865823575e-41, -4.917016181469351e-41, -2.464701174176298e-05, 0.0007132851169444621, -4.908468260836969e-41, -0.000342555926181376, 0.0007685004384256899, 4.906926832526212e-41, 9.681918163551018e-05, 9.598193831392835e-41, 0.0011370738502591848]
[2025-05-26 13:43:41,055]: Mean: 0.00006847
[2025-05-26 13:43:41,055]: Min: -0.10514567
[2025-05-26 13:43:41,056]: Max: 0.16258515
[2025-05-26 13:43:41,056]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-26 13:43:41,056]: Sample Values (25 elements): [0.4114091694355011, 0.6598212718963623, 0.048051975667476654, 0.35715070366859436, 0.606550931930542, 0.5660518407821655, 0.3761883080005646, 0.5104478597640991, 0.4615779519081116, 0.591915488243103, 0.39774367213249207, 0.25409215688705444, 0.43507853150367737, 0.20953693985939026, 0.5370731949806213, 0.47612467408180237, 0.48004838824272156, 0.3130239248275757, 0.40492743253707886, 0.6913203597068787, 0.515815794467926, 0.6138991713523865, 0.53871089220047, 0.6121585369110107, 0.2243385910987854]
[2025-05-26 13:43:41,056]: Mean: 0.47798645
[2025-05-26 13:43:41,056]: Min: -0.00325992
[2025-05-26 13:43:41,056]: Max: 0.88226652
[2025-05-26 13:43:41,056]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-26 13:43:41,058]: Sample Values (25 elements): [0.016032757237553596, -0.05465669929981232, -0.03725500777363777, 0.01735050044953823, -0.025012493133544922, -0.03008158691227436, 0.016852423548698425, 0.06142331659793854, -0.006996468175202608, -0.014287285506725311, 0.0027483089361339808, -0.05824517831206322, -0.04288388788700104, 0.028744280338287354, 0.005356391426175833, 0.0008118877885863185, 0.007806770969182253, 0.0041144429706037045, -0.057357411831617355, 0.00593193992972374, -0.0026348878163844347, 0.007726410869508982, 0.005240149796009064, 0.01567828096449375, -0.007200261112302542]
[2025-05-26 13:43:41,058]: Mean: -0.00095869
[2025-05-26 13:43:41,058]: Min: -0.13407536
[2025-05-26 13:43:41,058]: Max: 0.19569643
[2025-05-26 13:43:41,058]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-26 13:43:41,059]: Sample Values (25 elements): [0.3562212586402893, 0.6129848957061768, 0.421463280916214, 0.6452025771141052, 0.5711296796798706, 0.31730201840400696, 0.3160548806190491, 0.5699136853218079, 0.47559699416160583, 0.3036218285560608, 0.4028688371181488, 0.4547961354255676, 0.5222983956336975, 2.8837959159605744e-32, 0.2919454574584961, 0.28926438093185425, 0.6300652027130127, 0.43227940797805786, 0.5751240253448486, 0.49341586232185364, 0.22472688555717468, 0.72651606798172, 0.6103131771087646, 0.6723315715789795, 0.5785043239593506]
[2025-05-26 13:43:41,059]: Mean: 0.50110537
[2025-05-26 13:43:41,059]: Min: 0.00000000
[2025-05-26 13:43:41,059]: Max: 0.78858840
[2025-05-26 13:43:41,059]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 13:43:41,114]: Sample Values (25 elements): [-4.942239553827197e-41, 4.928927218416112e-41, -4.957513707088338e-41, 4.909589299608429e-41, -4.93789552858779e-41, -4.971246432038721e-41, -4.951768383384606e-41, -4.972927990195911e-41, -4.956112408624013e-41, -4.946863838759469e-41, -4.963679420331367e-41, 4.937475139048493e-41, -4.958074226474068e-41, 4.935373191352006e-41, -4.913512935308539e-41, 4.909309039915564e-41, -4.957233447395473e-41, -4.912251766690646e-41, 4.953590071388228e-41, 4.908328130990537e-41, -1.1827580121289414e-21, -4.909729429454862e-41, -4.909028780222699e-41, -6.015412772342644e-39, 4.926965400566057e-41]
[2025-05-26 13:43:41,114]: Mean: -0.00000519
[2025-05-26 13:43:41,114]: Min: -0.03791752
[2025-05-26 13:43:41,114]: Max: 0.05791683
[2025-05-26 13:43:41,114]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-26 13:43:41,116]: Sample Values (25 elements): [1.7458285861350793e-14, 3.2257463086056914e-18, 0.1901356279850006, 2.1960963877203392e-29, 3.78421661736938e-34, 2.432742688052174e-31, 1.5739776699774666e-06, 0.001346837729215622, 0.043636683374643326, 2.8603849386854563e-06, 4.583216199126459e-17, 4.82924660957765e-27, 7.374891170842945e-13, 1.1124328128161736e-19, 4.979944408234349e-12, 6.565284214485608e-17, 2.167869032291097e-22, 4.993561789707923e-23, 8.73792623882195e-16, -4.909729429454862e-41, 2.966680381715457e-15, 8.319621304809371e-14, 0.00039266180829145014, 1.1710538138157145e-13, 1.7837303745944666e-21]
[2025-05-26 13:43:41,116]: Mean: 0.01577947
[2025-05-26 13:43:41,116]: Min: -0.00000000
[2025-05-26 13:43:41,116]: Max: 0.61161155
[2025-05-26 13:43:41,117]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 13:43:41,162]: Sample Values (25 elements): [4.928646958723247e-41, -4.92430293348384e-41, 4.656234537258502e-41, 4.969705003727964e-41, -4.935653451044871e-41, -4.923322024558812e-41, -4.90566566390832e-41, 7.069072590181103e-20, 4.926685140873192e-41, -4.910990598072754e-41, -5.17107159305144e-41, -4.969284614188666e-41, -2.3914082298699991e-35, -4.912952415922809e-41, 4.907066962372644e-41, -4.922200985787352e-41, 4.974749678199533e-41, -4.907767611604807e-41, -6.187153109533365e-41, -0.0011102840071544051, -4.913933324847836e-41, -4.909028780222699e-41, -5.81113514928619e-22, 4.913232675615674e-41, 4.923322024558812e-41]
[2025-05-26 13:43:41,162]: Mean: -0.00004510
[2025-05-26 13:43:41,162]: Min: -0.04583683
[2025-05-26 13:43:41,163]: Max: 0.05266909
[2025-05-26 13:43:41,163]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-26 13:43:41,163]: Sample Values (25 elements): [0.2133551388978958, 0.1647900640964508, 0.23896455764770508, 0.23302654922008514, 0.20445695519447327, 0.26815831661224365, 0.10112710297107697, 0.3667948246002197, 0.014088748022913933, 0.3136318027973175, 0.18998868763446808, 0.10748514533042908, 0.25185415148735046, 0.2678104341030121, 0.1700439304113388, 0.2527405619621277, 0.18780043721199036, 0.017452875152230263, 0.1346917599439621, 0.14692197740077972, 0.1933545470237732, 0.22102221846580505, 0.21736955642700195, 0.4356096386909485, 0.10553617030382156]
[2025-05-26 13:43:41,163]: Mean: 0.21878050
[2025-05-26 13:43:41,163]: Min: 0.00000000
[2025-05-26 13:43:41,163]: Max: 0.49663252
[2025-05-26 13:43:41,163]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-26 13:43:41,164]: Sample Values (25 elements): [-0.07139661908149719, -0.05698353424668312, 0.07734188437461853, -0.014216559939086437, 0.05606527253985405, -0.04992479085922241, 0.05907513201236725, 0.03836255148053169, -0.04370969533920288, 0.06460573524236679, 0.04406791552901268, -0.020032066851854324, -0.16385120153427124, -0.04260384291410446, 0.09593214839696884, -0.04646049067378044, 0.009177845902740955, -0.028924018144607544, 0.09297370165586472, -0.07338884472846985, 0.0070986440405249596, -0.05100985988974571, 0.0045655593276023865, 0.1363915503025055, -0.043220918625593185]
[2025-05-26 13:43:41,164]: Mean: -0.01757707
[2025-05-26 13:43:41,164]: Min: -0.37491491
[2025-05-26 13:43:41,164]: Max: 0.28375247
[2025-05-26 13:43:41,164]: Checkpoint of model at path [checkpoint/ResNet18_relu.ckpt] will be used for QAT
[2025-05-27 09:22:43,903]: Checkpoint of model at path [checkpoint/ResNet18_relu.ckpt] will be used for QAT
[2025-05-27 09:22:43,914]: 


QAT of ResNet18 with relu down to 4 bits...
[2025-05-27 09:22:44,328]: [ResNet18_relu_quantized_4_bits] after configure_qat:
[2025-05-27 09:22:44,588]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 09:24:25,052]: [ResNet18_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.3004 Train Acc: 0.8961 Eval Loss: 0.3574 Eval Acc: 0.8862 (LR: 0.00100000)
[2025-05-27 09:26:06,178]: [ResNet18_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.3101 Train Acc: 0.8918 Eval Loss: 0.4298 Eval Acc: 0.8640 (LR: 0.00100000)
[2025-05-27 09:27:51,127]: [ResNet18_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.3128 Train Acc: 0.8919 Eval Loss: 0.4234 Eval Acc: 0.8641 (LR: 0.00100000)
[2025-05-27 09:29:33,577]: [ResNet18_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.3079 Train Acc: 0.8941 Eval Loss: 0.3865 Eval Acc: 0.8745 (LR: 0.00100000)
[2025-05-27 09:31:16,278]: [ResNet18_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.3027 Train Acc: 0.8946 Eval Loss: 0.3779 Eval Acc: 0.8770 (LR: 0.00100000)
[2025-05-27 09:33:06,011]: [ResNet18_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.3012 Train Acc: 0.8964 Eval Loss: 0.4009 Eval Acc: 0.8732 (LR: 0.00100000)
[2025-05-27 09:34:50,547]: [ResNet18_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.3053 Train Acc: 0.8959 Eval Loss: 0.4497 Eval Acc: 0.8559 (LR: 0.00010000)
[2025-05-27 09:36:32,901]: [ResNet18_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.2132 Train Acc: 0.9268 Eval Loss: 0.2656 Eval Acc: 0.9123 (LR: 0.00010000)
[2025-05-27 09:38:16,520]: [ResNet18_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.1786 Train Acc: 0.9386 Eval Loss: 0.2570 Eval Acc: 0.9170 (LR: 0.00010000)
[2025-05-27 09:40:02,034]: [ResNet18_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.1672 Train Acc: 0.9421 Eval Loss: 0.2635 Eval Acc: 0.9165 (LR: 0.00010000)
[2025-05-27 09:41:43,510]: [ResNet18_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.1537 Train Acc: 0.9461 Eval Loss: 0.2522 Eval Acc: 0.9218 (LR: 0.00010000)
[2025-05-27 09:43:26,408]: [ResNet18_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.1499 Train Acc: 0.9488 Eval Loss: 0.2603 Eval Acc: 0.9211 (LR: 0.00010000)
[2025-05-27 09:45:07,909]: [ResNet18_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.1429 Train Acc: 0.9495 Eval Loss: 0.2622 Eval Acc: 0.9228 (LR: 0.00010000)
[2025-05-27 09:46:48,969]: [ResNet18_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.1365 Train Acc: 0.9523 Eval Loss: 0.2572 Eval Acc: 0.9208 (LR: 0.00010000)
[2025-05-27 09:48:28,525]: [ResNet18_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.1325 Train Acc: 0.9536 Eval Loss: 0.2683 Eval Acc: 0.9235 (LR: 0.00010000)
[2025-05-27 09:50:23,595]: [ResNet18_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.1282 Train Acc: 0.9548 Eval Loss: 0.2584 Eval Acc: 0.9214 (LR: 0.00010000)
[2025-05-27 09:52:06,742]: [ResNet18_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.1215 Train Acc: 0.9574 Eval Loss: 0.2683 Eval Acc: 0.9231 (LR: 0.00001000)
[2025-05-27 09:53:50,200]: [ResNet18_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.1116 Train Acc: 0.9608 Eval Loss: 0.2530 Eval Acc: 0.9276 (LR: 0.00001000)
[2025-05-27 09:55:29,800]: [ResNet18_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.1086 Train Acc: 0.9620 Eval Loss: 0.2523 Eval Acc: 0.9276 (LR: 0.00001000)
[2025-05-27 09:57:11,238]: [ResNet18_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.1038 Train Acc: 0.9636 Eval Loss: 0.2514 Eval Acc: 0.9271 (LR: 0.00001000)
[2025-05-27 09:59:19,283]: [ResNet18_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.1051 Train Acc: 0.9639 Eval Loss: 0.2534 Eval Acc: 0.9289 (LR: 0.00001000)
[2025-05-27 10:01:07,351]: [ResNet18_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.1026 Train Acc: 0.9641 Eval Loss: 0.2515 Eval Acc: 0.9274 (LR: 0.00001000)
[2025-05-27 10:02:55,141]: [ResNet18_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.0994 Train Acc: 0.9656 Eval Loss: 0.2542 Eval Acc: 0.9281 (LR: 0.00001000)
[2025-05-27 10:04:38,120]: [ResNet18_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.0995 Train Acc: 0.9660 Eval Loss: 0.2540 Eval Acc: 0.9282 (LR: 0.00001000)
[2025-05-27 10:06:22,064]: [ResNet18_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.1012 Train Acc: 0.9640 Eval Loss: 0.2573 Eval Acc: 0.9275 (LR: 0.00001000)
[2025-05-27 10:08:08,552]: [ResNet18_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.0965 Train Acc: 0.9664 Eval Loss: 0.2578 Eval Acc: 0.9287 (LR: 0.00000100)
[2025-05-27 10:09:51,993]: [ResNet18_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.0980 Train Acc: 0.9664 Eval Loss: 0.2581 Eval Acc: 0.9278 (LR: 0.00000100)
[2025-05-27 10:11:34,983]: [ResNet18_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.0991 Train Acc: 0.9658 Eval Loss: 0.2583 Eval Acc: 0.9265 (LR: 0.00000100)
[2025-05-27 10:13:17,748]: [ResNet18_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.0977 Train Acc: 0.9665 Eval Loss: 0.2546 Eval Acc: 0.9275 (LR: 0.00000100)
[2025-05-27 10:15:00,577]: [ResNet18_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.0970 Train Acc: 0.9665 Eval Loss: 0.2527 Eval Acc: 0.9286 (LR: 0.00000100)
[2025-05-27 10:16:43,344]: [ResNet18_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.0981 Train Acc: 0.9655 Eval Loss: 0.2508 Eval Acc: 0.9271 (LR: 0.00000100)
[2025-05-27 10:18:21,663]: [ResNet18_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.0960 Train Acc: 0.9660 Eval Loss: 0.2505 Eval Acc: 0.9279 (LR: 0.00000100)
[2025-05-27 10:19:59,382]: [ResNet18_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.0937 Train Acc: 0.9675 Eval Loss: 0.2541 Eval Acc: 0.9287 (LR: 0.00000100)
[2025-05-27 10:21:37,078]: [ResNet18_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.0958 Train Acc: 0.9669 Eval Loss: 0.2526 Eval Acc: 0.9265 (LR: 0.00000100)
[2025-05-27 10:23:14,957]: [ResNet18_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.0978 Train Acc: 0.9658 Eval Loss: 0.2576 Eval Acc: 0.9258 (LR: 0.00000100)
[2025-05-27 10:24:52,677]: [ResNet18_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.0972 Train Acc: 0.9657 Eval Loss: 0.2498 Eval Acc: 0.9283 (LR: 0.00000100)
[2025-05-27 10:26:30,431]: [ResNet18_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.0952 Train Acc: 0.9666 Eval Loss: 0.2534 Eval Acc: 0.9277 (LR: 0.00000100)
[2025-05-27 10:28:08,249]: [ResNet18_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.0928 Train Acc: 0.9667 Eval Loss: 0.2555 Eval Acc: 0.9268 (LR: 0.00000100)
[2025-05-27 10:29:46,208]: [ResNet18_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.0961 Train Acc: 0.9660 Eval Loss: 0.2547 Eval Acc: 0.9274 (LR: 0.00000100)
[2025-05-27 10:31:24,464]: [ResNet18_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.0946 Train Acc: 0.9675 Eval Loss: 0.2482 Eval Acc: 0.9289 (LR: 0.00000100)
[2025-05-27 10:33:02,131]: [ResNet18_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.0937 Train Acc: 0.9672 Eval Loss: 0.2541 Eval Acc: 0.9261 (LR: 0.00000100)
[2025-05-27 10:34:40,019]: [ResNet18_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.0985 Train Acc: 0.9661 Eval Loss: 0.2569 Eval Acc: 0.9269 (LR: 0.00000100)
[2025-05-27 10:36:17,558]: [ResNet18_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.0936 Train Acc: 0.9673 Eval Loss: 0.2545 Eval Acc: 0.9276 (LR: 0.00000100)
[2025-05-27 10:37:55,310]: [ResNet18_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.0931 Train Acc: 0.9675 Eval Loss: 0.2559 Eval Acc: 0.9276 (LR: 0.00000100)
[2025-05-27 10:39:33,089]: [ResNet18_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.0976 Train Acc: 0.9665 Eval Loss: 0.2558 Eval Acc: 0.9280 (LR: 0.00000100)
[2025-05-27 10:41:10,589]: [ResNet18_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.0929 Train Acc: 0.9674 Eval Loss: 0.2561 Eval Acc: 0.9276 (LR: 0.00000010)
[2025-05-27 10:42:48,138]: [ResNet18_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.0958 Train Acc: 0.9664 Eval Loss: 0.2588 Eval Acc: 0.9271 (LR: 0.00000010)
[2025-05-27 10:44:25,751]: [ResNet18_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.0951 Train Acc: 0.9669 Eval Loss: 0.2578 Eval Acc: 0.9256 (LR: 0.00000010)
[2025-05-27 10:46:03,495]: [ResNet18_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.0959 Train Acc: 0.9674 Eval Loss: 0.2545 Eval Acc: 0.9279 (LR: 0.00000010)
[2025-05-27 10:47:41,035]: [ResNet18_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.0957 Train Acc: 0.9667 Eval Loss: 0.2585 Eval Acc: 0.9262 (LR: 0.00000010)
[2025-05-27 10:49:18,601]: [ResNet18_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.0939 Train Acc: 0.9677 Eval Loss: 0.2482 Eval Acc: 0.9278 (LR: 0.00000010)
[2025-05-27 10:50:56,195]: [ResNet18_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.0977 Train Acc: 0.9658 Eval Loss: 0.2548 Eval Acc: 0.9264 (LR: 0.00000010)
[2025-05-27 10:52:33,733]: [ResNet18_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.0943 Train Acc: 0.9665 Eval Loss: 0.2560 Eval Acc: 0.9257 (LR: 0.00000010)
[2025-05-27 10:54:11,313]: [ResNet18_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.0921 Train Acc: 0.9677 Eval Loss: 0.2542 Eval Acc: 0.9268 (LR: 0.00000010)
[2025-05-27 10:55:49,042]: [ResNet18_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.0953 Train Acc: 0.9669 Eval Loss: 0.2539 Eval Acc: 0.9273 (LR: 0.00000010)
[2025-05-27 10:57:26,778]: [ResNet18_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.0943 Train Acc: 0.9669 Eval Loss: 0.2586 Eval Acc: 0.9259 (LR: 0.00000010)
[2025-05-27 10:59:04,538]: [ResNet18_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.0974 Train Acc: 0.9660 Eval Loss: 0.2546 Eval Acc: 0.9275 (LR: 0.00000010)
[2025-05-27 11:00:42,290]: [ResNet18_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.0955 Train Acc: 0.9668 Eval Loss: 0.2520 Eval Acc: 0.9263 (LR: 0.00000010)
[2025-05-27 11:02:19,840]: [ResNet18_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.0921 Train Acc: 0.9682 Eval Loss: 0.2506 Eval Acc: 0.9284 (LR: 0.00000010)
[2025-05-27 11:03:58,124]: [ResNet18_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.0939 Train Acc: 0.9676 Eval Loss: 0.2514 Eval Acc: 0.9277 (LR: 0.00000010)
[2025-05-27 11:03:58,124]: [ResNet18_relu_quantized_4_bits] Best Eval Accuracy: 0.9289
[2025-05-27 11:03:58,209]: 


Quantization of model down to 4 bits finished
[2025-05-27 11:03:58,209]: Model Architecture:
[2025-05-27 11:03:58,266]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0558], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.836409568786621)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0348], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28916752338409424, max_val=0.2321491539478302)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6895], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.342020034790039)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0459], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4042699337005615, max_val=0.2840157747268677)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4627], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=21.93983268737793)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0461], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38505446910858154, max_val=0.305891752243042)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4421], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.632183074951172)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0388], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2730809450149536, max_val=0.3088918924331665)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4221], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=21.3314151763916)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0347], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24318315088748932, max_val=0.27738094329833984)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3130], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.695184707641602)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0365], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2589579224586487, max_val=0.2892889976501465)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32053202390670776, max_val=0.32210034132003784)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6225], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.337053298950195)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0361], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.288591206073761, max_val=0.25350475311279297)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4150], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.225298881530762)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0328], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23890803754329681, max_val=0.25323957204818726)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7211], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.816544532775879)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0368], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25464439392089844, max_val=0.29757964611053467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3641], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.461450099945068)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0338], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24214497208595276, max_val=0.26439785957336426)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0364], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27385497093200684, max_val=0.27201804518699646)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.063178062438965)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0317], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2209813892841339, max_val=0.25495821237564087)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2885], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.327491760253906)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20185470581054688, max_val=0.2519146800041199)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7007], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.510039329528809)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0343], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20102465152740479, max_val=0.31388163566589355)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5044], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.5662126541137695)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0185], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11808492243289948, max_val=0.15898871421813965)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0227], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1386520117521286, max_val=0.20110967755317688)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8493], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.739299774169922)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03624920919537544, max_val=0.06811795383691788)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1397], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.0952630043029785)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.052940599620342255, max_val=0.054108843207359314)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8446], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.668488502502441)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 11:03:58,266]: 
Model Weights:
[2025-05-27 11:03:58,266]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-27 11:03:58,269]: Sample Values (25 elements): [0.019552404060959816, 0.009749546647071838, -0.1093263253569603, 0.06217986345291138, -0.007357050199061632, -0.14447565376758575, -0.09761969745159149, -0.05419887602329254, -0.037150073796510696, 0.0002577554841991514, 0.14497064054012299, 0.11469355970621109, -0.20510277152061462, -0.09616133570671082, 0.03326106071472168, -0.034906383603811264, 0.16921576857566833, -0.20811331272125244, 0.20941124856472015, -0.2062874436378479, 0.005969993304461241, 0.1395646035671234, -0.12582050263881683, -0.06909335404634476, 0.07924496382474899]
[2025-05-27 11:03:58,280]: Mean: -0.00123641
[2025-05-27 11:03:58,281]: Min: -0.39002749
[2025-05-27 11:03:58,282]: Max: 0.34799114
[2025-05-27 11:03:58,282]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-27 11:03:58,283]: Sample Values (25 elements): [1.2749543190002441, 0.9376092553138733, 0.8681222200393677, 0.8278939127922058, 1.2036914825439453, 0.7663835287094116, 0.9408548474311829, 0.6742293834686279, 0.8377752304077148, 1.1707431077957153, 0.6733330488204956, 0.6883335113525391, 0.8715835213661194, 0.8136433362960815, 0.9351298213005066, 0.8713566064834595, 0.4818078875541687, 1.0791091918945312, 1.0736584663391113, 0.6738890409469604, 1.0691885948181152, 0.7556623220443726, 0.9854863286018372, 1.2906055450439453, 1.1050243377685547]
[2025-05-27 11:03:58,283]: Mean: 0.87078387
[2025-05-27 11:03:58,283]: Min: 0.48180789
[2025-05-27 11:03:58,283]: Max: 1.29060555
[2025-05-27 11:03:58,284]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 11:03:58,285]: Sample Values (25 elements): [-0.034754443913698196, 0.034754443913698196, 0.034754443913698196, -0.034754443913698196, 0.06950888782739639, 0.0, -0.10426333546638489, -0.034754443913698196, 0.0, 0.10426333546638489, -0.06950888782739639, 0.034754443913698196, -0.10426333546638489, 0.10426333546638489, -0.034754443913698196, 0.034754443913698196, -0.06950888782739639, -0.06950888782739639, 0.06950888782739639, -0.034754443913698196, -0.034754443913698196, -0.06950888782739639, 0.0, 0.13901777565479279, -0.034754443913698196]
[2025-05-27 11:03:58,285]: Mean: -0.00760913
[2025-05-27 11:03:58,285]: Min: -0.27803555
[2025-05-27 11:03:58,285]: Max: 0.24328111
[2025-05-27 11:03:58,285]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 11:03:58,286]: Sample Values (25 elements): [0.9204027056694031, 0.7335954308509827, 0.8483957648277283, 0.7183252573013306, 0.7342134118080139, 0.5354135632514954, 0.7122392058372498, 0.9331620335578918, 0.6967839598655701, 0.9289870262145996, 0.9320263266563416, 0.7150200009346008, 0.8935456275939941, 0.951414167881012, 0.6978048086166382, 1.168269157409668, 0.9266476035118103, 0.7677871584892273, 0.6101023554801941, 1.0791785717010498, 0.9889596104621887, 0.722602903842926, 0.7995067238807678, 1.0246957540512085, 0.7641942501068115]
[2025-05-27 11:03:58,286]: Mean: 0.80118036
[2025-05-27 11:03:58,286]: Min: 0.53541356
[2025-05-27 11:03:58,286]: Max: 1.22883117
[2025-05-27 11:03:58,287]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 11:03:58,288]: Sample Values (25 elements): [-0.04588571563363075, -0.04588571563363075, 0.0, 0.0, 0.04588571563363075, 0.0, -0.04588571563363075, 0.04588571563363075, -0.04588571563363075, 0.0, 0.0, 0.0, 0.04588571563363075, 0.0, 0.0, 0.0, 0.0, 0.13765715062618256, 0.04588571563363075, 0.0917714312672615, 0.0917714312672615, 0.0917714312672615, 0.0, 0.0, -0.04588571563363075]
[2025-05-27 11:03:58,288]: Mean: -0.00351761
[2025-05-27 11:03:58,288]: Min: -0.41297144
[2025-05-27 11:03:58,289]: Max: 0.27531430
[2025-05-27 11:03:58,289]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 11:03:58,289]: Sample Values (25 elements): [0.8532510995864868, 1.0018471479415894, 1.1492799520492554, 0.951232373714447, 1.018025279045105, 0.7650200724601746, 0.7200206518173218, 0.8142700791358948, 1.0403025150299072, 0.7005914449691772, 0.8630743622779846, 0.7765951752662659, 0.702455997467041, 1.1581904888153076, 0.9029024243354797, 0.8155813813209534, 0.9600209593772888, 0.8608770966529846, 0.7774444222450256, 1.2687746286392212, 1.21027672290802, 0.8334785103797913, 0.6995975971221924, 0.8509030342102051, 0.8684881925582886]
[2025-05-27 11:03:58,289]: Mean: 0.90176368
[2025-05-27 11:03:58,290]: Min: 0.59281391
[2025-05-27 11:03:58,290]: Max: 1.26877463
[2025-05-27 11:03:58,291]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 11:03:58,292]: Sample Values (25 elements): [0.0, 0.0, -0.04606308415532112, 0.0, -0.04606308415532112, -0.04606308415532112, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04606308415532112, 0.04606308415532112, 0.0, 0.04606308415532112, 0.0, -0.04606308415532112, 0.0, 0.04606308415532112, 0.0, 0.0, 0.0, 0.0, -0.09212616831064224, 0.09212616831064224]
[2025-05-27 11:03:58,292]: Mean: -0.00196553
[2025-05-27 11:03:58,292]: Min: -0.36850467
[2025-05-27 11:03:58,292]: Max: 0.32244158
[2025-05-27 11:03:58,292]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 11:03:58,293]: Sample Values (25 elements): [0.6777565479278564, 0.5369868874549866, 0.6883037090301514, 0.5947002172470093, 0.7338413000106812, 1.1174050569534302, 0.7966310381889343, 0.3600374460220337, 0.7305965423583984, 0.8847401142120361, 0.880216121673584, 0.6402899622917175, 0.7326019406318665, 0.6503651142120361, 0.6892420053482056, 0.943513035774231, 0.7190302610397339, 0.8183282613754272, 0.48177140951156616, 0.6826264262199402, 0.43346694111824036, 0.6142265796661377, 0.5766108632087708, 0.6043118834495544, 0.9231497049331665]
[2025-05-27 11:03:58,293]: Mean: 0.68362200
[2025-05-27 11:03:58,293]: Min: 0.36003745
[2025-05-27 11:03:58,293]: Max: 1.17986858
[2025-05-27 11:03:58,295]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 11:03:58,295]: Sample Values (25 elements): [0.03879819065332413, 0.03879819065332413, 0.0, 0.0, -0.1551927626132965, 0.0, 0.0, 0.0, 0.03879819065332413, 0.0, 0.03879819065332413, -0.03879819065332413, 0.0, 0.0, 0.03879819065332413, -0.11639457195997238, 0.07759638130664825, -0.1551927626132965, -0.03879819065332413, -0.1551927626132965, 0.0, 0.03879819065332413, 0.0, 0.03879819065332413, -0.07759638130664825]
[2025-05-27 11:03:58,295]: Mean: -0.00455929
[2025-05-27 11:03:58,296]: Min: -0.27158734
[2025-05-27 11:03:58,296]: Max: 0.31038553
[2025-05-27 11:03:58,296]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 11:03:58,296]: Sample Values (25 elements): [0.8742471933364868, 0.6048526763916016, 0.8211122155189514, 0.9471254348754883, 0.8752216100692749, 0.8938136100769043, 0.8174880743026733, 0.7351166605949402, 0.81882244348526, 0.8902186155319214, 0.6939313411712646, 0.7332285642623901, 0.7994725108146667, 0.9457148909568787, 1.1662204265594482, 0.7202003598213196, 0.753237783908844, 1.1571484804153442, 0.8646408319473267, 0.7708082795143127, 0.8475065231323242, 0.8754168748855591, 0.8572137355804443, 0.8683964014053345, 0.7227135300636292]
[2025-05-27 11:03:58,296]: Mean: 0.80537748
[2025-05-27 11:03:58,297]: Min: 0.51350212
[2025-05-27 11:03:58,297]: Max: 1.16622043
[2025-05-27 11:03:58,298]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-27 11:03:58,299]: Sample Values (25 elements): [-0.06940855085849762, 0.0, 0.24292993545532227, 0.0, -0.10411282628774643, 0.03470427542924881, -0.03470427542924881, 0.0, -0.03470427542924881, -0.10411282628774643, 0.03470427542924881, -0.03470427542924881, -0.06940855085849762, 0.06940855085849762, -0.03470427542924881, 0.03470427542924881, -0.06940855085849762, 0.03470427542924881, -0.03470427542924881, -0.10411282628774643, 0.03470427542924881, 0.03470427542924881, -0.03470427542924881, 0.0, 0.03470427542924881]
[2025-05-27 11:03:58,299]: Mean: -0.00331283
[2025-05-27 11:03:58,299]: Min: -0.24292994
[2025-05-27 11:03:58,299]: Max: 0.27763420
[2025-05-27 11:03:58,299]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-27 11:03:58,299]: Sample Values (25 elements): [0.3071596026420593, 0.6167791485786438, 0.6448261737823486, 0.3565235137939453, 0.5111245512962341, 0.5976853966712952, 0.48337894678115845, 0.48280128836631775, 0.5797594785690308, 0.6407759785652161, 0.5334863662719727, 0.5673689246177673, 0.5954341888427734, 0.5734257698059082, 0.5689082741737366, 0.3979250192642212, 0.5118046402931213, 0.7670239210128784, 0.397676557302475, 0.7117957472801208, 0.5731085538864136, 0.643632709980011, 0.7893334627151489, 0.7515655755996704, 0.2483835220336914]
[2025-05-27 11:03:58,300]: Mean: 0.56868541
[2025-05-27 11:03:58,300]: Min: 0.13222288
[2025-05-27 11:03:58,300]: Max: 0.92332256
[2025-05-27 11:03:58,301]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 11:03:58,303]: Sample Values (25 elements): [-0.03654979541897774, -0.03654979541897774, -0.07309959083795547, 0.0, 0.0, 0.0, 0.07309959083795547, -0.07309959083795547, -0.03654979541897774, 0.0, -0.03654979541897774, -0.03654979541897774, 0.0, -0.07309959083795547, 0.0, 0.0, 0.0, 0.03654979541897774, -0.03654979541897774, -0.03654979541897774, 0.03654979541897774, 0.0, -0.03654979541897774, 0.07309959083795547, -0.03654979541897774]
[2025-05-27 11:03:58,303]: Mean: -0.00257412
[2025-05-27 11:03:58,303]: Min: -0.25584856
[2025-05-27 11:03:58,303]: Max: 0.29239836
[2025-05-27 11:03:58,303]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-27 11:03:58,304]: Sample Values (25 elements): [0.7210732102394104, 0.6735719442367554, 0.7513785362243652, 0.5109935402870178, 0.6838692426681519, 0.693848729133606, 0.7794188261032104, 0.7560787796974182, 0.7588712573051453, 0.7364286184310913, 0.6311503052711487, 0.6868897676467896, 0.6962558627128601, 0.9008346199989319, 0.6930549144744873, 0.7266882061958313, 0.8251284956932068, 0.6732125878334045, 0.4589313566684723, 0.508808970451355, 0.7374087572097778, 0.7098847031593323, 0.4192860722541809, 0.7949045896530151, 0.6950653791427612]
[2025-05-27 11:03:58,304]: Mean: 0.69753742
[2025-05-27 11:03:58,304]: Min: 0.32302532
[2025-05-27 11:03:58,304]: Max: 0.95103157
[2025-05-27 11:03:58,305]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-27 11:03:58,306]: Sample Values (25 elements): [-0.042842160910367966, -0.042842160910367966, -0.042842160910367966, -0.08568432182073593, 0.08568432182073593, 0.042842160910367966, 0.0, 0.0, -0.1285264790058136, -0.1285264790058136, 0.042842160910367966, -0.042842160910367966, -0.17136864364147186, 0.042842160910367966, -0.08568432182073593, -0.042842160910367966, 0.08568432182073593, 0.042842160910367966, -0.17136864364147186, -0.042842160910367966, 0.08568432182073593, -0.1285264790058136, 0.08568432182073593, -0.042842160910367966, 0.0]
[2025-05-27 11:03:58,306]: Mean: -0.00258350
[2025-05-27 11:03:58,306]: Min: -0.29989514
[2025-05-27 11:03:58,306]: Max: 0.34273729
[2025-05-27 11:03:58,306]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-27 11:03:58,307]: Sample Values (25 elements): [0.3667733669281006, 0.5749959945678711, 0.6562987565994263, 0.5526429414749146, 0.5063360929489136, 0.287683367729187, 0.7189737558364868, 0.4408106803894043, 0.753333568572998, 0.5896706581115723, 0.5422868132591248, 0.4415247142314911, 0.41855451464653015, 0.6735188961029053, 0.5600454211235046, 0.624491274356842, 0.600982666015625, 0.5405320525169373, 0.5388137102127075, 0.5392180681228638, 0.4889293611049652, 0.7906433343887329, 0.5334656238555908, 0.5979676246643066, 0.6800259947776794]
[2025-05-27 11:03:58,307]: Mean: 0.55423844
[2025-05-27 11:03:58,307]: Min: 0.28768337
[2025-05-27 11:03:58,307]: Max: 0.88392651
[2025-05-27 11:03:58,308]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 11:03:58,310]: Sample Values (25 elements): [0.0, 0.0, -0.03613973408937454, 0.0, 0.0, 0.0, 0.07227946817874908, 0.03613973408937454, -0.14455893635749817, -0.03613973408937454, 0.0, -0.03613973408937454, 0.03613973408937454, 0.0, 0.0, 0.0, 0.03613973408937454, -0.10841920226812363, 0.0, 0.0, 0.0, 0.0, 0.03613973408937454, 0.0, 0.07227946817874908]
[2025-05-27 11:03:58,310]: Mean: -0.00323590
[2025-05-27 11:03:58,310]: Min: -0.28911787
[2025-05-27 11:03:58,311]: Max: 0.25297815
[2025-05-27 11:03:58,311]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-27 11:03:58,311]: Sample Values (25 elements): [0.29928675293922424, 0.2993653416633606, 0.0025267046876251698, 0.696656346321106, 9.947209764504805e-05, 0.5336756110191345, 0.743171215057373, 0.58941251039505, 0.48331353068351746, 0.5233180522918701, 0.4999701976776123, 0.9133577942848206, 0.01997199095785618, 0.6335703134536743, 0.6269490718841553, 0.48989853262901306, 0.4698019325733185, 0.45277732610702515, 0.21925389766693115, 0.5386775135993958, 0.5363826155662537, 0.5310602188110352, 0.34628164768218994, 0.17752453684806824, 0.48668742179870605]
[2025-05-27 11:03:58,311]: Mean: 0.51534247
[2025-05-27 11:03:58,311]: Min: -0.00000000
[2025-05-27 11:03:58,311]: Max: 0.91335779
[2025-05-27 11:03:58,313]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 11:03:58,314]: Sample Values (25 elements): [-0.03280984237790108, 0.03280984237790108, 0.06561968475580215, -0.03280984237790108, -0.03280984237790108, -0.03280984237790108, -0.03280984237790108, -0.03280984237790108, 0.0, 0.06561968475580215, 0.03280984237790108, 0.0, 0.0, -0.03280984237790108, 0.03280984237790108, 0.03280984237790108, 0.0, 0.06561968475580215, 0.0, 0.0, 0.0, -0.03280984237790108, 0.0, 0.0, -0.03280984237790108]
[2025-05-27 11:03:58,314]: Mean: -0.00230805
[2025-05-27 11:03:58,315]: Min: -0.22966890
[2025-05-27 11:03:58,315]: Max: 0.26247874
[2025-05-27 11:03:58,315]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-27 11:03:58,315]: Sample Values (25 elements): [0.4330862760543823, 0.700933039188385, 0.37276655435562134, 0.6008381843566895, 0.5697774887084961, 0.3683573603630066, 0.6535162329673767, 0.7415449023246765, 0.5297229290008545, 0.6167156100273132, 0.4605778455734253, 0.48694908618927, 0.5596977472305298, 0.593054473400116, 0.47563451528549194, 0.6794028282165527, 0.6031334400177002, 0.631719172000885, 0.7454860210418701, 0.4553031325340271, 0.43965351581573486, 0.5133540630340576, 0.5033248066902161, 0.457536518573761, 0.588302731513977]
[2025-05-27 11:03:58,315]: Mean: 0.55302405
[2025-05-27 11:03:58,316]: Min: 0.11549759
[2025-05-27 11:03:58,316]: Max: 0.85939062
[2025-05-27 11:03:58,317]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-27 11:03:58,321]: Sample Values (25 elements): [0.0, 0.1104448139667511, 0.0, 0.0, -0.036814939230680466, 0.036814939230680466, -0.036814939230680466, -0.07362987846136093, 0.036814939230680466, 0.0, 0.0, -0.036814939230680466, 0.0, 0.0, 0.0, -0.036814939230680466, 0.0, -0.036814939230680466, 0.0, 0.036814939230680466, 0.036814939230680466, 0.0, 0.036814939230680466, -0.07362987846136093, -0.036814939230680466]
[2025-05-27 11:03:58,321]: Mean: -0.00091341
[2025-05-27 11:03:58,321]: Min: -0.25770459
[2025-05-27 11:03:58,321]: Max: 0.29451951
[2025-05-27 11:03:58,321]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-27 11:03:58,322]: Sample Values (25 elements): [0.7954798936843872, 0.5379812717437744, 0.6472052931785583, 0.49104082584381104, 0.14684037864208221, 0.05767246335744858, 0.4990319311618805, 0.6237683892250061, 0.5500555038452148, 0.34880051016807556, 0.5215691924095154, 0.2513517737388611, 0.5790000557899475, 0.6535197496414185, 0.5271063446998596, 0.7143473625183105, 0.6730536222457886, 0.4747341275215149, 0.11337732523679733, 0.4085977077484131, 0.3114660382270813, 0.634432315826416, 0.6105237603187561, 0.5563904643058777, 0.47212839126586914]
[2025-05-27 11:03:58,322]: Mean: 0.38071778
[2025-05-27 11:03:58,322]: Min: -0.00000000
[2025-05-27 11:03:58,322]: Max: 0.91162252
[2025-05-27 11:03:58,323]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 11:03:58,330]: Sample Values (25 elements): [0.0, -0.03376952186226845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03376952186226845, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03376952186226845, 0.0, -0.03376952186226845, 0.0, 0.0, -0.03376952186226845, 0.0, 0.0, 0.03376952186226845, 0.0, 0.0, -0.03376952186226845]
[2025-05-27 11:03:58,330]: Mean: -0.00076840
[2025-05-27 11:03:58,330]: Min: -0.23638666
[2025-05-27 11:03:58,330]: Max: 0.27015617
[2025-05-27 11:03:58,331]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-27 11:03:58,331]: Sample Values (25 elements): [0.5943455100059509, 0.6425995826721191, 0.5145252346992493, 0.6302534341812134, 0.5928294062614441, -4.906366313140482e-41, 0.6707970499992371, 0.5704837441444397, 0.5439724922180176, 0.25876155495643616, 0.6442576050758362, 0.708487868309021, 0.5242026448249817, 0.6912880539894104, 0.7159326076507568, 0.3486890196800232, 0.21808288991451263, 0.4982262849807739, 0.0011734729632735252, 0.6288822293281555, 0.7410136461257935, 0.6373034715652466, 6.063140212120288e-09, 0.15339259803295135, 0.29041656851768494]
[2025-05-27 11:03:58,331]: Mean: 0.46634662
[2025-05-27 11:03:58,331]: Min: -0.00000121
[2025-05-27 11:03:58,331]: Max: 0.93145734
[2025-05-27 11:03:58,332]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-27 11:03:58,333]: Sample Values (25 elements): [0.0, 0.0, -0.03639153391122818, -0.07278306782245636, -0.03639153391122818, 0.03639153391122818, 0.07278306782245636, -0.03639153391122818, -0.03639153391122818, 0.0, -0.03639153391122818, 0.0, 0.0, 0.0, -0.10917460173368454, -0.03639153391122818, 0.0, 0.03639153391122818, 0.0, 0.0, -0.03639153391122818, -0.07278306782245636, 0.0, -0.03639153391122818, 0.07278306782245636]
[2025-05-27 11:03:58,333]: Mean: -0.00435237
[2025-05-27 11:03:58,333]: Min: -0.29113227
[2025-05-27 11:03:58,333]: Max: 0.25474074
[2025-05-27 11:03:58,334]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-27 11:03:58,334]: Sample Values (25 elements): [0.3925513029098511, 0.11694611608982086, 0.40619930624961853, 0.4015219807624817, 0.22625622153282166, 0.39351844787597656, 0.4379134476184845, 7.7895853149299875e-25, 0.34424662590026855, 0.28894177079200745, 0.25525858998298645, 0.3226982653141022, 0.14873896539211273, 0.51549232006073, 0.4212924838066101, 0.42966052889823914, 0.296638548374176, 0.3940737247467041, 0.4065847098827362, 0.6509121060371399, 0.4616340100765228, 0.5304228663444519, 0.39981764554977417, 0.44759926199913025, 0.2707339823246002]
[2025-05-27 11:03:58,334]: Mean: 0.31948105
[2025-05-27 11:03:58,334]: Min: 0.00000000
[2025-05-27 11:03:58,334]: Max: 0.65091211
[2025-05-27 11:03:58,336]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 11:03:58,344]: Sample Values (25 elements): [0.09518791735172272, 0.0, 0.031729307025671005, 0.031729307025671005, 0.0, 0.0, -0.031729307025671005, 0.0, 0.0, -0.06345861405134201, 0.0, 0.0, 0.0, 0.0, -0.031729307025671005, -0.031729307025671005, 0.0, 0.0, 0.031729307025671005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 11:03:58,345]: Mean: -0.00087551
[2025-05-27 11:03:58,345]: Min: -0.22210515
[2025-05-27 11:03:58,345]: Max: 0.25383446
[2025-05-27 11:03:58,345]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-27 11:03:58,345]: Sample Values (25 elements): [-5.705526827344925e-41, 0.7210075855255127, 0.502260148525238, 0.39539793133735657, 0.2564985156059265, 0.26635992527008057, -5.298169363765701e-41, 0.4236590266227722, 0.5781583786010742, 5.261875733539688e-41, 0.0007882870850153267, 0.5474309325218201, 0.0016950235003605485, 0.5677124857902527, 0.09225548058748245, 0.5638138651847839, 5.060509144216212e-41, -5.26874209601488e-41, -6.088922087184195e-41, 0.5003876090049744, 0.48957791924476624, -6.161229087943356e-41, -6.10559753890966e-41, 0.3635416626930237, 0.0016012624837458134]
[2025-05-27 11:03:58,345]: Mean: 0.20629837
[2025-05-27 11:03:58,346]: Min: -0.00000000
[2025-05-27 11:03:58,346]: Max: 0.74507594
[2025-05-27 11:03:58,347]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 11:03:58,353]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.030251294374465942, 0.0, 0.030251294374465942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.060502588748931885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 11:03:58,354]: Mean: -0.00068060
[2025-05-27 11:03:58,354]: Min: -0.21175906
[2025-05-27 11:03:58,354]: Max: 0.24201035
[2025-05-27 11:03:58,354]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-27 11:03:58,354]: Sample Values (25 elements): [-1.0384346715852644e-09, 0.2354087233543396, 0.08007709681987762, 0.5698191523551941, 0.060314226895570755, 1.3496351130015682e-06, 0.29389989376068115, 0.3209055960178375, 0.5412306189537048, 0.2662806510925293, 0.39351025223731995, 0.18100014328956604, -3.6986291807039606e-10, -5.607015545302891e-41, 0.02197839319705963, 3.0300443540909328e-06, 0.10502580553293228, 0.48280924558639526, 0.11873998492956161, 0.3663930892944336, 0.2636014521121979, 0.5812259912490845, 0.10651964694261551, 1.5087692872839398e-07, 0.17148524522781372]
[2025-05-27 11:03:58,355]: Mean: 0.28896272
[2025-05-27 11:03:58,355]: Min: -0.01184713
[2025-05-27 11:03:58,355]: Max: 0.91335267
[2025-05-27 11:03:58,356]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-27 11:03:58,375]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06865417212247849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06865417212247849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 11:03:58,375]: Mean: -0.00012213
[2025-05-27 11:03:58,375]: Min: -0.20596251
[2025-05-27 11:03:58,376]: Max: 0.30894378
[2025-05-27 11:03:58,376]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-27 11:03:58,376]: Sample Values (25 elements): [-5.889096926171476e-41, -6.252033228431604e-41, 6.177344020283091e-41, -4.937054749509196e-41, 5.125949782500181e-41, 0.7443015575408936, 0.4510175883769989, 5.82267537896248e-41, 6.102094292748848e-41, 5.733973186170719e-41, -6.027124824907471e-41, 5.717998383677416e-41, 0.3349398672580719, 4.917997090394378e-41, 5.186626006005445e-41, -5.473331671806303e-41, 5.258512617225309e-41, 5.516771924200372e-41, 5.429190770180071e-41, 0.13181793689727783, 0.31459394097328186, -5.275608458490071e-41, -5.624251516414086e-41, 6.220644142830728e-41, 5.24449963258206e-41]
[2025-05-27 11:03:58,376]: Mean: 0.07647812
[2025-05-27 11:03:58,376]: Min: -0.00000000
[2025-05-27 11:03:58,377]: Max: 0.93306452
[2025-05-27 11:03:58,378]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 11:03:58,423]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03694315627217293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018471578136086464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018471578136086464, 0.0]
[2025-05-27 11:03:58,423]: Mean: 0.00006389
[2025-05-27 11:03:58,424]: Min: -0.11082947
[2025-05-27 11:03:58,424]: Max: 0.16624421
[2025-05-27 11:03:58,424]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-27 11:03:58,424]: Sample Values (25 elements): [0.41483405232429504, 0.2734658122062683, 0.4009025990962982, 1.3423453459893153e-08, 0.33515238761901855, 5.0036353162756546e-21, 0.7685445547103882, 0.19347266852855682, 0.30152636766433716, 0.5162484049797058, 0.4446248412132263, 0.5356386303901672, 0.4856176972389221, 0.23492775857448578, 0.41122543811798096, 0.4562648832798004, 0.31284773349761963, 5.249219947600636e-10, 0.6538448333740234, 0.18389351665973663, 0.5490233898162842, 0.5424326658248901, -6.5780235403814755e-19, -5.987888467906376e-41, 0.09865593910217285]
[2025-05-27 11:03:58,425]: Mean: 0.37807781
[2025-05-27 11:03:58,425]: Min: -0.00000000
[2025-05-27 11:03:58,425]: Max: 0.86818939
[2025-05-27 11:03:58,426]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-27 11:03:58,428]: Sample Values (25 elements): [0.02265078015625477, 0.02265078015625477, -0.09060312062501907, 0.0, -0.02265078015625477, 0.0, 0.04530156031250954, -0.02265078015625477, -0.02265078015625477, 0.0, -0.02265078015625477, 0.0, 0.0, -0.02265078015625477, 0.04530156031250954, 0.0, 0.0, -0.02265078015625477, 0.0, 0.0, 0.0, 0.0, 0.0, -0.02265078015625477, 0.04530156031250954]
[2025-05-27 11:03:58,428]: Mean: 0.00038934
[2025-05-27 11:03:58,428]: Min: -0.13590468
[2025-05-27 11:03:58,428]: Max: 0.20385702
[2025-05-27 11:03:58,428]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-27 11:03:58,429]: Sample Values (25 elements): [0.36096662282943726, 0.35695046186447144, 0.5583274960517883, 0.0011560155544430017, 0.2795112133026123, 0.36341768503189087, 0.6085991859436035, 4.047218382315368e-17, 4.880744697288719e-08, 0.42266377806663513, 0.45977139472961426, 0.0006301022367551923, 0.538390576839447, 6.105096176033552e-21, 0.38063475489616394, 0.46749448776245117, 0.6766437292098999, 0.4273269474506378, 0.302314430475235, 0.6307087540626526, 0.516121506690979, 0.1794814020395279, 0.293556809425354, 0.31883540749549866, 1.1708284515521417e-11]
[2025-05-27 11:03:58,429]: Mean: 0.39084512
[2025-05-27 11:03:58,429]: Min: -0.00000000
[2025-05-27 11:03:58,429]: Max: 0.73815757
[2025-05-27 11:03:58,430]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 11:03:58,471]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 11:03:58,472]: Mean: 0.00000509
[2025-05-27 11:03:58,472]: Min: -0.03478906
[2025-05-27 11:03:58,472]: Max: 0.06957813
[2025-05-27 11:03:58,472]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-27 11:03:58,472]: Sample Values (25 elements): [5.254588981525199e-41, 4.907347222065509e-41, 5.554326723044277e-41, -5.175275488444414e-41, 5.228384700242325e-41, -5.832624598059186e-41, 5.365011300513995e-41, 6.136706364817671e-41, -5.925951075783219e-41, 5.640086189060956e-41, 6.14511415560362e-41, 0.06555847823619843, -6.234376867781111e-41, -5.205263275580965e-41, 5.680443584833511e-41, -5.774330581943274e-41, 5.052802002662425e-41, -6.290288676507671e-41, 5.016788632129278e-41, -5.196014705716422e-41, -5.128051730196668e-41, -5.243238463964168e-41, 4.915755012851458e-41, 5.55222477534779e-41, 5.964206523859286e-41]
[2025-05-27 11:03:58,472]: Mean: 0.00550479
[2025-05-27 11:03:58,473]: Min: -0.00000000
[2025-05-27 11:03:58,473]: Max: 0.39306724
[2025-05-27 11:03:58,474]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 11:03:58,515]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 11:03:58,515]: Mean: -0.00002151
[2025-05-27 11:03:58,516]: Min: -0.04995641
[2025-05-27 11:03:58,516]: Max: 0.05709304
[2025-05-27 11:03:58,516]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-27 11:03:58,518]: Sample Values (25 elements): [3.3005455124990423e-24, 0.24156787991523743, 0.047468122094869614, 0.12587349116802216, 0.14518818259239197, 3.058882628529519e-13, 6.044138700644908e-08, 0.15257756412029266, 7.67614594110455e-09, 0.02840110845863819, 0.18568338453769684, 0.1412750780582428, 0.1364704966545105, 0.03982704132795334, 0.11349394917488098, 0.01116962730884552, -0.02311270497739315, 0.1378525346517563, 2.832538394366974e-17, 0.07942190021276474, 0.07079704850912094, 0.03284488990902901, 3.470531074736148e-22, -1.6295357482132159e-16, 0.10201522707939148]
[2025-05-27 11:03:58,518]: Mean: 0.08615161
[2025-05-27 11:03:58,518]: Min: -0.03973196
[2025-05-27 11:03:58,518]: Max: 0.36505154
[2025-05-27 11:03:58,518]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-27 11:03:58,519]: Sample Values (25 elements): [0.030936574563384056, -0.03650197759270668, 0.06828499585390091, 0.07303034514188766, -0.11868752539157867, -5.426808562790719e-41, -0.10006029903888702, -4.3600817400957226e-11, 0.03423750773072243, -0.08677852153778076, 0.11417835205793381, 0.20868399739265442, 0.015982922166585922, -0.06466864794492722, 0.08749153465032578, -0.06849554926156998, -0.04683580622076988, -0.0069641368463635445, -0.04599463939666748, -0.0018652020953595638, -0.19721531867980957, 0.09243091940879822, -0.02154449187219143, 0.01467271987348795, 0.038913048803806305]
[2025-05-27 11:03:58,519]: Mean: -0.02170754
[2025-05-27 11:03:58,519]: Min: -0.41821975
[2025-05-27 11:03:58,519]: Max: 0.34237996
[2025-05-27 11:03:58,519]: 


QAT of ResNet18 with relu down to 3 bits...
[2025-05-27 11:03:58,752]: [ResNet18_relu_quantized_3_bits] after configure_qat:
[2025-05-27 11:03:58,796]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 11:05:36,771]: [ResNet18_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.5111 Train Acc: 0.8248 Eval Loss: 0.4890 Eval Acc: 0.8403 (LR: 0.00100000)
[2025-05-27 11:07:14,862]: [ResNet18_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.4370 Train Acc: 0.8476 Eval Loss: 0.5032 Eval Acc: 0.8391 (LR: 0.00100000)
[2025-05-27 11:08:52,802]: [ResNet18_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.4447 Train Acc: 0.8449 Eval Loss: 0.5495 Eval Acc: 0.8224 (LR: 0.00100000)
[2025-05-27 11:10:30,702]: [ResNet18_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.4410 Train Acc: 0.8475 Eval Loss: 0.5397 Eval Acc: 0.8222 (LR: 0.00100000)
[2025-05-27 11:12:08,650]: [ResNet18_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.4474 Train Acc: 0.8454 Eval Loss: 0.5570 Eval Acc: 0.8217 (LR: 0.00100000)
[2025-05-27 11:13:46,591]: [ResNet18_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.4453 Train Acc: 0.8458 Eval Loss: 0.5329 Eval Acc: 0.8287 (LR: 0.00100000)
[2025-05-27 11:15:24,479]: [ResNet18_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.4397 Train Acc: 0.8479 Eval Loss: 0.6890 Eval Acc: 0.7853 (LR: 0.00010000)
[2025-05-27 11:17:02,428]: [ResNet18_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.3472 Train Acc: 0.8805 Eval Loss: 0.3445 Eval Acc: 0.8834 (LR: 0.00010000)
[2025-05-27 11:18:40,393]: [ResNet18_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.3087 Train Acc: 0.8926 Eval Loss: 0.3432 Eval Acc: 0.8860 (LR: 0.00010000)
[2025-05-27 11:20:18,059]: [ResNet18_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.2920 Train Acc: 0.8983 Eval Loss: 0.3318 Eval Acc: 0.8908 (LR: 0.00010000)
[2025-05-27 11:21:55,784]: [ResNet18_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.2850 Train Acc: 0.9006 Eval Loss: 0.3285 Eval Acc: 0.8918 (LR: 0.00010000)
[2025-05-27 11:23:33,714]: [ResNet18_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.2779 Train Acc: 0.9048 Eval Loss: 0.3229 Eval Acc: 0.8930 (LR: 0.00010000)
[2025-05-27 11:25:11,637]: [ResNet18_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.2671 Train Acc: 0.9070 Eval Loss: 0.3338 Eval Acc: 0.8914 (LR: 0.00010000)
[2025-05-27 11:26:49,561]: [ResNet18_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.2660 Train Acc: 0.9074 Eval Loss: 0.3171 Eval Acc: 0.8969 (LR: 0.00010000)
[2025-05-27 11:28:27,463]: [ResNet18_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.2584 Train Acc: 0.9094 Eval Loss: 0.3267 Eval Acc: 0.8938 (LR: 0.00010000)
[2025-05-27 11:30:05,211]: [ResNet18_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.2523 Train Acc: 0.9125 Eval Loss: 0.3131 Eval Acc: 0.8975 (LR: 0.00010000)
[2025-05-27 11:31:43,167]: [ResNet18_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.2507 Train Acc: 0.9130 Eval Loss: 0.3240 Eval Acc: 0.8956 (LR: 0.00010000)
[2025-05-27 11:33:21,035]: [ResNet18_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.2481 Train Acc: 0.9120 Eval Loss: 0.3219 Eval Acc: 0.8942 (LR: 0.00010000)
[2025-05-27 11:34:58,760]: [ResNet18_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.2421 Train Acc: 0.9166 Eval Loss: 0.3112 Eval Acc: 0.8995 (LR: 0.00010000)
[2025-05-27 11:36:36,899]: [ResNet18_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.2363 Train Acc: 0.9173 Eval Loss: 0.3146 Eval Acc: 0.8993 (LR: 0.00010000)
[2025-05-27 11:38:15,025]: [ResNet18_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.2377 Train Acc: 0.9171 Eval Loss: 0.3134 Eval Acc: 0.8974 (LR: 0.00010000)
[2025-05-27 11:39:53,147]: [ResNet18_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.2308 Train Acc: 0.9203 Eval Loss: 0.3154 Eval Acc: 0.8983 (LR: 0.00010000)
[2025-05-27 11:41:31,048]: [ResNet18_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.2363 Train Acc: 0.9168 Eval Loss: 0.3218 Eval Acc: 0.8964 (LR: 0.00010000)
[2025-05-27 11:43:08,767]: [ResNet18_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.2246 Train Acc: 0.9213 Eval Loss: 0.3198 Eval Acc: 0.8991 (LR: 0.00010000)
[2025-05-27 11:44:46,476]: [ResNet18_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.2243 Train Acc: 0.9209 Eval Loss: 0.3101 Eval Acc: 0.9020 (LR: 0.00010000)
[2025-05-27 11:46:24,200]: [ResNet18_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.2229 Train Acc: 0.9209 Eval Loss: 0.3060 Eval Acc: 0.9006 (LR: 0.00010000)
[2025-05-27 11:48:02,135]: [ResNet18_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.2213 Train Acc: 0.9218 Eval Loss: 0.3175 Eval Acc: 0.8988 (LR: 0.00010000)
[2025-05-27 11:49:40,028]: [ResNet18_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.2217 Train Acc: 0.9225 Eval Loss: 0.3173 Eval Acc: 0.8963 (LR: 0.00010000)
[2025-05-27 11:51:17,720]: [ResNet18_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.2180 Train Acc: 0.9245 Eval Loss: 0.3208 Eval Acc: 0.9014 (LR: 0.00010000)
[2025-05-27 11:52:55,295]: [ResNet18_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.2137 Train Acc: 0.9246 Eval Loss: 0.3374 Eval Acc: 0.8918 (LR: 0.00010000)
[2025-05-27 11:54:33,195]: [ResNet18_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.2153 Train Acc: 0.9247 Eval Loss: 0.3128 Eval Acc: 0.8995 (LR: 0.00010000)
[2025-05-27 11:56:11,132]: [ResNet18_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.2133 Train Acc: 0.9245 Eval Loss: 0.3297 Eval Acc: 0.8996 (LR: 0.00001000)
[2025-05-27 11:57:49,001]: [ResNet18_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.1953 Train Acc: 0.9323 Eval Loss: 0.2913 Eval Acc: 0.9072 (LR: 0.00001000)
[2025-05-27 11:59:26,497]: [ResNet18_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.1872 Train Acc: 0.9329 Eval Loss: 0.2858 Eval Acc: 0.9083 (LR: 0.00001000)
[2025-05-27 12:01:04,021]: [ResNet18_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.1841 Train Acc: 0.9362 Eval Loss: 0.2982 Eval Acc: 0.9057 (LR: 0.00001000)
[2025-05-27 12:02:41,534]: [ResNet18_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.1822 Train Acc: 0.9358 Eval Loss: 0.2987 Eval Acc: 0.9041 (LR: 0.00001000)
[2025-05-27 12:04:18,849]: [ResNet18_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.1831 Train Acc: 0.9360 Eval Loss: 0.3019 Eval Acc: 0.9039 (LR: 0.00001000)
[2025-05-27 12:05:56,377]: [ResNet18_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.1803 Train Acc: 0.9365 Eval Loss: 0.2927 Eval Acc: 0.9103 (LR: 0.00001000)
[2025-05-27 12:07:47,713]: [ResNet18_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.1781 Train Acc: 0.9386 Eval Loss: 0.2903 Eval Acc: 0.9082 (LR: 0.00001000)
[2025-05-27 12:09:25,517]: [ResNet18_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.1823 Train Acc: 0.9354 Eval Loss: 0.2982 Eval Acc: 0.9070 (LR: 0.00000100)
[2025-05-27 12:11:03,544]: [ResNet18_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.1729 Train Acc: 0.9404 Eval Loss: 0.2917 Eval Acc: 0.9071 (LR: 0.00000100)
[2025-05-27 12:12:41,282]: [ResNet18_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.1728 Train Acc: 0.9387 Eval Loss: 0.2857 Eval Acc: 0.9095 (LR: 0.00000100)
[2025-05-27 12:14:18,960]: [ResNet18_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.1738 Train Acc: 0.9393 Eval Loss: 0.3034 Eval Acc: 0.9048 (LR: 0.00000100)
[2025-05-27 12:15:56,718]: [ResNet18_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.1759 Train Acc: 0.9391 Eval Loss: 0.2970 Eval Acc: 0.9084 (LR: 0.00000100)
[2025-05-27 12:17:34,190]: [ResNet18_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.1746 Train Acc: 0.9384 Eval Loss: 0.2983 Eval Acc: 0.9072 (LR: 0.00000100)
[2025-05-27 12:19:11,506]: [ResNet18_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.1725 Train Acc: 0.9397 Eval Loss: 0.2838 Eval Acc: 0.9085 (LR: 0.00000100)
[2025-05-27 12:20:48,796]: [ResNet18_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.1744 Train Acc: 0.9392 Eval Loss: 0.2893 Eval Acc: 0.9079 (LR: 0.00000100)
[2025-05-27 12:22:26,112]: [ResNet18_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.1732 Train Acc: 0.9396 Eval Loss: 0.2984 Eval Acc: 0.9082 (LR: 0.00000100)
[2025-05-27 12:24:03,411]: [ResNet18_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.1734 Train Acc: 0.9393 Eval Loss: 0.2856 Eval Acc: 0.9100 (LR: 0.00000100)
[2025-05-27 12:25:40,758]: [ResNet18_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.1716 Train Acc: 0.9400 Eval Loss: 0.2910 Eval Acc: 0.9089 (LR: 0.00000100)
[2025-05-27 12:27:18,513]: [ResNet18_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.1748 Train Acc: 0.9394 Eval Loss: 0.2901 Eval Acc: 0.9086 (LR: 0.00000100)
[2025-05-27 12:28:56,215]: [ResNet18_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.1742 Train Acc: 0.9392 Eval Loss: 0.2915 Eval Acc: 0.9088 (LR: 0.00000010)
[2025-05-27 12:30:33,744]: [ResNet18_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.1690 Train Acc: 0.9408 Eval Loss: 0.2942 Eval Acc: 0.9092 (LR: 0.00000010)
[2025-05-27 12:32:11,216]: [ResNet18_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.1697 Train Acc: 0.9414 Eval Loss: 0.2890 Eval Acc: 0.9104 (LR: 0.00000010)
[2025-05-27 12:33:48,787]: [ResNet18_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.1708 Train Acc: 0.9401 Eval Loss: 0.2938 Eval Acc: 0.9106 (LR: 0.00000010)
[2025-05-27 12:35:26,340]: [ResNet18_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.1735 Train Acc: 0.9394 Eval Loss: 0.2964 Eval Acc: 0.9106 (LR: 0.00000010)
[2025-05-27 12:37:03,452]: [ResNet18_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.1739 Train Acc: 0.9398 Eval Loss: 0.2907 Eval Acc: 0.9062 (LR: 0.00000010)
[2025-05-27 12:38:40,936]: [ResNet18_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.1738 Train Acc: 0.9390 Eval Loss: 0.2899 Eval Acc: 0.9061 (LR: 0.00000010)
[2025-05-27 12:40:18,235]: [ResNet18_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.1711 Train Acc: 0.9401 Eval Loss: 0.2959 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-05-27 12:41:55,973]: [ResNet18_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.1696 Train Acc: 0.9409 Eval Loss: 0.2905 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-05-27 12:41:55,974]: [ResNet18_relu_quantized_3_bits] Best Eval Accuracy: 0.9106
[2025-05-27 12:41:56,064]: 


Quantization of model down to 3 bits finished
[2025-05-27 12:41:56,064]: Model Architecture:
[2025-05-27 12:41:56,110]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.0664], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=21.464719772338867)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0733], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31111985445022583, max_val=0.2021678388118744)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3984], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.788769721984863)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0853], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3266814351081848, max_val=0.2702423930168152)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.9123], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=27.386281967163086)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0970], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3843231201171875, max_val=0.29501354694366455)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0260], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.1817474365234375)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0711], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22571347653865814, max_val=0.27227354049682617)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.9218], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=27.452455520629883)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0701], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21454013884067535, max_val=0.2760261595249176)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8438], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.906651973724365)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0677], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19988635182380676, max_val=0.2742404341697693)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3267001509666443, max_val=0.33911651372909546)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6083], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=11.258028030395508)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0725], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25238704681396484, max_val=0.2553715705871582)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9556], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.6894049644470215)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0599], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17923250794410706, max_val=0.2403680682182312)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9104], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.372516632080078)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0652], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17489376664161682, max_val=0.28133612871170044)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8537], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.976192951202393)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0706], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24541237950325012, max_val=0.2489110827445984)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0747], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2538719177246094, max_val=0.2691289186477661)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4658], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.260824203491211)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21377508342266083, max_val=0.23259040713310242)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6540], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.577786922454834)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0550], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18036144971847534, max_val=0.20490598678588867)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8128], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.689327239990234)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0671], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16857197880744934, max_val=0.30089327692985535)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2172], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.520082473754883)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0457], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10545825958251953, max_val=0.21443378925323486)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13182705640792847, max_val=0.22097092866897583)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.7971], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.579887390136719)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0184], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05093488097190857, max_val=0.07768811285495758)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3987], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.7905688285827637)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0197], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06473054736852646, max_val=0.0728878527879715)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8567], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.996618270874023)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 12:41:56,111]: 
Model Weights:
[2025-05-27 12:41:56,111]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-27 12:41:56,111]: Sample Values (25 elements): [0.0021717334166169167, -0.16194981336593628, -0.06378553062677383, 0.021914098411798477, 0.014589629136025906, 0.011960283853113651, -0.12197405844926834, 0.04749925062060356, -0.015942486003041267, 0.2020760476589203, 0.17184965312480927, 0.20922806859016418, -0.021059853956103325, -0.024898406118154526, -0.2501303255558014, 0.006754725705832243, 0.042431265115737915, -0.16262421011924744, -0.06276358664035797, -0.0998091995716095, -0.10110541433095932, -0.1400475800037384, -0.18732240796089172, -0.20900890231132507, 0.008152296766638756]
[2025-05-27 12:41:56,111]: Mean: -0.00163819
[2025-05-27 12:41:56,111]: Min: -0.33247048
[2025-05-27 12:41:56,111]: Max: 0.34732375
[2025-05-27 12:41:56,111]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-27 12:41:56,112]: Sample Values (25 elements): [1.2513346672058105, 1.6296294927597046, 1.021741271018982, 0.8061893582344055, 0.9654980897903442, 1.542100429534912, 1.1680666208267212, 1.1174771785736084, 0.8068438172340393, 1.438262701034546, 0.6928837895393372, 1.0943241119384766, 1.3532253503799438, 1.2164403200149536, 1.0735247135162354, 0.8253563642501831, 1.357837200164795, 0.8850020170211792, 1.0202618837356567, 0.632773220539093, 1.31276535987854, 1.5109175443649292, 0.8784722685813904, 1.3514751195907593, 1.0014901161193848]
[2025-05-27 12:41:56,112]: Mean: 1.16034496
[2025-05-27 12:41:56,112]: Min: 0.63277322
[2025-05-27 12:41:56,112]: Max: 1.79189694
[2025-05-27 12:41:56,113]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 12:41:56,114]: Sample Values (25 elements): [0.0, 0.0, 0.14665363729000092, 0.0, 0.0, 0.0, 0.0, -0.07332681864500046, 0.0, 0.0, -0.07332681864500046, 0.0, 0.0, 0.07332681864500046, -0.07332681864500046, 0.07332681864500046, 0.0, 0.0, 0.0, 0.07332681864500046, 0.07332681864500046, -0.14665363729000092, 0.07332681864500046, 0.0, 0.0]
[2025-05-27 12:41:56,114]: Mean: -0.00596934
[2025-05-27 12:41:56,114]: Min: -0.29330727
[2025-05-27 12:41:56,114]: Max: 0.21998045
[2025-05-27 12:41:56,114]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 12:41:56,115]: Sample Values (25 elements): [0.7829363942146301, 0.797444760799408, 1.0898956060409546, 0.9422186613082886, 0.9164027571678162, 0.812163770198822, 0.6329445242881775, 0.9296066761016846, 0.8761129379272461, 0.8584017753601074, 0.8456864953041077, 0.8999359607696533, 0.7452977299690247, 0.7246119379997253, 1.1147146224975586, 0.6835120916366577, 0.9772958159446716, 0.8274276852607727, 0.9434943795204163, 0.9717351794242859, 0.820360541343689, 0.816905677318573, 1.1066101789474487, 0.7775408625602722, 0.9650607705116272]
[2025-05-27 12:41:56,115]: Mean: 0.87792939
[2025-05-27 12:41:56,115]: Min: 0.63294452
[2025-05-27 12:41:56,115]: Max: 1.24464524
[2025-05-27 12:41:56,116]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 12:41:56,116]: Sample Values (25 elements): [0.0, 0.085274837911129, 0.0, -0.085274837911129, 0.0, 0.0, 0.085274837911129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.085274837911129, 0.0, -0.085274837911129, -0.085274837911129, 0.0, 0.0, 0.085274837911129, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,117]: Mean: -0.00338888
[2025-05-27 12:41:56,117]: Min: -0.34109935
[2025-05-27 12:41:56,117]: Max: 0.25582451
[2025-05-27 12:41:56,117]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 12:41:56,117]: Sample Values (25 elements): [1.0351794958114624, 1.0361049175262451, 1.1525042057037354, 1.0263842344284058, 1.096748948097229, 1.2628107070922852, 0.8219746947288513, 1.0203757286071777, 1.2136472463607788, 1.0759294033050537, 1.127094030380249, 1.371036171913147, 1.141306757926941, 1.258660078048706, 1.4442663192749023, 1.0405831336975098, 1.1287412643432617, 1.1624643802642822, 1.1339770555496216, 1.4005231857299805, 0.9270128607749939, 1.1432725191116333, 1.3365668058395386, 1.177933931350708, 1.2195504903793335]
[2025-05-27 12:41:56,117]: Mean: 1.15117741
[2025-05-27 12:41:56,117]: Min: 0.82197469
[2025-05-27 12:41:56,118]: Max: 1.55164528
[2025-05-27 12:41:56,119]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 12:41:56,119]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.09704809635877609, 0.0, 0.09704809635877609, 0.0, 0.0, 0.0, -0.09704809635877609, 0.0, 0.09704809635877609, 0.0, 0.0, 0.0, -0.09704809635877609, 0.0, 0.0, 0.0, -0.09704809635877609, 0.0, 0.09704809635877609, 0.0, 0.09704809635877609, 0.0]
[2025-05-27 12:41:56,119]: Mean: -0.00321440
[2025-05-27 12:41:56,119]: Min: -0.38819239
[2025-05-27 12:41:56,120]: Max: 0.29114428
[2025-05-27 12:41:56,120]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 12:41:56,120]: Sample Values (25 elements): [0.6407979726791382, 0.5858132839202881, 0.7467496991157532, 0.6955199837684631, 0.5542396306991577, 0.8040327429771423, 0.4567018747329712, 1.1203460693359375, 0.48241761326789856, 0.6041902303695679, 0.7950977087020874, 0.8537081480026245, 0.734778642654419, 1.0310144424438477, 0.6963270902633667, 0.9279800057411194, 0.6461191773414612, 0.8051738142967224, 0.7167999744415283, 0.5862588882446289, 0.7118387818336487, 0.83144211769104, 1.2151840925216675, 0.49898746609687805, 0.6107279062271118]
[2025-05-27 12:41:56,120]: Mean: 0.72818184
[2025-05-27 12:41:56,120]: Min: 0.45670187
[2025-05-27 12:41:56,120]: Max: 1.21518409
[2025-05-27 12:41:56,121]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 12:41:56,122]: Sample Values (25 elements): [0.07114100456237793, 0.0, -0.07114100456237793, -0.07114100456237793, 0.0, 0.0, 0.07114100456237793, 0.07114100456237793, -0.07114100456237793, -0.07114100456237793, -0.07114100456237793, 0.0, 0.07114100456237793, 0.07114100456237793, 0.0, 0.0, 0.0, 0.07114100456237793, -0.07114100456237793, 0.07114100456237793, 0.0, 0.0, 0.0, 0.07114100456237793, 0.0]
[2025-05-27 12:41:56,122]: Mean: -0.00314561
[2025-05-27 12:41:56,122]: Min: -0.21342301
[2025-05-27 12:41:56,122]: Max: 0.28456402
[2025-05-27 12:41:56,122]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 12:41:56,122]: Sample Values (25 elements): [0.9682437181472778, 1.0896191596984863, 0.8305012583732605, 0.9248037934303284, 1.0910242795944214, 1.0546183586120605, 1.3961315155029297, 0.7951133847236633, 0.9590962529182434, 1.0882500410079956, 0.9766722917556763, 1.197679877281189, 0.9651280045509338, 0.9965271353721619, 0.9664605259895325, 1.18873929977417, 0.9377611875534058, 1.1221171617507935, 0.8871548771858215, 1.0737426280975342, 1.0316553115844727, 0.9550338387489319, 0.8016884922981262, 0.9374629855155945, 0.959783673286438]
[2025-05-27 12:41:56,123]: Mean: 1.03086627
[2025-05-27 12:41:56,123]: Min: 0.67029655
[2025-05-27 12:41:56,123]: Max: 1.39613152
[2025-05-27 12:41:56,124]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-27 12:41:56,125]: Sample Values (25 elements): [0.0, 0.0, 0.07008089870214462, -0.07008089870214462, 0.0, 0.07008089870214462, 0.07008089870214462, 0.0, -0.07008089870214462, 0.07008089870214462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07008089870214462, 0.0, 0.0, -0.07008089870214462, 0.07008089870214462, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,125]: Mean: -0.00342192
[2025-05-27 12:41:56,125]: Min: -0.21024269
[2025-05-27 12:41:56,125]: Max: 0.28032359
[2025-05-27 12:41:56,125]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-27 12:41:56,126]: Sample Values (25 elements): [0.7195441722869873, 0.7533733248710632, 0.8015550971031189, 0.4052960276603699, 0.003617665497586131, 0.4699532985687256, 0.6102859377861023, 0.9803251624107361, 0.5974733233451843, 0.6460636258125305, 0.6282350420951843, 0.8039016723632812, 0.8189435601234436, 0.7461471557617188, 0.6688054203987122, 0.8361728191375732, 0.6387646198272705, 0.7899177074432373, 0.4163934290409088, 0.6728244423866272, 0.6627889275550842, 0.6394922137260437, 0.7418544888496399, 0.6520031094551086, 0.6418575644493103]
[2025-05-27 12:41:56,126]: Mean: 0.61614370
[2025-05-27 12:41:56,126]: Min: 0.00000000
[2025-05-27 12:41:56,126]: Max: 0.98032516
[2025-05-27 12:41:56,127]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 12:41:56,129]: Sample Values (25 elements): [0.0, 0.06773240119218826, 0.0, 0.0, -0.06773240119218826, 0.0, 0.0, 0.0, -0.06773240119218826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06773240119218826, 0.0, 0.06773240119218826, -0.06773240119218826, 0.06773240119218826, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,129]: Mean: -0.00169956
[2025-05-27 12:41:56,129]: Min: -0.20319721
[2025-05-27 12:41:56,129]: Max: 0.27092960
[2025-05-27 12:41:56,129]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-27 12:41:56,129]: Sample Values (25 elements): [0.8831360936164856, 0.7302001118659973, 0.8745698928833008, 0.7309783697128296, 0.7982348799705505, 0.9487674236297607, 0.8396209478378296, 0.8743510246276855, 0.45097991824150085, 0.8517359495162964, 0.3966408669948578, 0.7029626965522766, 0.8868747353553772, 0.785884439945221, 0.859100341796875, 0.9942891597747803, 0.6919857263565063, 0.9498301148414612, 0.7548395991325378, 0.5960648655891418, 0.6995924115180969, 0.7767689824104309, 0.6797972321510315, 0.7937617897987366, 0.8398227095603943]
[2025-05-27 12:41:56,130]: Mean: 0.76935613
[2025-05-27 12:41:56,130]: Min: 0.34782964
[2025-05-27 12:41:56,130]: Max: 1.09616160
[2025-05-27 12:41:56,131]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-27 12:41:56,131]: Sample Values (25 elements): [-0.09511666744947433, 0.0, 0.0, 0.0, 0.09511666744947433, 0.0, 0.0, -0.09511666744947433, -0.19023333489894867, 0.0, -0.09511666744947433, -0.09511666744947433, 0.0, -0.09511666744947433, 0.0, 0.09511666744947433, 0.0, 0.0, 0.0, -0.09511666744947433, 0.0, 0.0, 0.09511666744947433, 0.0, 0.0]
[2025-05-27 12:41:56,131]: Mean: -0.00597962
[2025-05-27 12:41:56,131]: Min: -0.28534999
[2025-05-27 12:41:56,132]: Max: 0.38046667
[2025-05-27 12:41:56,132]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-27 12:41:56,132]: Sample Values (25 elements): [0.3841027021408081, 0.6408210396766663, 0.5096980929374695, 0.6705953478813171, 0.8479995131492615, 0.5952381491661072, 0.48386794328689575, 0.37707391381263733, 0.45067936182022095, 0.5347565412521362, 0.6613458395004272, 0.6113547682762146, 0.4648720920085907, 0.3683387339115143, 0.6221045255661011, 0.5508058071136475, 0.454292893409729, 0.4781389534473419, 0.4778320789337158, 0.4932681620121002, 0.4074893295764923, 0.6122519969940186, 0.5333433151245117, 0.6035568714141846, 0.782271146774292]
[2025-05-27 12:41:56,132]: Mean: 0.52856743
[2025-05-27 12:41:56,132]: Min: 0.24124974
[2025-05-27 12:41:56,132]: Max: 0.84799951
[2025-05-27 12:41:56,133]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 12:41:56,135]: Sample Values (25 elements): [0.0, 0.0, 0.07253694534301758, 0.0, -0.07253694534301758, 0.0, -0.07253694534301758, 0.07253694534301758, 0.0, 0.0, 0.07253694534301758, 0.0, 0.0, -0.07253694534301758, 0.0, 0.0, 0.07253694534301758, -0.07253694534301758, 0.0, 0.0, -0.07253694534301758, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,135]: Mean: -0.00223972
[2025-05-27 12:41:56,135]: Min: -0.21761084
[2025-05-27 12:41:56,135]: Max: 0.29014778
[2025-05-27 12:41:56,135]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-27 12:41:56,135]: Sample Values (25 elements): [0.6085106730461121, 0.5718562006950378, 0.6046301126480103, 0.5828189253807068, 3.295269534130639e-14, 0.8416758179664612, 0.5935499668121338, 0.2923735976219177, 8.751799214223865e-06, 0.787663996219635, 0.5449120402336121, -5.061349923294807e-41, 0.4271615445613861, 0.5959610939025879, 0.5898820161819458, 0.4355209767818451, -4.914914233772863e-41, 0.7251083254814148, 0.6303201913833618, 0.6232068538665771, 0.6730179190635681, 0.7217996716499329, 0.5156399607658386, 0.51947420835495, 0.5846086144447327]
[2025-05-27 12:41:56,136]: Mean: 0.52931041
[2025-05-27 12:41:56,136]: Min: -0.00000000
[2025-05-27 12:41:56,136]: Max: 0.92244041
[2025-05-27 12:41:56,137]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 12:41:56,138]: Sample Values (25 elements): [0.05994293838739395, 0.0, -0.05994293838739395, -0.05994293838739395, 0.0, 0.0, 0.0, 0.0, -0.05994293838739395, 0.0, 0.0, 0.0, 0.0, -0.05994293838739395, 0.0, 0.0, 0.0, 0.05994293838739395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05994293838739395]
[2025-05-27 12:41:56,138]: Mean: -0.00114108
[2025-05-27 12:41:56,138]: Min: -0.17982882
[2025-05-27 12:41:56,139]: Max: 0.23977175
[2025-05-27 12:41:56,139]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-27 12:41:56,139]: Sample Values (25 elements): [0.7358201146125793, 0.5943212509155273, 0.41220325231552124, 0.5851464867591858, 0.7165287733078003, 0.733495831489563, 0.6337830424308777, 0.5790471434593201, 0.6291549205780029, 0.6660006642341614, 0.7410318851470947, 0.6693288087844849, 0.1747598946094513, 0.5611444115638733, 0.7621370553970337, 0.5682153105735779, 0.5461352467536926, 0.5107781291007996, 0.7097790241241455, 0.6002073287963867, 0.7615854740142822, 0.6633798480033875, 0.2926378548145294, 0.8957186937332153, 0.7238786816596985]
[2025-05-27 12:41:56,139]: Mean: 0.65514469
[2025-05-27 12:41:56,139]: Min: 0.10610602
[2025-05-27 12:41:56,139]: Max: 0.98136902
[2025-05-27 12:41:56,140]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-27 12:41:56,144]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.06517570465803146, 0.0, 0.0, 0.0, 0.0, -0.06517570465803146, 0.06517570465803146, 0.0, 0.0, 0.0, 0.06517570465803146, 0.0, 0.0, 0.06517570465803146, 0.0, 0.0, 0.0, 0.0, -0.06517570465803146, 0.0]
[2025-05-27 12:41:56,144]: Mean: -0.00055869
[2025-05-27 12:41:56,144]: Min: -0.19552711
[2025-05-27 12:41:56,145]: Max: 0.26070282
[2025-05-27 12:41:56,145]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-27 12:41:56,145]: Sample Values (25 elements): [0.5898338556289673, 0.5561773777008057, 0.6630752682685852, 0.370258092880249, 0.5465041995048523, 5.20610405465956e-41, 0.5384754538536072, 0.5613530874252319, 2.5348028987021214e-20, 0.701806902885437, 4.909309039915564e-41, 0.8045655488967896, -4.911270857765619e-41, 0.6614506840705872, 0.2108314335346222, 0.6029744744300842, 0.43872928619384766, 0.563339352607727, -4.92318189471238e-41, 0.5172211527824402, 0.2867961525917053, 0.5879939794540405, 0.6333961486816406, 0.7319414615631104, 2.980251408071094e-13]
[2025-05-27 12:41:56,145]: Mean: 0.40794161
[2025-05-27 12:41:56,145]: Min: -0.00000000
[2025-05-27 12:41:56,145]: Max: 0.95517486
[2025-05-27 12:41:56,146]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 12:41:56,153]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07061763852834702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07061763852834702, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,153]: Mean: -0.00010416
[2025-05-27 12:41:56,153]: Min: -0.21185291
[2025-05-27 12:41:56,154]: Max: 0.28247055
[2025-05-27 12:41:56,154]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-27 12:41:56,154]: Sample Values (25 elements): [0.6889950037002563, 0.3067559599876404, 0.4767538905143738, 0.6721232533454895, 0.23566050827503204, 0.8098310828208923, -5.973735353416695e-41, 0.6409904956817627, 0.8148571848869324, 0.7853283882141113, 0.4821127653121948, 0.5664458870887756, 0.5828359127044678, 0.7630903720855713, 0.5648915767669678, 0.4334868788719177, 0.371158629655838, 0.4966864585876465, 0.5991201400756836, 0.6410830020904541, 0.8825513124465942, 0.5542905330657959, 0.18296341598033905, 6.512548488135173e-23, 0.3161051273345947]
[2025-05-27 12:41:56,154]: Mean: 0.49896601
[2025-05-27 12:41:56,154]: Min: -0.00000000
[2025-05-27 12:41:56,154]: Max: 0.97401333
[2025-05-27 12:41:56,155]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-27 12:41:56,156]: Sample Values (25 elements): [0.0, 0.0, 0.07471440732479095, 0.0, 0.07471440732479095, 0.0, 0.0, 0.0, -0.07471440732479095, 0.0, 0.0, 0.0, 0.0, -0.07471440732479095, -0.07471440732479095, -0.07471440732479095, 0.07471440732479095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1494288146495819, -0.07471440732479095, 0.07471440732479095]
[2025-05-27 12:41:56,156]: Mean: -0.00349996
[2025-05-27 12:41:56,156]: Min: -0.22414322
[2025-05-27 12:41:56,156]: Max: 0.29885763
[2025-05-27 12:41:56,156]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-27 12:41:56,156]: Sample Values (25 elements): [4.982176560060455e-41, 0.4234082102775574, 0.40484264492988586, 4.969564873881531e-41, 4.909028780222699e-41, 0.10554482042789459, 0.4220573306083679, 0.4511373043060303, 0.3806931972503662, 0.39194491505622864, 0.38865604996681213, 0.1330166906118393, 0.5037797689437866, 0.09550707042217255, 0.09942816942930222, 0.43791553378105164, 0.43795129656791687, 0.35335734486579895, 0.42720964550971985, 0.47572267055511475, 0.25729063153266907, 0.32833531498908997, 0.36278900504112244, 0.3192839026451111, 0.4739721417427063]
[2025-05-27 12:41:56,157]: Mean: 0.30398139
[2025-05-27 12:41:56,157]: Min: -0.00000000
[2025-05-27 12:41:56,157]: Max: 0.63399440
[2025-05-27 12:41:56,158]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 12:41:56,164]: Sample Values (25 elements): [0.0, 0.0, 0.06376650184392929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06376650184392929, 0.0, -0.06376650184392929, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,165]: Mean: -0.00062023
[2025-05-27 12:41:56,165]: Min: -0.19129950
[2025-05-27 12:41:56,165]: Max: 0.25506601
[2025-05-27 12:41:56,165]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-27 12:41:56,165]: Sample Values (25 elements): [5.009361750268356e-41, 0.48779624700546265, 5.849300049784651e-41, -5.532606596847243e-41, 0.6177168488502502, 0.2445990890264511, 0.4524192810058594, 0.6217289566993713, 0.5808515548706055, 0.4996448755264282, -4.907066962372644e-41, 0.4814029932022095, -5.218575610992051e-41, -4.906506442986914e-41, -5.211428988823995e-41, -4.907066962372644e-41, 0.37745946645736694, 5.003336166871759e-41, 0.4759727716445923, 0.5958227515220642, 0.6009635329246521, 6.301779323915135e-41, 5.060509144216212e-41, 5.015387333664953e-41, 0.4549509882926941]
[2025-05-27 12:41:56,165]: Mean: 0.21337599
[2025-05-27 12:41:56,166]: Min: -0.00000000
[2025-05-27 12:41:56,166]: Max: 0.76543903
[2025-05-27 12:41:56,167]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 12:41:56,174]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0550382062792778]
[2025-05-27 12:41:56,174]: Mean: -0.00019624
[2025-05-27 12:41:56,174]: Min: -0.16511461
[2025-05-27 12:41:56,175]: Max: 0.22015283
[2025-05-27 12:41:56,175]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-27 12:41:56,175]: Sample Values (25 elements): [0.0775192603468895, 0.6692059636116028, 0.472848117351532, 0.3881075978279114, 0.18611320853233337, -4.985679806221267e-41, 0.2896154820919037, 0.21303455531597137, 0.42423689365386963, 0.3266775608062744, 0.07952925562858582, 5.645691382918255e-41, -5.108153292003256e-41, 0.49881038069725037, 0.5706628561019897, 0.2902362644672394, 0.617656409740448, 0.6161448359489441, 0.48383957147598267, 0.5649003982543945, 1.5983659362707127e-30, 0.8920285701751709, 4.0416443880531006e-07, 4.989042922535646e-41, -5.200078471262964e-41]
[2025-05-27 12:41:56,175]: Mean: 0.34065723
[2025-05-27 12:41:56,175]: Min: -0.00000000
[2025-05-27 12:41:56,175]: Max: 0.97579253
[2025-05-27 12:41:56,176]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-27 12:41:56,194]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,194]: Mean: 0.00001518
[2025-05-27 12:41:56,194]: Min: -0.20119938
[2025-05-27 12:41:56,194]: Max: 0.26826584
[2025-05-27 12:41:56,194]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-27 12:41:56,195]: Sample Values (25 elements): [-5.643869694914633e-41, 6.206491028341047e-41, -4.918417479933675e-41, -4.963118900945637e-41, 5.1337970539004e-41, 5.12987341820029e-41, -5.389814283332544e-41, -5.801515772151175e-41, 1.6043965382576473e-17, 5.809923562937124e-41, 0.00015132184489630163, -6.038475342468502e-41, -4.916876051622918e-41, 5.236372101488976e-41, -5.035285771858365e-41, -5.899326404961047e-41, 5.040470576176367e-41, -5.275608458490071e-41, 5.761578765917918e-41, -5.621729179178301e-41, 5.16939003489425e-41, 6.12871896357102e-41, -4.911831377151349e-41, 5.886994978474989e-41, -5.387992595328922e-41]
[2025-05-27 12:41:56,195]: Mean: 0.08417208
[2025-05-27 12:41:56,195]: Min: -0.00000000
[2025-05-27 12:41:56,195]: Max: 0.97374260
[2025-05-27 12:41:56,196]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 12:41:56,239]: Sample Values (25 elements): [0.0, 0.0456988662481308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0456988662481308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,239]: Mean: 0.00005710
[2025-05-27 12:41:56,239]: Min: -0.09139773
[2025-05-27 12:41:56,240]: Max: 0.22849433
[2025-05-27 12:41:56,240]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-27 12:41:56,241]: Sample Values (25 elements): [-6.174401293508009e-41, 4.28862914157356e-18, 0.43417441844940186, 0.6029520630836487, 0.15673768520355225, 0.5509378910064697, 0.4067233204841614, 0.4066197872161865, 0.6124531030654907, 0.31672754883766174, 0.49035170674324036, 0.3125998377799988, 0.510458767414093, 0.6163795590400696, 0.4724685847759247, 0.35259920358657837, 1.839476297334654e-27, 0.23400470614433289, 0.28536784648895264, 3.3658486815155457e-15, 0.17156532406806946, 0.5286358594894409, -4.910570208533456e-41, 0.6220006346702576, 0.37034136056900024]
[2025-05-27 12:41:56,241]: Mean: 0.35362196
[2025-05-27 12:41:56,241]: Min: -0.00000000
[2025-05-27 12:41:56,242]: Max: 0.84781986
[2025-05-27 12:41:56,243]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-27 12:41:56,244]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05039971321821213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,244]: Mean: 0.00141580
[2025-05-27 12:41:56,244]: Min: -0.15119913
[2025-05-27 12:41:56,244]: Max: 0.20159885
[2025-05-27 12:41:56,244]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-27 12:41:56,245]: Sample Values (25 elements): [0.3504493534564972, 0.24497723579406738, 0.35240426659584045, 0.6558427214622498, 0.5101014971733093, 0.13345202803611755, 0.6169754862785339, 0.2126060128211975, 0.40667492151260376, 0.5302703380584717, 0.31167659163475037, 0.22624655067920685, 0.7361689209938049, 0.5229313373565674, 0.22938939929008484, 0.4225897789001465, -4.90622618329405e-41, 0.39007270336151123, 0.06405828893184662, 0.5203976631164551, 1.8059428990826263e-23, 0.5013577938079834, 0.5675748586654663, 0.5773802399635315, 0.4437709152698517]
[2025-05-27 12:41:56,245]: Mean: 0.36239597
[2025-05-27 12:41:56,245]: Min: -0.00000000
[2025-05-27 12:41:56,245]: Max: 0.75366205
[2025-05-27 12:41:56,246]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 12:41:56,287]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,287]: Mean: 0.00000175
[2025-05-27 12:41:56,288]: Min: -0.05512415
[2025-05-27 12:41:56,288]: Max: 0.07349887
[2025-05-27 12:41:56,288]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-27 12:41:56,288]: Sample Values (25 elements): [5.181721461380309e-41, 6.225408557609432e-41, -5.500096472474907e-41, -5.11628082309634e-41, 6.22470790837727e-41, 6.225969076995162e-41, 5.828000313126914e-41, 5.748406560353265e-41, -4.972787860349478e-41, -5.763540583767973e-41, -5.488325565374579e-41, 5.788063306893657e-41, 5.28625832681894e-41, -5.297328584687106e-41, 5.502618809710692e-41, 5.194753537098529e-41, -5.243238463964168e-41, 5.025756942300956e-41, 5.830242390669834e-41, 5.132255625589643e-41, -6.048144301872343e-41, -6.18743336922623e-41, 5.988168727599241e-41, 5.252346903982279e-41, -5.320309879502033e-41]
[2025-05-27 12:41:56,288]: Mean: 0.00702252
[2025-05-27 12:41:56,289]: Min: -0.00000000
[2025-05-27 12:41:56,289]: Max: 0.43367171
[2025-05-27 12:41:56,290]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 12:41:56,327]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 12:41:56,327]: Mean: -0.00001614
[2025-05-27 12:41:56,327]: Min: -0.05897933
[2025-05-27 12:41:56,327]: Max: 0.07863910
[2025-05-27 12:41:56,328]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-27 12:41:56,328]: Sample Values (25 elements): [0.01744537614285946, 0.08599981665611267, 0.22528275847434998, 0.09816484898328781, 0.023238535970449448, 0.06532852351665497, 0.025357306003570557, 0.05603738874197006, -4.910990598072754e-41, -4.937615268894925e-41, 0.1608131378889084, -5.809363043551394e-41, 0.1875426471233368, 0.10013162344694138, 0.1265086680650711, 0.1576237678527832, 4.399482097003629e-19, 0.07715218514204025, 0.06322833150625229, 0.2597208321094513, 4.659754182512188e-08, -4.905945923601185e-41, 0.12569336593151093, 0.06578697264194489, -4.905385404215455e-41]
[2025-05-27 12:41:56,328]: Mean: 0.09187037
[2025-05-27 12:41:56,328]: Min: -0.04732701
[2025-05-27 12:41:56,328]: Max: 0.39501384
[2025-05-27 12:41:56,328]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-27 12:41:56,329]: Sample Values (25 elements): [-0.0790310874581337, 0.10227715969085693, -0.06313450634479523, 0.11512602120637894, -0.07996989041566849, 0.09111148118972778, 0.12849844992160797, -0.08172095566987991, 1.0629877778001173e-07, 1.6315512027270228e-19, -0.04316461831331253, 0.20457619428634644, 4.913512935308539e-41, -0.34379854798316956, -0.07876013219356537, -0.00653943931683898, 0.10314182937145233, 0.04792845994234085, 0.13794083893299103, 0.014371784403920174, 0.2560042440891266, 0.24790559709072113, 0.06130320206284523, -6.143423270099646e-11, 6.5340106323530685e-25]
[2025-05-27 12:41:56,329]: Mean: -0.02207289
[2025-05-27 12:41:56,329]: Min: -0.47524476
[2025-05-27 12:41:56,329]: Max: 0.33282727
[2025-05-27 12:41:56,329]: 


QAT of ResNet18 with relu down to 2 bits...
[2025-05-27 12:41:56,699]: [ResNet18_relu_quantized_2_bits] after configure_qat:
[2025-05-27 12:41:56,742]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 12:43:34,569]: [ResNet18_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.3659 Train Acc: 0.1231 Eval Loss: 2.3435 Eval Acc: 0.1311 (LR: 0.00100000)
[2025-05-27 12:45:12,251]: [ResNet18_relu_quantized_2_bits] Epoch: 002 Train Loss: 2.1954 Train Acc: 0.1846 Eval Loss: 2.0372 Eval Acc: 0.2245 (LR: 0.00100000)
[2025-05-27 12:46:49,918]: [ResNet18_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.8618 Train Acc: 0.2944 Eval Loss: 1.6735 Eval Acc: 0.3588 (LR: 0.00100000)
[2025-05-27 12:48:27,613]: [ResNet18_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.6324 Train Acc: 0.3854 Eval Loss: 1.5476 Eval Acc: 0.4296 (LR: 0.00100000)
[2025-05-27 12:50:05,285]: [ResNet18_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.6547 Train Acc: 0.3822 Eval Loss: 1.5880 Eval Acc: 0.4086 (LR: 0.00100000)
[2025-05-27 12:51:42,636]: [ResNet18_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.5122 Train Acc: 0.4393 Eval Loss: 1.4567 Eval Acc: 0.4535 (LR: 0.00100000)
[2025-05-27 12:53:19,956]: [ResNet18_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.4565 Train Acc: 0.4626 Eval Loss: 1.3974 Eval Acc: 0.4886 (LR: 0.00100000)
[2025-05-27 12:54:57,433]: [ResNet18_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.4277 Train Acc: 0.4777 Eval Loss: 1.4966 Eval Acc: 0.4622 (LR: 0.00100000)
[2025-05-27 12:56:34,733]: [ResNet18_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.4972 Train Acc: 0.4528 Eval Loss: 1.4409 Eval Acc: 0.4780 (LR: 0.00100000)
[2025-05-27 12:58:12,019]: [ResNet18_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.4640 Train Acc: 0.4687 Eval Loss: 1.4226 Eval Acc: 0.4838 (LR: 0.00100000)
[2025-05-27 12:59:49,592]: [ResNet18_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.4128 Train Acc: 0.4850 Eval Loss: 1.4675 Eval Acc: 0.4809 (LR: 0.00100000)
[2025-05-27 13:01:27,319]: [ResNet18_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.3838 Train Acc: 0.4973 Eval Loss: 1.4114 Eval Acc: 0.4910 (LR: 0.00100000)
[2025-05-27 13:03:05,251]: [ResNet18_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.3749 Train Acc: 0.5005 Eval Loss: 1.4480 Eval Acc: 0.4833 (LR: 0.00010000)
[2025-05-27 13:04:43,121]: [ResNet18_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.3335 Train Acc: 0.5189 Eval Loss: 1.2845 Eval Acc: 0.5322 (LR: 0.00010000)
[2025-05-27 13:06:20,795]: [ResNet18_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.2976 Train Acc: 0.5286 Eval Loss: 1.2514 Eval Acc: 0.5499 (LR: 0.00010000)
[2025-05-27 13:07:58,466]: [ResNet18_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.2730 Train Acc: 0.5416 Eval Loss: 1.2500 Eval Acc: 0.5465 (LR: 0.00010000)
[2025-05-27 13:09:35,779]: [ResNet18_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.2728 Train Acc: 0.5406 Eval Loss: 1.2486 Eval Acc: 0.5516 (LR: 0.00010000)
[2025-05-27 13:11:13,259]: [ResNet18_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.2642 Train Acc: 0.5444 Eval Loss: 1.2560 Eval Acc: 0.5426 (LR: 0.00010000)
[2025-05-27 13:12:50,551]: [ResNet18_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.2716 Train Acc: 0.5426 Eval Loss: 1.2113 Eval Acc: 0.5632 (LR: 0.00010000)
[2025-05-27 13:14:28,207]: [ResNet18_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.2575 Train Acc: 0.5438 Eval Loss: 1.2119 Eval Acc: 0.5602 (LR: 0.00010000)
[2025-05-27 13:16:05,601]: [ResNet18_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.2717 Train Acc: 0.5437 Eval Loss: 1.2222 Eval Acc: 0.5579 (LR: 0.00010000)
[2025-05-27 13:17:43,310]: [ResNet18_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.2685 Train Acc: 0.5423 Eval Loss: 1.2296 Eval Acc: 0.5560 (LR: 0.00010000)
[2025-05-27 13:19:21,218]: [ResNet18_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.2761 Train Acc: 0.5394 Eval Loss: 1.2314 Eval Acc: 0.5601 (LR: 0.00010000)
[2025-05-27 13:20:58,958]: [ResNet18_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.2764 Train Acc: 0.5393 Eval Loss: 1.2421 Eval Acc: 0.5555 (LR: 0.00010000)
[2025-05-27 13:22:36,595]: [ResNet18_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.2705 Train Acc: 0.5415 Eval Loss: 1.2225 Eval Acc: 0.5556 (LR: 0.00001000)
[2025-05-27 13:24:14,069]: [ResNet18_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.2677 Train Acc: 0.5419 Eval Loss: 1.1977 Eval Acc: 0.5680 (LR: 0.00001000)
[2025-05-27 13:25:51,555]: [ResNet18_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.2755 Train Acc: 0.5388 Eval Loss: 1.2277 Eval Acc: 0.5515 (LR: 0.00001000)
[2025-05-27 13:27:28,871]: [ResNet18_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.2732 Train Acc: 0.5371 Eval Loss: 1.2253 Eval Acc: 0.5574 (LR: 0.00001000)
[2025-05-27 13:29:06,197]: [ResNet18_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.2659 Train Acc: 0.5430 Eval Loss: 1.2379 Eval Acc: 0.5567 (LR: 0.00001000)
[2025-05-27 13:30:43,485]: [ResNet18_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.2630 Train Acc: 0.5422 Eval Loss: 1.2287 Eval Acc: 0.5603 (LR: 0.00001000)
[2025-05-27 13:32:21,000]: [ResNet18_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.2657 Train Acc: 0.5423 Eval Loss: 1.2186 Eval Acc: 0.5596 (LR: 0.00001000)
[2025-05-27 13:33:58,259]: [ResNet18_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.2782 Train Acc: 0.5393 Eval Loss: 1.2566 Eval Acc: 0.5464 (LR: 0.00000100)
[2025-05-27 13:35:35,605]: [ResNet18_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.2918 Train Acc: 0.5327 Eval Loss: 1.2406 Eval Acc: 0.5505 (LR: 0.00000100)
[2025-05-27 13:37:12,696]: [ResNet18_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.2863 Train Acc: 0.5350 Eval Loss: 1.2539 Eval Acc: 0.5481 (LR: 0.00000100)
[2025-05-27 13:38:49,965]: [ResNet18_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.2824 Train Acc: 0.5355 Eval Loss: 1.2264 Eval Acc: 0.5547 (LR: 0.00000100)
[2025-05-27 13:40:27,308]: [ResNet18_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.2746 Train Acc: 0.5402 Eval Loss: 1.2711 Eval Acc: 0.5398 (LR: 0.00000100)
[2025-05-27 13:42:04,571]: [ResNet18_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.2761 Train Acc: 0.5391 Eval Loss: 1.2859 Eval Acc: 0.5357 (LR: 0.00000100)
[2025-05-27 13:43:41,887]: [ResNet18_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.2922 Train Acc: 0.5338 Eval Loss: 1.2456 Eval Acc: 0.5488 (LR: 0.00000010)
[2025-05-27 13:45:19,214]: [ResNet18_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.2920 Train Acc: 0.5323 Eval Loss: 1.2643 Eval Acc: 0.5513 (LR: 0.00000010)
[2025-05-27 13:46:56,694]: [ResNet18_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.2889 Train Acc: 0.5356 Eval Loss: 1.2508 Eval Acc: 0.5529 (LR: 0.00000010)
[2025-05-27 13:48:34,301]: [ResNet18_relu_quantized_2_bits] Epoch: 041 Train Loss: 1.3035 Train Acc: 0.5277 Eval Loss: 1.2285 Eval Acc: 0.5529 (LR: 0.00000010)
[2025-05-27 13:50:12,112]: [ResNet18_relu_quantized_2_bits] Epoch: 042 Train Loss: 1.3033 Train Acc: 0.5270 Eval Loss: 1.2389 Eval Acc: 0.5533 (LR: 0.00000010)
[2025-05-27 13:51:49,591]: [ResNet18_relu_quantized_2_bits] Epoch: 043 Train Loss: 1.2980 Train Acc: 0.5314 Eval Loss: 1.2666 Eval Acc: 0.5403 (LR: 0.00000010)
[2025-05-27 13:53:26,896]: [ResNet18_relu_quantized_2_bits] Epoch: 044 Train Loss: 1.2829 Train Acc: 0.5372 Eval Loss: 1.2408 Eval Acc: 0.5541 (LR: 0.00000010)
[2025-05-27 13:55:04,209]: [ResNet18_relu_quantized_2_bits] Epoch: 045 Train Loss: 1.3196 Train Acc: 0.5200 Eval Loss: 1.2689 Eval Acc: 0.5417 (LR: 0.00000010)
[2025-05-27 13:56:41,731]: [ResNet18_relu_quantized_2_bits] Epoch: 046 Train Loss: 1.3458 Train Acc: 0.5137 Eval Loss: 1.3554 Eval Acc: 0.5105 (LR: 0.00000010)
[2025-05-27 13:58:19,265]: [ResNet18_relu_quantized_2_bits] Epoch: 047 Train Loss: 1.3793 Train Acc: 0.5014 Eval Loss: 1.3100 Eval Acc: 0.5234 (LR: 0.00000010)
[2025-05-27 13:59:57,125]: [ResNet18_relu_quantized_2_bits] Epoch: 048 Train Loss: 1.4032 Train Acc: 0.4921 Eval Loss: 1.3457 Eval Acc: 0.5168 (LR: 0.00000010)
[2025-05-27 14:01:34,808]: [ResNet18_relu_quantized_2_bits] Epoch: 049 Train Loss: 1.4094 Train Acc: 0.4898 Eval Loss: 1.3513 Eval Acc: 0.5108 (LR: 0.00000010)
[2025-05-27 14:03:12,518]: [ResNet18_relu_quantized_2_bits] Epoch: 050 Train Loss: 1.4183 Train Acc: 0.4891 Eval Loss: 1.3516 Eval Acc: 0.5127 (LR: 0.00000010)
[2025-05-27 14:04:50,024]: [ResNet18_relu_quantized_2_bits] Epoch: 051 Train Loss: 1.3987 Train Acc: 0.4940 Eval Loss: 1.3485 Eval Acc: 0.5142 (LR: 0.00000010)
[2025-05-27 14:06:27,505]: [ResNet18_relu_quantized_2_bits] Epoch: 052 Train Loss: 1.3887 Train Acc: 0.4953 Eval Loss: 1.3374 Eval Acc: 0.5121 (LR: 0.00000010)
[2025-05-27 14:08:04,987]: [ResNet18_relu_quantized_2_bits] Epoch: 053 Train Loss: 1.3877 Train Acc: 0.4984 Eval Loss: 1.3306 Eval Acc: 0.5191 (LR: 0.00000010)
[2025-05-27 14:09:42,336]: [ResNet18_relu_quantized_2_bits] Epoch: 054 Train Loss: 1.3598 Train Acc: 0.5100 Eval Loss: 1.2978 Eval Acc: 0.5316 (LR: 0.00000010)
[2025-05-27 14:11:19,439]: [ResNet18_relu_quantized_2_bits] Epoch: 055 Train Loss: 1.3576 Train Acc: 0.5077 Eval Loss: 1.2951 Eval Acc: 0.5283 (LR: 0.00000010)
[2025-05-27 14:12:56,710]: [ResNet18_relu_quantized_2_bits] Epoch: 056 Train Loss: 1.3696 Train Acc: 0.5035 Eval Loss: 1.3174 Eval Acc: 0.5240 (LR: 0.00000010)
[2025-05-27 14:14:34,064]: [ResNet18_relu_quantized_2_bits] Epoch: 057 Train Loss: 1.4003 Train Acc: 0.4952 Eval Loss: 1.3528 Eval Acc: 0.5075 (LR: 0.00000010)
[2025-05-27 14:16:11,087]: [ResNet18_relu_quantized_2_bits] Epoch: 058 Train Loss: 1.3874 Train Acc: 0.4979 Eval Loss: 1.3282 Eval Acc: 0.5165 (LR: 0.00000010)
[2025-05-27 14:17:48,773]: [ResNet18_relu_quantized_2_bits] Epoch: 059 Train Loss: 1.3935 Train Acc: 0.4955 Eval Loss: 1.3138 Eval Acc: 0.5244 (LR: 0.00000010)
[2025-05-27 14:19:26,297]: [ResNet18_relu_quantized_2_bits] Epoch: 060 Train Loss: 1.3808 Train Acc: 0.5002 Eval Loss: 1.3248 Eval Acc: 0.5224 (LR: 0.00000010)
[2025-05-27 14:19:26,298]: [ResNet18_relu_quantized_2_bits] Best Eval Accuracy: 0.5680
[2025-05-27 14:19:26,387]: 


Quantization of model down to 2 bits finished
[2025-05-27 14:19:26,387]: Model Architecture:
[2025-05-27 14:19:26,433]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.8503], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=23.55098533630371)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4201], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6266223192214966, max_val=0.6337102651596069)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.0997], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.29923152923584)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3393], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49577879905700684, max_val=0.522118866443634)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.9634], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=23.890226364135742)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3988], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6389142274856567, max_val=0.5573620796203613)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8531], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.55943775177002)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2618], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38443970680236816, max_val=0.401008278131485)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.2704], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=24.811113357543945)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3395], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5092889070510864, max_val=0.5091632604598999)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.3979], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.193689346313477)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2749], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.44171521067619324, max_val=0.38309985399246216)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3274], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.502028226852417, max_val=0.48026061058044434)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.6566], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.96978759765625)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2620], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37926769256591797, max_val=0.4068170189857483)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.4276], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.282729148864746)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2283], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3254561424255371, max_val=0.3595077395439148)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.7503], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=20.250791549682617)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2067], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2807890772819519, max_val=0.33941638469696045)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.0885], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.265386581420898)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2090], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3219681680202484, max_val=0.305046021938324)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2522], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32631421089172363, max_val=0.43020105361938477)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.5153], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.546008110046387)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1983], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31634044647216797, max_val=0.27858632802963257)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6226], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.867793083190918)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2140], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3094700574874878, max_val=0.33250176906585693)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.4096], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.228809356689453)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2142], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28305304050445557, max_val=0.3596796989440918)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.1346], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.403879165649414)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1752], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.269920289516449, max_val=0.25570911169052124)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2027], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3008286654949188, max_val=0.3071231245994568)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.6356], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.9067535400390625)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2370], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39348939061164856, max_val=0.317579984664917)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6117], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.835211277008057)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1853], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2830393314361572, max_val=0.2728668451309204)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.1688], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.506300926208496)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 14:19:26,434]: 
Model Weights:
[2025-05-27 14:19:26,434]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-27 14:19:26,434]: Sample Values (25 elements): [-0.022029252722859383, -0.03491131216287613, -0.13581345975399017, -0.30928707122802734, -0.21879097819328308, -0.1710336059331894, -0.14643654227256775, -0.3239925801753998, -0.29469892382621765, 0.08563407510519028, 0.15288004279136658, -0.132875457406044, -0.03424964100122452, 0.12473593652248383, -0.013016770593822002, 0.04599737375974655, -0.0698411762714386, 0.11579395085573196, -0.32086870074272156, -0.045616403222084045, 0.036125246435403824, -0.07496094703674316, 0.2661373019218445, -0.07063373178243637, 0.2289789915084839]
[2025-05-27 14:19:26,434]: Mean: -0.00320501
[2025-05-27 14:19:26,434]: Min: -0.43250102
[2025-05-27 14:19:26,434]: Max: 0.37582025
[2025-05-27 14:19:26,435]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-27 14:19:26,435]: Sample Values (25 elements): [0.6531248092651367, 1.3878059387207031, 1.0544662475585938, 2.1228015422821045, 1.2138595581054688, 0.8925146460533142, 0.8112114071846008, 0.6834582686424255, 0.7871068120002747, 0.94612056016922, 0.9818257093429565, 0.8951932191848755, 0.49630311131477356, 0.8237631320953369, 0.6832739114761353, 0.9980711936950684, 2.8663980960845947, 2.56416392326355, 0.9354303479194641, 1.358156442642212, 1.1581493616104126, 0.6767385005950928, 0.5256592631340027, 2.574223518371582, 2.2053062915802]
[2025-05-27 14:19:26,435]: Mean: 1.18005514
[2025-05-27 14:19:26,435]: Min: 0.25751546
[2025-05-27 14:19:26,435]: Max: 3.25772023
[2025-05-27 14:19:26,436]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 14:19:26,437]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,437]: Mean: -0.00203993
[2025-05-27 14:19:26,437]: Min: -0.42011088
[2025-05-27 14:19:26,437]: Max: 0.84022176
[2025-05-27 14:19:26,437]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 14:19:26,437]: Sample Values (25 elements): [1.1793094873428345, 1.779066562652588, 0.6255394816398621, -1.0205656715677643e-41, 1.8686703443527222, 0.8534210920333862, 0.9717791080474854, 1.438984990119934, 0.7754114270210266, 0.7140747308731079, 1.6253564357757568, 0.8933573961257935, 1.0068302154541016, 1.056604027748108, 0.8759620785713196, 0.034926630556583405, 0.8165962100028992, 0.7347052097320557, 1.4446886777877808, 1.1348016262054443, 0.9873092174530029, 0.9811176657676697, 0.7762556672096252, 0.9036955237388611, 1.0814422369003296]
[2025-05-27 14:19:26,438]: Mean: 0.93113434
[2025-05-27 14:19:26,438]: Min: -0.00000000
[2025-05-27 14:19:26,438]: Max: 1.86867034
[2025-05-27 14:19:26,439]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 14:19:26,439]: Sample Values (25 elements): [0.33929920196533203, 0.0, 0.0, 0.33929920196533203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.33929920196533203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.33929920196533203, 0.0, 0.0]
[2025-05-27 14:19:26,440]: Mean: -0.00289008
[2025-05-27 14:19:26,440]: Min: -0.33929920
[2025-05-27 14:19:26,440]: Max: 0.67859840
[2025-05-27 14:19:26,440]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 14:19:26,440]: Sample Values (25 elements): [1.0528658628463745, 1.588089942932129, 0.6280031204223633, 1.4378210306167603, 1.117042064666748, 1.3392226696014404, 1.0481445789337158, 1.4945718050003052, 1.5782862901687622, 1.1239975690841675, 1.499338984489441, 0.8017314672470093, 1.4053552150726318, 1.129549264907837, 1.002596139907837, 0.9874829053878784, 0.8652936220169067, 1.0953295230865479, 1.6223931312561035, 1.1884392499923706, 1.2124556303024292, 1.0318032503128052, 0.9337464570999146, 1.8907109498977661, 1.0493314266204834]
[2025-05-27 14:19:26,440]: Mean: 1.14028859
[2025-05-27 14:19:26,440]: Min: -0.11318452
[2025-05-27 14:19:26,441]: Max: 1.89071095
[2025-05-27 14:19:26,442]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 14:19:26,442]: Sample Values (25 elements): [0.39875876903533936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,442]: Mean: -0.00280161
[2025-05-27 14:19:26,442]: Min: -0.79751754
[2025-05-27 14:19:26,442]: Max: 0.39875877
[2025-05-27 14:19:26,443]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 14:19:26,443]: Sample Values (25 elements): [0.8927220106124878, 1.3958921432495117, 4.922200985787352e-41, 1.3546321392059326, 0.8272475600242615, 1.049407958984375, 1.7517868280410767, 2.4656244060339283e-11, 1.1324000358581543, 1.0522934198379517, 1.1096762418746948, 1.1524499654769897, 1.2039332389831543, 0.8739375472068787, 6.814692312254407e-23, 4.925984491641029e-41, 4.90510514452259e-41, 0.7200298309326172, 0.9040703773498535, 0.9160467982292175, 0.9818862080574036, 5.3291824286816336e-08, 0.8963887691497803, 1.0883698463439941, 1.3144543170928955]
[2025-05-27 14:19:26,443]: Mean: 0.83493626
[2025-05-27 14:19:26,443]: Min: 0.00000000
[2025-05-27 14:19:26,443]: Max: 1.80453086
[2025-05-27 14:19:26,444]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 14:19:26,445]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26181599497795105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,445]: Mean: 0.00012784
[2025-05-27 14:19:26,445]: Min: -0.26181599
[2025-05-27 14:19:26,445]: Max: 0.52363199
[2025-05-27 14:19:26,445]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 14:19:26,445]: Sample Values (25 elements): [0.9940903782844543, 0.9435238838195801, 1.541969656944275, 1.0513253211975098, 1.0857553482055664, 1.1487523317337036, 1.0965059995651245, 1.273617148399353, 0.7682734131813049, 1.3862175941467285, 0.3200359344482422, 1.0083924531936646, 1.4043989181518555, 0.8824291825294495, 1.3524525165557861, 1.5019259452819824, 1.3737897872924805, 1.7820496559143066, 1.1451597213745117, 1.430267572402954, 1.3442516326904297, 0.47038426995277405, 0.8051812648773193, 1.2329400777816772, 1.362059235572815]
[2025-05-27 14:19:26,446]: Mean: 1.16411376
[2025-05-27 14:19:26,446]: Min: 0.32003593
[2025-05-27 14:19:26,446]: Max: 1.89653873
[2025-05-27 14:19:26,447]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-27 14:19:26,448]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,448]: Mean: -0.00078738
[2025-05-27 14:19:26,448]: Min: -0.67896813
[2025-05-27 14:19:26,448]: Max: 0.33948407
[2025-05-27 14:19:26,448]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-27 14:19:26,448]: Sample Values (25 elements): [3.832503758793848e-12, 0.597528338432312, 1.5325151681900024, 0.6354432702064514, 1.4704135656356812, 0.30807188153266907, 1.3189904689788818, 0.5567444562911987, 0.7482460141181946, 0.9237698912620544, 1.2161169052124023, 0.8415092825889587, 0.4793883264064789, -4.913232675615674e-41, 0.5442706346511841, 0.6235853433609009, 0.5881868004798889, 0.9522074460983276, 0.7357274889945984, 1.2409030199050903, 1.3028706312179565, 1.230663776397705, 0.5715514421463013, 0.6333199143409729, 0.07070176303386688]
[2025-05-27 14:19:26,448]: Mean: 0.78272164
[2025-05-27 14:19:26,449]: Min: -0.00000000
[2025-05-27 14:19:26,449]: Max: 1.63044763
[2025-05-27 14:19:26,450]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 14:19:26,451]: Sample Values (25 elements): [0.0, 0.0, 0.2749383747577667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,452]: Mean: -0.00318278
[2025-05-27 14:19:26,452]: Min: -0.54987675
[2025-05-27 14:19:26,452]: Max: 0.27493837
[2025-05-27 14:19:26,452]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-27 14:19:26,452]: Sample Values (25 elements): [1.260810136795044, 1.2491135597229004, 1.0518745183944702, 1.0138362646102905, 1.2086832523345947, 1.2029004096984863, 1.3579227924346924, 0.26781758666038513, 0.970243513584137, 1.1584985256195068, 0.7764827609062195, 1.160370945930481, 1.1565788984298706, 0.9042636752128601, 0.6979004740715027, 1.1751856803894043, 0.20367586612701416, 1.0014512538909912, -7.730780940537127e-14, 1.0717185735702515, 0.9190635681152344, 1.2964216470718384, 1.0376590490341187, 0.8795623183250427, 0.07382645457983017]
[2025-05-27 14:19:26,453]: Mean: 0.94909847
[2025-05-27 14:19:26,453]: Min: -0.00000000
[2025-05-27 14:19:26,453]: Max: 1.56061065
[2025-05-27 14:19:26,454]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-27 14:19:26,454]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,454]: Mean: -0.00663493
[2025-05-27 14:19:26,454]: Min: -0.65485924
[2025-05-27 14:19:26,455]: Max: 0.32742962
[2025-05-27 14:19:26,455]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-27 14:19:26,455]: Sample Values (25 elements): [0.005576890893280506, 0.582836389541626, 1.1184841394424438, 1.1889461278915405, 1.0057802200317383, 0.9464704990386963, 0.9525514841079712, 0.9299237132072449, 0.4999621510505676, 0.9581847786903381, 0.6378377676010132, 0.8025630116462708, 0.6037642359733582, 0.7348776459693909, 0.5035561919212341, 0.007165548857301474, 1.026656150817871, 0.7714017033576965, 0.8048624396324158, 0.7130517363548279, 1.0462068319320679, 1.0538110733032227, 1.0696054697036743, 0.8377850651741028, 0.8966007828712463]
[2025-05-27 14:19:26,455]: Mean: 0.71793127
[2025-05-27 14:19:26,455]: Min: 0.00007658
[2025-05-27 14:19:26,455]: Max: 1.44822180
[2025-05-27 14:19:26,456]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 14:19:26,458]: Sample Values (25 elements): [0.0, 0.0, -0.2620282471179962, 0.0, 0.0, -0.2620282471179962, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2620282471179962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,458]: Mean: -0.00205598
[2025-05-27 14:19:26,458]: Min: -0.26202825
[2025-05-27 14:19:26,458]: Max: 0.52405649
[2025-05-27 14:19:26,458]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-27 14:19:26,459]: Sample Values (25 elements): [0.3673126697540283, 0.4838402569293976, 0.45225390791893005, 0.8964666128158569, 1.1542965173721313, 1.191001057624817, 1.0577600002288818, 1.016099214553833, 0.0013216383522376418, 0.9178252816200256, 0.2970931828022003, 0.7557197213172913, 0.06025132164359093, 0.9284451007843018, 0.29031428694725037, 0.009984572418034077, 0.603164792060852, 0.916888415813446, -4.960176174170555e-41, -4.927385790105354e-41, 0.4674384891986847, 1.0503965616226196, 1.039168119430542, 0.838498055934906, 1.456778883934021]
[2025-05-27 14:19:26,459]: Mean: 0.51347375
[2025-05-27 14:19:26,459]: Min: -0.00000000
[2025-05-27 14:19:26,459]: Max: 1.45677888
[2025-05-27 14:19:26,460]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 14:19:26,461]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22832129895687103, 0.0, 0.0, 0.0, 0.0, -0.22832129895687103, 0.0]
[2025-05-27 14:19:26,461]: Mean: -0.00074168
[2025-05-27 14:19:26,462]: Min: -0.22832130
[2025-05-27 14:19:26,462]: Max: 0.45664260
[2025-05-27 14:19:26,462]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-27 14:19:26,462]: Sample Values (25 elements): [-4.966762276952882e-41, 0.8785592317581177, 0.684377133846283, 0.7806030511856079, 1.1937313079833984, 0.6181309223175049, 0.6492910981178284, 1.0359768867492676, 0.16845040023326874, 0.45461732149124146, 1.0354397296905518, 0.6873854398727417, 0.6588431000709534, 0.8473142981529236, 0.6818249225616455, 0.03865814581513405, 0.02027747966349125, 1.118161916732788, 0.9921782612800598, 0.7983624935150146, 0.2782866954803467, 0.8237501382827759, 0.8857887387275696, 1.043925404548645, 1.1438233852386475]
[2025-05-27 14:19:26,462]: Mean: 0.88069093
[2025-05-27 14:19:26,462]: Min: -0.00000000
[2025-05-27 14:19:26,463]: Max: 1.61056447
[2025-05-27 14:19:26,463]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-27 14:19:26,466]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20673517882823944]
[2025-05-27 14:19:26,466]: Mean: -0.00021240
[2025-05-27 14:19:26,467]: Min: -0.20673518
[2025-05-27 14:19:26,467]: Max: 0.41347036
[2025-05-27 14:19:26,467]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-27 14:19:26,467]: Sample Values (25 elements): [-4.973488509581641e-41, 0.44115737080574036, 0.2574458420276642, 0.015064623206853867, 0.5408130884170532, 0.773478090763092, 0.8358858823776245, 5.14963172654727e-41, 0.967183530330658, 4.948405267070227e-41, 0.6321243047714233, 0.00592435896396637, 0.5373631715774536, 0.6280554533004761, 0.2728192210197449, 0.9724685549736023, 4.926545011026759e-41, 0.6609630584716797, 0.9199565649032593, 0.9281678795814514, 0.5980835556983948, 4.929767997494706e-41, -4.939997476284278e-41, 0.257686972618103, 0.5827845335006714]
[2025-05-27 14:19:26,467]: Mean: 0.43117616
[2025-05-27 14:19:26,467]: Min: -0.00000000
[2025-05-27 14:19:26,468]: Max: 1.42834628
[2025-05-27 14:19:26,468]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 14:19:26,475]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20900475978851318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,476]: Mean: 0.00001524
[2025-05-27 14:19:26,476]: Min: -0.41800952
[2025-05-27 14:19:26,476]: Max: 0.20900476
[2025-05-27 14:19:26,476]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-27 14:19:26,476]: Sample Values (25 elements): [0.810107409954071, 0.8903040289878845, 0.9167088866233826, -4.968443835110071e-41, 1.3164552450180054, 0.6455119848251343, 0.009795431047677994, -4.964099809870664e-41, 0.04505535587668419, 0.1842249631881714, 0.79207444190979, 0.6926499009132385, -5.005438114568247e-41, 1.0718436241149902, 1.309575080871582, 4.910009689147727e-41, 0.0006055250996723771, -4.952328902770336e-41, -4.974749678199533e-41, 0.8623022437095642, 0.2817308008670807, 0.48738783597946167, 0.8325771689414978, 0.8365485668182373, 1.0649107694625854]
[2025-05-27 14:19:26,476]: Mean: 0.48055467
[2025-05-27 14:19:26,477]: Min: -0.00000000
[2025-05-27 14:19:26,477]: Max: 1.38757753
[2025-05-27 14:19:26,478]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-27 14:19:26,478]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25217175483703613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,478]: Mean: -0.00209322
[2025-05-27 14:19:26,478]: Min: -0.25217175
[2025-05-27 14:19:26,479]: Max: 0.50434351
[2025-05-27 14:19:26,479]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-27 14:19:26,479]: Sample Values (25 elements): [0.4447808563709259, 2.7423423532013658e-08, 0.3862806260585785, 4.9614336603553966e-05, 0.4701365828514099, 0.2436232715845108, 0.3367666006088257, 0.08142540603876114, 0.3285602033138275, 0.3786812722682953, 0.26531487703323364, 5.17163211243717e-41, 0.048170313239097595, 0.5503875613212585, 4.281557157013788e-24, 0.168286994099617, 0.0033653676509857178, 4.937054749509196e-41, 0.014698694460093975, 4.914493844233566e-41, 0.671536386013031, 4.399382382184446e-33, -4.951628253538174e-41, 4.954010460927526e-41, 0.12952783703804016]
[2025-05-27 14:19:26,479]: Mean: 0.27662766
[2025-05-27 14:19:26,479]: Min: -0.00000000
[2025-05-27 14:19:26,479]: Max: 1.20080316
[2025-05-27 14:19:26,480]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 14:19:26,487]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.19830892980098724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,487]: Mean: -0.00033218
[2025-05-27 14:19:26,487]: Min: -0.39661786
[2025-05-27 14:19:26,487]: Max: 0.19830893
[2025-05-27 14:19:26,487]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-27 14:19:26,488]: Sample Values (25 elements): [0.3386958837509155, 5.724164096920445e-41, 5.504300367867881e-41, 0.4038896858692169, -4.980354872056832e-41, -5.590900612963155e-41, 0.637912929058075, 0.62871253490448, -6.245447125649277e-41, -5.471089594263383e-41, 0.7177441120147705, 0.08398661017417908, 5.035285771858365e-41, -5.109414460621148e-41, 0.32461923360824585, 4.973208249888776e-41, 6.165012593797033e-41, 0.49170026183128357, 4.917156311315783e-41, 0.6941331624984741, 4.923602284251677e-41, 0.36852869391441345, -4.974749678199533e-41, 4.4072094064484535e-12, 0.6415865421295166]
[2025-05-27 14:19:26,488]: Mean: 0.20648822
[2025-05-27 14:19:26,488]: Min: -0.00000000
[2025-05-27 14:19:26,488]: Max: 0.89967167
[2025-05-27 14:19:26,489]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 14:19:26,496]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21399061381816864, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,496]: Mean: 0.00017560
[2025-05-27 14:19:26,497]: Min: -0.21399061
[2025-05-27 14:19:26,497]: Max: 0.42798123
[2025-05-27 14:19:26,497]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-27 14:19:26,497]: Sample Values (25 elements): [0.05797474831342697, 0.5491493940353394, -4.937755398741358e-41, 0.5878903269767761, 0.89484041929245, 0.7113385200500488, 0.13095101714134216, 0.3497530519962311, 0.9019486904144287, 0.0013473865110427141, 0.67870032787323, 0.3108201324939728, 0.004331353586167097, 0.5681622624397278, 0.6544570326805115, 0.9383260607719421, -5.03332395400831e-41, 0.7528287172317505, 0.5499244332313538, -4.933551503348383e-41, 1.023725986480713, 1.133876919746399, 1.0141485929489136, 3.19399589443492e-07, 0.06645956635475159]
[2025-05-27 14:19:26,497]: Mean: 0.39975512
[2025-05-27 14:19:26,497]: Min: -0.49111038
[2025-05-27 14:19:26,498]: Max: 1.29629064
[2025-05-27 14:19:26,499]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-27 14:19:26,511]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,511]: Mean: 0.00001090
[2025-05-27 14:19:26,512]: Min: -0.21424425
[2025-05-27 14:19:26,512]: Max: 0.42848849
[2025-05-27 14:19:26,512]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-27 14:19:26,512]: Sample Values (25 elements): [-5.846077063316704e-41, 0.9127888679504395, 5.847338231934597e-41, -6.065520402829971e-41, 5.997277167617352e-41, 6.290989325739834e-41, -5.198396913105774e-41, 5.089375892581303e-41, -5.653959043857772e-41, 5.183262889691066e-41, 0.8738510608673096, 4.908047871297672e-41, 5.105350695074606e-41, 1.0177048444747925, -5.948091591519551e-41, 5.09764355352082e-41, -6.239561672099113e-41, 1.0080581903457642, 4.942239553827197e-41, 6.200045055405153e-41, -5.010482789039816e-41, -5.016788632129278e-41, -5.40424765751509e-41, -4.963118900945637e-41, 5.485943357985226e-41]
[2025-05-27 14:19:26,512]: Mean: 0.10519731
[2025-05-27 14:19:26,513]: Min: -0.00000000
[2025-05-27 14:19:26,513]: Max: 1.35473311
[2025-05-27 14:19:26,514]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 14:19:26,557]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,557]: Mean: -0.00006936
[2025-05-27 14:19:26,558]: Min: -0.35041961
[2025-05-27 14:19:26,558]: Max: 0.17520981
[2025-05-27 14:19:26,558]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-27 14:19:26,558]: Sample Values (25 elements): [0.5246586799621582, 0.5900706052780151, 0.5359789729118347, 0.6370170712471008, 0.5147114992141724, 4.965501108334989e-41, 0.3240366578102112, 0.5908041596412659, 0.4419281780719757, 0.6126036643981934, 0.4794616997241974, 0.2676805555820465, 0.2360832542181015, 0.8102532625198364, 0.6002255082130432, 0.5168019533157349, 0.679999053478241, 0.31461450457572937, 0.7226608395576477, 0.4960881471633911, 0.695705771446228, 0.5948161482810974, 0.36023426055908203, 0.6691896915435791, -4.914633974079998e-41]
[2025-05-27 14:19:26,558]: Mean: 0.42545733
[2025-05-27 14:19:26,558]: Min: -0.00000000
[2025-05-27 14:19:26,559]: Max: 0.97601032
[2025-05-27 14:19:26,560]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-27 14:19:26,561]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20265060663223267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,562]: Mean: -0.00057670
[2025-05-27 14:19:26,562]: Min: -0.20265061
[2025-05-27 14:19:26,562]: Max: 0.40530121
[2025-05-27 14:19:26,562]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-27 14:19:26,562]: Sample Values (25 elements): [0.24170427024364471, 0.37054020166397095, 0.017925096675753593, 0.5886040925979614, 5.49294985030685e-41, 0.21439707279205322, 0.24979911744594574, 0.5494980216026306, 0.7835754752159119, 0.5698100924491882, 5.066814987305674e-41, 0.4536810517311096, 0.5719120502471924, 0.5655948519706726, 4.964660329256394e-41, 0.5147002339363098, 0.4023868143558502, 0.47744250297546387, 0.8467040061950684, 0.5239580273628235, 0.46929341554641724, 0.5062922239303589, -4.956112408624013e-41, 0.332563191652298, 0.40226468443870544]
[2025-05-27 14:19:26,562]: Mean: 0.38440019
[2025-05-27 14:19:26,563]: Min: -0.00073281
[2025-05-27 14:19:26,563]: Max: 0.95704502
[2025-05-27 14:19:26,564]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 14:19:26,601]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,601]: Mean: -0.00001577
[2025-05-27 14:19:26,602]: Min: -0.47404629
[2025-05-27 14:19:26,602]: Max: 0.23702314
[2025-05-27 14:19:26,602]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-27 14:19:26,603]: Sample Values (25 elements): [-5.003616426564624e-41, -5.180600422608849e-41, 5.542415686097516e-41, 5.088955503042006e-41, -5.166307178272736e-41, -5.588098016034506e-41, -5.697539426098274e-41, -6.160808698404058e-41, -6.228351284384514e-41, 5.64877423953977e-41, -5.163924970883383e-41, 5.776852919179058e-41, 5.760457727146458e-41, -5.349316757713557e-41, -4.96101695324915e-41, -6.125776236795938e-41, 5.849860569170381e-41, 5.945849513976631e-41, 0.7891165614128113, 5.966028211862909e-41, 5.599308403749104e-41, 5.194753537098529e-41, -6.181127526136768e-41, 5.242958204271303e-41, -5.53358750577227e-41]
[2025-05-27 14:19:26,603]: Mean: 0.02718718
[2025-05-27 14:19:26,604]: Min: -0.00000000
[2025-05-27 14:19:26,604]: Max: 1.42347932
[2025-05-27 14:19:26,605]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 14:19:26,650]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 14:19:26,650]: Mean: -0.00018465
[2025-05-27 14:19:26,651]: Min: -0.37060413
[2025-05-27 14:19:26,651]: Max: 0.18530206
[2025-05-27 14:19:26,651]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-27 14:19:26,651]: Sample Values (25 elements): [0.7016773819923401, 0.44680118560791016, 0.46161654591560364, 0.6266564726829529, 0.8992208242416382, 0.6293313503265381, -5.946269903515929e-41, 0.0009449778008274734, 3.043950016490271e-07, 0.532805323600769, 0.5014906525611877, 0.4159185588359833, 0.4377107620239258, 0.26379474997520447, 0.5606333017349243, 0.5534132122993469, 0.6217250823974609, 0.6681978106498718, 0.6284757852554321, 0.654813289642334, 6.080671823838202e-07, 5.133190539297061e-10, 0.4461750090122223, 0.42349371314048767, -6.059354689586941e-41]
[2025-05-27 14:19:26,651]: Mean: 0.46001980
[2025-05-27 14:19:26,651]: Min: -0.09429806
[2025-05-27 14:19:26,652]: Max: 1.07349968
[2025-05-27 14:19:26,652]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-27 14:19:26,652]: Sample Values (25 elements): [-0.00451666722074151, 0.033007215708494186, -0.07349342107772827, -0.07440899312496185, 0.024069543927907944, -0.009030972607433796, -0.009678900241851807, -0.045422933995723724, -0.16754572093486786, 6.116667796777827e-41, -0.04041619598865509, 0.0006124211940914392, -0.10025228559970856, 0.11925861984491348, 0.05054839700460434, -0.17154374718666077, 0.01833617500960827, -0.0006676159682683647, -0.02502109669148922, -1.3558174316585792e-07, 0.011403867043554783, -0.034234095364809036, -0.03472645580768585, 5.82323589834821e-41, 0.09749197959899902]
[2025-05-27 14:19:26,652]: Mean: -0.01562086
[2025-05-27 14:19:26,652]: Min: -0.26586673
[2025-05-27 14:19:26,652]: Max: 0.36354142
