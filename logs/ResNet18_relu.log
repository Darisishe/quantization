[2025-05-13 06:45:42,323]: 
Training ResNet18 with relu
[2025-05-13 06:47:01,287]: [ResNet18_relu] Epoch: 001 Train Loss: 1.6451 Train Acc: 0.3893 Eval Loss: 1.4252 Eval Acc: 0.4682 (LR: 0.001000)
[2025-05-13 06:48:19,412]: [ResNet18_relu] Epoch: 002 Train Loss: 1.2410 Train Acc: 0.5489 Eval Loss: 1.1259 Eval Acc: 0.5985 (LR: 0.001000)
[2025-05-13 06:49:37,190]: [ResNet18_relu] Epoch: 003 Train Loss: 1.0379 Train Acc: 0.6284 Eval Loss: 1.0130 Eval Acc: 0.6430 (LR: 0.001000)
[2025-05-13 06:50:59,541]: [ResNet18_relu] Epoch: 004 Train Loss: 0.9028 Train Acc: 0.6786 Eval Loss: 0.9142 Eval Acc: 0.6775 (LR: 0.001000)
[2025-05-13 06:52:30,448]: [ResNet18_relu] Epoch: 005 Train Loss: 0.8013 Train Acc: 0.7162 Eval Loss: 0.7675 Eval Acc: 0.7354 (LR: 0.001000)
[2025-05-13 06:54:01,366]: [ResNet18_relu] Epoch: 006 Train Loss: 0.7265 Train Acc: 0.7444 Eval Loss: 0.8544 Eval Acc: 0.7056 (LR: 0.001000)
[2025-05-13 06:55:29,570]: [ResNet18_relu] Epoch: 007 Train Loss: 0.6724 Train Acc: 0.7662 Eval Loss: 0.7365 Eval Acc: 0.7453 (LR: 0.001000)
[2025-05-13 06:56:55,452]: [ResNet18_relu] Epoch: 008 Train Loss: 0.6239 Train Acc: 0.7797 Eval Loss: 0.6245 Eval Acc: 0.7877 (LR: 0.001000)
[2025-05-13 06:58:20,790]: [ResNet18_relu] Epoch: 009 Train Loss: 0.5869 Train Acc: 0.7949 Eval Loss: 0.6565 Eval Acc: 0.7784 (LR: 0.001000)
[2025-05-13 06:59:46,189]: [ResNet18_relu] Epoch: 010 Train Loss: 0.5506 Train Acc: 0.8078 Eval Loss: 0.5790 Eval Acc: 0.8019 (LR: 0.001000)
[2025-05-13 07:01:27,371]: [ResNet18_relu] Epoch: 011 Train Loss: 0.5174 Train Acc: 0.8194 Eval Loss: 0.5481 Eval Acc: 0.8133 (LR: 0.001000)
[2025-05-13 07:02:53,051]: [ResNet18_relu] Epoch: 012 Train Loss: 0.4902 Train Acc: 0.8286 Eval Loss: 0.5755 Eval Acc: 0.8059 (LR: 0.001000)
[2025-05-13 07:04:18,582]: [ResNet18_relu] Epoch: 013 Train Loss: 0.4664 Train Acc: 0.8368 Eval Loss: 0.5387 Eval Acc: 0.8186 (LR: 0.001000)
[2025-05-13 07:05:44,103]: [ResNet18_relu] Epoch: 014 Train Loss: 0.4484 Train Acc: 0.8439 Eval Loss: 0.5251 Eval Acc: 0.8255 (LR: 0.001000)
[2025-05-13 07:07:09,663]: [ResNet18_relu] Epoch: 015 Train Loss: 0.4235 Train Acc: 0.8511 Eval Loss: 0.4743 Eval Acc: 0.8426 (LR: 0.001000)
[2025-05-13 07:08:35,226]: [ResNet18_relu] Epoch: 016 Train Loss: 0.4103 Train Acc: 0.8586 Eval Loss: 0.5058 Eval Acc: 0.8325 (LR: 0.001000)
[2025-05-13 07:10:00,907]: [ResNet18_relu] Epoch: 017 Train Loss: 0.3906 Train Acc: 0.8633 Eval Loss: 0.5152 Eval Acc: 0.8352 (LR: 0.001000)
[2025-05-13 07:11:26,279]: [ResNet18_relu] Epoch: 018 Train Loss: 0.3749 Train Acc: 0.8673 Eval Loss: 0.4909 Eval Acc: 0.8359 (LR: 0.001000)
[2025-05-13 07:12:49,377]: [ResNet18_relu] Epoch: 019 Train Loss: 0.3629 Train Acc: 0.8723 Eval Loss: 0.5224 Eval Acc: 0.8327 (LR: 0.001000)
[2025-05-13 07:14:07,385]: [ResNet18_relu] Epoch: 020 Train Loss: 0.3462 Train Acc: 0.8789 Eval Loss: 0.4486 Eval Acc: 0.8532 (LR: 0.001000)
[2025-05-13 07:15:27,199]: [ResNet18_relu] Epoch: 021 Train Loss: 0.3341 Train Acc: 0.8826 Eval Loss: 0.4359 Eval Acc: 0.8585 (LR: 0.001000)
[2025-05-13 07:16:54,612]: [ResNet18_relu] Epoch: 022 Train Loss: 0.3220 Train Acc: 0.8872 Eval Loss: 0.4556 Eval Acc: 0.8538 (LR: 0.001000)
[2025-05-13 07:18:25,187]: [ResNet18_relu] Epoch: 023 Train Loss: 0.3091 Train Acc: 0.8915 Eval Loss: 0.4479 Eval Acc: 0.8554 (LR: 0.001000)
[2025-05-13 07:19:55,843]: [ResNet18_relu] Epoch: 024 Train Loss: 0.3002 Train Acc: 0.8964 Eval Loss: 0.4313 Eval Acc: 0.8622 (LR: 0.001000)
[2025-05-13 07:21:26,813]: [ResNet18_relu] Epoch: 025 Train Loss: 0.2883 Train Acc: 0.8974 Eval Loss: 0.4273 Eval Acc: 0.8650 (LR: 0.001000)
[2025-05-13 07:22:57,535]: [ResNet18_relu] Epoch: 026 Train Loss: 0.2782 Train Acc: 0.9017 Eval Loss: 0.4115 Eval Acc: 0.8701 (LR: 0.001000)
[2025-05-13 07:24:28,057]: [ResNet18_relu] Epoch: 027 Train Loss: 0.2696 Train Acc: 0.9056 Eval Loss: 0.4123 Eval Acc: 0.8696 (LR: 0.001000)
[2025-05-13 07:25:58,560]: [ResNet18_relu] Epoch: 028 Train Loss: 0.2617 Train Acc: 0.9087 Eval Loss: 0.4211 Eval Acc: 0.8746 (LR: 0.001000)
[2025-05-13 07:27:29,270]: [ResNet18_relu] Epoch: 029 Train Loss: 0.2506 Train Acc: 0.9116 Eval Loss: 0.4413 Eval Acc: 0.8679 (LR: 0.001000)
[2025-05-13 07:28:59,489]: [ResNet18_relu] Epoch: 030 Train Loss: 0.2417 Train Acc: 0.9143 Eval Loss: 0.4248 Eval Acc: 0.8686 (LR: 0.001000)
[2025-05-13 07:30:29,744]: [ResNet18_relu] Epoch: 031 Train Loss: 0.2372 Train Acc: 0.9172 Eval Loss: 0.4048 Eval Acc: 0.8740 (LR: 0.001000)
[2025-05-13 07:32:00,427]: [ResNet18_relu] Epoch: 032 Train Loss: 0.2278 Train Acc: 0.9210 Eval Loss: 0.4382 Eval Acc: 0.8673 (LR: 0.001000)
[2025-05-13 07:33:30,760]: [ResNet18_relu] Epoch: 033 Train Loss: 0.2237 Train Acc: 0.9215 Eval Loss: 0.4253 Eval Acc: 0.8698 (LR: 0.001000)
[2025-05-13 07:34:54,274]: [ResNet18_relu] Epoch: 034 Train Loss: 0.2155 Train Acc: 0.9244 Eval Loss: 0.4424 Eval Acc: 0.8690 (LR: 0.001000)
[2025-05-13 07:36:15,716]: [ResNet18_relu] Epoch: 035 Train Loss: 0.2076 Train Acc: 0.9269 Eval Loss: 0.4080 Eval Acc: 0.8799 (LR: 0.001000)
[2025-05-13 07:37:37,285]: [ResNet18_relu] Epoch: 036 Train Loss: 0.2004 Train Acc: 0.9281 Eval Loss: 0.3990 Eval Acc: 0.8817 (LR: 0.001000)
[2025-05-13 07:39:00,169]: [ResNet18_relu] Epoch: 037 Train Loss: 0.1973 Train Acc: 0.9302 Eval Loss: 0.3854 Eval Acc: 0.8846 (LR: 0.001000)
[2025-05-13 07:40:23,831]: [ResNet18_relu] Epoch: 038 Train Loss: 0.1898 Train Acc: 0.9320 Eval Loss: 0.4081 Eval Acc: 0.8804 (LR: 0.001000)
[2025-05-13 07:41:44,201]: [ResNet18_relu] Epoch: 039 Train Loss: 0.1847 Train Acc: 0.9343 Eval Loss: 0.4367 Eval Acc: 0.8771 (LR: 0.001000)
[2025-05-13 07:43:08,025]: [ResNet18_relu] Epoch: 040 Train Loss: 0.1760 Train Acc: 0.9381 Eval Loss: 0.4376 Eval Acc: 0.8759 (LR: 0.001000)
[2025-05-13 07:44:32,801]: [ResNet18_relu] Epoch: 041 Train Loss: 0.1739 Train Acc: 0.9378 Eval Loss: 0.4069 Eval Acc: 0.8823 (LR: 0.001000)
[2025-05-13 07:45:59,043]: [ResNet18_relu] Epoch: 042 Train Loss: 0.1665 Train Acc: 0.9411 Eval Loss: 0.3882 Eval Acc: 0.8852 (LR: 0.001000)
[2025-05-13 07:47:28,035]: [ResNet18_relu] Epoch: 043 Train Loss: 0.1647 Train Acc: 0.9418 Eval Loss: 0.3959 Eval Acc: 0.8860 (LR: 0.001000)
[2025-05-13 07:49:00,791]: [ResNet18_relu] Epoch: 044 Train Loss: 0.1606 Train Acc: 0.9438 Eval Loss: 0.4040 Eval Acc: 0.8849 (LR: 0.001000)
[2025-05-13 07:50:24,085]: [ResNet18_relu] Epoch: 045 Train Loss: 0.1507 Train Acc: 0.9467 Eval Loss: 0.3888 Eval Acc: 0.8890 (LR: 0.001000)
[2025-05-13 07:51:42,927]: [ResNet18_relu] Epoch: 046 Train Loss: 0.1509 Train Acc: 0.9460 Eval Loss: 0.4113 Eval Acc: 0.8828 (LR: 0.001000)
[2025-05-13 07:53:02,026]: [ResNet18_relu] Epoch: 047 Train Loss: 0.1463 Train Acc: 0.9477 Eval Loss: 0.4119 Eval Acc: 0.8829 (LR: 0.001000)
[2025-05-13 07:54:21,596]: [ResNet18_relu] Epoch: 048 Train Loss: 0.1366 Train Acc: 0.9514 Eval Loss: 0.3969 Eval Acc: 0.8875 (LR: 0.001000)
[2025-05-13 07:55:41,605]: [ResNet18_relu] Epoch: 049 Train Loss: 0.1329 Train Acc: 0.9539 Eval Loss: 0.4022 Eval Acc: 0.8864 (LR: 0.001000)
[2025-05-13 07:57:02,296]: [ResNet18_relu] Epoch: 050 Train Loss: 0.1313 Train Acc: 0.9537 Eval Loss: 0.3837 Eval Acc: 0.8923 (LR: 0.001000)
[2025-05-13 07:58:21,943]: [ResNet18_relu] Epoch: 051 Train Loss: 0.1291 Train Acc: 0.9543 Eval Loss: 0.3811 Eval Acc: 0.8961 (LR: 0.001000)
[2025-05-13 07:59:39,571]: [ResNet18_relu] Epoch: 052 Train Loss: 0.1213 Train Acc: 0.9564 Eval Loss: 0.3721 Eval Acc: 0.8931 (LR: 0.001000)
[2025-05-13 08:00:56,997]: [ResNet18_relu] Epoch: 053 Train Loss: 0.1241 Train Acc: 0.9561 Eval Loss: 0.4288 Eval Acc: 0.8853 (LR: 0.001000)
[2025-05-13 08:02:18,681]: [ResNet18_relu] Epoch: 054 Train Loss: 0.1209 Train Acc: 0.9573 Eval Loss: 0.4158 Eval Acc: 0.8894 (LR: 0.001000)
[2025-05-13 08:03:40,322]: [ResNet18_relu] Epoch: 055 Train Loss: 0.1123 Train Acc: 0.9601 Eval Loss: 0.4201 Eval Acc: 0.8889 (LR: 0.001000)
[2025-05-13 08:05:01,977]: [ResNet18_relu] Epoch: 056 Train Loss: 0.1148 Train Acc: 0.9593 Eval Loss: 0.4178 Eval Acc: 0.8890 (LR: 0.001000)
[2025-05-13 08:06:23,816]: [ResNet18_relu] Epoch: 057 Train Loss: 0.1081 Train Acc: 0.9626 Eval Loss: 0.4304 Eval Acc: 0.8896 (LR: 0.001000)
[2025-05-13 08:07:45,668]: [ResNet18_relu] Epoch: 058 Train Loss: 0.1053 Train Acc: 0.9637 Eval Loss: 0.4051 Eval Acc: 0.8941 (LR: 0.001000)
[2025-05-13 08:09:07,317]: [ResNet18_relu] Epoch: 059 Train Loss: 0.1042 Train Acc: 0.9626 Eval Loss: 0.4152 Eval Acc: 0.8914 (LR: 0.001000)
[2025-05-13 08:10:29,536]: [ResNet18_relu] Epoch: 060 Train Loss: 0.0995 Train Acc: 0.9648 Eval Loss: 0.3979 Eval Acc: 0.8960 (LR: 0.001000)
[2025-05-13 08:11:50,979]: [ResNet18_relu] Epoch: 061 Train Loss: 0.0989 Train Acc: 0.9643 Eval Loss: 0.4065 Eval Acc: 0.8959 (LR: 0.001000)
[2025-05-13 08:13:12,437]: [ResNet18_relu] Epoch: 062 Train Loss: 0.0940 Train Acc: 0.9664 Eval Loss: 0.4362 Eval Acc: 0.8887 (LR: 0.001000)
[2025-05-13 08:14:33,877]: [ResNet18_relu] Epoch: 063 Train Loss: 0.0901 Train Acc: 0.9688 Eval Loss: 0.4606 Eval Acc: 0.8865 (LR: 0.001000)
[2025-05-13 08:15:55,428]: [ResNet18_relu] Epoch: 064 Train Loss: 0.0889 Train Acc: 0.9687 Eval Loss: 0.4439 Eval Acc: 0.8926 (LR: 0.001000)
[2025-05-13 08:17:11,685]: [ResNet18_relu] Epoch: 065 Train Loss: 0.0899 Train Acc: 0.9680 Eval Loss: 0.4089 Eval Acc: 0.8955 (LR: 0.001000)
[2025-05-13 08:18:27,714]: [ResNet18_relu] Epoch: 066 Train Loss: 0.0814 Train Acc: 0.9711 Eval Loss: 0.4251 Eval Acc: 0.8943 (LR: 0.001000)
[2025-05-13 08:19:43,758]: [ResNet18_relu] Epoch: 067 Train Loss: 0.0841 Train Acc: 0.9713 Eval Loss: 0.4286 Eval Acc: 0.8935 (LR: 0.001000)
[2025-05-13 08:20:59,797]: [ResNet18_relu] Epoch: 068 Train Loss: 0.0810 Train Acc: 0.9724 Eval Loss: 0.4160 Eval Acc: 0.8960 (LR: 0.001000)
[2025-05-13 08:22:15,834]: [ResNet18_relu] Epoch: 069 Train Loss: 0.0760 Train Acc: 0.9734 Eval Loss: 0.4459 Eval Acc: 0.8933 (LR: 0.001000)
[2025-05-13 08:23:31,885]: [ResNet18_relu] Epoch: 070 Train Loss: 0.0782 Train Acc: 0.9718 Eval Loss: 0.4410 Eval Acc: 0.8941 (LR: 0.000100)
[2025-05-13 08:24:47,960]: [ResNet18_relu] Epoch: 071 Train Loss: 0.0502 Train Acc: 0.9835 Eval Loss: 0.3624 Eval Acc: 0.9109 (LR: 0.000100)
[2025-05-13 08:26:03,981]: [ResNet18_relu] Epoch: 072 Train Loss: 0.0408 Train Acc: 0.9871 Eval Loss: 0.3590 Eval Acc: 0.9106 (LR: 0.000100)
[2025-05-13 08:27:20,023]: [ResNet18_relu] Epoch: 073 Train Loss: 0.0370 Train Acc: 0.9886 Eval Loss: 0.3560 Eval Acc: 0.9116 (LR: 0.000100)
[2025-05-13 08:28:36,063]: [ResNet18_relu] Epoch: 074 Train Loss: 0.0344 Train Acc: 0.9893 Eval Loss: 0.3562 Eval Acc: 0.9130 (LR: 0.000100)
[2025-05-13 08:29:52,141]: [ResNet18_relu] Epoch: 075 Train Loss: 0.0345 Train Acc: 0.9902 Eval Loss: 0.3562 Eval Acc: 0.9129 (LR: 0.000100)
[2025-05-13 08:31:08,134]: [ResNet18_relu] Epoch: 076 Train Loss: 0.0331 Train Acc: 0.9902 Eval Loss: 0.3579 Eval Acc: 0.9138 (LR: 0.000100)
[2025-05-13 08:32:24,200]: [ResNet18_relu] Epoch: 077 Train Loss: 0.0323 Train Acc: 0.9905 Eval Loss: 0.3555 Eval Acc: 0.9156 (LR: 0.000100)
[2025-05-13 08:33:40,424]: [ResNet18_relu] Epoch: 078 Train Loss: 0.0300 Train Acc: 0.9917 Eval Loss: 0.3560 Eval Acc: 0.9137 (LR: 0.000100)
[2025-05-13 08:34:56,450]: [ResNet18_relu] Epoch: 079 Train Loss: 0.0311 Train Acc: 0.9908 Eval Loss: 0.3611 Eval Acc: 0.9130 (LR: 0.000100)
[2025-05-13 08:36:13,283]: [ResNet18_relu] Epoch: 080 Train Loss: 0.0307 Train Acc: 0.9911 Eval Loss: 0.3576 Eval Acc: 0.9143 (LR: 0.000100)
[2025-05-13 08:37:29,447]: [ResNet18_relu] Epoch: 081 Train Loss: 0.0290 Train Acc: 0.9918 Eval Loss: 0.3566 Eval Acc: 0.9162 (LR: 0.000100)
[2025-05-13 08:38:49,676]: [ResNet18_relu] Epoch: 082 Train Loss: 0.0290 Train Acc: 0.9916 Eval Loss: 0.3590 Eval Acc: 0.9137 (LR: 0.000100)
[2025-05-13 08:40:08,907]: [ResNet18_relu] Epoch: 083 Train Loss: 0.0286 Train Acc: 0.9920 Eval Loss: 0.3649 Eval Acc: 0.9119 (LR: 0.000100)
[2025-05-13 08:41:33,942]: [ResNet18_relu] Epoch: 084 Train Loss: 0.0291 Train Acc: 0.9917 Eval Loss: 0.3610 Eval Acc: 0.9122 (LR: 0.000100)
[2025-05-13 08:42:57,553]: [ResNet18_relu] Epoch: 085 Train Loss: 0.0277 Train Acc: 0.9919 Eval Loss: 0.3604 Eval Acc: 0.9136 (LR: 0.000100)
[2025-05-13 08:44:22,986]: [ResNet18_relu] Epoch: 086 Train Loss: 0.0273 Train Acc: 0.9922 Eval Loss: 0.3623 Eval Acc: 0.9137 (LR: 0.000100)
[2025-05-13 08:45:45,262]: [ResNet18_relu] Epoch: 087 Train Loss: 0.0265 Train Acc: 0.9932 Eval Loss: 0.3658 Eval Acc: 0.9134 (LR: 0.000100)
[2025-05-13 08:47:06,049]: [ResNet18_relu] Epoch: 088 Train Loss: 0.0253 Train Acc: 0.9926 Eval Loss: 0.3592 Eval Acc: 0.9126 (LR: 0.000100)
[2025-05-13 08:48:31,488]: [ResNet18_relu] Epoch: 089 Train Loss: 0.0260 Train Acc: 0.9927 Eval Loss: 0.3632 Eval Acc: 0.9134 (LR: 0.000100)
[2025-05-13 08:49:53,755]: [ResNet18_relu] Epoch: 090 Train Loss: 0.0259 Train Acc: 0.9925 Eval Loss: 0.3667 Eval Acc: 0.9128 (LR: 0.000100)
[2025-05-13 08:51:11,194]: [ResNet18_relu] Epoch: 091 Train Loss: 0.0247 Train Acc: 0.9932 Eval Loss: 0.3678 Eval Acc: 0.9123 (LR: 0.000100)
[2025-05-13 08:52:29,385]: [ResNet18_relu] Epoch: 092 Train Loss: 0.0251 Train Acc: 0.9929 Eval Loss: 0.3665 Eval Acc: 0.9123 (LR: 0.000100)
[2025-05-13 08:53:46,840]: [ResNet18_relu] Epoch: 093 Train Loss: 0.0240 Train Acc: 0.9933 Eval Loss: 0.3680 Eval Acc: 0.9126 (LR: 0.000100)
[2025-05-13 08:55:19,893]: [ResNet18_relu] Epoch: 094 Train Loss: 0.0254 Train Acc: 0.9927 Eval Loss: 0.3662 Eval Acc: 0.9135 (LR: 0.000100)
[2025-05-13 08:56:55,152]: [ResNet18_relu] Epoch: 095 Train Loss: 0.0246 Train Acc: 0.9926 Eval Loss: 0.3665 Eval Acc: 0.9126 (LR: 0.000100)
[2025-05-13 08:58:30,615]: [ResNet18_relu] Epoch: 096 Train Loss: 0.0230 Train Acc: 0.9939 Eval Loss: 0.3673 Eval Acc: 0.9153 (LR: 0.000100)
[2025-05-13 09:00:04,425]: [ResNet18_relu] Epoch: 097 Train Loss: 0.0247 Train Acc: 0.9928 Eval Loss: 0.3681 Eval Acc: 0.9128 (LR: 0.000100)
[2025-05-13 09:01:43,043]: [ResNet18_relu] Epoch: 098 Train Loss: 0.0233 Train Acc: 0.9934 Eval Loss: 0.3724 Eval Acc: 0.9138 (LR: 0.000100)
[2025-05-13 09:03:21,440]: [ResNet18_relu] Epoch: 099 Train Loss: 0.0239 Train Acc: 0.9928 Eval Loss: 0.3720 Eval Acc: 0.9117 (LR: 0.000100)
[2025-05-13 09:05:01,898]: [ResNet18_relu] Epoch: 100 Train Loss: 0.0227 Train Acc: 0.9936 Eval Loss: 0.3719 Eval Acc: 0.9123 (LR: 0.000010)
[2025-05-13 09:06:24,356]: [ResNet18_relu] Epoch: 101 Train Loss: 0.0227 Train Acc: 0.9938 Eval Loss: 0.3720 Eval Acc: 0.9123 (LR: 0.000010)
[2025-05-13 09:07:41,644]: [ResNet18_relu] Epoch: 102 Train Loss: 0.0236 Train Acc: 0.9933 Eval Loss: 0.3714 Eval Acc: 0.9135 (LR: 0.000010)
[2025-05-13 09:08:59,029]: [ResNet18_relu] Epoch: 103 Train Loss: 0.0230 Train Acc: 0.9935 Eval Loss: 0.3706 Eval Acc: 0.9136 (LR: 0.000010)
[2025-05-13 09:10:20,419]: [ResNet18_relu] Epoch: 104 Train Loss: 0.0219 Train Acc: 0.9941 Eval Loss: 0.3702 Eval Acc: 0.9138 (LR: 0.000010)
[2025-05-13 09:11:37,272]: [ResNet18_relu] Epoch: 105 Train Loss: 0.0228 Train Acc: 0.9938 Eval Loss: 0.3732 Eval Acc: 0.9121 (LR: 0.000010)
[2025-05-13 09:12:54,159]: [ResNet18_relu] Epoch: 106 Train Loss: 0.0224 Train Acc: 0.9938 Eval Loss: 0.3712 Eval Acc: 0.9132 (LR: 0.000010)
[2025-05-13 09:14:39,109]: [ResNet18_relu] Epoch: 107 Train Loss: 0.0217 Train Acc: 0.9937 Eval Loss: 0.3706 Eval Acc: 0.9128 (LR: 0.000010)
[2025-05-13 09:16:19,548]: [ResNet18_relu] Epoch: 108 Train Loss: 0.0216 Train Acc: 0.9940 Eval Loss: 0.3670 Eval Acc: 0.9145 (LR: 0.000010)
[2025-05-13 09:17:56,500]: [ResNet18_relu] Epoch: 109 Train Loss: 0.0216 Train Acc: 0.9942 Eval Loss: 0.3705 Eval Acc: 0.9131 (LR: 0.000010)
[2025-05-13 09:19:31,833]: [ResNet18_relu] Epoch: 110 Train Loss: 0.0231 Train Acc: 0.9933 Eval Loss: 0.3721 Eval Acc: 0.9130 (LR: 0.000010)
[2025-05-13 09:20:51,435]: [ResNet18_relu] Epoch: 111 Train Loss: 0.0210 Train Acc: 0.9939 Eval Loss: 0.3715 Eval Acc: 0.9128 (LR: 0.000010)
[2025-05-13 09:20:51,435]: Early stopping was triggered!
[2025-05-13 09:20:51,435]: [ResNet18_relu] Best Eval Accuracy: 0.9162
[2025-05-13 09:20:51,525]: 
Training of full-precision model finished!
[2025-05-13 09:20:51,525]: Model Architecture:
[2025-05-13 09:20:51,531]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 09:20:51,531]: 
Model Weights:
[2025-05-13 09:20:51,531]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-13 09:20:51,567]: Sample Values (25 elements): [0.08686971664428711, 0.11910133808851242, -0.10746756196022034, -0.12872175872325897, 0.14957195520401, 0.005075041204690933, 0.1752346307039261, 0.0816831886768341, 0.13841305673122406, 0.09271947294473648, 0.17360034584999084, -0.0992218554019928, 0.10678703337907791, 0.030153054744005203, -0.10791236162185669, 0.1944097876548767, 0.04135219752788544, -0.07750607281923294, 0.2262645959854126, -0.18077893555164337, -0.136540949344635, -0.021266572177410126, -0.16850753128528595, -0.1291997879743576, -0.140656977891922]
[2025-05-13 09:20:51,579]: Mean: -0.00016766
[2025-05-13 09:20:51,592]: Min: -0.28825995
[2025-05-13 09:20:51,594]: Max: 0.36740267
[2025-05-13 09:20:51,594]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-13 09:20:51,595]: Sample Values (25 elements): [0.9161608219146729, 0.9658447504043579, 0.9367368221282959, 0.9379570484161377, 0.9242203235626221, 1.0023207664489746, 0.9424605965614319, 1.001977562904358, 1.03970205783844, 1.0493181943893433, 0.9348084330558777, 0.96816486120224, 0.9367014765739441, 1.0149147510528564, 0.935062050819397, 1.000282645225525, 0.9546409249305725, 1.0107485055923462, 0.8990330696105957, 0.9692529439926147, 0.9123629331588745, 0.9697214961051941, 0.9817608594894409, 0.9478117823600769, 0.9770710468292236]
[2025-05-13 09:20:51,595]: Mean: 0.95509344
[2025-05-13 09:20:51,595]: Min: 0.86990637
[2025-05-13 09:20:51,596]: Max: 1.04931819
[2025-05-13 09:20:51,596]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 09:20:51,597]: Sample Values (25 elements): [-0.014378750696778297, 0.024730566889047623, -0.024479860439896584, 0.021212933585047722, 0.03663409873843193, -0.035895198583602905, -0.04494795575737953, 0.052289243787527084, -0.024131303653120995, 0.036707803606987, -0.023670317605137825, 0.027556223794817924, -0.007465712260454893, 0.03997388109564781, 0.023050006479024887, -0.0065770223736763, -0.010806398466229439, 0.0038101067766547203, 0.04127341881394386, 0.015209090895950794, -0.0019542258232831955, -0.0067208982072770596, 0.017643531784415245, -0.006377785932272673, -0.030158022418618202]
[2025-05-13 09:20:51,598]: Mean: -0.00127221
[2025-05-13 09:20:51,599]: Min: -0.10838763
[2025-05-13 09:20:51,599]: Max: 0.12142725
[2025-05-13 09:20:51,599]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 09:20:51,600]: Sample Values (25 elements): [0.9528061747550964, 0.9475916028022766, 0.9497831463813782, 0.9871026873588562, 0.9617070555686951, 0.9647998809814453, 0.9640510678291321, 0.983717679977417, 0.9726056456565857, 0.9618957042694092, 0.9989513158798218, 0.9859781861305237, 0.9629413485527039, 0.939050555229187, 0.9722042083740234, 0.9792078733444214, 0.9721353650093079, 0.9479827880859375, 0.9735663533210754, 0.937876284122467, 0.9249157905578613, 0.9738521575927734, 0.9660178422927856, 0.9578449130058289, 0.9676192998886108]
[2025-05-13 09:20:51,600]: Mean: 0.97125554
[2025-05-13 09:20:51,600]: Min: 0.92491579
[2025-05-13 09:20:51,600]: Max: 1.07596481
[2025-05-13 09:20:51,600]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 09:20:51,601]: Sample Values (25 elements): [-0.027260124683380127, -0.008736491203308105, -0.001426774193532765, -0.0421626940369606, -0.012348142452538013, -2.0827874323003925e-05, 0.01961432583630085, 0.02432779036462307, 0.021388132125139236, 0.009551365859806538, -0.0012067985953763127, -0.009631074965000153, 0.03938058018684387, -0.02130098268389702, 0.038255661725997925, -0.011314207687973976, 0.009421853348612785, -0.003510340116918087, 0.018775951117277145, -0.024139204993844032, 0.00016889908874873072, -0.030425826087594032, 0.011406959034502506, 0.018556827679276466, -0.01859411410987377]
[2025-05-13 09:20:51,601]: Mean: -0.00130498
[2025-05-13 09:20:51,601]: Min: -0.10231776
[2025-05-13 09:20:51,601]: Max: 0.10518283
[2025-05-13 09:20:51,601]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 09:20:51,601]: Sample Values (25 elements): [0.9458007216453552, 0.9929468631744385, 0.9588289260864258, 0.9759990572929382, 1.0028115510940552, 0.9797859787940979, 0.9543742537498474, 0.9555504322052002, 0.9655922055244446, 0.9779936671257019, 0.9974454045295715, 0.9766199588775635, 0.9516106247901917, 0.9837445020675659, 1.0093671083450317, 0.967339277267456, 0.9823774695396423, 0.9886981844902039, 0.9814002513885498, 0.9647508263587952, 0.9858139157295227, 0.9993311166763306, 0.9827751517295837, 0.9698679447174072, 0.9847873449325562]
[2025-05-13 09:20:51,602]: Mean: 0.97416854
[2025-05-13 09:20:51,602]: Min: 0.94188452
[2025-05-13 09:20:51,602]: Max: 1.01740241
[2025-05-13 09:20:51,602]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 09:20:51,603]: Sample Values (25 elements): [0.036008529365062714, 0.02377013862133026, -0.032571084797382355, -0.024814309552311897, 0.029514452442526817, 0.04551607370376587, 0.006245458964258432, 0.0063582295551896095, 0.03938520327210426, -0.01674370840191841, -0.016468403860926628, 0.03769322857260704, 0.02251248061656952, -0.008482340723276138, -0.03628132864832878, 0.014045543037354946, 0.033758021891117096, 0.044406939297914505, -0.013233672827482224, -0.012217702344059944, 0.01908452808856964, 0.0006413350347429514, 0.026627356186509132, 0.052830662578344345, 0.007594160735607147]
[2025-05-13 09:20:51,603]: Mean: -0.00067752
[2025-05-13 09:20:51,603]: Min: -0.11001772
[2025-05-13 09:20:51,603]: Max: 0.09948476
[2025-05-13 09:20:51,603]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 09:20:51,603]: Sample Values (25 elements): [0.9586660265922546, 0.9512531757354736, 0.9535692930221558, 0.9779895544052124, 0.9648137092590332, 0.9738888740539551, 0.9639782309532166, 0.9891846179962158, 0.970818817615509, 0.9532720446586609, 0.9655112028121948, 0.9640709161758423, 0.9630138874053955, 0.9762567281723022, 0.9763320684432983, 0.9840664267539978, 0.9703246355056763, 0.9709456562995911, 0.9843403697013855, 0.9660760760307312, 0.954874575138092, 0.9633517861366272, 0.9684994220733643, 0.9726428985595703, 0.9715731143951416]
[2025-05-13 09:20:51,604]: Mean: 0.97160113
[2025-05-13 09:20:51,604]: Min: 0.95125318
[2025-05-13 09:20:51,604]: Max: 1.00334227
[2025-05-13 09:20:51,604]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 09:20:51,604]: Sample Values (25 elements): [0.006589887198060751, 0.009890257380902767, 0.005934621673077345, -0.03934185206890106, -0.01058201864361763, -0.04399924725294113, 0.01611204259097576, -0.023805836215615273, 0.011224010027945042, 0.027485273778438568, -0.04348728433251381, -0.042907748371362686, 0.044984567910432816, 0.04809490591287613, -0.04736506938934326, -0.008699941448867321, -0.03771256282925606, -0.008297461085021496, 0.01969207637012005, 0.029026657342910767, 0.01199690718203783, 0.0056425537914037704, 0.0260404534637928, -0.007042969111353159, -0.008504173718392849]
[2025-05-13 09:20:51,605]: Mean: -0.00032828
[2025-05-13 09:20:51,605]: Min: -0.08454981
[2025-05-13 09:20:51,605]: Max: 0.08158658
[2025-05-13 09:20:51,605]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 09:20:51,605]: Sample Values (25 elements): [0.9653866291046143, 0.9846591353416443, 0.9852561950683594, 1.0052173137664795, 0.9715246558189392, 0.97081059217453, 0.9879514575004578, 1.0014779567718506, 0.9925888776779175, 0.9775623679161072, 0.9681101441383362, 0.956669270992279, 0.9855415225028992, 0.9904579520225525, 0.9853229522705078, 0.9738487005233765, 1.0155833959579468, 0.9883406758308411, 0.9739629030227661, 0.9872048497200012, 0.9783647060394287, 0.9740201234817505, 0.9694346785545349, 0.9812127351760864, 0.9777284860610962]
[2025-05-13 09:20:51,605]: Mean: 0.98298335
[2025-05-13 09:20:51,606]: Min: 0.95217079
[2025-05-13 09:20:51,606]: Max: 1.02061355
[2025-05-13 09:20:51,606]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-13 09:20:51,607]: Sample Values (25 elements): [0.018906138837337494, 0.004841163754463196, -0.006372413598001003, -0.01251438818871975, -0.033306632190942764, -0.0383535660803318, -0.03752750903367996, -0.005724788643419743, 0.011456229723989964, -0.011235179379582405, -0.026889938861131668, -0.025088202208280563, 0.014222493395209312, 0.03179444000124931, -0.0056549906730651855, -0.042237456887960434, 0.0307219959795475, -0.018636787310242653, 0.013665045611560345, -0.02272786758840084, 0.041330110281705856, -0.026978392153978348, -0.006806542631238699, -0.009520498104393482, 0.0026805223897099495]
[2025-05-13 09:20:51,607]: Mean: -0.00028944
[2025-05-13 09:20:51,607]: Min: -0.07210150
[2025-05-13 09:20:51,607]: Max: 0.07719382
[2025-05-13 09:20:51,607]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-13 09:20:51,607]: Sample Values (25 elements): [0.9727284908294678, 0.9688749313354492, 0.979542076587677, 0.9738596677780151, 0.9763303399085999, 0.9669371247291565, 0.9671739339828491, 0.9683566689491272, 0.9673128128051758, 0.9659539461135864, 0.9736157059669495, 0.9734084606170654, 0.9713993072509766, 0.9702889323234558, 0.9728026390075684, 0.9775645136833191, 0.9610016942024231, 0.9734848141670227, 0.9705934524536133, 0.9796010255813599, 0.9817379117012024, 0.9794352650642395, 0.9635545611381531, 0.9742875695228577, 0.9747337102890015]
[2025-05-13 09:20:51,608]: Mean: 0.97178698
[2025-05-13 09:20:51,608]: Min: 0.95776534
[2025-05-13 09:20:51,608]: Max: 0.98830485
[2025-05-13 09:20:51,608]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 09:20:51,610]: Sample Values (25 elements): [-0.014368654228746891, -0.009207836352288723, 0.011919328011572361, -0.031638454645872116, 0.015698183327913284, -0.009892693720757961, 0.018395621329545975, 0.022588610649108887, 0.0004916639300063252, -0.033633485436439514, 0.003441578708589077, -0.022945959120988846, -0.03757334128022194, 0.03501908481121063, 0.007555995602160692, 0.008457682095468044, -0.0035902122035622597, 0.01528395339846611, -0.03673717379570007, 0.029657656326889992, 0.01190110296010971, 0.022070011124014854, -0.004820606205612421, 0.025442901998758316, -0.018312402069568634]
[2025-05-13 09:20:51,610]: Mean: -0.00062268
[2025-05-13 09:20:51,610]: Min: -0.05949827
[2025-05-13 09:20:51,610]: Max: 0.06748597
[2025-05-13 09:20:51,610]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-13 09:20:51,611]: Sample Values (25 elements): [0.9745742082595825, 0.9679796099662781, 0.9665657877922058, 0.9792799353599548, 0.979412317276001, 0.9761465787887573, 0.9712010622024536, 0.9663692116737366, 0.9824559688568115, 0.9806671142578125, 0.9688297510147095, 0.9701125621795654, 0.9650600552558899, 0.9642542004585266, 0.9781897068023682, 0.9763638377189636, 0.9697890877723694, 0.9808043837547302, 0.9791341423988342, 0.9756850600242615, 0.9680464267730713, 0.9715130925178528, 0.9764065742492676, 0.9729689359664917, 0.9796109199523926]
[2025-05-13 09:20:51,611]: Mean: 0.97335643
[2025-05-13 09:20:51,611]: Min: 0.95901126
[2025-05-13 09:20:51,612]: Max: 0.99280089
[2025-05-13 09:20:51,612]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-13 09:20:51,612]: Sample Values (25 elements): [0.058362554758787155, -0.10392341017723083, -0.05565602704882622, 0.02597694657742977, -0.02552373893558979, 0.1131281927227974, -0.0225596334785223, -0.021953392773866653, 0.012129132635891438, 0.10474301874637604, -0.0517282709479332, 0.128754660487175, -0.07397408038377762, 0.052234403789043427, 0.02526232972741127, 0.044935192912817, -0.10449311137199402, 0.04029012471437454, 0.014753011055290699, -0.06545397639274597, 0.08148182183504105, -0.12042859941720963, -0.004367027897387743, 0.038450226187705994, 0.04040677100419998]
[2025-05-13 09:20:51,612]: Mean: -0.00023159
[2025-05-13 09:20:51,612]: Min: -0.14165069
[2025-05-13 09:20:51,613]: Max: 0.14467429
[2025-05-13 09:20:51,613]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-13 09:20:51,613]: Sample Values (25 elements): [0.9615041017532349, 0.9602236151695251, 0.9636942148208618, 0.9550809860229492, 0.9494730234146118, 0.9573333859443665, 0.9596942067146301, 0.9594809412956238, 0.9463399052619934, 0.9642594456672668, 0.9541209936141968, 0.9610065817832947, 0.963283121585846, 0.9523523449897766, 0.9575566649436951, 0.9593713283538818, 0.9543919563293457, 0.9643393754959106, 0.9573895931243896, 0.9679778814315796, 0.957396924495697, 0.9527370929718018, 0.960669219493866, 0.9568608403205872, 0.953669548034668]
[2025-05-13 09:20:51,613]: Mean: 0.95500439
[2025-05-13 09:20:51,613]: Min: 0.93480986
[2025-05-13 09:20:51,613]: Max: 0.96820360
[2025-05-13 09:20:51,613]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 09:20:51,615]: Sample Values (25 elements): [0.0329095833003521, -0.005545868072658777, 0.03351958841085434, -0.009739166125655174, 0.0038655265234410763, -0.0001737887941999361, -0.002969351364299655, -0.009840156883001328, 0.005126320291310549, 0.002354986732825637, -0.008471786044538021, 0.026994915679097176, 0.02343776822090149, 0.016647841781377792, 0.007522830739617348, -0.024554742500185966, -0.010035505518317223, 0.008241322822868824, -0.009910840541124344, 0.012450939044356346, -0.007039458025246859, 0.015137200243771076, -0.01631755381822586, 0.0009020859724842012, 0.003411213867366314]
[2025-05-13 09:20:51,615]: Mean: -0.00052792
[2025-05-13 09:20:51,615]: Min: -0.05933470
[2025-05-13 09:20:51,615]: Max: 0.06418770
[2025-05-13 09:20:51,615]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-13 09:20:51,616]: Sample Values (25 elements): [0.9687992334365845, 0.9668901562690735, 0.9711363315582275, 0.9661746025085449, 0.9578468203544617, 0.971240758895874, 0.9721472263336182, 0.9718194603919983, 0.9710179567337036, 0.9816501140594482, 0.967136800289154, 0.9750387072563171, 0.9704775810241699, 0.9720746874809265, 0.9624439477920532, 0.9752721190452576, 0.9734090566635132, 0.9782814979553223, 0.9733108282089233, 0.980507493019104, 0.9697761535644531, 0.9592346549034119, 0.9683672189712524, 0.9680355787277222, 0.9677149653434753]
[2025-05-13 09:20:51,616]: Mean: 0.97174621
[2025-05-13 09:20:51,616]: Min: 0.95784682
[2025-05-13 09:20:51,616]: Max: 0.99666286
[2025-05-13 09:20:51,616]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 09:20:51,617]: Sample Values (25 elements): [0.014736149460077286, 0.0059992787428200245, 0.023477597162127495, 0.011706937104463577, 0.013398485258221626, -0.018069220706820488, 0.004301195964217186, -0.0075872791931033134, 0.007218901999294758, 0.0042176684364676476, -0.0028802722226828337, -0.02022535167634487, 0.025234799832105637, 0.016224408522248268, -0.02485177479684353, -0.02350672520697117, -0.009132144972682, 0.02547505870461464, 0.02124379202723503, -0.02088831551373005, 0.0241064615547657, -0.038308847695589066, 0.021511642262339592, -0.012394022196531296, -0.008431942202150822]
[2025-05-13 09:20:51,618]: Mean: -0.00003560
[2025-05-13 09:20:51,618]: Min: -0.05334235
[2025-05-13 09:20:51,618]: Max: 0.06011549
[2025-05-13 09:20:51,618]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-13 09:20:51,618]: Sample Values (25 elements): [0.9937825798988342, 0.9878985285758972, 0.9837844371795654, 0.9861054420471191, 0.985886812210083, 0.9850729703903198, 0.9849857091903687, 0.9883020520210266, 0.9862670302391052, 0.9915273189544678, 0.9865074157714844, 0.9841277599334717, 0.991069495677948, 0.9845531582832336, 0.982449471950531, 0.9827316403388977, 0.9881112575531006, 0.9928861260414124, 0.9837244153022766, 0.9814450144767761, 0.9837676286697388, 0.9824819564819336, 0.9934041500091553, 0.989730954170227, 0.986128032207489]
[2025-05-13 09:20:51,618]: Mean: 0.98672700
[2025-05-13 09:20:51,619]: Min: 0.97286171
[2025-05-13 09:20:51,619]: Max: 1.01067472
[2025-05-13 09:20:51,619]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-13 09:20:51,621]: Sample Values (25 elements): [-0.0019058389589190483, 0.0027751235757023096, 0.006395097356289625, -0.009237096644937992, 0.005440096370875835, 0.006389718037098646, -0.028197025880217552, -0.03291458636522293, -0.0016520321369171143, -0.002116956515237689, -0.008997184224426746, -0.011534694582223892, -0.008955798111855984, -0.027104593813419342, -0.010532942600548267, -0.007815755903720856, -0.001817655167542398, -0.0026037278585135937, 0.017001183703541756, 0.01629369705915451, 0.0264056995511055, 0.026842592284083366, 0.020117416977882385, -0.006964272819459438, 0.01831521838903427]
[2025-05-13 09:20:51,622]: Mean: -0.00010403
[2025-05-13 09:20:51,622]: Min: -0.04831988
[2025-05-13 09:20:51,622]: Max: 0.05555911
[2025-05-13 09:20:51,622]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-13 09:20:51,622]: Sample Values (25 elements): [0.9718073606491089, 0.9688641428947449, 0.9689393043518066, 0.9716456532478333, 0.969389796257019, 0.9739904403686523, 0.9726741909980774, 0.9712410569190979, 0.966570258140564, 0.9678738117218018, 0.9718450903892517, 0.9754629731178284, 0.9732644557952881, 0.9688328504562378, 0.9716871380805969, 0.9766374230384827, 0.9640294909477234, 0.9668378233909607, 0.9749743938446045, 0.9744423031806946, 0.9683669209480286, 0.9692471027374268, 0.9781490564346313, 0.96382737159729, 0.9715842008590698]
[2025-05-13 09:20:51,622]: Mean: 0.97179735
[2025-05-13 09:20:51,623]: Min: 0.96011323
[2025-05-13 09:20:51,623]: Max: 0.98296374
[2025-05-13 09:20:51,623]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 09:20:51,629]: Sample Values (25 elements): [0.00936138816177845, -0.002287177601829171, -0.006815014872699976, -0.005312884226441383, -0.021092399954795837, -0.016382206231355667, -0.003821382764726877, -0.019101494923233986, -0.0025654223281890154, 0.011255782097578049, -0.005840015597641468, -0.0051638358272612095, 0.014854278415441513, 0.003854605136439204, -0.008302564732730389, 0.012166881002485752, -0.0072587947361171246, 0.017641214653849602, 0.013831504620611668, 0.003405977040529251, 0.0024405394215136766, -0.015194054692983627, 0.0026890721637755632, -0.013675321824848652, -0.016900354996323586]
[2025-05-13 09:20:51,629]: Mean: -0.00038182
[2025-05-13 09:20:51,629]: Min: -0.03960875
[2025-05-13 09:20:51,629]: Max: 0.04245608
[2025-05-13 09:20:51,629]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-13 09:20:51,630]: Sample Values (25 elements): [0.9761485457420349, 0.974694013595581, 0.9787627458572388, 0.9786555767059326, 0.9715995788574219, 0.9755434989929199, 0.9712703824043274, 0.9726547598838806, 0.9727498888969421, 0.9765182137489319, 0.9805932641029358, 0.9692906141281128, 0.9779982566833496, 0.9811545610427856, 0.9764237403869629, 0.9718528389930725, 0.9663667678833008, 0.9747216105461121, 0.9748812317848206, 0.9787737131118774, 0.9799445867538452, 0.9821677207946777, 0.9749947190284729, 0.9792495965957642, 0.9680415987968445]
[2025-05-13 09:20:51,630]: Mean: 0.97584814
[2025-05-13 09:20:51,630]: Min: 0.96393561
[2025-05-13 09:20:51,630]: Max: 0.99281669
[2025-05-13 09:20:51,630]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-13 09:20:51,631]: Sample Values (25 elements): [0.028744323179125786, 0.06765276193618774, -0.025856036692857742, 0.03599140793085098, -0.024952322244644165, 0.025603491812944412, 0.026697130873799324, -0.07509993016719818, -0.021346500143408775, -0.07367704808712006, 0.019103562459349632, -0.05606718361377716, 0.07090333104133606, -0.0725579485297203, -0.031198492273688316, 0.0447247214615345, 0.003339845919981599, 0.01480244193226099, -0.058651961386203766, -0.023495005443692207, 0.07569055259227753, -0.009017420001327991, 0.036068886518478394, 0.011840449646115303, 0.003994856961071491]
[2025-05-13 09:20:51,631]: Mean: -0.00012725
[2025-05-13 09:20:51,631]: Min: -0.10656838
[2025-05-13 09:20:51,631]: Max: 0.11044126
[2025-05-13 09:20:51,631]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-13 09:20:51,631]: Sample Values (25 elements): [0.9590615034103394, 0.9589252471923828, 0.9564784169197083, 0.9532158374786377, 0.9598867893218994, 0.9675295352935791, 0.9637330174446106, 0.9588756561279297, 0.9583471417427063, 0.9626911878585815, 0.9604867100715637, 0.9610387682914734, 0.9589057564735413, 0.9622902274131775, 0.9568362236022949, 0.9557056427001953, 0.9554638862609863, 0.9580245018005371, 0.9567136764526367, 0.9575576782226562, 0.962160587310791, 0.9572792053222656, 0.9628560543060303, 0.9577065706253052, 0.9598638415336609]
[2025-05-13 09:20:51,631]: Mean: 0.95873970
[2025-05-13 09:20:51,632]: Min: 0.94732249
[2025-05-13 09:20:51,632]: Max: 0.96752954
[2025-05-13 09:20:51,632]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 09:20:51,638]: Sample Values (25 elements): [0.01489107497036457, -0.014905125834047794, -0.004179154988378286, -0.015688715502619743, 0.001966689946129918, 0.002093660645186901, -0.011182140558958054, 0.0017040492966771126, 0.016387730836868286, -0.001273724832572043, -0.014820767566561699, 0.02194283716380596, 0.004305560141801834, 0.00814872421324253, -0.00867022480815649, -0.01194470189511776, -0.012529870495200157, 0.022350765764713287, 0.009065083228051662, 0.012915351428091526, 0.007399719208478928, 0.012045832350850105, 0.011365137994289398, 0.00895340833812952, 0.012344084680080414]
[2025-05-13 09:20:51,639]: Mean: -0.00037882
[2025-05-13 09:20:51,639]: Min: -0.04434874
[2025-05-13 09:20:51,639]: Max: 0.04129763
[2025-05-13 09:20:51,639]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-13 09:20:51,639]: Sample Values (25 elements): [0.9705367684364319, 0.9747213125228882, 0.9743198752403259, 0.9710835814476013, 0.9702147841453552, 0.972332239151001, 0.9732980728149414, 0.9758352637290955, 0.9774321913719177, 0.9716364145278931, 0.9703937768936157, 0.9717345237731934, 0.9692132472991943, 0.9731876254081726, 0.9701425433158875, 0.9727891683578491, 0.9682067036628723, 0.9702362418174744, 0.9728455543518066, 0.9755293130874634, 0.9767800569534302, 0.9701992273330688, 0.981688380241394, 0.972687304019928, 0.9698509573936462]
[2025-05-13 09:20:51,639]: Mean: 0.97176296
[2025-05-13 09:20:51,640]: Min: 0.96339726
[2025-05-13 09:20:51,640]: Max: 0.98583651
[2025-05-13 09:20:51,640]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 09:20:51,646]: Sample Values (25 elements): [0.014701851643621922, -0.020393453538417816, -0.005852118134498596, -0.027715889737010002, 0.019053203985095024, -0.01464359275996685, -0.008565417490899563, 0.0038731317035853863, -0.011463545262813568, 0.022739417850971222, 0.004307764582335949, -0.017580803483724594, 0.010401610285043716, 0.020176710560917854, -0.008503970690071583, 0.003130582859739661, 0.016423355787992477, 0.005772796459496021, -0.009558197110891342, -0.007978811860084534, -0.015962347388267517, -0.013757052831351757, 0.0006663057720288634, 0.004838514141738415, -0.014091309159994125]
[2025-05-13 09:20:51,646]: Mean: -0.00010370
[2025-05-13 09:20:51,646]: Min: -0.03569812
[2025-05-13 09:20:51,646]: Max: 0.04161847
[2025-05-13 09:20:51,646]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-13 09:20:51,647]: Sample Values (25 elements): [0.9827204942703247, 0.977643609046936, 0.989324152469635, 0.9790971875190735, 0.9858456254005432, 0.981265127658844, 0.976637601852417, 0.9837095737457275, 0.9779765009880066, 0.9800679683685303, 0.9862371683120728, 0.9889059066772461, 0.9768344163894653, 0.9816669821739197, 0.978657603263855, 0.987299382686615, 0.9766717553138733, 0.9761458039283752, 0.9780387282371521, 0.9804993271827698, 0.9779813885688782, 0.9859310388565063, 0.9786797761917114, 0.9757410883903503, 0.980855405330658]
[2025-05-13 09:20:51,647]: Mean: 0.98062837
[2025-05-13 09:20:51,647]: Min: 0.97193789
[2025-05-13 09:20:51,647]: Max: 1.00008249
[2025-05-13 09:20:51,647]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-13 09:20:51,659]: Sample Values (25 elements): [-0.012923899106681347, 0.01781371422111988, -0.02188185229897499, 0.01611662283539772, -0.018331235274672508, 0.009696167893707752, -0.01876360923051834, 0.005823502317070961, -0.01039835624396801, 0.014491192996501923, -0.003704656846821308, 0.020012876018881798, 0.014062177389860153, -0.009109165519475937, -0.0005530663765966892, -0.012510132975876331, 0.016968894749879837, 0.01835600472986698, -0.012404372915625572, 0.017109112814068794, 0.008119136095046997, 0.005222889129072428, -0.014544147998094559, 0.013463313691318035, 0.007747291587293148]
[2025-05-13 09:20:51,659]: Mean: -0.00003683
[2025-05-13 09:20:51,660]: Min: -0.03662477
[2025-05-13 09:20:51,660]: Max: 0.03894084
[2025-05-13 09:20:51,660]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-13 09:20:51,660]: Sample Values (25 elements): [0.9734231233596802, 0.9725382924079895, 0.9746860265731812, 0.9738708734512329, 0.9725602865219116, 0.9751048684120178, 0.973164439201355, 0.9728578925132751, 0.9701195359230042, 0.9744921326637268, 0.9725763201713562, 0.9707870483398438, 0.9700666069984436, 0.9712187647819519, 0.9731882810592651, 0.9730291962623596, 0.9706624746322632, 0.9730423092842102, 0.9713964462280273, 0.971693217754364, 0.9699888825416565, 0.971571683883667, 0.9713507294654846, 0.972481906414032, 0.9734378457069397]
[2025-05-13 09:20:51,660]: Mean: 0.97184789
[2025-05-13 09:20:51,661]: Min: 0.96734440
[2025-05-13 09:20:51,661]: Max: 0.97991294
[2025-05-13 09:20:51,661]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 09:20:51,703]: Sample Values (25 elements): [0.00615587318316102, -0.0037277264054864645, -0.005125546362251043, -0.006107136141508818, 0.015359153039753437, 0.003013820853084326, 0.009015241637825966, -0.008429259061813354, 0.013477103784680367, 0.0192607082426548, 0.0020034003537148237, 0.011206347495317459, -0.014689027331769466, 0.010090399533510208, -0.012327580712735653, -0.013244938105344772, 0.009047974832355976, -0.004922674503177404, -0.011600006371736526, -0.005304555408656597, 0.011504481546580791, 0.006138709839433432, -0.009439883753657341, -0.012590785510838032, -0.011352594941854477]
[2025-05-13 09:20:51,703]: Mean: -0.00003168
[2025-05-13 09:20:51,703]: Min: -0.02537077
[2025-05-13 09:20:51,704]: Max: 0.02634848
[2025-05-13 09:20:51,704]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-13 09:20:51,704]: Sample Values (25 elements): [0.9753434658050537, 0.9746869802474976, 0.9737459421157837, 0.9772385954856873, 0.9783867001533508, 0.9757535457611084, 0.9767944812774658, 0.973389208316803, 0.9757523536682129, 0.9808132648468018, 0.9769831895828247, 0.9768774509429932, 0.9759318828582764, 0.9756842851638794, 0.9730681777000427, 0.975019097328186, 0.9759254455566406, 0.9773901700973511, 0.974727213382721, 0.9744406342506409, 0.9745063781738281, 0.9727349281311035, 0.9778223037719727, 0.9729525446891785, 0.9749106764793396]
[2025-05-13 09:20:51,704]: Mean: 0.97599125
[2025-05-13 09:20:51,704]: Min: 0.97142982
[2025-05-13 09:20:51,704]: Max: 0.98412073
[2025-05-13 09:20:51,704]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-13 09:20:51,706]: Sample Values (25 elements): [0.03461276367306709, 0.014101768843829632, 0.009846430271863937, 0.019921263679862022, 0.014768699184060097, -0.03205019608139992, 0.0244416743516922, 0.043660640716552734, -0.04422145336866379, 0.06059814244508743, -0.015927869826555252, -0.007384837605059147, 0.02166009321808815, 0.05383679270744324, -0.05049316957592964, -0.00971987470984459, 0.058840181678533554, 0.03963885083794594, 0.01337289810180664, -0.053309570997953415, 0.05429420620203018, -0.0038816211745142937, -0.015190888196229935, -0.005641214549541473, -0.021168528124690056]
[2025-05-13 09:20:51,706]: Mean: -0.00000964
[2025-05-13 09:20:51,706]: Min: -0.06908744
[2025-05-13 09:20:51,706]: Max: 0.07181887
[2025-05-13 09:20:51,706]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-13 09:20:51,707]: Sample Values (25 elements): [0.9705201983451843, 0.9708281755447388, 0.9701576232910156, 0.9692566990852356, 0.9692243933677673, 0.9711708426475525, 0.9687188863754272, 0.971028745174408, 0.9710775017738342, 0.9686347842216492, 0.9697036743164062, 0.9701295495033264, 0.9691653847694397, 0.970721423625946, 0.9675201177597046, 0.9693637490272522, 0.9700128436088562, 0.9688229560852051, 0.9719407558441162, 0.9702969789505005, 0.9686039686203003, 0.968219518661499, 0.9708176851272583, 0.9698987007141113, 0.9687210917472839]
[2025-05-13 09:20:51,707]: Mean: 0.97007841
[2025-05-13 09:20:51,707]: Min: 0.96692526
[2025-05-13 09:20:51,707]: Max: 0.97441262
[2025-05-13 09:20:51,707]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 09:20:51,741]: Sample Values (25 elements): [0.0005528372712433338, 0.011221719905734062, 0.011552507989108562, 0.008138792589306831, -0.010305012576282024, 0.008297075517475605, 0.0138823501765728, -0.007609252817928791, 0.006620797328650951, -0.0024801299441605806, 0.00704363314434886, -0.010159043595194817, 0.00306334113702178, -0.012068688869476318, 0.005834654904901981, 0.0054386574774980545, 0.00026675270055420697, 0.009889911860227585, 0.001937743742018938, 0.008771611377596855, 0.008761744946241379, -0.0044668312184512615, 0.0025873405393213034, -0.00028579082572832704, 0.008727611973881721]
[2025-05-13 09:20:51,741]: Mean: -0.00012784
[2025-05-13 09:20:51,741]: Min: -0.02267683
[2025-05-13 09:20:51,742]: Max: 0.02453168
[2025-05-13 09:20:51,742]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-13 09:20:51,743]: Sample Values (25 elements): [0.9729820489883423, 0.9718064069747925, 0.9702325463294983, 0.9731013178825378, 0.9697316288948059, 0.9733532071113586, 0.9700479507446289, 0.9732756614685059, 0.9702061414718628, 0.9753329753875732, 0.9720442891120911, 0.979219377040863, 0.9697611927986145, 0.9702436923980713, 0.9702650308609009, 0.9724612236022949, 0.9713727831840515, 0.9696269035339355, 0.9760976433753967, 0.970038890838623, 0.970252275466919, 0.9719366431236267, 0.9695955514907837, 0.9742545485496521, 0.9708123207092285]
[2025-05-13 09:20:51,743]: Mean: 0.97186023
[2025-05-13 09:20:51,744]: Min: 0.96785510
[2025-05-13 09:20:51,744]: Max: 0.98566675
[2025-05-13 09:20:51,744]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 09:20:51,783]: Sample Values (25 elements): [0.010565226897597313, -0.006964301690459251, -0.006063708104193211, -0.005934200249612331, 0.00489776162430644, -0.006771164946258068, 0.007769755553454161, 0.001976249273866415, -0.01133826095610857, -0.007793170865625143, -0.012513763271272182, 0.009421508759260178, -0.0012968748342245817, 0.004904784262180328, -0.007426307536661625, -0.003029291518032551, -0.013093781657516956, 0.011259729973971844, -0.006698316894471645, -0.009242145344614983, 0.006437397096306086, 0.0013577654026448727, 0.006566061172634363, -0.008632210083305836, -0.00959436222910881]
[2025-05-13 09:20:51,783]: Mean: 0.00001844
[2025-05-13 09:20:51,784]: Min: -0.02077021
[2025-05-13 09:20:51,784]: Max: 0.02154634
[2025-05-13 09:20:51,784]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-13 09:20:51,784]: Sample Values (25 elements): [0.9748052358627319, 0.9767721891403198, 0.975907564163208, 0.9839965105056763, 0.981687068939209, 0.9811554551124573, 0.9802518486976624, 0.9790462851524353, 0.9789599180221558, 0.9767505526542664, 0.978746771812439, 0.9819343686103821, 0.9809314608573914, 0.9805706739425659, 0.9816794395446777, 0.9770567417144775, 0.9836751818656921, 0.9795737862586975, 0.9853636622428894, 0.9812780022621155, 0.9784419536590576, 0.9817274212837219, 0.9781706929206848, 0.9783535003662109, 0.9787684679031372]
[2025-05-13 09:20:51,784]: Mean: 0.97994637
[2025-05-13 09:20:51,785]: Min: 0.97410023
[2025-05-13 09:20:51,785]: Max: 0.98879987
[2025-05-13 09:20:51,785]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-13 09:20:51,785]: Sample Values (25 elements): [0.05596397444605827, 0.00017244073387701064, -0.021586555987596512, -0.011417956091463566, -0.022709595039486885, -0.057118844240903854, -0.06931693851947784, 0.0034611138980835676, -0.04579681530594826, -0.03907228633761406, -0.001564454403705895, 0.03173990175127983, 0.10774719715118408, 0.07108957320451736, -0.0029800436459481716, 0.04543027654290199, 0.05885360389947891, 0.08433639258146286, -0.04508877545595169, -0.07158099114894867, 0.060060616582632065, 0.007712217979133129, 0.1003764197230339, -0.06554189324378967, 0.024199465289711952]
[2025-05-13 09:20:51,785]: Mean: 0.00047564
[2025-05-13 09:20:51,785]: Min: -0.11341284
[2025-05-13 09:20:51,786]: Max: 0.14244121
[2025-05-13 09:20:51,786]: 


QAT of ResNet18 with relu down to 4 bits...
[2025-05-13 09:20:52,075]: [ResNet18_relu_quantized_4_bits] after configure_qat:
[2025-05-13 09:20:52,236]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 09:22:34,193]: [ResNet18_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.1588 Train Acc: 0.9438 Eval Loss: 0.5837 Eval Acc: 0.8607 (LR: 0.001000)
[2025-05-13 09:24:16,090]: [ResNet18_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.1492 Train Acc: 0.9462 Eval Loss: 0.5062 Eval Acc: 0.8718 (LR: 0.001000)
[2025-05-13 09:25:58,285]: [ResNet18_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.1410 Train Acc: 0.9502 Eval Loss: 0.4498 Eval Acc: 0.8848 (LR: 0.001000)
[2025-05-13 09:27:40,353]: [ResNet18_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.1341 Train Acc: 0.9516 Eval Loss: 0.4645 Eval Acc: 0.8829 (LR: 0.001000)
[2025-05-13 09:29:22,429]: [ResNet18_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.1259 Train Acc: 0.9548 Eval Loss: 0.4628 Eval Acc: 0.8832 (LR: 0.001000)
[2025-05-13 09:31:04,280]: [ResNet18_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.1217 Train Acc: 0.9571 Eval Loss: 0.6356 Eval Acc: 0.8472 (LR: 0.001000)
[2025-05-13 09:32:46,148]: [ResNet18_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.1251 Train Acc: 0.9553 Eval Loss: 0.4297 Eval Acc: 0.8868 (LR: 0.001000)
[2025-05-13 09:34:28,205]: [ResNet18_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.1151 Train Acc: 0.9592 Eval Loss: 0.4562 Eval Acc: 0.8852 (LR: 0.001000)
[2025-05-13 09:36:09,861]: [ResNet18_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.1145 Train Acc: 0.9594 Eval Loss: 0.4594 Eval Acc: 0.8841 (LR: 0.001000)
[2025-05-13 09:37:55,847]: [ResNet18_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.1110 Train Acc: 0.9593 Eval Loss: 0.4547 Eval Acc: 0.8823 (LR: 0.001000)
[2025-05-13 09:39:38,160]: [ResNet18_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.1059 Train Acc: 0.9621 Eval Loss: 0.4384 Eval Acc: 0.8878 (LR: 0.001000)
[2025-05-13 09:41:21,383]: [ResNet18_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.1022 Train Acc: 0.9638 Eval Loss: 0.4532 Eval Acc: 0.8857 (LR: 0.001000)
[2025-05-13 09:43:03,938]: [ResNet18_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.1043 Train Acc: 0.9615 Eval Loss: 0.4405 Eval Acc: 0.8846 (LR: 0.001000)
[2025-05-13 09:44:45,719]: [ResNet18_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.1018 Train Acc: 0.9641 Eval Loss: 0.4299 Eval Acc: 0.8916 (LR: 0.001000)
[2025-05-13 09:46:27,573]: [ResNet18_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.1008 Train Acc: 0.9644 Eval Loss: 0.4423 Eval Acc: 0.8927 (LR: 0.001000)
[2025-05-13 09:48:09,647]: [ResNet18_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.0981 Train Acc: 0.9646 Eval Loss: 0.4333 Eval Acc: 0.8907 (LR: 0.001000)
[2025-05-13 09:49:51,453]: [ResNet18_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.0974 Train Acc: 0.9653 Eval Loss: 0.4513 Eval Acc: 0.8891 (LR: 0.001000)
[2025-05-13 09:51:33,201]: [ResNet18_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.0883 Train Acc: 0.9681 Eval Loss: 0.4451 Eval Acc: 0.8910 (LR: 0.001000)
[2025-05-13 09:53:14,986]: [ResNet18_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.0921 Train Acc: 0.9672 Eval Loss: 0.4666 Eval Acc: 0.8843 (LR: 0.001000)
[2025-05-13 09:54:57,541]: [ResNet18_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.0906 Train Acc: 0.9676 Eval Loss: 0.4543 Eval Acc: 0.8874 (LR: 0.001000)
[2025-05-13 09:56:39,379]: [ResNet18_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.0871 Train Acc: 0.9694 Eval Loss: 0.4278 Eval Acc: 0.8945 (LR: 0.001000)
[2025-05-13 09:58:18,664]: [ResNet18_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.0850 Train Acc: 0.9697 Eval Loss: 0.4394 Eval Acc: 0.8920 (LR: 0.001000)
[2025-05-13 09:59:55,444]: [ResNet18_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.0848 Train Acc: 0.9700 Eval Loss: 0.4556 Eval Acc: 0.8889 (LR: 0.001000)
[2025-05-13 10:01:32,263]: [ResNet18_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.0836 Train Acc: 0.9695 Eval Loss: 0.4303 Eval Acc: 0.8949 (LR: 0.001000)
[2025-05-13 10:03:09,284]: [ResNet18_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.0831 Train Acc: 0.9706 Eval Loss: 0.4310 Eval Acc: 0.8977 (LR: 0.001000)
[2025-05-13 10:04:46,090]: [ResNet18_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.0780 Train Acc: 0.9727 Eval Loss: 0.4431 Eval Acc: 0.8955 (LR: 0.001000)
[2025-05-13 10:06:23,403]: [ResNet18_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.0794 Train Acc: 0.9714 Eval Loss: 0.4585 Eval Acc: 0.8923 (LR: 0.001000)
[2025-05-13 10:08:06,786]: [ResNet18_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.0781 Train Acc: 0.9720 Eval Loss: 0.4315 Eval Acc: 0.8984 (LR: 0.001000)
[2025-05-13 10:09:48,777]: [ResNet18_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.0798 Train Acc: 0.9716 Eval Loss: 0.4581 Eval Acc: 0.8896 (LR: 0.001000)
[2025-05-13 10:11:30,571]: [ResNet18_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.0782 Train Acc: 0.9725 Eval Loss: 0.4298 Eval Acc: 0.8992 (LR: 0.000250)
[2025-05-13 10:13:12,325]: [ResNet18_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.0509 Train Acc: 0.9823 Eval Loss: 0.3826 Eval Acc: 0.9075 (LR: 0.000250)
[2025-05-13 10:14:54,084]: [ResNet18_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.0399 Train Acc: 0.9872 Eval Loss: 0.3876 Eval Acc: 0.9055 (LR: 0.000250)
[2025-05-13 10:16:35,875]: [ResNet18_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.0392 Train Acc: 0.9874 Eval Loss: 0.3844 Eval Acc: 0.9056 (LR: 0.000250)
[2025-05-13 10:18:17,683]: [ResNet18_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.0376 Train Acc: 0.9881 Eval Loss: 0.3799 Eval Acc: 0.9097 (LR: 0.000250)
[2025-05-13 10:19:59,693]: [ResNet18_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.0346 Train Acc: 0.9889 Eval Loss: 0.3920 Eval Acc: 0.9055 (LR: 0.000250)
[2025-05-13 10:21:41,455]: [ResNet18_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.0345 Train Acc: 0.9891 Eval Loss: 0.3860 Eval Acc: 0.9071 (LR: 0.000250)
[2025-05-13 10:23:21,342]: [ResNet18_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.0356 Train Acc: 0.9887 Eval Loss: 0.3957 Eval Acc: 0.9055 (LR: 0.000250)
[2025-05-13 10:24:57,910]: [ResNet18_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.0330 Train Acc: 0.9897 Eval Loss: 0.3932 Eval Acc: 0.9068 (LR: 0.000250)
[2025-05-13 10:26:34,558]: [ResNet18_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.0320 Train Acc: 0.9896 Eval Loss: 0.3910 Eval Acc: 0.9082 (LR: 0.000250)
[2025-05-13 10:28:12,264]: [ResNet18_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.0320 Train Acc: 0.9898 Eval Loss: 0.3857 Eval Acc: 0.9109 (LR: 0.000250)
[2025-05-13 10:29:49,179]: [ResNet18_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.0319 Train Acc: 0.9898 Eval Loss: 0.3815 Eval Acc: 0.9102 (LR: 0.000250)
[2025-05-13 10:31:25,985]: [ResNet18_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.0334 Train Acc: 0.9889 Eval Loss: 0.3949 Eval Acc: 0.9089 (LR: 0.000250)
[2025-05-13 10:33:02,785]: [ResNet18_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.0320 Train Acc: 0.9901 Eval Loss: 0.3894 Eval Acc: 0.9074 (LR: 0.000250)
[2025-05-13 10:34:39,206]: [ResNet18_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.0309 Train Acc: 0.9904 Eval Loss: 0.4000 Eval Acc: 0.9081 (LR: 0.000250)
[2025-05-13 10:36:15,592]: [ResNet18_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.0300 Train Acc: 0.9903 Eval Loss: 0.4022 Eval Acc: 0.9050 (LR: 0.000063)
[2025-05-13 10:37:52,134]: [ResNet18_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.0283 Train Acc: 0.9915 Eval Loss: 0.3918 Eval Acc: 0.9098 (LR: 0.000063)
[2025-05-13 10:39:28,795]: [ResNet18_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.0260 Train Acc: 0.9918 Eval Loss: 0.4015 Eval Acc: 0.9085 (LR: 0.000063)
[2025-05-13 10:41:05,435]: [ResNet18_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.0266 Train Acc: 0.9915 Eval Loss: 0.3916 Eval Acc: 0.9101 (LR: 0.000063)
[2025-05-13 10:42:42,048]: [ResNet18_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.0256 Train Acc: 0.9918 Eval Loss: 0.3896 Eval Acc: 0.9104 (LR: 0.000063)
[2025-05-13 10:44:18,641]: [ResNet18_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.0271 Train Acc: 0.9914 Eval Loss: 0.3893 Eval Acc: 0.9104 (LR: 0.000063)
[2025-05-13 10:45:55,266]: [ResNet18_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.0250 Train Acc: 0.9922 Eval Loss: 0.3864 Eval Acc: 0.9125 (LR: 0.000063)
[2025-05-13 10:47:32,297]: [ResNet18_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.0243 Train Acc: 0.9928 Eval Loss: 0.3888 Eval Acc: 0.9110 (LR: 0.000063)
[2025-05-13 10:49:09,050]: [ResNet18_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.0235 Train Acc: 0.9933 Eval Loss: 0.3906 Eval Acc: 0.9117 (LR: 0.000063)
[2025-05-13 10:50:45,704]: [ResNet18_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.0243 Train Acc: 0.9924 Eval Loss: 0.3939 Eval Acc: 0.9096 (LR: 0.000063)
[2025-05-13 10:52:22,287]: [ResNet18_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.0262 Train Acc: 0.9914 Eval Loss: 0.3917 Eval Acc: 0.9119 (LR: 0.000063)
[2025-05-13 10:53:58,925]: [ResNet18_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.0255 Train Acc: 0.9917 Eval Loss: 0.3919 Eval Acc: 0.9132 (LR: 0.000063)
[2025-05-13 10:55:35,708]: [ResNet18_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.0249 Train Acc: 0.9922 Eval Loss: 0.3859 Eval Acc: 0.9140 (LR: 0.000063)
[2025-05-13 10:57:12,554]: [ResNet18_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.0253 Train Acc: 0.9921 Eval Loss: 0.3832 Eval Acc: 0.9117 (LR: 0.000063)
[2025-05-13 10:58:49,127]: [ResNet18_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.0254 Train Acc: 0.9917 Eval Loss: 0.3890 Eval Acc: 0.9131 (LR: 0.000063)
[2025-05-13 11:00:26,478]: [ResNet18_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.0228 Train Acc: 0.9933 Eval Loss: 0.3953 Eval Acc: 0.9119 (LR: 0.000063)
[2025-05-13 11:00:26,478]: [ResNet18_relu_quantized_4_bits] Best Eval Accuracy: 0.9140
[2025-05-13 11:00:26,558]: 


Quantization of model down to 4 bits finished
[2025-05-13 11:00:26,558]: Model Architecture:
[2025-05-13 11:00:26,618]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9781], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.671247482299805)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0164], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11564702540636063, max_val=0.13082019984722137)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8255], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.382011413574219)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11211646348237991, max_val=0.11192215979099274)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0159], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.238517761230469)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11625257879495621, max_val=0.10659836232662201)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5314], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.970731258392334)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0116], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08474066853523254, max_val=0.08936010301113129)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9876], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.813569068908691)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07549463212490082, max_val=0.08468685299158096)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4720], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.079284191131592)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0091], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06419247388839722, max_val=0.07249121367931366)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0201], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15079189836978912, max_val=0.15062984824180603)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6594], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.890886306762695)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06067672371864319, max_val=0.07039716094732285)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4134], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.201341152191162)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05861152336001396, max_val=0.06272491067647934)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8362], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.543364524841309)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05485531687736511, max_val=0.061203982681035995)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4228], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.341679573059082)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04435452073812485, max_val=0.04811569303274155)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1105862557888031, max_val=0.11318127065896988)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6177], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.264752388000488)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0064], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04776656627655029, max_val=0.0480620339512825)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.061489105224609)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0055], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04034982994198799, max_val=0.042537566274404526)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8107], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.160335540771484)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03848808631300926, max_val=0.04124923050403595)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4537], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.8047776222229)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0036], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02706991136074066, max_val=0.027077745646238327)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0096], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0703732967376709, max_val=0.07299330830574036)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6829], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.244135856628418)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02474992908537388, max_val=0.02565452829003334)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4377], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.565921783447266)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.021446486935019493, max_val=0.02190200425684452)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9081], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.62183666229248)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 11:00:26,618]: 
Model Weights:
[2025-05-13 11:00:26,618]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-13 11:00:26,619]: Sample Values (25 elements): [0.10389173030853271, -0.14996227622032166, 0.277554452419281, 0.03039342164993286, -0.06215347722172737, 0.10095175355672836, -0.11861418187618256, -0.12503312528133392, 0.12939219176769257, 0.08241799473762512, -0.02493596076965332, 0.10076656937599182, 0.14324750006198883, -0.013457557186484337, 0.07973183691501617, -0.09926411509513855, -0.06327074766159058, 0.1489304006099701, 0.15937845408916473, 0.1304774135351181, 0.1814219206571579, -0.14495311677455902, -0.017810558900237083, 0.16713866591453552, -0.01992892473936081]
[2025-05-13 11:00:26,619]: Mean: -0.00030674
[2025-05-13 11:00:26,619]: Min: -0.29922149
[2025-05-13 11:00:26,619]: Max: 0.37257522
[2025-05-13 11:00:26,619]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-13 11:00:26,619]: Sample Values (25 elements): [1.0285027027130127, 1.0418797731399536, 0.9446156620979309, 0.9927226305007935, 0.9915624260902405, 0.9541420340538025, 1.003943681716919, 0.8898230195045471, 0.9690938591957092, 0.9899912476539612, 1.0283088684082031, 0.9764777421951294, 1.0435466766357422, 0.9035924077033997, 0.9185814261436462, 0.9977045059204102, 0.9690531492233276, 0.9560545086860657, 0.9294726848602295, 1.0026274919509888, 0.9762987494468689, 0.9843015670776367, 0.992123544216156, 1.1079233884811401, 1.0830450057983398]
[2025-05-13 11:00:26,619]: Mean: 0.97530711
[2025-05-13 11:00:26,620]: Min: 0.86151445
[2025-05-13 11:00:26,620]: Max: 1.10792339
[2025-05-13 11:00:26,621]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 11:00:26,621]: Sample Values (25 elements): [0.032862305641174316, -0.016431152820587158, 0.032862305641174316, 0.0, 0.032862305641174316, -0.016431152820587158, -0.016431152820587158, -0.016431152820587158, 0.016431152820587158, 0.032862305641174316, -0.049293458461761475, 0.0, 0.016431152820587158, 0.049293458461761475, -0.016431152820587158, -0.032862305641174316, 0.06572461128234863, -0.032862305641174316, 0.016431152820587158, -0.049293458461761475, 0.032862305641174316, 0.016431152820587158, 0.049293458461761475, 0.016431152820587158, -0.032862305641174316]
[2025-05-13 11:00:26,622]: Mean: -0.00153908
[2025-05-13 11:00:26,622]: Min: -0.11501807
[2025-05-13 11:00:26,622]: Max: 0.13144922
[2025-05-13 11:00:26,622]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 11:00:26,622]: Sample Values (25 elements): [0.979142427444458, 0.9876201152801514, 0.9737179279327393, 0.9628106355667114, 0.9723132848739624, 0.9679481387138367, 0.9933967590332031, 0.9463381767272949, 0.9671577215194702, 0.9793128371238708, 0.9180752038955688, 0.9780833125114441, 0.9743797779083252, 0.9430527091026306, 0.9633011221885681, 0.9819728136062622, 1.0253242254257202, 0.9836165904998779, 1.0850170850753784, 1.0063344240188599, 0.9309977889060974, 0.9905907511711121, 0.9463723301887512, 0.9695505499839783, 0.9879133701324463]
[2025-05-13 11:00:26,622]: Mean: 0.97600502
[2025-05-13 11:00:26,622]: Min: 0.91807520
[2025-05-13 11:00:26,623]: Max: 1.11868370
[2025-05-13 11:00:26,624]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 11:00:26,624]: Sample Values (25 elements): [0.0, 0.0, -0.014935925602912903, -0.029871851205825806, 0.014935925602912903, -0.029871851205825806, 0.014935925602912903, 0.0, 0.029871851205825806, 0.014935925602912903, -0.05974370241165161, 0.014935925602912903, -0.04480777680873871, 0.029871851205825806, 0.0, 0.0, -0.029871851205825806, 0.029871851205825806, -0.029871851205825806, -0.04480777680873871, 0.04480777680873871, -0.029871851205825806, 0.04480777680873871, -0.014935925602912903, 0.0]
[2025-05-13 11:00:26,624]: Mean: -0.00167211
[2025-05-13 11:00:26,624]: Min: -0.11948740
[2025-05-13 11:00:26,625]: Max: 0.10455148
[2025-05-13 11:00:26,625]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 11:00:26,625]: Sample Values (25 elements): [0.958091676235199, 0.9981042146682739, 0.9691580533981323, 0.9644915461540222, 1.0011659860610962, 0.9601632952690125, 0.9938367605209351, 0.9873119592666626, 0.9599449634552002, 0.9590152502059937, 0.9833242893218994, 0.9836001396179199, 0.9328048229217529, 0.9832186102867126, 0.9744481444358826, 0.9711611270904541, 0.9769207835197449, 0.965969979763031, 0.9666578769683838, 0.9489190578460693, 1.0171253681182861, 0.9538012742996216, 0.9786789417266846, 0.9513044953346252, 1.0077898502349854]
[2025-05-13 11:00:26,625]: Mean: 0.97884238
[2025-05-13 11:00:26,625]: Min: 0.93280482
[2025-05-13 11:00:26,625]: Max: 1.05787361
[2025-05-13 11:00:26,626]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 11:00:26,627]: Sample Values (25 elements): [0.0, 0.05942685902118683, 0.029713429510593414, -0.014856714755296707, 0.029713429510593414, 0.05942685902118683, 0.029713429510593414, -0.014856714755296707, 0.0, 0.0, -0.029713429510593414, 0.04457014426589012, 0.014856714755296707, 0.029713429510593414, 0.04457014426589012, 0.0, 0.04457014426589012, 0.0, -0.014856714755296707, 0.04457014426589012, 0.0, 0.014856714755296707, 0.014856714755296707, 0.0, -0.029713429510593414]
[2025-05-13 11:00:26,627]: Mean: -0.00077016
[2025-05-13 11:00:26,627]: Min: -0.11885372
[2025-05-13 11:00:26,627]: Max: 0.10399701
[2025-05-13 11:00:26,627]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 11:00:26,628]: Sample Values (25 elements): [0.948956310749054, 0.9535062909126282, 0.9591995477676392, 0.95017409324646, 0.9608342051506042, 0.963752031326294, 0.9540048837661743, 1.0005009174346924, 0.9645965099334717, 0.9393059015274048, 0.9798372983932495, 0.9547264575958252, 0.9565154314041138, 0.9521554112434387, 0.9851011037826538, 0.9665238857269287, 0.9740941524505615, 0.9527832269668579, 0.9584680795669556, 0.9664551615715027, 0.9651311635971069, 0.9376809597015381, 0.9572190046310425, 0.9535158276557922, 0.9673537611961365]
[2025-05-13 11:00:26,628]: Mean: 0.96267152
[2025-05-13 11:00:26,628]: Min: 0.93696189
[2025-05-13 11:00:26,628]: Max: 1.00641346
[2025-05-13 11:00:26,629]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 11:00:26,630]: Sample Values (25 elements): [0.0, 0.011606722138822079, 0.023213444277644157, 0.011606722138822079, 0.011606722138822079, -0.03482016548514366, -0.046426888555288315, -0.011606722138822079, -0.023213444277644157, 0.03482016548514366, -0.011606722138822079, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011606722138822079, 0.03482016548514366, 0.023213444277644157, 0.0, 0.011606722138822079, 0.0, -0.023213444277644157, -0.023213444277644157, -0.046426888555288315]
[2025-05-13 11:00:26,630]: Mean: -0.00025975
[2025-05-13 11:00:26,630]: Min: -0.08124705
[2025-05-13 11:00:26,630]: Max: 0.09285378
[2025-05-13 11:00:26,630]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 11:00:26,630]: Sample Values (25 elements): [0.9705802798271179, 0.9881001114845276, 0.9851289391517639, 0.9917089939117432, 0.9717727303504944, 0.9913045167922974, 1.0040849447250366, 0.9953871369361877, 0.9856874346733093, 0.9735707640647888, 0.9757364392280579, 0.9679473638534546, 0.9915075898170471, 0.9727727770805359, 0.9881914854049683, 1.0112487077713013, 1.012760877609253, 0.9878903031349182, 1.0312741994857788, 0.9762061238288879, 0.9717153310775757, 0.9639521241188049, 0.9694528579711914, 0.9803712368011475, 1.0013954639434814]
[2025-05-13 11:00:26,631]: Mean: 0.98682517
[2025-05-13 11:00:26,631]: Min: 0.95388848
[2025-05-13 11:00:26,631]: Max: 1.03843462
[2025-05-13 11:00:26,632]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-13 11:00:26,633]: Sample Values (25 elements): [0.021357528865337372, -0.010678764432668686, -0.010678764432668686, 0.010678764432668686, -0.042715057730674744, 0.021357528865337372, -0.021357528865337372, -0.03203629329800606, 0.03203629329800606, -0.021357528865337372, -0.06407258659601212, 0.03203629329800606, 0.0, -0.010678764432668686, 0.042715057730674744, -0.03203629329800606, -0.010678764432668686, 0.021357528865337372, 0.010678764432668686, -0.010678764432668686, -0.021357528865337372, 0.010678764432668686, 0.021357528865337372, 0.0, -0.05339382216334343]
[2025-05-13 11:00:26,633]: Mean: -0.00036311
[2025-05-13 11:00:26,633]: Min: -0.07475135
[2025-05-13 11:00:26,633]: Max: 0.08543012
[2025-05-13 11:00:26,633]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-13 11:00:26,633]: Sample Values (25 elements): [0.971554696559906, 0.9511501789093018, 0.9598739147186279, 0.9590086340904236, 0.9672871232032776, 0.9606412649154663, 0.9492946267127991, 0.9481636881828308, 0.9698192477226257, 0.9602321982383728, 0.9798728227615356, 0.9618573784828186, 0.9607270956039429, 0.9513300657272339, 0.9577444791793823, 0.959053099155426, 0.9615316987037659, 0.9623746275901794, 0.9609489440917969, 0.9725613594055176, 0.9587493538856506, 0.9731398224830627, 0.9559961557388306, 0.9641740322113037, 0.9539600014686584]
[2025-05-13 11:00:26,634]: Mean: 0.96087658
[2025-05-13 11:00:26,634]: Min: 0.94209903
[2025-05-13 11:00:26,634]: Max: 0.97987282
[2025-05-13 11:00:26,635]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 11:00:26,636]: Sample Values (25 elements): [0.009112252853810787, 0.0, -0.009112252853810787, -0.009112252853810787, -0.018224505707621574, 0.009112252853810787, 0.027336757630109787, -0.009112252853810787, 0.0, 0.0, 0.009112252853810787, -0.009112252853810787, 0.03644901141524315, 0.0, 0.027336757630109787, 0.018224505707621574, 0.009112252853810787, 0.0, 0.018224505707621574, -0.018224505707621574, 0.03644901141524315, 0.009112252853810787, -0.009112252853810787, 0.027336757630109787, 0.027336757630109787]
[2025-05-13 11:00:26,636]: Mean: -0.00063885
[2025-05-13 11:00:26,636]: Min: -0.06378577
[2025-05-13 11:00:26,637]: Max: 0.07289802
[2025-05-13 11:00:26,637]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-13 11:00:26,637]: Sample Values (25 elements): [0.9516382217407227, 0.9590916037559509, 0.9663135409355164, 0.9685925841331482, 0.9832277894020081, 0.9685415625572205, 0.9706928730010986, 0.9745951294898987, 0.970737874507904, 0.955843448638916, 0.9642975926399231, 0.9739508032798767, 0.9753686189651489, 0.9588358402252197, 0.9574640393257141, 0.9739866256713867, 0.9528970718383789, 0.9611308574676514, 0.9584906697273254, 0.9590250253677368, 0.9835768342018127, 0.9613322615623474, 0.9689877033233643, 0.9503052234649658, 0.9517899751663208]
[2025-05-13 11:00:26,637]: Mean: 0.96660805
[2025-05-13 11:00:26,637]: Min: 0.94982553
[2025-05-13 11:00:26,637]: Max: 0.99231553
[2025-05-13 11:00:26,638]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-13 11:00:26,639]: Sample Values (25 elements): [-0.02009478025138378, -0.04018956050276756, 0.02009478025138378, 0.10047390311956406, -0.12056867778301239, 0.12056867778301239, 0.08037912100553513, -0.10047390311956406, 0.060284338891506195, 0.10047390311956406, 0.04018956050276756, -0.02009478025138378, -0.04018956050276756, -0.060284338891506195, 0.04018956050276756, 0.02009478025138378, 0.08037912100553513, 0.02009478025138378, 0.02009478025138378, 0.0, -0.02009478025138378, 0.0, 0.060284338891506195, 0.04018956050276756, 0.08037912100553513]
[2025-05-13 11:00:26,639]: Mean: -0.00028700
[2025-05-13 11:00:26,639]: Min: -0.16075824
[2025-05-13 11:00:26,639]: Max: 0.14066346
[2025-05-13 11:00:26,639]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-13 11:00:26,639]: Sample Values (25 elements): [0.9366999268531799, 0.949620246887207, 0.9408430457115173, 0.9295328855514526, 0.9278047680854797, 0.9271677732467651, 0.9318525791168213, 0.9296401143074036, 0.9445627331733704, 0.9360223412513733, 0.9322457313537598, 0.9324255585670471, 0.952233076095581, 0.936024010181427, 0.934444010257721, 0.9344624876976013, 0.9337009191513062, 0.9323992133140564, 0.9369370341300964, 0.9272046685218811, 0.9298804998397827, 0.9263916611671448, 0.937802255153656, 0.9436765313148499, 0.9270951747894287]
[2025-05-13 11:00:26,640]: Mean: 0.93462038
[2025-05-13 11:00:26,640]: Min: 0.90522742
[2025-05-13 11:00:26,640]: Max: 0.95223308
[2025-05-13 11:00:26,641]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 11:00:26,642]: Sample Values (25 elements): [-0.017476504668593407, 0.0, -0.026214756071567535, -0.008738252334296703, -0.017476504668593407, -0.026214756071567535, 0.017476504668593407, -0.017476504668593407, 0.0, -0.017476504668593407, 0.03495300933718681, -0.008738252334296703, -0.008738252334296703, -0.017476504668593407, -0.008738252334296703, -0.026214756071567535, -0.03495300933718681, 0.008738252334296703, -0.017476504668593407, 0.017476504668593407, 0.017476504668593407, 0.03495300933718681, 0.017476504668593407, -0.008738252334296703, -0.026214756071567535]
[2025-05-13 11:00:26,642]: Mean: -0.00063835
[2025-05-13 11:00:26,643]: Min: -0.06116777
[2025-05-13 11:00:26,643]: Max: 0.06990602
[2025-05-13 11:00:26,643]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-13 11:00:26,643]: Sample Values (25 elements): [0.9602236151695251, 0.9675492644309998, 0.957455039024353, 0.9760933518409729, 0.9653687477111816, 0.9589871168136597, 0.9537387490272522, 0.9530819654464722, 0.9525226354598999, 0.9854136109352112, 0.9628356695175171, 0.9583308100700378, 0.9661622643470764, 0.9516414403915405, 0.9602584838867188, 0.954904317855835, 0.9517362713813782, 0.9574083685874939, 0.9749306440353394, 0.974464476108551, 0.946103036403656, 0.9778008460998535, 0.9588398337364197, 0.9675912261009216, 0.9659718871116638]
[2025-05-13 11:00:26,643]: Mean: 0.95998919
[2025-05-13 11:00:26,643]: Min: 0.94155687
[2025-05-13 11:00:26,644]: Max: 0.99060231
[2025-05-13 11:00:26,645]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 11:00:26,646]: Sample Values (25 elements): [0.02426731213927269, 0.008089103735983372, -0.02426731213927269, 0.008089103735983372, 0.008089103735983372, 0.0, 0.02426731213927269, -0.016178207471966743, -0.008089103735983372, -0.008089103735983372, 0.0, 0.02426731213927269, 0.0, 0.03235641494393349, 0.008089103735983372, 0.008089103735983372, 0.0, -0.03235641494393349, 0.008089103735983372, 0.016178207471966743, 0.008089103735983372, -0.008089103735983372, -0.03235641494393349, -0.02426731213927269, 0.02426731213927269]
[2025-05-13 11:00:26,646]: Mean: 0.00001926
[2025-05-13 11:00:26,646]: Min: -0.05662373
[2025-05-13 11:00:26,646]: Max: 0.06471283
[2025-05-13 11:00:26,646]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-13 11:00:26,647]: Sample Values (25 elements): [0.9922596216201782, 0.9847847819328308, 0.9767420291900635, 0.9783665537834167, 0.9905344843864441, 0.9799401164054871, 0.9909567832946777, 0.9842281341552734, 0.991307258605957, 0.9704122543334961, 0.9719670414924622, 0.9778645634651184, 0.9833166599273682, 0.9797201752662659, 0.9800506234169006, 0.9874497652053833, 0.9816484451293945, 0.9827213883399963, 0.9792493581771851, 0.9779785871505737, 0.9746138453483582, 0.9698154926300049, 0.9888123273849487, 0.9691790342330933, 0.9692882895469666]
[2025-05-13 11:00:26,647]: Mean: 0.98300344
[2025-05-13 11:00:26,647]: Min: 0.96537316
[2025-05-13 11:00:26,647]: Max: 1.01313806
[2025-05-13 11:00:26,648]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-13 11:00:26,651]: Sample Values (25 elements): [-0.00773729057982564, 0.00773729057982564, 0.0, -0.023211872205138206, -0.023211872205138206, -0.01547458115965128, -0.023211872205138206, 0.00773729057982564, 0.0, -0.00773729057982564, 0.0, -0.01547458115965128, -0.00773729057982564, 0.023211872205138206, 0.03094916231930256, 0.0, 0.0, -0.03094916231930256, -0.00773729057982564, 0.03094916231930256, 0.023211872205138206, -0.01547458115965128, -0.01547458115965128, 0.023211872205138206, 0.0]
[2025-05-13 11:00:26,651]: Mean: -0.00014306
[2025-05-13 11:00:26,651]: Min: -0.05416103
[2025-05-13 11:00:26,652]: Max: 0.06189832
[2025-05-13 11:00:26,652]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-13 11:00:26,652]: Sample Values (25 elements): [0.964206337928772, 0.9565241932868958, 0.9608384966850281, 0.9561272859573364, 0.9581102132797241, 0.9660442471504211, 0.9586623311042786, 0.9548754692077637, 0.9610827565193176, 0.9598662257194519, 0.9571269750595093, 0.9599975347518921, 0.95799720287323, 0.962047815322876, 0.9640099406242371, 0.9599807262420654, 0.9518632292747498, 0.9684245586395264, 0.9593142867088318, 0.9731649160385132, 0.9592652320861816, 0.9558071494102478, 0.9633721113204956, 0.9591207504272461, 0.9569596648216248]
[2025-05-13 11:00:26,652]: Mean: 0.95980340
[2025-05-13 11:00:26,652]: Min: 0.94336146
[2025-05-13 11:00:26,652]: Max: 0.97316492
[2025-05-13 11:00:26,653]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 11:00:26,660]: Sample Values (25 elements): [0.018494077026844025, -0.012329384684562683, 0.024658769369125366, -0.012329384684562683, 0.012329384684562683, -0.012329384684562683, 0.0061646923422813416, -0.018494077026844025, -0.018494077026844025, 0.0, 0.0061646923422813416, -0.018494077026844025, 0.0061646923422813416, -0.012329384684562683, 0.018494077026844025, 0.0061646923422813416, -0.024658769369125366, 0.018494077026844025, 0.0, -0.0061646923422813416, 0.0061646923422813416, -0.018494077026844025, 0.024658769369125366, -0.012329384684562683, -0.018494077026844025]
[2025-05-13 11:00:26,660]: Mean: -0.00038526
[2025-05-13 11:00:26,660]: Min: -0.04315285
[2025-05-13 11:00:26,660]: Max: 0.04931754
[2025-05-13 11:00:26,660]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-13 11:00:26,660]: Sample Values (25 elements): [0.9660119414329529, 0.9705960750579834, 0.9669250845909119, 0.9674973487854004, 0.9573433995246887, 0.963171660900116, 0.9618772864341736, 0.9678299427032471, 0.9632903933525085, 0.9620568156242371, 0.9674851298332214, 0.96735680103302, 0.9664865732192993, 0.9722339510917664, 0.9716753959655762, 0.9683652520179749, 0.9721816778182983, 0.9652974009513855, 0.9679756164550781, 0.9665982127189636, 0.9653047323226929, 0.9765700101852417, 0.9713521599769592, 0.9634010195732117, 0.9743220210075378]
[2025-05-13 11:00:26,661]: Mean: 0.96691024
[2025-05-13 11:00:26,661]: Min: 0.95136404
[2025-05-13 11:00:26,661]: Max: 0.98876989
[2025-05-13 11:00:26,662]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-13 11:00:26,662]: Sample Values (25 elements): [0.029835663735866547, 0.04475349560379982, 0.05967132747173309, 0.0, -0.029835663735866547, 0.05967132747173309, -0.07458916306495667, 0.014917831867933273, -0.05967132747173309, 0.05967132747173309, -0.05967132747173309, 0.07458916306495667, 0.04475349560379982, -0.014917831867933273, -0.05967132747173309, -0.04475349560379982, -0.05967132747173309, 0.014917831867933273, 0.07458916306495667, 0.014917831867933273, -0.029835663735866547, 0.0, -0.029835663735866547, 0.05967132747173309, 0.0]
[2025-05-13 11:00:26,663]: Mean: -0.00012019
[2025-05-13 11:00:26,663]: Min: -0.10442482
[2025-05-13 11:00:26,663]: Max: 0.11934265
[2025-05-13 11:00:26,663]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-13 11:00:26,663]: Sample Values (25 elements): [0.9391040802001953, 0.9396117329597473, 0.9320008754730225, 0.9468992948532104, 0.9422115087509155, 0.9443066716194153, 0.9403411746025085, 0.9369733333587646, 0.941605269908905, 0.9465175867080688, 0.939329981803894, 0.9408465623855591, 0.9453709125518799, 0.9347959160804749, 0.9441132545471191, 0.9461928606033325, 0.9382317066192627, 0.946209728717804, 0.9493834972381592, 0.9441128969192505, 0.938717782497406, 0.9486805200576782, 0.9425767660140991, 0.946071445941925, 0.9388055205345154]
[2025-05-13 11:00:26,663]: Mean: 0.94069833
[2025-05-13 11:00:26,663]: Min: 0.92805886
[2025-05-13 11:00:26,664]: Max: 0.95187414
[2025-05-13 11:00:26,665]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 11:00:26,670]: Sample Values (25 elements): [-0.019165754318237305, -0.012777169235050678, -0.019165754318237305, 0.006388584617525339, 0.006388584617525339, 0.0, -0.006388584617525339, 0.0, 0.006388584617525339, -0.012777169235050678, 0.006388584617525339, 0.006388584617525339, -0.025554338470101357, 0.006388584617525339, 0.019165754318237305, 0.0, 0.006388584617525339, -0.012777169235050678, -0.019165754318237305, -0.012777169235050678, -0.012777169235050678, 0.0, 0.012777169235050678, -0.012777169235050678, 0.0]
[2025-05-13 11:00:26,671]: Mean: -0.00043229
[2025-05-13 11:00:26,671]: Min: -0.04472009
[2025-05-13 11:00:26,671]: Max: 0.05110868
[2025-05-13 11:00:26,671]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-13 11:00:26,671]: Sample Values (25 elements): [0.9606073498725891, 0.9571774005889893, 0.958768367767334, 0.9623062610626221, 0.958122968673706, 0.9561241269111633, 0.9532932043075562, 0.9591309428215027, 0.9606427550315857, 0.9622727036476135, 0.9695682525634766, 0.9631689190864563, 0.9582796096801758, 0.9573720097541809, 0.9605923295021057, 0.9564077854156494, 0.9559962749481201, 0.9551873803138733, 0.9632827639579773, 0.9585590362548828, 0.9562962055206299, 0.9528569579124451, 0.9601709246635437, 0.9531375765800476, 0.9571319818496704]
[2025-05-13 11:00:26,671]: Mean: 0.95961392
[2025-05-13 11:00:26,672]: Min: 0.94919688
[2025-05-13 11:00:26,672]: Max: 0.97721332
[2025-05-13 11:00:26,673]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 11:00:26,678]: Sample Values (25 elements): [-0.011051655746996403, 0.02762913890182972, 0.0, -0.011051655746996403, -0.005525827873498201, 0.011051655746996403, 0.0, -0.011051655746996403, 0.01657748408615589, 0.01657748408615589, 0.0, 0.0, 0.022103311493992805, 0.005525827873498201, 0.01657748408615589, -0.005525827873498201, -0.022103311493992805, -0.011051655746996403, -0.01657748408615589, 0.01657748408615589, 0.011051655746996403, -0.011051655746996403, -0.011051655746996403, 0.005525827873498201, 0.005525827873498201]
[2025-05-13 11:00:26,679]: Mean: -0.00011696
[2025-05-13 11:00:26,679]: Min: -0.03868080
[2025-05-13 11:00:26,679]: Max: 0.04420662
[2025-05-13 11:00:26,679]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-13 11:00:26,679]: Sample Values (25 elements): [0.9740737080574036, 0.9689993262290955, 0.9673848152160645, 0.9737723469734192, 0.9785104990005493, 0.9798769950866699, 0.9675509333610535, 0.9714139699935913, 0.9692949652671814, 0.9685502052307129, 0.9672473669052124, 0.9688286781311035, 0.9703200459480286, 0.9683811664581299, 0.978846549987793, 0.9723882675170898, 0.9765610098838806, 0.9762732982635498, 0.9741527438163757, 0.9702770709991455, 0.9658498764038086, 0.9699059724807739, 0.9806571006774902, 0.985889732837677, 0.9934118390083313]
[2025-05-13 11:00:26,680]: Mean: 0.97229254
[2025-05-13 11:00:26,680]: Min: 0.96036851
[2025-05-13 11:00:26,680]: Max: 0.99766690
[2025-05-13 11:00:26,681]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-13 11:00:26,693]: Sample Values (25 elements): [0.01063164509832859, -0.02126329019665718, 0.02126329019665718, -0.02126329019665718, 0.01594746857881546, 0.01063164509832859, 0.005315822549164295, 0.0, 0.01063164509832859, 0.02126329019665718, 0.01063164509832859, -0.01594746857881546, -0.01063164509832859, 0.005315822549164295, -0.005315822549164295, 0.0, -0.005315822549164295, -0.01594746857881546, 0.0, -0.02126329019665718, 0.0, 0.01594746857881546, 0.01594746857881546, -0.005315822549164295, 0.02126329019665718]
[2025-05-13 11:00:26,693]: Mean: -0.00003636
[2025-05-13 11:00:26,694]: Min: -0.03721076
[2025-05-13 11:00:26,694]: Max: 0.04252658
[2025-05-13 11:00:26,694]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-13 11:00:26,694]: Sample Values (25 elements): [0.9605547785758972, 0.9570157527923584, 0.9602069854736328, 0.961080014705658, 0.9613430500030518, 0.9565791487693787, 0.9603836536407471, 0.9576035141944885, 0.9629284739494324, 0.9557136297225952, 0.959100604057312, 0.9602862596511841, 0.9618625044822693, 0.95623779296875, 0.9563759565353394, 0.9596081376075745, 0.9633496999740601, 0.9625697135925293, 0.9621719121932983, 0.9604687690734863, 0.9581871032714844, 0.9609164595603943, 0.9584304094314575, 0.9581401348114014, 0.9616792798042297]
[2025-05-13 11:00:26,694]: Mean: 0.95916677
[2025-05-13 11:00:26,694]: Min: 0.95291090
[2025-05-13 11:00:26,695]: Max: 0.96756458
[2025-05-13 11:00:26,696]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 11:00:26,734]: Sample Values (25 elements): [0.003609839826822281, -0.014439359307289124, -0.018049199134111404, -0.003609839826822281, -0.010829519480466843, -0.007219679653644562, 0.003609839826822281, 0.007219679653644562, -0.014439359307289124, -0.014439359307289124, 0.003609839826822281, -0.007219679653644562, 0.0, 0.014439359307289124, 0.007219679653644562, 0.007219679653644562, -0.014439359307289124, -0.010829519480466843, -0.014439359307289124, 0.007219679653644562, 0.0, 0.014439359307289124, -0.014439359307289124, 0.003609839826822281, -0.007219679653644562]
[2025-05-13 11:00:26,734]: Mean: -0.00003010
[2025-05-13 11:00:26,734]: Min: -0.02526888
[2025-05-13 11:00:26,734]: Max: 0.02887872
[2025-05-13 11:00:26,735]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-13 11:00:26,736]: Sample Values (25 elements): [0.9606783390045166, 0.9633030891418457, 0.9674821496009827, 0.9612479209899902, 0.9602511525154114, 0.9659773111343384, 0.9638615250587463, 0.9645724296569824, 0.9635319113731384, 0.9665060639381409, 0.9704210162162781, 0.9640862345695496, 0.9703189730644226, 0.9686788320541382, 0.9647929668426514, 0.9653074741363525, 0.9638513922691345, 0.9649133682250977, 0.9640259146690369, 0.9673717021942139, 0.9635792374610901, 0.969234049320221, 0.9697813987731934, 0.9666852951049805, 0.9665743112564087]
[2025-05-13 11:00:26,736]: Mean: 0.96447611
[2025-05-13 11:00:26,736]: Min: 0.95864773
[2025-05-13 11:00:26,736]: Max: 0.97488648
[2025-05-13 11:00:26,738]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-13 11:00:26,739]: Sample Values (25 elements): [0.04778885096311569, -0.028673311695456505, -0.03823108226060867, 0.019115541130304337, 0.04778885096311569, -0.028673311695456505, -0.019115541130304337, -0.009557770565152168, -0.03823108226060867, -0.028673311695456505, -0.03823108226060867, -0.009557770565152168, 0.0, -0.009557770565152168, 0.028673311695456505, -0.028673311695456505, 0.05734662339091301, -0.019115541130304337, 0.028673311695456505, 0.0, -0.028673311695456505, -0.03823108226060867, 0.04778885096311569, 0.028673311695456505, -0.05734662339091301]
[2025-05-13 11:00:26,739]: Mean: 0.00000481
[2025-05-13 11:00:26,739]: Min: -0.06690440
[2025-05-13 11:00:26,739]: Max: 0.07646216
[2025-05-13 11:00:26,739]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-13 11:00:26,740]: Sample Values (25 elements): [0.9551391005516052, 0.957073986530304, 0.958928108215332, 0.9535998106002808, 0.9566078186035156, 0.9560765027999878, 0.9585726857185364, 0.9559074640274048, 0.9587965607643127, 0.9587220549583435, 0.9586178660392761, 0.9544579386711121, 0.9559809565544128, 0.9575181007385254, 0.9587478041648865, 0.9594623446464539, 0.9561229348182678, 0.9554166793823242, 0.9534333944320679, 0.9574419856071472, 0.9557430148124695, 0.9549636840820312, 0.9573894739151001, 0.9546541571617126, 0.9567766189575195]
[2025-05-13 11:00:26,740]: Mean: 0.95633250
[2025-05-13 11:00:26,740]: Min: 0.95197791
[2025-05-13 11:00:26,740]: Max: 0.96186173
[2025-05-13 11:00:26,741]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 11:00:26,784]: Sample Values (25 elements): [0.006720607168972492, 0.006720607168972492, -0.003360303584486246, 0.003360303584486246, 0.006720607168972492, 0.003360303584486246, -0.013441214337944984, 0.010080911219120026, 0.010080911219120026, 0.013441214337944984, 0.003360303584486246, 0.003360303584486246, 0.016801517456769943, -0.010080911219120026, 0.003360303584486246, -0.006720607168972492, 0.010080911219120026, 0.0, 0.003360303584486246, 0.006720607168972492, -0.006720607168972492, 0.010080911219120026, -0.003360303584486246, 0.010080911219120026, 0.003360303584486246]
[2025-05-13 11:00:26,784]: Mean: -0.00013314
[2025-05-13 11:00:26,785]: Min: -0.02352213
[2025-05-13 11:00:26,785]: Max: 0.02688243
[2025-05-13 11:00:26,785]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-13 11:00:26,785]: Sample Values (25 elements): [0.9605843424797058, 0.9622758030891418, 0.9575912356376648, 0.9559274315834045, 0.9572356343269348, 0.9560700058937073, 0.95843505859375, 0.9582516551017761, 0.957817792892456, 0.9587681293487549, 0.956364631652832, 0.9581509232521057, 0.9580522775650024, 0.9596577882766724, 0.9607502818107605, 0.9610617160797119, 0.9566605687141418, 0.9564213752746582, 0.9549959301948547, 0.9559125304222107, 0.9573551416397095, 0.9631724953651428, 0.9612852931022644, 0.9605745077133179, 0.9578986763954163]
[2025-05-13 11:00:26,785]: Mean: 0.95889789
[2025-05-13 11:00:26,785]: Min: 0.95432246
[2025-05-13 11:00:26,786]: Max: 0.97509927
[2025-05-13 11:00:26,787]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 11:00:26,827]: Sample Values (25 elements): [0.005779797676950693, 0.005779797676950693, -0.0028898988384753466, -0.01444949395954609, 0.011559595353901386, 0.008669696748256683, -0.005779797676950693, -0.0028898988384753466, 0.011559595353901386, -0.0028898988384753466, -0.0028898988384753466, -0.011559595353901386, 0.008669696748256683, 0.01444949395954609, 0.0, 0.0, 0.0028898988384753466, -0.005779797676950693, 0.0, -0.005779797676950693, 0.005779797676950693, 0.017339393496513367, -0.0028898988384753466, -0.0028898988384753466, -0.008669696748256683]
[2025-05-13 11:00:26,827]: Mean: 0.00002599
[2025-05-13 11:00:26,827]: Min: -0.02022929
[2025-05-13 11:00:26,827]: Max: 0.02311919
[2025-05-13 11:00:26,828]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-13 11:00:26,828]: Sample Values (25 elements): [0.9658449292182922, 0.9674618244171143, 0.9682506918907166, 0.9676556587219238, 0.9710145592689514, 0.9700905680656433, 0.969275951385498, 0.9673733711242676, 0.9672809839248657, 0.9699973464012146, 0.9696520566940308, 0.9688504934310913, 0.9701724052429199, 0.9691029787063599, 0.9704013466835022, 0.9706379771232605, 0.9716879725456238, 0.9640083909034729, 0.9638651013374329, 0.968999445438385, 0.9674980044364929, 0.9665277004241943, 0.9706689715385437, 0.9688122272491455, 0.9631212949752808]
[2025-05-13 11:00:26,828]: Mean: 0.96881020
[2025-05-13 11:00:26,828]: Min: 0.96182054
[2025-05-13 11:00:26,828]: Max: 0.97898883
[2025-05-13 11:00:26,828]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-13 11:00:26,829]: Sample Values (25 elements): [-0.03680277243256569, 0.1130877286195755, -0.021235859021544456, -0.05211407318711281, 0.01639801263809204, 0.09015198051929474, 0.011424449272453785, 0.05983993038535118, 0.06478773057460785, 0.020231612026691437, 0.007800940424203873, 0.001566605526022613, 0.07291632890701294, 0.002603264059871435, -0.09601964801549911, 0.06128980964422226, 0.01039322279393673, -0.05692566931247711, 0.05733519420027733, -0.04507574439048767, 0.05092613399028778, 0.011511905118823051, 0.02065054513514042, 0.005602492950856686, -0.051735762506723404]
[2025-05-13 11:00:26,829]: Mean: 0.00046915
[2025-05-13 11:00:26,829]: Min: -0.12215501
[2025-05-13 11:00:26,829]: Max: 0.14855288
[2025-05-13 11:00:26,829]: 


QAT of ResNet18 with relu down to 3 bits...
[2025-05-13 11:00:27,059]: [ResNet18_relu_quantized_3_bits] after configure_qat:
[2025-05-13 11:00:27,104]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 11:02:04,001]: [ResNet18_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.6017 Train Acc: 0.8022 Eval Loss: 0.5996 Eval Acc: 0.8125 (LR: 0.001000)
[2025-05-13 11:03:40,950]: [ResNet18_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.4316 Train Acc: 0.8489 Eval Loss: 0.5877 Eval Acc: 0.8159 (LR: 0.001000)
[2025-05-13 11:05:17,714]: [ResNet18_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.3890 Train Acc: 0.8629 Eval Loss: 0.5094 Eval Acc: 0.8382 (LR: 0.001000)
[2025-05-13 11:06:54,303]: [ResNet18_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.3563 Train Acc: 0.8733 Eval Loss: 0.5295 Eval Acc: 0.8384 (LR: 0.001000)
[2025-05-13 11:08:31,119]: [ResNet18_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.3366 Train Acc: 0.8807 Eval Loss: 0.5498 Eval Acc: 0.8320 (LR: 0.001000)
[2025-05-13 11:10:07,734]: [ResNet18_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.3234 Train Acc: 0.8862 Eval Loss: 0.4501 Eval Acc: 0.8578 (LR: 0.001000)
[2025-05-13 11:11:44,284]: [ResNet18_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.3201 Train Acc: 0.8856 Eval Loss: 0.4742 Eval Acc: 0.8524 (LR: 0.001000)
[2025-05-13 11:13:20,710]: [ResNet18_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.3051 Train Acc: 0.8920 Eval Loss: 0.4844 Eval Acc: 0.8493 (LR: 0.001000)
[2025-05-13 11:14:57,136]: [ResNet18_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.3023 Train Acc: 0.8946 Eval Loss: 0.4881 Eval Acc: 0.8496 (LR: 0.001000)
[2025-05-13 11:16:33,893]: [ResNet18_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.2903 Train Acc: 0.8969 Eval Loss: 0.5045 Eval Acc: 0.8507 (LR: 0.001000)
[2025-05-13 11:18:10,703]: [ResNet18_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.2802 Train Acc: 0.9005 Eval Loss: 0.4925 Eval Acc: 0.8465 (LR: 0.001000)
[2025-05-13 11:19:47,534]: [ResNet18_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.2822 Train Acc: 0.9002 Eval Loss: 0.4736 Eval Acc: 0.8537 (LR: 0.001000)
[2025-05-13 11:21:24,310]: [ResNet18_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.2689 Train Acc: 0.9050 Eval Loss: 0.4791 Eval Acc: 0.8582 (LR: 0.001000)
[2025-05-13 11:23:00,865]: [ResNet18_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.2671 Train Acc: 0.9065 Eval Loss: 0.4560 Eval Acc: 0.8586 (LR: 0.001000)
[2025-05-13 11:24:37,297]: [ResNet18_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.2685 Train Acc: 0.9045 Eval Loss: 0.4521 Eval Acc: 0.8618 (LR: 0.001000)
[2025-05-13 11:26:13,902]: [ResNet18_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.2583 Train Acc: 0.9075 Eval Loss: 0.4557 Eval Acc: 0.8685 (LR: 0.001000)
[2025-05-13 11:27:50,477]: [ResNet18_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.2576 Train Acc: 0.9084 Eval Loss: 0.4665 Eval Acc: 0.8603 (LR: 0.001000)
[2025-05-13 11:29:27,255]: [ResNet18_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.2446 Train Acc: 0.9127 Eval Loss: 0.4306 Eval Acc: 0.8705 (LR: 0.001000)
[2025-05-13 11:31:03,858]: [ResNet18_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.2488 Train Acc: 0.9122 Eval Loss: 0.4456 Eval Acc: 0.8642 (LR: 0.001000)
[2025-05-13 11:32:40,874]: [ResNet18_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.2443 Train Acc: 0.9128 Eval Loss: 0.4592 Eval Acc: 0.8637 (LR: 0.001000)
[2025-05-13 11:34:17,489]: [ResNet18_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.2358 Train Acc: 0.9171 Eval Loss: 0.4801 Eval Acc: 0.8583 (LR: 0.001000)
[2025-05-13 11:35:54,033]: [ResNet18_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.2302 Train Acc: 0.9182 Eval Loss: 0.4330 Eval Acc: 0.8679 (LR: 0.001000)
[2025-05-13 11:37:30,694]: [ResNet18_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.2270 Train Acc: 0.9201 Eval Loss: 0.4656 Eval Acc: 0.8624 (LR: 0.001000)
[2025-05-13 11:39:07,088]: [ResNet18_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.2282 Train Acc: 0.9188 Eval Loss: 0.4717 Eval Acc: 0.8619 (LR: 0.001000)
[2025-05-13 11:40:43,521]: [ResNet18_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.2260 Train Acc: 0.9187 Eval Loss: 0.4524 Eval Acc: 0.8672 (LR: 0.001000)
[2025-05-13 11:42:20,285]: [ResNet18_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.2243 Train Acc: 0.9215 Eval Loss: 0.4411 Eval Acc: 0.8717 (LR: 0.001000)
[2025-05-13 11:43:56,897]: [ResNet18_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.2249 Train Acc: 0.9200 Eval Loss: 0.4309 Eval Acc: 0.8721 (LR: 0.001000)
[2025-05-13 11:45:33,849]: [ResNet18_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.2169 Train Acc: 0.9232 Eval Loss: 0.4190 Eval Acc: 0.8740 (LR: 0.001000)
[2025-05-13 11:47:10,460]: [ResNet18_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.2126 Train Acc: 0.9241 Eval Loss: 0.4511 Eval Acc: 0.8704 (LR: 0.001000)
[2025-05-13 11:48:47,273]: [ResNet18_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.2112 Train Acc: 0.9241 Eval Loss: 0.4604 Eval Acc: 0.8653 (LR: 0.000250)
[2025-05-13 11:50:24,009]: [ResNet18_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.1781 Train Acc: 0.9369 Eval Loss: 0.3804 Eval Acc: 0.8877 (LR: 0.000250)
[2025-05-13 11:52:00,603]: [ResNet18_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.1725 Train Acc: 0.9389 Eval Loss: 0.3900 Eval Acc: 0.8863 (LR: 0.000250)
[2025-05-13 11:53:37,150]: [ResNet18_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.1621 Train Acc: 0.9427 Eval Loss: 0.3964 Eval Acc: 0.8833 (LR: 0.000250)
[2025-05-13 11:55:13,699]: [ResNet18_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.1622 Train Acc: 0.9429 Eval Loss: 0.3905 Eval Acc: 0.8877 (LR: 0.000250)
[2025-05-13 11:56:50,054]: [ResNet18_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.1622 Train Acc: 0.9427 Eval Loss: 0.3905 Eval Acc: 0.8848 (LR: 0.000250)
[2025-05-13 11:58:26,463]: [ResNet18_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.1604 Train Acc: 0.9431 Eval Loss: 0.3908 Eval Acc: 0.8855 (LR: 0.000250)
[2025-05-13 12:00:02,920]: [ResNet18_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.1545 Train Acc: 0.9457 Eval Loss: 0.3962 Eval Acc: 0.8834 (LR: 0.000250)
[2025-05-13 12:01:39,255]: [ResNet18_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.1552 Train Acc: 0.9450 Eval Loss: 0.3973 Eval Acc: 0.8869 (LR: 0.000250)
[2025-05-13 12:03:15,859]: [ResNet18_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.1555 Train Acc: 0.9443 Eval Loss: 0.4035 Eval Acc: 0.8804 (LR: 0.000250)
[2025-05-13 12:04:52,763]: [ResNet18_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.1537 Train Acc: 0.9448 Eval Loss: 0.3980 Eval Acc: 0.8857 (LR: 0.000250)
[2025-05-13 12:06:29,417]: [ResNet18_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.1559 Train Acc: 0.9439 Eval Loss: 0.4005 Eval Acc: 0.8804 (LR: 0.000250)
[2025-05-13 12:08:06,033]: [ResNet18_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.1497 Train Acc: 0.9468 Eval Loss: 0.3998 Eval Acc: 0.8828 (LR: 0.000250)
[2025-05-13 12:09:42,838]: [ResNet18_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.1549 Train Acc: 0.9451 Eval Loss: 0.3990 Eval Acc: 0.8813 (LR: 0.000250)
[2025-05-13 12:11:19,628]: [ResNet18_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.1548 Train Acc: 0.9451 Eval Loss: 0.4140 Eval Acc: 0.8834 (LR: 0.000250)
[2025-05-13 12:12:56,423]: [ResNet18_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.1454 Train Acc: 0.9486 Eval Loss: 0.3963 Eval Acc: 0.8898 (LR: 0.000063)
[2025-05-13 12:14:33,017]: [ResNet18_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.1400 Train Acc: 0.9500 Eval Loss: 0.3923 Eval Acc: 0.8885 (LR: 0.000063)
[2025-05-13 12:16:09,757]: [ResNet18_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.1369 Train Acc: 0.9510 Eval Loss: 0.3930 Eval Acc: 0.8874 (LR: 0.000063)
[2025-05-13 12:17:46,336]: [ResNet18_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.1373 Train Acc: 0.9517 Eval Loss: 0.3939 Eval Acc: 0.8876 (LR: 0.000063)
[2025-05-13 12:19:22,941]: [ResNet18_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.1396 Train Acc: 0.9509 Eval Loss: 0.3952 Eval Acc: 0.8875 (LR: 0.000063)
[2025-05-13 12:20:59,570]: [ResNet18_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.1379 Train Acc: 0.9512 Eval Loss: 0.3963 Eval Acc: 0.8909 (LR: 0.000063)
[2025-05-13 12:22:36,390]: [ResNet18_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.1356 Train Acc: 0.9528 Eval Loss: 0.3905 Eval Acc: 0.8879 (LR: 0.000063)
[2025-05-13 12:24:13,182]: [ResNet18_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.1358 Train Acc: 0.9521 Eval Loss: 0.3995 Eval Acc: 0.8859 (LR: 0.000063)
[2025-05-13 12:25:49,914]: [ResNet18_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.1373 Train Acc: 0.9519 Eval Loss: 0.3952 Eval Acc: 0.8875 (LR: 0.000063)
[2025-05-13 12:27:26,507]: [ResNet18_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.1353 Train Acc: 0.9519 Eval Loss: 0.4058 Eval Acc: 0.8870 (LR: 0.000063)
[2025-05-13 12:29:03,308]: [ResNet18_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.1347 Train Acc: 0.9524 Eval Loss: 0.4051 Eval Acc: 0.8874 (LR: 0.000063)
[2025-05-13 12:30:39,931]: [ResNet18_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.1330 Train Acc: 0.9526 Eval Loss: 0.4045 Eval Acc: 0.8873 (LR: 0.000063)
[2025-05-13 12:32:16,714]: [ResNet18_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.1357 Train Acc: 0.9521 Eval Loss: 0.4023 Eval Acc: 0.8862 (LR: 0.000063)
[2025-05-13 12:33:53,495]: [ResNet18_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.1379 Train Acc: 0.9519 Eval Loss: 0.3836 Eval Acc: 0.8901 (LR: 0.000063)
[2025-05-13 12:35:30,055]: [ResNet18_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.1344 Train Acc: 0.9520 Eval Loss: 0.4024 Eval Acc: 0.8891 (LR: 0.000063)
[2025-05-13 12:37:07,372]: [ResNet18_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.1331 Train Acc: 0.9535 Eval Loss: 0.3988 Eval Acc: 0.8871 (LR: 0.000063)
[2025-05-13 12:37:07,372]: [ResNet18_relu_quantized_3_bits] Best Eval Accuracy: 0.8909
[2025-05-13 12:37:07,463]: 


Quantization of model down to 3 bits finished
[2025-05-13 12:37:07,463]: Model Architecture:
[2025-05-13 12:37:07,513]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.4878], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.414369583129883)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0371], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12127134948968887, max_val=0.13832080364227295)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8819], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.173371315002441)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0327], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12242298573255539, max_val=0.10642018169164658)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5364], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.755037307739258)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0329], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1208268404006958, max_val=0.10943697392940521)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1644], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.150607109069824)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0270], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09219615906476974, max_val=0.09707064926624298)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.6612], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=18.628385543823242)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0240], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0814002975821495, max_val=0.08628598600625992)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0696], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.486936569213867)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0668918639421463, max_val=0.06907787919044495)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0461], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16151730716228485, max_val=0.16146136820316315)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6690], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=11.683013916015625)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0196], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06875509023666382, max_val=0.06862583011388779)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9638], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.746912479400635)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05638454854488373, max_val=0.06643842160701752)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0820], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.574179649353027)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0159], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05208741873502731, max_val=0.05886485427618027)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9637], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.746007919311523)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04717450216412544, max_val=0.04965733736753464)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0324], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11295392364263535, max_val=0.11395561695098877)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4810], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.366658210754395)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0142], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05157895386219025, max_val=0.04754161834716797)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9222], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.455562114715576)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0402054637670517, max_val=0.04388881102204323)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9592], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.714638710021973)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0117], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03892356902360916, max_val=0.04310351982712746)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9968], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.977518081665039)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0083], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.028118552640080452, max_val=0.029837490990757942)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0213], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.071535125374794, max_val=0.07747545093297958)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5081], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.5568265914917)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.028277374804019928, max_val=0.028417574241757393)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9586], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.709927558898926)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02053827978670597, max_val=0.02196861244738102)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9812], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.868520736694336)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 12:37:07,514]: 
Model Weights:
[2025-05-13 12:37:07,514]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-13 12:37:07,514]: Sample Values (25 elements): [0.12487325072288513, 0.010689964517951012, -0.2132733166217804, 0.0649639442563057, -0.06982944160699844, -0.07238541543483734, -0.1639890968799591, -0.08901802450418472, -0.10786477476358414, 0.1631096601486206, -0.12911714613437653, 0.023110926151275635, -0.03762088716030121, 0.027233362197875977, 0.14904028177261353, 0.19529932737350464, 0.08100490272045135, -0.12034497410058975, 0.000266991148237139, -0.1543666124343872, -0.2226094901561737, -0.06375618278980255, -0.2164558321237564, 0.16404330730438232, -0.017895033583045006]
[2025-05-13 12:37:07,514]: Mean: 0.00035003
[2025-05-13 12:37:07,515]: Min: -0.31138209
[2025-05-13 12:37:07,515]: Max: 0.39241505
[2025-05-13 12:37:07,515]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-13 12:37:07,515]: Sample Values (25 elements): [1.070522427558899, 1.1593389511108398, 1.074313759803772, 1.0372469425201416, 1.0800784826278687, 1.1157575845718384, 1.0488338470458984, 1.0966432094573975, 1.1251413822174072, 1.029473066329956, 1.0522761344909668, 1.0961687564849854, 1.1292897462844849, 0.901883065700531, 1.0306178331375122, 1.0876860618591309, 1.1182607412338257, 1.1899415254592896, 1.106216311454773, 1.2856156826019287, 1.2638994455337524, 1.2351667881011963, 1.2028672695159912, 1.01139235496521, 1.1462035179138184]
[2025-05-13 12:37:07,515]: Mean: 1.10943973
[2025-05-13 12:37:07,515]: Min: 0.90188307
[2025-05-13 12:37:07,516]: Max: 1.44129527
[2025-05-13 12:37:07,517]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 12:37:07,517]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.03708459064364433, 0.0, -0.07416918128728867, -0.03708459064364433, 0.03708459064364433, 0.0, 0.03708459064364433, -0.03708459064364433, -0.07416918128728867, 0.0, 0.0, -0.03708459064364433, 0.0, 0.03708459064364433, 0.03708459064364433, 0.0, 0.0, -0.03708459064364433, 0.0, 0.03708459064364433, -0.03708459064364433, 0.03708459064364433]
[2025-05-13 12:37:07,517]: Mean: -0.00253106
[2025-05-13 12:37:07,518]: Min: -0.11125377
[2025-05-13 12:37:07,518]: Max: 0.14833836
[2025-05-13 12:37:07,518]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 12:37:07,518]: Sample Values (25 elements): [1.024972677230835, 1.022265076637268, 0.9996137022972107, 1.0127832889556885, 1.0712239742279053, 1.0452617406845093, 1.152930498123169, 1.0077399015426636, 1.0236377716064453, 0.9829744100570679, 1.026654601097107, 1.0113940238952637, 0.964566707611084, 1.0291391611099243, 1.1102029085159302, 1.0191155672073364, 1.0563050508499146, 0.9773572683334351, 0.9276678562164307, 1.0594137907028198, 0.9677394032478333, 1.0107815265655518, 1.0329266786575317, 1.022178292274475, 1.0145280361175537]
[2025-05-13 12:37:07,518]: Mean: 1.01761103
[2025-05-13 12:37:07,518]: Min: 0.92766786
[2025-05-13 12:37:07,518]: Max: 1.20899391
[2025-05-13 12:37:07,519]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 12:37:07,520]: Sample Values (25 elements): [0.03269181400537491, 0.0, -0.03269181400537491, -0.03269181400537491, 0.0, 0.0, 0.03269181400537491, -0.03269181400537491, 0.0, 0.03269181400537491, -0.03269181400537491, 0.0, 0.0, 0.0, 0.03269181400537491, -0.03269181400537491, 0.0, 0.03269181400537491, 0.0, -0.03269181400537491, 0.0, 0.0, 0.0, 0.03269181400537491, -0.03269181400537491]
[2025-05-13 12:37:07,520]: Mean: -0.00198471
[2025-05-13 12:37:07,520]: Min: -0.13076726
[2025-05-13 12:37:07,520]: Max: 0.09807544
[2025-05-13 12:37:07,521]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 12:37:07,521]: Sample Values (25 elements): [1.0323222875595093, 1.0101677179336548, 1.0244370698928833, 1.0506824254989624, 0.9993028044700623, 1.0283781290054321, 0.9900280237197876, 1.0074031352996826, 1.0595521926879883, 0.9992333650588989, 1.0260435342788696, 1.028878092765808, 1.0087734460830688, 1.0466604232788086, 0.9893043041229248, 0.9924648404121399, 1.0136444568634033, 1.0071102380752563, 1.029506802558899, 1.0257505178451538, 1.005744457244873, 1.0265556573867798, 1.006930947303772, 1.069547176361084, 1.0003808736801147]
[2025-05-13 12:37:07,521]: Mean: 1.02144086
[2025-05-13 12:37:07,521]: Min: 0.96460652
[2025-05-13 12:37:07,521]: Max: 1.15795839
[2025-05-13 12:37:07,522]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 12:37:07,523]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.032894790172576904, 0.0, -0.032894790172576904, 0.032894790172576904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032894790172576904, -0.032894790172576904, 0.0, -0.032894790172576904, -0.032894790172576904, 0.0, 0.0, -0.06578958034515381, 0.0, -0.032894790172576904, -0.032894790172576904]
[2025-05-13 12:37:07,523]: Mean: -0.00103064
[2025-05-13 12:37:07,523]: Min: -0.13157916
[2025-05-13 12:37:07,523]: Max: 0.09868437
[2025-05-13 12:37:07,523]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 12:37:07,524]: Sample Values (25 elements): [0.9671133160591125, 0.9814778566360474, 0.9798177480697632, 0.9383322596549988, 0.9671677350997925, 0.9529898762702942, 1.0069644451141357, 1.0071017742156982, 0.9792947173118591, 0.947084367275238, 1.0224931240081787, 0.9630552530288696, 0.9600778222084045, 1.0054467916488647, 0.9890884160995483, 0.9455667734146118, 0.96303790807724, 0.987618088722229, 0.9713205099105835, 0.9608071446418762, 1.0234298706054688, 0.9506398439407349, 0.9817073941230774, 0.9740620851516724, 0.9659085869789124]
[2025-05-13 12:37:07,524]: Mean: 0.97474152
[2025-05-13 12:37:07,524]: Min: 0.93312281
[2025-05-13 12:37:07,524]: Max: 1.05830669
[2025-05-13 12:37:07,525]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 12:37:07,526]: Sample Values (25 elements): [0.027038145810365677, 0.027038145810365677, 0.027038145810365677, 0.0, 0.027038145810365677, -0.027038145810365677, -0.027038145810365677, 0.027038145810365677, 0.0, 0.0, 0.027038145810365677, -0.027038145810365677, 0.027038145810365677, -0.027038145810365677, 0.027038145810365677, 0.027038145810365677, 0.0, -0.027038145810365677, 0.054076291620731354, -0.027038145810365677, 0.027038145810365677, 0.027038145810365677, 0.0, 0.0, 0.0]
[2025-05-13 12:37:07,526]: Mean: -0.00041147
[2025-05-13 12:37:07,526]: Min: -0.08111444
[2025-05-13 12:37:07,526]: Max: 0.10815258
[2025-05-13 12:37:07,526]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 12:37:07,526]: Sample Values (25 elements): [0.9923040866851807, 1.01517915725708, 1.0254067182540894, 1.0035779476165771, 1.0454576015472412, 1.070249319076538, 1.0085155963897705, 0.989719569683075, 1.0032343864440918, 1.0096795558929443, 1.0153475999832153, 0.9866237640380859, 1.0413057804107666, 1.0221747159957886, 1.0514905452728271, 1.0101923942565918, 1.0223475694656372, 1.030397653579712, 1.0009431838989258, 0.9782536625862122, 0.9935640096664429, 1.0907351970672607, 1.0491312742233276, 1.0631412267684937, 1.0143702030181885]
[2025-05-13 12:37:07,527]: Mean: 1.02237749
[2025-05-13 12:37:07,527]: Min: 0.97825366
[2025-05-13 12:37:07,527]: Max: 1.09073520
[2025-05-13 12:37:07,528]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-13 12:37:07,529]: Sample Values (25 elements): [0.04791052266955376, 0.0, -0.04791052266955376, -0.02395526133477688, -0.02395526133477688, 0.02395526133477688, -0.02395526133477688, 0.02395526133477688, 0.02395526133477688, 0.02395526133477688, 0.0, -0.02395526133477688, 0.04791052266955376, -0.04791052266955376, 0.0, -0.02395526133477688, 0.0, 0.04791052266955376, 0.02395526133477688, 0.04791052266955376, -0.02395526133477688, -0.02395526133477688, 0.0, 0.0, 0.02395526133477688]
[2025-05-13 12:37:07,529]: Mean: -0.00056568
[2025-05-13 12:37:07,529]: Min: -0.07186578
[2025-05-13 12:37:07,529]: Max: 0.09582105
[2025-05-13 12:37:07,529]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-13 12:37:07,530]: Sample Values (25 elements): [0.9709396958351135, 0.9916746020317078, 0.9538571834564209, 0.9756821393966675, 0.9726908206939697, 0.9748224020004272, 0.9682406783103943, 0.9757171869277954, 0.9531729817390442, 0.9532857537269592, 0.9685550332069397, 0.9684205651283264, 0.9728490710258484, 0.979056179523468, 0.9673006534576416, 0.9628536105155945, 0.9724039435386658, 0.9675014019012451, 0.9635952115058899, 0.9603956937789917, 0.9662978649139404, 0.9647958278656006, 0.9635599851608276, 0.9761855006217957, 0.9746705889701843]
[2025-05-13 12:37:07,530]: Mean: 0.96877277
[2025-05-13 12:37:07,530]: Min: 0.94546086
[2025-05-13 12:37:07,530]: Max: 0.99444759
[2025-05-13 12:37:07,531]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 12:37:07,533]: Sample Values (25 elements): [-0.01942426525056362, -0.01942426525056362, -0.01942426525056362, 0.03884853050112724, 0.0, 0.0, 0.01942426525056362, 0.01942426525056362, 0.03884853050112724, 0.01942426525056362, 0.0, 0.0, -0.01942426525056362, 0.01942426525056362, -0.01942426525056362, 0.01942426525056362, -0.01942426525056362, 0.01942426525056362, 0.03884853050112724, 0.03884853050112724, 0.01942426525056362, 0.03884853050112724, 0.0, -0.03884853050112724, 0.0]
[2025-05-13 12:37:07,533]: Mean: -0.00050202
[2025-05-13 12:37:07,533]: Min: -0.05827279
[2025-05-13 12:37:07,533]: Max: 0.07769706
[2025-05-13 12:37:07,533]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-13 12:37:07,533]: Sample Values (25 elements): [0.9699535965919495, 0.9947517514228821, 0.9720712304115295, 0.9642637372016907, 0.9823750853538513, 1.0035731792449951, 0.9847139120101929, 0.996209979057312, 1.0105994939804077, 0.985653281211853, 0.9910234212875366, 0.9626953601837158, 0.964357316493988, 0.9969540238380432, 1.0049760341644287, 1.0089524984359741, 0.9725740551948547, 0.9876960515975952, 0.9892131090164185, 0.9905493259429932, 0.9678499698638916, 0.9783810377120972, 0.9725825190544128, 0.990542471408844, 0.9957075715065002]
[2025-05-13 12:37:07,534]: Mean: 0.98435009
[2025-05-13 12:37:07,534]: Min: 0.95648062
[2025-05-13 12:37:07,534]: Max: 1.01388741
[2025-05-13 12:37:07,535]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-13 12:37:07,535]: Sample Values (25 elements): [-0.13841953873634338, -0.04613984376192093, 0.0, 0.04613984376192093, 0.04613984376192093, 0.09227968752384186, 0.04613984376192093, -0.04613984376192093, 0.0, 0.09227968752384186, -0.13841953873634338, 0.0, 0.09227968752384186, 0.09227968752384186, -0.04613984376192093, 0.04613984376192093, 0.09227968752384186, 0.09227968752384186, 0.09227968752384186, 0.04613984376192093, -0.09227968752384186, -0.09227968752384186, -0.09227968752384186, 0.04613984376192093, -0.04613984376192093]
[2025-05-13 12:37:07,535]: Mean: -0.00090680
[2025-05-13 12:37:07,536]: Min: -0.18455938
[2025-05-13 12:37:07,536]: Max: 0.13841954
[2025-05-13 12:37:07,536]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-13 12:37:07,536]: Sample Values (25 elements): [0.9500066041946411, 0.9204475283622742, 0.9347632527351379, 0.9395070672035217, 0.9261922836303711, 0.9512560963630676, 0.9554163813591003, 0.9284012913703918, 0.9288994073867798, 0.9158614277839661, 0.9218549728393555, 0.9414830207824707, 0.9179564714431763, 0.9282591342926025, 0.9291720390319824, 0.9421819448471069, 0.9421168565750122, 0.9369375705718994, 0.9285726547241211, 0.9328380823135376, 0.9285277724266052, 0.9258493185043335, 0.9221155047416687, 0.9377294778823853, 0.9307132363319397]
[2025-05-13 12:37:07,536]: Mean: 0.93185186
[2025-05-13 12:37:07,536]: Min: 0.89609492
[2025-05-13 12:37:07,536]: Max: 0.95541638
[2025-05-13 12:37:07,537]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 12:37:07,539]: Sample Values (25 elements): [0.0, 0.0, 0.039251651614904404, 0.019625825807452202, 0.0, 0.039251651614904404, -0.019625825807452202, -0.019625825807452202, 0.019625825807452202, 0.0, 0.0, 0.0, 0.019625825807452202, 0.019625825807452202, 0.0, 0.0, -0.039251651614904404, 0.0, 0.0, -0.019625825807452202, 0.039251651614904404, 0.0, 0.0, -0.019625825807452202, 0.0]
[2025-05-13 12:37:07,539]: Mean: -0.00073922
[2025-05-13 12:37:07,539]: Min: -0.07850330
[2025-05-13 12:37:07,539]: Max: 0.05887748
[2025-05-13 12:37:07,539]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-13 12:37:07,540]: Sample Values (25 elements): [0.9695104956626892, 0.9626311659812927, 0.9680536985397339, 0.9855257868766785, 0.9607263207435608, 0.9741804599761963, 0.960770845413208, 0.9584547281265259, 0.9438812732696533, 0.9516707062721252, 0.9716867208480835, 0.9770684242248535, 0.9539273381233215, 0.9610615968704224, 0.9641981720924377, 0.9580889344215393, 0.9652230739593506, 0.9741659760475159, 0.9718523621559143, 0.9617520570755005, 0.9645318984985352, 0.9693585634231567, 0.9610522985458374, 0.9673354029655457, 0.9895068407058716]
[2025-05-13 12:37:07,540]: Mean: 0.96563864
[2025-05-13 12:37:07,540]: Min: 0.94388127
[2025-05-13 12:37:07,540]: Max: 1.00541842
[2025-05-13 12:37:07,541]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 12:37:07,543]: Sample Values (25 elements): [0.0175461508333683, 0.0, 0.0, 0.0175461508333683, 0.0, 0.0, 0.0, 0.0, -0.0175461508333683, 0.0, 0.0, 0.0175461508333683, -0.0175461508333683, 0.0, 0.0350923016667366, 0.0, 0.0, 0.0175461508333683, -0.0175461508333683, -0.0175461508333683, 0.0175461508333683, 0.0175461508333683, -0.0175461508333683, -0.0175461508333683, 0.0]
[2025-05-13 12:37:07,543]: Mean: 0.00007806
[2025-05-13 12:37:07,543]: Min: -0.05263845
[2025-05-13 12:37:07,543]: Max: 0.07018460
[2025-05-13 12:37:07,543]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-13 12:37:07,544]: Sample Values (25 elements): [0.9975069761276245, 1.0093848705291748, 0.9996742606163025, 1.0028566122055054, 0.9969558119773865, 1.0025990009307861, 0.974926233291626, 1.0374308824539185, 0.9975373148918152, 0.992487907409668, 1.0054439306259155, 1.002799391746521, 1.0060555934906006, 1.012015461921692, 0.9828616380691528, 1.0048571825027466, 0.9970390796661377, 1.0019819736480713, 0.9998528361320496, 1.0031121969223022, 0.9857187271118164, 0.9825482368469238, 0.988055408000946, 1.0007984638214111, 0.9926084280014038]
[2025-05-13 12:37:07,544]: Mean: 0.99976134
[2025-05-13 12:37:07,544]: Min: 0.97492623
[2025-05-13 12:37:07,544]: Max: 1.03743088
[2025-05-13 12:37:07,545]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-13 12:37:07,548]: Sample Values (25 elements): [-0.01585032232105732, -0.01585032232105732, 0.03170064464211464, 0.01585032232105732, 0.0, 0.01585032232105732, -0.03170064464211464, 0.0, 0.01585032232105732, -0.03170064464211464, 0.0, -0.03170064464211464, 0.01585032232105732, -0.01585032232105732, 0.0, -0.01585032232105732, 0.0, -0.01585032232105732, 0.01585032232105732, -0.01585032232105732, -0.01585032232105732, 0.0, 0.0, -0.01585032232105732, 0.01585032232105732]
[2025-05-13 12:37:07,548]: Mean: -0.00013936
[2025-05-13 12:37:07,548]: Min: -0.04755097
[2025-05-13 12:37:07,549]: Max: 0.06340129
[2025-05-13 12:37:07,549]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-13 12:37:07,549]: Sample Values (25 elements): [0.9718469381332397, 0.974972128868103, 0.9679732918739319, 0.9678220152854919, 0.9663828015327454, 0.9787774085998535, 0.9625895619392395, 0.9714779853820801, 0.9541640281677246, 0.9641603827476501, 0.9611087441444397, 0.9610125422477722, 0.9639808535575867, 0.9641470909118652, 0.9643766283988953, 0.9663118124008179, 0.9609963297843933, 0.9675241112709045, 0.9598098397254944, 0.9666092395782471, 0.9684141278266907, 0.9671055674552917, 0.9626967906951904, 0.967200517654419, 0.9694902896881104]
[2025-05-13 12:37:07,549]: Mean: 0.96427166
[2025-05-13 12:37:07,549]: Min: 0.94952714
[2025-05-13 12:37:07,549]: Max: 0.97942322
[2025-05-13 12:37:07,551]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 12:37:07,557]: Sample Values (25 elements): [0.0, 0.013833129778504372, 0.0, 0.013833129778504372, 0.0, 0.0, 0.0, 0.0, 0.013833129778504372, -0.013833129778504372, -0.013833129778504372, 0.013833129778504372, -0.013833129778504372, -0.013833129778504372, -0.013833129778504372, 0.013833129778504372, 0.0, -0.013833129778504372, 0.0, 0.013833129778504372, 0.0, 0.0, -0.013833129778504372, 0.027666259557008743, 0.0]
[2025-05-13 12:37:07,557]: Mean: -0.00026361
[2025-05-13 12:37:07,557]: Min: -0.04149939
[2025-05-13 12:37:07,557]: Max: 0.05533252
[2025-05-13 12:37:07,557]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-13 12:37:07,558]: Sample Values (25 elements): [0.9716859459877014, 0.990939199924469, 0.9789195656776428, 0.9621084332466125, 0.9676367044448853, 0.9796386361122131, 0.9875061511993408, 0.9726849794387817, 0.9781579375267029, 0.9758304953575134, 0.9768053293228149, 0.9641414880752563, 0.9720418453216553, 0.965712308883667, 0.9868009090423584, 0.9778711795806885, 0.9638621807098389, 0.9761438965797424, 0.9796631932258606, 0.9817500114440918, 0.9715996980667114, 0.9735199213027954, 0.9752447009086609, 0.9665098190307617, 0.9742065668106079]
[2025-05-13 12:37:07,558]: Mean: 0.97699285
[2025-05-13 12:37:07,558]: Min: 0.95803392
[2025-05-13 12:37:07,558]: Max: 1.00625169
[2025-05-13 12:37:07,559]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-13 12:37:07,560]: Sample Values (25 elements): [0.06483127176761627, 0.032415635883808136, -0.06483127176761627, 0.032415635883808136, 0.06483127176761627, -0.06483127176761627, 0.06483127176761627, 0.032415635883808136, -0.032415635883808136, 0.032415635883808136, 0.032415635883808136, 0.032415635883808136, 0.0, -0.032415635883808136, -0.032415635883808136, 0.06483127176761627, 0.032415635883808136, 0.06483127176761627, 0.032415635883808136, -0.06483127176761627, 0.0, 0.09724690765142441, 0.0, -0.032415635883808136, -0.06483127176761627]
[2025-05-13 12:37:07,560]: Mean: -0.00030865
[2025-05-13 12:37:07,560]: Min: -0.09724691
[2025-05-13 12:37:07,560]: Max: 0.12966254
[2025-05-13 12:37:07,560]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-13 12:37:07,560]: Sample Values (25 elements): [0.9327102899551392, 0.9367352724075317, 0.9320464134216309, 0.9366586208343506, 0.933731734752655, 0.9423943758010864, 0.9436304569244385, 0.9313217997550964, 0.9389671683311462, 0.93694669008255, 0.931515634059906, 0.9410102963447571, 0.9386019706726074, 0.93106609582901, 0.9439907073974609, 0.9422751069068909, 0.9314717054367065, 0.9402040243148804, 0.9419682621955872, 0.9497430920600891, 0.9400821328163147, 0.9318418502807617, 0.9368599653244019, 0.9430208206176758, 0.9350554943084717]
[2025-05-13 12:37:07,561]: Mean: 0.93885660
[2025-05-13 12:37:07,561]: Min: 0.92228431
[2025-05-13 12:37:07,561]: Max: 0.95246655
[2025-05-13 12:37:07,562]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 12:37:07,568]: Sample Values (25 elements): [-0.014160127379000187, 0.0, 0.0, -0.014160127379000187, 0.014160127379000187, 0.0, -0.028320254758000374, 0.014160127379000187, 0.014160127379000187, 0.014160127379000187, 0.0, 0.0, -0.014160127379000187, 0.014160127379000187, 0.0, -0.028320254758000374, 0.028320254758000374, 0.0, -0.014160127379000187, -0.014160127379000187, 0.0, 0.0, 0.0, -0.014160127379000187, 0.0]
[2025-05-13 12:37:07,568]: Mean: -0.00039106
[2025-05-13 12:37:07,568]: Min: -0.05664051
[2025-05-13 12:37:07,568]: Max: 0.04248038
[2025-05-13 12:37:07,568]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-13 12:37:07,569]: Sample Values (25 elements): [0.955988347530365, 0.9664778709411621, 0.9630864858627319, 0.9536889791488647, 0.9668909907341003, 0.9592486023902893, 0.9690151810646057, 0.9616864919662476, 0.9588147401809692, 0.9611810445785522, 0.9606516361236572, 0.9563183188438416, 0.9702675342559814, 0.9657135009765625, 0.962215006351471, 0.9624814391136169, 0.9601529240608215, 0.9578453302383423, 0.983010470867157, 0.9618867039680481, 0.9557256698608398, 0.9652507901191711, 0.9645067453384399, 0.9643360376358032, 0.9576343297958374]
[2025-05-13 12:37:07,569]: Mean: 0.96337038
[2025-05-13 12:37:07,569]: Min: 0.95012200
[2025-05-13 12:37:07,569]: Max: 0.98301047
[2025-05-13 12:37:07,570]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 12:37:07,577]: Sample Values (25 elements): [0.0, -0.024026911705732346, 0.012013455852866173, -0.012013455852866173, -0.012013455852866173, -0.012013455852866173, 0.012013455852866173, -0.012013455852866173, -0.012013455852866173, 0.012013455852866173, 0.0, 0.0, -0.012013455852866173, 0.0, -0.012013455852866173, 0.012013455852866173, 0.0, 0.0, 0.0, -0.012013455852866173, -0.012013455852866173, -0.012013455852866173, -0.012013455852866173, 0.0, 0.0]
[2025-05-13 12:37:07,577]: Mean: -0.00005357
[2025-05-13 12:37:07,578]: Min: -0.03604037
[2025-05-13 12:37:07,578]: Max: 0.04805382
[2025-05-13 12:37:07,578]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-13 12:37:07,578]: Sample Values (25 elements): [0.9820231199264526, 0.9875875115394592, 0.9854973554611206, 0.9854248762130737, 0.9690178036689758, 0.9885799288749695, 0.9869439005851746, 0.9983543157577515, 0.9756962656974792, 0.9750716090202332, 0.9675835967063904, 0.9884300827980042, 0.974670946598053, 0.9784201383590698, 0.9741728901863098, 0.972723126411438, 0.9863229393959045, 0.9770946502685547, 0.9718077182769775, 0.9853364825248718, 0.9877378344535828, 0.9813307523727417, 0.978310227394104, 0.9864538311958313, 0.9769653677940369]
[2025-05-13 12:37:07,578]: Mean: 0.97941571
[2025-05-13 12:37:07,579]: Min: 0.96355230
[2025-05-13 12:37:07,579]: Max: 1.00930870
[2025-05-13 12:37:07,580]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-13 12:37:07,593]: Sample Values (25 elements): [0.011718141846358776, 0.0, 0.0, 0.011718141846358776, 0.0, -0.023436283692717552, -0.023436283692717552, 0.011718141846358776, -0.011718141846358776, 0.023436283692717552, 0.011718141846358776, 0.011718141846358776, 0.0, -0.011718141846358776, -0.011718141846358776, 0.011718141846358776, 0.0, 0.011718141846358776, 0.011718141846358776, 0.0, 0.0, 0.0, 0.011718141846358776, 0.011718141846358776, 0.011718141846358776]
[2025-05-13 12:37:07,593]: Mean: -0.00002166
[2025-05-13 12:37:07,594]: Min: -0.03515442
[2025-05-13 12:37:07,594]: Max: 0.04687257
[2025-05-13 12:37:07,594]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-13 12:37:07,594]: Sample Values (25 elements): [0.9618192315101624, 0.9608601927757263, 0.9624402523040771, 0.9629741311073303, 0.966325581073761, 0.9610608220100403, 0.9603413939476013, 0.9624574780464172, 0.9626047611236572, 0.9595575332641602, 0.9591392278671265, 0.9564699530601501, 0.9603869915008545, 0.9581894874572754, 0.9623479247093201, 0.9600126147270203, 0.9594043493270874, 0.9604983925819397, 0.9568678736686707, 0.9612336158752441, 0.9622938632965088, 0.9583432078361511, 0.9636471271514893, 0.9591021537780762, 0.9644780158996582]
[2025-05-13 12:37:07,594]: Mean: 0.96080256
[2025-05-13 12:37:07,595]: Min: 0.95454282
[2025-05-13 12:37:07,595]: Max: 0.97102904
[2025-05-13 12:37:07,596]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 12:37:07,637]: Sample Values (25 elements): [-0.008279417641460896, 0.0, -0.008279417641460896, 0.0, -0.008279417641460896, -0.008279417641460896, 0.008279417641460896, -0.008279417641460896, 0.0, 0.008279417641460896, 0.008279417641460896, -0.008279417641460896, 0.0, 0.008279417641460896, 0.0, 0.0, 0.0, -0.008279417641460896, 0.008279417641460896, 0.0, -0.01655883528292179, 0.0, 0.0, 0.0, 0.0]
[2025-05-13 12:37:07,638]: Mean: -0.00003649
[2025-05-13 12:37:07,638]: Min: -0.02483825
[2025-05-13 12:37:07,638]: Max: 0.03311767
[2025-05-13 12:37:07,638]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-13 12:37:07,640]: Sample Values (25 elements): [0.9689887762069702, 0.969657838344574, 0.9651319980621338, 0.9656543731689453, 0.9639167785644531, 0.9712173938751221, 0.9705004096031189, 0.9639978408813477, 0.9653866291046143, 0.9648921489715576, 0.9668586850166321, 0.9675901532173157, 0.9676662683486938, 0.9718883037567139, 0.9696370959281921, 0.9662667512893677, 0.969610333442688, 0.9609999060630798, 0.9624719619750977, 0.9675755500793457, 0.9721783399581909, 0.9666467308998108, 0.9679611921310425, 0.9642408490180969, 0.9711273312568665]
[2025-05-13 12:37:07,640]: Mean: 0.96611547
[2025-05-13 12:37:07,640]: Min: 0.95904171
[2025-05-13 12:37:07,640]: Max: 0.98103333
[2025-05-13 12:37:07,641]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-13 12:37:07,643]: Sample Values (25 elements): [-0.042574409395456314, 0.0, -0.042574409395456314, -0.021287204697728157, -0.021287204697728157, -0.042574409395456314, -0.042574409395456314, 0.021287204697728157, 0.0, -0.021287204697728157, 0.0, 0.042574409395456314, 0.0, 0.0, 0.0, 0.021287204697728157, 0.042574409395456314, 0.042574409395456314, 0.0, 0.0, 0.042574409395456314, 0.0, 0.042574409395456314, 0.042574409395456314, 0.042574409395456314]
[2025-05-13 12:37:07,643]: Mean: 0.00007211
[2025-05-13 12:37:07,643]: Min: -0.06386162
[2025-05-13 12:37:07,643]: Max: 0.08514882
[2025-05-13 12:37:07,643]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-13 12:37:07,643]: Sample Values (25 elements): [0.9551758766174316, 0.9565529823303223, 0.9560210108757019, 0.9572943449020386, 0.955208957195282, 0.9544116854667664, 0.9581857919692993, 0.9568614363670349, 0.9580654501914978, 0.9570205807685852, 0.9560371041297913, 0.953820526599884, 0.9548922777175903, 0.9540370106697083, 0.9560471177101135, 0.9536668658256531, 0.9584259986877441, 0.9577048420906067, 0.954979658126831, 0.9557360410690308, 0.9558521509170532, 0.961974024772644, 0.9562294483184814, 0.9545360803604126, 0.953924834728241]
[2025-05-13 12:37:07,644]: Mean: 0.95588970
[2025-05-13 12:37:07,644]: Min: 0.94999713
[2025-05-13 12:37:07,644]: Max: 0.96197402
[2025-05-13 12:37:07,645]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 12:37:07,681]: Sample Values (25 elements): [0.008099302649497986, 0.0, 0.008099302649497986, -0.008099302649497986, -0.008099302649497986, 0.008099302649497986, -0.008099302649497986, 0.0, 0.0, -0.008099302649497986, 0.0, 0.0, -0.008099302649497986, -0.008099302649497986, -0.008099302649497986, -0.008099302649497986, 0.008099302649497986, -0.008099302649497986, 0.0, 0.0, 0.0, 0.008099302649497986, -0.008099302649497986, -0.008099302649497986, -0.008099302649497986]
[2025-05-13 12:37:07,681]: Mean: -0.00013657
[2025-05-13 12:37:07,681]: Min: -0.02429791
[2025-05-13 12:37:07,681]: Max: 0.03239721
[2025-05-13 12:37:07,681]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-13 12:37:07,682]: Sample Values (25 elements): [0.959184467792511, 0.9608020782470703, 0.9555701613426208, 0.9589952230453491, 0.95832759141922, 0.9573682546615601, 0.958370566368103, 0.9633058309555054, 0.9578502178192139, 0.9607683420181274, 0.9570239782333374, 0.9563892483711243, 0.9651616215705872, 0.9655253291130066, 0.9590032696723938, 0.9568259716033936, 0.9566552639007568, 0.9590879082679749, 0.9590246081352234, 0.9607297778129578, 0.9577034115791321, 0.9589325189590454, 0.960048496723175, 0.9621041417121887, 0.9578004479408264]
[2025-05-13 12:37:07,682]: Mean: 0.95940411
[2025-05-13 12:37:07,682]: Min: 0.95455843
[2025-05-13 12:37:07,682]: Max: 0.97545183
[2025-05-13 12:37:07,683]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 12:37:07,716]: Sample Values (25 elements): [-0.006072415038943291, 0.0, -0.006072415038943291, 0.012144830077886581, 0.012144830077886581, -0.012144830077886581, 0.0, 0.0, -0.006072415038943291, 0.0, -0.006072415038943291, -0.012144830077886581, 0.006072415038943291, 0.006072415038943291, 0.006072415038943291, 0.006072415038943291, 0.012144830077886581, 0.012144830077886581, -0.006072415038943291, 0.006072415038943291, 0.012144830077886581, 0.006072415038943291, 0.0, 0.006072415038943291, -0.012144830077886581]
[2025-05-13 12:37:07,717]: Mean: 0.00004543
[2025-05-13 12:37:07,717]: Min: -0.01821725
[2025-05-13 12:37:07,717]: Max: 0.02428966
[2025-05-13 12:37:07,717]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-13 12:37:07,718]: Sample Values (25 elements): [0.9642995595932007, 0.9691648483276367, 0.9638850092887878, 0.9646663665771484, 0.961609959602356, 0.9708383679389954, 0.9663055539131165, 0.9676021933555603, 0.9687857031822205, 0.9674387574195862, 0.9665183424949646, 0.9676013588905334, 0.9649145603179932, 0.963535487651825, 0.966792643070221, 0.9623027443885803, 0.966891348361969, 0.9682505130767822, 0.9686902165412903, 0.9652258157730103, 0.9649248123168945, 0.96296226978302, 0.9650310277938843, 0.9710438847541809, 0.9673408269882202]
[2025-05-13 12:37:07,718]: Mean: 0.96747124
[2025-05-13 12:37:07,718]: Min: 0.96110696
[2025-05-13 12:37:07,718]: Max: 0.97740120
[2025-05-13 12:37:07,718]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-13 12:37:07,718]: Sample Values (25 elements): [-0.08093808591365814, 0.0038975428324192762, -0.03952806815505028, 0.050261352211236954, -0.0018097009742632508, -0.0030263818334788084, 0.11130934953689575, 0.030078617855906487, -0.012367491610348225, 0.049862977117300034, -0.052816227078437805, -0.031079312786459923, -0.07847435772418976, 0.09452745318412781, -0.0784376859664917, -0.024936512112617493, -0.04535240679979324, 0.017093632370233536, 0.03595850244164467, -0.041052207350730896, 0.021599510684609413, -0.016499953344464302, -0.015789669007062912, 0.047168854624032974, -0.0008782487129792571]
[2025-05-13 12:37:07,719]: Mean: 0.00046919
[2025-05-13 12:37:07,719]: Min: -0.11522763
[2025-05-13 12:37:07,719]: Max: 0.13782059
[2025-05-13 12:37:07,719]: 


[2025-05-13 19:46:32,116]: 


QAT of ResNet18 with relu down to 2 bits...
[2025-05-13 19:46:32,473]: [ResNet18_relu_quantized_2_bits] after configure_qat:
[2025-05-13 19:46:32,616]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 19:48:15,352]: [ResNet18_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.2965 Train Acc: 0.1592 Eval Loss: 2.1117 Eval Acc: 0.2442 (LR: 0.000100)
[2025-05-13 19:49:58,971]: [ResNet18_relu_quantized_2_bits] Epoch: 002 Train Loss: 2.0637 Train Acc: 0.2657 Eval Loss: 2.0432 Eval Acc: 0.3086 (LR: 0.000100)
[2025-05-13 19:51:38,662]: [ResNet18_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.9784 Train Acc: 0.3006 Eval Loss: 1.9555 Eval Acc: 0.3166 (LR: 0.000100)
[2025-05-13 19:53:19,083]: [ResNet18_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.9655 Train Acc: 0.2923 Eval Loss: 1.9509 Eval Acc: 0.3077 (LR: 0.000100)
[2025-05-13 19:54:56,652]: [ResNet18_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.9894 Train Acc: 0.2723 Eval Loss: 2.1177 Eval Acc: 0.2669 (LR: 0.000100)
[2025-05-13 19:56:34,335]: [ResNet18_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.9323 Train Acc: 0.2833 Eval Loss: 1.8630 Eval Acc: 0.3217 (LR: 0.000100)
[2025-05-13 19:58:14,947]: [ResNet18_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.8628 Train Acc: 0.3089 Eval Loss: 1.7574 Eval Acc: 0.3463 (LR: 0.000100)
[2025-05-13 20:00:03,430]: [ResNet18_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.7971 Train Acc: 0.3269 Eval Loss: 1.7495 Eval Acc: 0.3588 (LR: 0.000100)
[2025-05-13 20:01:55,867]: [ResNet18_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.7650 Train Acc: 0.3421 Eval Loss: 1.7151 Eval Acc: 0.3605 (LR: 0.000100)
[2025-05-13 20:03:47,612]: [ResNet18_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.7384 Train Acc: 0.3497 Eval Loss: 1.6724 Eval Acc: 0.3797 (LR: 0.000100)
[2025-05-13 20:05:30,649]: [ResNet18_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.7243 Train Acc: 0.3532 Eval Loss: 1.6607 Eval Acc: 0.3741 (LR: 0.000100)
[2025-05-13 20:07:12,934]: [ResNet18_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.7010 Train Acc: 0.3621 Eval Loss: 1.6969 Eval Acc: 0.3766 (LR: 0.000100)
[2025-05-13 20:08:55,976]: [ResNet18_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.6883 Train Acc: 0.3695 Eval Loss: 1.6166 Eval Acc: 0.3940 (LR: 0.000100)
[2025-05-13 20:10:39,156]: [ResNet18_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.6593 Train Acc: 0.3809 Eval Loss: 1.6212 Eval Acc: 0.3980 (LR: 0.000100)
[2025-05-13 20:12:20,788]: [ResNet18_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.6490 Train Acc: 0.3822 Eval Loss: 1.6103 Eval Acc: 0.4067 (LR: 0.000100)
[2025-05-13 20:14:08,612]: [ResNet18_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.6417 Train Acc: 0.3870 Eval Loss: 1.5896 Eval Acc: 0.4016 (LR: 0.000100)
[2025-05-13 20:15:59,107]: [ResNet18_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.6305 Train Acc: 0.3939 Eval Loss: 1.5682 Eval Acc: 0.4220 (LR: 0.000100)
[2025-05-13 20:17:45,512]: [ResNet18_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.6249 Train Acc: 0.3972 Eval Loss: 1.5851 Eval Acc: 0.4094 (LR: 0.000100)
[2025-05-13 20:19:42,073]: [ResNet18_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.6259 Train Acc: 0.3964 Eval Loss: 1.5665 Eval Acc: 0.4197 (LR: 0.000100)
[2025-05-13 20:21:30,012]: [ResNet18_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.6264 Train Acc: 0.3955 Eval Loss: 1.6010 Eval Acc: 0.4034 (LR: 0.000100)
[2025-05-13 20:23:15,858]: [ResNet18_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.6109 Train Acc: 0.4022 Eval Loss: 1.5786 Eval Acc: 0.4165 (LR: 0.000100)
[2025-05-13 20:25:02,738]: [ResNet18_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.5958 Train Acc: 0.4074 Eval Loss: 1.5901 Eval Acc: 0.4186 (LR: 0.000100)
[2025-05-13 20:26:46,921]: [ResNet18_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.5982 Train Acc: 0.4049 Eval Loss: 1.5597 Eval Acc: 0.4191 (LR: 0.000100)
[2025-05-13 20:28:29,898]: [ResNet18_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.5874 Train Acc: 0.4098 Eval Loss: 1.5327 Eval Acc: 0.4367 (LR: 0.000100)
[2025-05-13 20:30:19,438]: [ResNet18_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.5857 Train Acc: 0.4105 Eval Loss: 1.5414 Eval Acc: 0.4271 (LR: 0.000100)
[2025-05-13 20:32:07,153]: [ResNet18_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.5838 Train Acc: 0.4113 Eval Loss: 1.5324 Eval Acc: 0.4405 (LR: 0.000100)
[2025-05-13 20:33:54,946]: [ResNet18_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.5800 Train Acc: 0.4134 Eval Loss: 1.5430 Eval Acc: 0.4315 (LR: 0.000100)
[2025-05-13 20:35:36,737]: [ResNet18_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.6066 Train Acc: 0.4068 Eval Loss: 1.5667 Eval Acc: 0.4260 (LR: 0.000100)
[2025-05-13 20:37:18,536]: [ResNet18_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.5799 Train Acc: 0.4117 Eval Loss: 1.5508 Eval Acc: 0.4256 (LR: 0.000100)
[2025-05-13 20:39:02,736]: [ResNet18_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.5860 Train Acc: 0.4072 Eval Loss: 1.5496 Eval Acc: 0.4302 (LR: 0.000025)
[2025-05-13 20:40:42,007]: [ResNet18_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.5576 Train Acc: 0.4277 Eval Loss: 1.5519 Eval Acc: 0.4226 (LR: 0.000025)
[2025-05-13 20:42:21,267]: [ResNet18_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.5677 Train Acc: 0.4180 Eval Loss: 1.5535 Eval Acc: 0.4378 (LR: 0.000025)
[2025-05-13 20:44:03,732]: [ResNet18_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.5549 Train Acc: 0.4222 Eval Loss: 1.5548 Eval Acc: 0.4326 (LR: 0.000025)
[2025-05-13 20:45:48,838]: [ResNet18_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.5408 Train Acc: 0.4315 Eval Loss: 1.5452 Eval Acc: 0.4303 (LR: 0.000025)
[2025-05-13 20:47:34,448]: [ResNet18_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.5646 Train Acc: 0.4224 Eval Loss: 1.5310 Eval Acc: 0.4290 (LR: 0.000025)
[2025-05-13 20:49:19,527]: [ResNet18_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.5736 Train Acc: 0.4164 Eval Loss: 1.5094 Eval Acc: 0.4436 (LR: 0.000025)
[2025-05-13 20:51:04,900]: [ResNet18_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.5855 Train Acc: 0.4121 Eval Loss: 1.5963 Eval Acc: 0.4173 (LR: 0.000025)
[2025-05-13 20:52:50,421]: [ResNet18_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.6611 Train Acc: 0.3870 Eval Loss: 1.6280 Eval Acc: 0.4081 (LR: 0.000025)
[2025-05-13 20:54:35,510]: [ResNet18_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.6004 Train Acc: 0.4127 Eval Loss: 1.5646 Eval Acc: 0.4314 (LR: 0.000025)
[2025-05-13 20:56:25,510]: [ResNet18_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.5854 Train Acc: 0.4182 Eval Loss: 1.6103 Eval Acc: 0.4096 (LR: 0.000025)
[2025-05-13 20:58:34,292]: [ResNet18_relu_quantized_2_bits] Epoch: 041 Train Loss: 1.6281 Train Acc: 0.3978 Eval Loss: 1.5869 Eval Acc: 0.4165 (LR: 0.000025)
[2025-05-13 21:00:43,944]: [ResNet18_relu_quantized_2_bits] Epoch: 042 Train Loss: 1.5913 Train Acc: 0.4113 Eval Loss: 1.5503 Eval Acc: 0.4189 (LR: 0.000025)
[2025-05-13 21:02:54,277]: [ResNet18_relu_quantized_2_bits] Epoch: 043 Train Loss: 1.5804 Train Acc: 0.4162 Eval Loss: 1.5836 Eval Acc: 0.4179 (LR: 0.000025)
[2025-05-13 21:05:03,252]: [ResNet18_relu_quantized_2_bits] Epoch: 044 Train Loss: 1.5780 Train Acc: 0.4180 Eval Loss: 1.5648 Eval Acc: 0.4251 (LR: 0.000025)
[2025-05-13 21:07:11,928]: [ResNet18_relu_quantized_2_bits] Epoch: 045 Train Loss: 1.5772 Train Acc: 0.4159 Eval Loss: 1.5037 Eval Acc: 0.4421 (LR: 0.000006)
[2025-05-13 21:09:20,514]: [ResNet18_relu_quantized_2_bits] Epoch: 046 Train Loss: 1.5817 Train Acc: 0.4167 Eval Loss: 1.6698 Eval Acc: 0.3825 (LR: 0.000006)
[2025-05-13 21:11:29,510]: [ResNet18_relu_quantized_2_bits] Epoch: 047 Train Loss: 1.5973 Train Acc: 0.4065 Eval Loss: 1.6638 Eval Acc: 0.3889 (LR: 0.000006)
[2025-05-13 21:13:38,531]: [ResNet18_relu_quantized_2_bits] Epoch: 048 Train Loss: 1.6737 Train Acc: 0.3826 Eval Loss: 1.5842 Eval Acc: 0.4170 (LR: 0.000006)
[2025-05-13 21:15:47,574]: [ResNet18_relu_quantized_2_bits] Epoch: 049 Train Loss: 1.5908 Train Acc: 0.4180 Eval Loss: 1.5318 Eval Acc: 0.4376 (LR: 0.000006)
[2025-05-13 21:17:56,790]: [ResNet18_relu_quantized_2_bits] Epoch: 050 Train Loss: 1.5803 Train Acc: 0.4196 Eval Loss: 1.5889 Eval Acc: 0.4110 (LR: 0.000006)
[2025-05-13 21:20:05,829]: [ResNet18_relu_quantized_2_bits] Epoch: 051 Train Loss: 1.5741 Train Acc: 0.4187 Eval Loss: 1.5332 Eval Acc: 0.4371 (LR: 0.000006)
[2025-05-13 21:22:15,130]: [ResNet18_relu_quantized_2_bits] Epoch: 052 Train Loss: 1.5738 Train Acc: 0.4197 Eval Loss: 1.5430 Eval Acc: 0.4370 (LR: 0.000006)
[2025-05-13 21:24:23,967]: [ResNet18_relu_quantized_2_bits] Epoch: 053 Train Loss: 1.5662 Train Acc: 0.4197 Eval Loss: 1.5425 Eval Acc: 0.4346 (LR: 0.000006)
[2025-05-13 21:26:33,058]: [ResNet18_relu_quantized_2_bits] Epoch: 054 Train Loss: 1.5631 Train Acc: 0.4274 Eval Loss: 1.5424 Eval Acc: 0.4305 (LR: 0.000006)
[2025-05-13 21:28:25,481]: [ResNet18_relu_quantized_2_bits] Epoch: 055 Train Loss: 1.5665 Train Acc: 0.4206 Eval Loss: 1.5388 Eval Acc: 0.4307 (LR: 0.000006)
[2025-05-13 21:30:13,551]: [ResNet18_relu_quantized_2_bits] Epoch: 056 Train Loss: 1.5679 Train Acc: 0.4203 Eval Loss: 1.5330 Eval Acc: 0.4347 (LR: 0.000006)
[2025-05-13 21:31:59,185]: [ResNet18_relu_quantized_2_bits] Epoch: 057 Train Loss: 1.5664 Train Acc: 0.4214 Eval Loss: 1.5450 Eval Acc: 0.4298 (LR: 0.000006)
[2025-05-13 21:34:08,196]: [ResNet18_relu_quantized_2_bits] Epoch: 058 Train Loss: 1.5762 Train Acc: 0.4153 Eval Loss: 1.5632 Eval Acc: 0.4193 (LR: 0.000006)
[2025-05-13 21:36:17,226]: [ResNet18_relu_quantized_2_bits] Epoch: 059 Train Loss: 1.5983 Train Acc: 0.4074 Eval Loss: 1.5723 Eval Acc: 0.4159 (LR: 0.000006)
[2025-05-13 21:38:08,859]: [ResNet18_relu_quantized_2_bits] Epoch: 060 Train Loss: 1.6092 Train Acc: 0.4029 Eval Loss: 1.5769 Eval Acc: 0.4168 (LR: 0.000006)
[2025-05-13 21:38:08,859]: [ResNet18_relu_quantized_2_bits] Best Eval Accuracy: 0.4436
[2025-05-13 21:38:08,944]: 


Quantization of model down to 2 bits finished
[2025-05-13 21:38:08,944]: Model Architecture:
[2025-05-13 21:38:09,006]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.3349], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.004724502563477)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1274], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19282746315002441, max_val=0.18952029943466187)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7805], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.34142017364502)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0928], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10760229080915451, max_val=0.17071042954921722)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.8812], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.643600463867188)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10147371143102646, max_val=0.09866771847009659)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5873], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.761813163757324)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0610], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08447887003421783, max_val=0.0983930453658104)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.3857], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=19.15716552734375)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0546], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08199954777956009, max_val=0.08178974688053131)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.6908], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.07253646850586)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0430], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06191272288560867, max_val=0.06711193174123764)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1252], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15495333075523376, max_val=0.220602348446846)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.8453], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=11.535757064819336)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0413], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05706615000963211, max_val=0.06690815091133118)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2245], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.673478126525879)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0408], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05706540867686272, max_val=0.06543274223804474)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.0921], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.276219367980957)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0345], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.049216531217098236, max_val=0.05441475659608841)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3568], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.0703654289245605)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0278], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04033299908041954, max_val=0.04309096559882164)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0726], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1066223606467247, max_val=0.11106648296117783)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.6312], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.893659591674805)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0284], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0432480052113533, max_val=0.042031046003103256)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1248], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.374423027038574)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0260], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03550814464688301, max_val=0.04248757287859917)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.6443], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.93282413482666)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0253], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03681797906756401, max_val=0.03913940489292145)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3120], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.935901641845703)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0177], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.025362728163599968, max_val=0.027668794617056847)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0478], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07024671882390976, max_val=0.07313032448291779)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.5737], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.720991134643555)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0157], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02296748012304306, max_val=0.024046873673796654)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1431], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.429289817810059)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.01988777332007885, max_val=0.021782856434583664)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.1305], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.39138412475586)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-13 21:38:09,006]: 
Model Weights:
[2025-05-13 21:38:09,006]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-13 21:38:09,013]: Sample Values (25 elements): [-0.20182721316814423, 0.09869883954524994, -0.24267089366912842, 0.035123344510793686, -0.09391052275896072, 0.0016661701956763864, -0.10373613983392715, -0.2504274845123291, 0.014314036816358566, 0.1317576766014099, -0.1996660977602005, -0.020706677809357643, -0.20883525907993317, 0.02524549886584282, -0.022387947887182236, 0.1342548429965973, -0.21951043605804443, 0.14380107820034027, 0.10607130080461502, 0.03137568384408951, -0.031416550278663635, -0.04356350004673004, -0.20807236433029175, 0.010680953040719032, 0.10950427502393723]
[2025-05-13 21:38:09,025]: Mean: 0.00173460
[2025-05-13 21:38:09,025]: Min: -0.45711052
[2025-05-13 21:38:09,026]: Max: 0.40790057
[2025-05-13 21:38:09,026]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-13 21:38:09,026]: Sample Values (25 elements): [1.183201551437378, 1.052130937576294, 1.0259459018707275, 1.3340519666671753, 1.055787444114685, 0.9884955883026123, 0.912588357925415, 1.0007623434066772, 0.9393450617790222, 1.493064522743225, 0.9924324154853821, 0.9558732509613037, 1.2184514999389648, 0.8864284753799438, 1.1571626663208008, 1.1893683671951294, 0.841101884841919, 1.033052921295166, 1.8112596273422241, 1.0340070724487305, 1.1520633697509766, 1.027400016784668, 1.3729195594787598, 1.0507380962371826, 1.2977739572525024]
[2025-05-13 21:38:09,027]: Mean: 1.17930961
[2025-05-13 21:38:09,027]: Min: 0.84110188
[2025-05-13 21:38:09,027]: Max: 2.39881968
[2025-05-13 21:38:09,028]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 21:38:09,029]: Sample Values (25 elements): [0.1274464875459671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1274464875459671, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,029]: Mean: -0.00078133
[2025-05-13 21:38:09,029]: Min: -0.25489298
[2025-05-13 21:38:09,029]: Max: 0.12744649
[2025-05-13 21:38:09,029]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 21:38:09,030]: Sample Values (25 elements): [1.0420758724212646, 0.9862837195396423, 0.9990563988685608, 1.1344963312149048, 1.0032401084899902, 0.9575755596160889, 1.1025769710540771, 1.0939531326293945, 1.000948429107666, 1.0264686346054077, 0.9911890029907227, 1.0433095693588257, 0.9660807251930237, 0.9112379550933838, 0.9908942580223083, 0.979136049747467, 1.0311360359191895, 1.0295166969299316, 0.9831975102424622, 1.0448225736618042, 0.9555150866508484, 0.9688227772712708, 1.0137773752212524, 1.1501020193099976, 0.971027135848999]
[2025-05-13 21:38:09,030]: Mean: 1.02544260
[2025-05-13 21:38:09,030]: Min: 0.91123796
[2025-05-13 21:38:09,030]: Max: 1.25456846
[2025-05-13 21:38:09,031]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 21:38:09,032]: Sample Values (25 elements): [0.0, 0.0, 0.09277044236660004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09277044236660004, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09277044236660004, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,032]: Mean: -0.00102927
[2025-05-13 21:38:09,032]: Min: -0.09277044
[2025-05-13 21:38:09,032]: Max: 0.18554088
[2025-05-13 21:38:09,032]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 21:38:09,033]: Sample Values (25 elements): [1.0179005861282349, 0.9901527762413025, 0.9699838757514954, 0.983366072177887, 1.472524642944336, 1.0100727081298828, 1.042994737625122, 1.0127922296524048, 1.0619004964828491, 1.1788095235824585, 1.0495229959487915, 0.971679151058197, 0.9804838299751282, 1.0028952360153198, 1.0698975324630737, 1.0638045072555542, 1.2177625894546509, 1.0013362169265747, 1.0943619012832642, 1.1602846384048462, 0.9747981429100037, 0.998090922832489, 1.172370433807373, 0.9853824973106384, 1.6817989349365234]
[2025-05-13 21:38:09,033]: Mean: 1.07000113
[2025-05-13 21:38:09,033]: Min: 0.96998388
[2025-05-13 21:38:09,033]: Max: 1.68179893
[2025-05-13 21:38:09,034]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 21:38:09,035]: Sample Values (25 elements): [0.0, 0.0, -0.06671398133039474, 0.0, 0.0, 0.06671398133039474, 0.0, 0.0, 0.06671398133039474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06671398133039474, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06671398133039474, -0.06671398133039474, 0.0]
[2025-05-13 21:38:09,035]: Mean: -0.00281594
[2025-05-13 21:38:09,035]: Min: -0.13342796
[2025-05-13 21:38:09,035]: Max: 0.06671398
[2025-05-13 21:38:09,035]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 21:38:09,035]: Sample Values (25 elements): [0.9838373064994812, 0.9589911699295044, 1.005373239517212, 1.047448992729187, 1.0970757007598877, 1.036453127861023, 0.9948447942733765, 1.017440676689148, 1.0120774507522583, 0.9947481155395508, 1.1369966268539429, 0.9984583258628845, 1.0134496688842773, 0.9621385931968689, 1.000824213027954, 1.0188140869140625, 1.0014708042144775, 1.0012623071670532, 1.0061863660812378, 1.014070987701416, 0.956608235836029, 1.1489086151123047, 0.9755297303199768, 1.0419485569000244, 0.9647337198257446]
[2025-05-13 21:38:09,036]: Mean: 1.00587821
[2025-05-13 21:38:09,036]: Min: 0.95660824
[2025-05-13 21:38:09,036]: Max: 1.14890862
[2025-05-13 21:38:09,037]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 21:38:09,037]: Sample Values (25 elements): [0.0, 0.0, -0.06095730513334274, 0.0, 0.0, 0.0, 0.0, 0.06095730513334274, 0.0, 0.0, 0.0, -0.06095730513334274, 0.0, 0.06095730513334274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,038]: Mean: -0.00098553
[2025-05-13 21:38:09,038]: Min: -0.06095731
[2025-05-13 21:38:09,038]: Max: 0.12191461
[2025-05-13 21:38:09,038]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 21:38:09,038]: Sample Values (25 elements): [1.0248322486877441, 1.0220848321914673, 1.0219568014144897, 1.0516705513000488, 1.020046591758728, 1.0184779167175293, 1.0150871276855469, 1.0050225257873535, 1.0056695938110352, 1.040593147277832, 1.0664969682693481, 1.0658388137817383, 1.0457121133804321, 1.0174161195755005, 1.0155240297317505, 0.9887351989746094, 1.0413089990615845, 0.9981565475463867, 0.9963316321372986, 1.0086880922317505, 1.0406110286712646, 1.001878023147583, 1.0880944728851318, 1.023686170578003, 0.9984331727027893]
[2025-05-13 21:38:09,038]: Mean: 1.02362132
[2025-05-13 21:38:09,038]: Min: 0.97583681
[2025-05-13 21:38:09,039]: Max: 1.24230254
[2025-05-13 21:38:09,040]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-13 21:38:09,040]: Sample Values (25 elements): [0.05459652096033096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05459652096033096, 0.0, 0.0, 0.05459652096033096, 0.0, -0.05459652096033096, -0.05459652096033096, 0.0, 0.0, 0.0, 0.05459652096033096, 0.0, 0.0, -0.05459652096033096, -0.05459652096033096, 0.0]
[2025-05-13 21:38:09,041]: Mean: -0.00194977
[2025-05-13 21:38:09,041]: Min: -0.10919304
[2025-05-13 21:38:09,041]: Max: 0.05459652
[2025-05-13 21:38:09,041]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-13 21:38:09,041]: Sample Values (25 elements): [1.0254992246627808, 0.9815943241119385, 0.9981110692024231, 0.968606173992157, 0.9786705374717712, 0.9910457134246826, 0.976970374584198, 0.9718009233474731, 0.9824144244194031, 1.0024298429489136, 0.9841593503952026, 0.9660506248474121, 0.9793980717658997, 0.9967812299728394, 1.0027064085006714, 0.9791140556335449, 0.9885976314544678, 0.9912461638450623, 0.9893749952316284, 1.0066808462142944, 0.9786879420280457, 0.971848726272583, 0.9944708943367004, 0.9607023000717163, 0.982159435749054]
[2025-05-13 21:38:09,041]: Mean: 0.98484236
[2025-05-13 21:38:09,041]: Min: 0.95886624
[2025-05-13 21:38:09,042]: Max: 1.02549922
[2025-05-13 21:38:09,043]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 21:38:09,045]: Sample Values (25 elements): [0.0, -0.043008219450712204, -0.043008219450712204, 0.0, 0.0, 0.0, 0.0, 0.0, -0.043008219450712204, 0.0, 0.0, 0.043008219450712204, 0.0, -0.043008219450712204, 0.0, 0.0, -0.043008219450712204, 0.0, -0.043008219450712204, 0.043008219450712204, 0.0, 0.0, -0.043008219450712204, 0.0, -0.043008219450712204]
[2025-05-13 21:38:09,045]: Mean: -0.00137113
[2025-05-13 21:38:09,045]: Min: -0.04300822
[2025-05-13 21:38:09,045]: Max: 0.08601644
[2025-05-13 21:38:09,046]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-13 21:38:09,046]: Sample Values (25 elements): [0.9863162636756897, 0.9853055477142334, 0.9792278409004211, 0.9719372987747192, 0.9847452640533447, 0.9887651801109314, 0.9919047951698303, 0.9832003712654114, 0.9891654849052429, 0.975030779838562, 0.9723093509674072, 0.9786110520362854, 1.020566463470459, 0.9586878418922424, 0.988623857498169, 0.9820880889892578, 0.9949540495872498, 0.972545325756073, 0.9739094972610474, 0.9956521987915039, 0.9794567227363586, 0.9686242341995239, 0.9793858528137207, 0.9756959080696106, 0.9779268503189087]
[2025-05-13 21:38:09,046]: Mean: 0.98474991
[2025-05-13 21:38:09,046]: Min: 0.95868784
[2025-05-13 21:38:09,046]: Max: 1.02862310
[2025-05-13 21:38:09,048]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-13 21:38:09,048]: Sample Values (25 elements): [0.12518534064292908, -0.12518534064292908, 0.0, 0.12518534064292908, 0.12518534064292908, 0.0, 0.0, 0.12518534064292908, -0.12518534064292908, 0.0, -0.12518534064292908, 0.12518534064292908, 0.0, 0.12518534064292908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12518534064292908, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,048]: Mean: -0.00239918
[2025-05-13 21:38:09,048]: Min: -0.12518534
[2025-05-13 21:38:09,048]: Max: 0.25037068
[2025-05-13 21:38:09,048]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-13 21:38:09,049]: Sample Values (25 elements): [0.948161244392395, 0.9624598622322083, 0.9526069164276123, 0.9540398120880127, 0.9409438371658325, 0.9550291299819946, 0.9612964391708374, 0.9529926180839539, 0.9603984951972961, 0.9620499014854431, 0.9449695348739624, 0.949581503868103, 0.95443195104599, 0.9735931754112244, 0.953578770160675, 0.9476465582847595, 0.9847543239593506, 0.9667593836784363, 0.9481824040412903, 0.9603930711746216, 0.9491633772850037, 0.9667160511016846, 0.9655032157897949, 0.9662213325500488, 0.955398678779602]
[2025-05-13 21:38:09,049]: Mean: 0.95654523
[2025-05-13 21:38:09,049]: Min: 0.93102807
[2025-05-13 21:38:09,049]: Max: 0.98475432
[2025-05-13 21:38:09,051]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 21:38:09,053]: Sample Values (25 elements): [0.0, 0.04132477194070816, 0.0, 0.0, -0.04132477194070816, -0.04132477194070816, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04132477194070816, 0.04132477194070816, 0.04132477194070816, 0.0, 0.0, 0.0, -0.04132477194070816, 0.0, 0.04132477194070816, 0.0, 0.0, 0.0, 0.04132477194070816, 0.0]
[2025-05-13 21:38:09,053]: Mean: -0.00118126
[2025-05-13 21:38:09,053]: Min: -0.04132477
[2025-05-13 21:38:09,053]: Max: 0.08264954
[2025-05-13 21:38:09,053]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-13 21:38:09,054]: Sample Values (25 elements): [0.9770936965942383, 0.9779308438301086, 0.9689313769340515, 0.9772769808769226, 0.9747564196586609, 0.9748833179473877, 0.9704672694206238, 0.9691500067710876, 0.9792962074279785, 0.968272864818573, 0.9842652082443237, 0.9739501476287842, 0.969501793384552, 0.9773552417755127, 0.9777225852012634, 0.9846190214157104, 0.9796615839004517, 0.9672169089317322, 0.9700341820716858, 0.9661033749580383, 0.9706950187683105, 1.0126043558120728, 0.977919340133667, 0.9768350124359131, 0.9674666523933411]
[2025-05-13 21:38:09,054]: Mean: 0.97487271
[2025-05-13 21:38:09,054]: Min: 0.95665962
[2025-05-13 21:38:09,054]: Max: 1.01260436
[2025-05-13 21:38:09,055]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-13 21:38:09,057]: Sample Values (25 elements): [0.04083271324634552, -0.04083271324634552, 0.04083271324634552, 0.0, 0.0, 0.0, 0.0, -0.04083271324634552, 0.04083271324634552, 0.0, 0.0, 0.0, 0.0, -0.04083271324634552, 0.04083271324634552, 0.04083271324634552, -0.04083271324634552, 0.04083271324634552, 0.0, 0.04083271324634552, 0.0, 0.0, -0.04083271324634552, 0.0, 0.0]
[2025-05-13 21:38:09,057]: Mean: -0.00039682
[2025-05-13 21:38:09,057]: Min: -0.04083271
[2025-05-13 21:38:09,057]: Max: 0.08166543
[2025-05-13 21:38:09,058]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-13 21:38:09,058]: Sample Values (25 elements): [1.0170968770980835, 0.9902169108390808, 0.9935475587844849, 1.0085910558700562, 1.0128384828567505, 0.9822238087654114, 0.9981660842895508, 0.9919398427009583, 0.9983940124511719, 1.0183916091918945, 0.9870645403862, 0.9927464127540588, 0.9852502942085266, 0.9894505143165588, 0.9872308373451233, 0.991048276424408, 0.9979889988899231, 0.9897748827934265, 0.9906056523323059, 1.0035759210586548, 0.987338125705719, 1.0026627779006958, 1.0077471733093262, 0.9872468709945679, 0.9903380870819092]
[2025-05-13 21:38:09,058]: Mean: 0.99391526
[2025-05-13 21:38:09,058]: Min: 0.97608066
[2025-05-13 21:38:09,058]: Max: 1.03311455
[2025-05-13 21:38:09,060]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-13 21:38:09,064]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.03454376012086868, 0.0, 0.03454376012086868, 0.03454376012086868, 0.03454376012086868, 0.03454376012086868, 0.0, -0.03454376012086868, 0.03454376012086868, 0.0, -0.03454376012086868, 0.0, 0.0, -0.03454376012086868, -0.03454376012086868, 0.0, -0.03454376012086868, 0.0, 0.03454376012086868, 0.0]
[2025-05-13 21:38:09,064]: Mean: -0.00035210
[2025-05-13 21:38:09,064]: Min: -0.03454376
[2025-05-13 21:38:09,064]: Max: 0.06908752
[2025-05-13 21:38:09,064]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-13 21:38:09,065]: Sample Values (25 elements): [0.9709330201148987, 0.9718044400215149, 0.9663168787956238, 0.9678458571434021, 0.9839445948600769, 0.9767428636550903, 0.9721931219100952, 0.9756395816802979, 0.9635668992996216, 0.9698202610015869, 0.9741126894950867, 0.9726232290267944, 0.9782729744911194, 0.9730122685432434, 0.9705595970153809, 0.9681487083435059, 0.9752785563468933, 0.9691752791404724, 0.974959671497345, 0.9747694134712219, 0.9782233238220215, 0.9784289002418518, 0.988784909248352, 0.9681316614151001, 0.9701135754585266]
[2025-05-13 21:38:09,065]: Mean: 0.97289693
[2025-05-13 21:38:09,065]: Min: 0.95818865
[2025-05-13 21:38:09,066]: Max: 0.98878491
[2025-05-13 21:38:09,067]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 21:38:09,073]: Sample Values (25 elements): [0.0, -0.027807995676994324, 0.0, 0.0, -0.027807995676994324, 0.0, 0.0, 0.0, 0.027807995676994324, 0.0, 0.027807995676994324, 0.0, 0.0, 0.0, 0.027807995676994324, 0.027807995676994324, 0.027807995676994324, 0.0, 0.0, 0.0, -0.027807995676994324, 0.0, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,073]: Mean: -0.00049400
[2025-05-13 21:38:09,073]: Min: -0.02780800
[2025-05-13 21:38:09,073]: Max: 0.05561599
[2025-05-13 21:38:09,073]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-13 21:38:09,074]: Sample Values (25 elements): [0.9766981601715088, 0.9797345399856567, 0.9886072278022766, 0.9798927307128906, 0.978793740272522, 0.9740625023841858, 0.9711562395095825, 0.9738950133323669, 0.9815579056739807, 0.9742164611816406, 0.975731611251831, 0.977429986000061, 0.9745298624038696, 0.9754360318183899, 0.9795746207237244, 0.9885992407798767, 0.9760473370552063, 0.97684645652771, 0.9758885502815247, 0.9791833162307739, 0.9806864261627197, 0.9729701280593872, 0.9826754331588745, 0.9766725301742554, 0.9714374542236328]
[2025-05-13 21:38:09,074]: Mean: 0.97842222
[2025-05-13 21:38:09,074]: Min: 0.96474290
[2025-05-13 21:38:09,074]: Max: 1.00035393
[2025-05-13 21:38:09,075]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-13 21:38:09,076]: Sample Values (25 elements): [-0.07256296277046204, -0.07256296277046204, 0.07256296277046204, -0.07256296277046204, 0.07256296277046204, -0.07256296277046204, 0.07256296277046204, 0.0, 0.07256296277046204, -0.07256296277046204, -0.07256296277046204, -0.07256296277046204, 0.0, -0.07256296277046204, 0.07256296277046204, 0.0, 0.0, 0.07256296277046204, 0.07256296277046204, 0.07256296277046204, -0.07256296277046204, 0.0, -0.07256296277046204, -0.07256296277046204, 0.0]
[2025-05-13 21:38:09,076]: Mean: -0.00049825
[2025-05-13 21:38:09,076]: Min: -0.07256296
[2025-05-13 21:38:09,076]: Max: 0.14512593
[2025-05-13 21:38:09,077]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-13 21:38:09,077]: Sample Values (25 elements): [0.9545647501945496, 0.9556700587272644, 0.9553340673446655, 0.9553762674331665, 0.9636602401733398, 0.9526908993721008, 0.9612987041473389, 0.9599006772041321, 0.9544411897659302, 0.9615471959114075, 0.9624845385551453, 0.9572771787643433, 0.9530030488967896, 0.9545948505401611, 0.9567800760269165, 0.9580615162849426, 0.9517350196838379, 0.9570056796073914, 0.9616622924804688, 0.9609947800636292, 0.9634014964103699, 0.9608583450317383, 0.953768253326416, 0.9533259272575378, 0.9598577618598938]
[2025-05-13 21:38:09,077]: Mean: 0.95802116
[2025-05-13 21:38:09,077]: Min: 0.94634271
[2025-05-13 21:38:09,077]: Max: 0.96973956
[2025-05-13 21:38:09,078]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 21:38:09,084]: Sample Values (25 elements): [0.0, 0.0, 0.02842634543776512, 0.02842634543776512, 0.0, 0.02842634543776512, 0.0, 0.0, 0.0, 0.0, 0.0, -0.02842634543776512, 0.02842634543776512, -0.02842634543776512, 0.02842634543776512, 0.0, 0.0, 0.0, -0.02842634543776512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,085]: Mean: -0.00060720
[2025-05-13 21:38:09,085]: Min: -0.05685269
[2025-05-13 21:38:09,085]: Max: 0.02842635
[2025-05-13 21:38:09,085]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-13 21:38:09,086]: Sample Values (25 elements): [0.9676447510719299, 0.9727643728256226, 0.9690355658531189, 0.966826319694519, 0.9780011773109436, 0.9675114154815674, 0.9715200066566467, 0.9688054919242859, 0.9668443202972412, 0.9736573696136475, 0.9692723751068115, 0.9686198830604553, 0.9739125370979309, 0.9688248038291931, 0.9730108380317688, 0.9699313044548035, 0.9724399447441101, 0.9698689579963684, 0.9695123434066772, 0.9750459790229797, 0.9791709780693054, 0.9677628874778748, 0.9738101363182068, 0.9733085632324219, 0.9703165888786316]
[2025-05-13 21:38:09,086]: Mean: 0.97205663
[2025-05-13 21:38:09,086]: Min: 0.96347427
[2025-05-13 21:38:09,087]: Max: 0.99060470
[2025-05-13 21:38:09,088]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-13 21:38:09,093]: Sample Values (25 elements): [0.02599857561290264, 0.0, 0.02599857561290264, 0.0, 0.0, 0.0, 0.02599857561290264, 0.02599857561290264, 0.02599857561290264, 0.0, 0.02599857561290264, -0.02599857561290264, 0.0, 0.02599857561290264, 0.02599857561290264, 0.02599857561290264, 0.02599857561290264, 0.0, 0.0, -0.02599857561290264, 0.0, 0.0, 0.0, 0.02599857561290264, 0.0]
[2025-05-13 21:38:09,094]: Mean: -0.00009204
[2025-05-13 21:38:09,094]: Min: -0.02599858
[2025-05-13 21:38:09,094]: Max: 0.05199715
[2025-05-13 21:38:09,094]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-13 21:38:09,094]: Sample Values (25 elements): [0.9781399369239807, 0.9770625829696655, 0.980768620967865, 0.9888426065444946, 0.9792400598526001, 0.9816462397575378, 0.9784742593765259, 0.9936795830726624, 0.9864658117294312, 0.9824965000152588, 0.9769964218139648, 0.9778079986572266, 0.9874759316444397, 0.97551029920578, 0.9846449494361877, 0.9861798286437988, 0.981022834777832, 0.9882372617721558, 0.9902986288070679, 0.9852542281150818, 0.9852243065834045, 0.9844192862510681, 0.987919270992279, 0.9817761778831482, 0.9769675731658936]
[2025-05-13 21:38:09,095]: Mean: 0.98281622
[2025-05-13 21:38:09,095]: Min: 0.97117877
[2025-05-13 21:38:09,095]: Max: 1.00928986
[2025-05-13 21:38:09,096]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-13 21:38:09,115]: Sample Values (25 elements): [0.0, 0.02531912922859192, 0.0, 0.0, 0.02531912922859192, 0.02531912922859192, 0.0, 0.0, -0.02531912922859192, 0.0, -0.02531912922859192, 0.0, 0.0, -0.02531912922859192, -0.02531912922859192, 0.0, -0.02531912922859192, 0.0, -0.02531912922859192, 0.0, -0.02531912922859192, 0.02531912922859192, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,115]: Mean: -0.00006349
[2025-05-13 21:38:09,115]: Min: -0.02531913
[2025-05-13 21:38:09,116]: Max: 0.05063826
[2025-05-13 21:38:09,116]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-13 21:38:09,116]: Sample Values (25 elements): [0.970466673374176, 0.9699621200561523, 0.9683712720870972, 0.9693903923034668, 0.9706469178199768, 0.9713776111602783, 0.9703198671340942, 0.969107985496521, 0.9728080630302429, 0.9700145721435547, 0.9728595018386841, 0.9724338054656982, 0.9708067774772644, 0.9703008532524109, 0.9683137536048889, 0.974329948425293, 0.972957193851471, 0.9724363088607788, 0.9720086455345154, 0.9715243577957153, 0.9719988107681274, 0.9718798995018005, 0.971740186214447, 0.9755614995956421, 0.9729869961738586]
[2025-05-13 21:38:09,116]: Mean: 0.97155648
[2025-05-13 21:38:09,116]: Min: 0.96702820
[2025-05-13 21:38:09,116]: Max: 0.97868937
[2025-05-13 21:38:09,117]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 21:38:09,164]: Sample Values (25 elements): [0.0, 0.0, 0.017677173018455505, -0.017677173018455505, 0.017677173018455505, 0.0, 0.017677173018455505, -0.017677173018455505, -0.017677173018455505, 0.0, -0.017677173018455505, 0.0, 0.0, -0.017677173018455505, 0.0, -0.017677173018455505, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017677173018455505, 0.0, 0.0, 0.0]
[2025-05-13 21:38:09,164]: Mean: -0.00009286
[2025-05-13 21:38:09,164]: Min: -0.01767717
[2025-05-13 21:38:09,165]: Max: 0.03535435
[2025-05-13 21:38:09,165]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-13 21:38:09,165]: Sample Values (25 elements): [0.9755127429962158, 0.9745985865592957, 0.9769418239593506, 0.9746246337890625, 0.9731916785240173, 0.9773680567741394, 0.9747210741043091, 0.9736833572387695, 0.9736801981925964, 0.9731377363204956, 0.9771677851676941, 0.9729155898094177, 0.977645754814148, 0.975896954536438, 0.9731112718582153, 0.973933219909668, 0.9767257571220398, 0.9728364944458008, 0.9746232628822327, 0.9725845456123352, 0.976375937461853, 0.9744722247123718, 0.9737598896026611, 0.9750233888626099, 0.9741630554199219]
[2025-05-13 21:38:09,165]: Mean: 0.97502577
[2025-05-13 21:38:09,165]: Min: 0.96989936
[2025-05-13 21:38:09,166]: Max: 0.98252863
[2025-05-13 21:38:09,167]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-13 21:38:09,168]: Sample Values (25 elements): [0.04779237508773804, 0.0, 0.0, 0.0, 0.04779237508773804, -0.04779237508773804, 0.04779237508773804, 0.0, 0.0, 0.04779237508773804, 0.0, 0.04779237508773804, 0.0, -0.04779237508773804, 0.04779237508773804, 0.04779237508773804, 0.04779237508773804, -0.04779237508773804, -0.04779237508773804, -0.04779237508773804, 0.0, -0.04779237508773804, -0.04779237508773804, 0.04779237508773804, 0.0]
[2025-05-13 21:38:09,168]: Mean: -0.00006199
[2025-05-13 21:38:09,168]: Min: -0.04779238
[2025-05-13 21:38:09,168]: Max: 0.09558475
[2025-05-13 21:38:09,169]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-13 21:38:09,169]: Sample Values (25 elements): [0.9690848588943481, 0.9679353833198547, 0.9676070809364319, 0.9687351584434509, 0.9698774218559265, 0.9662863612174988, 0.9686030149459839, 0.9702696204185486, 0.9666883945465088, 0.9680118560791016, 0.9680668115615845, 0.9676175713539124, 0.96681809425354, 0.9677287340164185, 0.9665195941925049, 0.9697011113166809, 0.9690021872520447, 0.9664773344993591, 0.9694951176643372, 0.968219518661499, 0.9689043760299683, 0.9686712026596069, 0.9671399593353271, 0.9661774039268494, 0.9665836095809937]
[2025-05-13 21:38:09,169]: Mean: 0.96841085
[2025-05-13 21:38:09,169]: Min: 0.96395409
[2025-05-13 21:38:09,169]: Max: 0.97480845
[2025-05-13 21:38:09,170]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 21:38:09,215]: Sample Values (25 elements): [0.0, 0.0, -0.015671461820602417, 0.0, 0.0, 0.0, 0.0, 0.015671461820602417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.015671461820602417, 0.015671461820602417, 0.0, 0.015671461820602417, 0.0, 0.015671461820602417]
[2025-05-13 21:38:09,215]: Mean: -0.00018977
[2025-05-13 21:38:09,216]: Min: -0.01567146
[2025-05-13 21:38:09,216]: Max: 0.03134292
[2025-05-13 21:38:09,216]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-13 21:38:09,218]: Sample Values (25 elements): [0.9699174761772156, 0.9713786244392395, 0.9685776829719543, 0.9707265496253967, 0.9728581309318542, 0.9707933068275452, 0.9709481000900269, 0.9707446694374084, 0.9696394801139832, 0.969656229019165, 0.9697958827018738, 0.9727957844734192, 0.9730034470558167, 0.9711580276489258, 0.9722839593887329, 0.9776328206062317, 0.9699317812919617, 0.9755584001541138, 0.9709534049034119, 0.9693926572799683, 0.9689725637435913, 0.9712477922439575, 0.9681933522224426, 0.9715335965156555, 0.9679336547851562]
[2025-05-13 21:38:09,218]: Mean: 0.97082895
[2025-05-13 21:38:09,218]: Min: 0.96706939
[2025-05-13 21:38:09,219]: Max: 0.98206449
[2025-05-13 21:38:09,220]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-13 21:38:09,283]: Sample Values (25 elements): [0.01389022171497345, 0.0, -0.01389022171497345, 0.0, -0.01389022171497345, 0.0, -0.01389022171497345, 0.0, 0.01389022171497345, 0.0, 0.01389022171497345, 0.01389022171497345, 0.0, -0.01389022171497345, -0.01389022171497345, -0.01389022171497345, 0.01389022171497345, -0.01389022171497345, 0.0, -0.01389022171497345, 0.0, 0.0, -0.01389022171497345, 0.01389022171497345, -0.01389022171497345]
[2025-05-13 21:38:09,283]: Mean: -0.00011150
[2025-05-13 21:38:09,283]: Min: -0.01389022
[2025-05-13 21:38:09,284]: Max: 0.02778044
[2025-05-13 21:38:09,284]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-13 21:38:09,285]: Sample Values (25 elements): [0.9769664406776428, 0.9779888391494751, 0.9787288308143616, 0.9746468663215637, 0.9784983396530151, 0.9787764549255371, 0.9746317863464355, 0.9796488285064697, 0.979119062423706, 0.9767868518829346, 0.9807654023170471, 0.977931559085846, 0.9781222343444824, 0.9803050756454468, 0.9755425453186035, 0.9819470643997192, 0.9774144887924194, 0.9760258793830872, 0.9778155088424683, 0.9749775528907776, 0.978206217288971, 0.9778092503547668, 0.9834036827087402, 0.9774839878082275, 0.9787835478782654]
[2025-05-13 21:38:09,285]: Mean: 0.97774911
[2025-05-13 21:38:09,286]: Min: 0.97240901
[2025-05-13 21:38:09,286]: Max: 0.98726636
[2025-05-13 21:38:09,286]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-13 21:38:09,287]: Sample Values (25 elements): [-0.03177635371685028, 0.02011476829648018, 0.0636056512594223, 0.058129869401454926, -0.05121353268623352, -0.0033623024355620146, -0.03689538314938545, -0.0029855992179363966, 0.06643803417682648, -0.025529654696583748, 0.0761554166674614, 0.06823709607124329, -0.07811397314071655, -0.04326092451810837, 0.0009001558064483106, 0.012697255238890648, 0.028985146433115005, -0.04429379478096962, -0.019399330019950867, 0.010876634158194065, 0.08075021207332611, 0.045103125274181366, 0.01828940212726593, 0.012341052293777466, -0.01592428609728813]
[2025-05-13 21:38:09,287]: Mean: 0.00047499
[2025-05-13 21:38:09,287]: Min: -0.10113332
[2025-05-13 21:38:09,287]: Max: 0.12481028
