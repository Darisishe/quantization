[2025-05-06 07:34:39,034]: 
Training LeNet5 with hardtanh
[2025-05-06 07:35:13,853]: [LeNet5_hardtanh] Epoch: 001 Train Loss: 2.1697 Train Acc: 0.2037 Eval Loss: 2.0148 Eval Acc: 0.2690 (LR: 0.001000)
[2025-05-06 07:35:47,992]: [LeNet5_hardtanh] Epoch: 002 Train Loss: 1.9835 Train Acc: 0.2741 Eval Loss: 1.8886 Eval Acc: 0.3209 (LR: 0.001000)
[2025-05-06 07:36:21,395]: [LeNet5_hardtanh] Epoch: 003 Train Loss: 1.8620 Train Acc: 0.3215 Eval Loss: 1.7440 Eval Acc: 0.3726 (LR: 0.001000)
[2025-05-06 07:36:54,186]: [LeNet5_hardtanh] Epoch: 004 Train Loss: 1.7543 Train Acc: 0.3556 Eval Loss: 1.6546 Eval Acc: 0.3961 (LR: 0.001000)
[2025-05-06 07:37:26,777]: [LeNet5_hardtanh] Epoch: 005 Train Loss: 1.6971 Train Acc: 0.3747 Eval Loss: 1.5996 Eval Acc: 0.4156 (LR: 0.001000)
[2025-05-06 07:37:59,867]: [LeNet5_hardtanh] Epoch: 006 Train Loss: 1.6482 Train Acc: 0.3938 Eval Loss: 1.5442 Eval Acc: 0.4353 (LR: 0.001000)
[2025-05-06 07:38:33,366]: [LeNet5_hardtanh] Epoch: 007 Train Loss: 1.6071 Train Acc: 0.4073 Eval Loss: 1.5036 Eval Acc: 0.4514 (LR: 0.001000)
[2025-05-06 07:39:07,050]: [LeNet5_hardtanh] Epoch: 008 Train Loss: 1.5756 Train Acc: 0.4190 Eval Loss: 1.4678 Eval Acc: 0.4590 (LR: 0.001000)
[2025-05-06 07:39:40,063]: [LeNet5_hardtanh] Epoch: 009 Train Loss: 1.5447 Train Acc: 0.4316 Eval Loss: 1.4401 Eval Acc: 0.4665 (LR: 0.001000)
[2025-05-06 07:40:12,876]: [LeNet5_hardtanh] Epoch: 010 Train Loss: 1.5188 Train Acc: 0.4404 Eval Loss: 1.4187 Eval Acc: 0.4761 (LR: 0.001000)
[2025-05-06 07:40:46,372]: [LeNet5_hardtanh] Epoch: 011 Train Loss: 1.4995 Train Acc: 0.4491 Eval Loss: 1.3961 Eval Acc: 0.4855 (LR: 0.001000)
[2025-05-06 07:41:21,091]: [LeNet5_hardtanh] Epoch: 012 Train Loss: 1.4799 Train Acc: 0.4565 Eval Loss: 1.3818 Eval Acc: 0.4909 (LR: 0.001000)
[2025-05-06 07:41:53,937]: [LeNet5_hardtanh] Epoch: 013 Train Loss: 1.4629 Train Acc: 0.4627 Eval Loss: 1.3546 Eval Acc: 0.5021 (LR: 0.001000)
[2025-05-06 07:42:26,887]: [LeNet5_hardtanh] Epoch: 014 Train Loss: 1.4485 Train Acc: 0.4702 Eval Loss: 1.3539 Eval Acc: 0.5050 (LR: 0.001000)
[2025-05-06 07:42:59,700]: [LeNet5_hardtanh] Epoch: 015 Train Loss: 1.4375 Train Acc: 0.4751 Eval Loss: 1.3494 Eval Acc: 0.5071 (LR: 0.001000)
[2025-05-06 07:43:32,802]: [LeNet5_hardtanh] Epoch: 016 Train Loss: 1.4244 Train Acc: 0.4793 Eval Loss: 1.3173 Eval Acc: 0.5164 (LR: 0.001000)
[2025-05-06 07:44:06,636]: [LeNet5_hardtanh] Epoch: 017 Train Loss: 1.4174 Train Acc: 0.4811 Eval Loss: 1.3131 Eval Acc: 0.5227 (LR: 0.001000)
[2025-05-06 07:44:40,119]: [LeNet5_hardtanh] Epoch: 018 Train Loss: 1.4087 Train Acc: 0.4877 Eval Loss: 1.3037 Eval Acc: 0.5235 (LR: 0.001000)
[2025-05-06 07:45:12,812]: [LeNet5_hardtanh] Epoch: 019 Train Loss: 1.3987 Train Acc: 0.4886 Eval Loss: 1.2934 Eval Acc: 0.5296 (LR: 0.001000)
[2025-05-06 07:45:45,774]: [LeNet5_hardtanh] Epoch: 020 Train Loss: 1.3863 Train Acc: 0.4941 Eval Loss: 1.2762 Eval Acc: 0.5402 (LR: 0.001000)
[2025-05-06 07:45:45,777]: [LeNet5_hardtanh] Best Eval Accuracy: 0.5402
[2025-05-06 07:45:45,781]: 
Training of full-precision model finished!
[2025-05-06 07:45:45,781]: Model Architecture:
[2025-05-06 07:45:45,781]: LeNet5(
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
  )
  (fc2): Sequential(
    (0): Linear(in_features=120, out_features=84, bias=True)
    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-06 07:45:45,781]: 
Model Weights:
[2025-05-06 07:45:45,782]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-06 07:45:45,793]: Sample Values (16 elements): [-0.05653856694698334, 0.2105645090341568, 0.04568423330783844, 0.08363758772611618, 0.060820989310741425, 0.04619709029793739, 0.14600546658039093, 0.05660797283053398, -0.09885874390602112, 0.13897104561328888, -0.08358864486217499, 0.15252354741096497, -0.12670956552028656, 0.030463997274637222, 0.058953557163476944, -0.02256808429956436]
[2025-05-06 07:45:45,800]: Mean: -0.0017
[2025-05-06 07:45:45,807]: Min: -0.3468
[2025-05-06 07:45:45,808]: Max: 0.3172
[2025-05-06 07:45:45,808]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-06 07:45:45,808]: Sample Values (16 elements): [-0.06407086551189423, 0.002215793589130044, -0.06771404296159744, 0.06679295003414154, 0.024207735434174538, -0.0540817528963089, -0.07209861278533936, 0.019645653665065765, -0.11969418823719025, -0.010943452827632427, -0.03916163370013237, 0.029751041904091835, -0.009242774918675423, 0.0048224544152617455, 0.004382839892059565, -0.007410503923892975]
[2025-05-06 07:45:45,808]: Mean: -0.0017
[2025-05-06 07:45:45,809]: Min: -0.2285
[2025-05-06 07:45:45,809]: Max: 0.2111
[2025-05-06 07:45:45,809]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-06 07:45:45,810]: Sample Values (16 elements): [-0.01968177780508995, 0.03387782350182533, 0.02856254391372204, -0.011536928825080395, -0.022357506677508354, 0.051757000386714935, -0.043250441551208496, 0.02537781186401844, -0.02776109054684639, 0.016575774177908897, -0.018592186272144318, 0.013549139723181725, 0.018723489716649055, 0.024232329800724983, 0.003596740774810314, 0.020734038203954697]
[2025-05-06 07:45:45,810]: Mean: 0.0001
[2025-05-06 07:45:45,810]: Min: -0.0984
[2025-05-06 07:45:45,810]: Max: 0.1138
[2025-05-06 07:45:45,810]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-06 07:45:45,811]: Sample Values (16 elements): [0.023508431389927864, 0.011471905745565891, 0.09273809939622879, -0.03796814754605293, -0.03950878232717514, -0.006886318791657686, -0.03337995707988739, 0.07961452007293701, -0.10554379969835281, 0.06861608475446701, -0.07092715799808502, -0.13156476616859436, -0.06582939624786377, 0.0190590787678957, 0.05648242309689522, 0.05822192132472992]
[2025-05-06 07:45:45,811]: Mean: -0.0007
[2025-05-06 07:45:45,811]: Min: -0.1480
[2025-05-06 07:45:45,811]: Max: 0.1408
[2025-05-06 07:45:45,811]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-06 07:45:45,812]: Sample Values (16 elements): [0.0088899414986372, 0.07428599148988724, -0.12186148017644882, 0.019595753401517868, -0.12764506042003632, -0.08658667653799057, 0.05841020494699478, -0.10924559831619263, -0.007810337468981743, -0.10215458273887634, 0.006945467088371515, -0.19603800773620605, 0.03461911901831627, -0.19604435563087463, 0.04815805330872536, -0.1640154868364334]
[2025-05-06 07:45:45,812]: Mean: -0.0031
[2025-05-06 07:45:45,812]: Min: -0.3059
[2025-05-06 07:45:45,812]: Max: 0.3311
[2025-05-06 07:45:45,812]: 


QAT of LeNet5 with hardtanh down to 4 bits...
[2025-05-06 07:45:45,849]: [LeNet5_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-06 07:45:45,892]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-06 07:46:21,219]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 1.3890 Train Acc: 0.4961 Eval Loss: 1.2995 Eval Acc: 0.5283 (LR: 0.001000)
[2025-05-06 07:46:57,479]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 1.3783 Train Acc: 0.5003 Eval Loss: 1.2830 Eval Acc: 0.5335 (LR: 0.001000)
[2025-05-06 07:47:32,441]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 1.3716 Train Acc: 0.5023 Eval Loss: 1.2600 Eval Acc: 0.5444 (LR: 0.001000)
[2025-05-06 07:48:07,357]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 1.3662 Train Acc: 0.5057 Eval Loss: 1.2443 Eval Acc: 0.5521 (LR: 0.001000)
[2025-05-06 07:48:42,229]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 1.3598 Train Acc: 0.5061 Eval Loss: 1.2377 Eval Acc: 0.5544 (LR: 0.001000)
[2025-05-06 07:49:16,775]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 1.3460 Train Acc: 0.5140 Eval Loss: 1.2427 Eval Acc: 0.5521 (LR: 0.001000)
[2025-05-06 07:49:51,509]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 1.3418 Train Acc: 0.5157 Eval Loss: 1.2381 Eval Acc: 0.5494 (LR: 0.001000)
[2025-05-06 07:50:26,468]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 1.3363 Train Acc: 0.5191 Eval Loss: 1.2142 Eval Acc: 0.5641 (LR: 0.001000)
[2025-05-06 07:51:01,135]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 1.3327 Train Acc: 0.5185 Eval Loss: 1.2196 Eval Acc: 0.5598 (LR: 0.001000)
[2025-05-06 07:51:35,880]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 1.3242 Train Acc: 0.5209 Eval Loss: 1.1967 Eval Acc: 0.5707 (LR: 0.001000)
[2025-05-06 07:52:10,531]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 1.3192 Train Acc: 0.5224 Eval Loss: 1.2079 Eval Acc: 0.5682 (LR: 0.001000)
[2025-05-06 07:52:45,225]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 1.3176 Train Acc: 0.5252 Eval Loss: 1.1880 Eval Acc: 0.5714 (LR: 0.001000)
[2025-05-06 07:53:19,953]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 1.3104 Train Acc: 0.5275 Eval Loss: 1.2192 Eval Acc: 0.5633 (LR: 0.001000)
[2025-05-06 07:53:54,803]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 1.3053 Train Acc: 0.5277 Eval Loss: 1.1811 Eval Acc: 0.5745 (LR: 0.001000)
[2025-05-06 07:54:29,508]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 1.3061 Train Acc: 0.5278 Eval Loss: 1.2043 Eval Acc: 0.5708 (LR: 0.001000)
[2025-05-06 07:55:04,174]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 1.2989 Train Acc: 0.5318 Eval Loss: 1.1698 Eval Acc: 0.5864 (LR: 0.001000)
[2025-05-06 07:55:39,001]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 1.2945 Train Acc: 0.5330 Eval Loss: 1.1700 Eval Acc: 0.5820 (LR: 0.001000)
[2025-05-06 07:56:13,688]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 1.2848 Train Acc: 0.5379 Eval Loss: 1.1587 Eval Acc: 0.5844 (LR: 0.001000)
[2025-05-06 07:56:48,663]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 1.2813 Train Acc: 0.5393 Eval Loss: 1.1863 Eval Acc: 0.5754 (LR: 0.001000)
[2025-05-06 07:57:23,131]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 1.2763 Train Acc: 0.5419 Eval Loss: 1.1612 Eval Acc: 0.5860 (LR: 0.001000)
[2025-05-06 07:57:57,874]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 1.2778 Train Acc: 0.5386 Eval Loss: 1.1579 Eval Acc: 0.5845 (LR: 0.001000)
[2025-05-06 07:58:32,481]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 1.2696 Train Acc: 0.5436 Eval Loss: 1.1564 Eval Acc: 0.5874 (LR: 0.001000)
[2025-05-06 07:59:07,274]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 1.2650 Train Acc: 0.5456 Eval Loss: 1.1430 Eval Acc: 0.5955 (LR: 0.001000)
[2025-05-06 07:59:41,803]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 1.2653 Train Acc: 0.5464 Eval Loss: 1.1365 Eval Acc: 0.5951 (LR: 0.001000)
[2025-05-06 08:00:16,290]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 1.2594 Train Acc: 0.5487 Eval Loss: 1.1523 Eval Acc: 0.5911 (LR: 0.001000)
[2025-05-06 08:00:50,894]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 1.2620 Train Acc: 0.5447 Eval Loss: 1.1509 Eval Acc: 0.5880 (LR: 0.001000)
[2025-05-06 08:01:25,542]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 1.2548 Train Acc: 0.5506 Eval Loss: 1.1298 Eval Acc: 0.5964 (LR: 0.001000)
[2025-05-06 08:02:00,280]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 1.2438 Train Acc: 0.5543 Eval Loss: 1.1357 Eval Acc: 0.5968 (LR: 0.001000)
[2025-05-06 08:02:34,964]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 1.2441 Train Acc: 0.5516 Eval Loss: 1.1631 Eval Acc: 0.5833 (LR: 0.001000)
[2025-05-06 08:03:09,533]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 1.2417 Train Acc: 0.5540 Eval Loss: 1.1509 Eval Acc: 0.5870 (LR: 0.000250)
[2025-05-06 08:03:44,273]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 1.2174 Train Acc: 0.5612 Eval Loss: 1.0979 Eval Acc: 0.6120 (LR: 0.000250)
[2025-05-06 08:04:18,947]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 1.2129 Train Acc: 0.5671 Eval Loss: 1.1007 Eval Acc: 0.6090 (LR: 0.000250)
[2025-05-06 08:04:53,503]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 1.2182 Train Acc: 0.5639 Eval Loss: 1.1058 Eval Acc: 0.6110 (LR: 0.000250)
[2025-05-06 08:05:28,099]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 1.2129 Train Acc: 0.5654 Eval Loss: 1.1063 Eval Acc: 0.6079 (LR: 0.000250)
[2025-05-06 08:06:02,975]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 1.2052 Train Acc: 0.5691 Eval Loss: 1.0894 Eval Acc: 0.6155 (LR: 0.000250)
[2025-05-06 08:06:37,975]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 1.2110 Train Acc: 0.5640 Eval Loss: 1.0921 Eval Acc: 0.6155 (LR: 0.000250)
[2025-05-06 08:07:12,784]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 1.2104 Train Acc: 0.5652 Eval Loss: 1.1049 Eval Acc: 0.6087 (LR: 0.000250)
[2025-05-06 08:07:47,481]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 1.2134 Train Acc: 0.5655 Eval Loss: 1.0887 Eval Acc: 0.6169 (LR: 0.000250)
[2025-05-06 08:08:22,102]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 1.2077 Train Acc: 0.5668 Eval Loss: 1.1109 Eval Acc: 0.6037 (LR: 0.000250)
[2025-05-06 08:08:57,022]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 1.2076 Train Acc: 0.5677 Eval Loss: 1.0990 Eval Acc: 0.6079 (LR: 0.000250)
[2025-05-06 08:09:31,858]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 1.2071 Train Acc: 0.5685 Eval Loss: 1.0918 Eval Acc: 0.6136 (LR: 0.000250)
[2025-05-06 08:10:06,601]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 1.2052 Train Acc: 0.5681 Eval Loss: 1.0897 Eval Acc: 0.6141 (LR: 0.000250)
[2025-05-06 08:10:43,283]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 1.2066 Train Acc: 0.5674 Eval Loss: 1.0931 Eval Acc: 0.6091 (LR: 0.000250)
[2025-05-06 08:11:20,035]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 1.1974 Train Acc: 0.5708 Eval Loss: 1.0944 Eval Acc: 0.6104 (LR: 0.000250)
[2025-05-06 08:11:56,555]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 1.2010 Train Acc: 0.5705 Eval Loss: 1.0837 Eval Acc: 0.6177 (LR: 0.000063)
