[2025-05-26 22:40:29,740]: 
Training LeNet5 with hardtanh
[2025-05-26 22:41:03,867]: [LeNet5_hardtanh] Epoch: 001 Train Loss: 1.7246 Train Acc: 0.3708 Eval Loss: 1.4354 Eval Acc: 0.4802 (LR: 0.00100000)
[2025-05-26 22:41:34,779]: [LeNet5_hardtanh] Epoch: 002 Train Loss: 1.4778 Train Acc: 0.4637 Eval Loss: 1.3098 Eval Acc: 0.5266 (LR: 0.00100000)
[2025-05-26 22:42:02,380]: [LeNet5_hardtanh] Epoch: 003 Train Loss: 1.4037 Train Acc: 0.4940 Eval Loss: 1.2763 Eval Acc: 0.5389 (LR: 0.00100000)
[2025-05-26 22:42:29,677]: [LeNet5_hardtanh] Epoch: 004 Train Loss: 1.3474 Train Acc: 0.5124 Eval Loss: 1.2134 Eval Acc: 0.5664 (LR: 0.00100000)
[2025-05-26 22:42:59,013]: [LeNet5_hardtanh] Epoch: 005 Train Loss: 1.3161 Train Acc: 0.5258 Eval Loss: 1.2176 Eval Acc: 0.5598 (LR: 0.00100000)
[2025-05-26 22:43:30,369]: [LeNet5_hardtanh] Epoch: 006 Train Loss: 1.2754 Train Acc: 0.5399 Eval Loss: 1.1569 Eval Acc: 0.5868 (LR: 0.00100000)
[2025-05-26 22:44:01,815]: [LeNet5_hardtanh] Epoch: 007 Train Loss: 1.2510 Train Acc: 0.5510 Eval Loss: 1.1356 Eval Acc: 0.5911 (LR: 0.00100000)
[2025-05-26 22:44:32,704]: [LeNet5_hardtanh] Epoch: 008 Train Loss: 1.2283 Train Acc: 0.5602 Eval Loss: 1.1258 Eval Acc: 0.5943 (LR: 0.00100000)
[2025-05-26 22:45:03,604]: [LeNet5_hardtanh] Epoch: 009 Train Loss: 1.2149 Train Acc: 0.5631 Eval Loss: 1.0959 Eval Acc: 0.5989 (LR: 0.00100000)
[2025-05-26 22:45:34,574]: [LeNet5_hardtanh] Epoch: 010 Train Loss: 1.1974 Train Acc: 0.5733 Eval Loss: 1.0991 Eval Acc: 0.6065 (LR: 0.00100000)
[2025-05-26 22:46:05,544]: [LeNet5_hardtanh] Epoch: 011 Train Loss: 1.1819 Train Acc: 0.5792 Eval Loss: 1.1015 Eval Acc: 0.6058 (LR: 0.00100000)
[2025-05-26 22:46:35,716]: [LeNet5_hardtanh] Epoch: 012 Train Loss: 1.1703 Train Acc: 0.5840 Eval Loss: 1.0573 Eval Acc: 0.6289 (LR: 0.00100000)
[2025-05-26 22:47:06,421]: [LeNet5_hardtanh] Epoch: 013 Train Loss: 1.1569 Train Acc: 0.5871 Eval Loss: 1.0489 Eval Acc: 0.6248 (LR: 0.00100000)
[2025-05-26 22:47:37,065]: [LeNet5_hardtanh] Epoch: 014 Train Loss: 1.1456 Train Acc: 0.5926 Eval Loss: 1.0918 Eval Acc: 0.6125 (LR: 0.00100000)
[2025-05-26 22:48:07,821]: [LeNet5_hardtanh] Epoch: 015 Train Loss: 1.1315 Train Acc: 0.5972 Eval Loss: 1.1119 Eval Acc: 0.6060 (LR: 0.00100000)
[2025-05-26 22:48:38,682]: [LeNet5_hardtanh] Epoch: 016 Train Loss: 1.1319 Train Acc: 0.5993 Eval Loss: 1.0456 Eval Acc: 0.6296 (LR: 0.00100000)
[2025-05-26 22:49:09,659]: [LeNet5_hardtanh] Epoch: 017 Train Loss: 1.1245 Train Acc: 0.5992 Eval Loss: 1.0435 Eval Acc: 0.6272 (LR: 0.00100000)
[2025-05-26 22:49:40,422]: [LeNet5_hardtanh] Epoch: 018 Train Loss: 1.1233 Train Acc: 0.5983 Eval Loss: 1.0629 Eval Acc: 0.6259 (LR: 0.00100000)
[2025-05-26 22:50:11,646]: [LeNet5_hardtanh] Epoch: 019 Train Loss: 1.1143 Train Acc: 0.6039 Eval Loss: 1.0206 Eval Acc: 0.6381 (LR: 0.00100000)
[2025-05-26 22:50:42,280]: [LeNet5_hardtanh] Epoch: 020 Train Loss: 1.1107 Train Acc: 0.6057 Eval Loss: 1.0184 Eval Acc: 0.6407 (LR: 0.00100000)
[2025-05-26 22:51:12,544]: [LeNet5_hardtanh] Epoch: 021 Train Loss: 1.0973 Train Acc: 0.6113 Eval Loss: 1.0307 Eval Acc: 0.6346 (LR: 0.00100000)
[2025-05-26 22:51:42,727]: [LeNet5_hardtanh] Epoch: 022 Train Loss: 1.0912 Train Acc: 0.6150 Eval Loss: 1.0125 Eval Acc: 0.6455 (LR: 0.00100000)
[2025-05-26 22:52:14,450]: [LeNet5_hardtanh] Epoch: 023 Train Loss: 1.0884 Train Acc: 0.6149 Eval Loss: 0.9905 Eval Acc: 0.6506 (LR: 0.00100000)
[2025-05-26 22:52:44,772]: [LeNet5_hardtanh] Epoch: 024 Train Loss: 1.0902 Train Acc: 0.6139 Eval Loss: 0.9968 Eval Acc: 0.6481 (LR: 0.00100000)
[2025-05-26 22:53:14,623]: [LeNet5_hardtanh] Epoch: 025 Train Loss: 1.0812 Train Acc: 0.6178 Eval Loss: 1.0227 Eval Acc: 0.6411 (LR: 0.00100000)
[2025-05-26 22:53:44,483]: [LeNet5_hardtanh] Epoch: 026 Train Loss: 1.0729 Train Acc: 0.6194 Eval Loss: 1.0136 Eval Acc: 0.6459 (LR: 0.00100000)
[2025-05-26 22:54:14,588]: [LeNet5_hardtanh] Epoch: 027 Train Loss: 1.0716 Train Acc: 0.6212 Eval Loss: 0.9880 Eval Acc: 0.6516 (LR: 0.00100000)
[2025-05-26 22:54:44,408]: [LeNet5_hardtanh] Epoch: 028 Train Loss: 1.0672 Train Acc: 0.6203 Eval Loss: 0.9812 Eval Acc: 0.6551 (LR: 0.00100000)
[2025-05-26 22:55:14,135]: [LeNet5_hardtanh] Epoch: 029 Train Loss: 1.0682 Train Acc: 0.6209 Eval Loss: 0.9970 Eval Acc: 0.6481 (LR: 0.00100000)
[2025-05-26 22:55:43,991]: [LeNet5_hardtanh] Epoch: 030 Train Loss: 1.0581 Train Acc: 0.6251 Eval Loss: 1.0265 Eval Acc: 0.6360 (LR: 0.00100000)
[2025-05-26 22:56:14,152]: [LeNet5_hardtanh] Epoch: 031 Train Loss: 1.0567 Train Acc: 0.6252 Eval Loss: 0.9681 Eval Acc: 0.6575 (LR: 0.00100000)
[2025-05-26 22:56:43,288]: [LeNet5_hardtanh] Epoch: 032 Train Loss: 1.0596 Train Acc: 0.6234 Eval Loss: 0.9844 Eval Acc: 0.6512 (LR: 0.00100000)
[2025-05-26 22:57:12,410]: [LeNet5_hardtanh] Epoch: 033 Train Loss: 1.0480 Train Acc: 0.6276 Eval Loss: 0.9546 Eval Acc: 0.6644 (LR: 0.00100000)
[2025-05-26 22:57:41,118]: [LeNet5_hardtanh] Epoch: 034 Train Loss: 1.0479 Train Acc: 0.6299 Eval Loss: 0.9744 Eval Acc: 0.6569 (LR: 0.00100000)
[2025-05-26 22:58:10,387]: [LeNet5_hardtanh] Epoch: 035 Train Loss: 1.0423 Train Acc: 0.6301 Eval Loss: 0.9688 Eval Acc: 0.6566 (LR: 0.00100000)
[2025-05-26 22:58:39,330]: [LeNet5_hardtanh] Epoch: 036 Train Loss: 1.0441 Train Acc: 0.6329 Eval Loss: 0.9520 Eval Acc: 0.6657 (LR: 0.00100000)
[2025-05-26 22:59:08,096]: [LeNet5_hardtanh] Epoch: 037 Train Loss: 1.0393 Train Acc: 0.6309 Eval Loss: 0.9629 Eval Acc: 0.6649 (LR: 0.00100000)
[2025-05-26 22:59:36,846]: [LeNet5_hardtanh] Epoch: 038 Train Loss: 1.0387 Train Acc: 0.6322 Eval Loss: 0.9648 Eval Acc: 0.6618 (LR: 0.00100000)
[2025-05-26 23:00:05,767]: [LeNet5_hardtanh] Epoch: 039 Train Loss: 1.0345 Train Acc: 0.6330 Eval Loss: 0.9487 Eval Acc: 0.6619 (LR: 0.00100000)
[2025-05-26 23:00:35,039]: [LeNet5_hardtanh] Epoch: 040 Train Loss: 1.0343 Train Acc: 0.6324 Eval Loss: 0.9860 Eval Acc: 0.6526 (LR: 0.00100000)
[2025-05-26 23:01:04,986]: [LeNet5_hardtanh] Epoch: 041 Train Loss: 1.0321 Train Acc: 0.6341 Eval Loss: 0.9453 Eval Acc: 0.6710 (LR: 0.00100000)
[2025-05-26 23:01:35,901]: [LeNet5_hardtanh] Epoch: 042 Train Loss: 1.0254 Train Acc: 0.6362 Eval Loss: 0.9680 Eval Acc: 0.6612 (LR: 0.00100000)
[2025-05-26 23:02:06,821]: [LeNet5_hardtanh] Epoch: 043 Train Loss: 1.0284 Train Acc: 0.6368 Eval Loss: 0.9473 Eval Acc: 0.6640 (LR: 0.00100000)
[2025-05-26 23:02:35,693]: [LeNet5_hardtanh] Epoch: 044 Train Loss: 1.0272 Train Acc: 0.6364 Eval Loss: 0.9719 Eval Acc: 0.6562 (LR: 0.00100000)
[2025-05-26 23:03:04,369]: [LeNet5_hardtanh] Epoch: 045 Train Loss: 1.0243 Train Acc: 0.6381 Eval Loss: 0.9479 Eval Acc: 0.6678 (LR: 0.00100000)
[2025-05-26 23:03:31,610]: [LeNet5_hardtanh] Epoch: 046 Train Loss: 1.0215 Train Acc: 0.6382 Eval Loss: 0.9364 Eval Acc: 0.6657 (LR: 0.00100000)
[2025-05-26 23:04:01,179]: [LeNet5_hardtanh] Epoch: 047 Train Loss: 1.0157 Train Acc: 0.6389 Eval Loss: 0.9567 Eval Acc: 0.6634 (LR: 0.00100000)
[2025-05-26 23:04:30,941]: [LeNet5_hardtanh] Epoch: 048 Train Loss: 1.0103 Train Acc: 0.6447 Eval Loss: 0.9385 Eval Acc: 0.6690 (LR: 0.00100000)
[2025-05-26 23:04:58,092]: [LeNet5_hardtanh] Epoch: 049 Train Loss: 1.0123 Train Acc: 0.6425 Eval Loss: 0.9498 Eval Acc: 0.6675 (LR: 0.00100000)
[2025-05-26 23:05:25,004]: [LeNet5_hardtanh] Epoch: 050 Train Loss: 1.0118 Train Acc: 0.6436 Eval Loss: 0.9567 Eval Acc: 0.6627 (LR: 0.00100000)
[2025-05-26 23:05:51,982]: [LeNet5_hardtanh] Epoch: 051 Train Loss: 1.0123 Train Acc: 0.6410 Eval Loss: 0.9290 Eval Acc: 0.6761 (LR: 0.00100000)
[2025-05-26 23:06:18,774]: [LeNet5_hardtanh] Epoch: 052 Train Loss: 1.0098 Train Acc: 0.6423 Eval Loss: 0.9413 Eval Acc: 0.6703 (LR: 0.00100000)
[2025-05-26 23:06:45,581]: [LeNet5_hardtanh] Epoch: 053 Train Loss: 0.9984 Train Acc: 0.6473 Eval Loss: 0.9381 Eval Acc: 0.6706 (LR: 0.00100000)
[2025-05-26 23:07:12,540]: [LeNet5_hardtanh] Epoch: 054 Train Loss: 1.0087 Train Acc: 0.6424 Eval Loss: 0.9274 Eval Acc: 0.6765 (LR: 0.00100000)
[2025-05-26 23:07:39,407]: [LeNet5_hardtanh] Epoch: 055 Train Loss: 1.0058 Train Acc: 0.6461 Eval Loss: 0.9314 Eval Acc: 0.6738 (LR: 0.00100000)
[2025-05-26 23:08:06,093]: [LeNet5_hardtanh] Epoch: 056 Train Loss: 0.9985 Train Acc: 0.6476 Eval Loss: 0.9127 Eval Acc: 0.6773 (LR: 0.00100000)
[2025-05-26 23:08:32,925]: [LeNet5_hardtanh] Epoch: 057 Train Loss: 1.0033 Train Acc: 0.6466 Eval Loss: 0.9486 Eval Acc: 0.6617 (LR: 0.00100000)
[2025-05-26 23:08:59,394]: [LeNet5_hardtanh] Epoch: 058 Train Loss: 1.0027 Train Acc: 0.6472 Eval Loss: 0.9435 Eval Acc: 0.6625 (LR: 0.00100000)
[2025-05-26 23:09:25,751]: [LeNet5_hardtanh] Epoch: 059 Train Loss: 0.9964 Train Acc: 0.6470 Eval Loss: 0.9139 Eval Acc: 0.6754 (LR: 0.00100000)
[2025-05-26 23:09:52,690]: [LeNet5_hardtanh] Epoch: 060 Train Loss: 0.9933 Train Acc: 0.6480 Eval Loss: 0.9178 Eval Acc: 0.6758 (LR: 0.00100000)
[2025-05-26 23:10:18,997]: [LeNet5_hardtanh] Epoch: 061 Train Loss: 0.9873 Train Acc: 0.6512 Eval Loss: 0.9134 Eval Acc: 0.6767 (LR: 0.00100000)
[2025-05-26 23:10:45,443]: [LeNet5_hardtanh] Epoch: 062 Train Loss: 0.9870 Train Acc: 0.6491 Eval Loss: 0.9483 Eval Acc: 0.6662 (LR: 0.00100000)
[2025-05-26 23:11:11,750]: [LeNet5_hardtanh] Epoch: 063 Train Loss: 0.9907 Train Acc: 0.6489 Eval Loss: 0.9291 Eval Acc: 0.6708 (LR: 0.00100000)
[2025-05-26 23:11:38,281]: [LeNet5_hardtanh] Epoch: 064 Train Loss: 0.9913 Train Acc: 0.6500 Eval Loss: 0.9273 Eval Acc: 0.6786 (LR: 0.00100000)
[2025-05-26 23:12:04,744]: [LeNet5_hardtanh] Epoch: 065 Train Loss: 0.9873 Train Acc: 0.6502 Eval Loss: 0.9421 Eval Acc: 0.6664 (LR: 0.00100000)
[2025-05-26 23:12:31,213]: [LeNet5_hardtanh] Epoch: 066 Train Loss: 0.9912 Train Acc: 0.6487 Eval Loss: 0.9047 Eval Acc: 0.6830 (LR: 0.00100000)
[2025-05-26 23:12:57,829]: [LeNet5_hardtanh] Epoch: 067 Train Loss: 0.9880 Train Acc: 0.6526 Eval Loss: 0.9268 Eval Acc: 0.6768 (LR: 0.00100000)
[2025-05-26 23:13:24,449]: [LeNet5_hardtanh] Epoch: 068 Train Loss: 0.9867 Train Acc: 0.6510 Eval Loss: 0.8857 Eval Acc: 0.6903 (LR: 0.00100000)
[2025-05-26 23:13:51,223]: [LeNet5_hardtanh] Epoch: 069 Train Loss: 0.9813 Train Acc: 0.6515 Eval Loss: 0.8864 Eval Acc: 0.6917 (LR: 0.00100000)
[2025-05-26 23:14:17,933]: [LeNet5_hardtanh] Epoch: 070 Train Loss: 0.9710 Train Acc: 0.6549 Eval Loss: 0.9120 Eval Acc: 0.6792 (LR: 0.00100000)
[2025-05-26 23:14:44,651]: [LeNet5_hardtanh] Epoch: 071 Train Loss: 0.9897 Train Acc: 0.6501 Eval Loss: 0.9444 Eval Acc: 0.6731 (LR: 0.00100000)
[2025-05-26 23:15:11,235]: [LeNet5_hardtanh] Epoch: 072 Train Loss: 0.9826 Train Acc: 0.6524 Eval Loss: 0.8850 Eval Acc: 0.6866 (LR: 0.00100000)
[2025-05-26 23:15:37,571]: [LeNet5_hardtanh] Epoch: 073 Train Loss: 0.9769 Train Acc: 0.6558 Eval Loss: 0.8958 Eval Acc: 0.6853 (LR: 0.00100000)
[2025-05-26 23:16:03,940]: [LeNet5_hardtanh] Epoch: 074 Train Loss: 0.9819 Train Acc: 0.6546 Eval Loss: 0.9272 Eval Acc: 0.6745 (LR: 0.00100000)
[2025-05-26 23:16:30,387]: [LeNet5_hardtanh] Epoch: 075 Train Loss: 0.9759 Train Acc: 0.6563 Eval Loss: 0.8871 Eval Acc: 0.6870 (LR: 0.00100000)
[2025-05-26 23:16:56,844]: [LeNet5_hardtanh] Epoch: 076 Train Loss: 0.9754 Train Acc: 0.6565 Eval Loss: 0.8938 Eval Acc: 0.6879 (LR: 0.00100000)
[2025-05-26 23:17:23,374]: [LeNet5_hardtanh] Epoch: 077 Train Loss: 0.9758 Train Acc: 0.6543 Eval Loss: 0.8935 Eval Acc: 0.6847 (LR: 0.00100000)
[2025-05-26 23:17:50,009]: [LeNet5_hardtanh] Epoch: 078 Train Loss: 0.9781 Train Acc: 0.6553 Eval Loss: 0.8931 Eval Acc: 0.6826 (LR: 0.00100000)
[2025-05-26 23:18:16,826]: [LeNet5_hardtanh] Epoch: 079 Train Loss: 0.9649 Train Acc: 0.6612 Eval Loss: 0.8902 Eval Acc: 0.6863 (LR: 0.00100000)
[2025-05-26 23:18:43,728]: [LeNet5_hardtanh] Epoch: 080 Train Loss: 0.9715 Train Acc: 0.6564 Eval Loss: 0.8829 Eval Acc: 0.6919 (LR: 0.00100000)
[2025-05-26 23:19:10,465]: [LeNet5_hardtanh] Epoch: 081 Train Loss: 0.9677 Train Acc: 0.6604 Eval Loss: 0.9139 Eval Acc: 0.6796 (LR: 0.00100000)
[2025-05-26 23:19:37,007]: [LeNet5_hardtanh] Epoch: 082 Train Loss: 0.9611 Train Acc: 0.6615 Eval Loss: 0.8966 Eval Acc: 0.6848 (LR: 0.00100000)
[2025-05-26 23:20:03,594]: [LeNet5_hardtanh] Epoch: 083 Train Loss: 0.9695 Train Acc: 0.6571 Eval Loss: 0.9136 Eval Acc: 0.6817 (LR: 0.00100000)
[2025-05-26 23:20:30,029]: [LeNet5_hardtanh] Epoch: 084 Train Loss: 0.9740 Train Acc: 0.6550 Eval Loss: 0.8906 Eval Acc: 0.6874 (LR: 0.00100000)
[2025-05-26 23:20:56,407]: [LeNet5_hardtanh] Epoch: 085 Train Loss: 0.9686 Train Acc: 0.6578 Eval Loss: 0.8957 Eval Acc: 0.6817 (LR: 0.00100000)
[2025-05-26 23:21:22,879]: [LeNet5_hardtanh] Epoch: 086 Train Loss: 0.9655 Train Acc: 0.6588 Eval Loss: 0.9221 Eval Acc: 0.6772 (LR: 0.00100000)
[2025-05-26 23:21:49,242]: [LeNet5_hardtanh] Epoch: 087 Train Loss: 0.9632 Train Acc: 0.6593 Eval Loss: 0.8896 Eval Acc: 0.6866 (LR: 0.00100000)
[2025-05-26 23:22:15,633]: [LeNet5_hardtanh] Epoch: 088 Train Loss: 0.9611 Train Acc: 0.6607 Eval Loss: 0.8807 Eval Acc: 0.6889 (LR: 0.00100000)
[2025-05-26 23:22:42,036]: [LeNet5_hardtanh] Epoch: 089 Train Loss: 0.9629 Train Acc: 0.6593 Eval Loss: 0.8785 Eval Acc: 0.6960 (LR: 0.00100000)
[2025-05-26 23:23:08,726]: [LeNet5_hardtanh] Epoch: 090 Train Loss: 0.9562 Train Acc: 0.6616 Eval Loss: 0.8648 Eval Acc: 0.6976 (LR: 0.00100000)
[2025-05-26 23:23:35,334]: [LeNet5_hardtanh] Epoch: 091 Train Loss: 0.9584 Train Acc: 0.6612 Eval Loss: 0.9429 Eval Acc: 0.6680 (LR: 0.00100000)
[2025-05-26 23:24:01,885]: [LeNet5_hardtanh] Epoch: 092 Train Loss: 0.9632 Train Acc: 0.6622 Eval Loss: 0.8929 Eval Acc: 0.6916 (LR: 0.00100000)
[2025-05-26 23:24:28,518]: [LeNet5_hardtanh] Epoch: 093 Train Loss: 0.9649 Train Acc: 0.6593 Eval Loss: 0.9308 Eval Acc: 0.6721 (LR: 0.00100000)
[2025-05-26 23:24:55,243]: [LeNet5_hardtanh] Epoch: 094 Train Loss: 0.9585 Train Acc: 0.6619 Eval Loss: 0.8633 Eval Acc: 0.6940 (LR: 0.00100000)
[2025-05-26 23:25:24,796]: [LeNet5_hardtanh] Epoch: 095 Train Loss: 0.9628 Train Acc: 0.6590 Eval Loss: 0.8717 Eval Acc: 0.6929 (LR: 0.00100000)
[2025-05-26 23:25:52,022]: [LeNet5_hardtanh] Epoch: 096 Train Loss: 0.9545 Train Acc: 0.6624 Eval Loss: 0.8824 Eval Acc: 0.6869 (LR: 0.00100000)
[2025-05-26 23:26:19,692]: [LeNet5_hardtanh] Epoch: 097 Train Loss: 0.9610 Train Acc: 0.6602 Eval Loss: 0.8744 Eval Acc: 0.6939 (LR: 0.00100000)
[2025-05-26 23:26:48,079]: [LeNet5_hardtanh] Epoch: 098 Train Loss: 0.9529 Train Acc: 0.6647 Eval Loss: 0.8875 Eval Acc: 0.6853 (LR: 0.00100000)
[2025-05-26 23:27:18,369]: [LeNet5_hardtanh] Epoch: 099 Train Loss: 0.9543 Train Acc: 0.6634 Eval Loss: 0.8881 Eval Acc: 0.6904 (LR: 0.00100000)
[2025-05-26 23:27:47,613]: [LeNet5_hardtanh] Epoch: 100 Train Loss: 0.9574 Train Acc: 0.6614 Eval Loss: 0.8859 Eval Acc: 0.6898 (LR: 0.00100000)
[2025-05-26 23:28:16,823]: [LeNet5_hardtanh] Epoch: 101 Train Loss: 0.9536 Train Acc: 0.6629 Eval Loss: 0.8783 Eval Acc: 0.6953 (LR: 0.00100000)
[2025-05-26 23:28:45,048]: [LeNet5_hardtanh] Epoch: 102 Train Loss: 0.9526 Train Acc: 0.6615 Eval Loss: 0.8922 Eval Acc: 0.6892 (LR: 0.00100000)
[2025-05-26 23:29:14,444]: [LeNet5_hardtanh] Epoch: 103 Train Loss: 0.9575 Train Acc: 0.6609 Eval Loss: 0.8742 Eval Acc: 0.6927 (LR: 0.00100000)
[2025-05-26 23:29:43,545]: [LeNet5_hardtanh] Epoch: 104 Train Loss: 0.9560 Train Acc: 0.6624 Eval Loss: 0.8877 Eval Acc: 0.6929 (LR: 0.00100000)
[2025-05-26 23:30:13,388]: [LeNet5_hardtanh] Epoch: 105 Train Loss: 0.9485 Train Acc: 0.6646 Eval Loss: 0.8892 Eval Acc: 0.6873 (LR: 0.00010000)
[2025-05-26 23:30:42,173]: [LeNet5_hardtanh] Epoch: 106 Train Loss: 0.8933 Train Acc: 0.6860 Eval Loss: 0.8211 Eval Acc: 0.7108 (LR: 0.00010000)
[2025-05-26 23:31:09,937]: [LeNet5_hardtanh] Epoch: 107 Train Loss: 0.8806 Train Acc: 0.6911 Eval Loss: 0.8143 Eval Acc: 0.7125 (LR: 0.00010000)
[2025-05-26 23:31:37,615]: [LeNet5_hardtanh] Epoch: 108 Train Loss: 0.8723 Train Acc: 0.6924 Eval Loss: 0.8100 Eval Acc: 0.7163 (LR: 0.00010000)
[2025-05-26 23:32:05,810]: [LeNet5_hardtanh] Epoch: 109 Train Loss: 0.8692 Train Acc: 0.6932 Eval Loss: 0.8116 Eval Acc: 0.7154 (LR: 0.00010000)
[2025-05-26 23:32:33,910]: [LeNet5_hardtanh] Epoch: 110 Train Loss: 0.8655 Train Acc: 0.6947 Eval Loss: 0.8090 Eval Acc: 0.7149 (LR: 0.00010000)
[2025-05-26 23:33:01,984]: [LeNet5_hardtanh] Epoch: 111 Train Loss: 0.8622 Train Acc: 0.6949 Eval Loss: 0.8066 Eval Acc: 0.7167 (LR: 0.00010000)
[2025-05-26 23:33:30,114]: [LeNet5_hardtanh] Epoch: 112 Train Loss: 0.8644 Train Acc: 0.6961 Eval Loss: 0.8071 Eval Acc: 0.7152 (LR: 0.00010000)
[2025-05-26 23:33:59,956]: [LeNet5_hardtanh] Epoch: 113 Train Loss: 0.8629 Train Acc: 0.6960 Eval Loss: 0.8018 Eval Acc: 0.7190 (LR: 0.00010000)
[2025-05-26 23:34:29,273]: [LeNet5_hardtanh] Epoch: 114 Train Loss: 0.8522 Train Acc: 0.7001 Eval Loss: 0.7993 Eval Acc: 0.7201 (LR: 0.00010000)
[2025-05-26 23:34:57,381]: [LeNet5_hardtanh] Epoch: 115 Train Loss: 0.8580 Train Acc: 0.6978 Eval Loss: 0.7989 Eval Acc: 0.7189 (LR: 0.00010000)
[2025-05-26 23:35:26,967]: [LeNet5_hardtanh] Epoch: 116 Train Loss: 0.8511 Train Acc: 0.6994 Eval Loss: 0.7983 Eval Acc: 0.7186 (LR: 0.00010000)
[2025-05-26 23:35:55,713]: [LeNet5_hardtanh] Epoch: 117 Train Loss: 0.8573 Train Acc: 0.6977 Eval Loss: 0.7990 Eval Acc: 0.7187 (LR: 0.00010000)
[2025-05-26 23:36:23,643]: [LeNet5_hardtanh] Epoch: 118 Train Loss: 0.8504 Train Acc: 0.7005 Eval Loss: 0.8005 Eval Acc: 0.7203 (LR: 0.00010000)
[2025-05-26 23:36:53,459]: [LeNet5_hardtanh] Epoch: 119 Train Loss: 0.8471 Train Acc: 0.7016 Eval Loss: 0.8032 Eval Acc: 0.7173 (LR: 0.00010000)
[2025-05-26 23:37:22,900]: [LeNet5_hardtanh] Epoch: 120 Train Loss: 0.8534 Train Acc: 0.6998 Eval Loss: 0.8031 Eval Acc: 0.7213 (LR: 0.00010000)
[2025-05-26 23:37:51,109]: [LeNet5_hardtanh] Epoch: 121 Train Loss: 0.8480 Train Acc: 0.7025 Eval Loss: 0.7950 Eval Acc: 0.7226 (LR: 0.00010000)
[2025-05-26 23:38:21,153]: [LeNet5_hardtanh] Epoch: 122 Train Loss: 0.8481 Train Acc: 0.7010 Eval Loss: 0.7983 Eval Acc: 0.7200 (LR: 0.00010000)
[2025-05-26 23:38:50,306]: [LeNet5_hardtanh] Epoch: 123 Train Loss: 0.8433 Train Acc: 0.7036 Eval Loss: 0.7963 Eval Acc: 0.7212 (LR: 0.00010000)
[2025-05-26 23:39:18,728]: [LeNet5_hardtanh] Epoch: 124 Train Loss: 0.8490 Train Acc: 0.7020 Eval Loss: 0.7935 Eval Acc: 0.7218 (LR: 0.00010000)
[2025-05-26 23:39:49,359]: [LeNet5_hardtanh] Epoch: 125 Train Loss: 0.8402 Train Acc: 0.7048 Eval Loss: 0.7919 Eval Acc: 0.7226 (LR: 0.00010000)
[2025-05-26 23:40:20,135]: [LeNet5_hardtanh] Epoch: 126 Train Loss: 0.8432 Train Acc: 0.7049 Eval Loss: 0.7941 Eval Acc: 0.7223 (LR: 0.00010000)
[2025-05-26 23:40:50,671]: [LeNet5_hardtanh] Epoch: 127 Train Loss: 0.8474 Train Acc: 0.7023 Eval Loss: 0.7918 Eval Acc: 0.7225 (LR: 0.00010000)
[2025-05-26 23:41:21,200]: [LeNet5_hardtanh] Epoch: 128 Train Loss: 0.8460 Train Acc: 0.7014 Eval Loss: 0.7957 Eval Acc: 0.7199 (LR: 0.00010000)
[2025-05-26 23:41:51,578]: [LeNet5_hardtanh] Epoch: 129 Train Loss: 0.8381 Train Acc: 0.7046 Eval Loss: 0.7942 Eval Acc: 0.7220 (LR: 0.00010000)
[2025-05-26 23:42:22,336]: [LeNet5_hardtanh] Epoch: 130 Train Loss: 0.8426 Train Acc: 0.7043 Eval Loss: 0.7960 Eval Acc: 0.7224 (LR: 0.00010000)
[2025-05-26 23:42:52,980]: [LeNet5_hardtanh] Epoch: 131 Train Loss: 0.8399 Train Acc: 0.7032 Eval Loss: 0.7940 Eval Acc: 0.7201 (LR: 0.00010000)
[2025-05-26 23:43:23,373]: [LeNet5_hardtanh] Epoch: 132 Train Loss: 0.8373 Train Acc: 0.7047 Eval Loss: 0.7925 Eval Acc: 0.7217 (LR: 0.00010000)
[2025-05-26 23:43:54,164]: [LeNet5_hardtanh] Epoch: 133 Train Loss: 0.8362 Train Acc: 0.7051 Eval Loss: 0.7905 Eval Acc: 0.7228 (LR: 0.00010000)
[2025-05-26 23:44:25,144]: [LeNet5_hardtanh] Epoch: 134 Train Loss: 0.8362 Train Acc: 0.7038 Eval Loss: 0.7907 Eval Acc: 0.7254 (LR: 0.00010000)
[2025-05-26 23:44:55,517]: [LeNet5_hardtanh] Epoch: 135 Train Loss: 0.8373 Train Acc: 0.7056 Eval Loss: 0.7940 Eval Acc: 0.7209 (LR: 0.00010000)
[2025-05-26 23:45:25,895]: [LeNet5_hardtanh] Epoch: 136 Train Loss: 0.8332 Train Acc: 0.7055 Eval Loss: 0.7938 Eval Acc: 0.7219 (LR: 0.00010000)
[2025-05-26 23:45:55,799]: [LeNet5_hardtanh] Epoch: 137 Train Loss: 0.8353 Train Acc: 0.7061 Eval Loss: 0.7897 Eval Acc: 0.7230 (LR: 0.00010000)
[2025-05-26 23:46:26,453]: [LeNet5_hardtanh] Epoch: 138 Train Loss: 0.8387 Train Acc: 0.7026 Eval Loss: 0.7858 Eval Acc: 0.7249 (LR: 0.00010000)
[2025-05-26 23:46:56,777]: [LeNet5_hardtanh] Epoch: 139 Train Loss: 0.8379 Train Acc: 0.7040 Eval Loss: 0.7853 Eval Acc: 0.7263 (LR: 0.00010000)
[2025-05-26 23:47:27,272]: [LeNet5_hardtanh] Epoch: 140 Train Loss: 0.8369 Train Acc: 0.7051 Eval Loss: 0.7873 Eval Acc: 0.7228 (LR: 0.00010000)
[2025-05-26 23:47:58,751]: [LeNet5_hardtanh] Epoch: 141 Train Loss: 0.8364 Train Acc: 0.7056 Eval Loss: 0.7879 Eval Acc: 0.7230 (LR: 0.00010000)
[2025-05-26 23:48:29,486]: [LeNet5_hardtanh] Epoch: 142 Train Loss: 0.8344 Train Acc: 0.7065 Eval Loss: 0.7833 Eval Acc: 0.7272 (LR: 0.00010000)
[2025-05-26 23:49:00,228]: [LeNet5_hardtanh] Epoch: 143 Train Loss: 0.8381 Train Acc: 0.7044 Eval Loss: 0.7867 Eval Acc: 0.7261 (LR: 0.00010000)
[2025-05-26 23:49:30,749]: [LeNet5_hardtanh] Epoch: 144 Train Loss: 0.8318 Train Acc: 0.7069 Eval Loss: 0.7835 Eval Acc: 0.7244 (LR: 0.00010000)
[2025-05-26 23:50:00,817]: [LeNet5_hardtanh] Epoch: 145 Train Loss: 0.8363 Train Acc: 0.7056 Eval Loss: 0.7889 Eval Acc: 0.7237 (LR: 0.00010000)
[2025-05-26 23:50:31,548]: [LeNet5_hardtanh] Epoch: 146 Train Loss: 0.8350 Train Acc: 0.7052 Eval Loss: 0.7883 Eval Acc: 0.7234 (LR: 0.00010000)
[2025-05-26 23:51:00,402]: [LeNet5_hardtanh] Epoch: 147 Train Loss: 0.8327 Train Acc: 0.7076 Eval Loss: 0.7830 Eval Acc: 0.7268 (LR: 0.00010000)
[2025-05-26 23:51:30,094]: [LeNet5_hardtanh] Epoch: 148 Train Loss: 0.8288 Train Acc: 0.7084 Eval Loss: 0.7840 Eval Acc: 0.7266 (LR: 0.00010000)
[2025-05-26 23:52:00,826]: [LeNet5_hardtanh] Epoch: 149 Train Loss: 0.8247 Train Acc: 0.7105 Eval Loss: 0.7896 Eval Acc: 0.7255 (LR: 0.00010000)
[2025-05-26 23:52:31,396]: [LeNet5_hardtanh] Epoch: 150 Train Loss: 0.8326 Train Acc: 0.7069 Eval Loss: 0.7848 Eval Acc: 0.7265 (LR: 0.00010000)
[2025-05-26 23:53:01,563]: [LeNet5_hardtanh] Epoch: 151 Train Loss: 0.8296 Train Acc: 0.7075 Eval Loss: 0.7859 Eval Acc: 0.7265 (LR: 0.00010000)
[2025-05-26 23:53:32,464]: [LeNet5_hardtanh] Epoch: 152 Train Loss: 0.8317 Train Acc: 0.7065 Eval Loss: 0.7846 Eval Acc: 0.7256 (LR: 0.00010000)
[2025-05-26 23:54:02,892]: [LeNet5_hardtanh] Epoch: 153 Train Loss: 0.8323 Train Acc: 0.7055 Eval Loss: 0.7836 Eval Acc: 0.7258 (LR: 0.00010000)
[2025-05-26 23:54:33,546]: [LeNet5_hardtanh] Epoch: 154 Train Loss: 0.8307 Train Acc: 0.7090 Eval Loss: 0.7777 Eval Acc: 0.7264 (LR: 0.00010000)
[2025-05-26 23:55:04,274]: [LeNet5_hardtanh] Epoch: 155 Train Loss: 0.8287 Train Acc: 0.7062 Eval Loss: 0.7868 Eval Acc: 0.7262 (LR: 0.00010000)
[2025-05-26 23:55:34,741]: [LeNet5_hardtanh] Epoch: 156 Train Loss: 0.8243 Train Acc: 0.7094 Eval Loss: 0.7797 Eval Acc: 0.7292 (LR: 0.00010000)
[2025-05-26 23:56:05,288]: [LeNet5_hardtanh] Epoch: 157 Train Loss: 0.8273 Train Acc: 0.7099 Eval Loss: 0.7884 Eval Acc: 0.7258 (LR: 0.00010000)
[2025-05-26 23:56:36,207]: [LeNet5_hardtanh] Epoch: 158 Train Loss: 0.8257 Train Acc: 0.7089 Eval Loss: 0.7869 Eval Acc: 0.7253 (LR: 0.00010000)
[2025-05-26 23:57:06,755]: [LeNet5_hardtanh] Epoch: 159 Train Loss: 0.8256 Train Acc: 0.7095 Eval Loss: 0.7828 Eval Acc: 0.7249 (LR: 0.00010000)
[2025-05-26 23:57:37,518]: [LeNet5_hardtanh] Epoch: 160 Train Loss: 0.8262 Train Acc: 0.7070 Eval Loss: 0.7852 Eval Acc: 0.7266 (LR: 0.00010000)
[2025-05-26 23:58:08,033]: [LeNet5_hardtanh] Epoch: 161 Train Loss: 0.8293 Train Acc: 0.7071 Eval Loss: 0.7832 Eval Acc: 0.7286 (LR: 0.00010000)
[2025-05-26 23:58:39,024]: [LeNet5_hardtanh] Epoch: 162 Train Loss: 0.8249 Train Acc: 0.7093 Eval Loss: 0.7816 Eval Acc: 0.7259 (LR: 0.00010000)
[2025-05-26 23:59:09,737]: [LeNet5_hardtanh] Epoch: 163 Train Loss: 0.8243 Train Acc: 0.7089 Eval Loss: 0.7848 Eval Acc: 0.7240 (LR: 0.00010000)
[2025-05-26 23:59:40,520]: [LeNet5_hardtanh] Epoch: 164 Train Loss: 0.8231 Train Acc: 0.7103 Eval Loss: 0.7797 Eval Acc: 0.7265 (LR: 0.00010000)
[2025-05-27 00:00:11,652]: [LeNet5_hardtanh] Epoch: 165 Train Loss: 0.8184 Train Acc: 0.7125 Eval Loss: 0.7787 Eval Acc: 0.7279 (LR: 0.00001000)
[2025-05-27 00:00:42,387]: [LeNet5_hardtanh] Epoch: 166 Train Loss: 0.8226 Train Acc: 0.7108 Eval Loss: 0.7756 Eval Acc: 0.7296 (LR: 0.00001000)
[2025-05-27 00:01:12,913]: [LeNet5_hardtanh] Epoch: 167 Train Loss: 0.8171 Train Acc: 0.7112 Eval Loss: 0.7756 Eval Acc: 0.7300 (LR: 0.00001000)
[2025-05-27 00:01:43,565]: [LeNet5_hardtanh] Epoch: 168 Train Loss: 0.8196 Train Acc: 0.7124 Eval Loss: 0.7761 Eval Acc: 0.7296 (LR: 0.00001000)
[2025-05-27 00:02:14,458]: [LeNet5_hardtanh] Epoch: 169 Train Loss: 0.8168 Train Acc: 0.7114 Eval Loss: 0.7755 Eval Acc: 0.7296 (LR: 0.00001000)
[2025-05-27 00:02:45,300]: [LeNet5_hardtanh] Epoch: 170 Train Loss: 0.8188 Train Acc: 0.7133 Eval Loss: 0.7751 Eval Acc: 0.7295 (LR: 0.00001000)
[2025-05-27 00:03:15,607]: [LeNet5_hardtanh] Epoch: 171 Train Loss: 0.8138 Train Acc: 0.7127 Eval Loss: 0.7745 Eval Acc: 0.7307 (LR: 0.00001000)
[2025-05-27 00:03:47,734]: [LeNet5_hardtanh] Epoch: 172 Train Loss: 0.8205 Train Acc: 0.7114 Eval Loss: 0.7752 Eval Acc: 0.7290 (LR: 0.00001000)
[2025-05-27 00:04:18,845]: [LeNet5_hardtanh] Epoch: 173 Train Loss: 0.8155 Train Acc: 0.7141 Eval Loss: 0.7753 Eval Acc: 0.7300 (LR: 0.00001000)
[2025-05-27 00:04:51,486]: [LeNet5_hardtanh] Epoch: 174 Train Loss: 0.8156 Train Acc: 0.7146 Eval Loss: 0.7758 Eval Acc: 0.7284 (LR: 0.00001000)
[2025-05-27 00:05:22,669]: [LeNet5_hardtanh] Epoch: 175 Train Loss: 0.8113 Train Acc: 0.7151 Eval Loss: 0.7746 Eval Acc: 0.7292 (LR: 0.00001000)
[2025-05-27 00:05:53,421]: [LeNet5_hardtanh] Epoch: 176 Train Loss: 0.8176 Train Acc: 0.7117 Eval Loss: 0.7754 Eval Acc: 0.7303 (LR: 0.00001000)
[2025-05-27 00:06:23,578]: [LeNet5_hardtanh] Epoch: 177 Train Loss: 0.8144 Train Acc: 0.7131 Eval Loss: 0.7744 Eval Acc: 0.7297 (LR: 0.00001000)
[2025-05-27 00:06:54,554]: [LeNet5_hardtanh] Epoch: 178 Train Loss: 0.8122 Train Acc: 0.7132 Eval Loss: 0.7734 Eval Acc: 0.7300 (LR: 0.00001000)
[2025-05-27 00:07:27,704]: [LeNet5_hardtanh] Epoch: 179 Train Loss: 0.8127 Train Acc: 0.7138 Eval Loss: 0.7749 Eval Acc: 0.7294 (LR: 0.00001000)
[2025-05-27 00:08:05,675]: [LeNet5_hardtanh] Epoch: 180 Train Loss: 0.8105 Train Acc: 0.7146 Eval Loss: 0.7747 Eval Acc: 0.7290 (LR: 0.00001000)
[2025-05-27 00:08:37,065]: [LeNet5_hardtanh] Epoch: 181 Train Loss: 0.8136 Train Acc: 0.7144 Eval Loss: 0.7742 Eval Acc: 0.7285 (LR: 0.00001000)
[2025-05-27 00:09:08,842]: [LeNet5_hardtanh] Epoch: 182 Train Loss: 0.8101 Train Acc: 0.7138 Eval Loss: 0.7730 Eval Acc: 0.7299 (LR: 0.00001000)
[2025-05-27 00:09:44,348]: [LeNet5_hardtanh] Epoch: 183 Train Loss: 0.8139 Train Acc: 0.7130 Eval Loss: 0.7745 Eval Acc: 0.7292 (LR: 0.00001000)
[2025-05-27 00:10:20,945]: [LeNet5_hardtanh] Epoch: 184 Train Loss: 0.8176 Train Acc: 0.7112 Eval Loss: 0.7747 Eval Acc: 0.7292 (LR: 0.00001000)
[2025-05-27 00:10:56,455]: [LeNet5_hardtanh] Epoch: 185 Train Loss: 0.8135 Train Acc: 0.7116 Eval Loss: 0.7731 Eval Acc: 0.7285 (LR: 0.00001000)
[2025-05-27 00:11:27,944]: [LeNet5_hardtanh] Epoch: 186 Train Loss: 0.8177 Train Acc: 0.7118 Eval Loss: 0.7743 Eval Acc: 0.7289 (LR: 0.00001000)
[2025-05-27 00:12:02,808]: [LeNet5_hardtanh] Epoch: 187 Train Loss: 0.8131 Train Acc: 0.7138 Eval Loss: 0.7736 Eval Acc: 0.7297 (LR: 0.00001000)
[2025-05-27 00:12:32,008]: [LeNet5_hardtanh] Epoch: 188 Train Loss: 0.8146 Train Acc: 0.7148 Eval Loss: 0.7752 Eval Acc: 0.7285 (LR: 0.00001000)
[2025-05-27 00:13:04,474]: [LeNet5_hardtanh] Epoch: 189 Train Loss: 0.8111 Train Acc: 0.7150 Eval Loss: 0.7735 Eval Acc: 0.7314 (LR: 0.00001000)
[2025-05-27 00:13:37,940]: [LeNet5_hardtanh] Epoch: 190 Train Loss: 0.8158 Train Acc: 0.7138 Eval Loss: 0.7740 Eval Acc: 0.7304 (LR: 0.00001000)
[2025-05-27 00:14:10,018]: [LeNet5_hardtanh] Epoch: 191 Train Loss: 0.8157 Train Acc: 0.7126 Eval Loss: 0.7749 Eval Acc: 0.7292 (LR: 0.00001000)
[2025-05-27 00:14:43,262]: [LeNet5_hardtanh] Epoch: 192 Train Loss: 0.8123 Train Acc: 0.7145 Eval Loss: 0.7733 Eval Acc: 0.7305 (LR: 0.00001000)
[2025-05-27 00:15:17,589]: [LeNet5_hardtanh] Epoch: 193 Train Loss: 0.8195 Train Acc: 0.7121 Eval Loss: 0.7733 Eval Acc: 0.7305 (LR: 0.00000100)
[2025-05-27 00:15:51,365]: [LeNet5_hardtanh] Epoch: 194 Train Loss: 0.8133 Train Acc: 0.7148 Eval Loss: 0.7732 Eval Acc: 0.7300 (LR: 0.00000100)
[2025-05-27 00:16:24,009]: [LeNet5_hardtanh] Epoch: 195 Train Loss: 0.8109 Train Acc: 0.7156 Eval Loss: 0.7732 Eval Acc: 0.7304 (LR: 0.00000100)
[2025-05-27 00:16:56,254]: [LeNet5_hardtanh] Epoch: 196 Train Loss: 0.8134 Train Acc: 0.7148 Eval Loss: 0.7733 Eval Acc: 0.7303 (LR: 0.00000100)
[2025-05-27 00:17:28,673]: [LeNet5_hardtanh] Epoch: 197 Train Loss: 0.8124 Train Acc: 0.7142 Eval Loss: 0.7733 Eval Acc: 0.7303 (LR: 0.00000100)
[2025-05-27 00:18:02,031]: [LeNet5_hardtanh] Epoch: 198 Train Loss: 0.8078 Train Acc: 0.7156 Eval Loss: 0.7734 Eval Acc: 0.7301 (LR: 0.00000100)
[2025-05-27 00:18:35,766]: [LeNet5_hardtanh] Epoch: 199 Train Loss: 0.8146 Train Acc: 0.7145 Eval Loss: 0.7731 Eval Acc: 0.7303 (LR: 0.00000100)
[2025-05-27 00:19:08,762]: [LeNet5_hardtanh] Epoch: 200 Train Loss: 0.8110 Train Acc: 0.7139 Eval Loss: 0.7730 Eval Acc: 0.7305 (LR: 0.00000100)
[2025-05-27 00:19:08,763]: [LeNet5_hardtanh] Best Eval Accuracy: 0.7314
[2025-05-27 00:19:09,467]: 
Training of full-precision model finished!
[2025-05-27 00:19:09,468]: Model Architecture:
[2025-05-27 00:19:09,636]: LeNet5(
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(in_features=120, out_features=84, bias=True)
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 00:19:09,648]: 
Model Weights:
[2025-05-27 00:19:09,649]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 00:19:10,387]: Sample Values (25 elements): [-0.0939154252409935, -0.006494642235338688, 0.10417354851961136, 0.0711015909910202, -0.20852059125900269, -0.04222997650504112, -0.021869800984859467, 0.019470391795039177, 0.17347419261932373, 0.1357155293226242, -0.05274870991706848, 0.026555562391877174, -0.30157795548439026, -0.023470235988497734, 0.2919776737689972, -0.030446043238043785, 0.03230150789022446, -0.15125945210456848, -0.0715055763721466, -0.05040555074810982, 0.05183317884802818, 0.12858228385448456, 0.03280816972255707, 0.2028726190328598, 0.038502372801303864]
[2025-05-27 00:19:10,446]: Mean: 0.00000867
[2025-05-27 00:19:10,487]: Min: -0.38534990
[2025-05-27 00:19:10,511]: Max: 0.44326741
[2025-05-27 00:19:10,511]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 00:19:10,520]: Sample Values (25 elements): [0.04399210214614868, -0.05147623270750046, 0.02011849917471409, -0.11107908189296722, -0.02178139239549637, -0.03930117189884186, -0.02457616664469242, -0.025732608512043953, -0.08433141559362411, 0.006929793860763311, 0.027751915156841278, -0.0552627295255661, -0.042222317308187485, -0.01726636476814747, -0.04245064780116081, 0.013287868350744247, 0.04156602546572685, -0.03530121222138405, 0.02104712650179863, 0.09865900874137878, -0.05297229066491127, -0.010695600882172585, -0.14805752038955688, 0.033581145107746124, 0.002421734621748328]
[2025-05-27 00:19:10,520]: Mean: -0.00385643
[2025-05-27 00:19:10,521]: Min: -0.66385537
[2025-05-27 00:19:10,521]: Max: 0.44856423
[2025-05-27 00:19:10,521]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 00:19:10,522]: Sample Values (25 elements): [0.06070927157998085, -0.1090494766831398, 0.05937758833169937, -0.11789403855800629, 0.1780785322189331, -0.024670036509633064, -0.013669788837432861, 0.04955308511853218, 0.10696850717067719, 0.09583309292793274, 0.034783992916345596, -0.10719176381826401, -0.06288334727287292, -0.001862138626165688, 0.011527636088430882, -0.20026874542236328, -0.08185659348964691, -0.06277129799127579, 0.036008238792419434, -0.021928565576672554, 0.06591758131980896, -0.1325230896472931, 0.06034843996167183, 0.09690741449594498, -0.028350647538900375]
[2025-05-27 00:19:10,533]: Mean: 0.00249424
[2025-05-27 00:19:10,534]: Min: -0.77530527
[2025-05-27 00:19:10,535]: Max: 0.63340199
[2025-05-27 00:19:10,535]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 00:19:10,544]: Sample Values (25 elements): [0.042880166321992874, 0.08963325619697571, 0.20804531872272491, 0.083247609436512, -0.03429168090224266, -0.1791740357875824, -0.19980576634407043, -0.027330439537763596, 0.11013767123222351, -0.06746682524681091, -0.13409140706062317, 0.270007848739624, 0.003545020706951618, -0.1399749368429184, 0.0030257445760071278, -0.07039976865053177, 0.11583728343248367, 0.020124949514865875, 0.036387182772159576, 0.06688221544027328, 0.009866650216281414, 0.059070710092782974, 0.06933527439832687, -0.05009552836418152, -0.043715618550777435]
[2025-05-27 00:19:10,545]: Mean: 0.00018165
[2025-05-27 00:19:10,545]: Min: -0.52677989
[2025-05-27 00:19:10,545]: Max: 0.53093088
[2025-05-27 00:19:10,545]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 00:19:10,546]: Sample Values (25 elements): [0.38160640001296997, 0.32162582874298096, -0.09128064662218094, -0.21480503678321838, -0.4767003059387207, 0.3888132870197296, -0.09267640113830566, -0.7967361211776733, -0.0658990889787674, -0.11893055588006973, -0.3427278399467468, 0.033750079572200775, -0.33097368478775024, 0.21153992414474487, 0.08013635128736496, 0.08518403023481369, 0.11466324329376221, 0.40855246782302856, -0.24669668078422546, -0.30276262760162354, -0.13639035820960999, 0.11407173424959183, 0.43967628479003906, -0.29554423689842224, 0.07510128617286682]
[2025-05-27 00:19:10,546]: Mean: 0.00237447
[2025-05-27 00:19:10,546]: Min: -0.79673612
[2025-05-27 00:19:10,546]: Max: 1.01561213
[2025-05-27 00:19:10,546]: Checkpoint of model at path [checkpoint/LeNet5_hardtanh.ckpt] will be used for QAT
[2025-05-27 20:42:45,939]: Checkpoint of model at path [checkpoint/LeNet5_hardtanh.ckpt] will be used for QAT
[2025-05-27 20:42:45,939]: 


QAT of LeNet5 with hardtanh down to 4 bits...
[2025-05-27 20:42:46,147]: [LeNet5_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-27 20:42:46,294]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 20:43:18,486]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 1.0174 Train Acc: 0.6415 Eval Loss: 0.9352 Eval Acc: 0.6726 (LR: 0.00100000)
[2025-05-27 20:43:47,394]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 1.0288 Train Acc: 0.6359 Eval Loss: 0.9377 Eval Acc: 0.6715 (LR: 0.00100000)
[2025-05-27 20:44:14,372]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 1.0350 Train Acc: 0.6340 Eval Loss: 0.9521 Eval Acc: 0.6670 (LR: 0.00100000)
[2025-05-27 20:44:43,502]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 1.0315 Train Acc: 0.6363 Eval Loss: 0.9580 Eval Acc: 0.6637 (LR: 0.00100000)
[2025-05-27 20:45:10,578]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 1.0398 Train Acc: 0.6331 Eval Loss: 0.9742 Eval Acc: 0.6589 (LR: 0.00100000)
[2025-05-27 20:45:38,881]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 1.0336 Train Acc: 0.6367 Eval Loss: 0.9764 Eval Acc: 0.6583 (LR: 0.00100000)
[2025-05-27 20:46:08,829]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 1.0431 Train Acc: 0.6322 Eval Loss: 0.9550 Eval Acc: 0.6659 (LR: 0.00010000)
[2025-05-27 20:46:36,110]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.9635 Train Acc: 0.6619 Eval Loss: 0.8792 Eval Acc: 0.6897 (LR: 0.00010000)
[2025-05-27 20:47:03,757]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.9471 Train Acc: 0.6673 Eval Loss: 0.8608 Eval Acc: 0.6965 (LR: 0.00010000)
[2025-05-27 20:47:32,135]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.9381 Train Acc: 0.6692 Eval Loss: 0.8764 Eval Acc: 0.6925 (LR: 0.00010000)
[2025-05-27 20:48:00,665]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.9380 Train Acc: 0.6677 Eval Loss: 0.8777 Eval Acc: 0.6920 (LR: 0.00010000)
[2025-05-27 20:48:27,418]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.9358 Train Acc: 0.6696 Eval Loss: 0.8713 Eval Acc: 0.6949 (LR: 0.00010000)
[2025-05-27 20:48:56,205]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.9316 Train Acc: 0.6718 Eval Loss: 0.8705 Eval Acc: 0.6926 (LR: 0.00010000)
[2025-05-27 20:49:23,942]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.9344 Train Acc: 0.6706 Eval Loss: 0.8622 Eval Acc: 0.6978 (LR: 0.00010000)
[2025-05-27 20:49:53,014]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.9343 Train Acc: 0.6712 Eval Loss: 0.8531 Eval Acc: 0.7024 (LR: 0.00010000)
[2025-05-27 20:50:20,971]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.9370 Train Acc: 0.6695 Eval Loss: 0.8710 Eval Acc: 0.6965 (LR: 0.00010000)
[2025-05-27 20:50:48,770]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.9327 Train Acc: 0.6711 Eval Loss: 0.8625 Eval Acc: 0.6939 (LR: 0.00010000)
[2025-05-27 20:51:17,778]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.9403 Train Acc: 0.6662 Eval Loss: 0.8683 Eval Acc: 0.6939 (LR: 0.00010000)
[2025-05-27 20:51:44,495]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.9293 Train Acc: 0.6714 Eval Loss: 0.8576 Eval Acc: 0.6965 (LR: 0.00010000)
[2025-05-27 20:52:14,601]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.9332 Train Acc: 0.6701 Eval Loss: 0.8687 Eval Acc: 0.6928 (LR: 0.00010000)
[2025-05-27 20:52:42,947]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.9292 Train Acc: 0.6720 Eval Loss: 0.8723 Eval Acc: 0.6913 (LR: 0.00001000)
[2025-05-27 20:53:09,431]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.9063 Train Acc: 0.6805 Eval Loss: 0.8455 Eval Acc: 0.7046 (LR: 0.00001000)
[2025-05-27 20:53:36,298]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.9068 Train Acc: 0.6810 Eval Loss: 0.8503 Eval Acc: 0.7017 (LR: 0.00001000)
[2025-05-27 20:54:04,196]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.9018 Train Acc: 0.6818 Eval Loss: 0.8487 Eval Acc: 0.7007 (LR: 0.00001000)
[2025-05-27 20:54:30,520]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.9071 Train Acc: 0.6815 Eval Loss: 0.8506 Eval Acc: 0.6983 (LR: 0.00001000)
[2025-05-27 20:54:58,608]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.9002 Train Acc: 0.6806 Eval Loss: 0.8409 Eval Acc: 0.7039 (LR: 0.00001000)
[2025-05-27 20:55:36,380]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.8995 Train Acc: 0.6794 Eval Loss: 0.8467 Eval Acc: 0.7011 (LR: 0.00001000)
[2025-05-27 20:56:06,923]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.9076 Train Acc: 0.6808 Eval Loss: 0.8515 Eval Acc: 0.7048 (LR: 0.00001000)
[2025-05-27 20:56:37,298]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.9025 Train Acc: 0.6823 Eval Loss: 0.8532 Eval Acc: 0.7044 (LR: 0.00001000)
[2025-05-27 20:57:03,488]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.9042 Train Acc: 0.6811 Eval Loss: 0.8397 Eval Acc: 0.7092 (LR: 0.00001000)
[2025-05-27 20:57:30,279]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.8977 Train Acc: 0.6844 Eval Loss: 0.8470 Eval Acc: 0.7061 (LR: 0.00001000)
[2025-05-27 20:57:58,287]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.9019 Train Acc: 0.6826 Eval Loss: 0.8412 Eval Acc: 0.7066 (LR: 0.00001000)
[2025-05-27 20:58:28,343]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.8993 Train Acc: 0.6809 Eval Loss: 0.8443 Eval Acc: 0.7107 (LR: 0.00001000)
[2025-05-27 20:58:59,722]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.9066 Train Acc: 0.6813 Eval Loss: 0.8505 Eval Acc: 0.6997 (LR: 0.00001000)
[2025-05-27 20:59:29,414]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.9002 Train Acc: 0.6846 Eval Loss: 0.8390 Eval Acc: 0.7088 (LR: 0.00001000)
[2025-05-27 20:59:58,252]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.9029 Train Acc: 0.6806 Eval Loss: 0.8441 Eval Acc: 0.7062 (LR: 0.00001000)
[2025-05-27 21:00:26,183]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.9047 Train Acc: 0.6793 Eval Loss: 0.8503 Eval Acc: 0.6988 (LR: 0.00001000)
[2025-05-27 21:00:55,316]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.9014 Train Acc: 0.6846 Eval Loss: 0.8391 Eval Acc: 0.7068 (LR: 0.00001000)
[2025-05-27 21:01:22,888]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.9121 Train Acc: 0.6804 Eval Loss: 0.8451 Eval Acc: 0.7046 (LR: 0.00001000)
[2025-05-27 21:01:52,065]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.9039 Train Acc: 0.6824 Eval Loss: 0.8414 Eval Acc: 0.7051 (LR: 0.00001000)
[2025-05-27 21:02:19,293]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.9075 Train Acc: 0.6798 Eval Loss: 0.8523 Eval Acc: 0.6996 (LR: 0.00000100)
[2025-05-27 21:02:47,771]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.8886 Train Acc: 0.6847 Eval Loss: 0.8425 Eval Acc: 0.7073 (LR: 0.00000100)
[2025-05-27 21:03:15,583]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.8847 Train Acc: 0.6885 Eval Loss: 0.8368 Eval Acc: 0.7082 (LR: 0.00000100)
[2025-05-27 21:03:42,212]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.8920 Train Acc: 0.6866 Eval Loss: 0.8366 Eval Acc: 0.7041 (LR: 0.00000100)
[2025-05-27 21:04:09,359]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.8940 Train Acc: 0.6879 Eval Loss: 0.8338 Eval Acc: 0.7062 (LR: 0.00000100)
[2025-05-27 21:04:36,587]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.8913 Train Acc: 0.6875 Eval Loss: 0.8291 Eval Acc: 0.7117 (LR: 0.00000100)
[2025-05-27 21:05:04,397]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.8892 Train Acc: 0.6878 Eval Loss: 0.8414 Eval Acc: 0.7094 (LR: 0.00000100)
[2025-05-27 21:05:31,605]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.8908 Train Acc: 0.6872 Eval Loss: 0.8289 Eval Acc: 0.7100 (LR: 0.00000100)
[2025-05-27 21:05:58,737]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.8928 Train Acc: 0.6854 Eval Loss: 0.8378 Eval Acc: 0.7080 (LR: 0.00000100)
[2025-05-27 21:06:26,144]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.8920 Train Acc: 0.6849 Eval Loss: 0.8388 Eval Acc: 0.7085 (LR: 0.00000100)
[2025-05-27 21:06:52,906]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.8946 Train Acc: 0.6855 Eval Loss: 0.8406 Eval Acc: 0.7060 (LR: 0.00000100)
[2025-05-27 21:07:19,469]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.8919 Train Acc: 0.6860 Eval Loss: 0.8338 Eval Acc: 0.7099 (LR: 0.00000100)
[2025-05-27 21:07:46,104]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.8943 Train Acc: 0.6848 Eval Loss: 0.8319 Eval Acc: 0.7082 (LR: 0.00000100)
[2025-05-27 21:08:13,021]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.8973 Train Acc: 0.6857 Eval Loss: 0.8334 Eval Acc: 0.7113 (LR: 0.00000010)
[2025-05-27 21:08:39,695]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.8881 Train Acc: 0.6854 Eval Loss: 0.8297 Eval Acc: 0.7085 (LR: 0.00000010)
[2025-05-27 21:09:06,657]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.8876 Train Acc: 0.6865 Eval Loss: 0.8327 Eval Acc: 0.7110 (LR: 0.00000010)
[2025-05-27 21:09:34,780]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.8843 Train Acc: 0.6903 Eval Loss: 0.8328 Eval Acc: 0.7057 (LR: 0.00000010)
[2025-05-27 21:10:06,793]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.8857 Train Acc: 0.6886 Eval Loss: 0.8284 Eval Acc: 0.7068 (LR: 0.00000010)
[2025-05-27 21:10:37,716]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.8875 Train Acc: 0.6865 Eval Loss: 0.8329 Eval Acc: 0.7054 (LR: 0.00000010)
[2025-05-27 21:11:06,339]: [LeNet5_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.8837 Train Acc: 0.6878 Eval Loss: 0.8359 Eval Acc: 0.7122 (LR: 0.00000010)
[2025-05-27 21:11:06,354]: [LeNet5_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.7122
[2025-05-27 21:11:06,817]: 


Quantization of model down to 4 bits finished
[2025-05-27 21:11:06,817]: Model Architecture:
[2025-05-27 21:11:09,014]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0874], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7826225757598877, max_val=0.5284073352813721)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1017], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.852599024772644, max_val=0.6732836961746216)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0678], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5151664018630981, max_val=0.5012470483779907)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 21:11:09,043]: 
Model Weights:
[2025-05-27 21:11:09,043]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 21:11:09,388]: Sample Values (25 elements): [-0.12944559752941132, -0.37782853841781616, -0.018927304074168205, 0.07649730890989304, 0.05397019162774086, -0.011020229198038578, -0.023722244426608086, 0.19366049766540527, -0.0974234789609909, 0.004352778196334839, -0.10459019988775253, -0.19423548877239227, 0.11012119799852371, 0.013054188340902328, -0.31343773007392883, 0.056085653603076935, 0.20566755533218384, -0.11386514455080032, -0.2640901207923889, -0.08532460033893585, -0.056929465383291245, -0.12070001661777496, -0.21976469457149506, -0.029962826520204544, -0.15141767263412476]
[2025-05-27 21:11:09,897]: Mean: 0.00010543
[2025-05-27 21:11:10,069]: Min: -0.37782854
[2025-05-27 21:11:10,155]: Max: 0.43659526
[2025-05-27 21:11:10,157]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 21:11:10,158]: Sample Values (25 elements): [0.0, 0.08740200102329254, 0.0, 0.0, 0.0, -0.08740200102329254, 0.0, -0.08740200102329254, -0.17480400204658508, -0.08740200102329254, 0.08740200102329254, -0.08740200102329254, 0.0, 0.0, 0.08740200102329254, 0.0, -0.08740200102329254, 0.08740200102329254, 0.08740200102329254, 0.0, 0.0, 0.0, 0.0, 0.08740200102329254, 0.0]
[2025-05-27 21:11:10,183]: Mean: -0.00426085
[2025-05-27 21:11:10,184]: Min: -0.78661799
[2025-05-27 21:11:10,184]: Max: 0.52441204
[2025-05-27 21:11:10,186]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 21:11:10,187]: Sample Values (25 elements): [0.0, -0.1017255187034607, 0.0, 0.1017255187034607, 0.1017255187034607, 0.1017255187034607, 0.1017255187034607, 0.0, 0.0, 0.0, 0.2034510374069214, 0.0, 0.0, 0.0, 0.3051765561103821, -0.1017255187034607, -0.2034510374069214, -0.1017255187034607, 0.0, -0.1017255187034607, 0.1017255187034607, 0.1017255187034607, 0.0, 0.0, 0.0]
[2025-05-27 21:11:10,187]: Mean: 0.00281229
[2025-05-27 21:11:10,187]: Min: -0.81380415
[2025-05-27 21:11:10,188]: Max: 0.71207863
[2025-05-27 21:11:10,189]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 21:11:10,190]: Sample Values (25 elements): [0.0677608996629715, -0.135521799325943, 0.0, 0.135521799325943, -0.0677608996629715, -0.135521799325943, 0.0, -0.135521799325943, -0.135521799325943, -0.135521799325943, 0.3388044834136963, 0.135521799325943, -0.0677608996629715, 0.135521799325943, -0.0677608996629715, 0.135521799325943, -0.135521799325943, 0.0677608996629715, -0.271043598651886, -0.2032826989889145, -0.2032826989889145, -0.0677608996629715, 0.0, -0.0677608996629715, 0.0677608996629715]
[2025-05-27 21:11:10,190]: Mean: 0.00034956
[2025-05-27 21:11:10,190]: Min: -0.54208720
[2025-05-27 21:11:10,191]: Max: 0.47432631
[2025-05-27 21:11:10,191]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 21:11:10,259]: Sample Values (25 elements): [-0.038874950259923935, -0.29636797308921814, 0.03821703419089317, 0.1976822316646576, 0.15687903761863708, -0.18916448950767517, 0.09392368048429489, -0.05938079580664635, -0.2088654488325119, 0.02507377415895462, -0.1823674440383911, -0.27312228083610535, 0.2259065955877304, -0.044459760189056396, -0.267050176858902, 0.15883828699588776, 0.18034547567367554, -0.22905442118644714, -0.1854439526796341, 0.2813209891319275, -0.13750214874744415, 0.14523988962173462, -0.2535777986049652, -0.26088055968284607, 0.01104961708188057]
[2025-05-27 21:11:10,260]: Mean: 0.00225046
[2025-05-27 21:11:10,260]: Min: -0.69802678
[2025-05-27 21:11:10,260]: Max: 0.94632643
[2025-05-27 21:11:10,261]: 


QAT of LeNet5 with hardtanh down to 3 bits...
[2025-05-27 21:11:18,028]: [LeNet5_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-27 21:11:18,060]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 21:11:49,129]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 1.2433 Train Acc: 0.5629 Eval Loss: 1.0919 Eval Acc: 0.6204 (LR: 0.00100000)
[2025-05-27 21:12:21,094]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 1.2097 Train Acc: 0.5737 Eval Loss: 1.1060 Eval Acc: 0.6103 (LR: 0.00100000)
[2025-05-27 21:12:54,079]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 1.2131 Train Acc: 0.5737 Eval Loss: 1.0850 Eval Acc: 0.6190 (LR: 0.00100000)
[2025-05-27 21:13:27,776]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 1.2037 Train Acc: 0.5742 Eval Loss: 1.1035 Eval Acc: 0.6183 (LR: 0.00100000)
[2025-05-27 21:14:00,865]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 1.2108 Train Acc: 0.5692 Eval Loss: 1.0924 Eval Acc: 0.6174 (LR: 0.00100000)
[2025-05-27 21:14:31,161]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 1.2206 Train Acc: 0.5680 Eval Loss: 1.1048 Eval Acc: 0.6064 (LR: 0.00100000)
[2025-05-27 21:15:01,830]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 1.2243 Train Acc: 0.5677 Eval Loss: 1.1048 Eval Acc: 0.6051 (LR: 0.00100000)
[2025-05-27 21:15:30,558]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 1.2264 Train Acc: 0.5680 Eval Loss: 1.1147 Eval Acc: 0.6035 (LR: 0.00100000)
[2025-05-27 21:15:58,345]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 1.2319 Train Acc: 0.5632 Eval Loss: 1.1188 Eval Acc: 0.6056 (LR: 0.00010000)
[2025-05-27 21:16:25,304]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 1.1354 Train Acc: 0.5967 Eval Loss: 1.0167 Eval Acc: 0.6392 (LR: 0.00010000)
[2025-05-27 21:16:52,842]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 1.1143 Train Acc: 0.6061 Eval Loss: 1.0129 Eval Acc: 0.6391 (LR: 0.00010000)
[2025-05-27 21:17:22,636]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 1.1112 Train Acc: 0.6051 Eval Loss: 1.0264 Eval Acc: 0.6403 (LR: 0.00010000)
[2025-05-27 21:17:53,264]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 1.1115 Train Acc: 0.6067 Eval Loss: 1.0151 Eval Acc: 0.6450 (LR: 0.00010000)
[2025-05-27 21:18:23,063]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 1.1130 Train Acc: 0.6054 Eval Loss: 1.0040 Eval Acc: 0.6495 (LR: 0.00010000)
[2025-05-27 21:18:50,538]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 1.1052 Train Acc: 0.6096 Eval Loss: 1.0019 Eval Acc: 0.6469 (LR: 0.00010000)
[2025-05-27 21:19:19,788]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 1.1083 Train Acc: 0.6083 Eval Loss: 1.0112 Eval Acc: 0.6489 (LR: 0.00010000)
[2025-05-27 21:19:47,526]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 1.1075 Train Acc: 0.6094 Eval Loss: 1.0008 Eval Acc: 0.6506 (LR: 0.00010000)
[2025-05-27 21:20:15,699]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 1.1052 Train Acc: 0.6114 Eval Loss: 1.0072 Eval Acc: 0.6451 (LR: 0.00010000)
[2025-05-27 21:20:44,268]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 1.1098 Train Acc: 0.6061 Eval Loss: 1.0102 Eval Acc: 0.6464 (LR: 0.00010000)
[2025-05-27 21:21:12,177]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 1.1076 Train Acc: 0.6058 Eval Loss: 1.0004 Eval Acc: 0.6456 (LR: 0.00010000)
[2025-05-27 21:21:40,502]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 1.1163 Train Acc: 0.6063 Eval Loss: 1.0213 Eval Acc: 0.6446 (LR: 0.00010000)
[2025-05-27 21:22:09,509]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 1.1061 Train Acc: 0.6070 Eval Loss: 1.0424 Eval Acc: 0.6306 (LR: 0.00010000)
[2025-05-27 21:22:36,981]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 1.1092 Train Acc: 0.6067 Eval Loss: 1.0270 Eval Acc: 0.6421 (LR: 0.00010000)
[2025-05-27 21:23:03,425]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 1.1121 Train Acc: 0.6061 Eval Loss: 1.0088 Eval Acc: 0.6470 (LR: 0.00010000)
[2025-05-27 21:23:31,578]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 1.1116 Train Acc: 0.6095 Eval Loss: 1.0178 Eval Acc: 0.6451 (LR: 0.00010000)
[2025-05-27 21:23:59,190]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 1.1130 Train Acc: 0.6052 Eval Loss: 1.0020 Eval Acc: 0.6478 (LR: 0.00001000)
[2025-05-27 21:24:26,114]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 1.0718 Train Acc: 0.6216 Eval Loss: 0.9670 Eval Acc: 0.6628 (LR: 0.00001000)
[2025-05-27 21:24:52,664]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 1.0737 Train Acc: 0.6185 Eval Loss: 0.9832 Eval Acc: 0.6490 (LR: 0.00001000)
[2025-05-27 21:25:19,444]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 1.0729 Train Acc: 0.6221 Eval Loss: 0.9916 Eval Acc: 0.6518 (LR: 0.00001000)
[2025-05-27 21:25:46,153]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 1.0761 Train Acc: 0.6165 Eval Loss: 0.9829 Eval Acc: 0.6543 (LR: 0.00001000)
[2025-05-27 21:26:14,836]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 1.0695 Train Acc: 0.6225 Eval Loss: 0.9894 Eval Acc: 0.6518 (LR: 0.00001000)
[2025-05-27 21:26:41,967]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 1.0673 Train Acc: 0.6227 Eval Loss: 0.9800 Eval Acc: 0.6551 (LR: 0.00001000)
[2025-05-27 21:27:08,809]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 1.0692 Train Acc: 0.6219 Eval Loss: 0.9979 Eval Acc: 0.6457 (LR: 0.00000100)
[2025-05-27 21:27:35,715]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 1.0487 Train Acc: 0.6317 Eval Loss: 0.9671 Eval Acc: 0.6580 (LR: 0.00000100)
[2025-05-27 21:28:02,740]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 1.0500 Train Acc: 0.6275 Eval Loss: 0.9806 Eval Acc: 0.6533 (LR: 0.00000100)
[2025-05-27 21:28:31,922]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 1.0609 Train Acc: 0.6266 Eval Loss: 0.9718 Eval Acc: 0.6619 (LR: 0.00000100)
[2025-05-27 21:29:01,581]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 1.0589 Train Acc: 0.6257 Eval Loss: 0.9689 Eval Acc: 0.6619 (LR: 0.00000100)
[2025-05-27 21:29:31,374]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 1.0603 Train Acc: 0.6263 Eval Loss: 0.9697 Eval Acc: 0.6563 (LR: 0.00000100)
[2025-05-27 21:30:01,257]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 1.0603 Train Acc: 0.6255 Eval Loss: 0.9757 Eval Acc: 0.6568 (LR: 0.00000010)
[2025-05-27 21:30:28,802]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 1.0468 Train Acc: 0.6315 Eval Loss: 0.9614 Eval Acc: 0.6596 (LR: 0.00000010)
[2025-05-27 21:30:58,495]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 1.0453 Train Acc: 0.6304 Eval Loss: 0.9639 Eval Acc: 0.6625 (LR: 0.00000010)
[2025-05-27 21:31:26,553]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 1.0467 Train Acc: 0.6313 Eval Loss: 0.9630 Eval Acc: 0.6586 (LR: 0.00000010)
[2025-05-27 21:31:54,691]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 1.0517 Train Acc: 0.6266 Eval Loss: 0.9741 Eval Acc: 0.6567 (LR: 0.00000010)
[2025-05-27 21:32:22,942]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 1.0516 Train Acc: 0.6292 Eval Loss: 0.9682 Eval Acc: 0.6618 (LR: 0.00000010)
[2025-05-27 21:32:53,125]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 1.0559 Train Acc: 0.6267 Eval Loss: 0.9651 Eval Acc: 0.6605 (LR: 0.00000010)
[2025-05-27 21:33:21,286]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 1.0575 Train Acc: 0.6258 Eval Loss: 0.9545 Eval Acc: 0.6630 (LR: 0.00000010)
[2025-05-27 21:33:48,977]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 1.0602 Train Acc: 0.6272 Eval Loss: 0.9612 Eval Acc: 0.6592 (LR: 0.00000010)
[2025-05-27 21:34:16,704]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 1.0578 Train Acc: 0.6256 Eval Loss: 0.9651 Eval Acc: 0.6607 (LR: 0.00000010)
[2025-05-27 21:34:44,346]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 1.0574 Train Acc: 0.6274 Eval Loss: 0.9606 Eval Acc: 0.6602 (LR: 0.00000010)
[2025-05-27 21:35:11,918]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 1.0598 Train Acc: 0.6226 Eval Loss: 0.9667 Eval Acc: 0.6571 (LR: 0.00000010)
[2025-05-27 21:35:39,366]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 1.0543 Train Acc: 0.6284 Eval Loss: 0.9583 Eval Acc: 0.6647 (LR: 0.00000010)
[2025-05-27 21:36:07,869]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 1.0528 Train Acc: 0.6267 Eval Loss: 0.9651 Eval Acc: 0.6607 (LR: 0.00000010)
[2025-05-27 21:36:34,882]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 1.0547 Train Acc: 0.6297 Eval Loss: 0.9725 Eval Acc: 0.6581 (LR: 0.00000010)
[2025-05-27 21:37:03,633]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 1.0543 Train Acc: 0.6270 Eval Loss: 0.9649 Eval Acc: 0.6573 (LR: 0.00000010)
[2025-05-27 21:37:30,565]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 1.0580 Train Acc: 0.6271 Eval Loss: 0.9834 Eval Acc: 0.6574 (LR: 0.00000010)
[2025-05-27 21:37:57,274]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 1.0552 Train Acc: 0.6287 Eval Loss: 0.9590 Eval Acc: 0.6623 (LR: 0.00000010)
[2025-05-27 21:38:24,377]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 1.0546 Train Acc: 0.6283 Eval Loss: 0.9699 Eval Acc: 0.6599 (LR: 0.00000010)
[2025-05-27 21:38:52,989]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 1.0507 Train Acc: 0.6282 Eval Loss: 0.9666 Eval Acc: 0.6585 (LR: 0.00000010)
[2025-05-27 21:39:21,864]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 1.0526 Train Acc: 0.6279 Eval Loss: 0.9702 Eval Acc: 0.6568 (LR: 0.00000010)
[2025-05-27 21:39:50,961]: [LeNet5_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 1.0514 Train Acc: 0.6265 Eval Loss: 0.9724 Eval Acc: 0.6572 (LR: 0.00000010)
[2025-05-27 21:39:50,961]: [LeNet5_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.6647
[2025-05-27 21:39:50,976]: 


Quantization of model down to 3 bits finished
[2025-05-27 21:39:50,976]: Model Architecture:
[2025-05-27 21:39:51,075]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2251], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9292949438095093, max_val=0.6464135646820068)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2350], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9540479183197021, max_val=0.690696120262146)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1529], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5398489236831665, max_val=0.5306081771850586)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 21:39:51,075]: 
Model Weights:
[2025-05-27 21:39:51,075]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 21:39:51,076]: Sample Values (25 elements): [-0.19031204283237457, 0.010161898098886013, -0.11623406410217285, -0.03954147920012474, -0.06425860524177551, 0.08603043109178543, -0.04388383403420448, 0.12116900086402893, 0.060631413012742996, 0.16306965053081512, -0.0007486580871045589, 0.05806693807244301, -0.049449969083070755, -0.0800618976354599, -0.01926126331090927, -0.040769804269075394, -0.32365882396698, 0.07102077454328537, 0.3109545111656189, -0.19773685932159424, 0.06468930095434189, 0.05713892728090286, -0.12716229259967804, 0.07361023873090744, -0.08123286813497543]
[2025-05-27 21:39:51,077]: Mean: -0.00003485
[2025-05-27 21:39:51,077]: Min: -0.37981051
[2025-05-27 21:39:51,077]: Max: 0.42520186
[2025-05-27 21:39:51,078]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 21:39:51,079]: Sample Values (25 elements): [-0.22510123252868652, -0.22510123252868652, 0.0, 0.0, 0.0, 0.0, -0.22510123252868652, 0.0, 0.22510123252868652, 0.0, 0.0, -0.22510123252868652, 0.22510123252868652, 0.0, -0.22510123252868652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22510123252868652, 0.0, 0.0, -0.22510123252868652, 0.0]
[2025-05-27 21:39:51,080]: Mean: -0.00600270
[2025-05-27 21:39:51,080]: Min: -0.90040493
[2025-05-27 21:39:51,081]: Max: 0.67530370
[2025-05-27 21:39:51,082]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 21:39:51,083]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23496344685554504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23496344685554504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23496344685554504, 0.0, 0.23496344685554504, 0.0, 0.23496344685554504]
[2025-05-27 21:39:51,083]: Mean: 0.00324054
[2025-05-27 21:39:51,084]: Min: -0.93985379
[2025-05-27 21:39:51,084]: Max: 0.70489037
[2025-05-27 21:39:51,085]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 21:39:51,086]: Sample Values (25 elements): [0.0, 0.0, 0.15292245149612427, -0.15292245149612427, -0.15292245149612427, 0.0, -0.15292245149612427, -0.15292245149612427, -0.30584490299224854, 0.15292245149612427, 0.0, 0.15292245149612427, 0.15292245149612427, 0.15292245149612427, 0.15292245149612427, -0.15292245149612427, 0.30584490299224854, 0.0, 0.0, 0.0, -0.15292245149612427, 0.15292245149612427, 0.15292245149612427, -0.15292245149612427, 0.15292245149612427]
[2025-05-27 21:39:51,086]: Mean: -0.00080406
[2025-05-27 21:39:51,087]: Min: -0.61168981
[2025-05-27 21:39:51,087]: Max: 0.45876735
[2025-05-27 21:39:51,087]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 21:39:51,087]: Sample Values (25 elements): [-0.053401608020067215, -0.09082324802875519, -0.17176635563373566, 0.0890970453619957, 0.2757260203361511, 0.08513251692056656, 0.30279338359832764, -0.24088409543037415, 0.04052051529288292, -0.130830317735672, -0.21628716588020325, 0.257962167263031, -0.15043610334396362, 0.14704389870166779, -0.058427780866622925, 0.22376178205013275, 0.09718114137649536, -0.03777006268501282, 0.08915304392576218, -0.2992401719093323, 0.01444130390882492, 0.15747006237506866, 0.1772645264863968, 0.32295265793800354, -0.08311264216899872]
[2025-05-27 21:39:51,088]: Mean: 0.00179507
[2025-05-27 21:39:51,088]: Min: -0.50953680
[2025-05-27 21:39:51,088]: Max: 0.78841144
[2025-05-27 21:39:51,088]: 


QAT of LeNet5 with hardtanh down to 2 bits...
[2025-05-27 21:39:51,189]: [LeNet5_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-27 21:39:51,201]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 21:40:18,353]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 2.1007 Train Acc: 0.2919 Eval Loss: 1.7410 Eval Acc: 0.3843 (LR: 0.00100000)
[2025-05-27 21:40:46,561]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 1.7520 Train Acc: 0.3674 Eval Loss: 1.5750 Eval Acc: 0.4287 (LR: 0.00100000)
[2025-05-27 21:41:16,092]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 1.6536 Train Acc: 0.4024 Eval Loss: 1.5084 Eval Acc: 0.4548 (LR: 0.00100000)
[2025-05-27 21:41:44,485]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 1.5934 Train Acc: 0.4231 Eval Loss: 1.4499 Eval Acc: 0.4756 (LR: 0.00100000)
[2025-05-27 21:42:12,579]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 1.5580 Train Acc: 0.4379 Eval Loss: 1.4527 Eval Acc: 0.4730 (LR: 0.00100000)
[2025-05-27 21:42:41,238]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 1.5452 Train Acc: 0.4439 Eval Loss: 1.4412 Eval Acc: 0.4730 (LR: 0.00100000)
[2025-05-27 21:43:10,368]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 1.5312 Train Acc: 0.4456 Eval Loss: 1.3840 Eval Acc: 0.4993 (LR: 0.00100000)
[2025-05-27 21:43:38,754]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 1.5230 Train Acc: 0.4509 Eval Loss: 1.3893 Eval Acc: 0.4972 (LR: 0.00100000)
[2025-05-27 21:44:06,542]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 1.5028 Train Acc: 0.4543 Eval Loss: 1.3769 Eval Acc: 0.4999 (LR: 0.00100000)
[2025-05-27 21:44:34,348]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 1.4953 Train Acc: 0.4595 Eval Loss: 1.4093 Eval Acc: 0.4946 (LR: 0.00100000)
[2025-05-27 21:45:02,953]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 1.4855 Train Acc: 0.4637 Eval Loss: 1.3863 Eval Acc: 0.4974 (LR: 0.00100000)
[2025-05-27 21:45:31,313]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 1.4765 Train Acc: 0.4656 Eval Loss: 1.3661 Eval Acc: 0.5043 (LR: 0.00100000)
[2025-05-27 21:45:58,290]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 1.4846 Train Acc: 0.4644 Eval Loss: 1.3433 Eval Acc: 0.5176 (LR: 0.00100000)
[2025-05-27 21:46:26,736]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 1.4680 Train Acc: 0.4683 Eval Loss: 1.3435 Eval Acc: 0.5114 (LR: 0.00100000)
[2025-05-27 21:46:58,424]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 1.4722 Train Acc: 0.4704 Eval Loss: 1.3833 Eval Acc: 0.5066 (LR: 0.00100000)
[2025-05-27 21:47:25,384]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 1.4653 Train Acc: 0.4734 Eval Loss: 1.3317 Eval Acc: 0.5246 (LR: 0.00100000)
[2025-05-27 21:47:54,739]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 1.4656 Train Acc: 0.4727 Eval Loss: 1.3478 Eval Acc: 0.5147 (LR: 0.00100000)
[2025-05-27 21:48:23,716]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 1.4620 Train Acc: 0.4745 Eval Loss: 1.3370 Eval Acc: 0.5166 (LR: 0.00100000)
[2025-05-27 21:48:51,063]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 1.4496 Train Acc: 0.4790 Eval Loss: 1.3640 Eval Acc: 0.5124 (LR: 0.00100000)
[2025-05-27 21:49:22,562]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 1.4420 Train Acc: 0.4812 Eval Loss: 1.3544 Eval Acc: 0.5194 (LR: 0.00100000)
[2025-05-27 21:49:49,459]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 1.4431 Train Acc: 0.4807 Eval Loss: 1.3826 Eval Acc: 0.5094 (LR: 0.00100000)
[2025-05-27 21:50:15,980]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 1.4516 Train Acc: 0.4783 Eval Loss: 1.3213 Eval Acc: 0.5227 (LR: 0.00100000)
[2025-05-27 21:50:42,093]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 1.4428 Train Acc: 0.4783 Eval Loss: 1.3170 Eval Acc: 0.5254 (LR: 0.00100000)
[2025-05-27 21:51:08,576]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 1.4392 Train Acc: 0.4808 Eval Loss: 1.4048 Eval Acc: 0.4916 (LR: 0.00100000)
[2025-05-27 21:51:35,223]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 1.4425 Train Acc: 0.4820 Eval Loss: 1.3446 Eval Acc: 0.5144 (LR: 0.00100000)
[2025-05-27 21:52:02,552]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 1.4416 Train Acc: 0.4802 Eval Loss: 1.3450 Eval Acc: 0.5138 (LR: 0.00100000)
[2025-05-27 21:52:29,904]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 1.4391 Train Acc: 0.4793 Eval Loss: 1.3736 Eval Acc: 0.5030 (LR: 0.00100000)
[2025-05-27 21:52:56,410]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 1.4473 Train Acc: 0.4773 Eval Loss: 1.3564 Eval Acc: 0.5163 (LR: 0.00100000)
[2025-05-27 21:53:23,589]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 1.4385 Train Acc: 0.4814 Eval Loss: 1.3195 Eval Acc: 0.5239 (LR: 0.00010000)
[2025-05-27 21:53:50,345]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 1.3898 Train Acc: 0.4995 Eval Loss: 1.2840 Eval Acc: 0.5401 (LR: 0.00010000)
[2025-05-27 21:54:17,771]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 1.3796 Train Acc: 0.5046 Eval Loss: 1.2876 Eval Acc: 0.5437 (LR: 0.00010000)
[2025-05-27 21:54:44,101]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 1.3743 Train Acc: 0.5084 Eval Loss: 1.2719 Eval Acc: 0.5481 (LR: 0.00010000)
[2025-05-27 21:55:10,822]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 1.3684 Train Acc: 0.5095 Eval Loss: 1.2682 Eval Acc: 0.5424 (LR: 0.00010000)
[2025-05-27 21:55:40,138]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 1.3598 Train Acc: 0.5121 Eval Loss: 1.2537 Eval Acc: 0.5485 (LR: 0.00010000)
[2025-05-27 21:56:06,910]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 1.3603 Train Acc: 0.5113 Eval Loss: 1.2633 Eval Acc: 0.5444 (LR: 0.00010000)
[2025-05-27 21:56:34,051]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 1.3587 Train Acc: 0.5120 Eval Loss: 1.2691 Eval Acc: 0.5435 (LR: 0.00010000)
[2025-05-27 21:57:00,526]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 1.3636 Train Acc: 0.5084 Eval Loss: 1.2618 Eval Acc: 0.5478 (LR: 0.00010000)
[2025-05-27 21:57:27,627]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 1.3637 Train Acc: 0.5099 Eval Loss: 1.2793 Eval Acc: 0.5423 (LR: 0.00010000)
[2025-05-27 21:57:54,148]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 1.3658 Train Acc: 0.5097 Eval Loss: 1.2759 Eval Acc: 0.5412 (LR: 0.00010000)
[2025-05-27 21:58:21,372]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 1.3656 Train Acc: 0.5104 Eval Loss: 1.2641 Eval Acc: 0.5457 (LR: 0.00001000)
[2025-05-27 21:58:47,919]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 1.3489 Train Acc: 0.5150 Eval Loss: 1.2539 Eval Acc: 0.5499 (LR: 0.00001000)
[2025-05-27 21:59:14,952]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 1.3496 Train Acc: 0.5163 Eval Loss: 1.2443 Eval Acc: 0.5539 (LR: 0.00001000)
[2025-05-27 21:59:42,439]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 1.3470 Train Acc: 0.5186 Eval Loss: 1.2502 Eval Acc: 0.5505 (LR: 0.00001000)
[2025-05-27 22:00:09,505]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 1.3502 Train Acc: 0.5155 Eval Loss: 1.2522 Eval Acc: 0.5512 (LR: 0.00001000)
[2025-05-27 22:00:36,670]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 1.3489 Train Acc: 0.5163 Eval Loss: 1.2499 Eval Acc: 0.5507 (LR: 0.00001000)
[2025-05-27 22:01:03,442]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 1.3469 Train Acc: 0.5183 Eval Loss: 1.2366 Eval Acc: 0.5570 (LR: 0.00001000)
[2025-05-27 22:01:30,694]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 1.3492 Train Acc: 0.5157 Eval Loss: 1.2463 Eval Acc: 0.5507 (LR: 0.00001000)
[2025-05-27 22:01:57,353]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 1.3438 Train Acc: 0.5192 Eval Loss: 1.2619 Eval Acc: 0.5449 (LR: 0.00001000)
[2025-05-27 22:02:24,742]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 1.3462 Train Acc: 0.5176 Eval Loss: 1.2384 Eval Acc: 0.5554 (LR: 0.00001000)
[2025-05-27 22:02:51,927]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 1.3404 Train Acc: 0.5205 Eval Loss: 1.2481 Eval Acc: 0.5505 (LR: 0.00001000)
[2025-05-27 22:03:19,131]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 1.3440 Train Acc: 0.5155 Eval Loss: 1.2456 Eval Acc: 0.5540 (LR: 0.00001000)
[2025-05-27 22:03:46,113]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 1.3488 Train Acc: 0.5168 Eval Loss: 1.2471 Eval Acc: 0.5553 (LR: 0.00000100)
[2025-05-27 22:04:13,283]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 1.3352 Train Acc: 0.5227 Eval Loss: 1.2381 Eval Acc: 0.5571 (LR: 0.00000100)
[2025-05-27 22:04:42,069]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 1.3339 Train Acc: 0.5243 Eval Loss: 1.2308 Eval Acc: 0.5628 (LR: 0.00000100)
[2025-05-27 22:05:09,588]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 1.3352 Train Acc: 0.5242 Eval Loss: 1.2385 Eval Acc: 0.5568 (LR: 0.00000100)
[2025-05-27 22:05:39,742]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 1.3351 Train Acc: 0.5231 Eval Loss: 1.2387 Eval Acc: 0.5558 (LR: 0.00000100)
[2025-05-27 22:06:13,924]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 1.3426 Train Acc: 0.5177 Eval Loss: 1.2393 Eval Acc: 0.5537 (LR: 0.00000100)
[2025-05-27 22:06:45,981]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 1.3350 Train Acc: 0.5207 Eval Loss: 1.2396 Eval Acc: 0.5548 (LR: 0.00000100)
[2025-05-27 22:07:17,064]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 1.3399 Train Acc: 0.5190 Eval Loss: 1.2445 Eval Acc: 0.5555 (LR: 0.00000100)
[2025-05-27 22:07:52,499]: [LeNet5_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 1.3421 Train Acc: 0.5162 Eval Loss: 1.2468 Eval Acc: 0.5548 (LR: 0.00000010)
[2025-05-27 22:07:52,499]: [LeNet5_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.5628
[2025-05-27 22:07:52,517]: 


Quantization of model down to 2 bits finished
[2025-05-27 22:07:52,517]: Model Architecture:
[2025-05-27 22:07:52,581]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8312], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.236573338508606, max_val=1.2569055557250977)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5385], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8079918622970581, max_val=0.8075898885726929)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4058], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.599089503288269, max_val=0.6183778643608093)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-27 22:07:52,581]: 
Model Weights:
[2025-05-27 22:07:52,581]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-27 22:07:52,606]: Sample Values (25 elements): [-0.12340286374092102, -0.1373845934867859, 0.4489687979221344, 0.026358744129538536, 0.005580003838986158, -0.12425141036510468, -0.11948467046022415, -0.037002284079790115, 0.02487032674252987, -0.2577965259552002, 0.09685897827148438, 0.0035353864077478647, -0.1266493946313858, 0.13805218040943146, 0.06268627196550369, -0.03858441114425659, 0.5243610143661499, -0.012780306860804558, 0.009205300360918045, 0.11963074654340744, -0.06326345354318619, -0.3374161720275879, 0.11114435642957687, -0.08981496095657349, 0.03433200716972351]
[2025-05-27 22:07:52,608]: Mean: -0.00103822
[2025-05-27 22:07:52,618]: Min: -0.58663237
[2025-05-27 22:07:52,629]: Max: 0.59794074
[2025-05-27 22:07:52,645]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-27 22:07:52,646]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.8311595916748047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8311595916748047, 0.8311595916748047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8311595916748047, 0.0, -0.8311595916748047]
[2025-05-27 22:07:52,646]: Mean: 0.00969686
[2025-05-27 22:07:52,647]: Min: -0.83115959
[2025-05-27 22:07:52,647]: Max: 1.66231918
[2025-05-27 22:07:52,649]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-27 22:07:52,651]: Sample Values (25 elements): [0.0, -0.538527250289917, 0.0, 0.0, 0.538527250289917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 22:07:52,656]: Mean: 0.00175021
[2025-05-27 22:07:52,657]: Min: -1.07705450
[2025-05-27 22:07:52,657]: Max: 0.53852725
[2025-05-27 22:07:52,659]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-27 22:07:52,659]: Sample Values (25 elements): [0.0, 0.0, -0.4058224558830261, 0.4058224558830261, 0.0, 0.0, -0.4058224558830261, 0.4058224558830261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4058224558830261, 0.0, 0.0, 0.0]
[2025-05-27 22:07:52,661]: Mean: 0.00112728
[2025-05-27 22:07:52,661]: Min: -0.40582246
[2025-05-27 22:07:52,661]: Max: 0.81164491
[2025-05-27 22:07:52,662]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-27 22:07:52,662]: Sample Values (25 elements): [-0.07515637576580048, 0.23390893638134003, 0.08115804195404053, -0.07372913509607315, 0.08076883852481842, -0.036833975464105606, -0.01641525886952877, 0.033895641565322876, -0.09595638513565063, -0.02024027518928051, -0.2087734043598175, -0.03462592512369156, 0.07595552504062653, -0.06342760473489761, -0.022618241608142853, 0.03947172686457634, 0.001521277124993503, -0.06614816188812256, 0.12904642522335052, -0.06569872051477432, -0.10211703926324844, -0.00506754731759429, -0.1141970157623291, -0.18114516139030457, 0.1412045657634735]
[2025-05-27 22:07:52,663]: Mean: 0.00192381
[2025-05-27 22:07:52,663]: Min: -0.44831407
[2025-05-27 22:07:52,663]: Max: 0.56228942
