[2025-06-12 19:29:46,166]: 
Training ResNet18 with parametrized_hardtanh
[2025-06-12 19:31:20,839]: [ResNet18_parametrized_hardtanh] Epoch: 001 Train Loss: 1.8673 Train Acc: 0.3099 Eval Loss: 1.7499 Eval Acc: 0.3454 (LR: 0.00100000)
[2025-06-12 19:32:50,717]: [ResNet18_parametrized_hardtanh] Epoch: 002 Train Loss: 1.4697 Train Acc: 0.4555 Eval Loss: 1.4097 Eval Acc: 0.4920 (LR: 0.00100000)
[2025-06-12 19:34:21,706]: [ResNet18_parametrized_hardtanh] Epoch: 003 Train Loss: 1.2240 Train Acc: 0.5594 Eval Loss: 1.1741 Eval Acc: 0.5761 (LR: 0.00100000)
[2025-06-12 19:36:01,053]: [ResNet18_parametrized_hardtanh] Epoch: 004 Train Loss: 1.0973 Train Acc: 0.6085 Eval Loss: 1.1106 Eval Acc: 0.6030 (LR: 0.00100000)
[2025-06-12 19:37:37,707]: [ResNet18_parametrized_hardtanh] Epoch: 005 Train Loss: 1.0162 Train Acc: 0.6357 Eval Loss: 0.9609 Eval Acc: 0.6605 (LR: 0.00100000)
[2025-06-12 19:39:12,958]: [ResNet18_parametrized_hardtanh] Epoch: 006 Train Loss: 0.9505 Train Acc: 0.6605 Eval Loss: 0.9940 Eval Acc: 0.6540 (LR: 0.00100000)
[2025-06-12 19:40:45,362]: [ResNet18_parametrized_hardtanh] Epoch: 007 Train Loss: 0.8886 Train Acc: 0.6823 Eval Loss: 0.8733 Eval Acc: 0.6936 (LR: 0.00100000)
[2025-06-12 19:42:19,395]: [ResNet18_parametrized_hardtanh] Epoch: 008 Train Loss: 0.8484 Train Acc: 0.6978 Eval Loss: 0.9279 Eval Acc: 0.6857 (LR: 0.00100000)
[2025-06-12 19:43:57,569]: [ResNet18_parametrized_hardtanh] Epoch: 009 Train Loss: 0.8076 Train Acc: 0.7166 Eval Loss: 0.8194 Eval Acc: 0.7163 (LR: 0.00100000)
[2025-06-12 19:45:29,788]: [ResNet18_parametrized_hardtanh] Epoch: 010 Train Loss: 0.7691 Train Acc: 0.7287 Eval Loss: 0.8674 Eval Acc: 0.7065 (LR: 0.00100000)
[2025-06-12 19:47:03,782]: [ResNet18_parametrized_hardtanh] Epoch: 011 Train Loss: 0.7248 Train Acc: 0.7445 Eval Loss: 0.7625 Eval Acc: 0.7388 (LR: 0.00100000)
[2025-06-12 19:48:47,289]: [ResNet18_parametrized_hardtanh] Epoch: 012 Train Loss: 0.6899 Train Acc: 0.7581 Eval Loss: 0.8451 Eval Acc: 0.7186 (LR: 0.00100000)
[2025-06-12 19:50:34,064]: [ResNet18_parametrized_hardtanh] Epoch: 013 Train Loss: 0.6530 Train Acc: 0.7701 Eval Loss: 0.8818 Eval Acc: 0.7077 (LR: 0.00100000)
[2025-06-12 19:52:10,637]: [ResNet18_parametrized_hardtanh] Epoch: 014 Train Loss: 0.6306 Train Acc: 0.7804 Eval Loss: 0.6709 Eval Acc: 0.7683 (LR: 0.00100000)
[2025-06-12 19:53:53,432]: [ResNet18_parametrized_hardtanh] Epoch: 015 Train Loss: 0.5980 Train Acc: 0.7914 Eval Loss: 0.7787 Eval Acc: 0.7477 (LR: 0.00100000)
[2025-06-12 19:55:35,939]: [ResNet18_parametrized_hardtanh] Epoch: 016 Train Loss: 0.5768 Train Acc: 0.7982 Eval Loss: 0.6982 Eval Acc: 0.7656 (LR: 0.00100000)
[2025-06-12 19:57:24,179]: [ResNet18_parametrized_hardtanh] Epoch: 017 Train Loss: 0.5536 Train Acc: 0.8080 Eval Loss: 0.6916 Eval Acc: 0.7834 (LR: 0.00100000)
[2025-06-12 19:58:58,916]: [ResNet18_parametrized_hardtanh] Epoch: 018 Train Loss: 0.5370 Train Acc: 0.8140 Eval Loss: 0.6252 Eval Acc: 0.7951 (LR: 0.00100000)
[2025-06-12 20:00:30,743]: [ResNet18_parametrized_hardtanh] Epoch: 019 Train Loss: 0.5195 Train Acc: 0.8194 Eval Loss: 0.6853 Eval Acc: 0.7909 (LR: 0.00100000)
[2025-06-12 20:02:07,322]: [ResNet18_parametrized_hardtanh] Epoch: 020 Train Loss: 0.5100 Train Acc: 0.8207 Eval Loss: 0.8540 Eval Acc: 0.7376 (LR: 0.00100000)
[2025-06-12 20:03:42,312]: [ResNet18_parametrized_hardtanh] Epoch: 021 Train Loss: 0.4929 Train Acc: 0.8289 Eval Loss: 0.7712 Eval Acc: 0.7564 (LR: 0.00100000)
[2025-06-12 20:05:16,619]: [ResNet18_parametrized_hardtanh] Epoch: 022 Train Loss: 0.4833 Train Acc: 0.8317 Eval Loss: 0.5969 Eval Acc: 0.8046 (LR: 0.00100000)
[2025-06-12 20:06:51,159]: [ResNet18_parametrized_hardtanh] Epoch: 023 Train Loss: 0.4685 Train Acc: 0.8358 Eval Loss: 0.8089 Eval Acc: 0.7550 (LR: 0.00100000)
[2025-06-12 20:08:25,561]: [ResNet18_parametrized_hardtanh] Epoch: 024 Train Loss: 0.4588 Train Acc: 0.8410 Eval Loss: 0.5433 Eval Acc: 0.8248 (LR: 0.00100000)
[2025-06-12 20:10:00,218]: [ResNet18_parametrized_hardtanh] Epoch: 025 Train Loss: 0.4551 Train Acc: 0.8421 Eval Loss: 0.6084 Eval Acc: 0.8051 (LR: 0.00100000)
[2025-06-12 20:11:33,679]: [ResNet18_parametrized_hardtanh] Epoch: 026 Train Loss: 0.4363 Train Acc: 0.8474 Eval Loss: 0.6970 Eval Acc: 0.7773 (LR: 0.00100000)
[2025-06-12 20:13:07,524]: [ResNet18_parametrized_hardtanh] Epoch: 027 Train Loss: 0.4341 Train Acc: 0.8500 Eval Loss: 0.6962 Eval Acc: 0.7782 (LR: 0.00100000)
[2025-06-12 20:14:42,273]: [ResNet18_parametrized_hardtanh] Epoch: 028 Train Loss: 0.4234 Train Acc: 0.8547 Eval Loss: 0.5251 Eval Acc: 0.8273 (LR: 0.00100000)
[2025-06-12 20:16:22,424]: [ResNet18_parametrized_hardtanh] Epoch: 029 Train Loss: 0.4177 Train Acc: 0.8550 Eval Loss: 0.6475 Eval Acc: 0.7978 (LR: 0.00100000)
[2025-06-12 20:18:03,919]: [ResNet18_parametrized_hardtanh] Epoch: 030 Train Loss: 0.4097 Train Acc: 0.8582 Eval Loss: 0.4821 Eval Acc: 0.8432 (LR: 0.00100000)
[2025-06-12 20:19:44,358]: [ResNet18_parametrized_hardtanh] Epoch: 031 Train Loss: 0.4025 Train Acc: 0.8614 Eval Loss: 0.6468 Eval Acc: 0.7990 (LR: 0.00100000)
[2025-06-12 20:21:23,241]: [ResNet18_parametrized_hardtanh] Epoch: 032 Train Loss: 0.4018 Train Acc: 0.8615 Eval Loss: 0.7187 Eval Acc: 0.7732 (LR: 0.00100000)
[2025-06-12 20:22:59,797]: [ResNet18_parametrized_hardtanh] Epoch: 033 Train Loss: 0.3892 Train Acc: 0.8655 Eval Loss: 0.5726 Eval Acc: 0.8223 (LR: 0.00100000)
[2025-06-12 20:24:39,918]: [ResNet18_parametrized_hardtanh] Epoch: 034 Train Loss: 0.3815 Train Acc: 0.8680 Eval Loss: 0.5574 Eval Acc: 0.8238 (LR: 0.00100000)
[2025-06-12 20:26:14,042]: [ResNet18_parametrized_hardtanh] Epoch: 035 Train Loss: 0.3810 Train Acc: 0.8674 Eval Loss: 0.6617 Eval Acc: 0.7992 (LR: 0.00100000)
[2025-06-12 20:27:47,179]: [ResNet18_parametrized_hardtanh] Epoch: 036 Train Loss: 0.3752 Train Acc: 0.8691 Eval Loss: 0.6045 Eval Acc: 0.8118 (LR: 0.00010000)
[2025-06-12 20:29:18,102]: [ResNet18_parametrized_hardtanh] Epoch: 037 Train Loss: 0.2661 Train Acc: 0.9091 Eval Loss: 0.3435 Eval Acc: 0.8944 (LR: 0.00010000)
[2025-06-12 20:30:50,738]: [ResNet18_parametrized_hardtanh] Epoch: 038 Train Loss: 0.2351 Train Acc: 0.9202 Eval Loss: 0.3337 Eval Acc: 0.8949 (LR: 0.00010000)
[2025-06-12 20:32:46,602]: [ResNet18_parametrized_hardtanh] Epoch: 039 Train Loss: 0.2243 Train Acc: 0.9221 Eval Loss: 0.3343 Eval Acc: 0.8961 (LR: 0.00010000)
[2025-06-12 20:34:19,673]: [ResNet18_parametrized_hardtanh] Epoch: 040 Train Loss: 0.2104 Train Acc: 0.9280 Eval Loss: 0.3411 Eval Acc: 0.8943 (LR: 0.00010000)
[2025-06-12 20:35:58,711]: [ResNet18_parametrized_hardtanh] Epoch: 041 Train Loss: 0.2011 Train Acc: 0.9305 Eval Loss: 0.3297 Eval Acc: 0.8993 (LR: 0.00010000)
[2025-06-12 20:37:37,409]: [ResNet18_parametrized_hardtanh] Epoch: 042 Train Loss: 0.1953 Train Acc: 0.9327 Eval Loss: 0.3305 Eval Acc: 0.9006 (LR: 0.00010000)
[2025-06-12 20:39:14,635]: [ResNet18_parametrized_hardtanh] Epoch: 043 Train Loss: 0.1876 Train Acc: 0.9346 Eval Loss: 0.3334 Eval Acc: 0.9015 (LR: 0.00010000)
[2025-06-12 20:40:46,226]: [ResNet18_parametrized_hardtanh] Epoch: 044 Train Loss: 0.1788 Train Acc: 0.9379 Eval Loss: 0.3383 Eval Acc: 0.8987 (LR: 0.00010000)
[2025-06-12 20:42:17,364]: [ResNet18_parametrized_hardtanh] Epoch: 045 Train Loss: 0.1765 Train Acc: 0.9379 Eval Loss: 0.3452 Eval Acc: 0.8996 (LR: 0.00010000)
[2025-06-12 20:43:59,142]: [ResNet18_parametrized_hardtanh] Epoch: 046 Train Loss: 0.1692 Train Acc: 0.9419 Eval Loss: 0.3421 Eval Acc: 0.8990 (LR: 0.00010000)
[2025-06-12 20:45:35,191]: [ResNet18_parametrized_hardtanh] Epoch: 047 Train Loss: 0.1649 Train Acc: 0.9432 Eval Loss: 0.3295 Eval Acc: 0.9010 (LR: 0.00010000)
[2025-06-12 20:47:13,571]: [ResNet18_parametrized_hardtanh] Epoch: 048 Train Loss: 0.1603 Train Acc: 0.9443 Eval Loss: 0.3315 Eval Acc: 0.9037 (LR: 0.00010000)
[2025-06-12 20:48:47,733]: [ResNet18_parametrized_hardtanh] Epoch: 049 Train Loss: 0.1545 Train Acc: 0.9471 Eval Loss: 0.3334 Eval Acc: 0.9024 (LR: 0.00010000)
[2025-06-12 20:50:23,191]: [ResNet18_parametrized_hardtanh] Epoch: 050 Train Loss: 0.1526 Train Acc: 0.9475 Eval Loss: 0.3395 Eval Acc: 0.9031 (LR: 0.00010000)
[2025-06-12 20:51:56,178]: [ResNet18_parametrized_hardtanh] Epoch: 051 Train Loss: 0.1444 Train Acc: 0.9482 Eval Loss: 0.3519 Eval Acc: 0.9018 (LR: 0.00010000)
[2025-06-12 20:53:30,032]: [ResNet18_parametrized_hardtanh] Epoch: 052 Train Loss: 0.1415 Train Acc: 0.9502 Eval Loss: 0.3617 Eval Acc: 0.8993 (LR: 0.00010000)
[2025-06-12 20:55:17,071]: [ResNet18_parametrized_hardtanh] Epoch: 053 Train Loss: 0.1406 Train Acc: 0.9519 Eval Loss: 0.3456 Eval Acc: 0.9023 (LR: 0.00001000)
[2025-06-12 20:56:48,437]: [ResNet18_parametrized_hardtanh] Epoch: 054 Train Loss: 0.1237 Train Acc: 0.9563 Eval Loss: 0.3267 Eval Acc: 0.9051 (LR: 0.00001000)
[2025-06-12 20:58:19,807]: [ResNet18_parametrized_hardtanh] Epoch: 055 Train Loss: 0.1220 Train Acc: 0.9579 Eval Loss: 0.3246 Eval Acc: 0.9064 (LR: 0.00001000)
[2025-06-12 20:59:55,256]: [ResNet18_parametrized_hardtanh] Epoch: 056 Train Loss: 0.1142 Train Acc: 0.9606 Eval Loss: 0.3271 Eval Acc: 0.9062 (LR: 0.00001000)
[2025-06-12 21:01:26,358]: [ResNet18_parametrized_hardtanh] Epoch: 057 Train Loss: 0.1150 Train Acc: 0.9592 Eval Loss: 0.3251 Eval Acc: 0.9085 (LR: 0.00001000)
[2025-06-12 21:03:03,645]: [ResNet18_parametrized_hardtanh] Epoch: 058 Train Loss: 0.1145 Train Acc: 0.9599 Eval Loss: 0.3252 Eval Acc: 0.9067 (LR: 0.00001000)
[2025-06-12 21:04:36,697]: [ResNet18_parametrized_hardtanh] Epoch: 059 Train Loss: 0.1124 Train Acc: 0.9611 Eval Loss: 0.3280 Eval Acc: 0.9076 (LR: 0.00001000)
[2025-06-12 21:06:09,146]: [ResNet18_parametrized_hardtanh] Epoch: 060 Train Loss: 0.1104 Train Acc: 0.9620 Eval Loss: 0.3291 Eval Acc: 0.9093 (LR: 0.00001000)
[2025-06-12 21:07:40,549]: [ResNet18_parametrized_hardtanh] Epoch: 061 Train Loss: 0.1098 Train Acc: 0.9629 Eval Loss: 0.3331 Eval Acc: 0.9074 (LR: 0.00000100)
[2025-06-12 21:09:11,907]: [ResNet18_parametrized_hardtanh] Epoch: 062 Train Loss: 0.1100 Train Acc: 0.9615 Eval Loss: 0.3310 Eval Acc: 0.9085 (LR: 0.00000100)
[2025-06-12 21:10:43,155]: [ResNet18_parametrized_hardtanh] Epoch: 063 Train Loss: 0.1113 Train Acc: 0.9612 Eval Loss: 0.3299 Eval Acc: 0.9084 (LR: 0.00000100)
[2025-06-12 21:12:16,338]: [ResNet18_parametrized_hardtanh] Epoch: 064 Train Loss: 0.1090 Train Acc: 0.9621 Eval Loss: 0.3289 Eval Acc: 0.9080 (LR: 0.00000100)
[2025-06-12 21:13:48,297]: [ResNet18_parametrized_hardtanh] Epoch: 065 Train Loss: 0.1084 Train Acc: 0.9621 Eval Loss: 0.3302 Eval Acc: 0.9085 (LR: 0.00000100)
[2025-06-12 21:15:41,251]: [ResNet18_parametrized_hardtanh] Epoch: 066 Train Loss: 0.1062 Train Acc: 0.9631 Eval Loss: 0.3289 Eval Acc: 0.9088 (LR: 0.00000100)
[2025-06-12 21:17:22,132]: [ResNet18_parametrized_hardtanh] Epoch: 067 Train Loss: 0.1077 Train Acc: 0.9624 Eval Loss: 0.3287 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-06-12 21:18:54,287]: [ResNet18_parametrized_hardtanh] Epoch: 068 Train Loss: 0.1093 Train Acc: 0.9625 Eval Loss: 0.3303 Eval Acc: 0.9081 (LR: 0.00000010)
[2025-06-12 21:20:29,012]: [ResNet18_parametrized_hardtanh] Epoch: 069 Train Loss: 0.1097 Train Acc: 0.9623 Eval Loss: 0.3309 Eval Acc: 0.9076 (LR: 0.00000010)
[2025-06-12 21:22:01,902]: [ResNet18_parametrized_hardtanh] Epoch: 070 Train Loss: 0.1062 Train Acc: 0.9630 Eval Loss: 0.3297 Eval Acc: 0.9081 (LR: 0.00000010)
[2025-06-12 21:23:33,811]: [ResNet18_parametrized_hardtanh] Epoch: 071 Train Loss: 0.1092 Train Acc: 0.9618 Eval Loss: 0.3307 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-06-12 21:25:03,366]: [ResNet18_parametrized_hardtanh] Epoch: 072 Train Loss: 0.1060 Train Acc: 0.9645 Eval Loss: 0.3283 Eval Acc: 0.9076 (LR: 0.00000010)
[2025-06-12 21:26:34,915]: [ResNet18_parametrized_hardtanh] Epoch: 073 Train Loss: 0.1084 Train Acc: 0.9626 Eval Loss: 0.3294 Eval Acc: 0.9085 (LR: 0.00000010)
[2025-06-12 21:28:08,799]: [ResNet18_parametrized_hardtanh] Epoch: 074 Train Loss: 0.1065 Train Acc: 0.9631 Eval Loss: 0.3301 Eval Acc: 0.9073 (LR: 0.00000010)
[2025-06-12 21:29:51,496]: [ResNet18_parametrized_hardtanh] Epoch: 075 Train Loss: 0.1079 Train Acc: 0.9622 Eval Loss: 0.3277 Eval Acc: 0.9087 (LR: 0.00000010)
[2025-06-12 21:31:26,538]: [ResNet18_parametrized_hardtanh] Epoch: 076 Train Loss: 0.1084 Train Acc: 0.9615 Eval Loss: 0.3312 Eval Acc: 0.9077 (LR: 0.00000010)
[2025-06-12 21:33:02,786]: [ResNet18_parametrized_hardtanh] Epoch: 077 Train Loss: 0.1089 Train Acc: 0.9618 Eval Loss: 0.3304 Eval Acc: 0.9085 (LR: 0.00000010)
[2025-06-12 21:34:41,249]: [ResNet18_parametrized_hardtanh] Epoch: 078 Train Loss: 0.1074 Train Acc: 0.9631 Eval Loss: 0.3287 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-06-12 21:36:15,984]: [ResNet18_parametrized_hardtanh] Epoch: 079 Train Loss: 0.1068 Train Acc: 0.9625 Eval Loss: 0.3293 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-06-12 21:37:50,580]: [ResNet18_parametrized_hardtanh] Epoch: 080 Train Loss: 0.1097 Train Acc: 0.9617 Eval Loss: 0.3317 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-06-12 21:39:25,236]: [ResNet18_parametrized_hardtanh] Epoch: 081 Train Loss: 0.1080 Train Acc: 0.9634 Eval Loss: 0.3287 Eval Acc: 0.9079 (LR: 0.00000010)
[2025-06-12 21:41:00,590]: [ResNet18_parametrized_hardtanh] Epoch: 082 Train Loss: 0.1074 Train Acc: 0.9625 Eval Loss: 0.3294 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-06-12 21:42:35,080]: [ResNet18_parametrized_hardtanh] Epoch: 083 Train Loss: 0.1093 Train Acc: 0.9623 Eval Loss: 0.3295 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-06-12 21:44:10,795]: [ResNet18_parametrized_hardtanh] Epoch: 084 Train Loss: 0.1090 Train Acc: 0.9627 Eval Loss: 0.3338 Eval Acc: 0.9062 (LR: 0.00000010)
[2025-06-12 21:45:47,809]: [ResNet18_parametrized_hardtanh] Epoch: 085 Train Loss: 0.1080 Train Acc: 0.9619 Eval Loss: 0.3286 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-06-12 21:47:22,186]: [ResNet18_parametrized_hardtanh] Epoch: 086 Train Loss: 0.1090 Train Acc: 0.9616 Eval Loss: 0.3290 Eval Acc: 0.9075 (LR: 0.00000010)
[2025-06-12 21:49:02,671]: [ResNet18_parametrized_hardtanh] Epoch: 087 Train Loss: 0.1073 Train Acc: 0.9625 Eval Loss: 0.3295 Eval Acc: 0.9092 (LR: 0.00000010)
[2025-06-12 21:50:42,749]: [ResNet18_parametrized_hardtanh] Epoch: 088 Train Loss: 0.1110 Train Acc: 0.9617 Eval Loss: 0.3296 Eval Acc: 0.9079 (LR: 0.00000010)
[2025-06-12 21:52:20,592]: [ResNet18_parametrized_hardtanh] Epoch: 089 Train Loss: 0.1072 Train Acc: 0.9634 Eval Loss: 0.3313 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-06-12 21:54:03,554]: [ResNet18_parametrized_hardtanh] Epoch: 090 Train Loss: 0.1042 Train Acc: 0.9645 Eval Loss: 0.3290 Eval Acc: 0.9075 (LR: 0.00000010)
[2025-06-12 21:55:38,832]: [ResNet18_parametrized_hardtanh] Epoch: 091 Train Loss: 0.1070 Train Acc: 0.9623 Eval Loss: 0.3348 Eval Acc: 0.9070 (LR: 0.00000010)
[2025-06-12 21:57:12,452]: [ResNet18_parametrized_hardtanh] Epoch: 092 Train Loss: 0.1085 Train Acc: 0.9620 Eval Loss: 0.3282 Eval Acc: 0.9088 (LR: 0.00000010)
[2025-06-12 21:58:47,839]: [ResNet18_parametrized_hardtanh] Epoch: 093 Train Loss: 0.1078 Train Acc: 0.9629 Eval Loss: 0.3293 Eval Acc: 0.9083 (LR: 0.00000010)
[2025-06-12 22:00:20,915]: [ResNet18_parametrized_hardtanh] Epoch: 094 Train Loss: 0.1071 Train Acc: 0.9629 Eval Loss: 0.3259 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-06-12 22:01:56,455]: [ResNet18_parametrized_hardtanh] Epoch: 095 Train Loss: 0.1058 Train Acc: 0.9630 Eval Loss: 0.3282 Eval Acc: 0.9092 (LR: 0.00000010)
[2025-06-12 22:03:27,341]: [ResNet18_parametrized_hardtanh] Epoch: 096 Train Loss: 0.1072 Train Acc: 0.9620 Eval Loss: 0.3287 Eval Acc: 0.9087 (LR: 0.00000010)
[2025-06-12 22:04:59,217]: [ResNet18_parametrized_hardtanh] Epoch: 097 Train Loss: 0.1079 Train Acc: 0.9626 Eval Loss: 0.3296 Eval Acc: 0.9084 (LR: 0.00000010)
[2025-06-12 22:06:32,759]: [ResNet18_parametrized_hardtanh] Epoch: 098 Train Loss: 0.1081 Train Acc: 0.9626 Eval Loss: 0.3283 Eval Acc: 0.9086 (LR: 0.00000010)
[2025-06-12 22:08:06,948]: [ResNet18_parametrized_hardtanh] Epoch: 099 Train Loss: 0.1062 Train Acc: 0.9635 Eval Loss: 0.3310 Eval Acc: 0.9080 (LR: 0.00000010)
[2025-06-12 22:09:41,408]: [ResNet18_parametrized_hardtanh] Epoch: 100 Train Loss: 0.1062 Train Acc: 0.9636 Eval Loss: 0.3291 Eval Acc: 0.9091 (LR: 0.00000010)
[2025-06-12 22:09:41,408]: [ResNet18_parametrized_hardtanh] Best Eval Accuracy: 0.9093
[2025-06-12 22:09:42,556]: 
Training of full-precision model finished!
[2025-06-12 22:09:42,556]: Model Architecture:
[2025-06-12 22:09:42,752]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 22:09:42,806]: 
Model Weights:
[2025-06-12 22:09:42,806]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-12 22:09:42,998]: Sample Values (25 elements): [-0.1802597939968109, 0.20566405355930328, 0.08071943372488022, 0.18124638497829437, -0.1893400251865387, -0.14378014206886292, 0.14916951954364777, -0.024416320025920868, -0.22616863250732422, -0.14002986252307892, -0.08399387449026108, -0.09418105334043503, -0.07108437269926071, 0.28405696153640747, 0.0089943278580904, -0.15202122926712036, -0.22869618237018585, 0.12677349150180817, -0.08857455104589462, -0.24246124923229218, -0.16969947516918182, -0.2298668771982193, 0.04466932639479637, 0.20587250590324402, -0.14762656390666962]
[2025-06-12 22:09:43,071]: Mean: 0.00110365
[2025-06-12 22:09:43,073]: Min: -0.45756155
[2025-06-12 22:09:43,109]: Max: 0.35497817
[2025-06-12 22:09:43,109]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-12 22:09:43,110]: Sample Values (25 elements): [0.598494827747345, 0.5545952320098877, 0.6644983887672424, 0.8936407566070557, 0.6363462209701538, 0.613538920879364, 1.1455892324447632, 0.49059924483299255, 0.6885451674461365, 0.532126784324646, 0.44301313161849976, 0.8454769253730774, 0.4508242905139923, 0.6454246640205383, 0.6170756220817566, 0.7690281271934509, 0.7522881031036377, 0.8417931199073792, 0.5329223871231079, 0.6110982298851013, 0.6239063739776611, 0.6136102080345154, 0.5563274025917053, 0.5229194164276123, 0.5534647703170776]
[2025-06-12 22:09:43,110]: Mean: 0.65440089
[2025-06-12 22:09:43,110]: Min: 0.30447441
[2025-06-12 22:09:43,110]: Max: 1.14558923
[2025-06-12 22:09:43,110]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 22:09:43,111]: Sample Values (25 elements): [-0.05862822011113167, -0.09677144885063171, -0.0794791504740715, 0.06766562163829803, -0.006907199509441853, 0.05140978842973709, -0.09250909835100174, -0.06232459098100662, -0.038142893463373184, 0.04078657180070877, 0.010376573540270329, 0.01812644675374031, -0.0019403258338570595, -0.0836539939045906, 0.055685922503471375, 0.021667355671525, 0.037477489560842514, -0.009297601878643036, 0.07735839486122131, -0.03605853021144867, -0.02073489874601364, 0.013075456954538822, 0.026162974536418915, -0.04350389540195465, 0.06173983961343765]
[2025-06-12 22:09:43,114]: Mean: 0.00005110
[2025-06-12 22:09:43,115]: Min: -0.33512878
[2025-06-12 22:09:43,115]: Max: 0.34371531
[2025-06-12 22:09:43,115]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-12 22:09:43,115]: Sample Values (25 elements): [0.6759669184684753, 0.5997004508972168, 0.5814904570579529, 0.506813108921051, 0.3614765703678131, 0.6884477734565735, 0.5304058790206909, 0.6175055503845215, 0.4634052813053131, 0.7695301175117493, 0.8027761578559875, 0.3350687325000763, 0.5794855952262878, 0.35577192902565, 0.6451088786125183, 0.509749174118042, 0.6718843579292297, 0.5615105628967285, 0.774423360824585, 0.669767439365387, 0.2732856869697571, 0.37947878241539, 0.522901177406311, 0.7922763228416443, 0.4123527705669403]
[2025-06-12 22:09:43,116]: Mean: 0.55479801
[2025-06-12 22:09:43,116]: Min: 0.23818545
[2025-06-12 22:09:43,116]: Max: 0.84866059
[2025-06-12 22:09:43,116]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 22:09:43,117]: Sample Values (25 elements): [-0.05022050067782402, -0.09169124066829681, -0.04147588834166527, 0.0829867348074913, 0.0017987656174227595, 0.05880901962518692, 0.0683460533618927, 0.04837917909026146, -0.03831859305500984, 0.04678206145763397, 0.011162744835019112, -0.07976776361465454, 0.02309923991560936, 0.03757983818650246, -0.054802440106868744, -0.09824918210506439, -0.01348920725286007, -0.041086021810770035, 0.03396483138203621, -0.0833173468708992, 0.05283977836370468, -0.010911369696259499, 0.011921434663236141, -0.057485658675432205, 0.13264861702919006]
[2025-06-12 22:09:43,117]: Mean: -0.00023303
[2025-06-12 22:09:43,117]: Min: -0.43269864
[2025-06-12 22:09:43,117]: Max: 0.36917168
[2025-06-12 22:09:43,117]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-12 22:09:43,118]: Sample Values (25 elements): [0.8589051961898804, 0.7289161682128906, 0.7672408819198608, 0.6754781007766724, 0.5614426136016846, 0.8401136994361877, 0.4581650197505951, 0.5253958106040955, 0.7694083452224731, 0.8399432301521301, 0.795490562915802, 0.6623851656913757, 0.6662902235984802, 0.7709172368049622, 0.7861469984054565, 0.618280827999115, 0.6785547733306885, 0.5610078573226929, 0.6914993524551392, 0.6796579957008362, 0.8401541709899902, 0.76130610704422, 0.6215435862541199, 0.5864261984825134, 0.628086268901825]
[2025-06-12 22:09:43,118]: Mean: 0.71381927
[2025-06-12 22:09:43,118]: Min: 0.45816502
[2025-06-12 22:09:43,118]: Max: 0.94178051
[2025-06-12 22:09:43,118]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 22:09:43,119]: Sample Values (25 elements): [-0.024973805993795395, -0.03474664315581322, 0.1200760081410408, -0.10308818519115448, -0.007980183698236942, 0.18415683507919312, 0.028884608298540115, -0.02556145377457142, -0.06282928586006165, 0.06551899015903473, 0.004443885758519173, -0.0009607696556486189, -0.000257702951785177, 0.015818867832422256, 0.01863817870616913, 0.0943164974451065, 0.03871988132596016, 0.048229116946458817, 0.03868034854531288, -0.07287228107452393, -0.08002926409244537, -0.0569831058382988, 0.06402643024921417, 0.013859445229172707, 0.07158548384904861]
[2025-06-12 22:09:43,119]: Mean: -0.00064205
[2025-06-12 22:09:43,119]: Min: -0.45106921
[2025-06-12 22:09:43,119]: Max: 0.35981438
[2025-06-12 22:09:43,119]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-12 22:09:43,120]: Sample Values (25 elements): [0.598724901676178, 0.637424647808075, 0.4924500584602356, 0.5752089619636536, 0.4773504137992859, 0.44191598892211914, 0.5360687971115112, 0.41770368814468384, 0.40362146496772766, 0.5661485195159912, 0.4711727797985077, 0.3519599139690399, 0.48523327708244324, 0.4502548277378082, 0.37398526072502136, 0.3671356439590454, 0.7086501121520996, 0.6225488781929016, 0.4865218698978424, 0.37181034684181213, 0.4059562087059021, 0.5490447878837585, 0.5806928873062134, 0.46990326046943665, 0.49582695960998535]
[2025-06-12 22:09:43,120]: Mean: 0.49471813
[2025-06-12 22:09:43,120]: Min: 0.29699707
[2025-06-12 22:09:43,121]: Max: 0.70865011
[2025-06-12 22:09:43,121]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-12 22:09:43,121]: Sample Values (25 elements): [0.043054353445768356, 0.03332635760307312, 0.11808598041534424, -0.07201265543699265, 0.0034877979196608067, 0.04223327338695526, -0.03765119984745979, 0.07051645964384079, -0.04750984534621239, -0.10657498240470886, 0.011101857759058475, -0.034760501235723495, -0.014577027410268784, -0.04890585318207741, 0.020919369533658028, -0.07462338358163834, -0.041040509939193726, -0.0013577478239312768, -0.02447117492556572, 0.03411635383963585, 0.04200822487473488, 0.11111852526664734, -0.07664275169372559, -0.043907251209020615, 0.028549188748002052]
[2025-06-12 22:09:43,121]: Mean: 0.00026245
[2025-06-12 22:09:43,122]: Min: -0.33679089
[2025-06-12 22:09:43,122]: Max: 0.35078377
[2025-06-12 22:09:43,122]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-12 22:09:43,122]: Sample Values (25 elements): [0.7782148718833923, 0.7787947654724121, 0.5223149061203003, 0.7071123123168945, 0.5255909562110901, 0.5249557495117188, 0.8940207362174988, 1.0530931949615479, 0.4329166114330292, 0.9394305348396301, 0.7707757949829102, 0.6037184000015259, 0.7892622351646423, 0.7909330129623413, 0.9058831930160522, 0.6907174587249756, 0.9847530722618103, 0.7230774164199829, 0.7137182950973511, 0.7859772443771362, 0.6009629368782043, 0.7086191773414612, 0.5925986766815186, 0.7288808226585388, 0.6512352824211121]
[2025-06-12 22:09:43,122]: Mean: 0.69782186
[2025-06-12 22:09:43,122]: Min: 0.35518551
[2025-06-12 22:09:43,123]: Max: 1.06802595
[2025-06-12 22:09:43,123]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-12 22:09:43,134]: Sample Values (25 elements): [0.044172946363687515, -0.03591784089803696, 0.0031374169047921896, 0.06063464656472206, 0.11286456137895584, -0.018375344574451447, -0.012641199864447117, 0.015199531801044941, 0.011355203576385975, -0.0007465602247975767, 0.021382929757237434, -0.02059650607407093, 0.044904254376888275, 0.006136644631624222, 0.019620122388005257, -0.06675607711076736, -0.12187525629997253, 0.07324735820293427, -0.04348241537809372, 0.026273516938090324, -0.005729743279516697, -0.08966723829507828, 0.0374438613653183, -0.07844535261392593, 0.01275293342769146]
[2025-06-12 22:09:43,134]: Mean: 0.00054173
[2025-06-12 22:09:43,134]: Min: -0.33628577
[2025-06-12 22:09:43,134]: Max: 0.33349183
[2025-06-12 22:09:43,134]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-12 22:09:43,136]: Sample Values (25 elements): [0.25485318899154663, 0.278979629278183, 0.34712567925453186, 0.290693461894989, 0.5258141160011292, 0.3135131597518921, 0.34989258646965027, 0.29528316855430603, 0.3268783688545227, 0.5324417948722839, 0.27882471680641174, 0.29074129462242126, 0.30742424726486206, 0.38587281107902527, 0.2807350158691406, 0.20904381573200226, 0.272124707698822, 0.4562232196331024, 0.2865571677684784, 0.30320924520492554, 0.31759992241859436, 0.296789288520813, 0.33787259459495544, 0.26797977089881897, 0.3228578269481659]
[2025-06-12 22:09:43,137]: Mean: 0.32631561
[2025-06-12 22:09:43,137]: Min: 0.10648637
[2025-06-12 22:09:43,137]: Max: 0.57678515
[2025-06-12 22:09:43,137]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 22:09:43,139]: Sample Values (25 elements): [-0.03589005023241043, -0.07947343587875366, -0.033999864012002945, 0.09074599295854568, 0.018865443766117096, -0.048770979046821594, 0.06296756863594055, -0.006394907366484404, 0.04477246105670929, 0.015100418590009212, 0.013055277056992054, -0.03633159026503563, -0.02474951557815075, 0.025474613532423973, -0.07928657531738281, 0.01995576173067093, 0.02482401393353939, -0.07294252514839172, -0.028575750067830086, -0.06668028235435486, 0.043922800570726395, -0.07613997161388397, -0.05286511033773422, -0.061657506972551346, 0.012003008276224136]
[2025-06-12 22:09:43,142]: Mean: 0.00001753
[2025-06-12 22:09:43,142]: Min: -0.23468924
[2025-06-12 22:09:43,142]: Max: 0.25396645
[2025-06-12 22:09:43,142]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-12 22:09:43,142]: Sample Values (25 elements): [0.6823849081993103, 0.6551600694656372, 0.7500944137573242, 0.7093307971954346, 0.6490733027458191, 0.5877160429954529, 0.7122907042503357, 0.5275847911834717, 0.4696739614009857, 0.5073390603065491, 0.6563056111335754, 0.7342814207077026, 0.5680163502693176, 0.5339047312736511, 0.6143493056297302, 0.498140811920166, 0.4616422951221466, 0.6015821695327759, 0.6625304222106934, 0.7274169921875, 0.7115598917007446, 0.516132652759552, 0.6895155310630798, 0.6483736634254456, 0.6991669535636902]
[2025-06-12 22:09:43,143]: Mean: 0.60559744
[2025-06-12 22:09:43,143]: Min: 0.29996309
[2025-06-12 22:09:43,143]: Max: 0.86846662
[2025-06-12 22:09:43,143]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-12 22:09:43,158]: Sample Values (25 elements): [-0.06792755424976349, -0.002169426064938307, -0.05687664821743965, 0.10087986290454865, -0.05740400403738022, -0.033755239099264145, 0.007601345889270306, 0.02156716398894787, 0.02026308700442314, 0.14090494811534882, 0.07627365738153458, -0.11992672085762024, -0.05263219028711319, -0.07131780683994293, 0.06864620745182037, 0.08588450402021408, 0.09228825569152832, 0.0314180813729763, 0.07366808503866196, -0.07103602588176727, -0.10506798326969147, -0.2121773511171341, -0.0197728481143713, -0.0031567136757075787, 0.01639571227133274]
[2025-06-12 22:09:43,159]: Mean: 0.00124198
[2025-06-12 22:09:43,159]: Min: -0.34506738
[2025-06-12 22:09:43,159]: Max: 0.35236198
[2025-06-12 22:09:43,159]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-12 22:09:43,159]: Sample Values (25 elements): [0.2821441888809204, 0.46365484595298767, 0.32943081855773926, 0.4981828033924103, 0.4990687966346741, 0.4113573431968689, 0.5249106287956238, 0.5256674885749817, 0.37695565819740295, 0.37415358424186707, 0.4429808259010315, 0.3809898793697357, 0.19153393805027008, 0.3751180171966553, 0.3857306241989136, 0.5551121830940247, 0.5188300013542175, 0.42687350511550903, 0.5652139782905579, 0.5151526927947998, 0.207071453332901, 0.4499993324279785, 0.45099207758903503, 0.5086730122566223, 0.5923410654067993]
[2025-06-12 22:09:43,159]: Mean: 0.43504336
[2025-06-12 22:09:43,160]: Min: 0.19153394
[2025-06-12 22:09:43,160]: Max: 0.71962911
[2025-06-12 22:09:43,160]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 22:09:43,161]: Sample Values (25 elements): [0.03301544860005379, 0.07270082831382751, -0.08238819986581802, 0.004168331623077393, -0.01027245819568634, -0.02031848579645157, -0.06533782929182053, 0.04823438823223114, -0.059177838265895844, -0.008420431986451149, -0.08434127271175385, -0.050691697746515274, 0.006943328306078911, -0.007593050599098206, -0.0390457920730114, -0.12259981036186218, 0.008214774541556835, -0.021650956943631172, 0.0913332924246788, 0.0163117665797472, -0.02500833570957184, -0.03132486715912819, -0.02379995957016945, -0.03987948223948479, 0.03174401447176933]
[2025-06-12 22:09:43,162]: Mean: 0.00022395
[2025-06-12 22:09:43,162]: Min: -0.26150376
[2025-06-12 22:09:43,162]: Max: 0.22627819
[2025-06-12 22:09:43,162]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-12 22:09:43,163]: Sample Values (25 elements): [0.18407778441905975, 0.2823731601238251, 0.26455816626548767, 0.2632496654987335, 0.5176528692245483, 0.3851516544818878, 0.23651599884033203, 0.18164712190628052, 0.32669004797935486, 0.19193370640277863, 0.3484879434108734, 0.27798882126808167, 0.19750846922397614, 0.3225491940975189, 0.4363665282726288, 0.3300952911376953, 0.3249945342540741, 0.24488414824008942, 0.2845655679702759, 0.3122253715991974, 0.1780732423067093, 0.24119390547275543, 0.18330442905426025, 0.20495286583900452, 0.20442336797714233]
[2025-06-12 22:09:43,163]: Mean: 0.27247003
[2025-06-12 22:09:43,163]: Min: 0.12460731
[2025-06-12 22:09:43,163]: Max: 0.52357048
[2025-06-12 22:09:43,163]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-12 22:09:43,175]: Sample Values (25 elements): [0.034630730748176575, -0.017521778121590614, 0.033516962081193924, -0.11378148198127747, 0.011077581904828548, -0.03808281570672989, -0.024127693846821785, 0.026920080184936523, 0.06181225925683975, -0.11765172332525253, -0.003206157125532627, -0.038133472204208374, 0.0028340669814497232, -0.009749406948685646, 0.05010218545794487, 0.016268456354737282, -0.05184276029467583, -0.042692750692367554, -0.005543679464608431, -0.06064049154520035, 0.08462464809417725, 0.0067656803876161575, -0.012381467968225479, 0.012073530815541744, -0.05905728042125702]
[2025-06-12 22:09:43,176]: Mean: 0.00013445
[2025-06-12 22:09:43,176]: Min: -0.23025984
[2025-06-12 22:09:43,176]: Max: 0.23823769
[2025-06-12 22:09:43,176]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-12 22:09:43,177]: Sample Values (25 elements): [0.37935927510261536, 0.2621251344680786, 0.569366455078125, 0.3871854245662689, 0.7449604272842407, 0.5414116382598877, 0.4649450480937958, 0.33621451258659363, 0.5300450325012207, 0.594698429107666, 0.5850220918655396, 0.6142697334289551, 0.5519040822982788, 0.4399603605270386, 0.6332024931907654, 0.3914976119995117, 0.515613853931427, 0.6924643516540527, 0.503516674041748, 0.5498541593551636, 0.49408474564552307, 0.4805587828159332, 0.5145065784454346, 0.613165020942688, 0.4644286632537842]
[2025-06-12 22:09:43,177]: Mean: 0.50583261
[2025-06-12 22:09:43,177]: Min: 0.15829825
[2025-06-12 22:09:43,177]: Max: 0.74496043
[2025-06-12 22:09:43,177]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-12 22:09:43,182]: Sample Values (25 elements): [-0.04482350870966911, -0.05328042432665825, 0.027393486350774765, 0.05884776636958122, 0.02317977324128151, -0.005791053641587496, -0.07394365221261978, -0.01285576168447733, -0.039321187883615494, -0.04481273889541626, 0.031092682853341103, 0.05291711166501045, -0.011454104445874691, 0.027548322454094887, 0.007877959869801998, 0.05382349342107773, 0.045430585741996765, -0.04669506102800369, 0.046313658356666565, -0.04585273936390877, -0.07410050928592682, -0.009518321603536606, 0.006365048233419657, 0.020075365900993347, 5.4479642130900174e-05]
[2025-06-12 22:09:43,182]: Mean: 0.00000542
[2025-06-12 22:09:43,183]: Min: -0.22002237
[2025-06-12 22:09:43,183]: Max: 0.24066919
[2025-06-12 22:09:43,183]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-12 22:09:43,185]: Sample Values (25 elements): [0.15904583036899567, 0.2165413349866867, 0.1904129683971405, 0.1299237459897995, 0.25733324885368347, 0.10264977067708969, 0.18888865411281586, 0.20472495257854462, 0.16876482963562012, 0.1516542136669159, 0.2071143239736557, 0.025923453271389008, 0.2829352617263794, 0.26802900433540344, 0.19135548174381256, 0.19881847500801086, 0.2362624555826187, 0.4151577949523926, 0.21253633499145508, 0.17757312953472137, 0.2146860957145691, 0.2083309441804886, 0.20121803879737854, 0.2691311538219452, 0.2400854229927063]
[2025-06-12 22:09:43,185]: Mean: 0.21194261
[2025-06-12 22:09:43,185]: Min: -0.00003775
[2025-06-12 22:09:43,186]: Max: 0.41515779
[2025-06-12 22:09:43,186]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 22:09:43,199]: Sample Values (25 elements): [-0.016341332346200943, 0.0037060773465782404, -0.02460428699851036, -0.016140837222337723, 0.005541309714317322, 0.035884495824575424, 0.010670182295143604, 0.017561402171850204, 0.002981688128784299, -0.011752738617360592, -0.03198883309960365, 0.005988335702568293, 0.0054701766930520535, -0.025335218757390976, 0.0554891899228096, 0.026733338832855225, -0.0012419361155480146, 0.0053412411361932755, -0.007715162821114063, 0.01930207386612892, -0.012280231341719627, 0.020812831819057465, -0.013195537030696869, 0.02263079769909382, -0.0006503551849164069]
[2025-06-12 22:09:43,199]: Mean: 0.00005067
[2025-06-12 22:09:43,200]: Min: -0.20770866
[2025-06-12 22:09:43,200]: Max: 0.25764951
[2025-06-12 22:09:43,200]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-12 22:09:43,200]: Sample Values (25 elements): [0.6634814143180847, 0.400332510471344, 0.3676894009113312, 0.5723419785499573, 0.5574508309364319, 0.25970658659935, 0.5293861627578735, 0.6102668642997742, 0.29943251609802246, 0.3462570309638977, 0.36999019980430603, 0.41552045941352844, 0.24947738647460938, 0.539596676826477, 0.43249234557151794, 0.6039167046546936, 0.5340427756309509, 0.3709624707698822, 0.12222462892532349, 0.5911165475845337, 0.6797930002212524, 0.3833180367946625, 0.2981507480144501, 0.5794042348861694, 0.4179782569408417]
[2025-06-12 22:09:43,200]: Mean: 0.39827967
[2025-06-12 22:09:43,200]: Min: -0.01287473
[2025-06-12 22:09:43,201]: Max: 0.75314218
[2025-06-12 22:09:43,201]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-12 22:09:43,201]: Sample Values (25 elements): [-0.040530893951654434, 0.16973455250263214, -0.032847095280885696, -0.012496518902480602, 0.12153013795614243, 0.07131282985210419, 0.01551446970552206, -0.13423408567905426, 0.009838994592428207, 0.051268186420202255, -0.011117953807115555, -0.005994289182126522, 0.0766877830028534, 0.08541004359722137, -0.012139730155467987, 0.017202463001012802, -0.03810882940888405, -0.046604640781879425, -0.0018307996215298772, 0.12422686815261841, 0.18726392090320587, -0.03123733028769493, 0.03484376147389412, 0.021636100485920906, 0.03233819082379341]
[2025-06-12 22:09:43,202]: Mean: -0.00037035
[2025-06-12 22:09:43,202]: Min: -0.26871932
[2025-06-12 22:09:43,202]: Max: 0.29399958
[2025-06-12 22:09:43,202]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-12 22:09:43,202]: Sample Values (25 elements): [0.2031191885471344, 0.27418994903564453, 0.2966928482055664, 0.3565139174461365, 0.36415794491767883, 0.30310431122779846, 0.2909565567970276, 0.29670581221580505, 0.37062275409698486, 0.23916752636432648, 0.16788846254348755, 0.2681799829006195, 0.4667509198188782, 0.3052312731742859, 0.23970790207386017, 0.11108941584825516, 0.4031498432159424, 0.23931699991226196, 0.28490832448005676, 0.3847912847995758, 0.23005171120166779, 0.35930272936820984, 0.12103094905614853, 0.19548103213310242, 0.4427710771560669]
[2025-06-12 22:09:43,203]: Mean: 0.28912529
[2025-06-12 22:09:43,203]: Min: 0.00899600
[2025-06-12 22:09:43,204]: Max: 0.61855716
[2025-06-12 22:09:43,204]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 22:09:43,211]: Sample Values (25 elements): [0.05025549605488777, 0.01603236049413681, -0.01952209696173668, -0.005312026012688875, -0.020769216120243073, -0.05395669490098953, 6.4342514558282185e-22, 0.010615361854434013, 0.01515066996216774, 0.02142835594713688, -0.01301521249115467, -0.020891930907964706, -0.09945395588874817, -0.006994793191552162, -0.026191219687461853, 0.0061625088565051556, -0.022374263033270836, 0.01630960777401924, -0.010759085416793823, 0.01660742238163948, -0.03243360295891762, -0.02024557814002037, -0.012759237550199032, -0.0021780645474791527, -0.05513773486018181]
[2025-06-12 22:09:43,211]: Mean: -0.00004751
[2025-06-12 22:09:43,211]: Min: -0.22375466
[2025-06-12 22:09:43,212]: Max: 0.17671329
[2025-06-12 22:09:43,212]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-12 22:09:43,212]: Sample Values (25 elements): [0.22974948585033417, 0.10028962045907974, 0.10497548431158066, 0.14623548090457916, 0.19257940351963043, 0.1684459149837494, 0.12359051406383514, 0.1653987169265747, 0.11280710995197296, 0.20160356163978577, 0.11229211091995239, 0.11778086423873901, 0.04925251752138138, 0.21751336753368378, 0.13768146932125092, 0.18399488925933838, 0.21477508544921875, 0.10951255261898041, 0.020961478352546692, 0.21479468047618866, 0.15585286915302277, 0.17870840430259705, 0.12053178995847702, 0.10673394054174423, 0.1735152155160904]
[2025-06-12 22:09:43,212]: Mean: 0.14899166
[2025-06-12 22:09:43,212]: Min: -0.00166589
[2025-06-12 22:09:43,212]: Max: 0.35664091
[2025-06-12 22:09:43,212]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-12 22:09:43,250]: Sample Values (25 elements): [-0.003590302774682641, -0.004337630700320005, -0.01000464241951704, -0.04841294884681702, -0.009905100800096989, 0.01695580594241619, 0.0034819862339645624, 0.03037017397582531, 0.002707029227167368, 0.00466160336509347, -0.061660218983888626, 0.009404136799275875, -0.027336785569787025, -0.012579157948493958, -0.0004843759525101632, -0.0040481300093233585, 0.0022360680159181356, 0.0035795920994132757, -0.00086025899508968, -0.005643099080771208, 0.0018070730147883296, 0.03221341595053673, 0.009194953367114067, -0.01566501334309578, 0.007433182559907436]
[2025-06-12 22:09:43,250]: Mean: -0.00007658
[2025-06-12 22:09:43,250]: Min: -0.23638214
[2025-06-12 22:09:43,250]: Max: 0.20294873
[2025-06-12 22:09:43,250]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-12 22:09:43,251]: Sample Values (25 elements): [0.08536140620708466, 0.36412379145622253, 0.22241534292697906, 0.23436255753040314, 0.3594330847263336, 0.2745988070964813, 0.2674379348754883, 0.05594048276543617, 0.5004901885986328, 0.39606693387031555, 0.4288792908191681, 0.38241520524024963, 0.022327279672026634, 0.3735469877719879, 0.3562264144420624, 0.18103933334350586, 0.08365185558795929, 0.13045813143253326, 0.17819365859031677, 0.33153629302978516, 0.28968629240989685, 0.08597365766763687, 0.3598289489746094, 0.35030218958854675, 0.251336008310318]
[2025-06-12 22:09:43,251]: Mean: 0.29734474
[2025-06-12 22:09:43,251]: Min: -0.02967737
[2025-06-12 22:09:43,251]: Max: 0.59661466
[2025-06-12 22:09:43,251]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-12 22:09:43,266]: Sample Values (25 elements): [2.6025428376244986e-28, -3.924867397297197e-21, 1.3468475932505454e-22, 6.257370532801928e-17, 3.365369588017332e-19, 0.017469001933932304, -9.871212144535016e-22, -1.0091516652269844e-14, 0.030294019728899002, 0.09095549583435059, -1.3690730602806728e-28, -2.5461687940027713e-23, 2.3685003277643213e-17, 4.0800420964332084e-25, 1.0544938132319576e-13, -7.02421736227532e-13, 6.787084349396677e-12, -2.6544263079954223e-18, 3.464117805674834e-26, -9.884486552844164e-31, 1.3929687588063174e-20, -9.691750440382618e-13, 2.2664217169676704e-09, 1.92902309319805e-15, 1.4304475826065755e-07]
[2025-06-12 22:09:43,266]: Mean: 0.00001848
[2025-06-12 22:09:43,267]: Min: -0.33548045
[2025-06-12 22:09:43,267]: Max: 0.29978740
[2025-06-12 22:09:43,267]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-12 22:09:43,267]: Sample Values (25 elements): [-3.0570780040761747e-07, -1.6529110098417732e-06, 0.03577931597828865, 5.184814244607594e-10, 3.2902928523981245e-06, 1.1862839528475888e-06, -1.414793700860173e-06, 5.74067883007956e-07, -1.3410985655326613e-09, -0.0041356272995471954, 4.893306595477043e-07, 5.907118065806571e-07, -1.362074897315324e-09, -2.7817527836759837e-08, -1.673095412213499e-10, 1.4383607549461885e-07, 4.1843614440040255e-07, -4.8577044253761414e-06, 3.689085659175362e-08, -2.8899631843160023e-07, 9.029603731391944e-09, -1.3434389600774921e-08, -1.7172943955756637e-08, -1.7357232096060216e-08, -3.602294782467652e-07]
[2025-06-12 22:09:43,267]: Mean: 0.03158943
[2025-06-12 22:09:43,268]: Min: -0.00413563
[2025-06-12 22:09:43,268]: Max: 1.05500090
[2025-06-12 22:09:43,268]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 22:09:43,306]: Sample Values (25 elements): [2.7001200131870353e-16, 0.0018855733796954155, 2.0471664185887917e-13, -3.271396248188082e-20, -4.058434921461185e-09, 1.6082115639193262e-09, 9.643490450628178e-09, -1.841692269131314e-20, -9.610360297866489e-24, -1.4079908514843562e-21, -1.635570789915164e-08, 3.053584975987178e-07, -1.0941728007940655e-19, 5.562619659652626e-20, -3.7169654443536564e-27, -1.3207840311224572e-05, -1.8163222575569813e-23, 1.8705651427808334e-07, 4.912465989548309e-10, -0.04502423107624054, 1.9057910094488761e-07, -7.384586098169166e-08, 3.565443496000853e-09, -2.795484590478736e-07, -1.8176794138583235e-22]
[2025-06-12 22:09:43,306]: Mean: -0.00000082
[2025-06-12 22:09:43,307]: Min: -0.22630230
[2025-06-12 22:09:43,307]: Max: 0.19042532
[2025-06-12 22:09:43,307]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-12 22:09:43,307]: Sample Values (25 elements): [0.1868561953306198, 0.509443461894989, 0.21172013878822327, 0.22980137169361115, 0.2150893658399582, 0.3574751317501068, 0.18423695862293243, 0.05688280239701271, 0.22881188988685608, -0.0021352400071918964, 0.46084150671958923, 0.2723950445652008, 0.4413593113422394, 0.3795338273048401, -0.00012423234875313938, 0.5775770545005798, 0.313219279050827, 0.4129473865032196, 2.5075344183278503e-06, 0.15230435132980347, -2.550818862800952e-07, 0.29129305481910706, 0.3492431044578552, 1.163588876806898e-05, 0.5019140243530273]
[2025-06-12 22:09:43,308]: Mean: 0.30867982
[2025-06-12 22:09:43,308]: Min: -0.01933582
[2025-06-12 22:09:43,308]: Max: 0.74737155
[2025-06-12 22:09:43,308]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-12 22:09:43,309]: Sample Values (25 elements): [-0.012254147790372372, -8.70557950682257e-31, -0.012531558983027935, 2.5629912023780765e-16, 0.0046245683915913105, -0.011612298898398876, -0.00329754245467484, -0.046197403222322464, 0.03779664263129234, 1.0852279402588022e-28, -0.015418405644595623, -0.010799058713018894, -0.008229395374655724, 2.393823261389443e-27, -0.005700994748622179, 0.028396040201187134, -0.038985252380371094, 0.014701089821755886, -3.425053706502718e-41, -0.003000086173415184, -0.004899723920971155, 0.012845263816416264, -0.005840636789798737, -0.024422097951173782, 1.029577188217337e-26]
[2025-06-12 22:09:43,310]: Mean: -0.00002638
[2025-06-12 22:09:43,310]: Min: -0.14007592
[2025-06-12 22:09:43,310]: Max: 0.13339344
[2025-06-12 22:09:43,310]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-12 22:09:43,310]: Sample Values (25 elements): [0.3817920386791229, 0.29509294033050537, 0.2972186803817749, 0.441928893327713, 0.32165709137916565, 0.33395612239837646, 0.23939649760723114, 0.2877677381038666, 0.1832980513572693, 0.12115254253149033, 0.32237693667411804, 0.35992151498794556, 7.088771236674063e-11, 0.3092377185821533, 3.365823968667565e-18, 0.488800585269928, 0.29377955198287964, 0.1714005023241043, 0.216404527425766, 0.3588135242462158, 0.38549453020095825, 1.4462240323676269e-27, -0.0002305221714777872, 0.07538218796253204, 0.255483478307724]
[2025-06-12 22:09:43,310]: Mean: 0.22017801
[2025-06-12 22:09:43,311]: Min: -0.01174447
[2025-06-12 22:09:43,311]: Max: 0.48880059
[2025-06-12 22:09:43,311]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 22:09:43,346]: Sample Values (25 elements): [1.5512080281301216e-13, -1.0012118245583679e-08, 9.407876255806968e-09, -1.562755187478615e-07, -4.8147063728487885e-14, -3.628389606546989e-07, -7.982485783486482e-09, -4.655379143514438e-06, 7.756812483053033e-22, 4.2792121348611545e-06, -5.070235715720628e-07, -1.1765522265488621e-14, 6.81525808010619e-11, -1.4864364139172983e-16, 1.1982196156168357e-05, 2.152558954549022e-05, -2.4674076115616117e-08, 2.5579018070320814e-11, -5.817830483860476e-13, -1.7934815091380507e-11, 1.050589435180882e-05, -1.864199020928936e-06, -6.389367257497724e-08, 1.498358699336677e-07, 2.503964680045101e-08]
[2025-06-12 22:09:43,358]: Mean: 0.00000183
[2025-06-12 22:09:43,358]: Min: -0.02535695
[2025-06-12 22:09:43,359]: Max: 0.02643546
[2025-06-12 22:09:43,359]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-12 22:09:43,360]: Sample Values (25 elements): [1.212976940223598e-06, 1.1324817933200393e-05, -7.19265017323778e-07, -1.2518023140728474e-05, 1.0859487247216748e-06, -2.6216259811917553e-06, 1.6190929272852372e-06, 0.0002750601270236075, -7.998502269401797e-07, -0.00016039112233556807, -1.7820403286350484e-07, 7.104850283212727e-06, -1.7817947082221508e-05, 0.0008433902403339744, 1.3630490229843417e-06, -0.001624598866328597, -5.58875285605609e-07, -2.223934416178963e-06, -7.080648174451198e-07, 1.759936276357621e-05, 1.745379876183506e-07, -2.240135700048995e-06, 1.8959615772473626e-05, 1.2577630741361645e-06, -7.048539600873482e-07]
[2025-06-12 22:09:43,360]: Mean: 0.01048280
[2025-06-12 22:09:43,361]: Min: -0.00482524
[2025-06-12 22:09:43,361]: Max: 0.48574373
[2025-06-12 22:09:43,361]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-12 22:09:43,404]: Sample Values (25 elements): [-4.785431519849226e-05, -8.92728557744249e-09, -9.076204516417479e-15, 6.897041965325812e-11, -5.995473041729051e-24, -7.850259748920507e-07, 8.56685346661834e-07, -2.324333297731661e-24, -1.6570126604165125e-08, -7.542634648416424e-06, 4.067178167588281e-07, 1.2780908265260965e-17, -1.908941671402431e-09, 3.3403200995138493e-13, -2.782039487669863e-08, 2.7960064471699297e-05, 9.427222891211784e-12, 8.088869094535767e-07, 5.3262532866181836e-14, 3.9774351347221515e-12, 0.003952976316213608, -5.110181028333827e-11, 2.255400049664888e-14, -8.144612217099223e-18, 2.249320087827078e-12]
[2025-06-12 22:09:43,404]: Mean: 0.00000115
[2025-06-12 22:09:43,405]: Min: -0.05604371
[2025-06-12 22:09:43,405]: Max: 0.05646411
[2025-06-12 22:09:43,405]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-12 22:09:43,407]: Sample Values (25 elements): [0.330694317817688, 0.1766626387834549, 0.24988102912902832, 0.11993386596441269, 0.28556838631629944, 0.17117840051651, 0.3414137065410614, -0.00021638095495291054, 0.10612015426158905, 0.2812339663505554, 0.4272097945213318, 0.1300630122423172, 0.03324789181351662, 0.23466598987579346, 0.4152992367744446, 0.39024606347084045, 0.08930864185094833, 0.2444569170475006, 0.23438288271427155, 0.3515826463699341, -1.473752408324223e-25, -1.6053508522517745e-10, 0.17635197937488556, 0.4127950668334961, 0.1716502159833908]
[2025-06-12 22:09:43,407]: Mean: 0.19062991
[2025-06-12 22:09:43,407]: Min: -0.04154562
[2025-06-12 22:09:43,408]: Max: 0.49953279
[2025-06-12 22:09:43,408]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-12 22:09:43,408]: Sample Values (25 elements): [-0.07838625460863113, -0.13851237297058105, 0.04791443422436714, 0.08753757178783417, 0.03819989785552025, -0.14851494133472443, -1.2805920050595887e-05, 0.0983627587556839, -0.0014872647589072585, 0.06186988204717636, -0.17829416692256927, -0.12803314626216888, -0.0006245233817026019, 0.06430315226316452, -0.034506868571043015, 0.011810900643467903, 0.027980929240584373, 0.0006256264168769121, -0.01349649392068386, -0.22219502925872803, -0.19083932042121887, 6.68322536512278e-05, 0.14047664403915405, 0.007404789328575134, -0.02288305014371872]
[2025-06-12 22:09:43,408]: Mean: 0.00079502
[2025-06-12 22:09:43,408]: Min: -0.62417370
[2025-06-12 22:09:43,409]: Max: 0.42156160
[2025-06-12 22:09:43,409]: Checkpoint of model at path [checkpoint/ResNet18_parametrized_hardtanh.ckpt] will be used for QAT
[2025-06-12 22:09:43,409]: 


QAT of ResNet18 with parametrized_hardtanh down to 4 bits...
[2025-06-12 22:09:44,526]: [ResNet18_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-06-12 22:09:44,817]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-12 22:11:43,694]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.3332 Train Acc: 0.8835 Eval Loss: 0.8497 Eval Acc: 0.7600 (LR: 0.00100000)
[2025-06-12 22:13:40,620]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.3461 Train Acc: 0.8793 Eval Loss: 0.7523 Eval Acc: 0.7794 (LR: 0.00100000)
[2025-06-12 22:15:43,423]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.3406 Train Acc: 0.8817 Eval Loss: 1.1934 Eval Acc: 0.7146 (LR: 0.00100000)
[2025-06-12 22:18:06,996]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.3380 Train Acc: 0.8838 Eval Loss: 0.5811 Eval Acc: 0.8170 (LR: 0.00100000)
[2025-06-12 22:20:04,682]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.3416 Train Acc: 0.8826 Eval Loss: 0.7538 Eval Acc: 0.7761 (LR: 0.00100000)
[2025-06-12 22:22:01,852]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.3406 Train Acc: 0.8816 Eval Loss: 0.6370 Eval Acc: 0.8051 (LR: 0.00100000)
[2025-06-12 22:24:02,552]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.3345 Train Acc: 0.8823 Eval Loss: 0.8916 Eval Acc: 0.7453 (LR: 0.00100000)
[2025-06-12 22:25:59,412]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.3340 Train Acc: 0.8844 Eval Loss: 0.4727 Eval Acc: 0.8480 (LR: 0.00100000)
[2025-06-12 22:27:55,926]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.3314 Train Acc: 0.8836 Eval Loss: 0.5020 Eval Acc: 0.8412 (LR: 0.00100000)
[2025-06-12 22:29:52,012]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.3310 Train Acc: 0.8865 Eval Loss: 0.6102 Eval Acc: 0.8021 (LR: 0.00100000)
[2025-06-12 22:31:46,771]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.3258 Train Acc: 0.8868 Eval Loss: 0.6676 Eval Acc: 0.7971 (LR: 0.00100000)
[2025-06-12 22:33:37,633]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.3205 Train Acc: 0.8890 Eval Loss: 1.0818 Eval Acc: 0.7123 (LR: 0.00100000)
[2025-06-12 22:35:29,412]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.3146 Train Acc: 0.8911 Eval Loss: 0.7815 Eval Acc: 0.7867 (LR: 0.00100000)
[2025-06-12 22:37:24,071]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.3163 Train Acc: 0.8892 Eval Loss: 0.7548 Eval Acc: 0.7874 (LR: 0.00010000)
[2025-06-12 22:39:27,313]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.2190 Train Acc: 0.9247 Eval Loss: 0.3020 Eval Acc: 0.9048 (LR: 0.00010000)
[2025-06-12 22:41:25,236]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.1841 Train Acc: 0.9370 Eval Loss: 0.3024 Eval Acc: 0.9090 (LR: 0.00010000)
[2025-06-12 22:43:24,410]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.1706 Train Acc: 0.9418 Eval Loss: 0.3200 Eval Acc: 0.9026 (LR: 0.00010000)
[2025-06-12 22:45:22,342]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.1634 Train Acc: 0.9424 Eval Loss: 0.2958 Eval Acc: 0.9109 (LR: 0.00010000)
[2025-06-12 22:47:19,058]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.1571 Train Acc: 0.9445 Eval Loss: 0.3154 Eval Acc: 0.9084 (LR: 0.00010000)
[2025-06-12 22:49:14,676]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.1445 Train Acc: 0.9500 Eval Loss: 0.3071 Eval Acc: 0.9107 (LR: 0.00010000)
[2025-06-12 22:51:09,945]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.1419 Train Acc: 0.9509 Eval Loss: 0.3032 Eval Acc: 0.9132 (LR: 0.00010000)
[2025-06-12 22:53:08,710]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.1342 Train Acc: 0.9534 Eval Loss: 0.3252 Eval Acc: 0.9078 (LR: 0.00010000)
[2025-06-12 22:55:03,991]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.1311 Train Acc: 0.9547 Eval Loss: 0.3053 Eval Acc: 0.9126 (LR: 0.00010000)
[2025-06-12 22:56:57,282]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.1272 Train Acc: 0.9557 Eval Loss: 0.3042 Eval Acc: 0.9147 (LR: 0.00001000)
[2025-06-12 22:58:54,167]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.1160 Train Acc: 0.9592 Eval Loss: 0.2967 Eval Acc: 0.9182 (LR: 0.00001000)
[2025-06-12 23:00:46,936]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.1092 Train Acc: 0.9625 Eval Loss: 0.3000 Eval Acc: 0.9156 (LR: 0.00001000)
[2025-06-12 23:02:40,141]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.1078 Train Acc: 0.9613 Eval Loss: 0.3013 Eval Acc: 0.9163 (LR: 0.00001000)
[2025-06-12 23:04:33,069]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.1057 Train Acc: 0.9635 Eval Loss: 0.2995 Eval Acc: 0.9178 (LR: 0.00001000)
[2025-06-12 23:06:25,770]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.1051 Train Acc: 0.9637 Eval Loss: 0.2953 Eval Acc: 0.9179 (LR: 0.00001000)
[2025-06-12 23:08:22,610]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.1020 Train Acc: 0.9653 Eval Loss: 0.2981 Eval Acc: 0.9174 (LR: 0.00001000)
[2025-06-12 23:10:15,781]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.1051 Train Acc: 0.9643 Eval Loss: 0.3003 Eval Acc: 0.9180 (LR: 0.00001000)
[2025-06-12 23:12:11,339]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.1009 Train Acc: 0.9641 Eval Loss: 0.3006 Eval Acc: 0.9179 (LR: 0.00001000)
[2025-06-12 23:14:04,664]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.1008 Train Acc: 0.9656 Eval Loss: 0.2991 Eval Acc: 0.9183 (LR: 0.00001000)
[2025-06-12 23:15:58,689]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.1015 Train Acc: 0.9652 Eval Loss: 0.3025 Eval Acc: 0.9148 (LR: 0.00001000)
[2025-06-12 23:17:51,651]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.1000 Train Acc: 0.9652 Eval Loss: 0.2985 Eval Acc: 0.9177 (LR: 0.00000100)
[2025-06-12 23:19:44,549]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.0947 Train Acc: 0.9676 Eval Loss: 0.3022 Eval Acc: 0.9166 (LR: 0.00000100)
[2025-06-12 23:21:37,408]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.0979 Train Acc: 0.9654 Eval Loss: 0.2992 Eval Acc: 0.9177 (LR: 0.00000100)
[2025-06-12 23:23:30,693]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.0974 Train Acc: 0.9671 Eval Loss: 0.3017 Eval Acc: 0.9179 (LR: 0.00000100)
[2025-06-12 23:25:24,405]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.0965 Train Acc: 0.9671 Eval Loss: 0.3004 Eval Acc: 0.9188 (LR: 0.00000100)
[2025-06-12 23:27:17,793]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.0977 Train Acc: 0.9662 Eval Loss: 0.2986 Eval Acc: 0.9196 (LR: 0.00000100)
[2025-06-12 23:29:10,847]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.0964 Train Acc: 0.9659 Eval Loss: 0.2984 Eval Acc: 0.9187 (LR: 0.00000010)
[2025-06-12 23:31:03,280]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.0987 Train Acc: 0.9664 Eval Loss: 0.2986 Eval Acc: 0.9183 (LR: 0.00000010)
[2025-06-12 23:32:55,788]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.0959 Train Acc: 0.9672 Eval Loss: 0.2991 Eval Acc: 0.9190 (LR: 0.00000010)
[2025-06-12 23:34:48,182]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.0968 Train Acc: 0.9665 Eval Loss: 0.3010 Eval Acc: 0.9169 (LR: 0.00000010)
[2025-06-12 23:36:40,555]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.0985 Train Acc: 0.9665 Eval Loss: 0.3033 Eval Acc: 0.9177 (LR: 0.00000010)
[2025-06-12 23:38:33,393]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.0948 Train Acc: 0.9676 Eval Loss: 0.2985 Eval Acc: 0.9180 (LR: 0.00000010)
[2025-06-12 23:40:25,789]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.0957 Train Acc: 0.9671 Eval Loss: 0.3029 Eval Acc: 0.9162 (LR: 0.00000010)
[2025-06-12 23:42:25,630]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.0977 Train Acc: 0.9656 Eval Loss: 0.2998 Eval Acc: 0.9189 (LR: 0.00000010)
[2025-06-12 23:44:18,424]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.0974 Train Acc: 0.9666 Eval Loss: 0.2965 Eval Acc: 0.9183 (LR: 0.00000010)
[2025-06-12 23:46:12,193]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.0968 Train Acc: 0.9671 Eval Loss: 0.3046 Eval Acc: 0.9175 (LR: 0.00000010)
[2025-06-12 23:48:07,967]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.0954 Train Acc: 0.9679 Eval Loss: 0.2999 Eval Acc: 0.9186 (LR: 0.00000010)
[2025-06-12 23:50:01,184]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.0965 Train Acc: 0.9659 Eval Loss: 0.2994 Eval Acc: 0.9194 (LR: 0.00000010)
[2025-06-12 23:51:58,221]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.0963 Train Acc: 0.9673 Eval Loss: 0.3004 Eval Acc: 0.9179 (LR: 0.00000010)
[2025-06-12 23:53:51,846]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.0960 Train Acc: 0.9666 Eval Loss: 0.3016 Eval Acc: 0.9176 (LR: 0.00000010)
[2025-06-12 23:55:46,316]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.0981 Train Acc: 0.9663 Eval Loss: 0.3003 Eval Acc: 0.9170 (LR: 0.00000010)
[2025-06-12 23:57:40,995]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.0975 Train Acc: 0.9656 Eval Loss: 0.3008 Eval Acc: 0.9177 (LR: 0.00000010)
[2025-06-12 23:59:35,061]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.0990 Train Acc: 0.9663 Eval Loss: 0.2985 Eval Acc: 0.9177 (LR: 0.00000010)
[2025-06-13 00:01:26,972]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.0969 Train Acc: 0.9661 Eval Loss: 0.3004 Eval Acc: 0.9180 (LR: 0.00000010)
[2025-06-13 00:03:18,524]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.0973 Train Acc: 0.9663 Eval Loss: 0.3025 Eval Acc: 0.9168 (LR: 0.00000010)
[2025-06-13 00:05:12,074]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.0960 Train Acc: 0.9666 Eval Loss: 0.2998 Eval Acc: 0.9181 (LR: 0.00000010)
[2025-06-13 00:05:12,074]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.9196
[2025-06-13 00:05:12,278]: 


Quantization of model down to 4 bits finished
[2025-06-13 00:05:12,278]: Model Architecture:
[2025-06-13 00:05:12,565]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1158], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8682172298431396, max_val=0.8682172298431396)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0476], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38052576780319214, max_val=0.33284395933151245)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0471], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0538], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4216371178627014, max_val=0.38559895753860474)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0931], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0519], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4378201365470886, max_val=0.3399578332901001)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0238], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0469], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.367947518825531, max_val=0.33593451976776123)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0870], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3287595510482788, max_val=0.34561794996261597)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0362], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26331305503845215, max_val=0.2793150842189789)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0515], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36573413014411926, max_val=0.40739142894744873)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0779], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0356], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2689815163612366, max_val=0.26558005809783936)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0161], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0331], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2587605118751526, max_val=0.23707976937294006)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0569], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0315], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23023319244384766, max_val=0.24151891469955444)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0166], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0319], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23155325651168823, max_val=0.24679002165794373)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0435], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3262489438056946, max_val=0.32686328887939453)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0468], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0320], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2576431632041931, max_val=0.2228688895702362)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0393], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3216101825237274, max_val=0.2684781551361084)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0304], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0490], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.390520840883255, max_val=0.345104455947876)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3243], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0320], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.251465380191803, max_val=0.2282269299030304)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0212], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16455934941768646, max_val=0.15374544262886047)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1819], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05494016781449318, max_val=0.048966746777296066)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1146], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08363334834575653, max_val=0.10474126040935516)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1024], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-13 00:05:12,576]: 
Model Weights:
[2025-06-13 00:05:12,576]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-13 00:05:12,577]: Sample Values (25 elements): [0.22501209378242493, -0.017491020262241364, 0.04342219606041908, 0.13834623992443085, 0.07921715825796127, 0.00790798757225275, 0.024530161172151566, 0.0005693348939530551, 0.2297171801328659, -0.022156214341521263, -0.14836819469928741, 0.15923091769218445, 0.040386490523815155, 0.05455049127340317, 0.1221243292093277, -0.023565804585814476, 0.1418110430240631, 0.03860912844538689, -0.09208204597234726, 0.02362576685845852, -0.3688831031322479, -0.16010016202926636, 0.0009388928883709013, -0.025714220479130745, -0.11669355630874634]
[2025-06-13 00:05:12,592]: Mean: 0.00097816
[2025-06-13 00:05:12,597]: Min: -0.44768035
[2025-06-13 00:05:12,600]: Max: 0.40174010
[2025-06-13 00:05:12,600]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-13 00:05:12,601]: Sample Values (25 elements): [0.9460594654083252, 0.7060030698776245, 0.5615857839584351, 1.0962504148483276, 0.8442663550376892, 0.8092412948608398, 0.9411927461624146, 0.4942573606967926, 0.42526331543922424, 0.5456287860870361, 0.8664661049842834, 0.3161450922489166, 0.3870541453361511, 0.6449013948440552, 0.726996660232544, 0.5889285802841187, 0.4940873086452484, 0.4187180697917938, 0.5598623752593994, 0.6340975761413574, 0.42157894372940063, 0.3459568917751312, 0.672594428062439, 0.6902030110359192, 0.5203987956047058]
[2025-06-13 00:05:12,601]: Mean: 0.59902126
[2025-06-13 00:05:12,601]: Min: 0.17924301
[2025-06-13 00:05:12,601]: Max: 1.09625041
[2025-06-13 00:05:12,603]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 00:05:12,603]: Sample Values (25 elements): [0.04755798354744911, -0.19023193418979645, 0.04755798354744911, 0.0, 0.04755798354744911, 0.04755798354744911, 0.0, -0.04755798354744911, 0.04755798354744911, 0.04755798354744911, -0.14267395436763763, 0.0, -0.04755798354744911, 0.0, 0.04755798354744911, 0.0, -0.04755798354744911, -0.04755798354744911, 0.04755798354744911, 0.0, 0.04755798354744911, 0.0, 0.0, -0.09511596709489822, -0.09511596709489822]
[2025-06-13 00:05:12,604]: Mean: -0.00018448
[2025-06-13 00:05:12,604]: Min: -0.38046387
[2025-06-13 00:05:12,604]: Max: 0.33290589
[2025-06-13 00:05:12,604]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 00:05:12,604]: Sample Values (25 elements): [0.281790167093277, 0.6549144387245178, 0.5240711569786072, 0.418575644493103, 0.4286564588546753, 0.2088775932788849, 0.41129058599472046, 0.35782384872436523, 0.6052249670028687, 0.4903881847858429, 0.19875431060791016, 0.38438165187835693, 0.6666345596313477, 0.71636962890625, 0.39925146102905273, 0.45460379123687744, 0.5334870219230652, 0.3756006062030792, 0.2967814803123474, 0.46681833267211914, 0.5001768469810486, 0.6962096691131592, 0.5340276956558228, 0.33781903982162476, 0.3548186421394348]
[2025-06-13 00:05:12,604]: Mean: 0.47925538
[2025-06-13 00:05:12,605]: Min: 0.17634889
[2025-06-13 00:05:12,605]: Max: 0.73489702
[2025-06-13 00:05:12,606]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 00:05:12,607]: Sample Values (25 elements): [0.10763148218393326, -0.05381574109196663, -0.05381574109196663, 0.0, 0.10763148218393326, 0.05381574109196663, -0.05381574109196663, -0.16144722700119019, -0.05381574109196663, 0.05381574109196663, 0.10763148218393326, -0.10763148218393326, -0.05381574109196663, -0.10763148218393326, 0.0, 0.05381574109196663, 0.0, -0.05381574109196663, 0.0, 0.05381574109196663, 0.16144722700119019, 0.0, -0.10763148218393326, -0.05381574109196663, 0.0]
[2025-06-13 00:05:12,607]: Mean: -0.00030657
[2025-06-13 00:05:12,607]: Min: -0.43052593
[2025-06-13 00:05:12,607]: Max: 0.37671018
[2025-06-13 00:05:12,607]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 00:05:12,608]: Sample Values (25 elements): [0.6459773778915405, 0.7054980397224426, 0.8896185159683228, 0.4988148808479309, 0.6996988654136658, 0.6080320477485657, 0.8308605551719666, 0.6608932018280029, 0.6430051326751709, 0.7887092232704163, 0.7545918822288513, 0.6103723049163818, 0.5603325366973877, 0.7187665700912476, 0.7212624549865723, 0.6702821254730225, 0.7570977807044983, 0.42715343832969666, 0.5817316770553589, 0.44424623250961304, 0.6642266511917114, 0.6429400444030762, 0.6669347286224365, 0.7193347215652466, 0.649712085723877]
[2025-06-13 00:05:12,608]: Mean: 0.65146190
[2025-06-13 00:05:12,608]: Min: 0.35180727
[2025-06-13 00:05:12,608]: Max: 0.88961852
[2025-06-13 00:05:12,609]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 00:05:12,610]: Sample Values (25 elements): [0.051851868629455566, 0.051851868629455566, -0.10370373725891113, 0.0, 0.051851868629455566, -0.051851868629455566, 0.051851868629455566, 0.0, 0.0, -0.051851868629455566, 0.051851868629455566, 0.10370373725891113, -0.1555556058883667, 0.0, 0.0, 0.1555556058883667, 0.1555556058883667, 0.0, 0.0, 0.0, 0.0, 0.10370373725891113, 0.051851868629455566, 0.051851868629455566, 0.0]
[2025-06-13 00:05:12,610]: Mean: -0.00058232
[2025-06-13 00:05:12,610]: Min: -0.41481495
[2025-06-13 00:05:12,610]: Max: 0.36296308
[2025-06-13 00:05:12,611]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 00:05:12,611]: Sample Values (25 elements): [0.33745038509368896, 0.443715900182724, 0.47243762016296387, 0.4819691479206085, 0.42938652634620667, 0.4477192759513855, 0.5761919021606445, 0.39016568660736084, 0.28260529041290283, 0.3849483132362366, 0.4423505961894989, 0.4353592097759247, 0.4091430902481079, 0.518442690372467, 0.44944527745246887, 0.5346569418907166, 0.5692663192749023, 0.6042329668998718, 0.41229915618896484, 0.23893573880195618, 0.53619384765625, 0.4986779987812042, 0.491204172372818, 0.5250921845436096, 0.4259576201438904]
[2025-06-13 00:05:12,611]: Mean: 0.44285470
[2025-06-13 00:05:12,611]: Min: 0.23893574
[2025-06-13 00:05:12,611]: Max: 0.68553096
[2025-06-13 00:05:12,613]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 00:05:12,613]: Sample Values (25 elements): [-0.14077641069889069, -0.04692547023296356, 0.0, -0.04692547023296356, 0.09385094046592712, -0.18770188093185425, 0.14077641069889069, -0.14077641069889069, -0.04692547023296356, -0.04692547023296356, 0.0, -0.09385094046592712, 0.04692547023296356, 0.04692547023296356, 0.0, 0.0, 0.14077641069889069, 0.04692547023296356, 0.0, -0.04692547023296356, -0.04692547023296356, -0.04692547023296356, 0.04692547023296356, -0.09385094046592712, 0.2346273511648178]
[2025-06-13 00:05:12,613]: Mean: 0.00042007
[2025-06-13 00:05:12,614]: Min: -0.37540376
[2025-06-13 00:05:12,614]: Max: 0.32847828
[2025-06-13 00:05:12,614]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 00:05:12,614]: Sample Values (25 elements): [0.6594403982162476, 0.5372105240821838, 0.5891183018684387, 0.4484291970729828, 0.5629509091377258, 0.8906773924827576, 0.4910579025745392, 0.7346886992454529, 0.3312082290649414, 0.7634055614471436, 0.5149720311164856, 0.5597862601280212, 0.3737441301345825, 0.9391065835952759, 0.8835873007774353, 0.8892426490783691, 0.6989553570747375, 0.7050522565841675, 0.7755908370018005, 0.5347500443458557, 0.27665263414382935, 0.7781511545181274, 0.7404100894927979, 0.6686909198760986, 0.7851278781890869]
[2025-06-13 00:05:12,614]: Mean: 0.65135193
[2025-06-13 00:05:12,615]: Min: 0.27665263
[2025-06-13 00:05:12,615]: Max: 1.06854737
[2025-06-13 00:05:12,616]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-13 00:05:12,617]: Sample Values (25 elements): [0.08991700410842896, 0.0, 0.0, -0.04495850205421448, -0.08991700410842896, 0.04495850205421448, -0.04495850205421448, -0.04495850205421448, -0.04495850205421448, 0.08991700410842896, 0.0, -0.08991700410842896, 0.08991700410842896, 0.0, -0.04495850205421448, 0.08991700410842896, 0.0, 0.0, -0.08991700410842896, 0.04495850205421448, 0.0, 0.04495850205421448, 0.08991700410842896, -0.08991700410842896, 0.0]
[2025-06-13 00:05:12,617]: Mean: 0.00035368
[2025-06-13 00:05:12,617]: Min: -0.31470951
[2025-06-13 00:05:12,617]: Max: 0.35966802
[2025-06-13 00:05:12,617]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-13 00:05:12,618]: Sample Values (25 elements): [0.19205734133720398, 0.05741310119628906, 0.32249659299850464, 0.316758930683136, 0.23512230813503265, 0.29752641916275024, 0.2756880819797516, 0.2207978069782257, 0.17065265774726868, 0.3252798914909363, 0.3019388020038605, 0.19975201785564423, 0.3563060760498047, 0.277677059173584, 0.2526509761810303, 0.4031743109226227, 0.274441123008728, 0.26869094371795654, 0.3596251904964447, 0.29119277000427246, 0.35432755947113037, 0.2983705699443817, 0.34073469042778015, 0.35071074962615967, 0.31475576758384705]
[2025-06-13 00:05:12,618]: Mean: 0.28235197
[2025-06-13 00:05:12,618]: Min: -0.00331629
[2025-06-13 00:05:12,618]: Max: 0.49591506
[2025-06-13 00:05:12,619]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 00:05:12,620]: Sample Values (25 elements): [0.0, -0.036175210028886795, 0.0, -0.07235042005777359, -0.036175210028886795, -0.07235042005777359, 0.0, -0.036175210028886795, -0.036175210028886795, 0.07235042005777359, 0.0, 0.036175210028886795, -0.036175210028886795, -0.036175210028886795, 0.036175210028886795, -0.036175210028886795, 0.07235042005777359, 0.18087604641914368, -0.036175210028886795, 0.10852563381195068, 0.0, 0.0, -0.036175210028886795, 0.18087604641914368, -0.07235042005777359]
[2025-06-13 00:05:12,621]: Mean: -0.00000761
[2025-06-13 00:05:12,621]: Min: -0.25322646
[2025-06-13 00:05:12,621]: Max: 0.28940168
[2025-06-13 00:05:12,621]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-13 00:05:12,621]: Sample Values (25 elements): [0.579730749130249, 0.39244139194488525, 0.7141198515892029, 0.5198956727981567, 0.3520124852657318, 0.3122885823249817, 0.5786269307136536, 0.623261570930481, 0.4969342350959778, 0.6279515624046326, 0.618610143661499, 0.6600582599639893, 0.635628879070282, 0.71631920337677, 0.7409628629684448, 0.6219509243965149, 0.5760460495948792, 0.2780139148235321, 0.6478391885757446, 0.5811300873756409, 0.40422579646110535, 0.5921099185943604, 0.49312156438827515, 0.30421552062034607, 0.37176060676574707]
[2025-06-13 00:05:12,621]: Mean: 0.54764742
[2025-06-13 00:05:12,622]: Min: 0.20084403
[2025-06-13 00:05:12,622]: Max: 0.82647002
[2025-06-13 00:05:12,623]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-13 00:05:12,623]: Sample Values (25 elements): [0.051541704684495926, 0.10308340936899185, 0.0, 0.10308340936899185, 0.051541704684495926, -0.10308340936899185, -0.10308340936899185, 0.0, 0.051541704684495926, 0.10308340936899185, -0.051541704684495926, -0.10308340936899185, 0.051541704684495926, 0.051541704684495926, 0.2061668187379837, 0.051541704684495926, 0.10308340936899185, -0.25770851969718933, -0.051541704684495926, 0.15462511777877808, 0.051541704684495926, 0.15462511777877808, 0.051541704684495926, -0.10308340936899185, 0.10308340936899185]
[2025-06-13 00:05:12,623]: Mean: 0.00096892
[2025-06-13 00:05:12,623]: Min: -0.36079192
[2025-06-13 00:05:12,623]: Max: 0.41233364
[2025-06-13 00:05:12,623]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-13 00:05:12,624]: Sample Values (25 elements): [0.35069477558135986, 0.26304808259010315, 0.2619418203830719, 0.2920452356338501, 0.41388314962387085, 0.4975276291370392, 0.48006671667099, 0.46055153012275696, 0.20532988011837006, 0.3971705734729767, 0.20288830995559692, 0.42089205980300903, 0.5150838494300842, 0.20530812442302704, 0.2950918674468994, 0.33617493510246277, 0.3740391731262207, 0.28630220890045166, 0.39019420742988586, 0.3048083484172821, 0.379518061876297, 0.314960241317749, 0.3742407262325287, 0.4817201793193817, 0.33363449573516846]
[2025-06-13 00:05:12,624]: Mean: 0.34811506
[2025-06-13 00:05:12,624]: Min: 0.13102649
[2025-06-13 00:05:12,624]: Max: 0.65160304
[2025-06-13 00:05:12,625]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 00:05:12,626]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.03563743457198143, -0.07127486914396286, -0.03563743457198143, 0.03563743457198143, 0.0, -0.07127486914396286, 0.03563743457198143, 0.07127486914396286, 0.0, 0.03563743457198143, 0.03563743457198143, 0.03563743457198143, 0.0, -0.03563743457198143, -0.07127486914396286, 0.10691229999065399, 0.0, 0.03563743457198143, 0.14254973828792572, 0.03563743457198143, 0.0, 0.03563743457198143]
[2025-06-13 00:05:12,627]: Mean: 0.00024265
[2025-06-13 00:05:12,627]: Min: -0.28509948
[2025-06-13 00:05:12,627]: Max: 0.24946204
[2025-06-13 00:05:12,627]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-13 00:05:12,627]: Sample Values (25 elements): [0.1454877406358719, 0.2677581310272217, 0.2842063903808594, 0.10922904312610626, 0.2501492202281952, 0.27756062150001526, 0.2836446464061737, 0.31916555762290955, 0.18833205103874207, 0.20695438981056213, 0.19371534883975983, 0.24365022778511047, 0.19809366762638092, 0.11806386709213257, 0.20899483561515808, 0.22335976362228394, 0.2299477756023407, 0.27633169293403625, 0.3152034282684326, 0.3549114167690277, 0.20722438395023346, 0.1826711744070053, 0.18046535551548004, 0.2744278907775879, 0.21228985488414764]
[2025-06-13 00:05:12,628]: Mean: 0.23942867
[2025-06-13 00:05:12,628]: Min: 0.10058256
[2025-06-13 00:05:12,628]: Max: 0.47342610
[2025-06-13 00:05:12,629]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 00:05:12,630]: Sample Values (25 elements): [0.03305601701140404, -0.09916804730892181, 0.03305601701140404, 0.03305601701140404, 0.0, 0.09916804730892181, 0.03305601701140404, -0.06611203402280807, 0.0, -0.03305601701140404, 0.0, 0.03305601701140404, -0.03305601701140404, -0.03305601701140404, 0.06611203402280807, 0.0, 0.03305601701140404, 0.03305601701140404, 0.0, 0.0, 0.06611203402280807, -0.03305601701140404, 0.09916804730892181, -0.09916804730892181, 0.0]
[2025-06-13 00:05:12,630]: Mean: -0.00013652
[2025-06-13 00:05:12,631]: Min: -0.26444814
[2025-06-13 00:05:12,631]: Max: 0.23139212
[2025-06-13 00:05:12,631]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-13 00:05:12,634]: Sample Values (25 elements): [0.4156551957130432, 0.3494395911693573, 0.564245343208313, 0.34276139736175537, 0.4986588656902313, 0.39871829748153687, 0.466447651386261, 0.42992323637008667, 0.3645367920398712, 0.4291549026966095, 0.4378565847873688, 0.4974305331707001, 0.46613091230392456, 0.4772767424583435, 0.4921111464500427, 0.5504564046859741, 0.3719226121902466, 0.5197733044624329, 0.4831181764602661, 0.5449233651161194, 0.5244945883750916, 0.7244616746902466, 0.3367834687232971, 0.519481360912323, 0.4766642451286316]
[2025-06-13 00:05:12,635]: Mean: 0.44519931
[2025-06-13 00:05:12,635]: Min: 0.01457113
[2025-06-13 00:05:12,635]: Max: 0.72446167
[2025-06-13 00:05:12,636]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-13 00:05:12,640]: Sample Values (25 elements): [-0.06290028244256973, -0.06290028244256973, -0.031450141221284866, 0.0, -0.06290028244256973, 0.0, 0.0943504273891449, 0.0, -0.031450141221284866, 0.0, 0.0, 0.0943504273891449, -0.031450141221284866, -0.031450141221284866, 0.06290028244256973, -0.031450141221284866, 0.031450141221284866, 0.031450141221284866, 0.031450141221284866, 0.0, 0.031450141221284866, 0.0, 0.0, -0.031450141221284866, -0.031450141221284866]
[2025-06-13 00:05:12,640]: Mean: 0.00003839
[2025-06-13 00:05:12,640]: Min: -0.22015099
[2025-06-13 00:05:12,640]: Max: 0.25160113
[2025-06-13 00:05:12,640]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-13 00:05:12,641]: Sample Values (25 elements): [0.26636117696762085, 0.19424518942832947, 0.2349395900964737, 0.17987768352031708, 0.17959798872470856, 0.18549562990665436, 0.14231818914413452, 0.1768062710762024, 0.18474578857421875, 0.1884990781545639, 0.17641417682170868, 0.16474224627017975, 0.2303621768951416, 0.14323030412197113, 0.23818126320838928, 0.24724964797496796, 0.17828595638275146, 0.16596631705760956, 0.29611197113990784, 0.20024771988391876, 0.17257298529148102, 0.2080678939819336, 0.16402095556259155, 0.1970224827528, 0.17540760338306427]
[2025-06-13 00:05:12,641]: Mean: 0.19402863
[2025-06-13 00:05:12,641]: Min: -0.00016914
[2025-06-13 00:05:12,641]: Max: 0.37124646
[2025-06-13 00:05:12,642]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 00:05:12,648]: Sample Values (25 elements): [0.0, -0.031889550387859344, 0.031889550387859344, 0.0, -0.031889550387859344, -0.09566865116357803, 0.031889550387859344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.031889550387859344, 0.0, 0.0, -0.031889550387859344, 0.0, 0.031889550387859344, 0.0, 0.0, 0.0, 0.0, 0.031889550387859344, 0.031889550387859344]
[2025-06-13 00:05:12,648]: Mean: -0.00002752
[2025-06-13 00:05:12,649]: Min: -0.22322685
[2025-06-13 00:05:12,649]: Max: 0.25511640
[2025-06-13 00:05:12,649]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-13 00:05:12,649]: Sample Values (25 elements): [0.5205445289611816, 0.41715312004089355, 0.36073195934295654, 0.4587036073207855, 0.3282950520515442, 0.30812034010887146, 0.434387743473053, 0.36285725235939026, 0.37831130623817444, 0.3732844293117523, 0.12814229726791382, 0.5263581275939941, 0.4718593955039978, 0.2991526126861572, 0.09275871515274048, 0.22646188735961914, 0.31116050481796265, 0.1831950545310974, 0.1105852872133255, 0.18209919333457947, 0.0004106480337213725, 0.305613249540329, 0.2802928686141968, 0.4819953441619873, -0.023164160549640656]
[2025-06-13 00:05:12,649]: Mean: 0.32170060
[2025-06-13 00:05:12,649]: Min: -0.02616058
[2025-06-13 00:05:12,649]: Max: 0.63091797
[2025-06-13 00:05:12,650]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-13 00:05:12,651]: Sample Values (25 elements): [0.08708163350820541, 0.08708163350820541, 0.04354081675410271, 0.04354081675410271, 0.0, -0.08708163350820541, 0.0, -0.08708163350820541, 0.13062244653701782, -0.08708163350820541, 0.0, 0.0, -0.08708163350820541, 0.04354081675410271, 0.04354081675410271, 0.08708163350820541, -0.08708163350820541, 0.04354081675410271, 0.04354081675410271, 0.0, 0.04354081675410271, 0.0, 0.04354081675410271, 0.04354081675410271, 0.0]
[2025-06-13 00:05:12,651]: Mean: -0.00055808
[2025-06-13 00:05:12,651]: Min: -0.30478573
[2025-06-13 00:05:12,651]: Max: 0.34832653
[2025-06-13 00:05:12,651]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-13 00:05:12,652]: Sample Values (25 elements): [0.4231089651584625, 0.4057234227657318, 0.13646715879440308, 0.15220801532268524, 0.26911479234695435, 0.29899001121520996, 0.22880198061466217, 0.1353902816772461, 0.1980210691690445, 0.07338601350784302, 0.0905192494392395, 0.22543953359127045, 0.1720796674489975, 0.16742146015167236, 0.36881139874458313, 0.2538212835788727, 0.15123076736927032, 0.24499285221099854, 0.19713549315929413, 0.1903279423713684, 0.32820868492126465, 0.2742983400821686, 0.1109294518828392, 0.16520613431930542, -3.09872859816096e-07]
[2025-06-13 00:05:12,666]: Mean: 0.22545144
[2025-06-13 00:05:12,666]: Min: -0.00641741
[2025-06-13 00:05:12,667]: Max: 0.57032979
[2025-06-13 00:05:12,668]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 00:05:12,673]: Sample Values (25 elements): [0.0, -0.03203414008021355, 0.0640682801604271, 0.03203414008021355, 0.03203414008021355, 0.0, 0.03203414008021355, 0.0, -0.0640682801604271, 0.03203414008021355, -0.0640682801604271, -0.03203414008021355, 0.0, 0.0, 0.0, 0.0, 0.0640682801604271, -0.03203414008021355, -0.0640682801604271, -0.03203414008021355, 0.0, 0.0, -0.03203414008021355, 0.0, -0.03203414008021355]
[2025-06-13 00:05:12,673]: Mean: -0.00001613
[2025-06-13 00:05:12,673]: Min: -0.25627312
[2025-06-13 00:05:12,673]: Max: 0.22423898
[2025-06-13 00:05:12,673]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-13 00:05:12,674]: Sample Values (25 elements): [0.14816321432590485, 0.12118392437696457, 0.07031772285699844, 0.11213896423578262, 0.14845699071884155, 0.12755164504051208, 0.05542019382119179, 0.1091337502002716, 0.19140923023223877, 0.14184196293354034, 0.1281345784664154, 0.17929580807685852, 0.10315489023923874, 0.08352719992399216, -0.011577557772397995, 0.1402025818824768, 0.2128632515668869, 0.32983168959617615, 0.12572656571865082, 0.16886135935783386, 0.1227012500166893, 0.13971514999866486, 0.07213456183671951, 0.11302769929170609, 0.13298358023166656]
[2025-06-13 00:05:12,674]: Mean: 0.12151061
[2025-06-13 00:05:12,674]: Min: -0.01157756
[2025-06-13 00:05:12,674]: Max: 0.32983169
[2025-06-13 00:05:12,675]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 00:05:12,682]: Sample Values (25 elements): [0.0, 0.0, 0.039339225739240646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039339225739240646, 0.0, 0.0, -0.039339225739240646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07867845147848129]
[2025-06-13 00:05:12,683]: Mean: -0.00012759
[2025-06-13 00:05:12,683]: Min: -0.31471381
[2025-06-13 00:05:12,683]: Max: 0.27537459
[2025-06-13 00:05:12,683]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-13 00:05:12,683]: Sample Values (25 elements): [0.33389589190483093, 0.0039775012992322445, 0.41066640615463257, 0.3758734464645386, 0.20080403983592987, 0.1321943700313568, 0.2489631474018097, 0.4688316881656647, 0.03663201257586479, 0.03815801441669464, 0.3636237382888794, 0.35799530148506165, 0.3062252104282379, 0.058671437203884125, 0.3910936415195465, 0.48091650009155273, 0.0627988874912262, 0.19392463564872742, 0.09381473809480667, 0.013559151440858841, 0.056810412555933, 0.42947837710380554, 0.2890881597995758, 0.31728219985961914, 0.07700929790735245]
[2025-06-13 00:05:12,684]: Mean: 0.21604544
[2025-06-13 00:05:12,684]: Min: -0.03193309
[2025-06-13 00:05:12,684]: Max: 0.53519744
[2025-06-13 00:05:12,685]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-13 00:05:12,699]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 00:05:12,699]: Mean: 0.00002370
[2025-06-13 00:05:12,699]: Min: -0.39233351
[2025-06-13 00:05:12,700]: Max: 0.34329182
[2025-06-13 00:05:12,700]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-13 00:05:12,700]: Sample Values (25 elements): [-5.463662712402462e-41, 5.745043444038885e-41, 5.314144166259004e-41, 5.771387855168192e-41, -5.989850285756431e-41, 6.003022491321084e-41, -6.109381044763337e-41, -5.578429056630664e-41, 5.455815441002243e-41, -5.008520971189761e-41, 6.183930123065418e-41, -4.979233833285372e-41, 6.128298574031722e-41, -5.884332511392772e-41, 5.871580695367416e-41, 1.0205347851510727e-11, 0.38759464025497437, 5.952856006298255e-41, -5.224180804849351e-41, 5.042712653719287e-41, 5.891198873867963e-41, -5.285417547740345e-41, -5.380705843314433e-41, 5.270844043711367e-41, -5.30321403823727e-41]
[2025-06-13 00:05:12,700]: Mean: 0.02508374
[2025-06-13 00:05:12,700]: Min: -0.00000000
[2025-06-13 00:05:12,701]: Max: 1.03419924
[2025-06-13 00:05:12,702]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 00:05:12,739]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 00:05:12,739]: Mean: -0.00000020
[2025-06-13 00:05:12,740]: Min: -0.25583589
[2025-06-13 00:05:12,740]: Max: 0.22385640
[2025-06-13 00:05:12,740]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-13 00:05:12,740]: Sample Values (25 elements): [0.5450214743614197, 0.30426231026649475, 0.5618466138839722, 0.2936861217021942, 0.05921563506126404, -0.0004551051533780992, 0.6159381866455078, 0.14233610033988953, 0.0005147409392520785, 0.16743622720241547, 0.15734338760375977, 0.43514665961265564, 0.32780247926712036, 0.41938814520835876, 0.44048869609832764, -5.667131249422425e-41, -4.922901635019515e-41, 0.5941574573516846, 0.384306401014328, 0.37979841232299805, 0.3819273114204407, 0.11964000761508942, -0.023920049890875816, 0.45130518078804016, 0.3701721727848053]
[2025-06-13 00:05:12,740]: Mean: 0.26155791
[2025-06-13 00:05:12,741]: Min: -0.02392005
[2025-06-13 00:05:12,741]: Max: 0.71933383
[2025-06-13 00:05:12,742]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-13 00:05:12,743]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.042440637946128845, -0.021220318973064423, 0.021220318973064423, 0.0, 0.0, 0.042440637946128845, 0.0, 0.0, 0.0, 0.042440637946128845, 0.0, 0.0, 0.0, 0.0, 0.021220318973064423, 0.0, 0.0, -0.021220318973064423, -0.021220318973064423, 0.021220318973064423, 0.0, 0.0]
[2025-06-13 00:05:12,743]: Mean: -0.00002720
[2025-06-13 00:05:12,744]: Min: -0.16976255
[2025-06-13 00:05:12,744]: Max: 0.14854223
[2025-06-13 00:05:12,744]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-13 00:05:12,744]: Sample Values (25 elements): [0.37128695845603943, 0.35112103819847107, 0.25324365496635437, 0.2324151247739792, 0.23707769811153412, 0.29129666090011597, 0.29856374859809875, 0.2568514347076416, 0.35899817943573, 0.20083971321582794, -5.6304309494934965e-14, 0.2527714669704437, 5.257111318760984e-41, 0.3423210084438324, 0.20739281177520752, 0.32073503732681274, 0.12499388307332993, 0.2905561029911041, 0.23749345541000366, -5.55278529473352e-41, 0.21674737334251404, 0.34079375863075256, 0.24337950348854065, 0.41681548953056335, -6.138948442360591e-41]
[2025-06-13 00:05:12,744]: Mean: 0.15238033
[2025-06-13 00:05:12,744]: Min: -0.02817495
[2025-06-13 00:05:12,745]: Max: 0.48699650
[2025-06-13 00:05:12,746]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 00:05:12,785]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 00:05:12,785]: Mean: 0.00000190
[2025-06-13 00:05:12,785]: Min: -0.05541702
[2025-06-13 00:05:12,785]: Max: 0.04848989
[2025-06-13 00:05:12,785]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-13 00:05:12,786]: Sample Values (25 elements): [4.944902020909414e-41, -5.905211858511212e-41, -6.231013751466732e-41, 5.729068641545582e-41, -5.170791333358575e-41, 5.752750585592672e-41, -5.394298438418383e-41, 6.071265726533702e-41, -5.294525987758456e-41, -5.894832497688185e-07, 5.682265272837133e-41, -6.201166094176613e-41, -5.896804067725263e-41, 5.960703277698474e-41, -5.693615790398164e-41, 4.979373963131805e-41, -5.405929215672279e-41, 6.106438317988255e-41, -5.795209929061713e-41, 5.414056746765363e-41, 5.863453164274332e-41, 5.153835621940245e-41, -5.607155675149323e-41, 5.527421792529241e-41, 5.542415686097516e-41]
[2025-06-13 00:05:12,786]: Mean: 0.00223431
[2025-06-13 00:05:12,786]: Min: -0.00113574
[2025-06-13 00:05:12,786]: Max: 0.41112536
[2025-06-13 00:05:12,788]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 00:05:12,828]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 00:05:12,828]: Mean: 0.00000059
[2025-06-13 00:05:12,828]: Min: -0.08790815
[2025-06-13 00:05:12,828]: Max: 0.10046646
[2025-06-13 00:05:12,828]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-13 00:05:12,830]: Sample Values (25 elements): [2.4124816727644216e-11, 0.13128869235515594, 0.38016393780708313, 0.2997151017189026, -2.0181050786050037e-07, 0.13598063588142395, 5.479777644742197e-41, -5.722062149223958e-41, 6.282721664800317e-41, 0.16225416958332062, 0.14004594087600708, 0.10451456904411316, 0.293588787317276, 0.1873539239168167, -4.960003025189508e-06, 0.25663191080093384, 0.12841971218585968, 0.21655407547950745, 0.021411897614598274, 0.05697464197874069, -4.983998248064077e-41, 0.1918673813343048, 0.15429304540157318, 0.0004558504733722657, 0.18758562207221985]
[2025-06-13 00:05:12,830]: Mean: 0.12368099
[2025-06-13 00:05:12,830]: Min: -0.04249918
[2025-06-13 00:05:12,830]: Max: 0.38816208
[2025-06-13 00:05:12,831]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-13 00:05:12,831]: Sample Values (25 elements): [-6.286084781114697e-41, -0.09980867803096771, -0.20801974833011627, -5.199798211570099e-41, 0.06232316419482231, -4.985539676374834e-41, -0.0009560135658830404, -0.05901097133755684, 0.08659978955984116, 0.1790602058172226, -0.15133318305015564, 6.018156514735792e-41, 0.14643539488315582, 0.029741091653704643, -4.668512687544535e-09, 5.465624530252517e-41, 0.13052979111671448, 0.004147615749388933, 0.0253011304885149, 0.11782272905111313, 0.1095127984881401, -5.804458498926257e-41, -0.018827149644494057, 0.13884331285953522, 0.06474428623914719]
[2025-06-13 00:05:12,831]: Mean: 0.00068740
[2025-06-13 00:05:12,832]: Min: -0.59951121
[2025-06-13 00:05:12,832]: Max: 0.48214883
[2025-06-13 00:05:12,832]: 


QAT of ResNet18 with parametrized_hardtanh down to 3 bits...
[2025-06-13 00:05:13,270]: [ResNet18_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-06-13 00:05:13,334]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-13 00:07:08,239]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.3583 Train Acc: 0.8741 Eval Loss: 0.7467 Eval Acc: 0.7772 (LR: 0.00100000)
[2025-06-13 00:09:08,548]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.3613 Train Acc: 0.8755 Eval Loss: 0.9289 Eval Acc: 0.7470 (LR: 0.00100000)
[2025-06-13 00:11:07,055]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.3603 Train Acc: 0.8746 Eval Loss: 0.8930 Eval Acc: 0.7405 (LR: 0.00100000)
[2025-06-13 00:13:13,203]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.3662 Train Acc: 0.8737 Eval Loss: 0.6328 Eval Acc: 0.8057 (LR: 0.00100000)
[2025-06-13 00:15:18,630]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.3620 Train Acc: 0.8756 Eval Loss: 0.5242 Eval Acc: 0.8353 (LR: 0.00100000)
[2025-06-13 00:17:21,965]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.3552 Train Acc: 0.8757 Eval Loss: 0.7726 Eval Acc: 0.7740 (LR: 0.00100000)
[2025-06-13 00:19:26,092]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.3602 Train Acc: 0.8742 Eval Loss: 0.5280 Eval Acc: 0.8347 (LR: 0.00100000)
[2025-06-13 00:21:31,300]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.3543 Train Acc: 0.8766 Eval Loss: 0.6691 Eval Acc: 0.7961 (LR: 0.00100000)
[2025-06-13 00:23:36,701]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.3514 Train Acc: 0.8785 Eval Loss: 0.6412 Eval Acc: 0.8000 (LR: 0.00100000)
[2025-06-13 00:25:43,738]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.3528 Train Acc: 0.8760 Eval Loss: 0.5645 Eval Acc: 0.8201 (LR: 0.00100000)
[2025-06-13 00:27:48,746]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.3499 Train Acc: 0.8789 Eval Loss: 0.5617 Eval Acc: 0.8206 (LR: 0.00010000)
[2025-06-13 00:29:53,658]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.2467 Train Acc: 0.9159 Eval Loss: 0.3053 Eval Acc: 0.9008 (LR: 0.00010000)
[2025-06-13 00:31:49,788]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.2094 Train Acc: 0.9286 Eval Loss: 0.3225 Eval Acc: 0.8969 (LR: 0.00010000)
[2025-06-13 00:33:54,027]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.1947 Train Acc: 0.9326 Eval Loss: 0.2998 Eval Acc: 0.9055 (LR: 0.00010000)
[2025-06-13 00:35:59,907]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.1870 Train Acc: 0.9345 Eval Loss: 0.3034 Eval Acc: 0.9067 (LR: 0.00010000)
[2025-06-13 00:38:05,766]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.1758 Train Acc: 0.9387 Eval Loss: 0.3144 Eval Acc: 0.9028 (LR: 0.00010000)
[2025-06-13 00:40:09,377]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.1711 Train Acc: 0.9404 Eval Loss: 0.3237 Eval Acc: 0.9044 (LR: 0.00010000)
[2025-06-13 00:42:10,508]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.1670 Train Acc: 0.9414 Eval Loss: 0.3208 Eval Acc: 0.9019 (LR: 0.00010000)
[2025-06-13 00:44:16,158]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.1618 Train Acc: 0.9429 Eval Loss: 0.3104 Eval Acc: 0.9072 (LR: 0.00010000)
[2025-06-13 00:46:23,664]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.1541 Train Acc: 0.9472 Eval Loss: 0.3155 Eval Acc: 0.9031 (LR: 0.00001000)
[2025-06-13 00:48:27,018]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.1406 Train Acc: 0.9514 Eval Loss: 0.2949 Eval Acc: 0.9133 (LR: 0.00001000)
[2025-06-13 00:50:32,687]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.1348 Train Acc: 0.9528 Eval Loss: 0.2937 Eval Acc: 0.9111 (LR: 0.00001000)
[2025-06-13 00:52:33,516]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.1345 Train Acc: 0.9537 Eval Loss: 0.2939 Eval Acc: 0.9138 (LR: 0.00001000)
[2025-06-13 00:54:37,209]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.1301 Train Acc: 0.9551 Eval Loss: 0.2963 Eval Acc: 0.9117 (LR: 0.00001000)
[2025-06-13 00:56:42,530]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.1309 Train Acc: 0.9547 Eval Loss: 0.2937 Eval Acc: 0.9135 (LR: 0.00001000)
[2025-06-13 00:58:49,097]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.1296 Train Acc: 0.9545 Eval Loss: 0.2967 Eval Acc: 0.9141 (LR: 0.00001000)
[2025-06-13 01:01:00,680]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.1279 Train Acc: 0.9558 Eval Loss: 0.2998 Eval Acc: 0.9118 (LR: 0.00001000)
[2025-06-13 01:03:06,445]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.1258 Train Acc: 0.9565 Eval Loss: 0.2915 Eval Acc: 0.9138 (LR: 0.00001000)
[2025-06-13 01:05:11,920]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.1259 Train Acc: 0.9566 Eval Loss: 0.3011 Eval Acc: 0.9127 (LR: 0.00001000)
[2025-06-13 01:07:16,939]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.1273 Train Acc: 0.9553 Eval Loss: 0.2959 Eval Acc: 0.9123 (LR: 0.00001000)
[2025-06-13 01:09:19,081]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.1208 Train Acc: 0.9584 Eval Loss: 0.2964 Eval Acc: 0.9125 (LR: 0.00001000)
[2025-06-13 01:11:28,645]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.1227 Train Acc: 0.9568 Eval Loss: 0.2996 Eval Acc: 0.9129 (LR: 0.00001000)
[2025-06-13 01:13:28,539]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.1222 Train Acc: 0.9578 Eval Loss: 0.2990 Eval Acc: 0.9151 (LR: 0.00001000)
[2025-06-13 01:15:34,208]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.1192 Train Acc: 0.9584 Eval Loss: 0.2971 Eval Acc: 0.9131 (LR: 0.00000100)
[2025-06-13 01:17:39,119]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.1217 Train Acc: 0.9574 Eval Loss: 0.3003 Eval Acc: 0.9139 (LR: 0.00000100)
[2025-06-13 01:19:36,936]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.1228 Train Acc: 0.9573 Eval Loss: 0.2971 Eval Acc: 0.9152 (LR: 0.00000100)
[2025-06-13 01:21:36,409]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.1186 Train Acc: 0.9589 Eval Loss: 0.2980 Eval Acc: 0.9144 (LR: 0.00000100)
[2025-06-13 01:23:35,789]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.1196 Train Acc: 0.9587 Eval Loss: 0.2978 Eval Acc: 0.9147 (LR: 0.00000100)
[2025-06-13 01:25:35,865]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.1190 Train Acc: 0.9589 Eval Loss: 0.2967 Eval Acc: 0.9153 (LR: 0.00000100)
[2025-06-13 01:27:28,428]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.1175 Train Acc: 0.9590 Eval Loss: 0.2990 Eval Acc: 0.9125 (LR: 0.00000010)
[2025-06-13 01:29:27,808]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.1182 Train Acc: 0.9586 Eval Loss: 0.3031 Eval Acc: 0.9128 (LR: 0.00000010)
[2025-06-13 01:31:23,828]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.1213 Train Acc: 0.9575 Eval Loss: 0.3029 Eval Acc: 0.9121 (LR: 0.00000010)
[2025-06-13 01:33:21,922]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.1157 Train Acc: 0.9597 Eval Loss: 0.2973 Eval Acc: 0.9132 (LR: 0.00000010)
[2025-06-13 01:35:11,559]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.1209 Train Acc: 0.9579 Eval Loss: 0.2989 Eval Acc: 0.9127 (LR: 0.00000010)
[2025-06-13 01:37:05,159]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.1197 Train Acc: 0.9588 Eval Loss: 0.2990 Eval Acc: 0.9128 (LR: 0.00000010)
[2025-06-13 01:39:07,891]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.1164 Train Acc: 0.9601 Eval Loss: 0.2959 Eval Acc: 0.9135 (LR: 0.00000010)
[2025-06-13 01:41:09,326]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.1192 Train Acc: 0.9592 Eval Loss: 0.3003 Eval Acc: 0.9147 (LR: 0.00000010)
[2025-06-13 01:43:10,010]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.1154 Train Acc: 0.9602 Eval Loss: 0.2982 Eval Acc: 0.9127 (LR: 0.00000010)
[2025-06-13 01:45:14,064]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.1187 Train Acc: 0.9592 Eval Loss: 0.2999 Eval Acc: 0.9146 (LR: 0.00000010)
[2025-06-13 01:47:15,421]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.1190 Train Acc: 0.9588 Eval Loss: 0.3001 Eval Acc: 0.9139 (LR: 0.00000010)
[2025-06-13 01:49:20,947]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.1188 Train Acc: 0.9579 Eval Loss: 0.2975 Eval Acc: 0.9144 (LR: 0.00000010)
[2025-06-13 01:51:27,155]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.1181 Train Acc: 0.9589 Eval Loss: 0.3011 Eval Acc: 0.9129 (LR: 0.00000010)
[2025-06-13 01:53:33,504]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.1185 Train Acc: 0.9588 Eval Loss: 0.3000 Eval Acc: 0.9129 (LR: 0.00000010)
[2025-06-13 01:55:34,946]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.1204 Train Acc: 0.9585 Eval Loss: 0.2982 Eval Acc: 0.9131 (LR: 0.00000010)
[2025-06-13 01:57:30,060]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.1174 Train Acc: 0.9590 Eval Loss: 0.2997 Eval Acc: 0.9134 (LR: 0.00000010)
[2025-06-13 01:59:36,235]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.1176 Train Acc: 0.9594 Eval Loss: 0.2991 Eval Acc: 0.9113 (LR: 0.00000010)
[2025-06-13 02:01:42,603]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.1189 Train Acc: 0.9587 Eval Loss: 0.2986 Eval Acc: 0.9144 (LR: 0.00000010)
[2025-06-13 02:03:47,516]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.1194 Train Acc: 0.9591 Eval Loss: 0.3000 Eval Acc: 0.9145 (LR: 0.00000010)
[2025-06-13 02:05:48,314]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.1167 Train Acc: 0.9588 Eval Loss: 0.2955 Eval Acc: 0.9131 (LR: 0.00000010)
[2025-06-13 02:07:48,315]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.1201 Train Acc: 0.9585 Eval Loss: 0.2985 Eval Acc: 0.9126 (LR: 0.00000010)
[2025-06-13 02:07:48,315]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.9153
[2025-06-13 02:07:48,512]: 


Quantization of model down to 3 bits finished
[2025-06-13 02:07:48,512]: Model Architecture:
[2025-06-13 02:07:49,253]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2523], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8832100629806519, max_val=0.8832100629806519)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1029], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3761548399925232, max_val=0.3440161645412445)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1288], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1244], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4638935327529907, max_val=0.40702611207962036)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2311], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1116], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42317789793014526, max_val=0.35786205530166626)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0639], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1059], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37204328179359436, max_val=0.36935991048812866)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2208], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1029], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35729262232780457, max_val=0.3631695508956909)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0852], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0745], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2633465528488159, max_val=0.25841373205184937)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1091], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3759223222732544, max_val=0.3879021406173706)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1907], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0778], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2939852476119995, max_val=0.2508164644241333)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0398], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0713], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2637494206428528, max_val=0.23508569598197937)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1465], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0680], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2423175573348999, max_val=0.23385706543922424)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0431], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0717], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24570533633232117, max_val=0.2564302086830139)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0912], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.306728720664978, max_val=0.3315870761871338)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1133], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0667], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24174772202968597, max_val=0.22539356350898743)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0276], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0757], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3013327717781067, max_val=0.22852885723114014)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0784], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1065], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42171454429626465, max_val=0.3239741921424866)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8154], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0618], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22355586290359497, max_val=0.20891407132148743)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0501], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1761932224035263, max_val=0.174278125166893)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4269], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0163], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.061206962913274765, max_val=0.053032469004392624)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2710], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0283], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09535042196512222, max_val=0.10241547226905823)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2369], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-13 02:07:49,253]: 
Model Weights:
[2025-06-13 02:07:49,253]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-13 02:07:49,274]: Sample Values (25 elements): [-0.09774725139141083, -0.12001203745603561, -0.2227221578359604, -0.2932009994983673, -0.20116101205348969, 0.03776932507753372, 0.2537705898284912, -0.034413352608680725, 0.23642727732658386, 0.3362886309623718, 0.27739831805229187, 0.11693556606769562, 0.0015807165764272213, -0.3448769450187683, -0.08650965988636017, 0.21172913908958435, -0.09861637651920319, 0.010015527717769146, 0.25248637795448303, 0.18167568743228912, 0.06019638106226921, -0.05669080466032028, -0.19696560502052307, -0.16072313487529755, -0.19216927886009216]
[2025-06-13 02:07:49,316]: Mean: 0.00099848
[2025-06-13 02:07:49,317]: Min: -0.44436929
[2025-06-13 02:07:49,317]: Max: 0.39916614
[2025-06-13 02:07:49,317]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-13 02:07:49,342]: Sample Values (25 elements): [0.6426588892936707, 0.7674733400344849, 0.9671677947044373, 0.5635975003242493, 0.467284619808197, 0.8380180597305298, 0.755133867263794, 0.5238447189331055, 0.38226109743118286, 0.9111375212669373, 0.6979427337646484, 0.7076515555381775, 0.9240776896476746, 0.4787810742855072, 0.6731608510017395, 0.7523981332778931, 0.6718013286590576, 0.9186714887619019, 0.5750747919082642, 0.65106201171875, 0.4821029305458069, 0.5878327488899231, 0.5512804388999939, 0.6737880110740662, 0.5994720458984375]
[2025-06-13 02:07:49,343]: Mean: 0.64179635
[2025-06-13 02:07:49,343]: Min: 0.28056663
[2025-06-13 02:07:49,343]: Max: 1.01088548
[2025-06-13 02:07:49,344]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 02:07:49,345]: Sample Values (25 elements): [0.0, 0.0, 0.10288157314062119, 0.10288157314062119, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10288157314062119, 0.0, 0.0, 0.10288157314062119, 0.10288157314062119, 0.0, 0.0, -0.10288157314062119, 0.10288157314062119, 0.0, 0.0, 0.10288157314062119, 0.0]
[2025-06-13 02:07:49,345]: Mean: -0.00006698
[2025-06-13 02:07:49,345]: Min: -0.41152629
[2025-06-13 02:07:49,345]: Max: 0.30864471
[2025-06-13 02:07:49,345]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 02:07:49,345]: Sample Values (25 elements): [0.2015056312084198, 0.5121123194694519, 0.26624542474746704, 0.2915990650653839, 0.38474923372268677, 0.37977614998817444, 0.4067397713661194, 0.5128506422042847, 0.4484594166278839, 0.6653361320495605, 0.43970465660095215, 0.6136118769645691, 0.5595934391021729, 0.5142741203308105, 0.4938511848449707, 0.4918709099292755, 0.5372975468635559, 0.6458280682563782, 0.22906315326690674, 0.3986959755420685, 0.33704355359077454, 0.40122896432876587, 0.5979865789413452, 0.5207260847091675, 0.4618265926837921]
[2025-06-13 02:07:49,345]: Mean: 0.50027269
[2025-06-13 02:07:49,346]: Min: 0.17886771
[2025-06-13 02:07:49,346]: Max: 0.77320212
[2025-06-13 02:07:49,347]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 02:07:49,347]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.12441709637641907, 0.0, 0.0, 0.0, 0.0, 0.12441709637641907, -0.12441709637641907, 0.0, 0.12441709637641907, 0.0, 0.0, -0.12441709637641907, 0.0, -0.12441709637641907, -0.12441709637641907, 0.12441709637641907, -0.12441709637641907, -0.12441709637641907, 0.0, 0.0, -0.12441709637641907, 0.0]
[2025-06-13 02:07:49,347]: Mean: 0.00031388
[2025-06-13 02:07:49,348]: Min: -0.49766839
[2025-06-13 02:07:49,348]: Max: 0.37325129
[2025-06-13 02:07:49,348]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 02:07:49,348]: Sample Values (25 elements): [0.7260577082633972, 0.4871213734149933, 0.5703227519989014, 0.7276563048362732, 0.7557975649833679, 0.6743091940879822, 0.7321594953536987, 0.6246213316917419, 0.6361830234527588, 0.4996040165424347, 0.7308958768844604, 0.7304987907409668, 0.5264132618904114, 0.8013560175895691, 0.6901634335517883, 0.7187684774398804, 0.6788850426673889, 0.641624391078949, 0.6214897036552429, 0.6665655374526978, 0.5923022627830505, 0.7402031421661377, 0.7082779407501221, 0.7010809779167175, 0.913243293762207]
[2025-06-13 02:07:49,348]: Mean: 0.66262227
[2025-06-13 02:07:49,348]: Min: 0.33386108
[2025-06-13 02:07:49,349]: Max: 0.91324329
[2025-06-13 02:07:49,350]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 02:07:49,350]: Sample Values (25 elements): [0.0, 0.11157713830471039, 0.0, -0.11157713830471039, -0.11157713830471039, 0.11157713830471039, 0.0, 0.11157713830471039, 0.0, -0.11157713830471039, 0.0, 0.0, 0.11157713830471039, 0.11157713830471039, 0.11157713830471039, -0.11157713830471039, 0.11157713830471039, 0.0, 0.0, 0.0, 0.11157713830471039, 0.0, 0.0, 0.0, -0.11157713830471039]
[2025-06-13 02:07:49,350]: Mean: -0.00044493
[2025-06-13 02:07:49,351]: Min: -0.44630855
[2025-06-13 02:07:49,351]: Max: 0.33473140
[2025-06-13 02:07:49,351]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 02:07:49,351]: Sample Values (25 elements): [0.4402056038379669, 0.41669169068336487, 0.4063117802143097, 0.42829397320747375, 0.49846577644348145, 0.30816757678985596, 0.3690934479236603, 0.43526312708854675, 0.5049876570701599, 0.3716809153556824, 0.38829925656318665, 0.32759547233581543, 0.6027153134346008, 0.5462210178375244, 0.6203929781913757, 0.5109530091285706, 0.31735923886299133, 0.28476211428642273, 0.571090817451477, 0.47327154874801636, 0.4254516661167145, 0.3906078338623047, 0.29894429445266724, 0.5710841417312622, 0.408340722322464]
[2025-06-13 02:07:49,351]: Mean: 0.45424381
[2025-06-13 02:07:49,352]: Min: 0.28476211
[2025-06-13 02:07:49,352]: Max: 0.67124760
[2025-06-13 02:07:49,353]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 02:07:49,354]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.10591474920511246, 0.0, -0.10591474920511246, -0.10591474920511246, 0.0, 0.0, -0.21182949841022491, -0.10591474920511246, 0.0, 0.0, 0.0, 0.0, -0.10591474920511246, 0.0, 0.21182949841022491, 0.0, 0.0, 0.0, 0.0, -0.10591474920511246]
[2025-06-13 02:07:49,354]: Mean: 0.00050280
[2025-06-13 02:07:49,354]: Min: -0.42365900
[2025-06-13 02:07:49,354]: Max: 0.31774426
[2025-06-13 02:07:49,354]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 02:07:49,354]: Sample Values (25 elements): [0.943383514881134, 0.4789579212665558, 0.7475771307945251, 0.5604413747787476, 0.910766065120697, 0.568334698677063, 0.7938281893730164, 0.7610151767730713, 0.5375718474388123, 0.7466579675674438, 0.6040233969688416, 0.47157806158065796, 0.5334175825119019, 0.40333864092826843, 0.37957075238227844, 0.7164219617843628, 0.6574407815933228, 0.8567699193954468, 0.5219627618789673, 0.5383039116859436, 0.6305451393127441, 0.6670198440551758, 0.7317051887512207, 0.5471999049186707, 0.8064361810684204]
[2025-06-13 02:07:49,355]: Mean: 0.66482472
[2025-06-13 02:07:49,355]: Min: 0.31059468
[2025-06-13 02:07:49,355]: Max: 1.08289099
[2025-06-13 02:07:49,356]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-13 02:07:49,357]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10292316973209381, 0.10292316973209381, 0.10292316973209381, 0.0, 0.0, 0.0, 0.0, -0.10292316973209381, -0.10292316973209381, -0.10292316973209381, 0.0, 0.0, -0.10292316973209381, 0.0, -0.10292316973209381, 0.0, 0.0]
[2025-06-13 02:07:49,357]: Mean: 0.00047743
[2025-06-13 02:07:49,357]: Min: -0.30876952
[2025-06-13 02:07:49,357]: Max: 0.41169268
[2025-06-13 02:07:49,357]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-13 02:07:49,358]: Sample Values (25 elements): [0.3310432434082031, 0.3151020407676697, 0.4510621726512909, 0.37408247590065, 0.3729090094566345, 0.28250187635421753, 0.1827486902475357, 0.3249528110027313, 0.34813812375068665, 0.26417237520217896, 0.277798056602478, 0.30252811312675476, 0.3168320953845978, 0.3906986117362976, 0.3038675785064697, 0.47121524810791016, 0.36381304264068604, 0.04610203951597214, 0.2840771973133087, 0.11771030724048615, 0.2421029508113861, 0.30768144130706787, 0.3193961977958679, 0.4140267074108124, 0.2773999571800232]
[2025-06-13 02:07:49,358]: Mean: 0.29800135
[2025-06-13 02:07:49,358]: Min: 0.03124363
[2025-06-13 02:07:49,358]: Max: 0.50146443
[2025-06-13 02:07:49,359]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 02:07:49,361]: Sample Values (25 elements): [0.0, 0.07453718781471252, 0.0, 0.07453718781471252, 0.07453718781471252, -0.07453718781471252, -0.07453718781471252, -0.07453718781471252, 0.07453718781471252, 0.0, 0.07453718781471252, 0.0, 0.07453718781471252, 0.07453718781471252, -0.07453718781471252, 0.0, 0.0, 0.0, 0.0, -0.07453718781471252, 0.0, 0.0, -0.07453718781471252, 0.07453718781471252, -0.07453718781471252]
[2025-06-13 02:07:49,361]: Mean: 0.00001668
[2025-06-13 02:07:49,361]: Min: -0.29814875
[2025-06-13 02:07:49,361]: Max: 0.22361156
[2025-06-13 02:07:49,361]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-13 02:07:49,362]: Sample Values (25 elements): [0.8155672550201416, 0.7227267622947693, 0.6860710382461548, 0.6317233443260193, 0.5830690264701843, 0.6621445417404175, 0.5472790598869324, 0.640730082988739, 0.664345920085907, 0.38671621680259705, 0.6926906704902649, 0.5830574631690979, 0.4999523460865021, 0.625399112701416, 0.565714955329895, 0.4446696639060974, 0.6834133863449097, 0.6002770066261292, 0.6581577062606812, 0.7499707937240601, 0.5546531081199646, 0.5800402760505676, 0.5998056530952454, 0.4146144986152649, 0.5056390762329102]
[2025-06-13 02:07:49,362]: Mean: 0.56000519
[2025-06-13 02:07:49,362]: Min: 0.19609952
[2025-06-13 02:07:49,362]: Max: 0.82284290
[2025-06-13 02:07:49,363]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-13 02:07:49,364]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.1091177836060524, 0.0, 0.1091177836060524, 0.0, -0.2182355672121048, -0.1091177836060524, 0.0, -0.1091177836060524, 0.1091177836060524, 0.1091177836060524, 0.1091177836060524, 0.1091177836060524, 0.0, -0.1091177836060524, 0.0, -0.1091177836060524, 0.0, 0.0, 0.2182355672121048, 0.0]
[2025-06-13 02:07:49,364]: Mean: 0.00182485
[2025-06-13 02:07:49,364]: Min: -0.32735336
[2025-06-13 02:07:49,364]: Max: 0.43647113
[2025-06-13 02:07:49,364]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-13 02:07:49,364]: Sample Values (25 elements): [0.2740539312362671, 0.39817577600479126, 0.31322991847991943, 0.343606173992157, 0.2953111529350281, 0.49635186791419983, 0.4315214157104492, 0.39415642619132996, 0.443362832069397, 0.38695335388183594, 0.29056957364082336, 0.34537413716316223, 0.37588417530059814, 0.24596333503723145, 0.25949394702911377, 0.4597288966178894, 0.4285309910774231, 0.20468038320541382, 0.29161199927330017, 0.22299621999263763, 0.5256425738334656, 0.4183122515678406, 0.24021539092063904, 0.4356938302516937, 0.5288867354393005]
[2025-06-13 02:07:49,364]: Mean: 0.35755590
[2025-06-13 02:07:49,365]: Min: 0.15010507
[2025-06-13 02:07:49,365]: Max: 0.63731402
[2025-06-13 02:07:49,366]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 02:07:49,368]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.07782881706953049, 0.0, 0.07782881706953049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07782881706953049, -0.07782881706953049, -0.07782881706953049, 0.0, 0.07782881706953049, -0.07782881706953049, -0.07782881706953049, 0.07782881706953049, 0.0, 0.0]
[2025-06-13 02:07:49,368]: Mean: 0.00027182
[2025-06-13 02:07:49,368]: Min: -0.31131527
[2025-06-13 02:07:49,368]: Max: 0.23348644
[2025-06-13 02:07:49,368]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-13 02:07:49,369]: Sample Values (25 elements): [0.2758553624153137, 0.14726115763187408, 0.43323957920074463, 0.19480100274085999, 0.20694288611412048, 0.22695951163768768, 0.3961648643016815, 0.30698227882385254, 0.19892562925815582, 0.22879870235919952, 0.30912038683891296, 0.25818896293640137, 0.2984885275363922, 0.29162418842315674, 0.17353734374046326, 0.21288113296031952, 0.22459129989147186, 0.17350983619689941, 0.2704581320285797, 0.17884767055511475, 0.2530861794948578, 0.2759963572025299, 0.17867738008499146, 0.2868975102901459, 0.2935335636138916]
[2025-06-13 02:07:49,369]: Mean: 0.24454430
[2025-06-13 02:07:49,369]: Min: 0.10562528
[2025-06-13 02:07:49,369]: Max: 0.50330693
[2025-06-13 02:07:49,370]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 02:07:49,372]: Sample Values (25 elements): [-0.0712621659040451, 0.0, 0.0712621659040451, 0.0712621659040451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0712621659040451, 0.0712621659040451, 0.0, 0.0, 0.0712621659040451, 0.0, 0.0, 0.0712621659040451, 0.0, 0.0, 0.0, 0.0712621659040451, 0.0]
[2025-06-13 02:07:49,372]: Mean: -0.00008506
[2025-06-13 02:07:49,372]: Min: -0.28504866
[2025-06-13 02:07:49,372]: Max: 0.21378650
[2025-06-13 02:07:49,372]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-13 02:07:49,373]: Sample Values (25 elements): [0.4844214916229248, 0.4408706724643707, 0.3900752663612366, 0.5942872166633606, 0.6926486492156982, 0.5305163264274597, 0.5486604571342468, 0.6351961493492126, 0.4316898286342621, 0.3827853798866272, 0.48718976974487305, 0.31047844886779785, 0.5028626918792725, 0.3486880660057068, 0.4811275005340576, 0.35041362047195435, 0.3685421943664551, 0.5612431764602661, 0.45015615224838257, 0.4270017147064209, 0.41881534457206726, 0.30972161889076233, 0.5334224700927734, 0.5143256187438965, 0.6153156757354736]
[2025-06-13 02:07:49,373]: Mean: 0.45928806
[2025-06-13 02:07:49,373]: Min: 0.06517921
[2025-06-13 02:07:49,373]: Max: 0.69990104
[2025-06-13 02:07:49,374]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-13 02:07:49,377]: Sample Values (25 elements): [0.0, -0.06802494823932648, 0.06802494823932648, 0.06802494823932648, 0.06802494823932648, 0.06802494823932648, -0.13604989647865295, 0.0, 0.0, 0.06802494823932648, -0.06802494823932648, 0.0, 0.0, 0.06802494823932648, 0.0, -0.06802494823932648, 0.0, -0.06802494823932648, -0.06802494823932648, 0.0, -0.06802494823932648, -0.06802494823932648, -0.06802494823932648, 0.06802494823932648, 0.0]
[2025-06-13 02:07:49,377]: Mean: 0.00005028
[2025-06-13 02:07:49,377]: Min: -0.27209979
[2025-06-13 02:07:49,378]: Max: 0.20407484
[2025-06-13 02:07:49,378]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-13 02:07:49,378]: Sample Values (25 elements): [0.20889391005039215, 0.15681569278240204, 0.18078118562698364, 0.1814013570547104, 0.20051716268062592, 0.23555363714694977, 0.15771125257015228, 0.16971944272518158, 0.20440272986888885, 0.2462516576051712, 0.23346631228923798, 0.16851398348808289, 0.14406011998653412, 0.17774598300457, 0.2133949100971222, 0.1092991903424263, 0.1745879054069519, 0.20489469170570374, 0.29071882367134094, 0.2715505063533783, 0.19597235321998596, 0.15518775582313538, 0.20339305698871613, 0.13377662003040314, 0.20708505809307098]
[2025-06-13 02:07:49,379]: Mean: 0.19532272
[2025-06-13 02:07:49,379]: Min: -0.00000000
[2025-06-13 02:07:49,379]: Max: 0.38135248
[2025-06-13 02:07:49,380]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 02:07:49,386]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.14346729218959808, -0.07173364609479904, 0.0, -0.07173364609479904, 0.0, 0.0, 0.0, 0.0, 0.07173364609479904, 0.0, -0.07173364609479904, 0.0, -0.07173364609479904, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 02:07:49,387]: Mean: 0.00006786
[2025-06-13 02:07:49,387]: Min: -0.21520093
[2025-06-13 02:07:49,387]: Max: 0.28693458
[2025-06-13 02:07:49,387]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-13 02:07:49,387]: Sample Values (25 elements): [0.33720827102661133, 0.4924812316894531, 0.5132647752761841, 0.2982552647590637, 0.6713131666183472, 0.44140705466270447, 0.4441432058811188, 0.4077164828777313, 0.307936429977417, 0.38199347257614136, 0.22129006683826447, 0.25843915343284607, 0.3179492652416229, 0.2840718626976013, 0.6466231346130371, 0.22380241751670837, 0.48516973853111267, 0.4424813389778137, 0.4422939419746399, 0.4395076632499695, 0.3146001398563385, 0.35765695571899414, 0.49290600419044495, 0.580019474029541, 0.48854219913482666]
[2025-06-13 02:07:49,388]: Mean: 0.34720877
[2025-06-13 02:07:49,388]: Min: -0.00379811
[2025-06-13 02:07:49,388]: Max: 0.67131317
[2025-06-13 02:07:49,389]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-13 02:07:49,389]: Sample Values (25 elements): [-0.0911879763007164, 0.0911879763007164, -0.0911879763007164, 0.0, 0.0, -0.0911879763007164, -0.0911879763007164, 0.0, 0.0, 0.0, -0.1823759526014328, -0.0911879763007164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0911879763007164, 0.0, -0.0911879763007164, 0.0911879763007164, 0.0, 0.0, -0.0911879763007164, -0.0911879763007164]
[2025-06-13 02:07:49,390]: Mean: -0.00038403
[2025-06-13 02:07:49,390]: Min: -0.27356392
[2025-06-13 02:07:49,390]: Max: 0.36475191
[2025-06-13 02:07:49,390]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-13 02:07:49,390]: Sample Values (25 elements): [0.2488606572151184, 0.2609637677669525, 0.05609465017914772, 0.2909103035926819, 0.08724360913038254, 0.10797486454248428, 0.18728932738304138, 0.31895798444747925, 0.08185862004756927, 0.20183145999908447, 0.24949833750724792, 0.40954166650772095, 0.30004847049713135, 0.2549571394920349, 0.12755094468593597, 0.20380167663097382, 0.008107499219477177, 0.1733749359846115, 0.18616876006126404, 0.187816321849823, 4.977131885588885e-41, 0.17396233975887299, 0.17580671608448029, 0.44823670387268066, 0.19061313569545746]
[2025-06-13 02:07:49,390]: Mean: 0.22764231
[2025-06-13 02:07:49,390]: Min: -0.00873625
[2025-06-13 02:07:49,391]: Max: 0.54090309
[2025-06-13 02:07:49,392]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 02:07:49,398]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.13346894085407257, 0.06673447042703629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06673447042703629, 0.0, 0.0, 0.0, 0.0, -0.06673447042703629, 0.0]
[2025-06-13 02:07:49,398]: Mean: -0.00000668
[2025-06-13 02:07:49,398]: Min: -0.26693788
[2025-06-13 02:07:49,399]: Max: 0.20020342
[2025-06-13 02:07:49,399]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-13 02:07:49,399]: Sample Values (25 elements): [0.19756922125816345, -5.050840184812371e-41, 0.10598322749137878, 0.11607753485441208, 0.18987685441970825, 0.0862671434879303, 0.1469062864780426, 0.13658122718334198, 0.07568929344415665, 0.1402805894613266, 0.10758224129676819, 0.2096564918756485, -5.783579151807817e-41, 0.10082896053791046, 0.09944409877061844, 0.10259019583463669, 0.09501969069242477, 0.20201846957206726, 0.08631788939237595, 0.1074272096157074, 0.15115468204021454, 0.23511292040348053, 0.11760003119707108, 0.10800542682409286, 0.08693361282348633]
[2025-06-13 02:07:49,399]: Mean: 0.12625289
[2025-06-13 02:07:49,399]: Min: -0.00007620
[2025-06-13 02:07:49,400]: Max: 0.36182061
[2025-06-13 02:07:49,401]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 02:07:49,406]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.07569452375173569, 0.0, -0.07569452375173569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07569452375173569, 0.0, 0.0]
[2025-06-13 02:07:49,407]: Mean: -0.00012975
[2025-06-13 02:07:49,407]: Min: -0.30277810
[2025-06-13 02:07:49,407]: Max: 0.22708356
[2025-06-13 02:07:49,407]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-13 02:07:49,407]: Sample Values (25 elements): [0.0876130610704422, -4.908047871297672e-41, 0.1976817101240158, 0.27202296257019043, 0.06853621453046799, 0.36024197936058044, 0.42113885283470154, 0.060429081320762634, 0.31079113483428955, 0.16495278477668762, 0.4526718258857727, 0.12233215570449829, 0.22976849973201752, 0.35420408844947815, 0.3357316255569458, 0.33536773920059204, 0.3563304841518402, 0.43550905585289, -5.51663179435394e-41, 0.522834062576294, 0.15456512570381165, 0.4556596577167511, 0.43772026896476746, 0.27043864130973816, 0.34470054507255554]
[2025-06-13 02:07:49,407]: Mean: 0.24181125
[2025-06-13 02:07:49,408]: Min: -0.00625292
[2025-06-13 02:07:49,408]: Max: 0.54584980
[2025-06-13 02:07:49,409]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-13 02:07:49,421]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 02:07:49,422]: Mean: 0.00003477
[2025-06-13 02:07:49,422]: Min: -0.42610785
[2025-06-13 02:07:49,422]: Max: 0.31958088
[2025-06-13 02:07:49,422]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-13 02:07:49,422]: Sample Values (25 elements): [5.863032774735035e-41, 6.008347425485518e-41, -4.950367084920281e-41, 5.607856324381485e-41, 0.8069363832473755, -5.341329356466905e-41, 5.51607127496821e-41, 5.510466081110911e-41, -5.03948966725134e-41, 5.571002174769743e-41, -5.749667728971157e-41, 1.130685806274414, 6.021239371357306e-41, -5.711132021202224e-41, -5.224601194388648e-41, 4.906086053447617e-41, -6.27837763956091e-41, 5.415878434768985e-41, 4.98245681975332e-41, 6.167955320572115e-41, -5.30321403823727e-41, -5.410973890143849e-41, -5.684647480226485e-41, 0.6502666473388672, -5.047897458037289e-41]
[2025-06-13 02:07:49,423]: Mean: 0.03448233
[2025-06-13 02:07:49,423]: Min: -0.00000000
[2025-06-13 02:07:49,423]: Max: 1.13068581
[2025-06-13 02:07:49,424]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 02:07:49,463]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 02:07:49,464]: Mean: -0.00000597
[2025-06-13 02:07:49,464]: Min: -0.24712569
[2025-06-13 02:07:49,464]: Max: 0.18534426
[2025-06-13 02:07:49,464]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-13 02:07:49,465]: Sample Values (25 elements): [0.14794635772705078, 5.260334305228931e-41, 0.19436703622341156, 0.2141198068857193, 0.19038966298103333, 0.3146299421787262, 0.31772035360336304, 4.969004354495801e-41, 0.10869207233190536, 0.20751507580280304, 0.22013235092163086, 0.10077337920665741, 0.2181582897901535, 0.41772428154945374, 0.4071267247200012, 0.38128888607025146, 0.3190896511077881, 0.39091458916664124, 0.16080905497074127, 0.3951052725315094, 0.36342573165893555, 0.3794443607330322, 0.04492967203259468, 0.37521982192993164, 5.235951711949679e-41]
[2025-06-13 02:07:49,465]: Mean: 0.25698441
[2025-06-13 02:07:49,465]: Min: -0.00000000
[2025-06-13 02:07:49,465]: Max: 0.70859230
[2025-06-13 02:07:49,466]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-13 02:07:49,468]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05006733909249306, 0.0, 0.0, 0.0, -0.10013467818498611, 0.0, 0.0, 0.05006733909249306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 02:07:49,468]: Mean: -0.00004125
[2025-06-13 02:07:49,468]: Min: -0.20026936
[2025-06-13 02:07:49,468]: Max: 0.15020202
[2025-06-13 02:07:49,468]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-13 02:07:49,469]: Sample Values (25 elements): [0.00029742365586571395, 0.19995388388633728, 0.3218766748905182, 0.47048240900039673, -5.520835689746914e-41, -6.132502469424697e-41, 0.3795078694820404, 0.2288762331008911, 5.657602419865016e-41, -5.953276395837553e-41, 0.22705689072608948, 0.22241073846817017, 0.4175369143486023, 0.17181067168712616, 0.3220076858997345, 0.3304471969604492, 0.34964317083358765, 0.2851312756538391, 0.12896369397640228, 0.13988037407398224, -5.775171361021869e-41, 0.2542906701564789, 0.38114961981773376, 0.474428653717041, 0.3448484241962433]
[2025-06-13 02:07:49,469]: Mean: 0.18766071
[2025-06-13 02:07:49,469]: Min: -0.00701679
[2025-06-13 02:07:49,469]: Max: 0.51431727
[2025-06-13 02:07:49,470]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 02:07:49,522]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 02:07:49,522]: Mean: 0.00000234
[2025-06-13 02:07:49,523]: Min: -0.06527967
[2025-06-13 02:07:49,523]: Max: 0.04895975
[2025-06-13 02:07:49,523]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-13 02:07:49,523]: Sample Values (25 elements): [4.989183052382079e-41, 5.336705071534633e-41, 5.05644537866967e-41, -6.267587641385609e-41, -4.995769155164405e-41, -6.077711699469597e-41, 5.550963606729898e-41, 4.982596949599752e-41, -6.114145459542042e-41, 5.021553046907982e-41, -5.895262639414505e-41, -5.781757463804195e-41, -5.318768451191276e-41, -5.538492050397407e-41, -5.528963220839998e-41, 5.427088822483584e-41, -5.712673449512982e-41, -6.227370375459487e-41, 6.276415821710856e-41, 5.069197194695026e-41, -6.197102328630071e-41, 5.696278257480381e-41, -5.573384382159095e-41, 5.234130023946057e-41, 6.168936229497142e-41]
[2025-06-13 02:07:49,524]: Mean: 0.00406914
[2025-06-13 02:07:49,524]: Min: -0.00000000
[2025-06-13 02:07:49,524]: Max: 0.44739524
[2025-06-13 02:07:49,525]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 02:07:49,565]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 02:07:49,566]: Mean: -0.00000025
[2025-06-13 02:07:49,566]: Min: -0.08475681
[2025-06-13 02:07:49,566]: Max: 0.11300908
[2025-06-13 02:07:49,566]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-13 02:07:49,568]: Sample Values (25 elements): [0.03889169916510582, 0.12544585764408112, 0.08129912614822388, 0.14822092652320862, 0.23632559180259705, 0.14338983595371246, 0.25717243552207947, 0.15015389025211334, 5.51887387189686e-41, 0.3145035207271576, 0.2296282798051834, 4.92150033655519e-41, 0.053864963352680206, 0.14217506349086761, 0.13370664417743683, 0.32724547386169434, -4.990724480692836e-41, 0.22414159774780273, 5.008520971189761e-41, 5.4737520613456e-41, 0.19009841978549957, 0.10988208651542664, 0.29615360498428345, 0.188736230134964, 0.261888325214386]
[2025-06-13 02:07:49,569]: Mean: 0.13281184
[2025-06-13 02:07:49,569]: Min: -0.10859893
[2025-06-13 02:07:49,569]: Max: 0.42191774
[2025-06-13 02:07:49,569]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-13 02:07:49,570]: Sample Values (25 elements): [-0.07275659590959549, 0.0337359718978405, 0.030270831659436226, 5.2105882097454e-41, 0.1887822449207306, -0.08732759952545166, -5.629856710271385e-41, 0.12040375918149948, -0.1359422504901886, 6.086119490255546e-41, 0.08941331505775452, 0.06463984400033951, 0.1960161030292511, -0.09489680081605911, 4.909309039915564e-41, 0.04042541980743408, -0.08689583092927933, 0.04622519016265869, 0.17431536316871643, -0.11236989498138428, 5.397801684579195e-41, 0.05511408671736717, 0.1099652349948883, 0.15567561984062195, -0.0022109001874923706]
[2025-06-13 02:07:49,570]: Mean: 0.00076103
[2025-06-13 02:07:49,570]: Min: -0.60982537
[2025-06-13 02:07:49,570]: Max: 0.42742059
[2025-06-13 02:07:49,570]: 


QAT of ResNet18 with parametrized_hardtanh down to 2 bits...
[2025-06-13 02:07:50,119]: [ResNet18_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-06-13 02:07:50,149]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-13 02:09:42,925]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 0.6103 Train Acc: 0.7887 Eval Loss: 0.7676 Eval Acc: 0.7632 (LR: 0.00100000)
[2025-06-13 02:11:36,348]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.5103 Train Acc: 0.8239 Eval Loss: 0.7047 Eval Acc: 0.7765 (LR: 0.00100000)
[2025-06-13 02:13:25,861]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.4896 Train Acc: 0.8310 Eval Loss: 0.6547 Eval Acc: 0.7885 (LR: 0.00100000)
[2025-06-13 02:15:21,439]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.4818 Train Acc: 0.8345 Eval Loss: 1.1913 Eval Acc: 0.6612 (LR: 0.00100000)
[2025-06-13 02:17:16,936]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.4653 Train Acc: 0.8390 Eval Loss: 0.7574 Eval Acc: 0.7710 (LR: 0.00100000)
[2025-06-13 02:19:07,517]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.4652 Train Acc: 0.8394 Eval Loss: 0.7300 Eval Acc: 0.7763 (LR: 0.00100000)
[2025-06-13 02:21:01,092]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.4606 Train Acc: 0.8425 Eval Loss: 0.6322 Eval Acc: 0.8016 (LR: 0.00100000)
[2025-06-13 02:22:53,342]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.4593 Train Acc: 0.8421 Eval Loss: 1.0421 Eval Acc: 0.7217 (LR: 0.00100000)
[2025-06-13 02:24:44,662]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.4519 Train Acc: 0.8444 Eval Loss: 1.3044 Eval Acc: 0.6467 (LR: 0.00100000)
[2025-06-13 02:26:36,770]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.4587 Train Acc: 0.8396 Eval Loss: 0.6769 Eval Acc: 0.7867 (LR: 0.00100000)
[2025-06-13 02:28:28,236]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.4623 Train Acc: 0.8399 Eval Loss: 0.8898 Eval Acc: 0.7335 (LR: 0.00100000)
[2025-06-13 02:30:20,979]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.4507 Train Acc: 0.8435 Eval Loss: 0.6835 Eval Acc: 0.7890 (LR: 0.00100000)
[2025-06-13 02:32:13,929]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.4488 Train Acc: 0.8434 Eval Loss: 0.8056 Eval Acc: 0.7677 (LR: 0.00010000)
[2025-06-13 02:34:07,091]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.3489 Train Acc: 0.8796 Eval Loss: 0.3818 Eval Acc: 0.8753 (LR: 0.00010000)
[2025-06-13 02:35:58,618]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.3209 Train Acc: 0.8891 Eval Loss: 0.3775 Eval Acc: 0.8754 (LR: 0.00010000)
[2025-06-13 02:37:50,120]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.3094 Train Acc: 0.8922 Eval Loss: 0.3692 Eval Acc: 0.8799 (LR: 0.00010000)
[2025-06-13 02:39:41,699]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.3050 Train Acc: 0.8954 Eval Loss: 0.3848 Eval Acc: 0.8753 (LR: 0.00010000)
[2025-06-13 02:41:33,085]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.2976 Train Acc: 0.8972 Eval Loss: 0.4023 Eval Acc: 0.8729 (LR: 0.00010000)
[2025-06-13 02:43:25,996]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.2991 Train Acc: 0.8978 Eval Loss: 0.3798 Eval Acc: 0.8772 (LR: 0.00010000)
[2025-06-13 02:45:24,749]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.2937 Train Acc: 0.8979 Eval Loss: 0.3933 Eval Acc: 0.8746 (LR: 0.00010000)
[2025-06-13 02:47:17,161]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.2935 Train Acc: 0.8969 Eval Loss: 0.4025 Eval Acc: 0.8717 (LR: 0.00010000)
[2025-06-13 02:49:08,921]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.2943 Train Acc: 0.8979 Eval Loss: 0.4029 Eval Acc: 0.8705 (LR: 0.00001000)
[2025-06-13 02:51:00,592]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.2676 Train Acc: 0.9062 Eval Loss: 0.3391 Eval Acc: 0.8935 (LR: 0.00001000)
[2025-06-13 02:52:53,255]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.2630 Train Acc: 0.9091 Eval Loss: 0.3388 Eval Acc: 0.8932 (LR: 0.00001000)
[2025-06-13 02:54:45,415]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.2607 Train Acc: 0.9089 Eval Loss: 0.3412 Eval Acc: 0.8879 (LR: 0.00001000)
[2025-06-13 02:56:38,197]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.2550 Train Acc: 0.9110 Eval Loss: 0.3401 Eval Acc: 0.8906 (LR: 0.00001000)
[2025-06-13 02:58:30,501]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.2597 Train Acc: 0.9108 Eval Loss: 0.3357 Eval Acc: 0.8929 (LR: 0.00001000)
[2025-06-13 03:00:24,345]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.2557 Train Acc: 0.9112 Eval Loss: 0.3355 Eval Acc: 0.8928 (LR: 0.00001000)
[2025-06-13 03:02:16,978]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.2544 Train Acc: 0.9109 Eval Loss: 0.3469 Eval Acc: 0.8896 (LR: 0.00001000)
[2025-06-13 03:04:13,520]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.2567 Train Acc: 0.9109 Eval Loss: 0.3299 Eval Acc: 0.8900 (LR: 0.00001000)
[2025-06-13 03:06:07,334]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.2515 Train Acc: 0.9125 Eval Loss: 0.3368 Eval Acc: 0.8936 (LR: 0.00001000)
[2025-06-13 03:08:01,641]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.2525 Train Acc: 0.9121 Eval Loss: 0.3507 Eval Acc: 0.8881 (LR: 0.00001000)
[2025-06-13 03:09:58,390]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.2531 Train Acc: 0.9115 Eval Loss: 0.3363 Eval Acc: 0.8931 (LR: 0.00001000)
[2025-06-13 03:12:17,372]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.2501 Train Acc: 0.9126 Eval Loss: 0.3487 Eval Acc: 0.8901 (LR: 0.00001000)
[2025-06-13 03:14:32,021]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.2572 Train Acc: 0.9107 Eval Loss: 0.3438 Eval Acc: 0.8910 (LR: 0.00001000)
[2025-06-13 03:16:52,355]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.2476 Train Acc: 0.9138 Eval Loss: 0.3483 Eval Acc: 0.8897 (LR: 0.00000100)
[2025-06-13 03:19:09,382]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.2463 Train Acc: 0.9145 Eval Loss: 0.3381 Eval Acc: 0.8923 (LR: 0.00000100)
[2025-06-13 03:21:29,458]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.2472 Train Acc: 0.9150 Eval Loss: 0.3359 Eval Acc: 0.8951 (LR: 0.00000100)
[2025-06-13 03:23:50,021]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.2496 Train Acc: 0.9139 Eval Loss: 0.3401 Eval Acc: 0.8941 (LR: 0.00000100)
[2025-06-13 03:26:00,932]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.2439 Train Acc: 0.9158 Eval Loss: 0.3350 Eval Acc: 0.8936 (LR: 0.00000100)
[2025-06-13 03:28:10,158]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.2437 Train Acc: 0.9148 Eval Loss: 0.3412 Eval Acc: 0.8929 (LR: 0.00000100)
[2025-06-13 03:30:21,164]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.2443 Train Acc: 0.9152 Eval Loss: 0.3393 Eval Acc: 0.8930 (LR: 0.00000010)
[2025-06-13 03:32:27,187]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.2404 Train Acc: 0.9157 Eval Loss: 0.3380 Eval Acc: 0.8938 (LR: 0.00000010)
[2025-06-13 03:34:35,446]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.2415 Train Acc: 0.9161 Eval Loss: 0.3319 Eval Acc: 0.8960 (LR: 0.00000010)
[2025-06-13 03:36:42,596]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.2421 Train Acc: 0.9160 Eval Loss: 0.3399 Eval Acc: 0.8929 (LR: 0.00000010)
[2025-06-13 03:38:36,103]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.2449 Train Acc: 0.9159 Eval Loss: 0.3354 Eval Acc: 0.8924 (LR: 0.00000010)
[2025-06-13 03:40:28,840]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.2412 Train Acc: 0.9166 Eval Loss: 0.3328 Eval Acc: 0.8963 (LR: 0.00000010)
[2025-06-13 03:43:14,054]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.2395 Train Acc: 0.9171 Eval Loss: 0.3350 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-06-13 03:45:45,176]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.2412 Train Acc: 0.9156 Eval Loss: 0.3455 Eval Acc: 0.8922 (LR: 0.00000010)
[2025-06-13 03:48:10,341]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.2428 Train Acc: 0.9158 Eval Loss: 0.3391 Eval Acc: 0.8957 (LR: 0.00000010)
[2025-06-13 03:50:35,865]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.2412 Train Acc: 0.9163 Eval Loss: 0.3353 Eval Acc: 0.8945 (LR: 0.00000010)
[2025-06-13 03:52:45,754]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.2403 Train Acc: 0.9176 Eval Loss: 0.3332 Eval Acc: 0.8943 (LR: 0.00000010)
[2025-06-13 03:55:00,922]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.2401 Train Acc: 0.9163 Eval Loss: 0.3403 Eval Acc: 0.8938 (LR: 0.00000010)
[2025-06-13 03:57:25,499]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.2427 Train Acc: 0.9160 Eval Loss: 0.3397 Eval Acc: 0.8937 (LR: 0.00000010)
[2025-06-13 03:59:52,952]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.2419 Train Acc: 0.9168 Eval Loss: 0.3336 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 04:02:23,026]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.2430 Train Acc: 0.9160 Eval Loss: 0.3394 Eval Acc: 0.8903 (LR: 0.00000010)
[2025-06-13 04:04:49,071]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.2395 Train Acc: 0.9158 Eval Loss: 0.3377 Eval Acc: 0.8953 (LR: 0.00000010)
[2025-06-13 04:07:15,689]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.2412 Train Acc: 0.9173 Eval Loss: 0.3315 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 04:09:43,126]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.2388 Train Acc: 0.9164 Eval Loss: 0.3410 Eval Acc: 0.8923 (LR: 0.00000010)
[2025-06-13 04:12:28,484]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.2407 Train Acc: 0.9171 Eval Loss: 0.3426 Eval Acc: 0.8935 (LR: 0.00000010)
[2025-06-13 04:12:28,484]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8963
[2025-06-13 04:12:28,780]: 


Quantization of model down to 2 bits finished
[2025-06-13 04:12:28,780]: Model Architecture:
[2025-06-13 04:12:29,999]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6041], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9061571359634399, max_val=0.9061571359634399)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2443], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36839979887008667, max_val=0.3645417094230652)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7519], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2898], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4347383677959442, max_val=0.43473345041275024)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0658], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3071], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46417635679244995, max_val=0.45719006657600403)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3790], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2622], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39337629079818726, max_val=0.39333686232566833)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2941], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2760], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41444042325019836, max_val=0.41361498832702637)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5505], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2029], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2947574257850647, max_val=0.313888281583786)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2848], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42456376552581787, max_val=0.42982345819473267)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7982], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1917], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2870274782180786, max_val=0.2881343364715576)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1931], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1835], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2903957962989807, max_val=0.26019978523254395)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9297], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2014], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2971641421318054, max_val=0.30707794427871704)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2560], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2037], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3024837374687195, max_val=0.3087618947029114)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2469], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3581922948360443, max_val=0.38256150484085083)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5351], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2121], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3282911777496338, max_val=0.30799955129623413)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1598], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2218], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3334496021270752, max_val=0.33183175325393677)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5658], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2426], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36636146903038025, max_val=0.36137229204177856)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.2694], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1470], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22055235505104065, max_val=0.22048646211624146)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1744], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2615358829498291, max_val=0.26152706146240234)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1591], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0634], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09979260712862015, max_val=0.09036941826343536)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9602], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1051], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19042649865150452, max_val=0.1250203251838684)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8720], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-06-13 04:12:30,000]: 
Model Weights:
[2025-06-13 04:12:30,000]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-06-13 04:12:30,053]: Sample Values (25 elements): [-0.14349576830863953, -0.035330358892679214, -0.031579576432704926, 0.2106640785932541, 0.12503764033317566, 0.08928116410970688, 0.03830264136195183, 0.030848905444145203, 0.00617495970800519, 0.03573347628116608, 0.05929645523428917, -0.12933211028575897, -0.04309734329581261, 0.2629523277282715, -0.009855696000158787, -0.03200838714838028, -0.24897615611553192, -0.039044808596372604, 0.05516050010919571, -0.01104515790939331, 0.00620335154235363, 0.23512350022792816, 0.009385800920426846, -0.07105760276317596, 0.019343961030244827]
[2025-06-13 04:12:30,082]: Mean: 0.00092682
[2025-06-13 04:12:30,083]: Min: -0.48594964
[2025-06-13 04:12:30,095]: Max: 0.40249422
[2025-06-13 04:12:30,095]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-06-13 04:12:30,095]: Sample Values (25 elements): [0.8906791806221008, 0.7216190099716187, 0.5851425528526306, 1.0379480123519897, 1.1357184648513794, 0.7362594604492188, 1.2309129238128662, 0.7545575499534607, 0.7211971879005432, 1.033692717552185, 1.0240869522094727, 0.7014675736427307, 0.6315348744392395, 0.8518531322479248, 0.9850108623504639, 1.0174046754837036, 0.8093283176422119, 0.9213621020317078, 0.7302851676940918, 0.8412562608718872, 0.4873785972595215, 0.6968843340873718, 1.0676895380020142, 0.9141291975975037, 0.6302671432495117]
[2025-06-13 04:12:30,096]: Mean: 0.80369079
[2025-06-13 04:12:30,096]: Min: 0.46332324
[2025-06-13 04:12:30,096]: Max: 1.23091292
[2025-06-13 04:12:30,098]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 04:12:30,099]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24431383609771729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,099]: Mean: 0.00012592
[2025-06-13 04:12:30,099]: Min: -0.48862767
[2025-06-13 04:12:30,099]: Max: 0.24431384
[2025-06-13 04:12:30,099]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 04:12:30,100]: Sample Values (25 elements): [0.2623975872993469, 0.5776931047439575, 0.7344390153884888, 0.5165945291519165, 0.5584036111831665, 0.4035792648792267, 0.7896194458007812, 0.7593328356742859, 0.3869873881340027, 0.6002093553543091, 0.8030182123184204, 0.6266171932220459, 0.7653518319129944, 0.6631478667259216, 0.808282196521759, 0.6467610597610474, 0.7205533385276794, 0.7116569876670837, 0.5472189784049988, 0.44134101271629333, 0.5452970862388611, 0.6446608304977417, 0.6736361384391785, 0.6254565119743347, 0.6159476637840271]
[2025-06-13 04:12:30,100]: Mean: 0.60813510
[2025-06-13 04:12:30,100]: Min: 0.19628887
[2025-06-13 04:12:30,101]: Max: 0.97739905
[2025-06-13 04:12:30,102]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 04:12:30,103]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2898239493370056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,103]: Mean: 0.00026731
[2025-06-13 04:12:30,103]: Min: -0.57964790
[2025-06-13 04:12:30,103]: Max: 0.28982395
[2025-06-13 04:12:30,103]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 04:12:30,104]: Sample Values (25 elements): [0.6904091835021973, 0.7200939059257507, 0.6943119764328003, 0.7107730507850647, 0.773171067237854, 0.8308078646659851, 0.7269333600997925, 0.7104840874671936, 0.9057163000106812, 0.929288387298584, 0.7157838940620422, 0.6559001207351685, 0.39627769589424133, 0.5302177667617798, 0.7840401530265808, 0.718681812286377, 0.8407703638076782, 0.7828646898269653, 0.8177255988121033, 0.8506393432617188, 0.9064135551452637, 0.6629617214202881, 0.356815904378891, 0.6739815473556519, 0.5000791549682617]
[2025-06-13 04:12:30,104]: Mean: 0.68934453
[2025-06-13 04:12:30,104]: Min: 0.35681590
[2025-06-13 04:12:30,104]: Max: 0.94002914
[2025-06-13 04:12:30,106]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 04:12:30,107]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.307122141122818, 0.0, 0.0, 0.0, 0.0, -0.307122141122818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,107]: Mean: -0.00006665
[2025-06-13 04:12:30,107]: Min: -0.61424428
[2025-06-13 04:12:30,108]: Max: 0.30712214
[2025-06-13 04:12:30,108]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 04:12:30,109]: Sample Values (25 elements): [0.5159832835197449, 0.5312063097953796, 0.4415053725242615, 0.3677518963813782, 0.3922635614871979, 0.5149348974227905, 0.5287086367607117, 0.4579751491546631, 0.4075544774532318, 0.5610833764076233, 0.6488451361656189, 0.5606762170791626, 0.4396829903125763, 0.247963085770607, 0.5756043791770935, 0.6934356093406677, 0.5207439661026001, 0.49209821224212646, 0.5920067429542542, 0.5219769477844238, 0.472379595041275, 0.4784611463546753, 0.37469369173049927, 0.29619696736335754, 0.35857880115509033]
[2025-06-13 04:12:30,110]: Mean: 0.47997314
[2025-06-13 04:12:30,110]: Min: 0.24796309
[2025-06-13 04:12:30,110]: Max: 0.78576487
[2025-06-13 04:12:30,112]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 04:12:30,113]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2622377276420593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2622377276420593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,113]: Mean: 0.00086787
[2025-06-13 04:12:30,113]: Min: -0.52447546
[2025-06-13 04:12:30,114]: Max: 0.26223773
[2025-06-13 04:12:30,114]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 04:12:30,114]: Sample Values (25 elements): [1.13417649269104, 0.6301987171173096, 0.8394807577133179, 0.6727834343910217, 0.5148119926452637, 0.7827997207641602, 0.9590086936950684, 0.7983030676841736, 0.8101437091827393, 0.7805368900299072, 0.647441565990448, 0.42421191930770874, 1.2521836757659912, 0.6849852204322815, 0.713388979434967, 0.6388455629348755, 0.702941358089447, 1.0186814069747925, 0.8750921487808228, 0.6723997592926025, 0.9822350144386292, 0.5618070363998413, 0.6187222599983215, 0.29144030809402466, 1.0994317531585693]
[2025-06-13 04:12:30,114]: Mean: 0.77789390
[2025-06-13 04:12:30,114]: Min: 0.29144031
[2025-06-13 04:12:30,115]: Max: 1.25218368
[2025-06-13 04:12:30,116]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-06-13 04:12:30,117]: Sample Values (25 elements): [-0.2760184705257416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,117]: Mean: 0.00009734
[2025-06-13 04:12:30,118]: Min: -0.55203694
[2025-06-13 04:12:30,118]: Max: 0.27601847
[2025-06-13 04:12:30,118]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-06-13 04:12:30,118]: Sample Values (25 elements): [0.3845955729484558, 0.4021144211292267, 0.505563497543335, 0.4635564386844635, 0.4235650599002838, 0.4211527109146118, 0.22697779536247253, 0.37023743987083435, 0.5307649970054626, 0.2844924330711365, 0.35874009132385254, 0.445289671421051, 0.4180678427219391, 0.3857828676700592, 0.43433791399002075, 0.4133225381374359, 0.36321285367012024, 0.45928555727005005, 0.34256646037101746, 0.3543086647987366, 0.2641112506389618, 0.4178940951824188, 0.4979311227798462, 0.37489819526672363, 0.36805620789527893]
[2025-06-13 04:12:30,119]: Mean: 0.37594080
[2025-06-13 04:12:30,119]: Min: -0.00000000
[2025-06-13 04:12:30,120]: Max: 0.60639334
[2025-06-13 04:12:30,121]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 04:12:30,124]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.20288191735744476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20288191735744476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,124]: Mean: -0.00007155
[2025-06-13 04:12:30,124]: Min: -0.20288192
[2025-06-13 04:12:30,125]: Max: 0.40576383
[2025-06-13 04:12:30,125]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-06-13 04:12:30,125]: Sample Values (25 elements): [0.49810197949409485, 0.7078995704650879, 0.6559273600578308, 0.7072480320930481, 0.5894374251365662, 0.7043134570121765, 0.6762869358062744, 0.8392204642295837, 0.6145629286766052, 0.6291042566299438, 0.6260333061218262, 0.4401582181453705, 0.47360920906066895, 0.5975345969200134, 0.2415953278541565, 0.4425322115421295, 0.7697290778160095, 0.5479068756103516, 0.5304142236709595, 0.6011741161346436, 0.5939573645591736, 0.6681304574012756, 0.5212886929512024, 0.5178089141845703, 0.726936936378479]
[2025-06-13 04:12:30,126]: Mean: 0.58623970
[2025-06-13 04:12:30,126]: Min: 0.24159533
[2025-06-13 04:12:30,127]: Max: 0.87221134
[2025-06-13 04:12:30,128]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-06-13 04:12:30,129]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.28479576110839844, 0.0, 0.0, 0.0, 0.0, -0.28479576110839844, 0.0, 0.0, 0.0, 0.0, 0.28479576110839844, 0.0, 0.28479576110839844, 0.28479576110839844, 0.0]
[2025-06-13 04:12:30,129]: Mean: 0.00149490
[2025-06-13 04:12:30,129]: Min: -0.28479576
[2025-06-13 04:12:30,129]: Max: 0.56959152
[2025-06-13 04:12:30,130]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-06-13 04:12:30,130]: Sample Values (25 elements): [0.24989770352840424, 0.43906694650650024, 0.39604651927948, 0.2001482993364334, 0.36488187313079834, 0.3393864631652832, 0.2513769567012787, 0.29014283418655396, 0.6534357666969299, 0.1760786920785904, 0.21198932826519012, 0.31077176332473755, 0.3186499774456024, 0.23187780380249023, 0.32612112164497375, 0.19019030034542084, 0.4537345767021179, 0.31516340374946594, 0.20844535529613495, 0.2828560471534729, 0.2720719575881958, 0.3267422318458557, 0.4093230366706848, 0.355406790971756, 0.45839619636535645]
[2025-06-13 04:12:30,130]: Mean: 0.32950950
[2025-06-13 04:12:30,130]: Min: 0.12094988
[2025-06-13 04:12:30,131]: Max: 0.65343577
[2025-06-13 04:12:30,132]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 04:12:30,135]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,135]: Mean: -0.00000390
[2025-06-13 04:12:30,135]: Min: -0.19172060
[2025-06-13 04:12:30,136]: Max: 0.38344121
[2025-06-13 04:12:30,136]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-06-13 04:12:30,136]: Sample Values (25 elements): [0.22575253248214722, 0.29448333382606506, 0.24699118733406067, 0.19969086349010468, 0.2010503113269806, 0.2664784789085388, 0.3331179618835449, 0.2774727940559387, 0.23572133481502533, 0.25318819284439087, 0.20906558632850647, 0.2741156220436096, 0.20345008373260498, 0.18438732624053955, 0.3043437898159027, 0.32479360699653625, 0.36500030755996704, 0.1966804713010788, 0.23852132260799408, 0.2911357581615448, 0.26888570189476013, 0.2684080898761749, 0.24815787374973297, 0.28082001209259033, 0.09959860146045685]
[2025-06-13 04:12:30,137]: Mean: 0.24528548
[2025-06-13 04:12:30,137]: Min: 0.05894408
[2025-06-13 04:12:30,137]: Max: 0.42977008
[2025-06-13 04:12:30,141]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-06-13 04:12:30,143]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.18353186547756195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18353186547756195, 0.0, 0.0, 0.18353186547756195, 0.0, 0.0, 0.0, 0.18353186547756195, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,143]: Mean: 0.00000747
[2025-06-13 04:12:30,144]: Min: -0.36706373
[2025-06-13 04:12:30,144]: Max: 0.18353187
[2025-06-13 04:12:30,144]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-06-13 04:12:30,145]: Sample Values (25 elements): [0.6113081574440002, 0.4914274215698242, 0.7305737137794495, 0.3931010365486145, 0.5559855699539185, 0.6071460843086243, 0.5102830529212952, 0.4050843417644501, 0.49594646692276, 0.45411404967308044, 0.4815244674682617, 0.5385676026344299, 0.38277190923690796, 0.43481749296188354, 0.6727950572967529, 0.6672661304473877, 0.30116280913352966, 0.3719114363193512, 0.6623350977897644, 0.3643989562988281, 0.6694801449775696, 0.3994472920894623, 0.5062658190727234, 0.3593098223209381, 0.5085679292678833]
[2025-06-13 04:12:30,145]: Mean: 0.55468535
[2025-06-13 04:12:30,145]: Min: 0.15667033
[2025-06-13 04:12:30,145]: Max: 0.82590336
[2025-06-13 04:12:30,148]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-06-13 04:12:30,152]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20141403377056122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,153]: Mean: -0.00005805
[2025-06-13 04:12:30,154]: Min: -0.20141403
[2025-06-13 04:12:30,154]: Max: 0.40282807
[2025-06-13 04:12:30,154]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-06-13 04:12:30,155]: Sample Values (25 elements): [0.24318301677703857, 0.28713953495025635, 0.23904117941856384, 0.2566137909889221, 0.2274397611618042, 0.29948392510414124, 0.18269212543964386, 0.17164677381515503, 0.21187293529510498, 0.22918549180030823, 0.2105611264705658, 0.2740793228149414, 0.2181200534105301, 0.28422924876213074, 0.22315692901611328, 0.2187683880329132, 0.1940372884273529, 0.1526976227760315, 0.25035420060157776, 0.2340652197599411, 0.1860109269618988, 0.20374219119548798, 5.38448934916811e-41, 0.23770369589328766, 4.954290720620391e-41]
[2025-06-13 04:12:30,156]: Mean: 0.21949615
[2025-06-13 04:12:30,157]: Min: -0.00000000
[2025-06-13 04:12:30,157]: Max: 0.38133872
[2025-06-13 04:12:30,159]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 04:12:30,169]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,170]: Mean: 0.00004594
[2025-06-13 04:12:30,170]: Min: -0.20374855
[2025-06-13 04:12:30,171]: Max: 0.40749711
[2025-06-13 04:12:30,171]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-06-13 04:12:30,171]: Sample Values (25 elements): [4.910990598072754e-41, 0.40838727355003357, 0.6260823607444763, 0.4384557008743286, 0.4983464479446411, 0.28641968965530396, 0.4622097909450531, 0.28652167320251465, 0.4844311475753784, 0.23583143949508667, 0.48773372173309326, 0.35684823989868164, 0.37414634227752686, 0.3967398405075073, 0.49148663878440857, 0.39809906482696533, 0.426516056060791, 4.919958908244433e-41, 0.3903168737888336, 0.48415541648864746, 0.5008125305175781, 0.368708074092865, 0.4767359495162964, 0.17503535747528076, 0.48042628169059753]
[2025-06-13 04:12:30,171]: Mean: 0.37203953
[2025-06-13 04:12:30,172]: Min: -0.00000000
[2025-06-13 04:12:30,173]: Max: 0.67790836
[2025-06-13 04:12:30,175]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-06-13 04:12:30,176]: Sample Values (25 elements): [0.0, 0.0, -0.2469179332256317, -0.2469179332256317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2469179332256317, 0.0, 0.0, 0.0, 0.0, -0.2469179332256317, 0.0, 0.0, 0.0, 0.0, -0.2469179332256317, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,176]: Mean: -0.00030895
[2025-06-13 04:12:30,176]: Min: -0.24691793
[2025-06-13 04:12:30,177]: Max: 0.49383587
[2025-06-13 04:12:30,177]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-06-13 04:12:30,177]: Sample Values (25 elements): [0.20893317461013794, 0.13333702087402344, 0.25324076414108276, 0.1315567046403885, 0.030811386182904243, 0.2125093787908554, 0.2455662339925766, 0.19745618104934692, 0.19469885528087616, 0.11473716050386429, 0.17207174003124237, 0.1945590078830719, 0.21492385864257812, 0.13948297500610352, -0.00011641232413239777, 0.231028214097023, 0.16230733692646027, 0.1531873196363449, 0.3694564402103424, 0.282558411359787, 0.15180662274360657, 0.09713691473007202, 0.2352614849805832, 0.09880419075489044, 0.22220082581043243]
[2025-06-13 04:12:30,177]: Mean: 0.19791266
[2025-06-13 04:12:30,178]: Min: -0.04479684
[2025-06-13 04:12:30,178]: Max: 0.49223801
[2025-06-13 04:12:30,179]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 04:12:30,188]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.2120969146490097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2120969146490097, 0.0]
[2025-06-13 04:12:30,189]: Mean: 0.00002409
[2025-06-13 04:12:30,189]: Min: -0.42419383
[2025-06-13 04:12:30,190]: Max: 0.21209691
[2025-06-13 04:12:30,190]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-06-13 04:12:30,190]: Sample Values (25 elements): [0.05124007537961006, 0.21386893093585968, 0.06028604134917259, 0.3111656606197357, -4.945042150755847e-41, 0.16819775104522705, 0.1796625852584839, 0.14584466814994812, 0.14107826352119446, 0.1447203904390335, 0.11564454436302185, 0.07126567512750626, 0.1817178875207901, 0.1196286678314209, 0.21981865167617798, 0.23389312624931335, 0.15118955075740814, 0.21528573334217072, -5.969811717716586e-41, 0.17001813650131226, 0.1280062198638916, 0.14482997357845306, 0.09415065497159958, 0.1050843670964241, 0.17457139492034912]
[2025-06-13 04:12:30,191]: Mean: 0.11924738
[2025-06-13 04:12:30,191]: Min: -0.01160534
[2025-06-13 04:12:30,191]: Max: 0.32313284
[2025-06-13 04:12:30,193]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-06-13 04:12:30,202]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,202]: Mean: -0.00010302
[2025-06-13 04:12:30,203]: Min: -0.44352090
[2025-06-13 04:12:30,203]: Max: 0.22176045
[2025-06-13 04:12:30,203]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-06-13 04:12:30,203]: Sample Values (25 elements): [0.530031681060791, 4.943921111984387e-41, 0.4837225377559662, 0.4080531895160675, -4.934532412273411e-41, 0.5993507504463196, 6.164031684872005e-41, 0.26283133029937744, 0.5343931317329407, -5.160141465029706e-41, 0.4191010594367981, -5.568760097226823e-41, 0.38734695315361023, 0.3641965985298157, 0.4673759341239929, -4.912672156229944e-41, 0.554275393486023, 0.21209923923015594, 0.00011027356231352314, 0.2411637306213379, 0.14976242184638977, 0.3779965043067932, -5.665029301725938e-41, 5.0186103201329e-41, 4.928927218416112e-41]
[2025-06-13 04:12:30,204]: Mean: 0.23155540
[2025-06-13 04:12:30,204]: Min: -0.01262587
[2025-06-13 04:12:30,204]: Max: 0.65368980
[2025-06-13 04:12:30,206]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-06-13 04:12:30,226]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,227]: Mean: 0.00001666
[2025-06-13 04:12:30,227]: Min: -0.48515582
[2025-06-13 04:12:30,227]: Max: 0.24257791
[2025-06-13 04:12:30,227]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-06-13 04:12:30,228]: Sample Values (25 elements): [-5.373559221146376e-41, -5.828841092205509e-41, 5.761298506225053e-41, -6.293932052514916e-41, 6.12591636664237e-41, -5.697679555944706e-41, 5.671054885122535e-41, -5.184804318001823e-41, 4.970966172345856e-41, 1.1187139749526978, 5.233709634406759e-41, -5.266359888625528e-41, 6.10783961645258e-41, -5.439980768355372e-41, 6.30149906422227e-41, 5.855746022720546e-41, 5.939123281347872e-41, 5.061490053141239e-41, 5.801655901997608e-41, -5.762419544996513e-41, -4.925564102101732e-41, -5.328437410595117e-41, -5.541294647326057e-41, -5.155797439790299e-41, 5.988308857445673e-41]
[2025-06-13 04:12:30,228]: Mean: 0.06206393
[2025-06-13 04:12:30,228]: Min: -0.00000000
[2025-06-13 04:12:30,228]: Max: 1.50670040
[2025-06-13 04:12:30,231]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 04:12:30,305]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,306]: Mean: 0.00000299
[2025-06-13 04:12:30,306]: Min: -0.29402587
[2025-06-13 04:12:30,307]: Max: 0.14701293
[2025-06-13 04:12:30,307]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-06-13 04:12:30,307]: Sample Values (25 elements): [-4.914213584540701e-41, 0.5467501282691956, 0.5501750707626343, 4.956392668316878e-41, -6.026424175675308e-41, 0.31898629665374756, 0.4825996458530426, 5.356042990342316e-41, 0.4844887852668762, -5.980601715891887e-41, 5.305035726240892e-41, -6.150018700228757e-41, 0.4375773072242737, -5.541154517479624e-41, 0.31788507103919983, -6.08639974994841e-41, 5.524619195600591e-41, 0.496685653924942, 0.5031999349594116, 0.2530345022678375, 0.28249847888946533, 0.39249318838119507, 0.2777557671070099, 0.18963612616062164, 0.2603487968444824]
[2025-06-13 04:12:30,307]: Mean: 0.21537521
[2025-06-13 04:12:30,308]: Min: -0.00000000
[2025-06-13 04:12:30,308]: Max: 0.67463946
[2025-06-13 04:12:30,310]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-06-13 04:12:30,313]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.17435431480407715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.17435431480407715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,314]: Mean: -0.00017426
[2025-06-13 04:12:30,314]: Min: -0.34870863
[2025-06-13 04:12:30,314]: Max: 0.17435431
[2025-06-13 04:12:30,314]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-06-13 04:12:30,315]: Sample Values (25 elements): [0.3893403708934784, 0.5411557555198669, 0.5737110376358032, 0.5245760679244995, 4.909168910069132e-41, -5.938702891808575e-41, 5.28401624927602e-41, 0.24390828609466553, 6.200045055405153e-41, 6.084998451484086e-41, 0.3328982889652252, 0.4279904365539551, 0.3709975779056549, 0.43589815497398376, 0.47197791934013367, 0.33211562037467957, 0.36687737703323364, 0.4534798562526703, 0.26049166917800903, 0.46534964442253113, -5.275748588336504e-41, 0.472273051738739, 0.2586578130722046, 5.536249972854487e-41, 0.34865227341651917]
[2025-06-13 04:12:30,315]: Mean: 0.28640366
[2025-06-13 04:12:30,315]: Min: -0.00000000
[2025-06-13 04:12:30,316]: Max: 0.71675575
[2025-06-13 04:12:30,320]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 04:12:30,394]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,394]: Mean: 0.00000086
[2025-06-13 04:12:30,395]: Min: -0.12677470
[2025-06-13 04:12:30,395]: Max: 0.06338735
[2025-06-13 04:12:30,395]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-06-13 04:12:30,397]: Sample Values (25 elements): [5.427649341869314e-41, 5.344272083241987e-41, -4.950226955073849e-41, 6.107138967220418e-41, 0.3415863811969757, 5.063311741144862e-41, 6.304581920843784e-41, -4.967182666492179e-41, 6.279919067871668e-41, -5.382947920857352e-41, -5.886854848628557e-41, 5.837809402377188e-41, 0.40262746810913086, 5.237493140260436e-41, 6.179445967979578e-41, 5.413215967686768e-41, 0.36937659978866577, 6.141891169135673e-41, 5.682265272837133e-41, -4.995909285010838e-41, -5.174995228751549e-41, 6.137687273742699e-41, -5.635321774282252e-41, 5.778954866875546e-41, 5.198116653412909e-41]
[2025-06-13 04:12:30,398]: Mean: 0.00849744
[2025-06-13 04:12:30,398]: Min: -0.00000000
[2025-06-13 04:12:30,398]: Max: 0.41230845
[2025-06-13 04:12:30,401]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-06-13 04:12:30,471]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 04:12:30,471]: Mean: 0.00000067
[2025-06-13 04:12:30,472]: Min: -0.21029788
[2025-06-13 04:12:30,472]: Max: 0.10514894
[2025-06-13 04:12:30,472]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-06-13 04:12:30,473]: Sample Values (25 elements): [0.33741357922554016, 0.0008677060250192881, 0.20202240347862244, 0.11907477676868439, 0.12366100400686264, 0.2569076716899872, 0.30477744340896606, -5.736915912945801e-41, 0.025311732664704323, 0.09223473817110062, 0.23763778805732727, 0.20668362081050873, 0.263275146484375, 0.060354407876729965, 0.14506201446056366, 1.207195593488919e-10, 0.2505631446838379, 0.1607205718755722, 0.07675225287675858, 0.33684852719306946, 0.23711252212524414, -5.730610069856339e-41, -5.485382838599496e-41, -5.186065486619715e-41, -5.890778484328666e-41]
[2025-06-13 04:12:30,473]: Mean: 0.12933508
[2025-06-13 04:12:30,474]: Min: -0.17003207
[2025-06-13 04:12:30,474]: Max: 0.41702312
[2025-06-13 04:12:30,474]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-06-13 04:12:30,476]: Sample Values (25 elements): [-0.03352240473031998, -0.05335799604654312, 6.208592976037535e-41, -0.004066254012286663, 0.00039222592022269964, 6.006805997174761e-41, 5.555868151355035e-41, 0.02632545679807663, -0.18502523005008698, 0.13688838481903076, 0.20916053652763367, -0.03436359763145447, -0.08552639186382294, -0.017720606178045273, -6.011009892567735e-41, -4.994507986546513e-41, -0.07091876864433289, 0.014328946359455585, 5.147529778850783e-41, 0.1180906891822815, -5.288360274515427e-41, 0.054836444556713104, 0.1209624633193016, -0.03764541447162628, 5.803617719847662e-41]
[2025-06-13 04:12:30,477]: Mean: 0.00073017
[2025-06-13 04:12:30,477]: Min: -0.58289027
[2025-06-13 04:12:30,477]: Max: 0.39811930
