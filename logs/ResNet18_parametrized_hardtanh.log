[2025-05-15 03:19:25,031]: 
Training ResNet18 with parametrized_hardtanh
[2025-05-15 03:21:54,701]: [ResNet18_parametrized_hardtanh] Epoch: 001 Train Loss: 1.8565 Train Acc: 0.3219 Eval Loss: 1.7438 Eval Acc: 0.3689 (LR: 0.001000)
[2025-05-15 03:23:30,190]: [ResNet18_parametrized_hardtanh] Epoch: 002 Train Loss: 1.5974 Train Acc: 0.4159 Eval Loss: 1.4454 Eval Acc: 0.4773 (LR: 0.001000)
[2025-05-15 03:24:59,502]: [ResNet18_parametrized_hardtanh] Epoch: 003 Train Loss: 1.4363 Train Acc: 0.4771 Eval Loss: 1.3816 Eval Acc: 0.4957 (LR: 0.001000)
[2025-05-15 03:26:33,460]: [ResNet18_parametrized_hardtanh] Epoch: 004 Train Loss: 1.3243 Train Acc: 0.5217 Eval Loss: 1.2725 Eval Acc: 0.5371 (LR: 0.001000)
[2025-05-15 03:28:13,765]: [ResNet18_parametrized_hardtanh] Epoch: 005 Train Loss: 1.2327 Train Acc: 0.5579 Eval Loss: 1.1399 Eval Acc: 0.5882 (LR: 0.001000)
[2025-05-15 03:29:55,052]: [ResNet18_parametrized_hardtanh] Epoch: 006 Train Loss: 1.1435 Train Acc: 0.5936 Eval Loss: 1.1004 Eval Acc: 0.6085 (LR: 0.001000)
[2025-05-15 03:31:53,752]: [ResNet18_parametrized_hardtanh] Epoch: 007 Train Loss: 1.0765 Train Acc: 0.6177 Eval Loss: 1.1083 Eval Acc: 0.6082 (LR: 0.001000)
[2025-05-15 03:33:33,802]: [ResNet18_parametrized_hardtanh] Epoch: 008 Train Loss: 1.0234 Train Acc: 0.6371 Eval Loss: 1.0244 Eval Acc: 0.6404 (LR: 0.001000)
[2025-05-15 03:35:13,700]: [ResNet18_parametrized_hardtanh] Epoch: 009 Train Loss: 0.9773 Train Acc: 0.6546 Eval Loss: 0.9423 Eval Acc: 0.6709 (LR: 0.001000)
[2025-05-15 03:36:49,675]: [ResNet18_parametrized_hardtanh] Epoch: 010 Train Loss: 0.9311 Train Acc: 0.6695 Eval Loss: 0.9289 Eval Acc: 0.6778 (LR: 0.001000)
[2025-05-15 03:38:22,657]: [ResNet18_parametrized_hardtanh] Epoch: 011 Train Loss: 0.8905 Train Acc: 0.6839 Eval Loss: 0.9240 Eval Acc: 0.6743 (LR: 0.001000)
[2025-05-15 03:39:53,627]: [ResNet18_parametrized_hardtanh] Epoch: 012 Train Loss: 0.8574 Train Acc: 0.6968 Eval Loss: 0.8712 Eval Acc: 0.6931 (LR: 0.001000)
[2025-05-15 03:41:23,849]: [ResNet18_parametrized_hardtanh] Epoch: 013 Train Loss: 0.8220 Train Acc: 0.7106 Eval Loss: 0.8630 Eval Acc: 0.6996 (LR: 0.001000)
[2025-05-15 03:42:55,735]: [ResNet18_parametrized_hardtanh] Epoch: 014 Train Loss: 0.7871 Train Acc: 0.7232 Eval Loss: 0.8234 Eval Acc: 0.7196 (LR: 0.001000)
[2025-05-15 03:44:38,809]: [ResNet18_parametrized_hardtanh] Epoch: 015 Train Loss: 0.7667 Train Acc: 0.7288 Eval Loss: 0.7868 Eval Acc: 0.7243 (LR: 0.001000)
[2025-05-15 03:46:27,020]: [ResNet18_parametrized_hardtanh] Epoch: 016 Train Loss: 0.7336 Train Acc: 0.7405 Eval Loss: 0.7913 Eval Acc: 0.7342 (LR: 0.001000)
[2025-05-15 03:48:00,409]: [ResNet18_parametrized_hardtanh] Epoch: 017 Train Loss: 0.7092 Train Acc: 0.7500 Eval Loss: 0.7803 Eval Acc: 0.7361 (LR: 0.001000)
[2025-05-15 03:49:32,060]: [ResNet18_parametrized_hardtanh] Epoch: 018 Train Loss: 0.6852 Train Acc: 0.7590 Eval Loss: 0.7596 Eval Acc: 0.7410 (LR: 0.001000)
[2025-05-15 03:51:03,362]: [ResNet18_parametrized_hardtanh] Epoch: 019 Train Loss: 0.6741 Train Acc: 0.7621 Eval Loss: 0.6972 Eval Acc: 0.7576 (LR: 0.001000)
[2025-05-15 03:52:50,253]: [ResNet18_parametrized_hardtanh] Epoch: 020 Train Loss: 0.6455 Train Acc: 0.7738 Eval Loss: 0.6787 Eval Acc: 0.7676 (LR: 0.001000)
[2025-05-15 03:55:11,188]: [ResNet18_parametrized_hardtanh] Epoch: 021 Train Loss: 0.6324 Train Acc: 0.7782 Eval Loss: 0.7199 Eval Acc: 0.7563 (LR: 0.001000)
[2025-05-15 03:57:22,656]: [ResNet18_parametrized_hardtanh] Epoch: 022 Train Loss: 0.6116 Train Acc: 0.7843 Eval Loss: 0.6937 Eval Acc: 0.7697 (LR: 0.001000)
[2025-05-15 03:58:56,925]: [ResNet18_parametrized_hardtanh] Epoch: 023 Train Loss: 0.5922 Train Acc: 0.7924 Eval Loss: 0.6346 Eval Acc: 0.7842 (LR: 0.001000)
[2025-05-15 04:00:30,113]: [ResNet18_parametrized_hardtanh] Epoch: 024 Train Loss: 0.5775 Train Acc: 0.7978 Eval Loss: 0.6334 Eval Acc: 0.7831 (LR: 0.001000)
[2025-05-15 04:02:01,865]: [ResNet18_parametrized_hardtanh] Epoch: 025 Train Loss: 0.5668 Train Acc: 0.8001 Eval Loss: 0.6594 Eval Acc: 0.7790 (LR: 0.001000)
[2025-05-15 04:03:31,501]: [ResNet18_parametrized_hardtanh] Epoch: 026 Train Loss: 0.5429 Train Acc: 0.8090 Eval Loss: 0.6392 Eval Acc: 0.7847 (LR: 0.001000)
[2025-05-15 04:05:01,095]: [ResNet18_parametrized_hardtanh] Epoch: 027 Train Loss: 0.5313 Train Acc: 0.8129 Eval Loss: 0.6492 Eval Acc: 0.7805 (LR: 0.001000)
[2025-05-15 04:06:28,808]: [ResNet18_parametrized_hardtanh] Epoch: 028 Train Loss: 0.5166 Train Acc: 0.8198 Eval Loss: 0.6222 Eval Acc: 0.7951 (LR: 0.001000)
[2025-05-15 04:07:57,007]: [ResNet18_parametrized_hardtanh] Epoch: 029 Train Loss: 0.5070 Train Acc: 0.8200 Eval Loss: 0.6316 Eval Acc: 0.7918 (LR: 0.001000)
[2025-05-15 04:09:35,865]: [ResNet18_parametrized_hardtanh] Epoch: 030 Train Loss: 0.4941 Train Acc: 0.8265 Eval Loss: 0.5919 Eval Acc: 0.8046 (LR: 0.001000)
[2025-05-15 04:11:06,894]: [ResNet18_parametrized_hardtanh] Epoch: 031 Train Loss: 0.4789 Train Acc: 0.8319 Eval Loss: 0.5875 Eval Acc: 0.8064 (LR: 0.001000)
[2025-05-15 04:12:39,397]: [ResNet18_parametrized_hardtanh] Epoch: 032 Train Loss: 0.4687 Train Acc: 0.8352 Eval Loss: 0.5961 Eval Acc: 0.8034 (LR: 0.001000)
[2025-05-15 04:14:08,166]: [ResNet18_parametrized_hardtanh] Epoch: 033 Train Loss: 0.4613 Train Acc: 0.8373 Eval Loss: 0.6119 Eval Acc: 0.7996 (LR: 0.001000)
[2025-05-15 04:15:36,900]: [ResNet18_parametrized_hardtanh] Epoch: 034 Train Loss: 0.4546 Train Acc: 0.8406 Eval Loss: 0.5940 Eval Acc: 0.8058 (LR: 0.001000)
[2025-05-15 04:17:05,516]: [ResNet18_parametrized_hardtanh] Epoch: 035 Train Loss: 0.4369 Train Acc: 0.8489 Eval Loss: 0.6125 Eval Acc: 0.8001 (LR: 0.001000)
[2025-05-15 04:18:34,284]: [ResNet18_parametrized_hardtanh] Epoch: 036 Train Loss: 0.4274 Train Acc: 0.8500 Eval Loss: 0.5417 Eval Acc: 0.8204 (LR: 0.001000)
[2025-05-15 04:20:03,884]: [ResNet18_parametrized_hardtanh] Epoch: 037 Train Loss: 0.4253 Train Acc: 0.8485 Eval Loss: 0.5708 Eval Acc: 0.8144 (LR: 0.001000)
[2025-05-15 04:21:33,262]: [ResNet18_parametrized_hardtanh] Epoch: 038 Train Loss: 0.4150 Train Acc: 0.8542 Eval Loss: 0.5641 Eval Acc: 0.8173 (LR: 0.001000)
[2025-05-15 04:23:03,045]: [ResNet18_parametrized_hardtanh] Epoch: 039 Train Loss: 0.4084 Train Acc: 0.8582 Eval Loss: 0.5664 Eval Acc: 0.8160 (LR: 0.001000)
[2025-05-15 04:24:33,584]: [ResNet18_parametrized_hardtanh] Epoch: 040 Train Loss: 0.4002 Train Acc: 0.8578 Eval Loss: 0.5888 Eval Acc: 0.8149 (LR: 0.001000)
[2025-05-15 04:26:03,169]: [ResNet18_parametrized_hardtanh] Epoch: 041 Train Loss: 0.3839 Train Acc: 0.8664 Eval Loss: 0.5731 Eval Acc: 0.8199 (LR: 0.001000)
[2025-05-15 04:27:32,786]: [ResNet18_parametrized_hardtanh] Epoch: 042 Train Loss: 0.3840 Train Acc: 0.8653 Eval Loss: 0.5573 Eval Acc: 0.8197 (LR: 0.001000)
[2025-05-15 04:29:02,616]: [ResNet18_parametrized_hardtanh] Epoch: 043 Train Loss: 0.3710 Train Acc: 0.8693 Eval Loss: 0.5575 Eval Acc: 0.8193 (LR: 0.001000)
[2025-05-15 04:30:30,542]: [ResNet18_parametrized_hardtanh] Epoch: 044 Train Loss: 0.3613 Train Acc: 0.8719 Eval Loss: 0.5547 Eval Acc: 0.8283 (LR: 0.001000)
[2025-05-15 04:31:58,541]: [ResNet18_parametrized_hardtanh] Epoch: 045 Train Loss: 0.3560 Train Acc: 0.8745 Eval Loss: 0.5217 Eval Acc: 0.8331 (LR: 0.001000)
[2025-05-15 04:33:26,471]: [ResNet18_parametrized_hardtanh] Epoch: 046 Train Loss: 0.3454 Train Acc: 0.8779 Eval Loss: 0.5218 Eval Acc: 0.8340 (LR: 0.001000)
[2025-05-15 04:34:54,319]: [ResNet18_parametrized_hardtanh] Epoch: 047 Train Loss: 0.3428 Train Acc: 0.8790 Eval Loss: 0.5393 Eval Acc: 0.8277 (LR: 0.001000)
[2025-05-15 04:36:22,059]: [ResNet18_parametrized_hardtanh] Epoch: 048 Train Loss: 0.3345 Train Acc: 0.8817 Eval Loss: 0.5403 Eval Acc: 0.8302 (LR: 0.001000)
[2025-05-15 04:37:49,915]: [ResNet18_parametrized_hardtanh] Epoch: 049 Train Loss: 0.3311 Train Acc: 0.8831 Eval Loss: 0.5301 Eval Acc: 0.8355 (LR: 0.001000)
[2025-05-15 04:39:17,879]: [ResNet18_parametrized_hardtanh] Epoch: 050 Train Loss: 0.3204 Train Acc: 0.8891 Eval Loss: 0.5273 Eval Acc: 0.8400 (LR: 0.001000)
[2025-05-15 04:40:48,662]: [ResNet18_parametrized_hardtanh] Epoch: 051 Train Loss: 0.3171 Train Acc: 0.8883 Eval Loss: 0.5560 Eval Acc: 0.8284 (LR: 0.001000)
[2025-05-15 04:42:17,688]: [ResNet18_parametrized_hardtanh] Epoch: 052 Train Loss: 0.3192 Train Acc: 0.8861 Eval Loss: 0.5182 Eval Acc: 0.8359 (LR: 0.001000)
[2025-05-15 04:43:47,899]: [ResNet18_parametrized_hardtanh] Epoch: 053 Train Loss: 0.3103 Train Acc: 0.8907 Eval Loss: 0.5176 Eval Acc: 0.8408 (LR: 0.001000)
[2025-05-15 04:45:16,849]: [ResNet18_parametrized_hardtanh] Epoch: 054 Train Loss: 0.2996 Train Acc: 0.8921 Eval Loss: 0.5309 Eval Acc: 0.8372 (LR: 0.001000)
[2025-05-15 04:46:48,061]: [ResNet18_parametrized_hardtanh] Epoch: 055 Train Loss: 0.2932 Train Acc: 0.8943 Eval Loss: 0.5232 Eval Acc: 0.8389 (LR: 0.001000)
[2025-05-15 04:48:20,086]: [ResNet18_parametrized_hardtanh] Epoch: 056 Train Loss: 0.2875 Train Acc: 0.8979 Eval Loss: 0.5615 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-15 04:49:49,170]: [ResNet18_parametrized_hardtanh] Epoch: 057 Train Loss: 0.2853 Train Acc: 0.8992 Eval Loss: 0.5591 Eval Acc: 0.8331 (LR: 0.001000)
[2025-05-15 04:51:20,139]: [ResNet18_parametrized_hardtanh] Epoch: 058 Train Loss: 0.2840 Train Acc: 0.9002 Eval Loss: 0.5226 Eval Acc: 0.8404 (LR: 0.001000)
[2025-05-15 04:52:58,775]: [ResNet18_parametrized_hardtanh] Epoch: 059 Train Loss: 0.2735 Train Acc: 0.9036 Eval Loss: 0.5363 Eval Acc: 0.8383 (LR: 0.001000)
[2025-05-15 04:54:29,281]: [ResNet18_parametrized_hardtanh] Epoch: 060 Train Loss: 0.2639 Train Acc: 0.9076 Eval Loss: 0.5231 Eval Acc: 0.8436 (LR: 0.001000)
[2025-05-15 04:55:58,365]: [ResNet18_parametrized_hardtanh] Epoch: 061 Train Loss: 0.2625 Train Acc: 0.9063 Eval Loss: 0.4837 Eval Acc: 0.8513 (LR: 0.001000)
[2025-05-15 04:57:33,262]: [ResNet18_parametrized_hardtanh] Epoch: 062 Train Loss: 0.2600 Train Acc: 0.9064 Eval Loss: 0.5196 Eval Acc: 0.8469 (LR: 0.001000)
[2025-05-15 04:59:13,202]: [ResNet18_parametrized_hardtanh] Epoch: 063 Train Loss: 0.2508 Train Acc: 0.9108 Eval Loss: 0.5398 Eval Acc: 0.8421 (LR: 0.001000)
[2025-05-15 05:01:04,027]: [ResNet18_parametrized_hardtanh] Epoch: 064 Train Loss: 0.2476 Train Acc: 0.9119 Eval Loss: 0.5344 Eval Acc: 0.8476 (LR: 0.001000)
[2025-05-15 05:02:42,511]: [ResNet18_parametrized_hardtanh] Epoch: 065 Train Loss: 0.2482 Train Acc: 0.9109 Eval Loss: 0.5812 Eval Acc: 0.8314 (LR: 0.001000)
[2025-05-15 05:04:23,227]: [ResNet18_parametrized_hardtanh] Epoch: 066 Train Loss: 0.2420 Train Acc: 0.9142 Eval Loss: 0.5252 Eval Acc: 0.8412 (LR: 0.001000)
[2025-05-15 05:06:10,622]: [ResNet18_parametrized_hardtanh] Epoch: 067 Train Loss: 0.2345 Train Acc: 0.9164 Eval Loss: 0.4968 Eval Acc: 0.8512 (LR: 0.001000)
[2025-05-15 05:07:44,648]: [ResNet18_parametrized_hardtanh] Epoch: 068 Train Loss: 0.2348 Train Acc: 0.9149 Eval Loss: 0.5555 Eval Acc: 0.8393 (LR: 0.001000)
[2025-05-15 05:09:26,214]: [ResNet18_parametrized_hardtanh] Epoch: 069 Train Loss: 0.2326 Train Acc: 0.9175 Eval Loss: 0.5126 Eval Acc: 0.8538 (LR: 0.001000)
[2025-05-15 05:11:11,260]: [ResNet18_parametrized_hardtanh] Epoch: 070 Train Loss: 0.2295 Train Acc: 0.9183 Eval Loss: 0.5194 Eval Acc: 0.8479 (LR: 0.000100)
[2025-05-15 05:12:56,312]: [ResNet18_parametrized_hardtanh] Epoch: 071 Train Loss: 0.1649 Train Acc: 0.9427 Eval Loss: 0.4388 Eval Acc: 0.8696 (LR: 0.000100)
[2025-05-15 05:14:40,677]: [ResNet18_parametrized_hardtanh] Epoch: 072 Train Loss: 0.1469 Train Acc: 0.9500 Eval Loss: 0.4366 Eval Acc: 0.8712 (LR: 0.000100)
[2025-05-15 05:16:18,702]: [ResNet18_parametrized_hardtanh] Epoch: 073 Train Loss: 0.1427 Train Acc: 0.9522 Eval Loss: 0.4406 Eval Acc: 0.8704 (LR: 0.000100)
[2025-05-15 05:17:56,249]: [ResNet18_parametrized_hardtanh] Epoch: 074 Train Loss: 0.1380 Train Acc: 0.9532 Eval Loss: 0.4460 Eval Acc: 0.8713 (LR: 0.000100)
[2025-05-15 05:19:34,362]: [ResNet18_parametrized_hardtanh] Epoch: 075 Train Loss: 0.1370 Train Acc: 0.9540 Eval Loss: 0.4467 Eval Acc: 0.8696 (LR: 0.000100)
[2025-05-15 05:21:10,124]: [ResNet18_parametrized_hardtanh] Epoch: 076 Train Loss: 0.1315 Train Acc: 0.9554 Eval Loss: 0.4508 Eval Acc: 0.8714 (LR: 0.000100)
[2025-05-15 05:22:40,158]: [ResNet18_parametrized_hardtanh] Epoch: 077 Train Loss: 0.1309 Train Acc: 0.9554 Eval Loss: 0.4472 Eval Acc: 0.8712 (LR: 0.000100)
[2025-05-15 05:24:11,347]: [ResNet18_parametrized_hardtanh] Epoch: 078 Train Loss: 0.1288 Train Acc: 0.9568 Eval Loss: 0.4512 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-15 05:25:44,509]: [ResNet18_parametrized_hardtanh] Epoch: 079 Train Loss: 0.1264 Train Acc: 0.9570 Eval Loss: 0.4532 Eval Acc: 0.8710 (LR: 0.000100)
[2025-05-15 05:27:17,737]: [ResNet18_parametrized_hardtanh] Epoch: 080 Train Loss: 0.1249 Train Acc: 0.9566 Eval Loss: 0.4548 Eval Acc: 0.8714 (LR: 0.000100)
[2025-05-15 05:28:46,242]: [ResNet18_parametrized_hardtanh] Epoch: 081 Train Loss: 0.1225 Train Acc: 0.9583 Eval Loss: 0.4586 Eval Acc: 0.8709 (LR: 0.000100)
[2025-05-15 05:30:18,514]: [ResNet18_parametrized_hardtanh] Epoch: 082 Train Loss: 0.1201 Train Acc: 0.9599 Eval Loss: 0.4570 Eval Acc: 0.8696 (LR: 0.000100)
[2025-05-15 05:32:06,857]: [ResNet18_parametrized_hardtanh] Epoch: 083 Train Loss: 0.1215 Train Acc: 0.9576 Eval Loss: 0.4641 Eval Acc: 0.8700 (LR: 0.000100)
[2025-05-15 05:34:36,350]: [ResNet18_parametrized_hardtanh] Epoch: 084 Train Loss: 0.1199 Train Acc: 0.9585 Eval Loss: 0.4603 Eval Acc: 0.8707 (LR: 0.000100)
[2025-05-15 05:36:33,248]: [ResNet18_parametrized_hardtanh] Epoch: 085 Train Loss: 0.1179 Train Acc: 0.9586 Eval Loss: 0.4589 Eval Acc: 0.8723 (LR: 0.000100)
[2025-05-15 05:38:22,456]: [ResNet18_parametrized_hardtanh] Epoch: 086 Train Loss: 0.1174 Train Acc: 0.9593 Eval Loss: 0.4613 Eval Acc: 0.8712 (LR: 0.000100)
[2025-05-15 05:39:59,239]: [ResNet18_parametrized_hardtanh] Epoch: 087 Train Loss: 0.1137 Train Acc: 0.9608 Eval Loss: 0.4604 Eval Acc: 0.8714 (LR: 0.000100)
[2025-05-15 05:41:47,018]: [ResNet18_parametrized_hardtanh] Epoch: 088 Train Loss: 0.1167 Train Acc: 0.9593 Eval Loss: 0.4646 Eval Acc: 0.8723 (LR: 0.000100)
[2025-05-15 05:43:37,321]: [ResNet18_parametrized_hardtanh] Epoch: 089 Train Loss: 0.1134 Train Acc: 0.9615 Eval Loss: 0.4699 Eval Acc: 0.8708 (LR: 0.000100)
[2025-05-15 05:45:25,516]: [ResNet18_parametrized_hardtanh] Epoch: 090 Train Loss: 0.1125 Train Acc: 0.9613 Eval Loss: 0.4662 Eval Acc: 0.8720 (LR: 0.000100)
[2025-05-15 05:47:13,715]: [ResNet18_parametrized_hardtanh] Epoch: 091 Train Loss: 0.1123 Train Acc: 0.9616 Eval Loss: 0.4755 Eval Acc: 0.8712 (LR: 0.000100)
[2025-05-15 05:49:04,140]: [ResNet18_parametrized_hardtanh] Epoch: 092 Train Loss: 0.1111 Train Acc: 0.9617 Eval Loss: 0.4719 Eval Acc: 0.8706 (LR: 0.000100)
[2025-05-15 05:50:46,629]: [ResNet18_parametrized_hardtanh] Epoch: 093 Train Loss: 0.1106 Train Acc: 0.9621 Eval Loss: 0.4705 Eval Acc: 0.8739 (LR: 0.000100)
[2025-05-15 05:52:18,264]: [ResNet18_parametrized_hardtanh] Epoch: 094 Train Loss: 0.1087 Train Acc: 0.9627 Eval Loss: 0.4710 Eval Acc: 0.8718 (LR: 0.000100)
[2025-05-15 05:53:50,427]: [ResNet18_parametrized_hardtanh] Epoch: 095 Train Loss: 0.1071 Train Acc: 0.9635 Eval Loss: 0.4751 Eval Acc: 0.8712 (LR: 0.000100)
[2025-05-15 05:55:28,800]: [ResNet18_parametrized_hardtanh] Epoch: 096 Train Loss: 0.1068 Train Acc: 0.9627 Eval Loss: 0.4734 Eval Acc: 0.8727 (LR: 0.000100)
[2025-05-15 05:57:00,779]: [ResNet18_parametrized_hardtanh] Epoch: 097 Train Loss: 0.1045 Train Acc: 0.9643 Eval Loss: 0.4766 Eval Acc: 0.8713 (LR: 0.000100)
[2025-05-15 05:58:29,879]: [ResNet18_parametrized_hardtanh] Epoch: 098 Train Loss: 0.1038 Train Acc: 0.9640 Eval Loss: 0.4753 Eval Acc: 0.8716 (LR: 0.000100)
[2025-05-15 05:59:59,989]: [ResNet18_parametrized_hardtanh] Epoch: 099 Train Loss: 0.1034 Train Acc: 0.9647 Eval Loss: 0.4817 Eval Acc: 0.8705 (LR: 0.000100)
[2025-05-15 06:01:31,788]: [ResNet18_parametrized_hardtanh] Epoch: 100 Train Loss: 0.1027 Train Acc: 0.9645 Eval Loss: 0.4842 Eval Acc: 0.8712 (LR: 0.000010)
[2025-05-15 06:03:02,195]: [ResNet18_parametrized_hardtanh] Epoch: 101 Train Loss: 0.0984 Train Acc: 0.9663 Eval Loss: 0.4755 Eval Acc: 0.8731 (LR: 0.000010)
[2025-05-15 06:04:51,300]: [ResNet18_parametrized_hardtanh] Epoch: 102 Train Loss: 0.0984 Train Acc: 0.9658 Eval Loss: 0.4777 Eval Acc: 0.8732 (LR: 0.000010)
[2025-05-15 06:06:32,785]: [ResNet18_parametrized_hardtanh] Epoch: 103 Train Loss: 0.0990 Train Acc: 0.9668 Eval Loss: 0.4776 Eval Acc: 0.8729 (LR: 0.000010)
[2025-05-15 06:08:06,218]: [ResNet18_parametrized_hardtanh] Epoch: 104 Train Loss: 0.0978 Train Acc: 0.9663 Eval Loss: 0.4766 Eval Acc: 0.8715 (LR: 0.000010)
[2025-05-15 06:09:45,133]: [ResNet18_parametrized_hardtanh] Epoch: 105 Train Loss: 0.0945 Train Acc: 0.9673 Eval Loss: 0.4792 Eval Acc: 0.8728 (LR: 0.000010)
[2025-05-15 06:11:19,387]: [ResNet18_parametrized_hardtanh] Epoch: 106 Train Loss: 0.0951 Train Acc: 0.9681 Eval Loss: 0.4751 Eval Acc: 0.8740 (LR: 0.000010)
[2025-05-15 06:12:52,394]: [ResNet18_parametrized_hardtanh] Epoch: 107 Train Loss: 0.0966 Train Acc: 0.9677 Eval Loss: 0.4755 Eval Acc: 0.8726 (LR: 0.000010)
[2025-05-15 06:14:25,828]: [ResNet18_parametrized_hardtanh] Epoch: 108 Train Loss: 0.0966 Train Acc: 0.9671 Eval Loss: 0.4757 Eval Acc: 0.8738 (LR: 0.000010)
[2025-05-15 06:15:58,964]: [ResNet18_parametrized_hardtanh] Epoch: 109 Train Loss: 0.0959 Train Acc: 0.9677 Eval Loss: 0.4773 Eval Acc: 0.8732 (LR: 0.000010)
[2025-05-15 06:17:32,688]: [ResNet18_parametrized_hardtanh] Epoch: 110 Train Loss: 0.0949 Train Acc: 0.9672 Eval Loss: 0.4774 Eval Acc: 0.8746 (LR: 0.000010)
[2025-05-15 06:19:12,585]: [ResNet18_parametrized_hardtanh] Epoch: 111 Train Loss: 0.0944 Train Acc: 0.9682 Eval Loss: 0.4750 Eval Acc: 0.8724 (LR: 0.000010)
[2025-05-15 06:20:46,787]: [ResNet18_parametrized_hardtanh] Epoch: 112 Train Loss: 0.0956 Train Acc: 0.9672 Eval Loss: 0.4760 Eval Acc: 0.8746 (LR: 0.000010)
[2025-05-15 06:22:15,512]: [ResNet18_parametrized_hardtanh] Epoch: 113 Train Loss: 0.0934 Train Acc: 0.9680 Eval Loss: 0.4775 Eval Acc: 0.8714 (LR: 0.000010)
[2025-05-15 06:23:43,147]: [ResNet18_parametrized_hardtanh] Epoch: 114 Train Loss: 0.0932 Train Acc: 0.9681 Eval Loss: 0.4774 Eval Acc: 0.8735 (LR: 0.000010)
[2025-05-15 06:25:10,791]: [ResNet18_parametrized_hardtanh] Epoch: 115 Train Loss: 0.0926 Train Acc: 0.9686 Eval Loss: 0.4780 Eval Acc: 0.8731 (LR: 0.000010)
[2025-05-15 06:26:38,599]: [ResNet18_parametrized_hardtanh] Epoch: 116 Train Loss: 0.0937 Train Acc: 0.9684 Eval Loss: 0.4776 Eval Acc: 0.8730 (LR: 0.000010)
[2025-05-15 06:28:10,666]: [ResNet18_parametrized_hardtanh] Epoch: 117 Train Loss: 0.0962 Train Acc: 0.9680 Eval Loss: 0.4794 Eval Acc: 0.8724 (LR: 0.000010)
[2025-05-15 06:29:44,076]: [ResNet18_parametrized_hardtanh] Epoch: 118 Train Loss: 0.0936 Train Acc: 0.9681 Eval Loss: 0.4768 Eval Acc: 0.8736 (LR: 0.000010)
[2025-05-15 06:31:17,336]: [ResNet18_parametrized_hardtanh] Epoch: 119 Train Loss: 0.0960 Train Acc: 0.9677 Eval Loss: 0.4771 Eval Acc: 0.8719 (LR: 0.000010)
[2025-05-15 06:32:51,413]: [ResNet18_parametrized_hardtanh] Epoch: 120 Train Loss: 0.0942 Train Acc: 0.9681 Eval Loss: 0.4761 Eval Acc: 0.8745 (LR: 0.000010)
[2025-05-15 06:32:51,413]: [ResNet18_parametrized_hardtanh] Best Eval Accuracy: 0.8746
[2025-05-15 06:32:51,515]: 
Training of full-precision model finished!
[2025-05-15 06:32:51,515]: Model Architecture:
[2025-05-15 06:32:51,521]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-15 06:32:51,521]: 
Model Weights:
[2025-05-15 06:32:51,521]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-15 06:32:51,554]: Sample Values (25 elements): [0.12434021383523941, -0.15104155242443085, -0.15144601464271545, 0.17820897698402405, -0.1082577109336853, 0.18359261751174927, 0.12868545949459076, -0.13116195797920227, -0.10927893966436386, -0.24754683673381805, 0.09430701285600662, -0.04025595262646675, -0.05299062281847, 0.2927182614803314, -0.1675797551870346, -0.18770943582057953, 0.1036125123500824, -0.16674506664276123, -0.015477034263312817, -0.1488766074180603, -0.20160847902297974, 0.004010260105133057, -0.030360912904143333, -0.024992477148771286, 0.012417437508702278]
[2025-05-15 06:32:51,570]: Mean: 0.00078360
[2025-05-15 06:32:51,583]: Min: -0.41120452
[2025-05-15 06:32:51,586]: Max: 0.39359844
[2025-05-15 06:32:51,586]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-15 06:32:51,586]: Sample Values (25 elements): [0.9116148352622986, 0.9551081657409668, 1.0434249639511108, 0.9388318657875061, 0.9705438017845154, 0.9619328379631042, 0.8752133250236511, 1.0061025619506836, 0.8377150297164917, 1.0441429615020752, 0.9622702598571777, 0.8929070830345154, 0.9798679947853088, 0.8756006956100464, 1.0470106601715088, 0.9284124374389648, 0.9625970125198364, 0.9313601851463318, 0.9578903317451477, 0.9402377009391785, 1.012029767036438, 1.0618832111358643, 0.9744430780410767, 1.057863712310791, 1.029183268547058]
[2025-05-15 06:32:51,586]: Mean: 0.96746510
[2025-05-15 06:32:51,587]: Min: 0.83310366
[2025-05-15 06:32:51,587]: Max: 1.21639800
[2025-05-15 06:32:51,587]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 06:32:51,587]: Sample Values (25 elements): [0.003728213720023632, -0.001096502528525889, -0.03509048372507095, 0.014550970867276192, -0.006962357554584742, -0.016968443989753723, 0.009410122409462929, 0.035576850175857544, 0.03003235161304474, -0.020959768444299698, -0.02081925980746746, -0.001684211427345872, -0.019924752414226532, 0.008686699904501438, -0.0043958695605397224, 0.00972115807235241, -0.008132200688123703, -0.008227014914155006, 0.011857778765261173, -0.007920946925878525, -0.007503300439566374, 0.024988897144794464, 0.028414953500032425, -0.018212556838989258, 0.056564949452877045]
[2025-05-15 06:32:51,588]: Mean: -0.00006408
[2025-05-15 06:32:51,588]: Min: -0.14179780
[2025-05-15 06:32:51,588]: Max: 0.14775780
[2025-05-15 06:32:51,588]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-15 06:32:51,588]: Sample Values (25 elements): [0.9777173399925232, 0.9864124655723572, 0.9625332355499268, 0.9555999636650085, 0.9655479788780212, 0.9924834370613098, 0.9878058433532715, 0.9895964860916138, 0.9686455130577087, 0.9594645500183105, 0.9710235595703125, 0.9763351678848267, 0.9825875163078308, 0.9854060411453247, 0.9931465983390808, 1.0121170282363892, 0.971706211566925, 1.1001056432724, 0.9542377591133118, 0.9836545586585999, 0.9636098146438599, 1.003455400466919, 0.968229353427887, 0.9652022123336792, 0.9575316309928894]
[2025-05-15 06:32:51,588]: Mean: 0.97736210
[2025-05-15 06:32:51,589]: Min: 0.92706364
[2025-05-15 06:32:51,589]: Max: 1.10010564
[2025-05-15 06:32:51,589]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 06:32:51,590]: Sample Values (25 elements): [-0.019544268026947975, 0.00010775130795082077, 0.044779304414987564, -0.01612204872071743, 0.017044521868228912, -0.016405219212174416, 0.007286443840712309, -0.02987699769437313, -0.020627034828066826, -0.0004670700873248279, 0.015784813091158867, 0.0352606363594532, 0.010797887109220028, -0.07007528841495514, -0.0004877207102254033, 0.019349245354533195, 0.0027813997585326433, -0.018590133637189865, -0.0649481788277626, 0.05527329444885254, -0.023009078577160835, -0.03701717406511307, 0.051360808312892914, -0.0371234267950058, -0.03695983812212944]
[2025-05-15 06:32:51,590]: Mean: -0.00007848
[2025-05-15 06:32:51,590]: Min: -0.12884875
[2025-05-15 06:32:51,590]: Max: 0.12089157
[2025-05-15 06:32:51,590]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-15 06:32:51,591]: Sample Values (25 elements): [0.9578476548194885, 0.9644168615341187, 0.9601584076881409, 0.9703624248504639, 0.9459487795829773, 0.965910017490387, 0.9443226456642151, 0.9339372515678406, 0.9351593852043152, 0.9713102579116821, 0.9400350451469421, 0.9628951549530029, 0.9645415544509888, 1.1153901815414429, 0.9743552207946777, 0.9280973672866821, 0.9646605849266052, 0.94244784116745, 0.9845237731933594, 0.9828981161117554, 0.9497635960578918, 1.0120505094528198, 0.9488353133201599, 0.9679971933364868, 0.9677585363388062]
[2025-05-15 06:32:51,591]: Mean: 0.96913123
[2025-05-15 06:32:51,591]: Min: 0.92498618
[2025-05-15 06:32:51,591]: Max: 1.11985648
[2025-05-15 06:32:51,591]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 06:32:51,592]: Sample Values (25 elements): [-0.01902899518609047, -0.0008524682489223778, 0.009595729410648346, 0.007481975480914116, -0.007516474463045597, 0.025897247716784477, 0.009282134473323822, -0.041330598294734955, 0.027987172827124596, 0.02820700965821743, -0.017841028049588203, 0.03359933942556381, -0.05945751816034317, 0.01000023353844881, 0.03244234248995781, 0.02036837860941887, 0.022556552663445473, -0.028480732813477516, 0.021531544625759125, 0.06098819151520729, 0.03870652988553047, -0.00263856234960258, -0.009602129459381104, 0.001136314938776195, -0.026910841464996338]
[2025-05-15 06:32:51,592]: Mean: 0.00033940
[2025-05-15 06:32:51,592]: Min: -0.09597174
[2025-05-15 06:32:51,592]: Max: 0.11175645
[2025-05-15 06:32:51,593]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-15 06:32:51,593]: Sample Values (25 elements): [0.9672318696975708, 0.9656546115875244, 0.9753287434577942, 0.9724940061569214, 0.97734135389328, 0.9804646372795105, 0.9719914197921753, 0.988062858581543, 0.9991708397865295, 0.9780800342559814, 0.9598140120506287, 0.9735369682312012, 0.9691874980926514, 0.9663304090499878, 0.9769163727760315, 0.9782103300094604, 0.973718523979187, 0.970088541507721, 0.9678249955177307, 0.9827491641044617, 0.9710932374000549, 0.9725697636604309, 0.9547393321990967, 0.9637848734855652, 0.9682085514068604]
[2025-05-15 06:32:51,593]: Mean: 0.97838348
[2025-05-15 06:32:51,593]: Min: 0.95473933
[2025-05-15 06:32:51,593]: Max: 1.06992745
[2025-05-15 06:32:51,593]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 06:32:51,594]: Sample Values (25 elements): [0.001029368257150054, -0.009581949561834335, -0.030480192974209785, -0.00830849353224039, -0.015958266332745552, -0.029667874798178673, 0.0077930171974003315, -0.032822731882333755, -0.03187606483697891, 0.04212907701730728, -0.019523179158568382, -0.0012470590882003307, -0.03917918726801872, 0.021870696917176247, -0.0006554301944561303, -0.026532595977187157, -0.01881350390613079, -0.04278821870684624, 0.015527954325079918, -0.0372045561671257, 0.020801769569516182, -0.0015668468549847603, 0.020834943279623985, -0.015168326906859875, 0.023272600024938583]
[2025-05-15 06:32:51,594]: Mean: -0.00010767
[2025-05-15 06:32:51,594]: Min: -0.08975548
[2025-05-15 06:32:51,594]: Max: 0.08391234
[2025-05-15 06:32:51,594]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-15 06:32:51,595]: Sample Values (25 elements): [0.9672477841377258, 0.9825685024261475, 0.9763668775558472, 0.9689163565635681, 0.9856022596359253, 0.9942256808280945, 0.9892193078994751, 0.9588354825973511, 0.9881146550178528, 0.958812952041626, 1.0199527740478516, 0.9648759961128235, 0.9514076709747314, 0.9742448925971985, 0.9492195844650269, 0.9755816459655762, 0.9671469926834106, 0.9598252177238464, 0.9473491907119751, 0.9897561073303223, 0.9742931127548218, 0.9426024556159973, 0.9345899224281311, 0.9467261433601379, 1.0043011903762817]
[2025-05-15 06:32:51,595]: Mean: 0.97126728
[2025-05-15 06:32:51,595]: Min: 0.92684186
[2025-05-15 06:32:51,595]: Max: 1.01995277
[2025-05-15 06:32:51,595]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-15 06:32:51,596]: Sample Values (25 elements): [-0.045798223465681076, -0.0383431576192379, -0.02836022526025772, -0.017451204359531403, -0.03248756006360054, 0.02855648845434189, 0.009082501754164696, -0.015364190563559532, -0.03569473326206207, 0.017738401889801025, -0.04272884875535965, 0.035604704171419144, -0.005573789123445749, -0.0122872618958354, -0.01704530604183674, 0.03161363676190376, 0.02818979322910309, -0.02469225414097309, -0.015596183948218822, 0.01644052192568779, -0.02647807076573372, 0.008410112001001835, 0.029311371967196465, -0.002013634890317917, 0.023840397596359253]
[2025-05-15 06:32:51,596]: Mean: 0.00001423
[2025-05-15 06:32:51,596]: Min: -0.06994440
[2025-05-15 06:32:51,597]: Max: 0.07167622
[2025-05-15 06:32:51,597]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-15 06:32:51,597]: Sample Values (25 elements): [0.9790022969245911, 0.9733628034591675, 0.9730313420295715, 0.9744225740432739, 0.9771174192428589, 0.9679608941078186, 0.965830385684967, 0.9690929055213928, 0.9787100553512573, 0.9729838967323303, 0.9684039950370789, 0.971073567867279, 0.9829988479614258, 0.9804151058197021, 0.9749287962913513, 0.9727317690849304, 0.9697283506393433, 0.9716233015060425, 0.9893290996551514, 0.9738872051239014, 0.9716190099716187, 0.9859005808830261, 0.9697635769844055, 0.9739898443222046, 0.9791398048400879]
[2025-05-15 06:32:51,597]: Mean: 0.97356260
[2025-05-15 06:32:51,597]: Min: 0.96172726
[2025-05-15 06:32:51,597]: Max: 0.98932910
[2025-05-15 06:32:51,597]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 06:32:51,599]: Sample Values (25 elements): [0.0325966440141201, 0.01513401698321104, -0.019577205181121826, -0.004165553022176027, -0.03335002437233925, -0.009082547388970852, 0.01962343603372574, 0.019424401223659515, 0.0297198835760355, -0.01977815106511116, -0.0189007930457592, -0.013483607210218906, -0.013619769364595413, 0.012009166181087494, -0.008281724527478218, 0.0035365780349820852, 0.019791757687926292, 0.015274119563400745, 0.010361624881625175, -0.022674957290291786, 0.004096166230738163, 0.020939383655786514, 0.0014031112659722567, 0.007734071463346481, -0.02988116443157196]
[2025-05-15 06:32:51,599]: Mean: 0.00007068
[2025-05-15 06:32:51,599]: Min: -0.05814445
[2025-05-15 06:32:51,600]: Max: 0.05927856
[2025-05-15 06:32:51,600]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-15 06:32:51,600]: Sample Values (25 elements): [0.9675616025924683, 0.9881186485290527, 0.9799613952636719, 0.9752340316772461, 0.97639000415802, 0.9698312878608704, 0.9644221663475037, 0.9761549830436707, 0.9697363376617432, 0.9785082340240479, 0.9696914553642273, 0.9638206362724304, 0.9678189158439636, 0.9740045666694641, 0.9852516651153564, 0.979250431060791, 0.9743855595588684, 0.9729222655296326, 0.9686746001243591, 0.9728155732154846, 0.9795023798942566, 0.9767953157424927, 0.9752179384231567, 0.9668017029762268, 0.9779819846153259]
[2025-05-15 06:32:51,600]: Mean: 0.97492361
[2025-05-15 06:32:51,600]: Min: 0.95364130
[2025-05-15 06:32:51,600]: Max: 0.99651057
[2025-05-15 06:32:51,600]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-15 06:32:51,601]: Sample Values (25 elements): [0.130635067820549, 0.02302459441125393, -0.09430745989084244, -0.03931180015206337, -0.013938548043370247, -0.11846094578504562, 0.10576097667217255, 0.030864886939525604, -0.13403968513011932, -0.06106450408697128, 0.03950310871005058, -0.015180972404778004, -0.0715811550617218, 0.06227968633174896, -0.060867320746183395, -0.007994642481207848, -0.11018452793359756, 0.047184739261865616, 0.0954144150018692, 0.01314796507358551, 0.05882865935564041, 0.01478311512619257, 0.0694131925702095, 0.008646639063954353, 0.10209835320711136]
[2025-05-15 06:32:51,601]: Mean: -0.00094543
[2025-05-15 06:32:51,601]: Min: -0.15664156
[2025-05-15 06:32:51,601]: Max: 0.15331252
[2025-05-15 06:32:51,601]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-15 06:32:51,602]: Sample Values (25 elements): [0.9726186394691467, 0.9552741050720215, 0.9425989985466003, 0.9587097764015198, 0.9631949663162231, 0.9644200801849365, 0.9576662182807922, 0.9624584317207336, 0.9614471197128296, 0.9654739499092102, 0.9579309821128845, 0.9562928080558777, 0.9596961140632629, 0.9462177157402039, 0.9476118087768555, 0.9525372385978699, 0.9513313174247742, 0.9538318514823914, 0.957832932472229, 0.9542273879051208, 0.9538099765777588, 0.9555531144142151, 0.9545849561691284, 0.9590057134628296, 0.9545294642448425]
[2025-05-15 06:32:51,602]: Mean: 0.95925868
[2025-05-15 06:32:51,602]: Min: 0.94191211
[2025-05-15 06:32:51,602]: Max: 0.98660910
[2025-05-15 06:32:51,602]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 06:32:51,604]: Sample Values (25 elements): [0.013165807351469994, -0.007007237523794174, 0.009132206439971924, 0.03369397297501564, 0.038717299699783325, -0.0015270713483914733, -0.02311226911842823, 0.03746607527136803, -0.018759993836283684, 0.008668839931488037, -0.0005175662809051573, 0.006636782083660364, -0.007007697597146034, -0.034229908138513565, 0.016271186992526054, -0.021453076973557472, 0.021325189620256424, 0.022990025579929352, 0.024256691336631775, -0.02650529146194458, 0.016257507726550102, -0.006198136135935783, -0.01695052906870842, -0.007014425005763769, 0.011490131728351116]
[2025-05-15 06:32:51,604]: Mean: 0.00000259
[2025-05-15 06:32:51,604]: Min: -0.05711386
[2025-05-15 06:32:51,604]: Max: 0.05527501
[2025-05-15 06:32:51,604]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-15 06:32:51,605]: Sample Values (25 elements): [0.9863041639328003, 0.9715670347213745, 0.9800199866294861, 0.9724696278572083, 0.9790224432945251, 0.9755308032035828, 0.9804860949516296, 0.980920672416687, 0.9717809557914734, 0.9778846502304077, 0.9784939289093018, 0.9766545295715332, 0.9818695783615112, 0.9793440103530884, 0.9857416152954102, 0.9752415418624878, 0.9711495041847229, 0.973966121673584, 0.9736611247062683, 0.9661793112754822, 0.9779219627380371, 0.9745320677757263, 0.9768469333648682, 0.9808956980705261, 0.969767689704895]
[2025-05-15 06:32:51,605]: Mean: 0.97588944
[2025-05-15 06:32:51,605]: Min: 0.96543038
[2025-05-15 06:32:51,605]: Max: 0.98713213
[2025-05-15 06:32:51,605]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 06:32:51,607]: Sample Values (25 elements): [-0.0058271498419344425, -0.0034341071732342243, 0.0032978225499391556, 0.014076144434511662, 0.019484203308820724, 0.022800257429480553, 0.026362445205450058, -0.015676595270633698, 0.025270309299230576, 0.012111800722777843, 0.011605308391153812, -0.00010215340444119647, 0.020173218101263046, 0.02076508291065693, 0.008345266804099083, -0.011753560975193977, 0.0315682515501976, -0.007310566958039999, -0.004487878177314997, 0.01385435089468956, 0.019715184345841408, -0.0005463677807711065, 0.004046787042170763, -0.003605829318985343, 0.016554348170757294]
[2025-05-15 06:32:51,607]: Mean: -0.00006356
[2025-05-15 06:32:51,607]: Min: -0.05334765
[2025-05-15 06:32:51,607]: Max: 0.05494652
[2025-05-15 06:32:51,607]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-15 06:32:51,608]: Sample Values (25 elements): [0.9656065106391907, 0.9733211398124695, 0.9814517498016357, 0.9795888662338257, 0.9720668196678162, 0.9954961538314819, 0.9795219302177429, 0.9731932282447815, 0.9871426224708557, 0.9923867583274841, 0.9824929237365723, 0.9866752624511719, 0.9789091348648071, 0.9823062419891357, 0.9724505543708801, 0.9937458634376526, 0.9837357997894287, 0.9786747097969055, 0.9782343506813049, 0.9829905033111572, 0.9809907674789429, 0.9723073244094849, 0.9712012410163879, 0.9801645874977112, 0.976024866104126]
[2025-05-15 06:32:51,608]: Mean: 0.97976553
[2025-05-15 06:32:51,608]: Min: 0.96281105
[2025-05-15 06:32:51,608]: Max: 0.99691445
[2025-05-15 06:32:51,608]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-15 06:32:51,611]: Sample Values (25 elements): [-0.001284926780499518, -0.014345274306833744, 0.012315623462200165, -0.024109380319714546, -0.021890735253691673, 0.02547212317585945, 0.023132309317588806, 0.019022155553102493, 0.021079355850815773, 0.005256227683275938, 0.018497560173273087, -0.024687131866812706, -0.033355552703142166, 0.006519457791000605, 0.009606469422578812, -0.029109496623277664, 0.02020101621747017, -0.009737145155668259, 0.023066366091370583, 0.03174653649330139, -0.02174004353582859, 0.008322875015437603, -0.014122388325631618, 0.017723925411701202, 0.002607070840895176]
[2025-05-15 06:32:51,611]: Mean: -0.00000885
[2025-05-15 06:32:51,611]: Min: -0.04945124
[2025-05-15 06:32:51,611]: Max: 0.04820519
[2025-05-15 06:32:51,612]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-15 06:32:51,612]: Sample Values (25 elements): [0.9828376173973083, 0.9704439043998718, 0.9737317562103271, 0.9730525612831116, 0.9701876044273376, 0.9768370985984802, 0.9703113436698914, 0.971559464931488, 0.9688243865966797, 0.9744547009468079, 0.971137523651123, 0.9731054306030273, 0.9742880463600159, 0.9706010818481445, 0.9702693819999695, 0.9733697772026062, 0.9721279740333557, 0.9700425267219543, 0.9705726504325867, 0.9719861745834351, 0.9726132750511169, 0.976913571357727, 0.9678337574005127, 0.9729398488998413, 0.975104808807373]
[2025-05-15 06:32:51,612]: Mean: 0.97346711
[2025-05-15 06:32:51,612]: Min: 0.96688914
[2025-05-15 06:32:51,612]: Max: 0.98316169
[2025-05-15 06:32:51,613]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 06:32:51,618]: Sample Values (25 elements): [-0.003075641579926014, 0.0070076812990009785, 0.006215372588485479, -0.010565525852143764, 0.007368636317551136, -0.014416417106986046, -0.01843436434864998, -0.004256632644683123, -0.017743617296218872, 0.001088357879780233, -0.0047181579284369946, -0.012324503622949123, 0.010405192151665688, -0.01847987249493599, 0.003652635496109724, -0.003796944161877036, -0.01693928800523281, -0.005510034970939159, 0.018387435004115105, -0.02255125716328621, 0.009854992851614952, 0.0011398146161809564, 0.006677665282040834, 0.020765384659171104, 0.0018414335791021585]
[2025-05-15 06:32:51,618]: Mean: -0.00000848
[2025-05-15 06:32:51,618]: Min: -0.04171261
[2025-05-15 06:32:51,618]: Max: 0.04152089
[2025-05-15 06:32:51,618]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-15 06:32:51,619]: Sample Values (25 elements): [0.969399094581604, 0.9760229587554932, 0.974475622177124, 0.9680209755897522, 0.9785686135292053, 0.9714275002479553, 0.9791654944419861, 0.9727824926376343, 0.9795382022857666, 0.9730086326599121, 0.9759179353713989, 0.9701113700866699, 0.9749817848205566, 0.9770386815071106, 0.9796465635299683, 0.9744011759757996, 0.9724218249320984, 0.9782722592353821, 0.9746471643447876, 0.97205650806427, 0.9750282764434814, 0.9755414128303528, 0.9805124402046204, 0.9731629490852356, 0.9715202450752258]
[2025-05-15 06:32:51,619]: Mean: 0.97601342
[2025-05-15 06:32:51,619]: Min: 0.96371138
[2025-05-15 06:32:51,619]: Max: 0.99014699
[2025-05-15 06:32:51,619]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-15 06:32:51,620]: Sample Values (25 elements): [0.05147213861346245, -0.08359478414058685, 0.06644278764724731, -0.02101745642721653, -0.02541608177125454, -0.034816090017557144, 0.019210508093237877, -0.046419065445661545, 0.07011355459690094, 0.027190586552023888, 0.05349231883883476, -0.027383308857679367, -0.052094485610723495, -0.03614009544253349, 0.02938811667263508, 0.04687760770320892, 0.04254427179694176, -0.05062001571059227, 0.02699062041938305, -0.029774846509099007, -0.07387518882751465, -0.032281361520290375, -0.049194108694791794, 0.005489078350365162, 0.03056137077510357]
[2025-05-15 06:32:51,620]: Mean: -0.00008918
[2025-05-15 06:32:51,620]: Min: -0.10795404
[2025-05-15 06:32:51,620]: Max: 0.10766604
[2025-05-15 06:32:51,620]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-15 06:32:51,621]: Sample Values (25 elements): [0.9642304182052612, 0.9634446501731873, 0.9656150341033936, 0.9592813849449158, 0.9600311517715454, 0.959222674369812, 0.9622583985328674, 0.9634039402008057, 0.9606331586837769, 0.9584519863128662, 0.9602487087249756, 0.9683231115341187, 0.9570095539093018, 0.9638276696205139, 0.9643041491508484, 0.9576950073242188, 0.9749674797058105, 0.963575005531311, 0.9527742862701416, 0.960422933101654, 0.9636038541793823, 0.9560875296592712, 0.9605872631072998, 0.9611981511116028, 0.9565637707710266]
[2025-05-15 06:32:51,621]: Mean: 0.96172339
[2025-05-15 06:32:51,621]: Min: 0.94957584
[2025-05-15 06:32:51,621]: Max: 0.97496748
[2025-05-15 06:32:51,621]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 06:32:51,627]: Sample Values (25 elements): [-0.006812374107539654, -0.0035849199630320072, -0.02152192033827305, 0.022561879828572273, 0.014277911745011806, 0.003551406553015113, 0.007352142129093409, 0.007126076612621546, -0.005224193446338177, 0.013139710761606693, 0.01331418938934803, -0.011010460555553436, -0.010771882720291615, -0.023286951705813408, -0.0034057889133691788, -0.016670260578393936, -0.018549229949712753, 0.002111028181388974, 0.001294880174100399, -0.001040734932757914, -0.0057063838467001915, -0.014105750247836113, 0.014577287249267101, -0.004660398233681917, -0.012775241397321224]
[2025-05-15 06:32:51,627]: Mean: 0.00000481
[2025-05-15 06:32:51,627]: Min: -0.04080170
[2025-05-15 06:32:51,627]: Max: 0.04125913
[2025-05-15 06:32:51,627]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-15 06:32:51,628]: Sample Values (25 elements): [0.9769774079322815, 0.9741196036338806, 0.9789115190505981, 0.9743130803108215, 0.97393399477005, 0.9742644429206848, 0.972197949886322, 0.9740743041038513, 0.9746665954589844, 0.9791437387466431, 0.9735673069953918, 0.9715204238891602, 0.9757468104362488, 0.9721435308456421, 0.9744390249252319, 0.9713919162750244, 0.9733356237411499, 0.974739134311676, 0.9714156985282898, 0.973853349685669, 0.9774801731109619, 0.9730245471000671, 0.9703468680381775, 0.9773381352424622, 0.9697698354721069]
[2025-05-15 06:32:51,628]: Mean: 0.97426969
[2025-05-15 06:32:51,628]: Min: 0.96688676
[2025-05-15 06:32:51,628]: Max: 0.98522693
[2025-05-15 06:32:51,628]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 06:32:51,633]: Sample Values (25 elements): [-0.009882340207695961, -0.015261984430253506, -0.003523419378325343, 0.007064372766762972, 0.009562080726027489, -0.006607356481254101, -0.012939438223838806, 0.01518330443650484, -0.019937168806791306, -0.0010370904346928, -0.012799563817679882, -0.02355096861720085, 0.019295159727334976, -0.012193121947348118, 0.009336165152490139, -0.026032719761133194, 0.01734727993607521, 0.008348342962563038, -0.005476867314428091, -0.007707861717790365, -0.011086031794548035, -0.01777339167892933, -0.011401921510696411, 0.005844827741384506, -0.01168642845004797]
[2025-05-15 06:32:51,633]: Mean: -0.00001515
[2025-05-15 06:32:51,634]: Min: -0.03928332
[2025-05-15 06:32:51,634]: Max: 0.03949097
[2025-05-15 06:32:51,634]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-15 06:32:51,634]: Sample Values (25 elements): [0.9831033945083618, 0.9779820442199707, 0.9763681292533875, 0.9799618124961853, 0.9754441976547241, 0.9763010144233704, 0.9747858047485352, 0.9834631681442261, 0.9784753322601318, 0.9831331372261047, 0.9763142466545105, 0.97532057762146, 0.9718911647796631, 0.9795722365379333, 0.976891279220581, 0.9714683294296265, 0.9784533977508545, 0.9785130620002747, 0.9817187786102295, 0.9825323820114136, 0.9810255169868469, 0.984089195728302, 0.9727181196212769, 0.9775899648666382, 0.9723137617111206]
[2025-05-15 06:32:51,634]: Mean: 0.97928059
[2025-05-15 06:32:51,634]: Min: 0.96658754
[2025-05-15 06:32:51,635]: Max: 0.99817777
[2025-05-15 06:32:51,635]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-15 06:32:51,647]: Sample Values (25 elements): [0.002344281179830432, 0.0008070642943494022, 0.0023735438007861376, 0.011461847461760044, 0.0056270877830684185, -0.01642817072570324, 0.0012429642956703901, 0.013689029030501842, 0.009320971556007862, -0.022646257653832436, -0.012640838511288166, -0.005495967343449593, -0.0015397589886561036, 0.015063744969666004, 0.003278476884588599, -0.006451012566685677, -0.0042295558378100395, -0.018736587837338448, 0.010901504196226597, 0.009583904407918453, -0.004178863484412432, -0.01142164971679449, 0.01015246007591486, 0.008534260094165802, 0.007667426951229572]
[2025-05-15 06:32:51,647]: Mean: 0.00000894
[2025-05-15 06:32:51,648]: Min: -0.03777236
[2025-05-15 06:32:51,648]: Max: 0.03703844
[2025-05-15 06:32:51,648]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-15 06:32:51,648]: Sample Values (25 elements): [0.972466766834259, 0.9733266234397888, 0.9743046760559082, 0.9789615273475647, 0.9756832718849182, 0.9759660363197327, 0.9701945781707764, 0.9736563563346863, 0.9725244641304016, 0.9717389941215515, 0.9735289216041565, 0.9719127416610718, 0.9716770052909851, 0.9721410274505615, 0.9788199067115784, 0.9713281989097595, 0.9707834720611572, 0.9711498618125916, 0.9724926352500916, 0.9697532653808594, 0.9738026261329651, 0.9715585112571716, 0.9738314747810364, 0.9706522226333618, 0.972592830657959]
[2025-05-15 06:32:51,649]: Mean: 0.97271097
[2025-05-15 06:32:51,650]: Min: 0.96872735
[2025-05-15 06:32:51,651]: Max: 0.97896153
[2025-05-15 06:32:51,651]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 06:32:51,696]: Sample Values (25 elements): [0.02240785025060177, 0.011867066845297813, -0.005849689245223999, -0.0027127170469611883, -0.006803452968597412, 0.010576070286333561, -0.009597123600542545, 0.01006797794252634, 0.01319732517004013, 0.004489260260015726, 0.009961542673408985, 0.012129218317568302, -0.00275197415612638, 0.004150439519435167, 0.005026325583457947, -0.013815614394843578, 0.004976465366780758, 0.0026835890021175146, -0.002560599707067013, -0.007069081999361515, 0.009648672305047512, -0.009328946471214294, 0.00814090110361576, -0.01646845042705536, -0.011103357188403606]
[2025-05-15 06:32:51,697]: Mean: 0.00000592
[2025-05-15 06:32:51,697]: Min: -0.02720523
[2025-05-15 06:32:51,697]: Max: 0.02594111
[2025-05-15 06:32:51,697]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-15 06:32:51,697]: Sample Values (25 elements): [0.9771550893783569, 0.9760634303092957, 0.9758987426757812, 0.9753443002700806, 0.9773752093315125, 0.9772012829780579, 0.9778152704238892, 0.9743034839630127, 0.9742938876152039, 0.976198673248291, 0.9781529307365417, 0.9780622124671936, 0.9747750759124756, 0.9749505519866943, 0.9750662446022034, 0.9745515584945679, 0.9772794842720032, 0.9772838354110718, 0.976621687412262, 0.9734029769897461, 0.9729973077774048, 0.9773856401443481, 0.974084734916687, 0.9772923588752747, 0.9783200621604919]
[2025-05-15 06:32:51,698]: Mean: 0.97578406
[2025-05-15 06:32:51,698]: Min: 0.97010195
[2025-05-15 06:32:51,698]: Max: 0.98410487
[2025-05-15 06:32:51,698]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-15 06:32:51,699]: Sample Values (25 elements): [0.05734081566333771, 0.016993656754493713, 0.02189471200108528, 0.05640626326203346, 0.008125624619424343, -0.05010819807648659, 0.024092216044664383, -0.0036206895019859076, -0.05150185897946358, 0.011835412122309208, 0.04331434890627861, -0.03806779533624649, -0.040726158767938614, 0.04746459797024727, -0.030897630378603935, -0.00268899230286479, 0.02361740916967392, -0.0086906086653471, 0.05050600692629814, 0.03676394745707512, 0.05502671003341675, -0.032217685133218765, 0.030705781653523445, -0.007740273606032133, -0.003009207546710968]
[2025-05-15 06:32:51,700]: Mean: 0.00005125
[2025-05-15 06:32:51,700]: Min: -0.07322528
[2025-05-15 06:32:51,700]: Max: 0.07505479
[2025-05-15 06:32:51,700]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-15 06:32:51,700]: Sample Values (25 elements): [0.968041718006134, 0.9684162139892578, 0.9661833047866821, 0.9705412983894348, 0.9684371948242188, 0.965475857257843, 0.9665873050689697, 0.9665862321853638, 0.9655619859695435, 0.9633539915084839, 0.9669510722160339, 0.9666436910629272, 0.9693384766578674, 0.9646407961845398, 0.9693392515182495, 0.9675405025482178, 0.9670190215110779, 0.9673321843147278, 0.9657673239707947, 0.9654964804649353, 0.971367597579956, 0.9637179374694824, 0.9657252430915833, 0.9684737920761108, 0.966535747051239]
[2025-05-15 06:32:51,700]: Mean: 0.96765536
[2025-05-15 06:32:51,700]: Min: 0.96070290
[2025-05-15 06:32:51,701]: Max: 0.97234744
[2025-05-15 06:32:51,701]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 06:32:51,734]: Sample Values (25 elements): [-0.01436560694128275, -0.007515741512179375, 0.0016030179103836417, -0.004588091280311346, -0.010676363483071327, -0.0060507808811962605, 0.011589051224291325, -0.015106858685612679, -0.012798883952200413, -0.006438289303332567, 0.006146633066236973, -0.007685911376029253, 0.006882620509713888, -0.012279733084142208, 0.015607226639986038, 0.007907695136964321, -0.010223793797194958, 0.00842691957950592, -0.004813672974705696, 0.004033271688967943, 0.005263899452984333, -0.005455322097986937, 0.00540580740198493, 0.009316686540842056, -0.0034692990593612194]
[2025-05-15 06:32:51,734]: Mean: -0.00000619
[2025-05-15 06:32:51,735]: Min: -0.02700178
[2025-05-15 06:32:51,735]: Max: 0.02869938
[2025-05-15 06:32:51,735]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-15 06:32:51,736]: Sample Values (25 elements): [0.9744234085083008, 0.9764865636825562, 0.9719512462615967, 0.9721437096595764, 0.9734788537025452, 0.9717175960540771, 0.9755507707595825, 0.9712359309196472, 0.9728941917419434, 0.9730551838874817, 0.9714115262031555, 0.9749730229377747, 0.9716764092445374, 0.9707242250442505, 0.9739393591880798, 0.9733479619026184, 0.9761326313018799, 0.9756260514259338, 0.973422646522522, 0.9750803709030151, 0.9730045199394226, 0.9727136492729187, 0.9721390008926392, 0.9738045930862427, 0.9734534025192261]
[2025-05-15 06:32:51,736]: Mean: 0.97333342
[2025-05-15 06:32:51,737]: Min: 0.96965742
[2025-05-15 06:32:51,737]: Max: 0.98009229
[2025-05-15 06:32:51,737]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 06:32:51,782]: Sample Values (25 elements): [0.000508768716827035, 0.001149558462202549, -0.013670842163264751, -0.011622495017945766, -0.012536929920315742, 0.0056036487221717834, 0.014940756373107433, -0.00552942231297493, -0.001103790127672255, -0.011150687001645565, -0.008150240406394005, -0.007752112578600645, 0.0013459223555400968, -0.005631539039313793, 0.006550442427396774, -0.01104224007576704, 0.01183988805860281, -0.0009777124505490065, 0.009363087825477123, -0.004608412738889456, 0.010601870715618134, 0.011008401401340961, 0.010941016487777233, 0.0076612187549471855, 0.0033912749495357275]
[2025-05-15 06:32:51,783]: Mean: 0.00000390
[2025-05-15 06:32:51,783]: Min: -0.02406637
[2025-05-15 06:32:51,783]: Max: 0.02178293
[2025-05-15 06:32:51,783]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-15 06:32:51,783]: Sample Values (25 elements): [0.9814566373825073, 0.978857696056366, 0.9777094125747681, 0.9774737358093262, 0.9754889607429504, 0.9808917045593262, 0.9803285598754883, 0.9770112633705139, 0.9766798615455627, 0.9767385125160217, 0.9841606616973877, 0.9784630537033081, 0.9764137268066406, 0.9790367484092712, 0.9783263802528381, 0.9810747504234314, 0.9821529984474182, 0.9811372756958008, 0.9788826107978821, 0.9806880354881287, 0.9772995710372925, 0.978168249130249, 0.97823566198349, 0.9787566661834717, 0.9787614941596985]
[2025-05-15 06:32:51,783]: Mean: 0.97862649
[2025-05-15 06:32:51,784]: Min: 0.97330290
[2025-05-15 06:32:51,784]: Max: 0.98536015
[2025-05-15 06:32:51,784]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-15 06:32:51,784]: Sample Values (25 elements): [-0.07320769131183624, 0.03286140039563179, 0.00541461119428277, -0.08299954980611801, -0.06255436688661575, 0.03022712469100952, -0.002660593716427684, -0.02903527021408081, 0.06256036460399628, 0.07620545476675034, -0.03980683907866478, -0.011748583987355232, 0.0024131466634571552, -0.06578776240348816, 0.09898477047681808, -0.0406641885638237, -0.04731494560837746, -0.03608844056725502, -0.03813759982585907, 0.08562538027763367, 0.009437629021704197, -0.02768484503030777, -0.060329437255859375, -0.05641312524676323, 0.0259110014885664]
[2025-05-15 06:32:51,784]: Mean: 0.00079885
[2025-05-15 06:32:51,784]: Min: -0.12705013
[2025-05-15 06:32:51,785]: Max: 0.12743290
[2025-05-15 06:32:51,785]: 


QAT of ResNet18 with parametrized_hardtanh down to 4 bits...
[2025-05-15 06:32:52,092]: [ResNet18_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-15 06:32:52,231]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-15 06:34:47,867]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.2673 Train Acc: 0.9043 Eval Loss: 0.8907 Eval Acc: 0.7709 (LR: 0.001000)
[2025-05-15 06:36:52,805]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.2438 Train Acc: 0.9126 Eval Loss: 0.5737 Eval Acc: 0.8333 (LR: 0.001000)
[2025-05-15 06:38:49,194]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.2432 Train Acc: 0.9118 Eval Loss: 0.5685 Eval Acc: 0.8384 (LR: 0.001000)
[2025-05-15 06:40:45,552]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.2417 Train Acc: 0.9129 Eval Loss: 0.5765 Eval Acc: 0.8308 (LR: 0.001000)
[2025-05-15 06:42:41,931]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.2335 Train Acc: 0.9153 Eval Loss: 0.5838 Eval Acc: 0.8384 (LR: 0.001000)
[2025-05-15 06:44:36,475]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.2267 Train Acc: 0.9191 Eval Loss: 0.5728 Eval Acc: 0.8328 (LR: 0.001000)
[2025-05-15 06:46:30,724]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.2254 Train Acc: 0.9190 Eval Loss: 0.6093 Eval Acc: 0.8238 (LR: 0.001000)
[2025-05-15 06:48:23,970]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.2217 Train Acc: 0.9204 Eval Loss: 0.5731 Eval Acc: 0.8334 (LR: 0.001000)
[2025-05-15 06:50:16,976]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.2252 Train Acc: 0.9189 Eval Loss: 0.5815 Eval Acc: 0.8399 (LR: 0.001000)
[2025-05-15 06:52:09,833]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.2118 Train Acc: 0.9228 Eval Loss: 0.5976 Eval Acc: 0.8387 (LR: 0.001000)
[2025-05-15 06:54:02,317]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.2058 Train Acc: 0.9266 Eval Loss: 0.5422 Eval Acc: 0.8433 (LR: 0.001000)
[2025-05-15 06:55:54,082]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.2135 Train Acc: 0.9244 Eval Loss: 0.5952 Eval Acc: 0.8344 (LR: 0.001000)
[2025-05-15 06:57:45,267]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.2077 Train Acc: 0.9245 Eval Loss: 0.5444 Eval Acc: 0.8483 (LR: 0.001000)
[2025-05-15 06:59:40,601]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.1994 Train Acc: 0.9298 Eval Loss: 0.5844 Eval Acc: 0.8400 (LR: 0.001000)
[2025-05-15 07:01:41,053]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.1993 Train Acc: 0.9286 Eval Loss: 0.5647 Eval Acc: 0.8459 (LR: 0.001000)
[2025-05-15 07:03:37,464]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.2000 Train Acc: 0.9283 Eval Loss: 0.5395 Eval Acc: 0.8475 (LR: 0.001000)
[2025-05-15 07:05:33,473]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.1910 Train Acc: 0.9316 Eval Loss: 0.5771 Eval Acc: 0.8415 (LR: 0.001000)
[2025-05-15 07:07:29,384]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.1892 Train Acc: 0.9313 Eval Loss: 0.5758 Eval Acc: 0.8430 (LR: 0.001000)
[2025-05-15 07:09:25,095]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.1851 Train Acc: 0.9340 Eval Loss: 0.5765 Eval Acc: 0.8402 (LR: 0.001000)
[2025-05-15 07:11:22,142]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.1867 Train Acc: 0.9327 Eval Loss: 0.5390 Eval Acc: 0.8480 (LR: 0.001000)
[2025-05-15 07:13:18,149]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.1854 Train Acc: 0.9344 Eval Loss: 0.6200 Eval Acc: 0.8332 (LR: 0.001000)
[2025-05-15 07:15:14,053]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.1802 Train Acc: 0.9362 Eval Loss: 0.5799 Eval Acc: 0.8469 (LR: 0.001000)
[2025-05-15 07:17:10,245]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.1722 Train Acc: 0.9383 Eval Loss: 0.5715 Eval Acc: 0.8450 (LR: 0.001000)
[2025-05-15 07:19:06,023]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.1755 Train Acc: 0.9379 Eval Loss: 0.6080 Eval Acc: 0.8354 (LR: 0.001000)
[2025-05-15 07:21:02,168]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.1757 Train Acc: 0.9359 Eval Loss: 0.6066 Eval Acc: 0.8409 (LR: 0.001000)
[2025-05-15 07:22:58,395]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.1684 Train Acc: 0.9389 Eval Loss: 0.6506 Eval Acc: 0.8325 (LR: 0.001000)
[2025-05-15 07:24:54,467]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.1711 Train Acc: 0.9391 Eval Loss: 0.6056 Eval Acc: 0.8384 (LR: 0.001000)
[2025-05-15 07:26:50,417]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.1615 Train Acc: 0.9419 Eval Loss: 0.5360 Eval Acc: 0.8545 (LR: 0.001000)
[2025-05-15 07:28:46,634]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.1644 Train Acc: 0.9408 Eval Loss: 0.5465 Eval Acc: 0.8536 (LR: 0.001000)
[2025-05-15 07:30:42,810]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.1696 Train Acc: 0.9388 Eval Loss: 0.6024 Eval Acc: 0.8436 (LR: 0.000250)
[2025-05-15 07:32:38,738]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.1028 Train Acc: 0.9643 Eval Loss: 0.6088 Eval Acc: 0.8431 (LR: 0.000250)
[2025-05-15 07:34:34,698]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.0923 Train Acc: 0.9689 Eval Loss: 0.4990 Eval Acc: 0.8706 (LR: 0.000250)
[2025-05-15 07:36:30,483]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.0863 Train Acc: 0.9704 Eval Loss: 0.4895 Eval Acc: 0.8736 (LR: 0.000250)
[2025-05-15 07:38:28,986]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.0842 Train Acc: 0.9714 Eval Loss: 0.5089 Eval Acc: 0.8704 (LR: 0.000250)
[2025-05-15 07:40:36,907]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.0811 Train Acc: 0.9725 Eval Loss: 0.4896 Eval Acc: 0.8733 (LR: 0.000250)
[2025-05-15 07:42:29,574]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.0804 Train Acc: 0.9732 Eval Loss: 0.5081 Eval Acc: 0.8710 (LR: 0.000250)
[2025-05-15 07:44:20,972]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.0771 Train Acc: 0.9737 Eval Loss: 0.5090 Eval Acc: 0.8692 (LR: 0.000250)
[2025-05-15 07:46:19,101]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.0781 Train Acc: 0.9734 Eval Loss: 0.5207 Eval Acc: 0.8701 (LR: 0.000250)
[2025-05-15 07:48:15,717]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.0747 Train Acc: 0.9746 Eval Loss: 0.5778 Eval Acc: 0.8599 (LR: 0.000250)
[2025-05-15 07:50:15,344]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.0739 Train Acc: 0.9752 Eval Loss: 0.5297 Eval Acc: 0.8703 (LR: 0.000250)
[2025-05-15 07:52:04,411]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.0745 Train Acc: 0.9741 Eval Loss: 0.5266 Eval Acc: 0.8726 (LR: 0.000250)
[2025-05-15 07:53:53,914]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.0708 Train Acc: 0.9754 Eval Loss: 0.5204 Eval Acc: 0.8729 (LR: 0.000250)
[2025-05-15 07:55:42,684]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.0708 Train Acc: 0.9756 Eval Loss: 0.5398 Eval Acc: 0.8725 (LR: 0.000250)
[2025-05-15 07:57:31,308]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.0690 Train Acc: 0.9768 Eval Loss: 0.5546 Eval Acc: 0.8645 (LR: 0.000250)
[2025-05-15 07:59:20,192]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.0667 Train Acc: 0.9778 Eval Loss: 0.5279 Eval Acc: 0.8742 (LR: 0.000063)
[2025-05-15 08:01:08,913]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.0601 Train Acc: 0.9804 Eval Loss: 0.5080 Eval Acc: 0.8778 (LR: 0.000063)
[2025-05-15 08:02:57,965]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.0556 Train Acc: 0.9824 Eval Loss: 0.5222 Eval Acc: 0.8761 (LR: 0.000063)
[2025-05-15 08:04:46,514]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.0581 Train Acc: 0.9803 Eval Loss: 0.5256 Eval Acc: 0.8750 (LR: 0.000063)
[2025-05-15 08:06:35,296]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.0553 Train Acc: 0.9816 Eval Loss: 0.5171 Eval Acc: 0.8715 (LR: 0.000063)
[2025-05-15 08:08:35,780]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.0528 Train Acc: 0.9826 Eval Loss: 0.5143 Eval Acc: 0.8771 (LR: 0.000063)
[2025-05-15 08:10:48,273]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.0528 Train Acc: 0.9830 Eval Loss: 0.5162 Eval Acc: 0.8745 (LR: 0.000063)
[2025-05-15 08:13:00,607]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.0531 Train Acc: 0.9826 Eval Loss: 0.5670 Eval Acc: 0.8677 (LR: 0.000063)
[2025-05-15 08:15:12,652]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.0528 Train Acc: 0.9825 Eval Loss: 0.5093 Eval Acc: 0.8773 (LR: 0.000063)
[2025-05-15 08:17:24,404]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.0512 Train Acc: 0.9830 Eval Loss: 0.5358 Eval Acc: 0.8734 (LR: 0.000063)
[2025-05-15 08:19:36,324]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.0525 Train Acc: 0.9830 Eval Loss: 0.5485 Eval Acc: 0.8720 (LR: 0.000063)
[2025-05-15 08:21:48,199]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.0507 Train Acc: 0.9838 Eval Loss: 0.5365 Eval Acc: 0.8714 (LR: 0.000063)
[2025-05-15 08:24:00,547]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.0507 Train Acc: 0.9831 Eval Loss: 0.5575 Eval Acc: 0.8714 (LR: 0.000063)
[2025-05-15 08:26:12,870]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.0528 Train Acc: 0.9825 Eval Loss: 0.5345 Eval Acc: 0.8780 (LR: 0.000063)
[2025-05-15 08:28:24,968]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.0523 Train Acc: 0.9830 Eval Loss: 0.5307 Eval Acc: 0.8793 (LR: 0.000063)
[2025-05-15 08:30:38,624]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.0505 Train Acc: 0.9837 Eval Loss: 0.6335 Eval Acc: 0.8547 (LR: 0.000063)
[2025-05-15 08:30:38,625]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8793
[2025-05-15 08:30:38,729]: 


Quantization of model down to 4 bits finished
[2025-05-15 08:30:38,729]: Model Architecture:
[2025-05-15 08:30:38,784]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0991], device='cuda:0'), zero_point=tensor([14], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4365005493164062, max_val=0.04959505423903465)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0218], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16425149142742157, max_val=0.16212865710258484)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0902], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49579301476478577, max_val=0.8569788932800293)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15299944579601288, max_val=0.13830241560935974)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0984], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4427978992462158, max_val=1.033178687095642)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0159], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10988226532936096, max_val=0.12842218577861786)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0880], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6596587896347046, max_val=0.6596566438674927)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09802934527397156, max_val=0.0932774692773819)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1422], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.92403244972229, max_val=1.2083451747894287)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0101], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07577094435691833, max_val=0.07565218955278397)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1030], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9782546758651733, max_val=0.5663648247718811)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06564725935459137, max_val=0.06364446133375168)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1671818345785141, max_val=0.1610690951347351)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1081], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.918957531452179, max_val=0.7027137875556946)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0084], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06495434790849686, max_val=0.06069401651620865)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0768], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6528242826461792, max_val=0.49923959374427795)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0082], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.059703510254621506, max_val=0.06398898363113403)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1496], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0998657941818237, max_val=1.1444261074066162)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.051572613418102264, max_val=0.05402980372309685)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0871], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7403417229652405, max_val=0.5660548806190491)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04675573110580444, max_val=0.04583652690052986)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0152], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11221909523010254, max_val=0.1159798875451088)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0981], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.834205150604248, max_val=0.6379439830780029)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0063], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04433079808950424, max_val=0.050217460840940475)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0685], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5823359489440918, max_val=0.44525444507598877)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0060], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04649116471409798, max_val=0.04402939975261688)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1195], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7767227292060852, max_val=1.0157233476638794)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.039236053824424744, max_val=0.04121139273047447)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0890], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5784437656402588, max_val=0.756393551826477)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.029771694913506508, max_val=0.029749231413006783)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07690033316612244, max_val=0.0796051025390625)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1178], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5303420424461365, max_val=1.237406611442566)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02967674285173416, max_val=0.03015333041548729)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0537], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4030689597129822, max_val=0.4030054211616516)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02537558414041996, max_val=0.022598812356591225)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2359], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5447334051132202, max_val=1.9932210445404053)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-15 08:30:38,784]: 
Model Weights:
[2025-05-15 08:30:38,785]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-15 08:30:38,785]: Sample Values (25 elements): [0.02605379745364189, -0.013544310815632343, -0.014491526409983635, 0.0946493074297905, 0.15914687514305115, -0.03692370653152466, -0.03006000630557537, -0.03498000651597977, 0.002265151357278228, -0.1079072505235672, -0.11203549802303314, 0.2498968094587326, -0.20741991698741913, -0.10051529109477997, 0.021309247240424156, 0.22276034951210022, -0.2194163203239441, 0.14144077897071838, -0.331144779920578, 0.10335031151771545, 0.11964120715856552, -0.0877053439617157, -0.09126952290534973, -0.14358048141002655, 0.05963340029120445]
[2025-05-15 08:30:38,785]: Mean: 0.00130787
[2025-05-15 08:30:38,785]: Min: -0.46896154
[2025-05-15 08:30:38,786]: Max: 0.44032913
[2025-05-15 08:30:38,786]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-15 08:30:38,786]: Sample Values (25 elements): [0.8900275230407715, 1.0052217245101929, 1.0503771305084229, 1.0857470035552979, 0.9908614754676819, 1.055696964263916, 0.9975528120994568, 0.9161355495452881, 0.9444766044616699, 0.9932683706283569, 1.0974739789962769, 0.8628397583961487, 0.9592008590698242, 0.859271764755249, 1.2649916410446167, 0.9643378257751465, 0.8798660039901733, 1.030449390411377, 0.8272072076797485, 1.0411381721496582, 0.8398323655128479, 0.8129567503929138, 0.9276178479194641, 1.1757898330688477, 0.860474169254303]
[2025-05-15 08:30:38,786]: Mean: 0.95243919
[2025-05-15 08:30:38,786]: Min: 0.77495748
[2025-05-15 08:30:38,786]: Max: 1.26499164
[2025-05-15 08:30:38,787]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 08:30:38,788]: Sample Values (25 elements): [-0.04351739212870598, 0.04351739212870598, 0.02175869606435299, 0.0, -0.04351739212870598, -0.02175869606435299, -0.02175869606435299, -0.08703478425741196, -0.06527608633041382, 0.0, 0.02175869606435299, 0.0, 0.02175869606435299, 0.04351739212870598, 0.02175869606435299, -0.06527608633041382, 0.02175869606435299, 0.0, 0.04351739212870598, 0.0, -0.02175869606435299, -0.02175869606435299, -0.02175869606435299, -0.04351739212870598, 0.0]
[2025-05-15 08:30:38,788]: Mean: -0.00008618
[2025-05-15 08:30:38,788]: Min: -0.17406957
[2025-05-15 08:30:38,789]: Max: 0.15231088
[2025-05-15 08:30:38,789]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-15 08:30:38,789]: Sample Values (25 elements): [0.9521978497505188, 0.9284507036209106, 1.041473627090454, 0.9767956137657166, 1.015784502029419, 0.9546640515327454, 0.9359525442123413, 0.9678131341934204, 0.9354920387268066, 0.9454050660133362, 0.9694266319274902, 0.9483951330184937, 0.9672209620475769, 0.9479801058769226, 0.9718451499938965, 0.9801055192947388, 1.1163116693496704, 0.9634753465652466, 0.9722868800163269, 0.9518718123435974, 0.9321820735931396, 1.0144075155258179, 0.9785166382789612, 0.9728946089744568, 0.9862963557243347]
[2025-05-15 08:30:38,789]: Mean: 0.96411151
[2025-05-15 08:30:38,789]: Min: 0.89097828
[2025-05-15 08:30:38,789]: Max: 1.11631167
[2025-05-15 08:30:38,790]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 08:30:38,791]: Sample Values (25 elements): [-0.058260366320610046, -0.038840245455503464, -0.038840245455503464, -0.038840245455503464, 0.019420122727751732, -0.019420122727751732, -0.038840245455503464, 0.019420122727751732, 0.019420122727751732, -0.019420122727751732, 0.07768049091100693, 0.0, -0.019420122727751732, 0.0, -0.019420122727751732, -0.019420122727751732, -0.058260366320610046, -0.038840245455503464, -0.019420122727751732, -0.038840245455503464, 0.0, 0.019420122727751732, -0.019420122727751732, 0.0, 0.0]
[2025-05-15 08:30:38,791]: Mean: -0.00012327
[2025-05-15 08:30:38,791]: Min: -0.15536098
[2025-05-15 08:30:38,792]: Max: 0.13594086
[2025-05-15 08:30:38,792]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-15 08:30:38,792]: Sample Values (25 elements): [1.0961860418319702, 0.9555912017822266, 0.947571337223053, 0.9134182333946228, 0.957845151424408, 1.066819429397583, 0.9674415588378906, 0.8915311694145203, 0.8876358270645142, 0.8984798789024353, 0.9119471311569214, 0.9726778864860535, 0.9320864081382751, 0.9199386835098267, 0.9193726778030396, 0.9942491054534912, 0.9366995096206665, 0.9238040447235107, 0.9547799229621887, 1.066649317741394, 1.1412309408187866, 0.9400657415390015, 0.9062275886535645, 0.9320522546768188, 0.9242973327636719]
[2025-05-15 08:30:38,792]: Mean: 0.95658040
[2025-05-15 08:30:38,792]: Min: 0.88763583
[2025-05-15 08:30:38,793]: Max: 1.14123094
[2025-05-15 08:30:38,794]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 08:30:38,794]: Sample Values (25 elements): [0.015886934474110603, -0.015886934474110603, 0.03177386894822121, 0.015886934474110603, -0.04766080528497696, 0.0, 0.0, -0.03177386894822121, 0.0, 0.0, -0.015886934474110603, -0.03177386894822121, -0.03177386894822121, 0.0, 0.015886934474110603, -0.015886934474110603, 0.04766080528497696, 0.015886934474110603, 0.0, 0.0, 0.0, 0.03177386894822121, 0.03177386894822121, -0.03177386894822121, 0.0]
[2025-05-15 08:30:38,795]: Mean: 0.00032236
[2025-05-15 08:30:38,795]: Min: -0.11120854
[2025-05-15 08:30:38,795]: Max: 0.12709548
[2025-05-15 08:30:38,795]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-15 08:30:38,795]: Sample Values (25 elements): [0.9579839110374451, 0.9503183960914612, 0.9617988467216492, 0.9566342830657959, 0.9597974419593811, 0.9739108085632324, 0.9437573552131653, 0.959192156791687, 0.9838873744010925, 0.9780169725418091, 0.9635210633277893, 0.9704188108444214, 0.9736235737800598, 0.9460909962654114, 0.9800366759300232, 0.9458749294281006, 0.9818867444992065, 0.9581835269927979, 0.9614353775978088, 0.9806651473045349, 0.9412305355072021, 0.9992874264717102, 0.976569652557373, 0.9960533976554871, 0.9451451897621155]
[2025-05-15 08:30:38,795]: Mean: 0.96505356
[2025-05-15 08:30:38,795]: Min: 0.93993634
[2025-05-15 08:30:38,796]: Max: 1.08060408
[2025-05-15 08:30:38,797]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 08:30:38,797]: Sample Values (25 elements): [-0.03826136887073517, -0.025507578626275063, -0.012753789313137531, -0.025507578626275063, 0.012753789313137531, -0.051015157252550125, 0.0, 0.0, 0.0, -0.03826136887073517, 0.012753789313137531, -0.03826136887073517, 0.025507578626275063, 0.012753789313137531, -0.03826136887073517, 0.025507578626275063, -0.012753789313137531, 0.012753789313137531, -0.012753789313137531, -0.012753789313137531, -0.012753789313137531, 0.012753789313137531, -0.012753789313137531, 0.025507578626275063, 0.012753789313137531]
[2025-05-15 08:30:38,797]: Mean: -0.00014565
[2025-05-15 08:30:38,798]: Min: -0.10203031
[2025-05-15 08:30:38,798]: Max: 0.08927652
[2025-05-15 08:30:38,798]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-15 08:30:38,798]: Sample Values (25 elements): [0.9263787865638733, 0.9774487018585205, 0.9576719999313354, 0.9282322525978088, 0.9449032545089722, 0.9811099171638489, 1.008444905281067, 0.9878414869308472, 0.9641644954681396, 0.9441995024681091, 0.954472005367279, 0.9530671834945679, 0.9972768425941467, 0.972202479839325, 0.9343762397766113, 0.9350060224533081, 0.9255474209785461, 0.9383377432823181, 0.9614288806915283, 0.9421560168266296, 0.9737842082977295, 0.9786674380302429, 0.9270676374435425, 0.9605766534805298, 0.9001836180686951]
[2025-05-15 08:30:38,798]: Mean: 0.95942527
[2025-05-15 08:30:38,798]: Min: 0.90018362
[2025-05-15 08:30:38,798]: Max: 1.00844491
[2025-05-15 08:30:38,800]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-15 08:30:38,801]: Sample Values (25 elements): [-0.03028460405766964, -0.020189736038446426, -0.020189736038446426, -0.010094868019223213, -0.010094868019223213, 0.03028460405766964, 0.03028460405766964, 0.03028460405766964, -0.020189736038446426, 0.010094868019223213, 0.0, 0.020189736038446426, -0.03028460405766964, -0.020189736038446426, 0.06056920811533928, -0.03028460405766964, 0.0, -0.03028460405766964, 0.03028460405766964, -0.010094868019223213, 0.0, -0.020189736038446426, -0.020189736038446426, 0.010094868019223213, 0.010094868019223213]
[2025-05-15 08:30:38,801]: Mean: 0.00000192
[2025-05-15 08:30:38,801]: Min: -0.08075894
[2025-05-15 08:30:38,801]: Max: 0.07066408
[2025-05-15 08:30:38,801]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-15 08:30:38,801]: Sample Values (25 elements): [0.9612749814987183, 0.9524543881416321, 0.9629183411598206, 0.9579122066497803, 0.9580808877944946, 0.9641619324684143, 0.9619516134262085, 0.9664635062217712, 0.9611344933509827, 0.9632413983345032, 0.9659735560417175, 0.9561903476715088, 0.9673404097557068, 0.9528419971466064, 0.9620688557624817, 0.9615248441696167, 0.9585015177726746, 0.9601784944534302, 0.9730100631713867, 0.9571118950843811, 0.950584888458252, 0.9632850289344788, 0.9575128555297852, 0.956899106502533, 0.9794471263885498]
[2025-05-15 08:30:38,801]: Mean: 0.96090865
[2025-05-15 08:30:38,802]: Min: 0.94558489
[2025-05-15 08:30:38,802]: Max: 0.98262984
[2025-05-15 08:30:38,803]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 08:30:38,804]: Sample Values (25 elements): [0.025858338922262192, -0.01723889261484146, -0.025858338922262192, 0.0, -0.025858338922262192, 0.0, -0.00861944630742073, 0.00861944630742073, -0.01723889261484146, 0.0, -0.03447778522968292, 0.03447778522968292, 0.025858338922262192, -0.00861944630742073, 0.0, -0.01723889261484146, -0.01723889261484146, -0.01723889261484146, 0.025858338922262192, -0.00861944630742073, 0.00861944630742073, -0.01723889261484146, -0.025858338922262192, -0.00861944630742073, 0.00861944630742073]
[2025-05-15 08:30:38,804]: Mean: 0.00006161
[2025-05-15 08:30:38,805]: Min: -0.06895557
[2025-05-15 08:30:38,805]: Max: 0.06033612
[2025-05-15 08:30:38,805]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-15 08:30:38,805]: Sample Values (25 elements): [0.9660532474517822, 0.9627381563186646, 0.9675353765487671, 0.9676976203918457, 0.957850992679596, 0.9559469223022461, 0.9628537893295288, 0.9624035358428955, 0.9596517086029053, 0.9617533087730408, 0.9655206799507141, 0.9637945890426636, 0.96979159116745, 0.950553834438324, 0.9714984893798828, 0.9652301669120789, 0.9636775851249695, 0.9612137675285339, 0.963274359703064, 0.9610934853553772, 0.9562774300575256, 0.9574041962623596, 0.9380821585655212, 0.9723160862922668, 0.965358555316925]
[2025-05-15 08:30:38,805]: Mean: 0.96289295
[2025-05-15 08:30:38,805]: Min: 0.92898667
[2025-05-15 08:30:38,806]: Max: 0.98841172
[2025-05-15 08:30:38,807]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-15 08:30:38,807]: Sample Values (25 elements): [-0.10941711068153381, 0.10941711068153381, 0.043766845017671585, 0.10941711068153381, 0.0, -0.13130053877830505, 0.0, -0.10941711068153381, -0.13130053877830505, -0.021883422508835793, 0.08753369003534317, -0.06565026938915253, 0.0, 0.043766845017671585, -0.043766845017671585, -0.10941711068153381, -0.08753369003534317, -0.043766845017671585, 0.0, -0.043766845017671585, -0.043766845017671585, -0.021883422508835793, 0.021883422508835793, -0.043766845017671585, 0.021883422508835793]
[2025-05-15 08:30:38,807]: Mean: -0.00099106
[2025-05-15 08:30:38,807]: Min: -0.17506738
[2025-05-15 08:30:38,808]: Max: 0.15318395
[2025-05-15 08:30:38,808]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-15 08:30:38,808]: Sample Values (25 elements): [0.950251042842865, 0.9490160346031189, 0.9659256339073181, 0.9394609332084656, 0.9282810688018799, 0.9583743810653687, 0.9561593532562256, 0.9505206346511841, 0.9289913773536682, 0.9409570693969727, 0.9353104829788208, 0.9560428857803345, 0.931530237197876, 0.9384379982948303, 0.9442741274833679, 0.9285869002342224, 0.9184520244598389, 0.9446187019348145, 0.9333541989326477, 0.92962247133255, 0.9562416672706604, 0.9446815252304077, 0.9359311461448669, 0.9529237747192383, 0.9284654855728149]
[2025-05-15 08:30:38,808]: Mean: 0.94109511
[2025-05-15 08:30:38,808]: Min: 0.91377044
[2025-05-15 08:30:38,808]: Max: 0.97413599
[2025-05-15 08:30:38,809]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 08:30:38,811]: Sample Values (25 elements): [0.04188283905386925, 0.02512970194220543, 0.0, -0.03350627049803734, 0.03350627049803734, 0.008376567624509335, 0.0, 0.03350627049803734, 0.008376567624509335, 0.008376567624509335, 0.03350627049803734, 0.02512970194220543, 0.008376567624509335, 0.008376567624509335, 0.008376567624509335, 0.008376567624509335, 0.008376567624509335, 0.02512970194220543, 0.0, 0.008376567624509335, 0.0, 0.0, 0.03350627049803734, -0.008376567624509335, 0.03350627049803734]
[2025-05-15 08:30:38,811]: Mean: 0.00000608
[2025-05-15 08:30:38,811]: Min: -0.06701254
[2025-05-15 08:30:38,812]: Max: 0.05863597
[2025-05-15 08:30:38,812]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-15 08:30:38,812]: Sample Values (25 elements): [0.9552069306373596, 0.9633956551551819, 0.9658666253089905, 0.9658907055854797, 0.9637821316719055, 0.9551905393600464, 0.9719622135162354, 0.965632975101471, 0.9627094268798828, 0.9628843665122986, 0.9602096080780029, 0.9655670523643494, 0.9637971520423889, 0.9687362313270569, 0.9734458923339844, 0.9723761081695557, 0.9546490907669067, 0.9606624841690063, 0.9727904200553894, 0.9591818451881409, 0.9586515426635742, 0.9604899287223816, 0.952893078327179, 0.9609293341636658, 0.9743047952651978]
[2025-05-15 08:30:38,812]: Mean: 0.96354926
[2025-05-15 08:30:38,812]: Min: 0.95015812
[2025-05-15 08:30:38,812]: Max: 0.97680241
[2025-05-15 08:30:38,813]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 08:30:38,815]: Sample Values (25 elements): [-0.024738486856222153, -0.008246161974966526, -0.024738486856222153, -0.008246161974966526, -0.032984647899866104, 0.008246161974966526, -0.008246161974966526, 0.024738486856222153, -0.032984647899866104, 0.008246161974966526, 0.024738486856222153, 0.008246161974966526, 0.024738486856222153, -0.024738486856222153, 0.008246161974966526, -0.032984647899866104, -0.008246161974966526, -0.008246161974966526, -0.024738486856222153, -0.008246161974966526, 0.0, 0.024738486856222153, -0.008246161974966526, 0.0, -0.008246161974966526]
[2025-05-15 08:30:38,815]: Mean: -0.00007874
[2025-05-15 08:30:38,815]: Min: -0.05772313
[2025-05-15 08:30:38,816]: Max: 0.06596930
[2025-05-15 08:30:38,816]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-15 08:30:38,816]: Sample Values (25 elements): [0.9712401032447815, 0.9768999814987183, 0.9688891172409058, 0.9742680788040161, 0.9733467698097229, 0.9738907217979431, 0.9514935612678528, 0.9881062507629395, 0.9552536010742188, 0.9715474247932434, 0.9704491496086121, 0.9796995520591736, 0.9721712470054626, 0.9905310273170471, 0.9715508818626404, 0.9742997884750366, 0.9660453200340271, 0.9628432989120483, 0.9904892444610596, 0.9648208022117615, 0.9753046631813049, 0.9566532969474792, 0.9725215435028076, 0.9753084778785706, 0.955056369304657]
[2025-05-15 08:30:38,816]: Mean: 0.97088033
[2025-05-15 08:30:38,816]: Min: 0.95050478
[2025-05-15 08:30:38,816]: Max: 0.99487054
[2025-05-15 08:30:38,817]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-15 08:30:38,820]: Sample Values (25 elements): [0.021120483055710793, -0.021120483055710793, 0.0, 0.0, -0.014080322347581387, -0.014080322347581387, 0.021120483055710793, 0.014080322347581387, 0.007040161173790693, 0.0, 0.007040161173790693, 0.007040161173790693, -0.007040161173790693, 0.007040161173790693, 0.014080322347581387, -0.007040161173790693, 0.014080322347581387, 0.014080322347581387, 0.007040161173790693, -0.007040161173790693, 0.028160644695162773, -0.014080322347581387, -0.035200804471969604, -0.014080322347581387, 0.0]
[2025-05-15 08:30:38,821]: Mean: -0.00000420
[2025-05-15 08:30:38,821]: Min: -0.04928113
[2025-05-15 08:30:38,821]: Max: 0.05632129
[2025-05-15 08:30:38,821]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-15 08:30:38,821]: Sample Values (25 elements): [0.961559534072876, 0.9635793566703796, 0.9577197432518005, 0.9577649831771851, 0.953974187374115, 0.9580137729644775, 0.9629694223403931, 0.956990122795105, 0.9566036462783813, 0.9592357277870178, 0.9598509669303894, 0.9635915756225586, 0.9654255509376526, 0.9543147683143616, 0.9604601263999939, 0.9611747860908508, 0.9587193131446838, 0.9631686806678772, 0.9663400053977966, 0.9600992798805237, 0.9579846262931824, 0.9573230743408203, 0.9600321054458618, 0.9643741846084595, 0.9623231291770935]
[2025-05-15 08:30:38,822]: Mean: 0.96074134
[2025-05-15 08:30:38,822]: Min: 0.95238525
[2025-05-15 08:30:38,822]: Max: 0.97314191
[2025-05-15 08:30:38,823]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 08:30:38,829]: Sample Values (25 elements): [0.0, 0.006172815337777138, 0.0, 0.018518446013331413, -0.012345630675554276, 0.012345630675554276, 0.02469126135110855, 0.0, -0.018518446013331413, -0.012345630675554276, 0.0, -0.018518446013331413, 0.006172815337777138, -0.02469126135110855, -0.006172815337777138, 0.02469126135110855, -0.006172815337777138, 0.0, 0.012345630675554276, 0.012345630675554276, 0.018518446013331413, 0.012345630675554276, -0.006172815337777138, 0.006172815337777138, 0.012345630675554276]
[2025-05-15 08:30:38,829]: Mean: -0.00000310
[2025-05-15 08:30:38,830]: Min: -0.04938252
[2025-05-15 08:30:38,830]: Max: 0.04320971
[2025-05-15 08:30:38,830]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-15 08:30:38,830]: Sample Values (25 elements): [0.9630665183067322, 0.9584236145019531, 0.9617916345596313, 0.9650329351425171, 0.9532197713851929, 0.9647151827812195, 0.9693449139595032, 0.966010332107544, 0.9659518003463745, 0.9589303135871887, 0.9609463810920715, 0.9671751260757446, 0.9666284918785095, 0.9691815972328186, 0.96346515417099, 0.9631259441375732, 0.961973249912262, 0.9553601741790771, 0.9582793712615967, 0.9655330181121826, 0.9693863987922668, 0.9672398567199707, 0.971691906452179, 0.9729822278022766, 0.9606205224990845]
[2025-05-15 08:30:38,830]: Mean: 0.96449804
[2025-05-15 08:30:38,830]: Min: 0.95049238
[2025-05-15 08:30:38,831]: Max: 0.98166293
[2025-05-15 08:30:38,832]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-15 08:30:38,832]: Sample Values (25 elements): [0.0456397719681263, -0.0760662853717804, 0.030426515266299248, 0.060853030532598495, -0.030426515266299248, 0.0912795439362526, 0.0456397719681263, -0.0456397719681263, -0.0456397719681263, 0.060853030532598495, -0.0456397719681263, -0.0912795439362526, 0.0760662853717804, 0.060853030532598495, 0.0456397719681263, -0.0456397719681263, -0.015213257633149624, 0.0, 0.0, -0.015213257633149624, 0.0456397719681263, 0.015213257633149624, 0.0, -0.030426515266299248, -0.060853030532598495]
[2025-05-15 08:30:38,832]: Mean: -0.00011653
[2025-05-15 08:30:38,832]: Min: -0.10649280
[2025-05-15 08:30:38,833]: Max: 0.12170606
[2025-05-15 08:30:38,833]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-15 08:30:38,833]: Sample Values (25 elements): [0.9506850242614746, 0.9453443884849548, 0.9414333701133728, 0.9423441290855408, 0.9438496232032776, 0.9467517733573914, 0.9400590062141418, 0.9466733336448669, 0.9447439908981323, 0.9481470584869385, 0.9492023587226868, 0.9394956827163696, 0.9453544020652771, 0.9517906308174133, 0.948563277721405, 0.9509848952293396, 0.9367821216583252, 0.9534034132957458, 0.9434890747070312, 0.9486668109893799, 0.933281660079956, 0.9336075782775879, 0.9473675489425659, 0.945264995098114, 0.9340250492095947]
[2025-05-15 08:30:38,833]: Mean: 0.94383001
[2025-05-15 08:30:38,833]: Min: 0.92843157
[2025-05-15 08:30:38,833]: Max: 0.96001160
[2025-05-15 08:30:38,834]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 08:30:38,841]: Sample Values (25 elements): [-0.012606414966285229, 0.0, -0.018909621983766556, -0.006303207483142614, 0.0, -0.012606414966285229, 0.012606414966285229, -0.006303207483142614, 0.006303207483142614, 0.0, -0.018909621983766556, 0.006303207483142614, 0.006303207483142614, 0.025212829932570457, -0.006303207483142614, 0.018909621983766556, -0.018909621983766556, 0.0, -0.018909621983766556, -0.018909621983766556, 0.0, 0.012606414966285229, -0.006303207483142614, -0.018909621983766556, 0.018909621983766556]
[2025-05-15 08:30:38,841]: Mean: 0.00000607
[2025-05-15 08:30:38,841]: Min: -0.04412245
[2025-05-15 08:30:38,841]: Max: 0.05042566
[2025-05-15 08:30:38,841]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-15 08:30:38,841]: Sample Values (25 elements): [0.962788462638855, 0.9616866111755371, 0.9629757404327393, 0.9642075896263123, 0.9624419212341309, 0.9603226184844971, 0.9571785926818848, 0.9625972509384155, 0.9591922163963318, 0.9626833200454712, 0.9596307873725891, 0.9579198360443115, 0.9588536024093628, 0.9614764451980591, 0.964927077293396, 0.952152669429779, 0.9656968116760254, 0.9663594961166382, 0.9622654318809509, 0.9675723910331726, 0.961090624332428, 0.961940348148346, 0.9588562846183777, 0.9599694609642029, 0.9612671732902527]
[2025-05-15 08:30:38,842]: Mean: 0.96141315
[2025-05-15 08:30:38,842]: Min: 0.95215267
[2025-05-15 08:30:38,842]: Max: 0.97734952
[2025-05-15 08:30:38,843]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 08:30:38,849]: Sample Values (25 elements): [-0.012069414369761944, 0.012069414369761944, 0.01810412108898163, -0.006034707184880972, 0.006034707184880972, 0.0, -0.01810412108898163, 0.024138828739523888, -0.012069414369761944, 0.012069414369761944, -0.006034707184880972, 0.0, -0.012069414369761944, 0.012069414369761944, 0.0, -0.006034707184880972, -0.01810412108898163, -0.006034707184880972, 0.0, -0.012069414369761944, -0.012069414369761944, 0.012069414369761944, 0.0, -0.024138828739523888, 0.012069414369761944]
[2025-05-15 08:30:38,849]: Mean: -0.00002586
[2025-05-15 08:30:38,849]: Min: -0.04827766
[2025-05-15 08:30:38,850]: Max: 0.04224295
[2025-05-15 08:30:38,850]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-15 08:30:38,850]: Sample Values (25 elements): [0.9544892907142639, 0.9684968590736389, 0.9655217528343201, 0.9654296636581421, 0.9602583646774292, 0.9733917713165283, 0.9614271521568298, 0.9712809324264526, 0.9718239903450012, 0.9932747483253479, 0.9682316780090332, 0.9657000303268433, 0.9806906580924988, 0.975874662399292, 0.9701871871948242, 0.9694711565971375, 0.9661734104156494, 0.9577566385269165, 0.9609339237213135, 0.9741401076316833, 0.9823493957519531, 0.9718560576438904, 0.9641392827033997, 0.9613884091377258, 0.9600200057029724]
[2025-05-15 08:30:38,850]: Mean: 0.96903926
[2025-05-15 08:30:38,850]: Min: 0.95289910
[2025-05-15 08:30:38,850]: Max: 0.99327475
[2025-05-15 08:30:38,851]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-15 08:30:38,866]: Sample Values (25 elements): [0.016089484095573425, 0.005363161209970713, 0.02145264483988285, -0.005363161209970713, 0.0, 0.005363161209970713, 0.010726322419941425, -0.016089484095573425, -0.010726322419941425, 0.0, 0.016089484095573425, -0.010726322419941425, -0.02145264483988285, 0.016089484095573425, -0.010726322419941425, 0.02145264483988285, 0.005363161209970713, -0.02145264483988285, 0.016089484095573425, 0.0, 0.016089484095573425, -0.026815805584192276, -0.010726322419941425, -0.02145264483988285, 0.016089484095573425]
[2025-05-15 08:30:38,867]: Mean: 0.00001473
[2025-05-15 08:30:38,867]: Min: -0.03754213
[2025-05-15 08:30:38,867]: Max: 0.04290529
[2025-05-15 08:30:38,867]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-15 08:30:38,868]: Sample Values (25 elements): [0.9584289789199829, 0.9625352621078491, 0.958361804485321, 0.960269570350647, 0.9618734121322632, 0.9603824019432068, 0.9626855254173279, 0.9604067802429199, 0.9571775794029236, 0.9591168761253357, 0.9577435255050659, 0.962084174156189, 0.9606367945671082, 0.9592546224594116, 0.9594257473945618, 0.9620694518089294, 0.9601019024848938, 0.9577251672744751, 0.9586253762245178, 0.9594059586524963, 0.9595232009887695, 0.9597711563110352, 0.9614731669425964, 0.9576923251152039, 0.9580732583999634]
[2025-05-15 08:30:38,868]: Mean: 0.95973164
[2025-05-15 08:30:38,868]: Min: 0.95442998
[2025-05-15 08:30:38,868]: Max: 0.96719962
[2025-05-15 08:30:38,869]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 08:30:38,910]: Sample Values (25 elements): [-0.023808401077985764, -0.01587226800620556, -0.00793613400310278, -0.00793613400310278, -0.00793613400310278, 0.011904200538992882, 0.00793613400310278, 0.011904200538992882, 0.00396806700155139, -0.00396806700155139, 0.00396806700155139, 0.011904200538992882, 0.00396806700155139, -0.011904200538992882, 0.011904200538992882, -0.00793613400310278, 0.00793613400310278, 0.0, -0.011904200538992882, -0.00793613400310278, 0.00396806700155139, 0.00396806700155139, -0.00793613400310278, -0.01587226800620556, 0.00793613400310278]
[2025-05-15 08:30:38,911]: Mean: 0.00000736
[2025-05-15 08:30:38,911]: Min: -0.03174454
[2025-05-15 08:30:38,911]: Max: 0.02777647
[2025-05-15 08:30:38,911]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-15 08:30:38,913]: Sample Values (25 elements): [0.9628297686576843, 0.9641923308372498, 0.9621085524559021, 0.962090015411377, 0.9621481895446777, 0.9631279706954956, 0.9666928648948669, 0.9624873995780945, 0.9654595255851746, 0.9635055661201477, 0.9664441347122192, 0.9619939923286438, 0.9660322666168213, 0.9705932140350342, 0.9682098627090454, 0.9608873724937439, 0.963237464427948, 0.9649301767349243, 0.9623756408691406, 0.964653730392456, 0.9657154679298401, 0.9675065875053406, 0.9633880853652954, 0.9623078107833862, 0.962264358997345]
[2025-05-15 08:30:38,913]: Mean: 0.96440649
[2025-05-15 08:30:38,913]: Min: 0.95592505
[2025-05-15 08:30:38,913]: Max: 0.97671872
[2025-05-15 08:30:38,914]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-15 08:30:38,916]: Sample Values (25 elements): [-0.05216846242547035, -0.03130107745528221, -0.03130107745528221, -0.04173476994037628, -0.01043369248509407, 0.06260215491056442, -0.04173476994037628, -0.01043369248509407, -0.04173476994037628, 0.02086738497018814, -0.04173476994037628, -0.04173476994037628, -0.07303585112094879, -0.04173476994037628, 0.01043369248509407, 0.01043369248509407, 0.04173476994037628, 0.02086738497018814, -0.01043369248509407, 0.0, 0.02086738497018814, -0.04173476994037628, 0.01043369248509407, -0.04173476994037628, 0.05216846242547035]
[2025-05-15 08:30:38,916]: Mean: 0.00003606
[2025-05-15 08:30:38,916]: Min: -0.07303585
[2025-05-15 08:30:38,916]: Max: 0.08346954
[2025-05-15 08:30:38,916]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-15 08:30:38,917]: Sample Values (25 elements): [0.9486873149871826, 0.9529232978820801, 0.9524269104003906, 0.9555456042289734, 0.950271487236023, 0.9506022334098816, 0.9538173079490662, 0.9555830955505371, 0.9541755318641663, 0.9487951993942261, 0.9501139521598816, 0.9523325562477112, 0.9529210925102234, 0.9531739354133606, 0.9514391422271729, 0.9486836194992065, 0.953223466873169, 0.9517173767089844, 0.9533658623695374, 0.9529045224189758, 0.9541730880737305, 0.9551343321800232, 0.9528709053993225, 0.9546318650245667, 0.9538269639015198]
[2025-05-15 08:30:38,917]: Mean: 0.95265317
[2025-05-15 08:30:38,917]: Min: 0.94193566
[2025-05-15 08:30:38,917]: Max: 0.95900619
[2025-05-15 08:30:38,918]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 08:30:38,959]: Sample Values (25 elements): [0.003988673910498619, -0.003988673910498619, 0.011966021731495857, 0.003988673910498619, -0.007977347820997238, 0.003988673910498619, -0.011966021731495857, -0.003988673910498619, 0.011966021731495857, -0.007977347820997238, -0.003988673910498619, 0.015954695641994476, -0.007977347820997238, 0.0, -0.015954695641994476, -0.003988673910498619, -0.007977347820997238, 0.0, -0.003988673910498619, 0.007977347820997238, -0.003988673910498619, -0.011966021731495857, -0.007977347820997238, -0.003988673910498619, 0.0]
[2025-05-15 08:30:38,960]: Mean: -0.00001030
[2025-05-15 08:30:38,960]: Min: -0.02792072
[2025-05-15 08:30:38,960]: Max: 0.03190939
[2025-05-15 08:30:38,960]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-15 08:30:38,961]: Sample Values (25 elements): [0.9596367478370667, 0.9576539397239685, 0.9632610082626343, 0.9602987170219421, 0.9599700570106506, 0.9595115184783936, 0.9598502516746521, 0.959578812122345, 0.9584140181541443, 0.9605350494384766, 0.9614300727844238, 0.9586220383644104, 0.961065948009491, 0.9615261554718018, 0.9641019701957703, 0.9640142917633057, 0.9592651724815369, 0.9612000584602356, 0.9581671357154846, 0.9586561918258667, 0.9602281451225281, 0.960649311542511, 0.9587221145629883, 0.9584057331085205, 0.9583944082260132]
[2025-05-15 08:30:38,961]: Mean: 0.96031570
[2025-05-15 08:30:38,961]: Min: 0.95631301
[2025-05-15 08:30:38,961]: Max: 0.96787572
[2025-05-15 08:30:38,962]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 08:30:38,997]: Sample Values (25 elements): [-0.0031982935033738613, 0.009594880044460297, 0.0031982935033738613, 0.006396587006747723, 0.009594880044460297, 0.0031982935033738613, 0.006396587006747723, 0.0, -0.006396587006747723, 0.009594880044460297, 0.009594880044460297, 0.0031982935033738613, 0.0, 0.009594880044460297, 0.0031982935033738613, -0.006396587006747723, 0.0, -0.0031982935033738613, 0.0031982935033738613, 0.012793174013495445, 0.006396587006747723, 0.012793174013495445, -0.0031982935033738613, -0.012793174013495445, 0.006396587006747723]
[2025-05-15 08:30:38,998]: Mean: 0.00000714
[2025-05-15 08:30:38,998]: Min: -0.02558635
[2025-05-15 08:30:38,998]: Max: 0.02238805
[2025-05-15 08:30:38,998]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-15 08:30:38,999]: Sample Values (25 elements): [0.96978360414505, 0.9677157998085022, 0.9655986428260803, 0.968079149723053, 0.9723193049430847, 0.9682947993278503, 0.9685804843902588, 0.966486930847168, 0.9672392010688782, 0.9680604338645935, 0.9732546210289001, 0.9686586260795593, 0.9737944602966309, 0.9728445410728455, 0.9661490321159363, 0.9693114161491394, 0.9676946401596069, 0.9670608043670654, 0.9744771718978882, 0.9710052013397217, 0.9660645127296448, 0.9660977721214294, 0.9678994417190552, 0.9640394449234009, 0.9669412970542908]
[2025-05-15 08:30:38,999]: Mean: 0.96799237
[2025-05-15 08:30:38,999]: Min: 0.96161431
[2025-05-15 08:30:38,999]: Max: 0.97712779
[2025-05-15 08:30:38,999]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-15 08:30:39,000]: Sample Values (25 elements): [0.04819187894463539, 0.07779791206121445, -0.002099912613630295, -0.057996027171611786, -0.0084177665412426, 0.02340615540742874, -0.030213752761483192, -0.06270154565572739, 0.04497991502285004, -0.11633063107728958, 0.03685905411839485, 0.050692636519670486, 0.01061238069087267, 0.09354154765605927, 0.0076988739892840385, 0.07353857159614563, 0.007832222618162632, 0.06897320598363876, 0.07373714447021484, 0.09226395189762115, -0.003149597905576229, 0.025265734642744064, -0.033865150064229965, 0.025499435141682625, 0.008529271930456161]
[2025-05-15 08:30:39,000]: Mean: 0.00078807
[2025-05-15 08:30:39,000]: Min: -0.13701494
[2025-05-15 08:30:39,000]: Max: 0.13851675

[2025-05-15 20:37:28,836]: 


QAT of ResNet18 with parametrized_hardtanh down to 3 bits...
[2025-05-15 20:37:29,290]: [ResNet18_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-15 20:37:29,774]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-15 20:39:25,350]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.3364 Train Acc: 0.8810 Eval Loss: 0.6421 Eval Acc: 0.8170 (LR: 0.001000)
[2025-05-15 20:41:22,579]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.2921 Train Acc: 0.8952 Eval Loss: 0.5849 Eval Acc: 0.8273 (LR: 0.001000)
[2025-05-15 20:43:16,273]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.2918 Train Acc: 0.8955 Eval Loss: 0.5636 Eval Acc: 0.8388 (LR: 0.001000)
[2025-05-15 20:45:05,252]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.2784 Train Acc: 0.9001 Eval Loss: 0.5563 Eval Acc: 0.8361 (LR: 0.001000)
[2025-05-15 20:46:55,478]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.2773 Train Acc: 0.9002 Eval Loss: 0.5502 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-15 20:48:46,856]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.2793 Train Acc: 0.9003 Eval Loss: 0.6446 Eval Acc: 0.8104 (LR: 0.001000)
[2025-05-15 20:50:36,912]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.2734 Train Acc: 0.9021 Eval Loss: 0.5708 Eval Acc: 0.8296 (LR: 0.001000)
[2025-05-15 20:52:25,445]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.2688 Train Acc: 0.9047 Eval Loss: 1.3938 Eval Acc: 0.6656 (LR: 0.001000)
[2025-05-15 20:54:14,239]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.2635 Train Acc: 0.9046 Eval Loss: 0.5323 Eval Acc: 0.8415 (LR: 0.001000)
[2025-05-15 20:56:03,120]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.2613 Train Acc: 0.9068 Eval Loss: 0.5399 Eval Acc: 0.8389 (LR: 0.001000)
[2025-05-15 20:57:56,090]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.2597 Train Acc: 0.9073 Eval Loss: 0.7483 Eval Acc: 0.7955 (LR: 0.001000)
[2025-05-15 20:59:50,306]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.2524 Train Acc: 0.9088 Eval Loss: 0.5759 Eval Acc: 0.8303 (LR: 0.001000)
[2025-05-15 21:01:44,407]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.2473 Train Acc: 0.9112 Eval Loss: 0.5339 Eval Acc: 0.8431 (LR: 0.001000)
[2025-05-15 21:03:42,425]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.2511 Train Acc: 0.9098 Eval Loss: 1.0571 Eval Acc: 0.7461 (LR: 0.001000)
[2025-05-15 21:05:57,207]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.2416 Train Acc: 0.9147 Eval Loss: 0.8123 Eval Acc: 0.7884 (LR: 0.001000)
[2025-05-15 21:08:30,796]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.2365 Train Acc: 0.9161 Eval Loss: 0.6456 Eval Acc: 0.8171 (LR: 0.001000)
[2025-05-15 21:10:19,711]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.2371 Train Acc: 0.9141 Eval Loss: 0.6116 Eval Acc: 0.8297 (LR: 0.001000)
[2025-05-15 21:12:08,429]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.2341 Train Acc: 0.9156 Eval Loss: 0.6887 Eval Acc: 0.8101 (LR: 0.001000)
[2025-05-15 21:13:57,131]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.2357 Train Acc: 0.9155 Eval Loss: 0.6869 Eval Acc: 0.8150 (LR: 0.001000)
[2025-05-15 21:15:47,237]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.2300 Train Acc: 0.9171 Eval Loss: 0.5427 Eval Acc: 0.8437 (LR: 0.001000)
[2025-05-15 21:17:35,853]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.2238 Train Acc: 0.9198 Eval Loss: 0.5963 Eval Acc: 0.8311 (LR: 0.001000)
[2025-05-15 21:19:24,378]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.2260 Train Acc: 0.9178 Eval Loss: 0.7884 Eval Acc: 0.7918 (LR: 0.001000)
[2025-05-15 21:21:15,405]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.2209 Train Acc: 0.9197 Eval Loss: 0.6589 Eval Acc: 0.8202 (LR: 0.001000)
[2025-05-15 21:23:28,929]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.2193 Train Acc: 0.9222 Eval Loss: 0.6846 Eval Acc: 0.8070 (LR: 0.001000)
[2025-05-15 21:25:50,096]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.2177 Train Acc: 0.9217 Eval Loss: 1.2800 Eval Acc: 0.7000 (LR: 0.001000)
[2025-05-15 21:27:53,584]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.2218 Train Acc: 0.9204 Eval Loss: 0.8099 Eval Acc: 0.7964 (LR: 0.001000)
[2025-05-15 21:29:53,864]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.2294 Train Acc: 0.9171 Eval Loss: 0.7546 Eval Acc: 0.8079 (LR: 0.001000)
[2025-05-15 21:31:56,723]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.2183 Train Acc: 0.9228 Eval Loss: 0.8006 Eval Acc: 0.7977 (LR: 0.001000)
[2025-05-15 21:33:52,443]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.2196 Train Acc: 0.9202 Eval Loss: 0.9604 Eval Acc: 0.7424 (LR: 0.001000)
[2025-05-15 21:35:48,478]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.2186 Train Acc: 0.9221 Eval Loss: 0.5403 Eval Acc: 0.8480 (LR: 0.000100)
[2025-05-15 21:37:41,795]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.1432 Train Acc: 0.9517 Eval Loss: 0.6753 Eval Acc: 0.8106 (LR: 0.000100)
[2025-05-15 21:39:37,228]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.1266 Train Acc: 0.9563 Eval Loss: 0.5092 Eval Acc: 0.8601 (LR: 0.000100)
[2025-05-15 21:41:34,820]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.1230 Train Acc: 0.9576 Eval Loss: 0.4691 Eval Acc: 0.8672 (LR: 0.000100)
[2025-05-15 21:43:26,222]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.1136 Train Acc: 0.9610 Eval Loss: 0.4725 Eval Acc: 0.8647 (LR: 0.000100)
[2025-05-15 21:45:22,599]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.1111 Train Acc: 0.9620 Eval Loss: 0.4762 Eval Acc: 0.8633 (LR: 0.000100)
[2025-05-15 21:47:17,406]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.1103 Train Acc: 0.9630 Eval Loss: 0.6774 Eval Acc: 0.8219 (LR: 0.000100)
[2025-05-15 21:49:12,296]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.1089 Train Acc: 0.9634 Eval Loss: 0.5683 Eval Acc: 0.8489 (LR: 0.000100)
[2025-05-15 21:51:01,740]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.1076 Train Acc: 0.9637 Eval Loss: 0.4789 Eval Acc: 0.8682 (LR: 0.000100)
[2025-05-15 21:52:50,683]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.1060 Train Acc: 0.9639 Eval Loss: 0.4598 Eval Acc: 0.8743 (LR: 0.000100)
[2025-05-15 21:54:40,917]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.1065 Train Acc: 0.9633 Eval Loss: 0.5740 Eval Acc: 0.8508 (LR: 0.000100)
[2025-05-15 21:56:32,502]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.1009 Train Acc: 0.9657 Eval Loss: 0.5759 Eval Acc: 0.8470 (LR: 0.000100)
[2025-05-15 21:58:43,473]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.1026 Train Acc: 0.9647 Eval Loss: 0.6384 Eval Acc: 0.8341 (LR: 0.000100)
[2025-05-15 22:00:34,474]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.0995 Train Acc: 0.9662 Eval Loss: 0.5571 Eval Acc: 0.8581 (LR: 0.000100)
[2025-05-15 22:02:25,106]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.0992 Train Acc: 0.9669 Eval Loss: 0.8552 Eval Acc: 0.8055 (LR: 0.000100)
[2025-05-15 22:04:21,367]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.0958 Train Acc: 0.9666 Eval Loss: 0.8413 Eval Acc: 0.8073 (LR: 0.000010)
[2025-05-15 22:06:18,121]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.0908 Train Acc: 0.9696 Eval Loss: 0.8073 Eval Acc: 0.8125 (LR: 0.000010)
[2025-05-15 22:08:32,214]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.0894 Train Acc: 0.9698 Eval Loss: 0.5044 Eval Acc: 0.8624 (LR: 0.000010)
[2025-05-15 22:10:40,050]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.0888 Train Acc: 0.9702 Eval Loss: 0.8183 Eval Acc: 0.8004 (LR: 0.000010)
[2025-05-15 22:12:41,333]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.0908 Train Acc: 0.9691 Eval Loss: 0.5581 Eval Acc: 0.8544 (LR: 0.000010)
[2025-05-15 22:14:48,397]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.0881 Train Acc: 0.9696 Eval Loss: 0.5575 Eval Acc: 0.8594 (LR: 0.000010)
[2025-05-15 22:16:59,058]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.0902 Train Acc: 0.9691 Eval Loss: 0.6408 Eval Acc: 0.8375 (LR: 0.000010)
[2025-05-15 22:19:11,603]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.0875 Train Acc: 0.9705 Eval Loss: 0.6971 Eval Acc: 0.8234 (LR: 0.000010)
[2025-05-15 22:21:10,051]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.0882 Train Acc: 0.9704 Eval Loss: 0.6842 Eval Acc: 0.8240 (LR: 0.000010)
[2025-05-15 22:23:11,893]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.0862 Train Acc: 0.9709 Eval Loss: 0.5007 Eval Acc: 0.8682 (LR: 0.000010)
[2025-05-15 22:25:12,636]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.0895 Train Acc: 0.9695 Eval Loss: 0.6073 Eval Acc: 0.8442 (LR: 0.000010)
[2025-05-15 22:27:20,654]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.0874 Train Acc: 0.9700 Eval Loss: 0.5165 Eval Acc: 0.8678 (LR: 0.000010)
[2025-05-15 22:29:30,828]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.0876 Train Acc: 0.9699 Eval Loss: 0.5604 Eval Acc: 0.8571 (LR: 0.000010)
[2025-05-15 22:31:30,038]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.0869 Train Acc: 0.9711 Eval Loss: 0.6127 Eval Acc: 0.8417 (LR: 0.000010)
[2025-05-15 22:33:29,133]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.0880 Train Acc: 0.9702 Eval Loss: 0.6356 Eval Acc: 0.8380 (LR: 0.000010)
[2025-05-15 22:35:31,577]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.0883 Train Acc: 0.9701 Eval Loss: 0.7257 Eval Acc: 0.8189 (LR: 0.000010)
[2025-05-15 22:35:31,578]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8743
[2025-05-15 22:35:31,701]: 


Quantization of model down to 3 bits finished
[2025-05-15 22:35:31,701]: Model Architecture:
[2025-05-15 22:35:31,763]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2173], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.412546157836914, max_val=0.1086595430970192)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0484], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16918163001537323, max_val=0.16945523023605347)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1994], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4985475242137909, max_val=0.8973839282989502)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0455], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1723073571920395, max_val=0.14623194932937622)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2231], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33467069268226624, max_val=1.2271078824996948)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0358], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11586643010377884, max_val=0.13504153490066528)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1960], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6859922409057617, max_val=0.685992419719696)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0283], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09841439872980118, max_val=0.09961870312690735)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3074], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.07028329372406, max_val=1.0811694860458374)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0225], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07872239500284195, max_val=0.07880738377571106)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2288], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0296437740325928, max_val=0.5720263123512268)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06804928183555603, max_val=0.06765168905258179)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0475], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16632966697216034, max_val=0.16628409922122955)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2360], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.062108039855957, max_val=0.5900629758834839)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06872983276844025, max_val=0.06295900046825409)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1693], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7619397640228271, max_val=0.4233049154281616)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0184], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06205853447318077, max_val=0.06677808612585068)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3331], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.165834903717041, max_val=1.1658380031585693)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05368480831384659, max_val=0.0553341805934906)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1884], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8477133512496948, max_val=0.470956414937973)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0132], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0463065467774868, max_val=0.04619377851486206)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0326], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11274023354053497, max_val=0.11527971923351288)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2200], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9899552464485168, max_val=0.5499768853187561)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0137], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04774020239710808, max_val=0.04790019616484642)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1573], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7076765298843384, max_val=0.3931576907634735)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04453105479478836, max_val=0.04319090023636818)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2572], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6430095434188843, max_val=1.1574070453643799)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03914799913764, max_val=0.040388017892837524)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2008], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7028728127479553, max_val=0.7028735876083374)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02979060634970665, max_val=0.03069855459034443)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0222], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07677978277206421, max_val=0.07829050719738007)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2563], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6408034563064575, max_val=1.1534488201141357)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.029104432091116905, max_val=0.0310440044850111)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1238], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43325915932655334, max_val=0.43325740098953247)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0068], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02536602132022381, max_val=0.022431112825870514)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4858], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4710814952850342, max_val=1.9295001029968262)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-15 22:35:31,763]: 
Model Weights:
[2025-05-15 22:35:31,763]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-15 22:35:31,768]: Sample Values (25 elements): [0.1509237140417099, 0.13286888599395752, -0.004985408391803503, -0.0777878388762474, -0.10530608147382736, -0.05267706885933876, -0.18902328610420227, 0.22687965631484985, 0.26711663603782654, -0.10032506287097931, -0.013589714653789997, 0.25105684995651245, -0.19367913901805878, 0.20460116863250732, -0.2613641321659088, -0.06642134487628937, -0.04332674294710159, 0.0799734815955162, 0.059090349823236465, -0.10516246408224106, 0.1509081870317459, -0.10368123650550842, -0.22473379969596863, 0.04370761662721634, 0.24035203456878662]
[2025-05-15 22:35:31,780]: Mean: 0.00122654
[2025-05-15 22:35:31,781]: Min: -0.48704427
[2025-05-15 22:35:31,782]: Max: 0.45561287
[2025-05-15 22:35:31,782]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-15 22:35:31,782]: Sample Values (25 elements): [0.9637064933776855, 1.0647215843200684, 0.7500532269477844, 0.9523999691009521, 0.8443076014518738, 1.116819977760315, 1.0753859281539917, 0.82582026720047, 0.9925253391265869, 0.9038653373718262, 0.9820478558540344, 0.8323675394058228, 0.8053010106086731, 0.8932287693023682, 0.869831383228302, 1.294328212738037, 1.0671876668930054, 1.0063806772232056, 1.1277720928192139, 1.0191541910171509, 0.8730828762054443, 1.203309178352356, 0.8874086141586304, 0.9676148891448975, 0.9519648551940918]
[2025-05-15 22:35:31,782]: Mean: 0.96014750
[2025-05-15 22:35:31,783]: Min: 0.75005323
[2025-05-15 22:35:31,783]: Max: 1.29432821
[2025-05-15 22:35:31,784]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 22:35:31,784]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.048376694321632385, -0.048376694321632385, 0.0, 0.048376694321632385, 0.0, 0.048376694321632385, 0.0, 0.0, 0.0, 0.048376694321632385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.048376694321632385, 0.048376694321632385, -0.048376694321632385, 0.048376694321632385]
[2025-05-15 22:35:31,785]: Mean: 0.00001181
[2025-05-15 22:35:31,785]: Min: -0.14513008
[2025-05-15 22:35:31,785]: Max: 0.19350678
[2025-05-15 22:35:31,785]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-15 22:35:31,785]: Sample Values (25 elements): [0.9713940024375916, 0.9980076551437378, 0.9696095585823059, 0.9587269425392151, 0.9616049528121948, 0.9516018033027649, 0.9549926519393921, 0.9965712428092957, 0.9689822196960449, 0.9479022026062012, 0.9659855365753174, 0.9173325896263123, 0.9338110685348511, 0.9820109009742737, 1.0088845491409302, 0.9688301086425781, 0.9569052457809448, 0.9583783149719238, 0.9123091697692871, 0.9384379982948303, 1.0545222759246826, 0.9353919625282288, 0.9702185988426208, 0.960961639881134, 0.9386659264564514]
[2025-05-15 22:35:31,785]: Mean: 0.96317768
[2025-05-15 22:35:31,786]: Min: 0.88948643
[2025-05-15 22:35:31,786]: Max: 1.12258756
[2025-05-15 22:35:31,787]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 22:35:31,787]: Sample Values (25 elements): [0.04550560936331749, 0.0, 0.04550560936331749, -0.04550560936331749, -0.04550560936331749, 0.04550560936331749, -0.04550560936331749, -0.04550560936331749, 0.0, -0.04550560936331749, -0.04550560936331749, -0.04550560936331749, 0.0, -0.04550560936331749, -0.04550560936331749, 0.0, 0.0, 0.04550560936331749, 0.0, 0.04550560936331749, 0.0, 0.04550560936331749, 0.04550560936331749, 0.0, -0.04550560936331749]
[2025-05-15 22:35:31,788]: Mean: -0.00026170
[2025-05-15 22:35:31,788]: Min: -0.18202244
[2025-05-15 22:35:31,788]: Max: 0.13651682
[2025-05-15 22:35:31,788]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-15 22:35:31,788]: Sample Values (25 elements): [0.9253134727478027, 1.06987726688385, 0.9968061447143555, 0.9313790798187256, 0.9752098917961121, 0.9473691582679749, 0.942226231098175, 0.9797196984291077, 0.9314049482345581, 0.9655571579933167, 0.9596257209777832, 0.9601820707321167, 0.935843288898468, 0.9077127575874329, 0.9458637237548828, 0.923206090927124, 0.9626107215881348, 0.9344438910484314, 1.1618704795837402, 0.9515892863273621, 0.9171519875526428, 1.1626956462860107, 0.956015944480896, 0.9567751288414001, 1.0839817523956299]
[2025-05-15 22:35:31,789]: Mean: 0.96036834
[2025-05-15 22:35:31,789]: Min: 0.89780825
[2025-05-15 22:35:31,789]: Max: 1.16269565
[2025-05-15 22:35:31,790]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 22:35:31,791]: Sample Values (25 elements): [-0.035843994468450546, -0.035843994468450546, 0.035843994468450546, 0.035843994468450546, -0.035843994468450546, 0.035843994468450546, -0.07168798893690109, 0.0, 0.0, -0.035843994468450546, 0.0, -0.035843994468450546, -0.07168798893690109, -0.07168798893690109, 0.0, 0.07168798893690109, -0.035843994468450546, 0.0, 0.07168798893690109, 0.0, 0.035843994468450546, 0.035843994468450546, 0.0, 0.0, -0.035843994468450546]
[2025-05-15 22:35:31,791]: Mean: 0.00041810
[2025-05-15 22:35:31,791]: Min: -0.10753198
[2025-05-15 22:35:31,791]: Max: 0.14337598
[2025-05-15 22:35:31,791]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-15 22:35:31,791]: Sample Values (25 elements): [0.9518362283706665, 0.9523324370384216, 0.9914271831512451, 0.9765514135360718, 0.9516259431838989, 1.0997049808502197, 0.9477807283401489, 0.9538911581039429, 0.9540120959281921, 1.0218935012817383, 0.9584501385688782, 1.049822449684143, 0.9618889689445496, 0.9576729536056519, 0.9483729004859924, 0.9742183685302734, 0.9409770369529724, 0.975922703742981, 0.9542310237884521, 0.9585554003715515, 0.9582269191741943, 0.9458455443382263, 0.9836947321891785, 0.9511194229125977, 0.9743964076042175]
[2025-05-15 22:35:31,791]: Mean: 0.96649152
[2025-05-15 22:35:31,792]: Min: 0.93964309
[2025-05-15 22:35:31,792]: Max: 1.09970498
[2025-05-15 22:35:31,793]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-15 22:35:31,793]: Sample Values (25 elements): [-0.02829044684767723, 0.0, 0.02829044684767723, 0.0, -0.02829044684767723, 0.02829044684767723, -0.02829044684767723, -0.02829044684767723, -0.02829044684767723, -0.02829044684767723, 0.02829044684767723, 0.0, -0.02829044684767723, 0.0, -0.02829044684767723, -0.02829044684767723, 0.0, 0.02829044684767723, -0.05658089369535446, 0.0, 0.02829044684767723, -0.02829044684767723, 0.02829044684767723, 0.0, -0.02829044684767723]
[2025-05-15 22:35:31,794]: Mean: -0.00013967
[2025-05-15 22:35:31,794]: Min: -0.08487134
[2025-05-15 22:35:31,794]: Max: 0.11316179
[2025-05-15 22:35:31,794]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-15 22:35:31,794]: Sample Values (25 elements): [0.9319719672203064, 0.9434676766395569, 0.9930761456489563, 0.9600803256034851, 0.9676589369773865, 0.9423330426216125, 0.9412662386894226, 0.9481133818626404, 0.9668728113174438, 0.950408935546875, 0.9376627802848816, 0.9822180867195129, 0.9356158375740051, 0.956605076789856, 0.9867780804634094, 0.9723559617996216, 0.9491899013519287, 0.9893904328346252, 0.9906309843063354, 0.9984415769577026, 0.9480808973312378, 1.0145922899246216, 0.9533798098564148, 0.9561305046081543, 0.9768272042274475]
[2025-05-15 22:35:31,794]: Mean: 0.96564788
[2025-05-15 22:35:31,795]: Min: 0.91024178
[2025-05-15 22:35:31,795]: Max: 1.02283669
[2025-05-15 22:35:31,796]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-15 22:35:31,797]: Sample Values (25 elements): [0.0, -0.022504253312945366, 0.04500850662589073, 0.022504253312945366, 0.0, 0.04500850662589073, 0.04500850662589073, 0.022504253312945366, -0.04500850662589073, 0.022504253312945366, -0.022504253312945366, -0.022504253312945366, -0.022504253312945366, 0.0, 0.0, 0.022504253312945366, 0.022504253312945366, -0.022504253312945366, -0.04500850662589073, 0.022504253312945366, -0.022504253312945366, 0.0, -0.022504253312945366, 0.022504253312945366, 0.0]
[2025-05-15 22:35:31,797]: Mean: 0.00000519
[2025-05-15 22:35:31,797]: Min: -0.06751276
[2025-05-15 22:35:31,797]: Max: 0.09001701
[2025-05-15 22:35:31,797]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-15 22:35:31,797]: Sample Values (25 elements): [0.9638155102729797, 0.9661220908164978, 0.964745819568634, 0.9585140347480774, 0.9510708451271057, 0.951516330242157, 0.9606358408927917, 0.9640932679176331, 0.9708850979804993, 0.9678717255592346, 0.9625097513198853, 0.9547573924064636, 0.96561598777771, 0.9708561301231384, 0.9638263583183289, 0.9686944484710693, 0.9645810127258301, 0.9632918238639832, 0.9542989134788513, 0.9575389623641968, 0.9664794206619263, 0.9614678025245667, 0.9600630402565002, 0.9522334933280945, 0.983681321144104]
[2025-05-15 22:35:31,798]: Mean: 0.96227074
[2025-05-15 22:35:31,798]: Min: 0.94653410
[2025-05-15 22:35:31,798]: Max: 0.98368132
[2025-05-15 22:35:31,799]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 22:35:31,800]: Sample Values (25 elements): [0.038771726191043854, -0.019385863095521927, 0.0, 0.0, -0.019385863095521927, 0.0, -0.019385863095521927, -0.019385863095521927, 0.038771726191043854, 0.019385863095521927, 0.019385863095521927, 0.0, 0.0, 0.0, -0.019385863095521927, -0.038771726191043854, -0.019385863095521927, 0.0, 0.0, 0.019385863095521927, -0.019385863095521927, 0.0, -0.019385863095521927, 0.019385863095521927, -0.038771726191043854]
[2025-05-15 22:35:31,800]: Mean: 0.00004891
[2025-05-15 22:35:31,801]: Min: -0.07754345
[2025-05-15 22:35:31,801]: Max: 0.05815759
[2025-05-15 22:35:31,801]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-15 22:35:31,801]: Sample Values (25 elements): [0.975493848323822, 0.9687672853469849, 0.9558156132698059, 0.9719334840774536, 0.9670915603637695, 0.9710870981216431, 0.9673418998718262, 0.9636404514312744, 0.9638943076133728, 0.9573960304260254, 0.9667061567306519, 0.9759547114372253, 0.9517148733139038, 0.9703364968299866, 0.9603601098060608, 0.9728112816810608, 0.9649503827095032, 0.9557642340660095, 0.9606032967567444, 0.9713176488876343, 0.9528576135635376, 0.965714693069458, 0.9689272046089172, 0.9856811761856079, 0.9681663513183594]
[2025-05-15 22:35:31,801]: Mean: 0.96444428
[2025-05-15 22:35:31,801]: Min: 0.92740798
[2025-05-15 22:35:31,802]: Max: 0.99439305
[2025-05-15 22:35:31,803]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-15 22:35:31,803]: Sample Values (25 elements): [0.09503249824047089, -0.04751624912023544, 0.0, -0.09503249824047089, 0.0, 0.0, 0.09503249824047089, -0.14254873991012573, -0.09503249824047089, 0.04751624912023544, -0.04751624912023544, 0.0, 0.09503249824047089, -0.04751624912023544, -0.04751624912023544, 0.09503249824047089, 0.09503249824047089, -0.14254873991012573, -0.14254873991012573, 0.0, 0.0, -0.09503249824047089, 0.04751624912023544, 0.09503249824047089, 0.04751624912023544]
[2025-05-15 22:35:31,803]: Mean: -0.00100346
[2025-05-15 22:35:31,803]: Min: -0.19006500
[2025-05-15 22:35:31,803]: Max: 0.14254874
[2025-05-15 22:35:31,804]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-15 22:35:31,804]: Sample Values (25 elements): [0.9540653824806213, 0.9563862085342407, 0.9380517601966858, 0.9487568736076355, 0.9483715891838074, 0.953213632106781, 0.9257093667984009, 0.9226952791213989, 0.9500269293785095, 0.9448286294937134, 0.9412930607795715, 0.9475583434104919, 0.9451186656951904, 0.9287453889846802, 0.9515068531036377, 0.9448155164718628, 0.9196754693984985, 0.9445717930793762, 0.9244452714920044, 0.9320445656776428, 0.9599915146827698, 0.9320682287216187, 0.9375618696212769, 0.9557010531425476, 0.9541710019111633]
[2025-05-15 22:35:31,804]: Mean: 0.94175506
[2025-05-15 22:35:31,804]: Min: 0.91428226
[2025-05-15 22:35:31,804]: Max: 0.97442454
[2025-05-15 22:35:31,805]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 22:35:31,807]: Sample Values (25 elements): [0.03762537240982056, -0.01881268620491028, 0.0, 0.0, -0.01881268620491028, 0.0, 0.0, 0.01881268620491028, -0.01881268620491028, -0.03762537240982056, 0.01881268620491028, 0.0, 0.01881268620491028, -0.01881268620491028, 0.01881268620491028, -0.01881268620491028, 0.01881268620491028, 0.0, 0.0, 0.03762537240982056, 0.0, 0.01881268620491028, 0.01881268620491028, 0.01881268620491028, 0.0]
[2025-05-15 22:35:31,807]: Mean: 0.00001391
[2025-05-15 22:35:31,807]: Min: -0.07525074
[2025-05-15 22:35:31,807]: Max: 0.05643806
[2025-05-15 22:35:31,807]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-15 22:35:31,808]: Sample Values (25 elements): [0.9687883853912354, 0.9612014293670654, 0.9633503556251526, 0.9672293066978455, 0.9491845369338989, 0.9575201272964478, 0.9627771377563477, 0.9668435454368591, 0.9764198064804077, 0.9716141223907471, 0.956681489944458, 0.9680445790290833, 0.9699741005897522, 0.9664995670318604, 0.9619295001029968, 0.9580587148666382, 0.9642290472984314, 0.9625581502914429, 0.9733153581619263, 0.9629064798355103, 0.9715256690979004, 0.9574057459831238, 0.9608088135719299, 0.9601773023605347, 0.9633660316467285]
[2025-05-15 22:35:31,808]: Mean: 0.96476948
[2025-05-15 22:35:31,808]: Min: 0.94918454
[2025-05-15 22:35:31,808]: Max: 0.97757286
[2025-05-15 22:35:31,809]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-15 22:35:31,811]: Sample Values (25 elements): [0.018405236303806305, 0.0, 0.0, 0.018405236303806305, -0.018405236303806305, -0.018405236303806305, -0.018405236303806305, 0.0, 0.0, -0.018405236303806305, -0.018405236303806305, 0.0, 0.018405236303806305, -0.018405236303806305, 0.03681047260761261, 0.0, 0.0, 0.0, 0.0, 0.03681047260761261, -0.03681047260761261, -0.018405236303806305, -0.018405236303806305, -0.018405236303806305, -0.018405236303806305]
[2025-05-15 22:35:31,811]: Mean: -0.00006228
[2025-05-15 22:35:31,811]: Min: -0.05521571
[2025-05-15 22:35:31,811]: Max: 0.07362095
[2025-05-15 22:35:31,812]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-15 22:35:31,812]: Sample Values (25 elements): [0.9923238158226013, 0.961674153804779, 0.9694019556045532, 0.9675363898277283, 0.9770311713218689, 0.9759881496429443, 0.9696208238601685, 0.9755623936653137, 0.9761432409286499, 0.9724750518798828, 0.9550492763519287, 0.9835482835769653, 0.9596240520477295, 0.9883177280426025, 0.9620463848114014, 0.9718299508094788, 0.9778971672058105, 0.9718824028968811, 0.9860115647315979, 0.970001757144928, 0.9753590226173401, 0.9801644086837769, 0.9687923192977905, 0.9741297364234924, 0.9627611637115479]
[2025-05-15 22:35:31,812]: Mean: 0.97307432
[2025-05-15 22:35:31,812]: Min: 0.95026898
[2025-05-15 22:35:31,812]: Max: 1.00523460
[2025-05-15 22:35:31,813]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-15 22:35:31,816]: Sample Values (25 elements): [0.015574135817587376, -0.03114827163517475, 0.03114827163517475, 0.0, 0.0, -0.015574135817587376, 0.0, -0.015574135817587376, 0.015574135817587376, -0.03114827163517475, 0.03114827163517475, -0.015574135817587376, 0.03114827163517475, 0.015574135817587376, 0.0, 0.015574135817587376, -0.015574135817587376, -0.015574135817587376, -0.03114827163517475, -0.03114827163517475, -0.015574135817587376, -0.015574135817587376, 0.0, 0.0, 0.0]
[2025-05-15 22:35:31,816]: Mean: -0.00001077
[2025-05-15 22:35:31,816]: Min: -0.04672241
[2025-05-15 22:35:31,817]: Max: 0.06229654
[2025-05-15 22:35:31,817]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-15 22:35:31,817]: Sample Values (25 elements): [0.9586336016654968, 0.9649692177772522, 0.9631037712097168, 0.9644492268562317, 0.9608190059661865, 0.9579998850822449, 0.9630287885665894, 0.9618711471557617, 0.9611411094665527, 0.9632407426834106, 0.9648582339286804, 0.9582390785217285, 0.961367130279541, 0.9615471959114075, 0.9590969681739807, 0.9641347527503967, 0.9575594663619995, 0.9622206091880798, 0.9647886157035828, 0.9644108414649963, 0.9610738158226013, 0.9590688943862915, 0.9628306031227112, 0.9587886333465576, 0.9608703255653381]
[2025-05-15 22:35:31,818]: Mean: 0.96197045
[2025-05-15 22:35:31,818]: Min: 0.95287263
[2025-05-15 22:35:31,818]: Max: 0.97495550
[2025-05-15 22:35:31,819]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 22:35:31,825]: Sample Values (25 elements): [0.0, 0.026428671553730965, -0.013214335776865482, 0.013214335776865482, -0.013214335776865482, 0.0, -0.013214335776865482, 0.013214335776865482, 0.013214335776865482, -0.013214335776865482, 0.013214335776865482, 0.026428671553730965, -0.026428671553730965, 0.0, 0.026428671553730965, 0.013214335776865482, -0.013214335776865482, 0.0, 0.0, -0.013214335776865482, -0.013214335776865482, 0.0, 0.013214335776865482, -0.013214335776865482, 0.013214335776865482]
[2025-05-15 22:35:31,825]: Mean: -0.00000845
[2025-05-15 22:35:31,826]: Min: -0.05285734
[2025-05-15 22:35:31,826]: Max: 0.03964301
[2025-05-15 22:35:31,826]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-15 22:35:31,826]: Sample Values (25 elements): [0.9698194861412048, 0.9638147950172424, 0.970801830291748, 0.9677931666374207, 0.9583712220191956, 0.9601243734359741, 0.9661701321601868, 0.9603589177131653, 0.9809547066688538, 0.9651128649711609, 0.9625022411346436, 0.964768648147583, 0.9680163860321045, 0.9622260928153992, 0.9678280353546143, 0.9726557731628418, 0.9646904468536377, 0.9655008912086487, 0.9698291420936584, 0.965305507183075, 0.9605165719985962, 0.9680559635162354, 0.9696395993232727, 0.9696726202964783, 0.9633335471153259]
[2025-05-15 22:35:31,826]: Mean: 0.96653676
[2025-05-15 22:35:31,826]: Min: 0.95186198
[2025-05-15 22:35:31,827]: Max: 0.98712838
[2025-05-15 22:35:31,828]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-15 22:35:31,828]: Sample Values (25 elements): [0.03257429227232933, 0.03257429227232933, -0.03257429227232933, -0.06514858454465866, 0.0, 0.0, 0.03257429227232933, -0.03257429227232933, 0.03257429227232933, 0.0, 0.03257429227232933, -0.06514858454465866, 0.06514858454465866, 0.09772287309169769, 0.03257429227232933, 0.06514858454465866, -0.03257429227232933, -0.03257429227232933, 0.0, -0.09772287309169769, 0.03257429227232933, 0.06514858454465866, 0.03257429227232933, -0.06514858454465866, -0.06514858454465866]
[2025-05-15 22:35:31,828]: Mean: -0.00016800
[2025-05-15 22:35:31,828]: Min: -0.09772287
[2025-05-15 22:35:31,829]: Max: 0.13029717
[2025-05-15 22:35:31,829]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-15 22:35:31,829]: Sample Values (25 elements): [0.9425414800643921, 0.9480596780776978, 0.9445382952690125, 0.9444776773452759, 0.943803071975708, 0.9454349279403687, 0.9366328120231628, 0.939968466758728, 0.9417136311531067, 0.9388371109962463, 0.9453409314155579, 0.9294788241386414, 0.9373332262039185, 0.9496933221817017, 0.9412552118301392, 0.944628894329071, 0.9395405054092407, 0.9518072009086609, 0.9403926134109497, 0.9535930156707764, 0.942039430141449, 0.9477434158325195, 0.9459699988365173, 0.9446577429771423, 0.9499403238296509]
[2025-05-15 22:35:31,829]: Mean: 0.94442230
[2025-05-15 22:35:31,829]: Min: 0.92924488
[2025-05-15 22:35:31,829]: Max: 0.95930743
[2025-05-15 22:35:31,830]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 22:35:31,837]: Sample Values (25 elements): [0.0, 0.0, 0.013662920333445072, -0.027325840666890144, 0.027325840666890144, 0.0, -0.027325840666890144, -0.013662920333445072, 0.013662920333445072, 0.013662920333445072, 0.027325840666890144, 0.013662920333445072, -0.013662920333445072, 0.013662920333445072, 0.0, 0.0, -0.013662920333445072, -0.027325840666890144, -0.027325840666890144, 0.0, -0.013662920333445072, 0.0, -0.013662920333445072, 0.013662920333445072, -0.027325840666890144]
[2025-05-15 22:35:31,838]: Mean: 0.00000248
[2025-05-15 22:35:31,838]: Min: -0.04098876
[2025-05-15 22:35:31,838]: Max: 0.05465168
[2025-05-15 22:35:31,838]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-15 22:35:31,839]: Sample Values (25 elements): [0.9654202461242676, 0.9589458703994751, 0.9598847031593323, 0.956500768661499, 0.965985894203186, 0.9724500775337219, 0.9642741084098816, 0.9605526328086853, 0.9625615477561951, 0.9596748948097229, 0.9612261652946472, 0.960374653339386, 0.9663188457489014, 0.9631555676460266, 0.9618929624557495, 0.9634497165679932, 0.9665006995201111, 0.9632948637008667, 0.9608624577522278, 0.9611844420433044, 0.9662449359893799, 0.9688405990600586, 0.9663125276565552, 0.9568519592285156, 0.9613361358642578]
[2025-05-15 22:35:31,839]: Mean: 0.96265328
[2025-05-15 22:35:31,839]: Min: 0.95391941
[2025-05-15 22:35:31,839]: Max: 0.97641021
[2025-05-15 22:35:31,840]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-15 22:35:31,847]: Sample Values (25 elements): [0.025063417851924896, 0.012531708925962448, -0.012531708925962448, 0.012531708925962448, 0.0, -0.012531708925962448, 0.0, -0.012531708925962448, -0.012531708925962448, -0.012531708925962448, 0.025063417851924896, -0.012531708925962448, 0.0, -0.025063417851924896, -0.012531708925962448, -0.025063417851924896, 0.012531708925962448, 0.025063417851924896, 0.012531708925962448, -0.012531708925962448, -0.025063417851924896, -0.025063417851924896, -0.012531708925962448, -0.025063417851924896, -0.025063417851924896]
[2025-05-15 22:35:31,848]: Mean: 0.00000508
[2025-05-15 22:35:31,848]: Min: -0.05012684
[2025-05-15 22:35:31,848]: Max: 0.03759513
[2025-05-15 22:35:31,848]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-15 22:35:31,849]: Sample Values (25 elements): [0.9831663966178894, 0.971146821975708, 0.9658215641975403, 0.970934271812439, 0.9709368348121643, 0.9701936841011047, 0.9580642580986023, 0.9687417149543762, 0.9702858924865723, 0.9804867506027222, 0.9774595499038696, 0.9646018147468567, 0.9672789573669434, 0.965867280960083, 0.9637888669967651, 0.9727733135223389, 0.973416268825531, 0.9706404209136963, 0.9786978363990784, 0.9749690294265747, 0.9944564700126648, 0.9737801551818848, 0.9658064246177673, 0.9670732617378235, 0.9815958142280579]
[2025-05-15 22:35:31,849]: Mean: 0.97121167
[2025-05-15 22:35:31,849]: Min: 0.95392716
[2025-05-15 22:35:31,849]: Max: 0.99445647
[2025-05-15 22:35:31,850]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-15 22:35:31,868]: Sample Values (25 elements): [0.011362285353243351, 0.011362285353243351, 0.0, -0.022724570706486702, -0.011362285353243351, -0.022724570706486702, 0.0, 0.011362285353243351, 0.0, 0.011362285353243351, 0.022724570706486702, 0.011362285353243351, 0.0, 0.011362285353243351, 0.0, 0.011362285353243351, -0.011362285353243351, 0.011362285353243351, 0.011362285353243351, 0.011362285353243351, 0.011362285353243351, 0.0, 0.011362285353243351, 0.011362285353243351, 0.0]
[2025-05-15 22:35:31,868]: Mean: 0.00001595
[2025-05-15 22:35:31,868]: Min: -0.03408686
[2025-05-15 22:35:31,868]: Max: 0.04544914
[2025-05-15 22:35:31,868]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-15 22:35:31,869]: Sample Values (25 elements): [0.9675812125205994, 0.9593291878700256, 0.9635375738143921, 0.9578520059585571, 0.9642851948738098, 0.9574941992759705, 0.9638605117797852, 0.9604208469390869, 0.9605496525764465, 0.9619498252868652, 0.9594063758850098, 0.9613305330276489, 0.9619641304016113, 0.95759117603302, 0.9615799188613892, 0.9639871120452881, 0.9609165787696838, 0.9600716233253479, 0.9628413915634155, 0.9634045958518982, 0.9609569907188416, 0.9648786783218384, 0.9641788601875305, 0.9611300230026245, 0.9588128924369812]
[2025-05-15 22:35:31,869]: Mean: 0.96105081
[2025-05-15 22:35:31,869]: Min: 0.95581615
[2025-05-15 22:35:31,869]: Max: 0.97049797
[2025-05-15 22:35:31,870]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 22:35:31,915]: Sample Values (25 elements): [0.008641310036182404, 0.0, -0.008641310036182404, 0.0, 0.008641310036182404, -0.008641310036182404, 0.008641310036182404, 0.0, 0.0, 0.0, -0.008641310036182404, 0.0, 0.0, -0.017282620072364807, -0.008641310036182404, 0.0, 0.0, 0.008641310036182404, -0.008641310036182404, -0.008641310036182404, 0.008641310036182404, 0.008641310036182404, 0.0, -0.008641310036182404, 0.008641310036182404]
[2025-05-15 22:35:31,915]: Mean: 0.00000826
[2025-05-15 22:35:31,915]: Min: -0.02592393
[2025-05-15 22:35:31,916]: Max: 0.03456524
[2025-05-15 22:35:31,916]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-15 22:35:31,916]: Sample Values (25 elements): [0.9625537991523743, 0.9695431590080261, 0.9675822854042053, 0.9673604369163513, 0.9653701186180115, 0.9643151164054871, 0.9639809131622314, 0.9645333290100098, 0.9646831154823303, 0.9680374264717102, 0.9651761651039124, 0.9667777419090271, 0.9619532227516174, 0.9634842276573181, 0.970324695110321, 0.9651785492897034, 0.9640003442764282, 0.9634549617767334, 0.9722229242324829, 0.9645501971244812, 0.9690409898757935, 0.965984582901001, 0.9657070636749268, 0.9643669128417969, 0.9661893248558044]
[2025-05-15 22:35:31,916]: Mean: 0.96562958
[2025-05-15 22:35:31,916]: Min: 0.95706439
[2025-05-15 22:35:31,917]: Max: 0.97836930
[2025-05-15 22:35:31,918]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-15 22:35:31,919]: Sample Values (25 elements): [-0.02215290255844593, 0.0, 0.06645870953798294, -0.02215290255844593, 0.0, -0.04430580511689186, 0.0, -0.06645870953798294, -0.02215290255844593, 0.02215290255844593, 0.04430580511689186, 0.02215290255844593, 0.04430580511689186, 0.02215290255844593, -0.04430580511689186, 0.04430580511689186, 0.06645870953798294, -0.04430580511689186, 0.04430580511689186, 0.04430580511689186, 0.02215290255844593, -0.04430580511689186, 0.02215290255844593, 0.0, 0.02215290255844593]
[2025-05-15 22:35:31,919]: Mean: 0.00004327
[2025-05-15 22:35:31,919]: Min: -0.06645871
[2025-05-15 22:35:31,919]: Max: 0.08861161
[2025-05-15 22:35:31,919]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-15 22:35:31,920]: Sample Values (25 elements): [0.9505269527435303, 0.9558780789375305, 0.9528036117553711, 0.9557346105575562, 0.9510419368743896, 0.9596654772758484, 0.9524291753768921, 0.9533616304397583, 0.9558433294296265, 0.952273964881897, 0.9554765224456787, 0.9523726105690002, 0.95414799451828, 0.9553688764572144, 0.952110767364502, 0.9553384780883789, 0.9542706608772278, 0.9505351781845093, 0.9476122260093689, 0.9495652914047241, 0.9523407220840454, 0.9530383944511414, 0.9533304572105408, 0.9546371102333069, 0.9543694257736206]
[2025-05-15 22:35:31,920]: Mean: 0.95379257
[2025-05-15 22:35:31,920]: Min: 0.94209880
[2025-05-15 22:35:31,920]: Max: 0.96025211
[2025-05-15 22:35:31,921]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 22:35:31,973]: Sample Values (25 elements): [-0.008592631667852402, 0.008592631667852402, 0.008592631667852402, 0.008592631667852402, -0.008592631667852402, 0.017185263335704803, -0.017185263335704803, -0.008592631667852402, 0.0, -0.017185263335704803, 0.0, -0.008592631667852402, 0.0, 0.0, 0.0, -0.017185263335704803, -0.008592631667852402, 0.008592631667852402, 0.0, -0.017185263335704803, -0.008592631667852402, -0.017185263335704803, -0.017185263335704803, 0.008592631667852402, -0.008592631667852402]
[2025-05-15 22:35:31,973]: Mean: -0.00000932
[2025-05-15 22:35:31,973]: Min: -0.02577790
[2025-05-15 22:35:31,974]: Max: 0.03437053
[2025-05-15 22:35:31,974]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-15 22:35:31,974]: Sample Values (25 elements): [0.96202152967453, 0.9635781645774841, 0.9613561630249023, 0.9618157148361206, 0.9634935259819031, 0.9639146327972412, 0.9626232385635376, 0.9620592594146729, 0.9617017507553101, 0.9650327563285828, 0.9630661010742188, 0.9601075053215027, 0.963693380355835, 0.9607996344566345, 0.9612805843353271, 0.9589498043060303, 0.9600641131401062, 0.9610561728477478, 0.9616872668266296, 0.9615506529808044, 0.9642741680145264, 0.9592875838279724, 0.9595876932144165, 0.9614582657814026, 0.9614139199256897]
[2025-05-15 22:35:31,974]: Mean: 0.96152127
[2025-05-15 22:35:31,974]: Min: 0.95750558
[2025-05-15 22:35:31,975]: Max: 0.97033125
[2025-05-15 22:35:31,976]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-15 22:35:32,023]: Sample Values (25 elements): [0.0, 0.006828163284808397, 0.0, -0.006828163284808397, -0.006828163284808397, -0.006828163284808397, -0.013656326569616795, -0.006828163284808397, 0.0, 0.0, 0.006828163284808397, -0.006828163284808397, 0.013656326569616795, 0.0, 0.013656326569616795, -0.013656326569616795, -0.006828163284808397, -0.006828163284808397, -0.006828163284808397, 0.013656326569616795, 0.0, 0.006828163284808397, -0.006828163284808397, -0.006828163284808397, 0.006828163284808397]
[2025-05-15 22:35:32,023]: Mean: 0.00000602
[2025-05-15 22:35:32,023]: Min: -0.02731265
[2025-05-15 22:35:32,024]: Max: 0.02048449
[2025-05-15 22:35:32,024]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-15 22:35:32,026]: Sample Values (25 elements): [0.9670389890670776, 0.9691070914268494, 0.9657573699951172, 0.9675382971763611, 0.9665284752845764, 0.9674385190010071, 0.9655938744544983, 0.972122848033905, 0.9685277938842773, 0.9714046120643616, 0.9688541889190674, 0.9659400582313538, 0.9706973433494568, 0.9717150330543518, 0.9659719467163086, 0.9750818610191345, 0.9695266485214233, 0.9678236246109009, 0.966830849647522, 0.9662100672721863, 0.9727771878242493, 0.9748656153678894, 0.9668179750442505, 0.9676627516746521, 0.974246621131897]
[2025-05-15 22:35:32,026]: Mean: 0.96843064
[2025-05-15 22:35:32,027]: Min: 0.96291625
[2025-05-15 22:35:32,027]: Max: 0.97676343
[2025-05-15 22:35:32,027]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-15 22:35:32,027]: Sample Values (25 elements): [-0.06969505548477173, -0.07249241322278976, -0.037595611065626144, 0.06565510481595993, 0.046018168330192566, -0.03939766436815262, -0.026957090944051743, 0.015247316099703312, 0.051415059715509415, 0.05896131321787834, 0.015187429264187813, 0.04353836178779602, 0.1107669472694397, -0.012984899803996086, -0.03136037290096283, 0.037485070526599884, -0.07464113086462021, -0.056747451424598694, 0.04842476546764374, -0.07154028117656708, -0.07709795981645584, -0.10430022329092026, -0.03548523783683777, 0.0434039942920208, -0.052334900945425034]
[2025-05-15 22:35:32,027]: Mean: 0.00078901
[2025-05-15 22:35:32,028]: Min: -0.13177218
[2025-05-15 22:35:32,028]: Max: 0.13858230

[2025-05-16 01:08:25,111]: 


QAT of ResNet18 with parametrized_hardtanh down to 2 bits...
[2025-05-16 01:08:25,499]: [ResNet18_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-16 01:08:25,749]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-16 01:10:20,732]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 0.6038 Train Acc: 0.7936 Eval Loss: 1.0069 Eval Acc: 0.7021 (LR: 0.001000)
[2025-05-16 01:12:13,052]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.5204 Train Acc: 0.8151 Eval Loss: 1.2433 Eval Acc: 0.6061 (LR: 0.001000)
[2025-05-16 01:14:05,613]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.5226 Train Acc: 0.8157 Eval Loss: 1.0479 Eval Acc: 0.6781 (LR: 0.001000)
[2025-05-16 01:15:59,480]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.5003 Train Acc: 0.8225 Eval Loss: 0.7106 Eval Acc: 0.7753 (LR: 0.001000)
[2025-05-16 01:17:50,490]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.4717 Train Acc: 0.8309 Eval Loss: 1.3983 Eval Acc: 0.6176 (LR: 0.001000)
[2025-05-16 01:19:42,464]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.4861 Train Acc: 0.8278 Eval Loss: 3.0464 Eval Acc: 0.3476 (LR: 0.001000)
[2025-05-16 01:21:36,651]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.4624 Train Acc: 0.8359 Eval Loss: 1.0258 Eval Acc: 0.7082 (LR: 0.001000)
[2025-05-16 01:23:32,035]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.4500 Train Acc: 0.8404 Eval Loss: 2.4441 Eval Acc: 0.4337 (LR: 0.001000)
[2025-05-16 01:25:27,700]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.4370 Train Acc: 0.8454 Eval Loss: 1.2234 Eval Acc: 0.6422 (LR: 0.001000)
[2025-05-16 01:27:17,693]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.4435 Train Acc: 0.8429 Eval Loss: 2.1249 Eval Acc: 0.4910 (LR: 0.001000)
[2025-05-16 01:29:06,799]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.4349 Train Acc: 0.8446 Eval Loss: 1.2336 Eval Acc: 0.6701 (LR: 0.001000)
[2025-05-16 01:30:58,947]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.4232 Train Acc: 0.8486 Eval Loss: 1.0485 Eval Acc: 0.7174 (LR: 0.001000)
[2025-05-16 01:32:52,312]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.4209 Train Acc: 0.8498 Eval Loss: 0.7316 Eval Acc: 0.7758 (LR: 0.001000)
[2025-05-16 01:34:46,131]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.4102 Train Acc: 0.8547 Eval Loss: 0.7784 Eval Acc: 0.7612 (LR: 0.001000)
[2025-05-16 01:36:38,642]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.3958 Train Acc: 0.8588 Eval Loss: 0.5413 Eval Acc: 0.8214 (LR: 0.001000)
[2025-05-16 01:38:39,786]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.3951 Train Acc: 0.8602 Eval Loss: 2.7187 Eval Acc: 0.4561 (LR: 0.001000)
[2025-05-16 01:40:32,611]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.3939 Train Acc: 0.8598 Eval Loss: 1.0361 Eval Acc: 0.7073 (LR: 0.001000)
[2025-05-16 01:42:27,270]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.3941 Train Acc: 0.8599 Eval Loss: 2.4922 Eval Acc: 0.4169 (LR: 0.001000)
[2025-05-16 01:44:21,145]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.3894 Train Acc: 0.8622 Eval Loss: 1.4300 Eval Acc: 0.6277 (LR: 0.001000)
[2025-05-16 01:46:15,233]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.3809 Train Acc: 0.8637 Eval Loss: 1.2623 Eval Acc: 0.6422 (LR: 0.001000)
[2025-05-16 01:48:08,704]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.3798 Train Acc: 0.8658 Eval Loss: 1.3059 Eval Acc: 0.6264 (LR: 0.001000)
[2025-05-16 01:50:01,393]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.3696 Train Acc: 0.8709 Eval Loss: 0.8261 Eval Acc: 0.7347 (LR: 0.001000)
[2025-05-16 01:51:55,214]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.3699 Train Acc: 0.8685 Eval Loss: 1.6731 Eval Acc: 0.5469 (LR: 0.001000)
[2025-05-16 01:53:47,798]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.3708 Train Acc: 0.8697 Eval Loss: 2.2237 Eval Acc: 0.4964 (LR: 0.001000)
[2025-05-16 01:55:41,351]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.3519 Train Acc: 0.8739 Eval Loss: 0.9530 Eval Acc: 0.7237 (LR: 0.001000)
[2025-05-16 01:57:33,960]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.3654 Train Acc: 0.8719 Eval Loss: 1.4728 Eval Acc: 0.6105 (LR: 0.001000)
[2025-05-16 01:59:27,177]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.3551 Train Acc: 0.8741 Eval Loss: 1.9896 Eval Acc: 0.5451 (LR: 0.001000)
[2025-05-16 02:01:23,815]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.3490 Train Acc: 0.8753 Eval Loss: 2.5879 Eval Acc: 0.4812 (LR: 0.001000)
[2025-05-16 02:03:16,841]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.3492 Train Acc: 0.8771 Eval Loss: 0.5985 Eval Acc: 0.8045 (LR: 0.001000)
[2025-05-16 02:05:10,001]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.3617 Train Acc: 0.8718 Eval Loss: 2.1769 Eval Acc: 0.4752 (LR: 0.000100)
[2025-05-16 02:07:02,348]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.2695 Train Acc: 0.9047 Eval Loss: 0.8191 Eval Acc: 0.7531 (LR: 0.000100)
[2025-05-16 02:08:55,062]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.2600 Train Acc: 0.9092 Eval Loss: 1.2695 Eval Acc: 0.6799 (LR: 0.000100)
[2025-05-16 02:10:47,876]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.2476 Train Acc: 0.9131 Eval Loss: 2.6052 Eval Acc: 0.4313 (LR: 0.000100)
[2025-05-16 02:12:40,824]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.2419 Train Acc: 0.9146 Eval Loss: 0.9612 Eval Acc: 0.7471 (LR: 0.000100)
[2025-05-16 02:14:33,026]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.2427 Train Acc: 0.9143 Eval Loss: 0.8188 Eval Acc: 0.7413 (LR: 0.000100)
[2025-05-16 02:16:25,098]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.2396 Train Acc: 0.9172 Eval Loss: 0.6694 Eval Acc: 0.7936 (LR: 0.000100)
[2025-05-16 02:18:16,677]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.2355 Train Acc: 0.9171 Eval Loss: 0.5882 Eval Acc: 0.8295 (LR: 0.000100)
[2025-05-16 02:20:13,389]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.2317 Train Acc: 0.9186 Eval Loss: 0.4900 Eval Acc: 0.8502 (LR: 0.000100)
[2025-05-16 02:22:11,183]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.2324 Train Acc: 0.9175 Eval Loss: 0.5631 Eval Acc: 0.8292 (LR: 0.000100)
[2025-05-16 02:24:09,831]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.2348 Train Acc: 0.9175 Eval Loss: 0.5968 Eval Acc: 0.8201 (LR: 0.000100)
[2025-05-16 02:26:05,475]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.2291 Train Acc: 0.9202 Eval Loss: 0.5897 Eval Acc: 0.8071 (LR: 0.000100)
[2025-05-16 02:27:59,760]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.2247 Train Acc: 0.9208 Eval Loss: 0.5919 Eval Acc: 0.8270 (LR: 0.000100)
[2025-05-16 02:29:50,378]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.2247 Train Acc: 0.9195 Eval Loss: 0.8897 Eval Acc: 0.7575 (LR: 0.000100)
[2025-05-16 02:31:38,482]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.2243 Train Acc: 0.9208 Eval Loss: 0.9635 Eval Acc: 0.7316 (LR: 0.000100)
[2025-05-16 02:33:27,568]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.2239 Train Acc: 0.9215 Eval Loss: 2.4597 Eval Acc: 0.5259 (LR: 0.000010)
[2025-05-16 02:35:15,962]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.2139 Train Acc: 0.9250 Eval Loss: 0.6553 Eval Acc: 0.8023 (LR: 0.000010)
[2025-05-16 02:37:09,395]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.2102 Train Acc: 0.9263 Eval Loss: 1.4890 Eval Acc: 0.6177 (LR: 0.000010)
[2025-05-16 02:38:57,474]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.2087 Train Acc: 0.9281 Eval Loss: 1.2352 Eval Acc: 0.6582 (LR: 0.000010)
[2025-05-16 02:40:47,994]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.2081 Train Acc: 0.9258 Eval Loss: 1.7058 Eval Acc: 0.6285 (LR: 0.000010)
[2025-05-16 02:42:40,106]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.2024 Train Acc: 0.9279 Eval Loss: 2.4594 Eval Acc: 0.4641 (LR: 0.000010)
[2025-05-16 02:44:32,367]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.2051 Train Acc: 0.9282 Eval Loss: 0.6055 Eval Acc: 0.8125 (LR: 0.000010)
[2025-05-16 02:46:26,271]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.2036 Train Acc: 0.9285 Eval Loss: 0.8293 Eval Acc: 0.7701 (LR: 0.000010)
[2025-05-16 02:48:27,918]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.2013 Train Acc: 0.9285 Eval Loss: 0.7064 Eval Acc: 0.8090 (LR: 0.000010)
[2025-05-16 02:50:30,890]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.2026 Train Acc: 0.9290 Eval Loss: 0.4655 Eval Acc: 0.8476 (LR: 0.000010)
[2025-05-16 02:52:31,937]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.2026 Train Acc: 0.9296 Eval Loss: 2.1169 Eval Acc: 0.5527 (LR: 0.000010)
[2025-05-16 02:54:46,256]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.2032 Train Acc: 0.9286 Eval Loss: 1.1564 Eval Acc: 0.7086 (LR: 0.000010)
[2025-05-16 02:56:41,516]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.1998 Train Acc: 0.9298 Eval Loss: 1.9833 Eval Acc: 0.5674 (LR: 0.000010)
[2025-05-16 02:58:55,303]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.2060 Train Acc: 0.9272 Eval Loss: 1.2871 Eval Acc: 0.6506 (LR: 0.000010)
[2025-05-16 03:01:12,097]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.2030 Train Acc: 0.9283 Eval Loss: 2.5484 Eval Acc: 0.5098 (LR: 0.000010)
[2025-05-16 03:03:04,008]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.2052 Train Acc: 0.9282 Eval Loss: 0.6448 Eval Acc: 0.8134 (LR: 0.000010)
[2025-05-16 03:03:04,008]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8502
[2025-05-16 03:03:04,115]: 


Quantization of model down to 2 bits finished
[2025-05-16 03:03:04,115]: Model Architecture:
[2025-05-16 03:03:04,171]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5660], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4150129556655884, max_val=0.28299403190612793)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1317], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20041877031326294, max_val=0.19473296403884888)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6489], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6461238861083984, max_val=1.300645112991333)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1384], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21078014373779297, max_val=0.20429612696170807)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5328], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2664150297641754, max_val=1.33203125)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1052], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15283915400505066, max_val=0.1626780927181244)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4941], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7410936951637268, max_val=0.7410921454429626)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0759], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11495205760002136, max_val=0.11265510320663452)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7692], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1538190841674805, max_val=1.1538277864456177)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0553], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08266636729240417, max_val=0.08313258737325668)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5122], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.280397653579712, max_val=0.25609415769577026)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0471], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07113765180110931, max_val=0.07028698921203613)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1213], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18498440086841583, max_val=0.17893368005752563)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5938], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8907333612442017, max_val=0.8907288312911987)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0502], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07738242298364639, max_val=0.07330707460641861)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3918], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5877209305763245, max_val=0.5877072811126709)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06597898155450821, max_val=0.06742550432682037)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7980], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1970088481903076, max_val=1.1970139741897583)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0373], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.055132072418928146, max_val=0.056687578558921814)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4992], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7488192319869995, max_val=0.748813807964325)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0332], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04903542995452881, max_val=0.050611190497875214)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0826], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12421955913305283, max_val=0.12370070070028305)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5442], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8162930607795715, max_val=0.8162925243377686)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0332], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.049288060516119, max_val=0.050325702875852585)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4216], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6323451399803162, max_val=0.632340669631958)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.045406293123960495, max_val=0.045404743403196335)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6732], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0098531246185303, max_val=1.009859561920166)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0277], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.040779199451208115, max_val=0.04217918589711189)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4971], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.745675802230835, max_val=0.7456795573234558)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0198], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.029613176360726357, max_val=0.029653696343302727)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0534], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08005712181329727, max_val=0.07999289035797119)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7138], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8344404697418213, max_val=1.307080864906311)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0206], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.030748840421438217, max_val=0.03095061145722866)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3059], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4588507115840912, max_val=0.4588504135608673)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0161], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.025805732235312462, max_val=0.022359080612659454)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1124], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5057435035705566, max_val=1.831587791442871)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-16 03:03:04,172]: 
Model Weights:
[2025-05-16 03:03:04,172]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-16 03:03:04,177]: Sample Values (25 elements): [-0.5085158944129944, -0.029892930760979652, -0.16191324591636658, -0.19797594845294952, 0.16470982134342194, 0.27230358123779297, -0.11602986603975296, 0.0009281024103984237, -0.24770668148994446, 0.2168864607810974, -0.11924894899129868, 0.05248289927840233, 0.03591589629650116, -0.2541956305503845, -0.3265872299671173, 0.1167420968413353, -0.0673971176147461, -0.01691126637160778, -0.10167810320854187, -0.09067477285861969, -0.2402057945728302, 0.017503315582871437, -0.24402973055839539, -0.16968846321105957, 0.11487361788749695]
[2025-05-16 03:03:04,188]: Mean: 0.00167853
[2025-05-16 03:03:04,189]: Min: -0.58103108
[2025-05-16 03:03:04,189]: Max: 0.53212434
[2025-05-16 03:03:04,190]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-16 03:03:04,190]: Sample Values (25 elements): [0.9785259962081909, 1.1453595161437988, 0.9456151127815247, 1.2180777788162231, 1.3547803163528442, 1.0460319519042969, 0.7402063608169556, 0.8718146085739136, 1.0502506494522095, 0.8686659336090088, 1.125192642211914, 1.194049596786499, 0.8858256340026855, 0.9834409952163696, 1.1366914510726929, 0.8261076211929321, 0.9098289012908936, 0.9557433724403381, 0.8621140122413635, 1.0975884199142456, 0.8264533281326294, 0.8720287680625916, 0.9902934432029724, 0.8886497020721436, 0.9537443518638611]
[2025-05-16 03:03:04,190]: Mean: 1.01180756
[2025-05-16 03:03:04,190]: Min: 0.74020636
[2025-05-16 03:03:04,190]: Max: 1.48540139
[2025-05-16 03:03:04,191]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-16 03:03:04,192]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1317172646522522, 0.0, 0.0, 0.0, 0.0, 0.1317172646522522, 0.0, 0.0]
[2025-05-16 03:03:04,192]: Mean: -0.00023939
[2025-05-16 03:03:04,192]: Min: -0.26343453
[2025-05-16 03:03:04,192]: Max: 0.13171726
[2025-05-16 03:03:04,192]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-16 03:03:04,193]: Sample Values (25 elements): [0.9121516942977905, 1.0892367362976074, 1.079915165901184, 0.9852607250213623, 0.9302529692649841, 0.8906590938568115, 0.9340901970863342, 0.9327690601348877, 1.0392038822174072, 0.9602038264274597, 0.9902249574661255, 0.9569084048271179, 0.9774779677391052, 0.9245964288711548, 1.093966007232666, 0.9628903269767761, 1.0368015766143799, 0.9112616181373596, 1.1117420196533203, 0.9720957279205322, 0.9169567823410034, 0.9705515503883362, 0.9074638485908508, 1.0588781833648682, 0.9741844534873962]
[2025-05-16 03:03:04,193]: Mean: 0.97261143
[2025-05-16 03:03:04,193]: Min: 0.88472486
[2025-05-16 03:03:04,193]: Max: 1.20629251
[2025-05-16 03:03:04,194]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-16 03:03:04,195]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1383587121963501, 0.0, -0.1383587121963501]
[2025-05-16 03:03:04,195]: Mean: -0.00060427
[2025-05-16 03:03:04,195]: Min: -0.27671742
[2025-05-16 03:03:04,195]: Max: 0.13835871
[2025-05-16 03:03:04,195]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-16 03:03:04,195]: Sample Values (25 elements): [0.8651196956634521, 0.9368040561676025, 0.9395906329154968, 1.1528620719909668, 0.9843727946281433, 0.8977686166763306, 0.8948498368263245, 0.8658776879310608, 0.9422311186790466, 0.9064419269561768, 0.9630299210548401, 0.9991381764411926, 1.1201505661010742, 0.9458429217338562, 0.8986509442329407, 0.9371786713600159, 0.8995893001556396, 1.0477607250213623, 0.9479731917381287, 0.974692702293396, 0.8956976532936096, 0.9434228539466858, 1.1894536018371582, 0.9297109246253967, 0.8784244060516357]
[2025-05-16 03:03:04,195]: Mean: 0.96765608
[2025-05-16 03:03:04,196]: Min: 0.86511970
[2025-05-16 03:03:04,196]: Max: 1.33363807
[2025-05-16 03:03:04,197]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-16 03:03:04,197]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10517244040966034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10517244040966034, 0.0, -0.10517244040966034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-16 03:03:04,197]: Mean: 0.00043365
[2025-05-16 03:03:04,197]: Min: -0.10517244
[2025-05-16 03:03:04,198]: Max: 0.21034488
[2025-05-16 03:03:04,198]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-16 03:03:04,198]: Sample Values (25 elements): [0.9519174695014954, 0.9451612830162048, 1.0149526596069336, 1.0050145387649536, 0.9513508081436157, 1.0456225872039795, 0.9988755583763123, 0.9547358155250549, 0.9591600298881531, 0.9494385123252869, 0.9522629380226135, 0.9431080222129822, 0.9827398061752319, 0.9552376866340637, 0.9829282164573669, 0.9493494629859924, 0.9547442197799683, 0.9716777801513672, 0.958424985408783, 0.950924813747406, 0.9393091797828674, 0.9611063599586487, 0.9466556310653687, 0.9524375200271606, 0.9588261246681213]
[2025-05-16 03:03:04,198]: Mean: 0.97376263
[2025-05-16 03:03:04,198]: Min: 0.93003863
[2025-05-16 03:03:04,198]: Max: 1.15360534
[2025-05-16 03:03:04,199]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-16 03:03:04,200]: Sample Values (25 elements): [0.07586902379989624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07586902379989624, 0.0, 0.07586902379989624, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07586902379989624, 0.0, 0.0, 0.0, 0.0, 0.07586902379989624, 0.07586902379989624, 0.0, 0.0, 0.0, 0.0]
[2025-05-16 03:03:04,200]: Mean: -0.00022639
[2025-05-16 03:03:04,200]: Min: -0.15173805
[2025-05-16 03:03:04,200]: Max: 0.07586902
[2025-05-16 03:03:04,200]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-16 03:03:04,201]: Sample Values (25 elements): [0.9884260296821594, 0.9660417437553406, 0.9719595909118652, 1.0194251537322998, 0.9702321290969849, 1.0372583866119385, 0.9676355719566345, 1.0299745798110962, 0.9842286705970764, 1.0205442905426025, 0.9811261296272278, 0.9778603315353394, 0.9927602410316467, 1.0396476984024048, 0.9742639660835266, 1.0049725770950317, 0.9933502078056335, 1.0170300006866455, 0.9700132608413696, 0.9259989261627197, 0.9277951121330261, 0.9655598998069763, 0.985963761806488, 0.927828848361969, 0.950415313243866]
[2025-05-16 03:03:04,201]: Mean: 0.98911953
[2025-05-16 03:03:04,201]: Min: 0.90119022
[2025-05-16 03:03:04,201]: Max: 1.07834554
[2025-05-16 03:03:04,202]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-16 03:03:04,203]: Sample Values (25 elements): [0.0, -0.055266302078962326, 0.0, -0.055266302078962326, 0.0, 0.0, 0.0, 0.0, -0.055266302078962326, 0.0, 0.0, 0.0, -0.055266302078962326, 0.0, 0.0, 0.055266302078962326, 0.0, 0.055266302078962326, 0.0, 0.055266302078962326, 0.0, 0.0, 0.0, 0.0, -0.055266302078962326]
[2025-05-16 03:03:04,203]: Mean: -0.00001349
[2025-05-16 03:03:04,203]: Min: -0.05526630
[2025-05-16 03:03:04,203]: Max: 0.11053260
[2025-05-16 03:03:04,203]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-16 03:03:04,204]: Sample Values (25 elements): [0.969049870967865, 0.9594386219978333, 0.96224045753479, 0.9566584229469299, 0.9736939668655396, 0.9571803212165833, 0.9750365018844604, 0.9738134145736694, 0.9660819172859192, 0.9614759087562561, 0.9794398546218872, 0.9670734405517578, 0.9674435257911682, 0.9534885883331299, 0.9725898504257202, 0.9681466221809387, 0.9651608467102051, 0.9597523808479309, 0.9660969972610474, 0.9548659920692444, 0.9577792882919312, 0.955761730670929, 0.9586212038993835, 0.964678943157196, 0.9587857127189636]
[2025-05-16 03:03:04,204]: Mean: 0.96433091
[2025-05-16 03:03:04,204]: Min: 0.94342494
[2025-05-16 03:03:04,204]: Max: 0.98924196
[2025-05-16 03:03:04,205]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-16 03:03:04,207]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04714156314730644, 0.04714156314730644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04714156314730644, -0.04714156314730644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-16 03:03:04,207]: Mean: 0.00010422
[2025-05-16 03:03:04,207]: Min: -0.09428313
[2025-05-16 03:03:04,207]: Max: 0.04714156
[2025-05-16 03:03:04,207]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-16 03:03:04,207]: Sample Values (25 elements): [1.0012799501419067, 0.9762779474258423, 0.9645577073097229, 0.9565405249595642, 1.007304310798645, 0.9687270522117615, 0.9702367782592773, 0.9943215250968933, 0.9726808071136475, 0.9723777770996094, 0.9695621728897095, 0.9860289692878723, 0.9635003209114075, 0.9795902371406555, 0.9585991501808167, 0.9670620560646057, 0.9617587327957153, 0.9546850323677063, 0.9522987008094788, 0.977607786655426, 0.9779195785522461, 0.9654539823532104, 0.9658955931663513, 0.9897392988204956, 0.9933426380157471]
[2025-05-16 03:03:04,208]: Mean: 0.97345650
[2025-05-16 03:03:04,208]: Min: 0.94841820
[2025-05-16 03:03:04,208]: Max: 1.01074231
[2025-05-16 03:03:04,209]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-16 03:03:04,209]: Sample Values (25 elements): [0.0, -0.12130602449178696, 0.0, 0.0, 0.12130602449178696, -0.12130602449178696, 0.0, -0.12130602449178696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12130602449178696, -0.12130602449178696, 0.0, 0.0, 0.0, -0.12130602449178696, 0.0, 0.0, 0.0, 0.0, -0.12130602449178696, 0.12130602449178696]
[2025-05-16 03:03:04,209]: Mean: -0.00091809
[2025-05-16 03:03:04,209]: Min: -0.24261205
[2025-05-16 03:03:04,210]: Max: 0.12130602
[2025-05-16 03:03:04,210]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-16 03:03:04,210]: Sample Values (25 elements): [0.9226193428039551, 0.9323816299438477, 0.938765287399292, 0.9433078169822693, 0.927106499671936, 0.9567186832427979, 0.9265098571777344, 0.9348455667495728, 0.9414046406745911, 0.9098711013793945, 0.9151426553726196, 0.9389738440513611, 0.9485732316970825, 0.9518687129020691, 0.9064664244651794, 0.925091564655304, 0.9245866537094116, 0.9464938044548035, 0.9347104430198669, 0.950677216053009, 0.9283077120780945, 0.956810712814331, 0.9546752572059631, 0.9284806251525879, 0.9452271461486816]
[2025-05-16 03:03:04,210]: Mean: 0.93426192
[2025-05-16 03:03:04,210]: Min: 0.90205276
[2025-05-16 03:03:04,210]: Max: 0.96444106
[2025-05-16 03:03:04,211]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-16 03:03:04,213]: Sample Values (25 elements): [0.0, 0.0, 0.050229839980602264, 0.050229839980602264, 0.0, 0.0, 0.0, 0.0, -0.050229839980602264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050229839980602264, 0.0, 0.0, 0.0, 0.0, 0.050229839980602264, 0.0, 0.0, 0.0, -0.050229839980602264]
[2025-05-16 03:03:04,213]: Mean: 0.00000818
[2025-05-16 03:03:04,213]: Min: -0.10045968
[2025-05-16 03:03:04,213]: Max: 0.05022984
[2025-05-16 03:03:04,213]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-16 03:03:04,213]: Sample Values (25 elements): [0.9665282964706421, 0.9794336557388306, 0.981237530708313, 0.9702529311180115, 0.9835366606712341, 0.9696608185768127, 0.9662806391716003, 0.9715070128440857, 0.9661800861358643, 0.9672271013259888, 0.9692570567131042, 0.9785211086273193, 0.9654659032821655, 0.9828760027885437, 0.9725407958030701, 0.9648164510726929, 0.9835244417190552, 0.9547304511070251, 0.959505558013916, 0.9607869386672974, 0.9575002789497375, 0.9800538420677185, 0.9561968445777893, 0.9647218585014343, 0.9542462229728699]
[2025-05-16 03:03:04,214]: Mean: 0.96843994
[2025-05-16 03:03:04,214]: Min: 0.95191324
[2025-05-16 03:03:04,214]: Max: 0.98353666
[2025-05-16 03:03:04,215]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-16 03:03:04,216]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04446816444396973, 0.04446816444396973, 0.0, 0.0, 0.04446816444396973, 0.04446816444396973, 0.04446816444396973, -0.04446816444396973, 0.0, 0.04446816444396973, 0.0, 0.04446816444396973, 0.0, 0.0, 0.04446816444396973, -0.04446816444396973, 0.0, 0.0]
[2025-05-16 03:03:04,216]: Mean: -0.00001025
[2025-05-16 03:03:04,216]: Min: -0.04446816
[2025-05-16 03:03:04,217]: Max: 0.08893633
[2025-05-16 03:03:04,217]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-16 03:03:04,217]: Sample Values (25 elements): [1.0207030773162842, 0.9804282784461975, 1.0086883306503296, 0.986939549446106, 1.0086897611618042, 1.0054234266281128, 0.9822452664375305, 0.9831932783126831, 0.98250812292099, 0.9907115697860718, 0.9709141254425049, 0.9718286991119385, 0.9780089855194092, 0.9840131402015686, 0.9914584755897522, 0.9919540286064148, 0.9783599376678467, 0.963333010673523, 0.9983521699905396, 0.9656810760498047, 1.0030978918075562, 0.9820207953453064, 0.9780202507972717, 0.9886797666549683, 0.9872890710830688]
[2025-05-16 03:03:04,217]: Mean: 0.98457074
[2025-05-16 03:03:04,217]: Min: 0.95687371
[2025-05-16 03:03:04,217]: Max: 1.02070308
[2025-05-16 03:03:04,218]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-16 03:03:04,221]: Sample Values (25 elements): [0.0, 0.03727322071790695, 0.03727322071790695, 0.0, 0.0, -0.03727322071790695, 0.0, -0.03727322071790695, 0.0, 0.0, 0.0, 0.03727322071790695, -0.03727322071790695, 0.0, 0.03727322071790695, 0.0, 0.03727322071790695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03727322071790695]
[2025-05-16 03:03:04,221]: Mean: -0.00000708
[2025-05-16 03:03:04,221]: Min: -0.03727322
[2025-05-16 03:03:04,221]: Max: 0.07454644
[2025-05-16 03:03:04,221]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-16 03:03:04,222]: Sample Values (25 elements): [0.9667506217956543, 0.957145094871521, 0.9705507159233093, 0.9659311175346375, 0.9716597199440002, 0.9569770693778992, 0.9665961265563965, 0.9610121250152588, 0.9653633832931519, 0.9685200452804565, 0.9676826000213623, 0.9643398523330688, 0.966751217842102, 0.9641991257667542, 0.9805357456207275, 0.9640157222747803, 0.9561357498168945, 0.9664677977561951, 0.9634953737258911, 0.963607132434845, 0.9629076719284058, 0.9633591771125793, 0.9703730344772339, 0.9637773036956787, 0.9738785028457642]
[2025-05-16 03:03:04,222]: Mean: 0.96411031
[2025-05-16 03:03:04,222]: Min: 0.95155871
[2025-05-16 03:03:04,222]: Max: 0.98345047
[2025-05-16 03:03:04,223]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-16 03:03:04,228]: Sample Values (25 elements): [0.0, 0.0, 0.03321555629372597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03321555629372597, -0.03321555629372597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03321555629372597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-16 03:03:04,228]: Mean: -0.00002253
[2025-05-16 03:03:04,228]: Min: -0.03321556
[2025-05-16 03:03:04,229]: Max: 0.06643111
[2025-05-16 03:03:04,229]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-16 03:03:04,229]: Sample Values (25 elements): [0.972006618976593, 0.967147946357727, 0.9721132516860962, 0.9730942249298096, 0.9655159115791321, 0.9711835384368896, 0.9686293005943298, 0.9652310609817505, 0.9700842499732971, 0.9748486280441284, 0.9625113010406494, 0.9712870121002197, 0.967513918876648, 0.9782230257987976, 0.9691414833068848, 0.9623824954032898, 0.9659955501556396, 0.9684116840362549, 0.9660061001777649, 0.9730268716812134, 0.9689962863922119, 0.9625669121742249, 0.9653133153915405, 0.9810453057289124, 0.9751534461975098]
[2025-05-16 03:03:04,229]: Mean: 0.97051871
[2025-05-16 03:03:04,229]: Min: 0.95133215
[2025-05-16 03:03:04,229]: Max: 0.99018538
[2025-05-16 03:03:04,230]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-16 03:03:04,231]: Sample Values (25 elements): [0.0, 0.08264012634754181, 0.0, -0.08264012634754181, -0.08264012634754181, 0.0, 0.0, 0.0, -0.08264012634754181, 0.08264012634754181, 0.08264012634754181, -0.08264012634754181, 0.08264012634754181, 0.0, 0.08264012634754181, 0.0, 0.0, -0.08264012634754181, 0.08264012634754181, 0.0, 0.08264012634754181, 0.0, 0.0, -0.08264012634754181, 0.0]
[2025-05-16 03:03:04,231]: Mean: 0.00000504
[2025-05-16 03:03:04,231]: Min: -0.16528025
[2025-05-16 03:03:04,231]: Max: 0.08264013
[2025-05-16 03:03:04,231]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-16 03:03:04,232]: Sample Values (25 elements): [0.9428216218948364, 0.9343095421791077, 0.9341382384300232, 0.9376136660575867, 0.9349461793899536, 0.9426515698432922, 0.9329026937484741, 0.9501960873603821, 0.9267014265060425, 0.9363551139831543, 0.9313141107559204, 0.9423151612281799, 0.9429875612258911, 0.9348165392875671, 0.9345619082450867, 0.9304417371749878, 0.9335687756538391, 0.9324834942817688, 0.9424850344657898, 0.9411724209785461, 0.945289671421051, 0.9438664317131042, 0.946712851524353, 0.9476114511489868, 0.9342637062072754]
[2025-05-16 03:03:04,232]: Mean: 0.93989658
[2025-05-16 03:03:04,232]: Min: 0.92308456
[2025-05-16 03:03:04,232]: Max: 0.96025217
[2025-05-16 03:03:04,233]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-16 03:03:04,239]: Sample Values (25 elements): [0.0, 0.033204611390829086, 0.0, 0.033204611390829086, 0.033204611390829086, 0.033204611390829086, 0.0, 0.0, 0.0, -0.033204611390829086, 0.033204611390829086, 0.0, 0.033204611390829086, 0.0, 0.0, 0.0, -0.033204611390829086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.033204611390829086, 0.0]
[2025-05-16 03:03:04,240]: Mean: 0.00001711
[2025-05-16 03:03:04,240]: Min: -0.03320461
[2025-05-16 03:03:04,240]: Max: 0.06640922
[2025-05-16 03:03:04,240]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-16 03:03:04,240]: Sample Values (25 elements): [0.9624237418174744, 0.9595646262168884, 0.9603226184844971, 0.9610865116119385, 0.9637647867202759, 0.9636036157608032, 0.9660195708274841, 0.9869979619979858, 0.9576991200447083, 0.9670940637588501, 0.9602036476135254, 0.9643211960792542, 0.9745941162109375, 0.9573057293891907, 0.9567399621009827, 0.962121307849884, 0.9577277898788452, 0.9731563329696655, 0.9608685374259949, 0.9603701829910278, 0.9557176828384399, 0.958032488822937, 0.9676835536956787, 0.9580140709877014, 0.9606894850730896]
[2025-05-16 03:03:04,241]: Mean: 0.96482497
[2025-05-16 03:03:04,241]: Min: 0.95289952
[2025-05-16 03:03:04,241]: Max: 0.98699796
[2025-05-16 03:03:04,242]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-16 03:03:04,247]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.030270345509052277, 0.0, -0.030270345509052277, 0.0, 0.0, 0.0, 0.0, 0.030270345509052277, -0.030270345509052277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030270345509052277, 0.0, 0.0, 0.030270345509052277, 0.0]
[2025-05-16 03:03:04,247]: Mean: -0.00006533
[2025-05-16 03:03:04,247]: Min: -0.03027035
[2025-05-16 03:03:04,247]: Max: 0.03027035
[2025-05-16 03:03:04,247]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-16 03:03:04,248]: Sample Values (25 elements): [0.9675318598747253, 0.9793826341629028, 0.9671686291694641, 0.9791640639305115, 0.9900493025779724, 0.9810815453529358, 0.978033721446991, 0.9704141616821289, 0.9690847396850586, 0.9863306283950806, 0.9774256348609924, 0.9707242250442505, 0.9881956577301025, 0.966137170791626, 0.9791481494903564, 0.9618481397628784, 0.9653645753860474, 0.9806928038597107, 0.9964226484298706, 0.967688262462616, 0.9859151244163513, 0.9679783582687378, 0.9747025370597839, 0.9766285419464111, 0.9837031960487366]
[2025-05-16 03:03:04,248]: Mean: 0.97723478
[2025-05-16 03:03:04,248]: Min: 0.95926356
[2025-05-16 03:03:04,248]: Max: 1.00315249
[2025-05-16 03:03:04,249]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-16 03:03:04,260]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.0276528000831604, 0.0, 0.0, 0.0, 0.0276528000831604, 0.0, 0.0, -0.0276528000831604, 0.0, 0.0276528000831604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0276528000831604, 0.0, 0.0, 0.0, -0.0276528000831604, 0.0, 0.0]
[2025-05-16 03:03:04,261]: Mean: 0.00001960
[2025-05-16 03:03:04,261]: Min: -0.02765280
[2025-05-16 03:03:04,261]: Max: 0.05530560
[2025-05-16 03:03:04,261]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-16 03:03:04,261]: Sample Values (25 elements): [0.9617741107940674, 0.9596933722496033, 0.961329996585846, 0.9607672691345215, 0.9659411907196045, 0.9623266458511353, 0.9604438543319702, 0.9612382650375366, 0.9619532227516174, 0.9627986550331116, 0.9626837968826294, 0.9679287075996399, 0.9596668481826782, 0.9627092480659485, 0.9609488844871521, 0.9629224538803101, 0.9597560167312622, 0.9691175818443298, 0.963338315486908, 0.9623865485191345, 0.959091067314148, 0.9645524024963379, 0.9627513885498047, 0.9620780944824219, 0.9624214172363281]
[2025-05-16 03:03:04,262]: Mean: 0.96166563
[2025-05-16 03:03:04,262]: Min: 0.95652652
[2025-05-16 03:03:04,262]: Max: 0.97327465
[2025-05-16 03:03:04,263]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-16 03:03:04,302]: Sample Values (25 elements): [0.0, -0.019755631685256958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.019755631685256958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019755631685256958, 0.019755631685256958, -0.019755631685256958, 0.0, 0.0, 0.0, 0.019755631685256958, -0.019755631685256958, 0.019755631685256958, -0.019755631685256958, 0.0]
[2025-05-16 03:03:04,302]: Mean: 0.00000454
[2025-05-16 03:03:04,302]: Min: -0.01975563
[2025-05-16 03:03:04,303]: Max: 0.03951126
[2025-05-16 03:03:04,303]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-16 03:03:04,304]: Sample Values (25 elements): [0.9644439816474915, 0.9657506942749023, 0.9628823399543762, 0.9706855416297913, 0.9677356481552124, 0.9687821269035339, 0.960304856300354, 0.9647518396377563, 0.9669767022132874, 0.9829061031341553, 0.9692035913467407, 0.9670536518096924, 0.9660221338272095, 0.9635869264602661, 0.9722793698310852, 0.968607485294342, 0.963603138923645, 0.9679756760597229, 0.967046856880188, 0.962561309337616, 0.9646400809288025, 0.9650802612304688, 0.9685075879096985, 0.9632036089897156, 0.970557451248169]
[2025-05-16 03:03:04,304]: Mean: 0.96741104
[2025-05-16 03:03:04,305]: Min: 0.95876169
[2025-05-16 03:03:04,305]: Max: 0.98290610
[2025-05-16 03:03:04,306]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-16 03:03:04,307]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.05335000157356262, 0.0, 0.05335000157356262, 0.0, -0.05335000157356262, 0.0, 0.0, -0.05335000157356262, 0.05335000157356262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05335000157356262, 0.05335000157356262]
[2025-05-16 03:03:04,307]: Mean: 0.00006268
[2025-05-16 03:03:04,307]: Min: -0.10670000
[2025-05-16 03:03:04,308]: Max: 0.05335000
[2025-05-16 03:03:04,308]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-16 03:03:04,308]: Sample Values (25 elements): [0.9526242017745972, 0.9463221430778503, 0.9539685845375061, 0.9426249861717224, 0.9548713564872742, 0.9526945948600769, 0.9534518122673035, 0.9570985436439514, 0.951173722743988, 0.9530929923057556, 0.9542757868766785, 0.9525266289710999, 0.9530082941055298, 0.9525805115699768, 0.9560626745223999, 0.9485589861869812, 0.9552656412124634, 0.9560806751251221, 0.9527220129966736, 0.9502038955688477, 0.9540582895278931, 0.9569631218910217, 0.9544304609298706, 0.9522190690040588, 0.9578660130500793]
[2025-05-16 03:03:04,308]: Mean: 0.95298731
[2025-05-16 03:03:04,308]: Min: 0.94159114
[2025-05-16 03:03:04,308]: Max: 0.96290743
[2025-05-16 03:03:04,310]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-16 03:03:04,352]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.020566489547491074, 0.020566489547491074, -0.020566489547491074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020566489547491074, 0.0, 0.0, 0.0]
[2025-05-16 03:03:04,352]: Mean: -0.00000872
[2025-05-16 03:03:04,353]: Min: -0.02056649
[2025-05-16 03:03:04,353]: Max: 0.04113298
[2025-05-16 03:03:04,353]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-16 03:03:04,353]: Sample Values (25 elements): [0.9604219198226929, 0.9610430598258972, 0.9630703330039978, 0.960651159286499, 0.9656612873077393, 0.960245668888092, 0.9651067852973938, 0.962454080581665, 0.9595468044281006, 0.9624281525611877, 0.9609060883522034, 0.9614323377609253, 0.9597896933555603, 0.9620586037635803, 0.9590354561805725, 0.9609204530715942, 0.9618242383003235, 0.9605422019958496, 0.9628007411956787, 0.961172878742218, 0.9592424035072327, 0.9623959064483643, 0.9621719121932983, 0.9609124064445496, 0.9623750448226929]
[2025-05-16 03:03:04,353]: Mean: 0.96170628
[2025-05-16 03:03:04,354]: Min: 0.95694041
[2025-05-16 03:03:04,354]: Max: 0.96928841
[2025-05-16 03:03:04,355]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-16 03:03:04,388]: Sample Values (25 elements): [0.0, -0.01605493575334549, 0.01605493575334549, -0.01605493575334549, 0.01605493575334549, 0.0, 0.0, -0.01605493575334549, 0.0, 0.0, -0.01605493575334549, -0.01605493575334549, 0.0, 0.01605493575334549, 0.01605493575334549, 0.0, 0.0, 0.0, -0.01605493575334549, 0.0, -0.01605493575334549, 0.0, 0.0, 0.0, -0.01605493575334549]
[2025-05-16 03:03:04,388]: Mean: 0.00000695
[2025-05-16 03:03:04,388]: Min: -0.03210987
[2025-05-16 03:03:04,388]: Max: 0.01605494
[2025-05-16 03:03:04,388]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-16 03:03:04,389]: Sample Values (25 elements): [0.9661444425582886, 0.9641793370246887, 0.9685020446777344, 0.9669224619865417, 0.9694404602050781, 0.9683794975280762, 0.9673206806182861, 0.9711626768112183, 0.9666396379470825, 0.9727107882499695, 0.9719159603118896, 0.9676784873008728, 0.9654549956321716, 0.9667138457298279, 0.96331387758255, 0.9685170650482178, 0.965051531791687, 0.9713985323905945, 0.9698078036308289, 0.9721091389656067, 0.9740774631500244, 0.964361310005188, 0.9661288261413574, 0.9652243256568909, 0.9680852293968201]
[2025-05-16 03:03:04,389]: Mean: 0.96783286
[2025-05-16 03:03:04,389]: Min: 0.96160364
[2025-05-16 03:03:04,389]: Max: 0.97610581
[2025-05-16 03:03:04,389]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-16 03:03:04,390]: Sample Values (25 elements): [-0.001111721619963646, -0.029845301061868668, -0.1268676370382309, 0.07038236409425735, 0.08938390761613846, 0.024201519787311554, 0.0506003238260746, 0.08730597794055939, -0.07976324111223221, 0.005599132273346186, -0.01675286889076233, -0.07178223878145218, 0.01639801263809204, 0.02224244363605976, 0.0006741730612702668, -0.038844186812639236, -0.03484230116009712, 0.03812618926167488, -0.04807068780064583, 0.020948227494955063, -0.06035200133919716, 0.0523642934858799, -0.013170396909117699, 0.05183922126889229, -0.08486475795507431]
[2025-05-16 03:03:04,390]: Mean: 0.00078901
[2025-05-16 03:03:04,390]: Min: -0.13108997
[2025-05-16 03:03:04,390]: Max: 0.13396232
