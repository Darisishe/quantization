[2025-05-21 13:29:41,278]: Checkpoint of model at path [checkpoint/ResNet18_hardtanh.ckpt] will be used for QAT
[2025-05-21 13:29:41,278]: 


QAT of ResNet18 with parametrized_hardtanh down to 2 bits...
[2025-05-21 13:29:41,504]: [ResNet18_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-21 13:29:41,530]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 13:31:27,698]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 2.3200 Train Acc: 0.1053 Eval Loss: 2.3028 Eval Acc: 0.1000 (LR: 0.010000)
[2025-05-21 13:33:13,600]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 2.3038 Train Acc: 0.1000 Eval Loss: 2.3035 Eval Acc: 0.1000 (LR: 0.010000)
[2025-05-21 13:34:59,522]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 2.3038 Train Acc: 0.0987 Eval Loss: 2.3034 Eval Acc: 0.1000 (LR: 0.010000)
[2025-05-21 13:36:45,281]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 2.3034 Train Acc: 0.0977 Eval Loss: 2.3034 Eval Acc: 0.1000 (LR: 0.010000)
[2025-05-21 13:38:31,079]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 2.0450 Train Acc: 0.2119 Eval Loss: 1.3212 Eval Acc: 0.5243 (LR: 0.010000)
[2025-05-21 13:40:16,958]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 1.0357 Train Acc: 0.6323 Eval Loss: 1.0513 Eval Acc: 0.6441 (LR: 0.010000)
[2025-05-21 13:42:02,847]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.8464 Train Acc: 0.7012 Eval Loss: 0.9785 Eval Acc: 0.6652 (LR: 0.010000)
[2025-05-21 13:43:48,842]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.7697 Train Acc: 0.7306 Eval Loss: 0.8510 Eval Acc: 0.7164 (LR: 0.010000)
[2025-05-21 13:45:34,753]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.7168 Train Acc: 0.7496 Eval Loss: 0.7777 Eval Acc: 0.7411 (LR: 0.010000)
[2025-05-21 13:47:20,657]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.6847 Train Acc: 0.7622 Eval Loss: 0.8693 Eval Acc: 0.7139 (LR: 0.010000)
[2025-05-21 13:49:06,475]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.6616 Train Acc: 0.7681 Eval Loss: 0.7164 Eval Acc: 0.7619 (LR: 0.010000)
[2025-05-21 13:50:52,426]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.6444 Train Acc: 0.7758 Eval Loss: 0.7141 Eval Acc: 0.7627 (LR: 0.010000)
[2025-05-21 13:52:38,276]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.6164 Train Acc: 0.7842 Eval Loss: 0.8619 Eval Acc: 0.7292 (LR: 0.010000)
[2025-05-21 13:54:24,200]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.6018 Train Acc: 0.7904 Eval Loss: 0.7511 Eval Acc: 0.7583 (LR: 0.010000)
[2025-05-21 13:56:10,138]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.5842 Train Acc: 0.7970 Eval Loss: 0.7075 Eval Acc: 0.7642 (LR: 0.001000)
[2025-05-21 13:57:56,101]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.4524 Train Acc: 0.8417 Eval Loss: 0.5155 Eval Acc: 0.8312 (LR: 0.001000)
[2025-05-21 13:59:42,284]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.4270 Train Acc: 0.8509 Eval Loss: 0.5207 Eval Acc: 0.8323 (LR: 0.001000)
[2025-05-21 14:01:28,462]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.4181 Train Acc: 0.8550 Eval Loss: 0.4969 Eval Acc: 0.8374 (LR: 0.001000)
[2025-05-21 14:03:14,762]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.4063 Train Acc: 0.8574 Eval Loss: 0.5090 Eval Acc: 0.8354 (LR: 0.001000)
[2025-05-21 14:05:01,997]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.4065 Train Acc: 0.8582 Eval Loss: 0.5128 Eval Acc: 0.8345 (LR: 0.001000)
[2025-05-21 14:06:48,258]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.3985 Train Acc: 0.8600 Eval Loss: 0.5203 Eval Acc: 0.8323 (LR: 0.001000)
[2025-05-21 14:08:34,482]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.3934 Train Acc: 0.8627 Eval Loss: 0.5078 Eval Acc: 0.8355 (LR: 0.001000)
[2025-05-21 14:10:20,662]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.3897 Train Acc: 0.8637 Eval Loss: 0.5066 Eval Acc: 0.8338 (LR: 0.001000)
[2025-05-21 14:12:06,615]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.3833 Train Acc: 0.8659 Eval Loss: 0.5128 Eval Acc: 0.8321 (LR: 0.001000)
[2025-05-21 14:13:52,524]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.3893 Train Acc: 0.8641 Eval Loss: 0.5328 Eval Acc: 0.8308 (LR: 0.001000)
[2025-05-21 14:15:38,270]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.3828 Train Acc: 0.8651 Eval Loss: 0.5304 Eval Acc: 0.8318 (LR: 0.001000)
[2025-05-21 14:17:23,801]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.3792 Train Acc: 0.8669 Eval Loss: 0.5122 Eval Acc: 0.8359 (LR: 0.001000)
[2025-05-21 14:19:09,576]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.3722 Train Acc: 0.8684 Eval Loss: 0.5159 Eval Acc: 0.8371 (LR: 0.001000)
[2025-05-21 14:20:55,308]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.3741 Train Acc: 0.8679 Eval Loss: 0.5107 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-21 14:22:41,240]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.3717 Train Acc: 0.8694 Eval Loss: 0.4969 Eval Acc: 0.8423 (LR: 0.000100)
[2025-05-21 14:24:27,184]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.3400 Train Acc: 0.8801 Eval Loss: 0.4863 Eval Acc: 0.8460 (LR: 0.000100)
[2025-05-21 14:26:13,103]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.3378 Train Acc: 0.8806 Eval Loss: 0.4846 Eval Acc: 0.8444 (LR: 0.000100)
[2025-05-21 14:27:59,025]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.3365 Train Acc: 0.8811 Eval Loss: 0.4803 Eval Acc: 0.8506 (LR: 0.000100)
[2025-05-21 14:29:44,952]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.3354 Train Acc: 0.8823 Eval Loss: 0.4765 Eval Acc: 0.8453 (LR: 0.000100)
[2025-05-21 14:31:30,742]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.3360 Train Acc: 0.8824 Eval Loss: 0.4773 Eval Acc: 0.8465 (LR: 0.000100)
[2025-05-21 14:33:16,617]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.3359 Train Acc: 0.8806 Eval Loss: 0.4828 Eval Acc: 0.8480 (LR: 0.000100)
[2025-05-21 14:35:02,360]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.3349 Train Acc: 0.8825 Eval Loss: 0.4919 Eval Acc: 0.8439 (LR: 0.000100)
[2025-05-21 14:36:48,300]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.3334 Train Acc: 0.8835 Eval Loss: 0.4807 Eval Acc: 0.8466 (LR: 0.000100)
[2025-05-21 14:38:34,222]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.3342 Train Acc: 0.8826 Eval Loss: 0.4955 Eval Acc: 0.8485 (LR: 0.000100)
[2025-05-21 14:40:21,285]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.3301 Train Acc: 0.8835 Eval Loss: 0.4903 Eval Acc: 0.8456 (LR: 0.000100)
[2025-05-21 14:42:07,333]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.3350 Train Acc: 0.8809 Eval Loss: 0.4782 Eval Acc: 0.8520 (LR: 0.000100)
[2025-05-21 14:43:53,540]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.3309 Train Acc: 0.8837 Eval Loss: 0.4799 Eval Acc: 0.8489 (LR: 0.000100)
[2025-05-21 14:45:39,398]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.3290 Train Acc: 0.8837 Eval Loss: 0.4838 Eval Acc: 0.8471 (LR: 0.000100)
[2025-05-21 14:47:24,898]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.3309 Train Acc: 0.8833 Eval Loss: 0.4851 Eval Acc: 0.8454 (LR: 0.000100)
[2025-05-21 14:49:10,628]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.3281 Train Acc: 0.8830 Eval Loss: 0.4943 Eval Acc: 0.8470 (LR: 0.000010)
[2025-05-21 14:50:56,206]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.3261 Train Acc: 0.8841 Eval Loss: 0.4867 Eval Acc: 0.8466 (LR: 0.000010)
[2025-05-21 14:52:42,083]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.3213 Train Acc: 0.8877 Eval Loss: 0.4817 Eval Acc: 0.8491 (LR: 0.000010)
[2025-05-21 14:54:28,078]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.3208 Train Acc: 0.8867 Eval Loss: 0.4826 Eval Acc: 0.8474 (LR: 0.000010)
[2025-05-21 14:56:14,024]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.3239 Train Acc: 0.8865 Eval Loss: 0.4798 Eval Acc: 0.8479 (LR: 0.000010)
[2025-05-21 14:58:00,146]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.3231 Train Acc: 0.8865 Eval Loss: 0.4843 Eval Acc: 0.8476 (LR: 0.000010)
[2025-05-21 14:59:45,902]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.3226 Train Acc: 0.8872 Eval Loss: 0.4778 Eval Acc: 0.8465 (LR: 0.000010)
[2025-05-21 15:01:31,925]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.3238 Train Acc: 0.8849 Eval Loss: 0.4844 Eval Acc: 0.8459 (LR: 0.000010)
[2025-05-21 15:03:17,831]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.3206 Train Acc: 0.8871 Eval Loss: 0.4765 Eval Acc: 0.8496 (LR: 0.000010)
[2025-05-21 15:05:03,807]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.3231 Train Acc: 0.8867 Eval Loss: 0.4759 Eval Acc: 0.8555 (LR: 0.000010)
[2025-05-21 15:06:49,641]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.3244 Train Acc: 0.8860 Eval Loss: 0.4860 Eval Acc: 0.8474 (LR: 0.000010)
[2025-05-21 15:08:35,618]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.3184 Train Acc: 0.8884 Eval Loss: 0.4838 Eval Acc: 0.8464 (LR: 0.000010)
[2025-05-21 15:10:21,501]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.3200 Train Acc: 0.8864 Eval Loss: 0.4777 Eval Acc: 0.8510 (LR: 0.000010)
[2025-05-21 15:12:07,447]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.3227 Train Acc: 0.8883 Eval Loss: 0.4910 Eval Acc: 0.8484 (LR: 0.000010)
[2025-05-21 15:13:53,372]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.3257 Train Acc: 0.8856 Eval Loss: 0.4818 Eval Acc: 0.8470 (LR: 0.000010)
[2025-05-21 15:15:40,443]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.3215 Train Acc: 0.8874 Eval Loss: 0.4791 Eval Acc: 0.8484 (LR: 0.000010)
[2025-05-21 15:15:40,443]: [ResNet18_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8555
[2025-05-21 15:15:40,540]: 


Quantization of model down to 2 bits finished
[2025-05-21 15:15:40,540]: Model Architecture:
[2025-05-21 15:15:40,585]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1231], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19131392240524292, max_val=0.1781070977449417)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5043], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2192], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2974061965942383, max_val=0.3600841760635376)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5457], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1656], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2676960229873657, max_val=0.2292008250951767)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8302], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1047], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15699589252471924, max_val=0.15698014199733734)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8448], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1149], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18374678492546082, max_val=0.16109579801559448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1792], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0533], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07889479398727417, max_val=0.08112390339374542)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1467], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2202267050743103, max_val=0.21974273025989532)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1291], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0502], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0761900395154953, max_val=0.07451988756656647)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5846], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0474], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0678916871547699, max_val=0.07428351789712906)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5591], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0438], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0685625821352005, max_val=0.06293146312236786)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7766], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0381], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05704197287559509, max_val=0.057275980710983276)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0909], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1365853101015091, max_val=0.13596555590629578)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0810], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0378], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.057795796543359756, max_val=0.05554148182272911)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6161], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0363], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05564354360103607, max_val=0.05325205624103546)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3404], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0297], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0447070337831974, max_val=0.04429617524147034)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8310], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0254], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03938617929816246, max_val=0.036750052124261856)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0573], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08560368418693542, max_val=0.08635453879833221)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1337], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0251], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03678615763783455, max_val=0.03844747692346573)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4819], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0182], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02790776640176773, max_val=0.02657018043100834)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2394], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 15:15:40,585]: 
Model Weights:
[2025-05-21 15:15:40,585]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-21 15:15:40,586]: Sample Values (25 elements): [0.21104855835437775, 0.2102925181388855, -0.07404060661792755, -0.23186789453029633, 0.07732204347848892, 0.07188417762517929, -0.04324638471007347, -0.34443339705467224, 0.01364833489060402, 0.15129755437374115, 0.1503027379512787, 0.12201763689517975, 0.15833015739917755, 0.04105095565319061, -0.3002677857875824, 0.029743505641818047, 0.03463379293680191, 0.056517135351896286, -0.16545844078063965, 0.1835596114397049, -0.185362309217453, -0.10797020047903061, 0.1766340583562851, -0.12702687084674835, -0.08774358779191971]
[2025-05-21 15:15:40,586]: Mean: -0.00085190
[2025-05-21 15:15:40,586]: Min: -0.46436813
[2025-05-21 15:15:40,586]: Max: 0.44998077
[2025-05-21 15:15:40,586]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-21 15:15:40,586]: Sample Values (25 elements): [0.896773636341095, 0.8172234296798706, 0.9878273010253906, 0.6645981073379517, 0.8966872096061707, 0.8308266997337341, 0.8671765327453613, 1.1174534559249878, 1.082604169845581, 0.8850947022438049, 0.8646594882011414, 1.1781482696533203, 1.0924922227859497, 0.874444305896759, 0.7277130484580994, 0.8214322924613953, 0.9076310396194458, 1.0364594459533691, 0.9548261165618896, 0.8779506087303162, 0.8863505721092224, 0.986650824546814, 0.9317535758018494, 1.1629753112792969, 0.7894201278686523]
[2025-05-21 15:15:40,587]: Mean: 0.93576527
[2025-05-21 15:15:40,587]: Min: 0.66459811
[2025-05-21 15:15:40,587]: Max: 1.20461285
[2025-05-21 15:15:40,588]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 15:15:40,588]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.12314033508300781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,589]: Mean: 0.00052110
[2025-05-21 15:15:40,589]: Min: -0.24628067
[2025-05-21 15:15:40,589]: Max: 0.12314034
[2025-05-21 15:15:40,589]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 15:15:40,589]: Sample Values (25 elements): [0.9055308699607849, 1.0686767101287842, 1.021845817565918, 1.0733250379562378, 0.9816389679908752, 0.88986736536026, 0.9479014873504639, 0.971580982208252, 1.0197956562042236, 0.956071138381958, 0.8728034496307373, 0.9160414934158325, 0.9063871502876282, 0.9827175736427307, 0.8828012347221375, 0.8964341282844543, 0.8659654855728149, 0.9163391590118408, 0.8865418434143066, 0.9404214024543762, 1.058859944343567, 0.9986938834190369, 0.9998401403427124, 1.0834174156188965, 0.9998403787612915]
[2025-05-21 15:15:40,589]: Mean: 0.96979606
[2025-05-21 15:15:40,589]: Min: 0.83958375
[2025-05-21 15:15:40,590]: Max: 1.16455841
[2025-05-21 15:15:40,591]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 15:15:40,591]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,591]: Mean: -0.00036860
[2025-05-21 15:15:40,591]: Min: -0.21916346
[2025-05-21 15:15:40,592]: Max: 0.43832693
[2025-05-21 15:15:40,592]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 15:15:40,592]: Sample Values (25 elements): [1.5765153169631958, 0.8594040870666504, 0.9269939064979553, 0.9462977647781372, 0.8576407432556152, 0.9276373982429504, 0.9676199555397034, 1.0332238674163818, 0.9441071152687073, 0.844261884689331, 0.8998419046401978, 0.9060806035995483, 0.9310728907585144, 0.8052015900611877, 0.8895290493965149, 0.9575396776199341, 0.8927856683731079, 0.833379328250885, 1.0516852140426636, 0.8689613342285156, 0.906927764415741, 0.8539431691169739, 1.2483209371566772, 0.8072534203529358, 0.910784900188446]
[2025-05-21 15:15:40,592]: Mean: 0.92380118
[2025-05-21 15:15:40,592]: Min: 0.78647906
[2025-05-21 15:15:40,592]: Max: 1.57651532
[2025-05-21 15:15:40,593]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 15:15:40,594]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,594]: Mean: 0.00024712
[2025-05-21 15:15:40,594]: Min: -0.33126459
[2025-05-21 15:15:40,594]: Max: 0.16563229
[2025-05-21 15:15:40,594]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 15:15:40,594]: Sample Values (25 elements): [1.0113189220428467, 0.9085183143615723, 0.9438483119010925, 0.9496546387672424, 0.9133689999580383, 1.009826898574829, 0.9585328698158264, 0.9305393099784851, 0.9638280868530273, 0.9277527928352356, 0.8983065485954285, 0.9073776602745056, 0.9633473753929138, 0.9682060480117798, 0.9087597727775574, 0.9509533643722534, 0.9905224442481995, 0.9535000324249268, 0.9318708181381226, 0.918971061706543, 1.031334400177002, 0.8936926126480103, 0.9617125988006592, 0.9362218976020813, 0.9345995187759399]
[2025-05-21 15:15:40,595]: Mean: 0.94194865
[2025-05-21 15:15:40,595]: Min: 0.87924927
[2025-05-21 15:15:40,595]: Max: 1.03133440
[2025-05-21 15:15:40,596]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 15:15:40,596]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10465867817401886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,597]: Mean: -0.00010221
[2025-05-21 15:15:40,597]: Min: -0.20931736
[2025-05-21 15:15:40,597]: Max: 0.10465868
[2025-05-21 15:15:40,597]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 15:15:40,597]: Sample Values (25 elements): [0.9507125020027161, 0.9770746231079102, 1.0992460250854492, 1.0539807081222534, 0.9077834486961365, 0.9030742645263672, 0.9289295077323914, 0.9625101089477539, 0.9899250864982605, 1.0412888526916504, 1.0150389671325684, 1.06924307346344, 0.9619073867797852, 0.9592090249061584, 0.8754385709762573, 0.9792953133583069, 0.9344401359558105, 0.9101342558860779, 0.9164899587631226, 0.9244354367256165, 0.9855536222457886, 0.9314674139022827, 0.9823175668716431, 0.923764705657959, 0.919091522693634]
[2025-05-21 15:15:40,597]: Mean: 0.97681797
[2025-05-21 15:15:40,597]: Min: 0.87543857
[2025-05-21 15:15:40,598]: Max: 1.55055785
[2025-05-21 15:15:40,599]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-21 15:15:40,599]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11494752019643784, 0.0, -0.11494752019643784, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,599]: Mean: 0.00013096
[2025-05-21 15:15:40,600]: Min: -0.22989504
[2025-05-21 15:15:40,600]: Max: 0.11494752
[2025-05-21 15:15:40,600]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-21 15:15:40,600]: Sample Values (25 elements): [0.942422091960907, 0.9118280410766602, 0.9392374753952026, 0.9657973051071167, 0.9381455183029175, 0.9258692264556885, 0.9077340364456177, 0.9329221844673157, 0.91217440366745, 0.9204558730125427, 0.9269914031028748, 0.9208080768585205, 0.9158069491386414, 0.9314910173416138, 0.9477105140686035, 0.9284791946411133, 0.9208657741546631, 0.9630997776985168, 0.9192675352096558, 0.9151068925857544, 0.9352623820304871, 0.9782100915908813, 0.9257093667984009, 0.9318178296089172, 0.9143223166465759]
[2025-05-21 15:15:40,600]: Mean: 0.92937315
[2025-05-21 15:15:40,600]: Min: 0.89767772
[2025-05-21 15:15:40,601]: Max: 1.01748049
[2025-05-21 15:15:40,601]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 15:15:40,603]: Sample Values (25 elements): [-0.05333956331014633, 0.05333956331014633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05333956331014633, 0.0, -0.05333956331014633, 0.05333956331014633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,603]: Mean: -0.00001121
[2025-05-21 15:15:40,603]: Min: -0.05333956
[2025-05-21 15:15:40,603]: Max: 0.10667913
[2025-05-21 15:15:40,603]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-21 15:15:40,604]: Sample Values (25 elements): [0.9317315220832825, 0.9633826017379761, 0.9582644701004028, 0.91914963722229, 0.9311060905456543, 0.9092733263969421, 0.9399632215499878, 0.9288199543952942, 0.8934037089347839, 0.8998048305511475, 0.9496758580207825, 0.9230705499649048, 0.9097601175308228, 0.9502842426300049, 0.9291471242904663, 0.9449229836463928, 0.9271671772003174, 0.916700005531311, 0.92294842004776, 0.9685847163200378, 0.9373193383216858, 0.9368727207183838, 0.9258798956871033, 0.9143624305725098, 0.9279274940490723]
[2025-05-21 15:15:40,604]: Mean: 0.92994308
[2025-05-21 15:15:40,604]: Min: 0.89340371
[2025-05-21 15:15:40,604]: Max: 0.99767387
[2025-05-21 15:15:40,605]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-21 15:15:40,605]: Sample Values (25 elements): [0.0, 0.0, -0.14665648341178894, -0.14665648341178894, 0.14665648341178894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14665648341178894, -0.14665648341178894, 0.14665648341178894, 0.0, 0.0, 0.0, 0.0, 0.0, -0.14665648341178894, -0.14665648341178894, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,605]: Mean: -0.00044756
[2025-05-21 15:15:40,606]: Min: -0.29331297
[2025-05-21 15:15:40,606]: Max: 0.14665648
[2025-05-21 15:15:40,606]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-21 15:15:40,606]: Sample Values (25 elements): [0.9064087867736816, 0.8339195251464844, 0.8419033885002136, 0.8962692618370056, 0.8788127899169922, 0.9536980390548706, 0.9058786630630493, 0.8765106797218323, 0.8791190385818481, 0.8571240305900574, 0.8407869935035706, 0.8645622730255127, 0.8875893950462341, 0.8598297834396362, 0.8967071175575256, 0.8713567852973938, 0.8729881644248962, 0.8982659578323364, 0.887260377407074, 0.8999366164207458, 0.8815637826919556, 0.864774227142334, 0.8861725926399231, 0.8565144538879395, 0.8848416805267334]
[2025-05-21 15:15:40,606]: Mean: 0.88622409
[2025-05-21 15:15:40,606]: Min: 0.83391953
[2025-05-21 15:15:40,606]: Max: 0.95369804
[2025-05-21 15:15:40,607]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 15:15:40,609]: Sample Values (25 elements): [0.0, 0.0, 0.05023663863539696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05023663863539696, 0.0, 0.0, 0.0, 0.0, -0.05023663863539696, 0.0, 0.0, 0.0, -0.05023663863539696, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,609]: Mean: -0.00006371
[2025-05-21 15:15:40,609]: Min: -0.10047328
[2025-05-21 15:15:40,609]: Max: 0.05023664
[2025-05-21 15:15:40,609]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-21 15:15:40,609]: Sample Values (25 elements): [0.9480111002922058, 0.9435731768608093, 0.9049633741378784, 0.9268490672111511, 0.9136751294136047, 0.9138472080230713, 0.910592794418335, 0.9392585158348083, 0.9177656769752502, 0.9229511022567749, 0.9318058490753174, 0.9582209587097168, 0.9403005242347717, 0.9213959574699402, 0.9401739835739136, 0.9216209650039673, 0.9427591562271118, 0.9092696905136108, 0.9319554567337036, 0.9305251836776733, 0.9186485409736633, 0.9329198598861694, 0.9200608730316162, 0.9102045893669128, 0.9184045791625977]
[2025-05-21 15:15:40,610]: Mean: 0.92702937
[2025-05-21 15:15:40,610]: Min: 0.90290993
[2025-05-21 15:15:40,610]: Max: 0.97059864
[2025-05-21 15:15:40,611]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 15:15:40,612]: Sample Values (25 elements): [0.0, 0.04739173501729965, 0.0, -0.04739173501729965, 0.0, 0.0, 0.0, 0.0, 0.04739173501729965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04739173501729965, 0.0, 0.0, -0.04739173501729965, 0.0, 0.0, 0.04739173501729965, 0.0]
[2025-05-21 15:15:40,613]: Mean: 0.00008646
[2025-05-21 15:15:40,613]: Min: -0.04739174
[2025-05-21 15:15:40,613]: Max: 0.09478347
[2025-05-21 15:15:40,613]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-21 15:15:40,613]: Sample Values (25 elements): [0.9419512152671814, 0.9509466886520386, 0.9390016794204712, 0.9585086703300476, 0.9493920207023621, 0.9458931684494019, 0.9433234333992004, 0.9416010975837708, 0.985910952091217, 0.9476041197776794, 0.9746779203414917, 0.9419195055961609, 0.9476163983345032, 0.9456095695495605, 0.9345127940177917, 0.954383373260498, 0.9832974672317505, 0.9547423124313354, 0.9601596593856812, 0.9672375917434692, 0.9577410221099854, 0.9347524046897888, 0.943813145160675, 0.9275436401367188, 0.9707606434822083]
[2025-05-21 15:15:40,613]: Mean: 0.95249641
[2025-05-21 15:15:40,613]: Min: 0.91211873
[2025-05-21 15:15:40,614]: Max: 1.01186621
[2025-05-21 15:15:40,615]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-21 15:15:40,618]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043831340968608856, 0.0, -0.043831340968608856, 0.0, 0.0, 0.0, -0.043831340968608856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043831340968608856, 0.0, 0.0]
[2025-05-21 15:15:40,618]: Mean: 0.00011905
[2025-05-21 15:15:40,618]: Min: -0.08766268
[2025-05-21 15:15:40,618]: Max: 0.04383134
[2025-05-21 15:15:40,618]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-21 15:15:40,618]: Sample Values (25 elements): [0.9215953350067139, 0.9174183011054993, 0.9160258769989014, 0.9278594851493835, 0.9196691513061523, 0.9277054667472839, 0.9258931875228882, 0.9124629497528076, 0.9253867864608765, 0.9351413249969482, 0.9126912951469421, 0.9168004393577576, 0.9223071336746216, 0.9185757040977478, 0.9168765544891357, 0.9279590845108032, 0.9232229590415955, 0.9103673100471497, 0.927437424659729, 0.9039859175682068, 0.9181571006774902, 0.9097144603729248, 0.9151818156242371, 0.9215884208679199, 0.9177666306495667]
[2025-05-21 15:15:40,619]: Mean: 0.91942072
[2025-05-21 15:15:40,619]: Min: 0.90123302
[2025-05-21 15:15:40,619]: Max: 0.94687045
[2025-05-21 15:15:40,620]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 15:15:40,625]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.038105983287096024, 0.0, 0.0, 0.0, 0.0, 0.038105983287096024, 0.0, 0.038105983287096024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,625]: Mean: 0.00003967
[2025-05-21 15:15:40,625]: Min: -0.03810598
[2025-05-21 15:15:40,625]: Max: 0.07621197
[2025-05-21 15:15:40,626]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-21 15:15:40,626]: Sample Values (25 elements): [0.921553373336792, 0.9470182061195374, 0.9374410510063171, 0.9112246632575989, 0.9260875582695007, 0.9236624836921692, 0.930814802646637, 0.9319943189620972, 0.8985938429832458, 0.9190160036087036, 0.9230769872665405, 0.9233068227767944, 0.9277516603469849, 0.924144983291626, 0.9443755745887756, 0.9382081031799316, 0.9449076056480408, 0.9139931797981262, 0.9316144585609436, 0.9288052916526794, 0.9434163570404053, 0.9396470785140991, 0.9306756258010864, 0.9191279411315918, 0.9278165698051453]
[2025-05-21 15:15:40,626]: Mean: 0.92900431
[2025-05-21 15:15:40,626]: Min: 0.89859384
[2025-05-21 15:15:40,626]: Max: 0.95962548
[2025-05-21 15:15:40,627]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-21 15:15:40,628]: Sample Values (25 elements): [-0.09085029363632202, 0.09085029363632202, 0.0, 0.0, 0.09085029363632202, 0.09085029363632202, 0.09085029363632202, -0.09085029363632202, 0.0, 0.0, 0.0, 0.0, -0.09085029363632202, -0.09085029363632202, 0.0, 0.0, -0.09085029363632202, 0.09085029363632202, 0.0, 0.09085029363632202, 0.0, 0.0, -0.09085029363632202, 0.0, -0.09085029363632202]
[2025-05-21 15:15:40,628]: Mean: -0.00044083
[2025-05-21 15:15:40,628]: Min: -0.18170059
[2025-05-21 15:15:40,628]: Max: 0.09085029
[2025-05-21 15:15:40,628]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-21 15:15:40,629]: Sample Values (25 elements): [0.8900308012962341, 0.9048202037811279, 0.8881458044052124, 0.9039608240127563, 0.8932733535766602, 0.8901078701019287, 0.8811758756637573, 0.886816143989563, 0.902847409248352, 0.8792875409126282, 0.8936102986335754, 0.8849325776100159, 0.888005793094635, 0.8858804702758789, 0.8675048351287842, 0.8958561420440674, 0.8990288972854614, 0.8921151161193848, 0.8840125799179077, 0.8965373635292053, 0.8952997326850891, 0.8857570290565491, 0.88551926612854, 0.8891501426696777, 0.8878117799758911]
[2025-05-21 15:15:40,629]: Mean: 0.89110959
[2025-05-21 15:15:40,629]: Min: 0.85648239
[2025-05-21 15:15:40,629]: Max: 0.92668772
[2025-05-21 15:15:40,630]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 15:15:40,635]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.03777909278869629, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03777909278869629, 0.0, 0.0, -0.03777909278869629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03777909278869629, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,636]: Mean: -0.00005630
[2025-05-21 15:15:40,636]: Min: -0.07555819
[2025-05-21 15:15:40,636]: Max: 0.03777909
[2025-05-21 15:15:40,636]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-21 15:15:40,636]: Sample Values (25 elements): [0.9178929924964905, 0.9358243942260742, 0.9237868189811707, 0.9251438975334167, 0.9288190603256226, 0.9148725867271423, 0.9180476665496826, 0.9243985414505005, 0.9174191951751709, 0.9294049739837646, 0.914117693901062, 0.9238017201423645, 0.9553384780883789, 0.9164109230041504, 0.9214074611663818, 0.9241353869438171, 0.9187421202659607, 0.9198159575462341, 0.9177298545837402, 0.916470468044281, 0.9481968283653259, 0.9177936911582947, 0.9245400428771973, 0.9169080257415771, 0.9202913045883179]
[2025-05-21 15:15:40,636]: Mean: 0.92302191
[2025-05-21 15:15:40,637]: Min: 0.90551561
[2025-05-21 15:15:40,637]: Max: 0.97088796
[2025-05-21 15:15:40,638]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 15:15:40,644]: Sample Values (25 elements): [0.03629852831363678, 0.0, 0.0, 0.0, -0.03629852831363678, 0.0, 0.0, 0.03629852831363678, 0.0, 0.0, 0.0, -0.03629852831363678, 0.0, 0.03629852831363678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03629852831363678, 0.0, 0.03629852831363678, 0.0]
[2025-05-21 15:15:40,644]: Mean: 0.00000394
[2025-05-21 15:15:40,644]: Min: -0.07259706
[2025-05-21 15:15:40,644]: Max: 0.03629853
[2025-05-21 15:15:40,644]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-21 15:15:40,645]: Sample Values (25 elements): [0.9333826303482056, 0.9293681979179382, 0.9478767514228821, 0.9415000677108765, 0.9275045394897461, 0.9355871081352234, 0.9419875144958496, 0.9549954533576965, 0.9433786869049072, 0.9223924875259399, 0.943620502948761, 0.9581888318061829, 0.9444146752357483, 0.9362971782684326, 0.9205306768417358, 0.9266611933708191, 0.9282242059707642, 0.9306004643440247, 0.9376553893089294, 0.9575120210647583, 0.9260452389717102, 0.9286885261535645, 0.9525619745254517, 0.9409090876579285, 0.9243981838226318]
[2025-05-21 15:15:40,645]: Mean: 0.93864954
[2025-05-21 15:15:40,645]: Min: 0.90841335
[2025-05-21 15:15:40,645]: Max: 0.98765326
[2025-05-21 15:15:40,646]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-21 15:15:40,661]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.02966773882508278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.02966773882508278, -0.02966773882508278, 0.02966773882508278, 0.0, -0.02966773882508278, 0.0, 0.02966773882508278, -0.02966773882508278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 15:15:40,661]: Mean: -0.00001514
[2025-05-21 15:15:40,662]: Min: -0.05933548
[2025-05-21 15:15:40,662]: Max: 0.02966774
[2025-05-21 15:15:40,662]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-21 15:15:40,662]: Sample Values (25 elements): [0.9160808324813843, 0.9106323719024658, 0.908589243888855, 0.9189153909683228, 0.9125007390975952, 0.9232483506202698, 0.9211110472679138, 0.9187386631965637, 0.912187933921814, 0.9199235439300537, 0.9249529838562012, 0.9137547016143799, 0.9089639782905579, 0.9197977781295776, 0.9108458757400513, 0.9095345735549927, 0.9219873547554016, 0.9124759435653687, 0.9184783697128296, 0.9176969528198242, 0.9085038900375366, 0.9086278080940247, 0.9184946417808533, 0.9272124767303467, 0.9154873490333557]
[2025-05-21 15:15:40,662]: Mean: 0.91678190
[2025-05-21 15:15:40,662]: Min: 0.90696639
[2025-05-21 15:15:40,663]: Max: 0.93663776
[2025-05-21 15:15:40,664]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 15:15:40,699]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.025378748774528503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025378748774528503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.025378748774528503, 0.0]
[2025-05-21 15:15:40,699]: Mean: -0.00000186
[2025-05-21 15:15:40,700]: Min: -0.05075750
[2025-05-21 15:15:40,700]: Max: 0.02537875
[2025-05-21 15:15:40,700]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-21 15:15:40,700]: Sample Values (25 elements): [0.923400342464447, 0.9208213686943054, 0.9312841892242432, 0.9121406674385071, 0.9288026094436646, 0.932429313659668, 0.9135874509811401, 0.9201641082763672, 0.9189562201499939, 0.9262440800666809, 0.91814786195755, 0.9183679223060608, 0.9315661191940308, 0.9191595911979675, 0.925783634185791, 0.9242709279060364, 0.9310122132301331, 0.9410179853439331, 0.9256305694580078, 0.9344876408576965, 0.9334865808486938, 0.9191948175430298, 0.9340611100196838, 0.922054648399353, 0.9205411672592163]
[2025-05-21 15:15:40,700]: Mean: 0.92368466
[2025-05-21 15:15:40,700]: Min: 0.90915340
[2025-05-21 15:15:40,701]: Max: 0.95536649
[2025-05-21 15:15:40,702]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-21 15:15:40,703]: Sample Values (25 elements): [0.05731940269470215, 0.0, 0.0, 0.0, 0.0, 0.05731940269470215, 0.0, 0.05731940269470215, -0.05731940269470215, 0.05731940269470215, 0.0, 0.0, 0.0, -0.05731940269470215, 0.0, -0.05731940269470215, 0.0, 0.05731940269470215, 0.0, 0.0, 0.05731940269470215, 0.0, 0.0, 0.05731940269470215, -0.05731940269470215]
[2025-05-21 15:15:40,703]: Mean: -0.00000700
[2025-05-21 15:15:40,703]: Min: -0.05731940
[2025-05-21 15:15:40,703]: Max: 0.11463881
[2025-05-21 15:15:40,703]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-21 15:15:40,704]: Sample Values (25 elements): [0.9075316786766052, 0.9096283316612244, 0.9078059196472168, 0.9093907475471497, 0.9055247902870178, 0.9006675481796265, 0.9018301367759705, 0.9059802889823914, 0.9030371904373169, 0.9052137136459351, 0.9136468768119812, 0.9041759967803955, 0.9174294471740723, 0.9052001237869263, 0.898699939250946, 0.9103974103927612, 0.8984951376914978, 0.9035529494285583, 0.9054769277572632, 0.9054283499717712, 0.9139219522476196, 0.9023517966270447, 0.9098868370056152, 0.9154236912727356, 0.9105015397071838]
[2025-05-21 15:15:40,704]: Mean: 0.90648723
[2025-05-21 15:15:40,704]: Min: 0.89242148
[2025-05-21 15:15:40,704]: Max: 0.92528301
[2025-05-21 15:15:40,705]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 15:15:40,734]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.02507787011563778, 0.0, 0.0, -0.02507787011563778, 0.0, 0.0, 0.0, 0.0, 0.02507787011563778, 0.02507787011563778, 0.0, 0.02507787011563778, 0.0, 0.0, 0.0, 0.0, -0.02507787011563778, 0.0, 0.0, 0.0, -0.02507787011563778, 0.0]
[2025-05-21 15:15:40,735]: Mean: -0.00001210
[2025-05-21 15:15:40,735]: Min: -0.02507787
[2025-05-21 15:15:40,735]: Max: 0.05015574
[2025-05-21 15:15:40,735]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-21 15:15:40,736]: Sample Values (25 elements): [0.9156196713447571, 0.9233394265174866, 0.9141762852668762, 0.9097966551780701, 0.9103243350982666, 0.9137783050537109, 0.9157344102859497, 0.9145259261131287, 0.91717529296875, 0.9200825095176697, 0.9173886775970459, 0.9193752408027649, 0.9128345251083374, 0.9154503345489502, 0.9135360717773438, 0.9132826328277588, 0.921162486076355, 0.9142909049987793, 0.9181573390960693, 0.9165465831756592, 0.9208993315696716, 0.9106361269950867, 0.9139425754547119, 0.9264694452285767, 0.9125056862831116]
[2025-05-21 15:15:40,737]: Mean: 0.91584986
[2025-05-21 15:15:40,737]: Min: 0.90521896
[2025-05-21 15:15:40,737]: Max: 0.93247342
[2025-05-21 15:15:40,738]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 15:15:40,780]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.01815931499004364, 0.0, 0.0, 0.0, 0.01815931499004364, 0.0, -0.01815931499004364, 0.0, -0.01815931499004364, 0.0, 0.0, 0.0, 0.0, 0.01815931499004364, -0.01815931499004364, 0.0, -0.01815931499004364, 0.01815931499004364, 0.0, -0.01815931499004364, 0.0, 0.0]
[2025-05-21 15:15:40,780]: Mean: 0.00000069
[2025-05-21 15:15:40,780]: Min: -0.03631863
[2025-05-21 15:15:40,781]: Max: 0.01815931
[2025-05-21 15:15:40,781]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-21 15:15:40,781]: Sample Values (25 elements): [0.9242972135543823, 0.9164531826972961, 0.9121648073196411, 0.9181415438652039, 0.9191815257072449, 0.9187324047088623, 0.915928065776825, 0.9160544872283936, 0.9222986102104187, 0.9144488573074341, 0.9174155592918396, 0.9166169166564941, 0.9152858257293701, 0.9177517890930176, 0.9186270236968994, 0.9186837673187256, 0.914075493812561, 0.9207375645637512, 0.9106545448303223, 0.9177324771881104, 0.9103517532348633, 0.9204039573669434, 0.914371907711029, 0.9141516089439392, 0.9148464798927307]
[2025-05-21 15:15:40,781]: Mean: 0.91624349
[2025-05-21 15:15:40,781]: Min: 0.90553814
[2025-05-21 15:15:40,781]: Max: 0.92917609
[2025-05-21 15:15:40,781]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-21 15:15:40,782]: Sample Values (25 elements): [-0.06704126298427582, -0.04399576783180237, 0.034885384142398834, 0.07344800233840942, -0.009572303853929043, -0.09226532280445099, -0.023351887241005898, 9.717255124996882e-06, 0.06792500615119934, 0.04898591712117195, 0.07957957684993744, -0.06071652099490166, 0.054923590272665024, -0.0966692641377449, -0.009195707738399506, -0.044419605284929276, -0.0409226268529892, -0.0863274559378624, -0.0671253576874733, 0.0022130573634058237, 0.09941516816616058, 0.09914521872997284, -0.06792692095041275, 0.0038625982124358416, 0.027165886014699936]
[2025-05-21 15:15:40,782]: Mean: 0.00007419
[2025-05-21 15:15:40,782]: Min: -0.17024866
[2025-05-21 15:15:40,782]: Max: 0.19991930
[2025-05-21 15:15:40,782]: 


QAT of ResNet18 with parametrized_hardtanh down to 3 bits...
[2025-05-21 15:15:40,982]: [ResNet18_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-21 15:15:41,006]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 15:17:27,135]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.9701 Train Acc: 0.6626 Eval Loss: 0.8130 Eval Acc: 0.7227 (LR: 0.010000)
[2025-05-21 15:19:13,507]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.6979 Train Acc: 0.7554 Eval Loss: 0.7421 Eval Acc: 0.7446 (LR: 0.010000)
[2025-05-21 15:20:59,830]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.6335 Train Acc: 0.7802 Eval Loss: 0.7260 Eval Acc: 0.7565 (LR: 0.010000)
[2025-05-21 15:22:45,934]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.5864 Train Acc: 0.7954 Eval Loss: 0.6393 Eval Acc: 0.7868 (LR: 0.010000)
[2025-05-21 15:24:31,866]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.5528 Train Acc: 0.8067 Eval Loss: 0.6844 Eval Acc: 0.7724 (LR: 0.010000)
[2025-05-21 15:26:17,824]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.5341 Train Acc: 0.8131 Eval Loss: 0.6018 Eval Acc: 0.8039 (LR: 0.010000)
[2025-05-21 15:28:03,780]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.5047 Train Acc: 0.8247 Eval Loss: 0.6240 Eval Acc: 0.7980 (LR: 0.010000)
[2025-05-21 15:29:49,751]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.4884 Train Acc: 0.8295 Eval Loss: 0.5934 Eval Acc: 0.8080 (LR: 0.010000)
[2025-05-21 15:31:35,681]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.4613 Train Acc: 0.8376 Eval Loss: 0.5961 Eval Acc: 0.8036 (LR: 0.010000)
[2025-05-21 15:33:21,562]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.4470 Train Acc: 0.8431 Eval Loss: 0.5751 Eval Acc: 0.8145 (LR: 0.010000)
[2025-05-21 15:35:07,500]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.4260 Train Acc: 0.8507 Eval Loss: 0.5354 Eval Acc: 0.8260 (LR: 0.010000)
[2025-05-21 15:36:53,459]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.4130 Train Acc: 0.8568 Eval Loss: 0.6045 Eval Acc: 0.8005 (LR: 0.010000)
[2025-05-21 15:38:39,477]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.3973 Train Acc: 0.8616 Eval Loss: 0.6005 Eval Acc: 0.8142 (LR: 0.010000)
[2025-05-21 15:40:25,585]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.3898 Train Acc: 0.8635 Eval Loss: 0.5021 Eval Acc: 0.8427 (LR: 0.010000)
[2025-05-21 15:42:11,740]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.3818 Train Acc: 0.8658 Eval Loss: 0.6486 Eval Acc: 0.8025 (LR: 0.001000)
[2025-05-21 15:43:57,813]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.2675 Train Acc: 0.9060 Eval Loss: 0.4227 Eval Acc: 0.8707 (LR: 0.001000)
[2025-05-21 15:45:43,989]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.2430 Train Acc: 0.9150 Eval Loss: 0.4178 Eval Acc: 0.8703 (LR: 0.001000)
[2025-05-21 15:47:30,121]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.2337 Train Acc: 0.9199 Eval Loss: 0.4145 Eval Acc: 0.8757 (LR: 0.001000)
[2025-05-21 15:49:16,336]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.2221 Train Acc: 0.9227 Eval Loss: 0.4204 Eval Acc: 0.8712 (LR: 0.001000)
[2025-05-21 15:51:04,215]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.2172 Train Acc: 0.9233 Eval Loss: 0.4283 Eval Acc: 0.8742 (LR: 0.001000)
[2025-05-21 15:52:50,445]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.2115 Train Acc: 0.9248 Eval Loss: 0.4200 Eval Acc: 0.8732 (LR: 0.001000)
[2025-05-21 15:54:36,714]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.2074 Train Acc: 0.9275 Eval Loss: 0.4180 Eval Acc: 0.8748 (LR: 0.001000)
[2025-05-21 15:56:22,619]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.2030 Train Acc: 0.9296 Eval Loss: 0.4384 Eval Acc: 0.8686 (LR: 0.001000)
[2025-05-21 15:58:08,346]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.2017 Train Acc: 0.9298 Eval Loss: 0.4297 Eval Acc: 0.8740 (LR: 0.001000)
[2025-05-21 15:59:54,095]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.1946 Train Acc: 0.9322 Eval Loss: 0.4215 Eval Acc: 0.8762 (LR: 0.001000)
[2025-05-21 16:01:39,847]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.1909 Train Acc: 0.9327 Eval Loss: 0.4235 Eval Acc: 0.8743 (LR: 0.001000)
[2025-05-21 16:03:25,778]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.1890 Train Acc: 0.9331 Eval Loss: 0.4333 Eval Acc: 0.8737 (LR: 0.001000)
[2025-05-21 16:05:11,927]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.1831 Train Acc: 0.9360 Eval Loss: 0.4358 Eval Acc: 0.8777 (LR: 0.001000)
[2025-05-21 16:06:57,900]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.1793 Train Acc: 0.9362 Eval Loss: 0.4353 Eval Acc: 0.8764 (LR: 0.001000)
[2025-05-21 16:08:43,833]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.1800 Train Acc: 0.9370 Eval Loss: 0.4373 Eval Acc: 0.8760 (LR: 0.000100)
[2025-05-21 16:10:30,004]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.1620 Train Acc: 0.9443 Eval Loss: 0.4199 Eval Acc: 0.8800 (LR: 0.000100)
[2025-05-21 16:12:16,197]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.1572 Train Acc: 0.9447 Eval Loss: 0.4195 Eval Acc: 0.8806 (LR: 0.000100)
[2025-05-21 16:14:07,912]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.1607 Train Acc: 0.9444 Eval Loss: 0.4162 Eval Acc: 0.8809 (LR: 0.000100)
[2025-05-21 16:15:54,217]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.1569 Train Acc: 0.9445 Eval Loss: 0.4209 Eval Acc: 0.8776 (LR: 0.000100)
[2025-05-21 16:17:39,954]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.1533 Train Acc: 0.9459 Eval Loss: 0.4170 Eval Acc: 0.8802 (LR: 0.000100)
[2025-05-21 16:19:25,904]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.1530 Train Acc: 0.9471 Eval Loss: 0.4212 Eval Acc: 0.8796 (LR: 0.000100)
[2025-05-21 16:21:11,806]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.1519 Train Acc: 0.9466 Eval Loss: 0.4237 Eval Acc: 0.8806 (LR: 0.000100)
[2025-05-21 16:22:57,557]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.1549 Train Acc: 0.9459 Eval Loss: 0.4216 Eval Acc: 0.8792 (LR: 0.000100)
[2025-05-21 16:24:43,514]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.1520 Train Acc: 0.9461 Eval Loss: 0.4243 Eval Acc: 0.8807 (LR: 0.000100)
[2025-05-21 16:26:31,149]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.1530 Train Acc: 0.9470 Eval Loss: 0.4256 Eval Acc: 0.8793 (LR: 0.000100)
[2025-05-21 16:28:17,030]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.1513 Train Acc: 0.9469 Eval Loss: 0.4243 Eval Acc: 0.8808 (LR: 0.000100)
[2025-05-21 16:30:03,143]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.1511 Train Acc: 0.9465 Eval Loss: 0.4217 Eval Acc: 0.8804 (LR: 0.000100)
[2025-05-21 16:31:49,023]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.1505 Train Acc: 0.9464 Eval Loss: 0.4229 Eval Acc: 0.8812 (LR: 0.000100)
[2025-05-21 16:33:34,828]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.1502 Train Acc: 0.9464 Eval Loss: 0.4203 Eval Acc: 0.8825 (LR: 0.000100)
[2025-05-21 16:35:20,698]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.1534 Train Acc: 0.9461 Eval Loss: 0.4260 Eval Acc: 0.8805 (LR: 0.000010)
[2025-05-21 16:37:06,455]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.1498 Train Acc: 0.9465 Eval Loss: 0.4220 Eval Acc: 0.8815 (LR: 0.000010)
[2025-05-21 16:38:52,182]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.1484 Train Acc: 0.9484 Eval Loss: 0.4236 Eval Acc: 0.8818 (LR: 0.000010)
[2025-05-21 16:40:38,138]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.1478 Train Acc: 0.9492 Eval Loss: 0.4242 Eval Acc: 0.8845 (LR: 0.000010)
[2025-05-21 16:42:24,494]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.1468 Train Acc: 0.9483 Eval Loss: 0.4223 Eval Acc: 0.8815 (LR: 0.000010)
[2025-05-21 16:44:10,438]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.1502 Train Acc: 0.9482 Eval Loss: 0.4253 Eval Acc: 0.8831 (LR: 0.000010)
[2025-05-21 16:45:56,387]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.1486 Train Acc: 0.9477 Eval Loss: 0.4222 Eval Acc: 0.8831 (LR: 0.000010)
[2025-05-21 16:47:42,353]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.1445 Train Acc: 0.9498 Eval Loss: 0.4272 Eval Acc: 0.8809 (LR: 0.000010)
[2025-05-21 16:49:28,477]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.1486 Train Acc: 0.9477 Eval Loss: 0.4226 Eval Acc: 0.8829 (LR: 0.000010)
[2025-05-21 16:51:14,239]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.1474 Train Acc: 0.9479 Eval Loss: 0.4268 Eval Acc: 0.8812 (LR: 0.000010)
[2025-05-21 16:53:00,215]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.1482 Train Acc: 0.9487 Eval Loss: 0.4275 Eval Acc: 0.8792 (LR: 0.000010)
[2025-05-21 16:54:46,282]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.1469 Train Acc: 0.9491 Eval Loss: 0.4295 Eval Acc: 0.8799 (LR: 0.000010)
[2025-05-21 16:56:32,044]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.1469 Train Acc: 0.9491 Eval Loss: 0.4252 Eval Acc: 0.8800 (LR: 0.000010)
[2025-05-21 16:58:17,997]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.1483 Train Acc: 0.9473 Eval Loss: 0.4241 Eval Acc: 0.8817 (LR: 0.000010)
[2025-05-21 17:00:04,202]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.1437 Train Acc: 0.9494 Eval Loss: 0.4236 Eval Acc: 0.8817 (LR: 0.000010)
[2025-05-21 17:01:51,787]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.1483 Train Acc: 0.9473 Eval Loss: 0.4252 Eval Acc: 0.8795 (LR: 0.000010)
[2025-05-21 17:01:51,787]: [ResNet18_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8845
[2025-05-21 17:01:51,875]: 


Quantization of model down to 3 bits finished
[2025-05-21 17:01:51,875]: Model Architecture:
[2025-05-21 17:01:51,920]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0605], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21696621179580688, max_val=0.2064816951751709)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2775], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0908], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2790345251560211, max_val=0.35636162757873535)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0759], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2896500825881958, max_val=0.24183008074760437)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1932], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0500], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1752556562423706, max_val=0.17479805648326874)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3439], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0472], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1603430211544037, max_val=0.1697418987751007)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2255], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0289], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10158025473356247, max_val=0.10080992430448532)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0708], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2650880217552185, max_val=0.23060905933380127)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2737], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0277], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10110922157764435, max_val=0.09294699132442474)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1546], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0251], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08052293956279755, max_val=0.09513050317764282)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3106], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0220], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07962090522050858, max_val=0.07458996027708054)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1785], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0191], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06528714299201965, max_val=0.06816108524799347)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0409], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14679129421710968, max_val=0.13984081149101257)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2488], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0191], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06363388150930405, max_val=0.06974484026432037)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1458], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0179], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06271317601203918, max_val=0.06240398436784744)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2584], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0155], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05271525681018829, max_val=0.05552715063095093)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1885], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0119], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04128913953900337, max_val=0.04220210015773773)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0292], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10205324739217758, max_val=0.10217645019292831)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2683], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04437173902988434, max_val=0.044079139828681946)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1176], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.030218197032809258, max_val=0.030220970511436462)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4245], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 17:01:51,920]: 
Model Weights:
[2025-05-21 17:01:51,920]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-21 17:01:51,920]: Sample Values (25 elements): [0.05161852389574051, -0.2858625054359436, 0.17191928625106812, -0.07827136665582657, -0.2748812437057495, -0.07207370549440384, -0.36315473914146423, -0.027806123718619347, -0.17569568753242493, 0.08293712139129639, 0.18003758788108826, 0.11016290634870529, -0.15622267127037048, 0.506820559501648, -0.14492474496364594, -0.012818618677556515, 0.2434900850057602, 0.19378796219825745, -0.0950421541929245, 0.03550140559673309, 0.006594033911824226, 0.08983314782381058, -0.12686987221240997, 0.13938820362091064, 0.1900220662355423]
[2025-05-21 17:01:51,921]: Mean: 0.00051349
[2025-05-21 17:01:51,921]: Min: -0.53867620
[2025-05-21 17:01:51,921]: Max: 0.53281868
[2025-05-21 17:01:51,921]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-21 17:01:51,921]: Sample Values (25 elements): [0.6316930651664734, 0.6913830041885376, 0.9965940117835999, 0.5704761743545532, 0.6362395286560059, 0.8346161842346191, 0.8225480914115906, 0.752723753452301, 1.0228793621063232, 0.9529266953468323, 1.0625427961349487, 0.6831124424934387, 0.8742372989654541, 0.824411153793335, 0.7044123411178589, 0.6426562666893005, 0.8476642370223999, 0.7112649083137512, 0.6761988401412964, 0.8028020858764648, 0.7831091284751892, 0.9371254444122314, 0.982478678226471, 0.9444302916526794, 0.9509751200675964]
[2025-05-21 17:01:51,921]: Mean: 0.82879424
[2025-05-21 17:01:51,921]: Min: 0.57047617
[2025-05-21 17:01:51,922]: Max: 1.13426161
[2025-05-21 17:01:51,923]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 17:01:51,923]: Sample Values (25 elements): [0.060492560267448425, 0.0, 0.060492560267448425, 0.0, 0.0, 0.0, 0.060492560267448425, 0.0, 0.060492560267448425, -0.060492560267448425, -0.060492560267448425, 0.0, 0.060492560267448425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12098512053489685, -0.060492560267448425, 0.0, 0.0, 0.0, 0.060492560267448425, 0.060492560267448425]
[2025-05-21 17:01:51,923]: Mean: -0.00000328
[2025-05-21 17:01:51,923]: Min: -0.24197024
[2025-05-21 17:01:51,924]: Max: 0.18147768
[2025-05-21 17:01:51,924]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 17:01:51,924]: Sample Values (25 elements): [0.9533765912055969, 0.8829824924468994, 0.9108549952507019, 0.9014589190483093, 0.8247068524360657, 0.8861585855484009, 0.9452634453773499, 0.8714648485183716, 0.9247236251831055, 0.9789113998413086, 1.038639783859253, 0.8971325755119324, 0.9059410691261292, 0.9539777636528015, 0.8951275944709778, 1.026972770690918, 0.8813336491584778, 0.9650686979293823, 0.9554623961448669, 1.0400135517120361, 0.8980342149734497, 0.9060302376747131, 0.9541522860527039, 0.9247013330459595, 0.9762439727783203]
[2025-05-21 17:01:51,924]: Mean: 0.92535031
[2025-05-21 17:01:51,924]: Min: 0.79127336
[2025-05-21 17:01:51,924]: Max: 1.07156372
[2025-05-21 17:01:51,925]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 17:01:51,926]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09077088534832001, 0.09077088534832001, 0.09077088534832001, 0.09077088534832001, -0.09077088534832001, 0.0, 0.0, 0.0, 0.09077088534832001, 0.0, 0.0, 0.09077088534832001, 0.0, 0.0, 0.0, 0.0, -0.09077088534832001, 0.0, 0.0]
[2025-05-21 17:01:51,926]: Mean: -0.00000985
[2025-05-21 17:01:51,926]: Min: -0.27231264
[2025-05-21 17:01:51,926]: Max: 0.36308354
[2025-05-21 17:01:51,926]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 17:01:51,927]: Sample Values (25 elements): [0.9656834006309509, 0.9354485273361206, 0.9028218388557434, 0.8536843657493591, 0.8944568634033203, 0.9105148911476135, 0.7781656980514526, 0.7410692572593689, 0.9091158509254456, 0.9085900783538818, 0.9755569100379944, 0.9446188807487488, 0.7988843321800232, 1.091691017150879, 0.8511372208595276, 0.9128459095954895, 1.028173804283142, 0.9691019654273987, 0.8123403191566467, 0.9517305493354797, 0.9275124073028564, 0.9330642819404602, 1.084694266319275, 0.8470041155815125, 0.9579529166221619]
[2025-05-21 17:01:51,927]: Mean: 0.90340900
[2025-05-21 17:01:51,927]: Min: 0.74106926
[2025-05-21 17:01:51,927]: Max: 1.39121377
[2025-05-21 17:01:51,928]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 17:01:51,929]: Sample Values (25 elements): [0.0, 0.0, -0.07592574506998062, -0.07592574506998062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07592574506998062, -0.07592574506998062, 0.07592574506998062, 0.0, 0.0, 0.07592574506998062, 0.0, 0.0, -0.07592574506998062, 0.0, 0.0, 0.07592574506998062, 0.0, 0.0, -0.07592574506998062, 0.0]
[2025-05-21 17:01:51,929]: Mean: 0.00015035
[2025-05-21 17:01:51,929]: Min: -0.30370298
[2025-05-21 17:01:51,929]: Max: 0.22777724
[2025-05-21 17:01:51,929]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 17:01:51,929]: Sample Values (25 elements): [0.8893986940383911, 0.9091241359710693, 0.9450032114982605, 0.9040360450744629, 0.9459847211837769, 0.9339382648468018, 0.8686056137084961, 0.8935454487800598, 0.9636932611465454, 0.9057813286781311, 0.9266824126243591, 0.9195635914802551, 0.9381862878799438, 0.8871999382972717, 0.9122391939163208, 0.9382274746894836, 0.9047350883483887, 0.9135536551475525, 0.9063247442245483, 0.9571881890296936, 0.9822086095809937, 0.932507336139679, 0.9906855225563049, 0.955527126789093, 0.9142794013023376]
[2025-05-21 17:01:51,929]: Mean: 0.92367905
[2025-05-21 17:01:51,930]: Min: 0.86105502
[2025-05-21 17:01:51,930]: Max: 0.99963510
[2025-05-21 17:01:51,931]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 17:01:51,931]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.05000767856836319, 0.0, 0.0, 0.05000767856836319, 0.05000767856836319, 0.0, 0.0, 0.0, -0.05000767856836319, -0.05000767856836319, 0.05000767856836319, 0.0, -0.05000767856836319, -0.05000767856836319, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05000767856836319, 0.0, -0.05000767856836319]
[2025-05-21 17:01:51,931]: Mean: 0.00047208
[2025-05-21 17:01:51,931]: Min: -0.20003071
[2025-05-21 17:01:51,932]: Max: 0.15002304
[2025-05-21 17:01:51,932]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 17:01:51,932]: Sample Values (25 elements): [0.9417092204093933, 0.8825274705886841, 0.8848891854286194, 0.9398490190505981, 0.8418760299682617, 0.815759539604187, 0.898228108882904, 0.8694446682929993, 0.8913804292678833, 0.8574234843254089, 0.8916211724281311, 0.8369860649108887, 0.9175316691398621, 0.945808470249176, 0.8531549572944641, 0.9322178363800049, 0.8977797627449036, 0.86295485496521, 0.8101134300231934, 0.9790176153182983, 0.8870389461517334, 0.8641425371170044, 0.8525208830833435, 0.9724757075309753, 0.9602457284927368]
[2025-05-21 17:01:51,932]: Mean: 0.91588819
[2025-05-21 17:01:51,932]: Min: 0.79125053
[2025-05-21 17:01:51,932]: Max: 1.30438566
[2025-05-21 17:01:51,933]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-21 17:01:51,934]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.0471549890935421, -0.0471549890935421, -0.0471549890935421, -0.0471549890935421, 0.0, -0.0471549890935421, 0.0, -0.0471549890935421, 0.0, 0.0, -0.0471549890935421, 0.0, 0.0, 0.0471549890935421, 0.0, 0.0, 0.0, 0.0471549890935421, 0.0, 0.0471549890935421, 0.0, 0.0]
[2025-05-21 17:01:51,934]: Mean: 0.00020978
[2025-05-21 17:01:51,935]: Min: -0.14146496
[2025-05-21 17:01:51,935]: Max: 0.18861996
[2025-05-21 17:01:51,935]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-21 17:01:51,935]: Sample Values (25 elements): [0.9083117842674255, 0.9093571305274963, 0.9440581202507019, 0.9246943593025208, 0.9004144668579102, 0.9147042632102966, 0.9281852841377258, 0.9024457335472107, 0.9101892113685608, 0.9243323802947998, 0.9187127351760864, 0.902149498462677, 0.9082231521606445, 0.930814802646637, 0.9264890551567078, 0.9145671725273132, 0.9249195456504822, 0.9125398993492126, 0.925984263420105, 0.9629238843917847, 0.9511209726333618, 0.907804012298584, 0.9328420162200928, 0.9433249831199646, 0.9222440719604492]
[2025-05-21 17:01:51,935]: Mean: 0.91909802
[2025-05-21 17:01:51,935]: Min: 0.87787032
[2025-05-21 17:01:51,935]: Max: 0.96292388
[2025-05-21 17:01:51,936]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 17:01:51,938]: Sample Values (25 elements): [-0.02891288697719574, 0.02891288697719574, 0.02891288697719574, -0.02891288697719574, 0.0, 0.02891288697719574, -0.02891288697719574, -0.02891288697719574, 0.0, 0.0, 0.0, 0.02891288697719574, -0.02891288697719574, -0.02891288697719574, -0.02891288697719574, 0.0, -0.02891288697719574, 0.0, 0.02891288697719574, -0.02891288697719574, 0.0, -0.05782577395439148, -0.02891288697719574, 0.0, 0.0]
[2025-05-21 17:01:51,938]: Mean: -0.00006549
[2025-05-21 17:01:51,938]: Min: -0.11565155
[2025-05-21 17:01:51,938]: Max: 0.08673866
[2025-05-21 17:01:51,938]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-21 17:01:51,939]: Sample Values (25 elements): [0.9023760557174683, 0.91478431224823, 0.900460958480835, 0.9028418660163879, 0.929244875907898, 0.9357701539993286, 0.9367138743400574, 0.9446035623550415, 0.9227458238601685, 0.9256952404975891, 0.9670972228050232, 0.9629833698272705, 0.9056439995765686, 0.9050356149673462, 0.9274604320526123, 0.9079031348228455, 0.9242144227027893, 0.9334170818328857, 0.9391297101974487, 0.9499517679214478, 0.9396764039993286, 0.908399760723114, 0.9364045858383179, 0.9187846779823303, 0.877385139465332]
[2025-05-21 17:01:51,939]: Mean: 0.92836356
[2025-05-21 17:01:51,939]: Min: 0.87738514
[2025-05-21 17:01:51,939]: Max: 0.98591805
[2025-05-21 17:01:51,940]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-21 17:01:51,940]: Sample Values (25 elements): [-0.1416277289390564, -0.0708138644695282, 0.1416277289390564, -0.1416277289390564, -0.1416277289390564, -0.0708138644695282, 0.0708138644695282, 0.0, 0.0, 0.0, 0.0, 0.0708138644695282, -0.0708138644695282, -0.0708138644695282, 0.0, 0.0708138644695282, -0.0708138644695282, -0.0708138644695282, -0.0708138644695282, -0.1416277289390564, -0.0708138644695282, 0.0708138644695282, 0.0, 0.0708138644695282, 0.0708138644695282]
[2025-05-21 17:01:51,940]: Mean: 0.00032848
[2025-05-21 17:01:51,941]: Min: -0.28325546
[2025-05-21 17:01:51,941]: Max: 0.21244159
[2025-05-21 17:01:51,941]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-21 17:01:51,941]: Sample Values (25 elements): [0.893535852432251, 0.865971565246582, 0.8706934452056885, 0.8839133381843567, 0.9063944220542908, 0.8260611891746521, 0.9153285026550293, 0.8998153805732727, 0.8917061686515808, 0.8620374202728271, 0.8893179893493652, 0.8837125897407532, 0.9152231216430664, 0.8882429599761963, 0.8619179725646973, 0.9014374613761902, 0.8663215637207031, 0.8720921277999878, 0.895557701587677, 0.8912681341171265, 0.8717628121376038, 0.9671468734741211, 0.871673047542572, 0.8550016283988953, 0.913727343082428]
[2025-05-21 17:01:51,941]: Mean: 0.88691550
[2025-05-21 17:01:51,941]: Min: 0.82606119
[2025-05-21 17:01:51,941]: Max: 0.97033024
[2025-05-21 17:01:51,942]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 17:01:51,944]: Sample Values (25 elements): [-0.027722317725419998, 0.027722317725419998, 0.027722317725419998, -0.027722317725419998, 0.0, 0.0, -0.027722317725419998, 0.0, 0.0, 0.0, 0.027722317725419998, 0.0, 0.027722317725419998, 0.027722317725419998, 0.055444635450839996, 0.0, -0.055444635450839996, -0.055444635450839996, 0.055444635450839996, -0.055444635450839996, 0.055444635450839996, 0.0, 0.0, -0.027722317725419998, 0.027722317725419998]
[2025-05-21 17:01:51,944]: Mean: -0.00014176
[2025-05-21 17:01:51,944]: Min: -0.11088927
[2025-05-21 17:01:51,944]: Max: 0.08316696
[2025-05-21 17:01:51,944]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-21 17:01:51,945]: Sample Values (25 elements): [0.9042044878005981, 0.9241024255752563, 0.9408582448959351, 0.9203197956085205, 0.9191752672195435, 0.9257300496101379, 0.9430954456329346, 0.9165306687355042, 0.9030699729919434, 0.9106764793395996, 0.9218633770942688, 0.9530490636825562, 0.9175752997398376, 0.9129745364189148, 0.9152915477752686, 0.9098924994468689, 0.937932550907135, 0.9150354862213135, 0.9251661896705627, 0.911352276802063, 0.9132087230682373, 0.9296254515647888, 0.9229362607002258, 0.9397278428077698, 0.9406113624572754]
[2025-05-21 17:01:51,945]: Mean: 0.92293310
[2025-05-21 17:01:51,945]: Min: 0.89495742
[2025-05-21 17:01:51,945]: Max: 0.96872425
[2025-05-21 17:01:51,946]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 17:01:51,947]: Sample Values (25 elements): [0.025093352422118187, 0.025093352422118187, -0.025093352422118187, -0.025093352422118187, -0.025093352422118187, 0.025093352422118187, 0.025093352422118187, 0.0, -0.025093352422118187, -0.025093352422118187, -0.025093352422118187, 0.0, 0.025093352422118187, -0.025093352422118187, 0.025093352422118187, 0.025093352422118187, -0.025093352422118187, 0.0, 0.025093352422118187, 0.0, 0.025093352422118187, -0.050186704844236374, 0.050186704844236374, 0.025093352422118187, 0.025093352422118187]
[2025-05-21 17:01:51,947]: Mean: 0.00003472
[2025-05-21 17:01:51,948]: Min: -0.07528006
[2025-05-21 17:01:51,948]: Max: 0.10037341
[2025-05-21 17:01:51,948]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-21 17:01:51,948]: Sample Values (25 elements): [0.901922881603241, 0.946797251701355, 0.9228754639625549, 0.9326180815696716, 0.9736579656600952, 0.9283995628356934, 0.9101132750511169, 0.920062780380249, 0.9263303279876709, 0.9269528985023499, 0.9601188898086548, 0.912939727306366, 0.9815661907196045, 0.9246108531951904, 0.9240894913673401, 0.9204967617988586, 0.8770239353179932, 0.9173364043235779, 0.9180142879486084, 0.9296718239784241, 0.9152145385742188, 0.9162492156028748, 0.9162511825561523, 0.9431312680244446, 0.9325782656669617]
[2025-05-21 17:01:51,948]: Mean: 0.92204112
[2025-05-21 17:01:51,948]: Min: 0.86537647
[2025-05-21 17:01:51,949]: Max: 0.98156619
[2025-05-21 17:01:51,950]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-21 17:01:51,953]: Sample Values (25 elements): [0.022030126303434372, -0.022030126303434372, 0.0, 0.022030126303434372, 0.044060252606868744, -0.044060252606868744, -0.022030126303434372, -0.044060252606868744, 0.0, 0.0, 0.022030126303434372, 0.022030126303434372, -0.022030126303434372, 0.044060252606868744, 0.0, 0.022030126303434372, 0.0, 0.022030126303434372, 0.022030126303434372, 0.0, 0.0, 0.0, 0.0, 0.022030126303434372, 0.0]
[2025-05-21 17:01:51,953]: Mean: 0.00018727
[2025-05-21 17:01:51,953]: Min: -0.08812051
[2025-05-21 17:01:51,953]: Max: 0.06609038
[2025-05-21 17:01:51,953]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-21 17:01:51,953]: Sample Values (25 elements): [0.9202315807342529, 0.929958701133728, 0.916563868522644, 0.9015428423881531, 0.9183527827262878, 0.906231701374054, 0.9079405665397644, 0.92381751537323, 0.9124253988265991, 0.9117997884750366, 0.9173751473426819, 0.9106168746948242, 0.9157118797302246, 0.9137690663337708, 0.9195948243141174, 0.9134512543678284, 0.9144482016563416, 0.9109413027763367, 0.9070754051208496, 0.9133831262588501, 0.9228137731552124, 0.9204750657081604, 0.9259061813354492, 0.9210817217826843, 0.9253934621810913]
[2025-05-21 17:01:51,954]: Mean: 0.91590619
[2025-05-21 17:01:51,954]: Min: 0.90012914
[2025-05-21 17:01:51,954]: Max: 0.95117599
[2025-05-21 17:01:51,955]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 17:01:51,960]: Sample Values (25 elements): [0.0, 0.019064031541347504, 0.0, 0.0, -0.019064031541347504, -0.019064031541347504, 0.03812806308269501, 0.0, 0.019064031541347504, 0.0, -0.019064031541347504, 0.0, 0.0, 0.0, 0.019064031541347504, -0.019064031541347504, 0.019064031541347504, 0.019064031541347504, 0.0, 0.019064031541347504, 0.019064031541347504, 0.0, -0.019064031541347504, -0.019064031541347504, 0.019064031541347504]
[2025-05-21 17:01:51,960]: Mean: 0.00002983
[2025-05-21 17:01:51,960]: Min: -0.05719209
[2025-05-21 17:01:51,961]: Max: 0.07625613
[2025-05-21 17:01:51,961]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-21 17:01:51,961]: Sample Values (25 elements): [0.9541152715682983, 0.9206784963607788, 0.9077751040458679, 0.9118939638137817, 0.9135052561759949, 0.9311310052871704, 0.9274753332138062, 0.9265792369842529, 0.9222479462623596, 0.9270246624946594, 0.9306838512420654, 0.925906240940094, 0.9224478006362915, 0.9207115173339844, 0.9280526041984558, 0.9209965467453003, 0.9129195809364319, 0.9290246963500977, 0.9211709499359131, 0.9317921996116638, 0.9421008825302124, 0.9334411025047302, 0.9201630353927612, 0.9350955486297607, 0.9231541752815247]
[2025-05-21 17:01:51,961]: Mean: 0.92708278
[2025-05-21 17:01:51,961]: Min: 0.90009880
[2025-05-21 17:01:51,961]: Max: 0.95817304
[2025-05-21 17:01:51,962]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-21 17:01:51,963]: Sample Values (25 elements): [0.0818948969244957, 0.0, 0.04094744846224785, -0.04094744846224785, 0.04094744846224785, 0.12284234166145325, -0.04094744846224785, -0.04094744846224785, -0.0818948969244957, 0.0, 0.04094744846224785, -0.04094744846224785, 0.04094744846224785, 0.0818948969244957, 0.0818948969244957, 0.0, 0.0, 0.0, 0.04094744846224785, 0.0, 0.0, 0.04094744846224785, 0.0818948969244957, 0.04094744846224785, 0.04094744846224785]
[2025-05-21 17:01:51,963]: Mean: -0.00016120
[2025-05-21 17:01:51,963]: Min: -0.16378979
[2025-05-21 17:01:51,963]: Max: 0.12284234
[2025-05-21 17:01:51,963]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-21 17:01:51,964]: Sample Values (25 elements): [0.8894205689430237, 0.8963914513587952, 0.9115221500396729, 0.9056572318077087, 0.9020523428916931, 0.8909326791763306, 0.9195804595947266, 0.8929469585418701, 0.8925976753234863, 0.8917279839515686, 0.9021327495574951, 0.8966577649116516, 0.8982440829277039, 0.8893997669219971, 0.8899378180503845, 0.8927520513534546, 0.8869050145149231, 0.9018956422805786, 0.8764667510986328, 0.8658210635185242, 0.8905800580978394, 0.8922865390777588, 0.8694742918014526, 0.898675262928009, 0.8817664980888367]
[2025-05-21 17:01:51,964]: Mean: 0.89130378
[2025-05-21 17:01:51,964]: Min: 0.85648847
[2025-05-21 17:01:51,964]: Max: 0.93345088
[2025-05-21 17:01:51,965]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 17:01:51,971]: Sample Values (25 elements): [0.0, 0.019054105505347252, 0.0, 0.019054105505347252, 0.0, 0.0, -0.019054105505347252, -0.019054105505347252, 0.019054105505347252, -0.019054105505347252, -0.019054105505347252, 0.0, 0.0, -0.019054105505347252, 0.0, 0.019054105505347252, -0.019054105505347252, -0.019054105505347252, 0.019054105505347252, 0.0, 0.019054105505347252, 0.019054105505347252, 0.0, -0.019054105505347252, 0.0]
[2025-05-21 17:01:51,971]: Mean: -0.00005292
[2025-05-21 17:01:51,971]: Min: -0.05716231
[2025-05-21 17:01:51,971]: Max: 0.07621642
[2025-05-21 17:01:51,971]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-21 17:01:51,971]: Sample Values (25 elements): [0.9372925162315369, 0.9114163517951965, 0.9142192602157593, 0.9221489429473877, 0.9121209979057312, 0.9126135110855103, 0.9256876707077026, 0.9106562733650208, 0.9133926033973694, 0.9096646904945374, 0.9420720338821411, 0.9119135141372681, 0.9215233325958252, 0.9197245240211487, 0.959790050983429, 0.9151561856269836, 0.9323574900627136, 0.9241402745246887, 0.9192583560943604, 0.929491400718689, 0.9110456109046936, 0.9258708357810974, 0.9382911920547485, 0.9185047149658203, 0.9258055686950684]
[2025-05-21 17:01:51,972]: Mean: 0.92039907
[2025-05-21 17:01:51,972]: Min: 0.90597206
[2025-05-21 17:01:51,972]: Max: 0.95979005
[2025-05-21 17:01:51,973]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 17:01:51,979]: Sample Values (25 elements): [0.01787388138473034, 0.0, 0.0, 0.0, 0.01787388138473034, 0.0, 0.0, 0.0, -0.03574776276946068, 0.01787388138473034, -0.01787388138473034, 0.01787388138473034, 0.0, 0.01787388138473034, 0.0, -0.01787388138473034, 0.01787388138473034, 0.01787388138473034, 0.01787388138473034, 0.01787388138473034, 0.01787388138473034, 0.0, 0.0, 0.01787388138473034, 0.01787388138473034]
[2025-05-21 17:01:51,979]: Mean: -0.00003491
[2025-05-21 17:01:51,979]: Min: -0.07149553
[2025-05-21 17:01:51,979]: Max: 0.05362164
[2025-05-21 17:01:51,979]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-21 17:01:51,980]: Sample Values (25 elements): [0.8934226036071777, 0.9179405570030212, 0.9493773579597473, 0.9088305830955505, 0.942848265171051, 0.9250790476799011, 0.910014271736145, 0.9115798473358154, 0.9351152777671814, 0.9205860495567322, 0.9261285066604614, 0.9300307631492615, 0.9092563986778259, 0.9100449085235596, 0.945485532283783, 0.9176554679870605, 0.9535465836524963, 0.9219145178794861, 0.9221928715705872, 0.9242721199989319, 0.9398062229156494, 0.9095396399497986, 0.9112508296966553, 0.912888765335083, 0.9253795742988586]
[2025-05-21 17:01:51,980]: Mean: 0.92245066
[2025-05-21 17:01:51,980]: Min: 0.86241865
[2025-05-21 17:01:51,980]: Max: 0.97337735
[2025-05-21 17:01:51,981]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-21 17:01:51,992]: Sample Values (25 elements): [0.0, 0.0, 0.015463200397789478, 0.030926400795578957, -0.015463200397789478, -0.015463200397789478, 0.015463200397789478, -0.030926400795578957, 0.015463200397789478, 0.0, 0.0, 0.015463200397789478, 0.0, 0.0, 0.015463200397789478, 0.0, -0.015463200397789478, 0.015463200397789478, 0.0, 0.015463200397789478, 0.0, 0.030926400795578957, 0.0, 0.015463200397789478, 0.0]
[2025-05-21 17:01:51,992]: Mean: -0.00001922
[2025-05-21 17:01:51,992]: Min: -0.04638960
[2025-05-21 17:01:51,993]: Max: 0.06185280
[2025-05-21 17:01:51,993]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-21 17:01:51,993]: Sample Values (25 elements): [0.9123721122741699, 0.9113843441009521, 0.9129809141159058, 0.9124639630317688, 0.9088838696479797, 0.9087700843811035, 0.9177114367485046, 0.9090737104415894, 0.9079094529151917, 0.9085455536842346, 0.9118630290031433, 0.9166852235794067, 0.9139887690544128, 0.9175976514816284, 0.9122996926307678, 0.9125616550445557, 0.9220724105834961, 0.9099110960960388, 0.9209818243980408, 0.9077666401863098, 0.9087449312210083, 0.914872407913208, 0.9170777797698975, 0.9158329367637634, 0.9245486855506897]
[2025-05-21 17:01:51,993]: Mean: 0.91427207
[2025-05-21 17:01:51,993]: Min: 0.90476120
[2025-05-21 17:01:51,993]: Max: 0.92978537
[2025-05-21 17:01:51,995]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 17:01:52,028]: Sample Values (25 elements): [0.0, 0.0, 0.011927319690585136, 0.011927319690585136, -0.011927319690585136, -0.011927319690585136, 0.0, 0.0, 0.011927319690585136, -0.011927319690585136, 0.0, 0.0, 0.011927319690585136, 0.0, -0.011927319690585136, 0.011927319690585136, 0.0, 0.0, 0.0, -0.023854639381170273, -0.011927319690585136, 0.0, 0.0, 0.011927319690585136, 0.011927319690585136]
[2025-05-21 17:01:52,029]: Mean: 0.00000458
[2025-05-21 17:01:52,029]: Min: -0.03578196
[2025-05-21 17:01:52,029]: Max: 0.04770928
[2025-05-21 17:01:52,029]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-21 17:01:52,029]: Sample Values (25 elements): [0.9150100946426392, 0.9170455932617188, 0.9406158924102783, 0.9218456745147705, 0.9242386817932129, 0.9198147058486938, 0.9309338927268982, 0.9168748259544373, 0.9212202429771423, 0.9133531451225281, 0.9219523668289185, 0.9227898120880127, 0.9156478047370911, 0.9226885437965393, 0.9103739857673645, 0.9307524561882019, 0.9192337989807129, 0.9233222007751465, 0.9218113422393799, 0.9202402234077454, 0.9227036237716675, 0.9288674592971802, 0.9203563332557678, 0.9217346906661987, 0.9227131009101868]
[2025-05-21 17:01:52,029]: Mean: 0.92332250
[2025-05-21 17:01:52,030]: Min: 0.90572733
[2025-05-21 17:01:52,030]: Max: 0.95855999
[2025-05-21 17:01:52,031]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-21 17:01:52,032]: Sample Values (25 elements): [-0.02917567268013954, 0.02917567268013954, -0.05835134536027908, 0.02917567268013954, 0.02917567268013954, -0.02917567268013954, 0.0, 0.0, 0.02917567268013954, -0.02917567268013954, 0.02917567268013954, 0.0, 0.05835134536027908, -0.05835134536027908, -0.05835134536027908, 0.0, 0.0, 0.0, -0.02917567268013954, -0.02917567268013954, 0.02917567268013954, -0.05835134536027908, 0.0, 0.0, 0.0]
[2025-05-21 17:01:52,032]: Mean: -0.00006633
[2025-05-21 17:01:52,032]: Min: -0.08752702
[2025-05-21 17:01:52,033]: Max: 0.11670269
[2025-05-21 17:01:52,033]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-21 17:01:52,033]: Sample Values (25 elements): [0.9029611349105835, 0.9060140252113342, 0.9190314412117004, 0.910295844078064, 0.9046178460121155, 0.9052774906158447, 0.9081137776374817, 0.9019986391067505, 0.9092952609062195, 0.9041622281074524, 0.8957372903823853, 0.9095810055732727, 0.9078524708747864, 0.9049620032310486, 0.9073304533958435, 0.9101440906524658, 0.9094589948654175, 0.9115837812423706, 0.9018037915229797, 0.8926705718040466, 0.9082423448562622, 0.9049510359764099, 0.9120550155639648, 0.9083465337753296, 0.9016563296318054]
[2025-05-21 17:01:52,033]: Mean: 0.90600216
[2025-05-21 17:01:52,033]: Min: 0.88405383
[2025-05-21 17:01:52,033]: Max: 0.92340583
[2025-05-21 17:01:52,034]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 17:01:52,067]: Sample Values (25 elements): [0.02527168020606041, -0.012635840103030205, -0.012635840103030205, 0.0, 0.012635840103030205, -0.012635840103030205, 0.012635840103030205, 0.012635840103030205, -0.012635840103030205, 0.012635840103030205, 0.012635840103030205, 0.012635840103030205, -0.012635840103030205, -0.012635840103030205, 0.0, 0.0, 0.0, 0.0, -0.012635840103030205, 0.012635840103030205, -0.012635840103030205, 0.0, 0.012635840103030205, -0.012635840103030205, 0.0]
[2025-05-21 17:01:52,068]: Mean: -0.00000498
[2025-05-21 17:01:52,068]: Min: -0.05054336
[2025-05-21 17:01:52,068]: Max: 0.03790752
[2025-05-21 17:01:52,068]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-21 17:01:52,069]: Sample Values (25 elements): [0.9208193421363831, 0.9213858246803284, 0.913223385810852, 0.9146006107330322, 0.9160860776901245, 0.9135048389434814, 0.9136866927146912, 0.9193832874298096, 0.9121275544166565, 0.9111722111701965, 0.9110528230667114, 0.9152211546897888, 0.9144721627235413, 0.9197825193405151, 0.9139138460159302, 0.9216419458389282, 0.9136134386062622, 0.9215652346611023, 0.9095449447631836, 0.9114313125610352, 0.913483738899231, 0.9143540263175964, 0.9139485359191895, 0.910750687122345, 0.9147622585296631]
[2025-05-21 17:01:52,070]: Mean: 0.91498631
[2025-05-21 17:01:52,070]: Min: 0.90616375
[2025-05-21 17:01:52,070]: Max: 0.93228394
[2025-05-21 17:01:52,071]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 17:01:52,110]: Sample Values (25 elements): [0.017268333584070206, -0.008634166792035103, 0.0, 0.008634166792035103, 0.0, -0.008634166792035103, 0.008634166792035103, 0.008634166792035103, -0.008634166792035103, 0.0, 0.008634166792035103, -0.017268333584070206, -0.008634166792035103, -0.008634166792035103, -0.008634166792035103, 0.008634166792035103, 0.0, -0.008634166792035103, 0.008634166792035103, 0.0, 0.017268333584070206, -0.008634166792035103, 0.008634166792035103, 0.0, 0.008634166792035103]
[2025-05-21 17:01:52,111]: Mean: -0.00000415
[2025-05-21 17:01:52,111]: Min: -0.02590250
[2025-05-21 17:01:52,111]: Max: 0.03453667
[2025-05-21 17:01:52,111]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-21 17:01:52,111]: Sample Values (25 elements): [0.9311766028404236, 0.9255384206771851, 0.9151038527488708, 0.9118854403495789, 0.9117900133132935, 0.9126181602478027, 0.9197031259536743, 0.9172651171684265, 0.92062908411026, 0.9112376570701599, 0.9193883538246155, 0.9155739545822144, 0.9196411967277527, 0.9215032458305359, 0.9076253175735474, 0.9155598282814026, 0.9132470488548279, 0.9186289310455322, 0.9152066111564636, 0.9193596243858337, 0.9160223007202148, 0.9188121557235718, 0.9172850251197815, 0.9111156463623047, 0.9110395908355713]
[2025-05-21 17:01:52,111]: Mean: 0.91621137
[2025-05-21 17:01:52,112]: Min: 0.90447485
[2025-05-21 17:01:52,112]: Max: 0.93117660
[2025-05-21 17:01:52,112]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-21 17:01:52,112]: Sample Values (25 elements): [-0.041003789752721786, 0.03593503311276436, 0.007697065826505423, 0.07395537942647934, 0.06670910120010376, 0.05183101072907448, 0.07670881599187851, 0.08257932215929031, -0.019242292270064354, 0.04234103485941887, 0.11630626767873764, 0.0009432229562662542, -0.039240624755620956, 0.03169224038720131, -0.13114690780639648, -0.042710501700639725, 0.10130515694618225, 0.07854291051626205, 0.09729580581188202, -0.015544160269200802, -0.10383560508489609, -0.08028554171323776, -0.024278998374938965, 0.04198267310857773, 0.03370654955506325]
[2025-05-21 17:01:52,112]: Mean: 0.00007421
[2025-05-21 17:01:52,113]: Min: -0.19716185
[2025-05-21 17:01:52,113]: Max: 0.18842009
[2025-05-21 17:01:52,113]: 


QAT of ResNet18 with parametrized_hardtanh down to 4 bits...
[2025-05-21 17:01:52,307]: [ResNet18_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-21 17:01:52,331]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 17:03:38,685]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.9017 Train Acc: 0.6867 Eval Loss: 0.9516 Eval Acc: 0.6992 (LR: 0.010000)
[2025-05-21 17:05:25,230]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.6604 Train Acc: 0.7698 Eval Loss: 0.7529 Eval Acc: 0.7473 (LR: 0.010000)
[2025-05-21 17:07:11,329]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.6028 Train Acc: 0.7900 Eval Loss: 0.7454 Eval Acc: 0.7631 (LR: 0.010000)
[2025-05-21 17:08:57,279]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.5575 Train Acc: 0.8061 Eval Loss: 0.7692 Eval Acc: 0.7606 (LR: 0.010000)
[2025-05-21 17:10:43,448]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.5289 Train Acc: 0.8146 Eval Loss: 0.6079 Eval Acc: 0.7975 (LR: 0.010000)
[2025-05-21 17:12:29,604]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.5055 Train Acc: 0.8235 Eval Loss: 0.5899 Eval Acc: 0.8096 (LR: 0.010000)
[2025-05-21 17:14:15,948]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.4817 Train Acc: 0.8317 Eval Loss: 0.5679 Eval Acc: 0.8184 (LR: 0.010000)
[2025-05-21 17:16:02,294]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.4528 Train Acc: 0.8424 Eval Loss: 0.6074 Eval Acc: 0.8081 (LR: 0.010000)
[2025-05-21 17:17:48,429]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.4374 Train Acc: 0.8483 Eval Loss: 0.5430 Eval Acc: 0.8233 (LR: 0.010000)
[2025-05-21 17:19:34,561]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.4218 Train Acc: 0.8529 Eval Loss: 0.5460 Eval Acc: 0.8244 (LR: 0.010000)
[2025-05-21 17:21:20,552]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.4018 Train Acc: 0.8605 Eval Loss: 0.5267 Eval Acc: 0.8330 (LR: 0.010000)
[2025-05-21 17:23:06,424]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.3929 Train Acc: 0.8637 Eval Loss: 0.5363 Eval Acc: 0.8284 (LR: 0.010000)
[2025-05-21 17:24:52,371]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.3781 Train Acc: 0.8666 Eval Loss: 0.5451 Eval Acc: 0.8312 (LR: 0.010000)
[2025-05-21 17:26:38,299]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.3682 Train Acc: 0.8708 Eval Loss: 0.6436 Eval Acc: 0.8021 (LR: 0.010000)
[2025-05-21 17:28:24,528]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.3562 Train Acc: 0.8758 Eval Loss: 0.5908 Eval Acc: 0.8156 (LR: 0.001000)
[2025-05-21 17:30:10,662]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.2475 Train Acc: 0.9136 Eval Loss: 0.4257 Eval Acc: 0.8706 (LR: 0.001000)
[2025-05-21 17:31:56,999]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.2206 Train Acc: 0.9227 Eval Loss: 0.4290 Eval Acc: 0.8732 (LR: 0.001000)
[2025-05-21 17:33:42,996]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.2104 Train Acc: 0.9255 Eval Loss: 0.4321 Eval Acc: 0.8739 (LR: 0.001000)
[2025-05-21 17:35:29,141]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.2052 Train Acc: 0.9292 Eval Loss: 0.4252 Eval Acc: 0.8745 (LR: 0.001000)
[2025-05-21 17:37:17,017]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.1934 Train Acc: 0.9328 Eval Loss: 0.4263 Eval Acc: 0.8775 (LR: 0.001000)
[2025-05-21 17:39:03,400]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.1914 Train Acc: 0.9331 Eval Loss: 0.4388 Eval Acc: 0.8767 (LR: 0.001000)
[2025-05-21 17:40:49,719]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.1867 Train Acc: 0.9330 Eval Loss: 0.4482 Eval Acc: 0.8740 (LR: 0.001000)
[2025-05-21 17:42:35,894]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.1823 Train Acc: 0.9358 Eval Loss: 0.4461 Eval Acc: 0.8751 (LR: 0.001000)
[2025-05-21 17:44:22,031]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.1751 Train Acc: 0.9389 Eval Loss: 0.4413 Eval Acc: 0.8750 (LR: 0.001000)
[2025-05-21 17:46:08,157]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.1711 Train Acc: 0.9394 Eval Loss: 0.4444 Eval Acc: 0.8749 (LR: 0.001000)
[2025-05-21 17:47:54,090]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.1664 Train Acc: 0.9413 Eval Loss: 0.4402 Eval Acc: 0.8791 (LR: 0.001000)
[2025-05-21 17:49:40,027]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.1649 Train Acc: 0.9419 Eval Loss: 0.4451 Eval Acc: 0.8775 (LR: 0.001000)
[2025-05-21 17:51:26,223]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.1634 Train Acc: 0.9424 Eval Loss: 0.4419 Eval Acc: 0.8781 (LR: 0.001000)
[2025-05-21 17:53:12,367]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.1543 Train Acc: 0.9453 Eval Loss: 0.4527 Eval Acc: 0.8785 (LR: 0.001000)
[2025-05-21 17:54:58,335]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.1545 Train Acc: 0.9455 Eval Loss: 0.4604 Eval Acc: 0.8777 (LR: 0.000100)
[2025-05-21 17:56:44,477]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.1424 Train Acc: 0.9496 Eval Loss: 0.4408 Eval Acc: 0.8812 (LR: 0.000100)
[2025-05-21 17:58:30,636]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.1391 Train Acc: 0.9521 Eval Loss: 0.4432 Eval Acc: 0.8810 (LR: 0.000100)
[2025-05-21 18:00:17,005]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.1372 Train Acc: 0.9522 Eval Loss: 0.4412 Eval Acc: 0.8821 (LR: 0.000100)
[2025-05-21 18:02:03,364]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.1359 Train Acc: 0.9521 Eval Loss: 0.4440 Eval Acc: 0.8826 (LR: 0.000100)
[2025-05-21 18:03:49,636]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.1388 Train Acc: 0.9515 Eval Loss: 0.4421 Eval Acc: 0.8820 (LR: 0.000100)
[2025-05-21 18:05:35,625]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.1351 Train Acc: 0.9537 Eval Loss: 0.4438 Eval Acc: 0.8829 (LR: 0.000100)
[2025-05-21 18:07:21,662]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.1355 Train Acc: 0.9532 Eval Loss: 0.4432 Eval Acc: 0.8837 (LR: 0.000100)
[2025-05-21 18:09:07,728]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.1343 Train Acc: 0.9519 Eval Loss: 0.4466 Eval Acc: 0.8813 (LR: 0.000100)
[2025-05-21 18:10:53,856]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.1289 Train Acc: 0.9548 Eval Loss: 0.4461 Eval Acc: 0.8821 (LR: 0.000100)
[2025-05-21 18:12:41,353]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.1325 Train Acc: 0.9545 Eval Loss: 0.4427 Eval Acc: 0.8828 (LR: 0.000100)
[2025-05-21 18:14:27,804]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.1327 Train Acc: 0.9530 Eval Loss: 0.4468 Eval Acc: 0.8815 (LR: 0.000100)
[2025-05-21 18:16:14,047]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.1308 Train Acc: 0.9542 Eval Loss: 0.4458 Eval Acc: 0.8826 (LR: 0.000100)
[2025-05-21 18:17:59,875]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.1317 Train Acc: 0.9541 Eval Loss: 0.4470 Eval Acc: 0.8829 (LR: 0.000100)
[2025-05-21 18:19:46,020]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.1324 Train Acc: 0.9548 Eval Loss: 0.4485 Eval Acc: 0.8829 (LR: 0.000100)
[2025-05-21 18:21:31,946]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.1290 Train Acc: 0.9542 Eval Loss: 0.4471 Eval Acc: 0.8845 (LR: 0.000010)
[2025-05-21 18:23:17,908]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.1257 Train Acc: 0.9557 Eval Loss: 0.4474 Eval Acc: 0.8822 (LR: 0.000010)
[2025-05-21 18:25:03,672]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.1314 Train Acc: 0.9537 Eval Loss: 0.4463 Eval Acc: 0.8839 (LR: 0.000010)
[2025-05-21 18:26:49,594]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.1294 Train Acc: 0.9549 Eval Loss: 0.4458 Eval Acc: 0.8828 (LR: 0.000010)
[2025-05-21 18:28:35,583]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.1278 Train Acc: 0.9553 Eval Loss: 0.4470 Eval Acc: 0.8835 (LR: 0.000010)
[2025-05-21 18:30:21,695]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.1314 Train Acc: 0.9545 Eval Loss: 0.4498 Eval Acc: 0.8822 (LR: 0.000010)
[2025-05-21 18:32:07,640]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.1292 Train Acc: 0.9531 Eval Loss: 0.4480 Eval Acc: 0.8835 (LR: 0.000010)
[2025-05-21 18:33:53,798]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.1274 Train Acc: 0.9547 Eval Loss: 0.4493 Eval Acc: 0.8836 (LR: 0.000010)
[2025-05-21 18:35:39,735]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.1278 Train Acc: 0.9554 Eval Loss: 0.4463 Eval Acc: 0.8833 (LR: 0.000010)
[2025-05-21 18:37:25,633]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.1267 Train Acc: 0.9555 Eval Loss: 0.4466 Eval Acc: 0.8839 (LR: 0.000010)
[2025-05-21 18:39:11,591]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.1289 Train Acc: 0.9538 Eval Loss: 0.4477 Eval Acc: 0.8833 (LR: 0.000010)
[2025-05-21 18:40:57,542]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.1243 Train Acc: 0.9563 Eval Loss: 0.4477 Eval Acc: 0.8839 (LR: 0.000010)
[2025-05-21 18:42:43,479]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.1267 Train Acc: 0.9562 Eval Loss: 0.4476 Eval Acc: 0.8839 (LR: 0.000010)
[2025-05-21 18:44:29,511]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.1293 Train Acc: 0.9548 Eval Loss: 0.4500 Eval Acc: 0.8831 (LR: 0.000010)
[2025-05-21 18:46:15,332]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.1292 Train Acc: 0.9550 Eval Loss: 0.4511 Eval Acc: 0.8817 (LR: 0.000010)
[2025-05-21 18:48:13,122]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.1269 Train Acc: 0.9562 Eval Loss: 0.4523 Eval Acc: 0.8825 (LR: 0.000010)
[2025-05-21 18:48:13,122]: [ResNet18_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8845
[2025-05-21 18:48:13,224]: 


Quantization of model down to 4 bits finished
[2025-05-21 18:48:13,224]: Model Architecture:
[2025-05-21 18:48:13,271]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0286], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21638937294483185, max_val=0.21247248351573944)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1085], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0420], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27763432264328003, max_val=0.3517042398452759)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1241], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0355], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2924889922142029, max_val=0.2396438866853714)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0807], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0223], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17104001343250275, max_val=0.16307026147842407)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1375], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0225], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16895943880081177, max_val=0.16885846853256226)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0876], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0132], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0966939702630043, max_val=0.10197057574987411)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0326], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24947868287563324, max_val=0.23903723061084747)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09108605980873108, max_val=0.08922622352838516)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0622], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08634129166603088, max_val=0.09341151267290115)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1202], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0103], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08002183586359024, max_val=0.0752149447798729)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0673], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06352336704730988, max_val=0.06480439007282257)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0192], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1413341611623764, max_val=0.14639809727668762)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1003], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0095], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0698452740907669, max_val=0.07200928032398224)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06708388775587082, max_val=0.06167841702699661)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0962], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0074], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.056986965239048004, max_val=0.0544087179005146)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0758], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0058], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.043348751962184906, max_val=0.04431068152189255)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09105604141950607, max_val=0.09763975441455841)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0058], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04394889622926712, max_val=0.043054837733507156)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0460], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.030312448740005493, max_val=0.031089123338460922)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1833], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 18:48:13,272]: 
Model Weights:
[2025-05-21 18:48:13,272]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-21 18:48:13,272]: Sample Values (25 elements): [0.14071044325828552, 0.10490056872367859, -0.07370815426111221, 0.20189037919044495, 0.0965203270316124, 0.3025723993778229, -0.21226796507835388, -0.1094580665230751, -0.2898431718349457, 0.005278474651277065, -0.04641878232359886, 0.30291324853897095, -0.06221326068043709, -0.2132105827331543, -0.2350599467754364, 0.07303692400455475, 0.008562660776078701, 0.24008017778396606, 0.30655187368392944, 0.24943387508392334, -0.16356168687343597, -0.13569489121437073, -0.13406792283058167, -0.0846032127737999, 0.20956668257713318]
[2025-05-21 18:48:13,272]: Mean: 0.00041628
[2025-05-21 18:48:13,272]: Min: -0.51721430
[2025-05-21 18:48:13,273]: Max: 0.53330803
[2025-05-21 18:48:13,273]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-21 18:48:13,273]: Sample Values (25 elements): [0.7277936339378357, 0.8828368782997131, 0.7118814587593079, 0.7853178381919861, 0.8164993524551392, 0.7059292793273926, 0.5669406056404114, 0.9210482835769653, 0.7590015530586243, 0.7943153977394104, 0.8623771071434021, 0.7721017599105835, 0.9183719158172607, 0.8764162063598633, 0.8679404258728027, 0.7569179534912109, 0.6130722165107727, 0.8013891577720642, 0.7050794959068298, 0.6604434847831726, 0.9608022570610046, 0.8921996355056763, 0.9707517027854919, 1.0954216718673706, 0.7889901399612427]
[2025-05-21 18:48:13,273]: Mean: 0.82245940
[2025-05-21 18:48:13,273]: Min: 0.56655425
[2025-05-21 18:48:13,273]: Max: 1.09542167
[2025-05-21 18:48:13,275]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 18:48:13,275]: Sample Values (25 elements): [0.028590789064764977, 0.028590789064764977, 0.1143631562590599, 0.0, -0.028590789064764977, 0.0, 0.028590789064764977, -0.028590789064764977, 0.028590789064764977, -0.028590789064764977, -0.028590789064764977, -0.028590789064764977, -0.028590789064764977, 0.028590789064764977, -0.028590789064764977, 0.0, 0.0, 0.0, 0.028590789064764977, -0.028590789064764977, 0.05718157812952995, -0.08577236533164978, 0.028590789064764977, -0.028590789064764977, 0.028590789064764977]
[2025-05-21 18:48:13,275]: Mean: 0.00020863
[2025-05-21 18:48:13,275]: Min: -0.22872631
[2025-05-21 18:48:13,276]: Max: 0.20013553
[2025-05-21 18:48:13,276]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 18:48:13,276]: Sample Values (25 elements): [0.9283275008201599, 0.9873619675636292, 1.0359622240066528, 0.9717947244644165, 0.874011754989624, 0.9466768503189087, 0.8859137892723083, 0.9295575618743896, 0.8898197412490845, 0.933951735496521, 0.918821394443512, 0.845317542552948, 0.9635904431343079, 0.9353702664375305, 0.9658387303352356, 1.0865414142608643, 0.9171504974365234, 0.9120451211929321, 0.8348150253295898, 0.8821658492088318, 0.8767789006233215, 0.8443146347999573, 0.9538994431495667, 0.8374725580215454, 0.8806875944137573]
[2025-05-21 18:48:13,276]: Mean: 0.92173535
[2025-05-21 18:48:13,276]: Min: 0.79334766
[2025-05-21 18:48:13,277]: Max: 1.08654141
[2025-05-21 18:48:13,278]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 18:48:13,278]: Sample Values (25 elements): [0.04195590317249298, -0.04195590317249298, 0.0, -0.04195590317249298, -0.04195590317249298, 0.0, 0.0, 0.0, 0.04195590317249298, 0.0, 0.04195590317249298, 0.0, 0.0, 0.08391180634498596, 0.0, -0.04195590317249298, -0.04195590317249298, 0.0, 0.0, 0.04195590317249298, -0.04195590317249298, 0.0, 0.04195590317249298, -0.08391180634498596, -0.04195590317249298]
[2025-05-21 18:48:13,278]: Mean: -0.00006146
[2025-05-21 18:48:13,279]: Min: -0.29369134
[2025-05-21 18:48:13,279]: Max: 0.33564723
[2025-05-21 18:48:13,279]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 18:48:13,279]: Sample Values (25 elements): [0.919786274433136, 0.8505217432975769, 0.8737099170684814, 0.8892055153846741, 0.8933583498001099, 1.4227067232131958, 0.9329549074172974, 0.8418142199516296, 0.9773675799369812, 0.8083469867706299, 0.8616762757301331, 0.9198988080024719, 0.8607118725776672, 0.7907320857048035, 0.8482943177223206, 0.9108739495277405, 0.9467579126358032, 0.8568183779716492, 0.8270097970962524, 0.9401175379753113, 0.8458461165428162, 0.9133325219154358, 0.8437942862510681, 0.9756824970245361, 1.022480845451355]
[2025-05-21 18:48:13,279]: Mean: 0.91240799
[2025-05-21 18:48:13,279]: Min: 0.73476803
[2025-05-21 18:48:13,280]: Max: 1.42270672
[2025-05-21 18:48:13,281]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 18:48:13,281]: Sample Values (25 elements): [0.0, 0.07095105201005936, -0.03547552600502968, 0.0, -0.03547552600502968, 0.0, 0.0, 0.0, 0.03547552600502968, 0.0, -0.03547552600502968, -0.03547552600502968, 0.03547552600502968, 0.03547552600502968, -0.10642658174037933, -0.03547552600502968, -0.03547552600502968, 0.0, 0.0, 0.03547552600502968, 0.03547552600502968, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 18:48:13,281]: Mean: -0.00006929
[2025-05-21 18:48:13,281]: Min: -0.28380421
[2025-05-21 18:48:13,282]: Max: 0.24832869
[2025-05-21 18:48:13,282]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 18:48:13,282]: Sample Values (25 elements): [0.9654015302658081, 0.8910102844238281, 0.8792376518249512, 0.920002818107605, 1.0133966207504272, 0.9204424023628235, 0.9355578422546387, 0.9747898578643799, 0.921905517578125, 0.9061815738677979, 0.9267908930778503, 0.9219039082527161, 0.8697615265846252, 0.8936119079589844, 0.9251587986946106, 0.9041868448257446, 0.9205780625343323, 0.9182628989219666, 0.9187944531440735, 0.9212964773178101, 0.9348459243774414, 0.9235695004463196, 0.9482088088989258, 0.8939898610115051, 0.8910129070281982]
[2025-05-21 18:48:13,282]: Mean: 0.92395771
[2025-05-21 18:48:13,282]: Min: 0.86240768
[2025-05-21 18:48:13,282]: Max: 1.01339662
[2025-05-21 18:48:13,283]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 18:48:13,284]: Sample Values (25 elements): [-0.022274022921919823, 0.0, 0.022274022921919823, 0.044548045843839645, 0.022274022921919823, -0.022274022921919823, 0.022274022921919823, 0.044548045843839645, 0.044548045843839645, -0.022274022921919823, -0.022274022921919823, -0.044548045843839645, -0.022274022921919823, 0.022274022921919823, 0.0, -0.044548045843839645, -0.022274022921919823, 0.08909609168767929, -0.044548045843839645, 0.0, -0.044548045843839645, 0.0, 0.0, 0.0, 0.022274022921919823]
[2025-05-21 18:48:13,284]: Mean: 0.00025015
[2025-05-21 18:48:13,284]: Min: -0.17819218
[2025-05-21 18:48:13,284]: Max: 0.15591817
[2025-05-21 18:48:13,285]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 18:48:13,285]: Sample Values (25 elements): [1.0013978481292725, 0.8798376321792603, 0.8566084504127502, 0.9113712906837463, 0.9375710487365723, 0.8965309858322144, 0.936540961265564, 0.9411461353302002, 0.9872279763221741, 0.8256161212921143, 0.8973551988601685, 0.9451478123664856, 1.2564971446990967, 0.8606683015823364, 0.9103695154190063, 0.8976202011108398, 0.9258710145950317, 1.1110033988952637, 0.8836314678192139, 0.8354037404060364, 0.9267774224281311, 0.8555882573127747, 1.001009225845337, 0.9343703985214233, 0.8877440690994263]
[2025-05-21 18:48:13,285]: Mean: 0.90316355
[2025-05-21 18:48:13,285]: Min: 0.77746892
[2025-05-21 18:48:13,285]: Max: 1.25649714
[2025-05-21 18:48:13,287]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-21 18:48:13,288]: Sample Values (25 elements): [0.0, -0.04504238814115524, -0.02252119407057762, 0.0, -0.02252119407057762, 0.02252119407057762, 0.0, 0.0, 0.0, -0.02252119407057762, 0.0, 0.02252119407057762, 0.0, -0.04504238814115524, 0.0, -0.04504238814115524, 0.04504238814115524, 0.0, -0.02252119407057762, -0.02252119407057762, 0.02252119407057762, -0.04504238814115524, -0.02252119407057762, 0.0, 0.0]
[2025-05-21 18:48:13,288]: Mean: 0.00002596
[2025-05-21 18:48:13,288]: Min: -0.18016955
[2025-05-21 18:48:13,288]: Max: 0.15764835
[2025-05-21 18:48:13,288]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-21 18:48:13,289]: Sample Values (25 elements): [0.9361761212348938, 0.9025593400001526, 0.9342817068099976, 0.9104783535003662, 0.9051879048347473, 0.9291685223579407, 0.9462161064147949, 0.9019391536712646, 0.9243826270103455, 0.9133664965629578, 0.9197869300842285, 0.8973239660263062, 0.9046329259872437, 0.9146694540977478, 0.9263268113136292, 0.9143006801605225, 0.9478327035903931, 0.8892389535903931, 0.9156457781791687, 0.8984705209732056, 0.9180630445480347, 0.9021211266517639, 0.9098312854766846, 0.9170516729354858, 0.9369066953659058]
[2025-05-21 18:48:13,289]: Mean: 0.91780776
[2025-05-21 18:48:13,289]: Min: 0.88923895
[2025-05-21 18:48:13,289]: Max: 0.96017867
[2025-05-21 18:48:13,290]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 18:48:13,292]: Sample Values (25 elements): [0.013244304805994034, 0.026488609611988068, -0.0397329144179821, -0.026488609611988068, 0.0, -0.026488609611988068, -0.013244304805994034, -0.013244304805994034, -0.026488609611988068, 0.026488609611988068, 0.0397329144179821, 0.013244304805994034, 0.0, 0.0, -0.013244304805994034, 0.0, 0.0, 0.026488609611988068, -0.026488609611988068, 0.026488609611988068, 0.013244304805994034, 0.052977219223976135, -0.026488609611988068, 0.026488609611988068, 0.0]
[2025-05-21 18:48:13,292]: Mean: 0.00001590
[2025-05-21 18:48:13,292]: Min: -0.09271014
[2025-05-21 18:48:13,292]: Max: 0.10595444
[2025-05-21 18:48:13,292]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-21 18:48:13,292]: Sample Values (25 elements): [0.9200625419616699, 0.9395473003387451, 0.9494290947914124, 0.9237827062606812, 0.9517202377319336, 0.9385598301887512, 0.913942277431488, 0.9314560890197754, 0.945124089717865, 0.9081676602363586, 0.9008438587188721, 0.9242846965789795, 0.9003092050552368, 0.9463402628898621, 0.9086955189704895, 0.9271917939186096, 0.9659826159477234, 0.9228829145431519, 0.9072965979576111, 0.932288408279419, 0.9199539422988892, 0.9568127989768982, 0.9079384207725525, 0.9589023590087891, 0.9443346858024597]
[2025-05-21 18:48:13,293]: Mean: 0.92394555
[2025-05-21 18:48:13,293]: Min: 0.87168127
[2025-05-21 18:48:13,293]: Max: 0.99127018
[2025-05-21 18:48:13,294]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-21 18:48:13,294]: Sample Values (25 elements): [0.03256772458553314, 0.03256772458553314, 0.03256772458553314, 0.09770317375659943, 0.0, -0.03256772458553314, 0.13027089834213257, -0.03256772458553314, -0.03256772458553314, -0.06513544917106628, 0.0, 0.06513544917106628, -0.13027089834213257, -0.06513544917106628, 0.09770317375659943, 0.06513544917106628, 0.03256772458553314, -0.06513544917106628, 0.09770317375659943, -0.06513544917106628, -0.13027089834213257, -0.06513544917106628, -0.03256772458553314, 0.06513544917106628, 0.09770317375659943]
[2025-05-21 18:48:13,294]: Mean: -0.00024251
[2025-05-21 18:48:13,294]: Min: -0.26054180
[2025-05-21 18:48:13,295]: Max: 0.22797407
[2025-05-21 18:48:13,295]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-21 18:48:13,295]: Sample Values (25 elements): [0.9284189939498901, 0.869026243686676, 0.9128323197364807, 0.9635737538337708, 0.8513242602348328, 0.8814053535461426, 0.8642057180404663, 0.9088571071624756, 0.8767766356468201, 0.8905256986618042, 0.9147830605506897, 0.9147030115127563, 0.8798631429672241, 0.9121427536010742, 0.9033824801445007, 0.9030123949050903, 0.8784234523773193, 0.8459471464157104, 0.8707917332649231, 0.888633131980896, 0.9386259317398071, 0.8671595454216003, 0.8960301280021667, 0.8762902617454529, 0.8890801668167114]
[2025-05-21 18:48:13,295]: Mean: 0.89038086
[2025-05-21 18:48:13,295]: Min: 0.84064913
[2025-05-21 18:48:13,295]: Max: 0.97824049
[2025-05-21 18:48:13,296]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 18:48:13,298]: Sample Values (25 elements): [-0.024041637778282166, 0.024041637778282166, -0.012020818889141083, -0.012020818889141083, 0.0, 0.03606245666742325, 0.0, -0.024041637778282166, -0.024041637778282166, 0.0, 0.0, 0.024041637778282166, 0.0, -0.024041637778282166, -0.024041637778282166, -0.012020818889141083, -0.024041637778282166, 0.024041637778282166, 0.012020818889141083, 0.024041637778282166, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 18:48:13,298]: Mean: -0.00003677
[2025-05-21 18:48:13,298]: Min: -0.09616655
[2025-05-21 18:48:13,298]: Max: 0.08414573
[2025-05-21 18:48:13,298]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-21 18:48:13,298]: Sample Values (25 elements): [0.9196876287460327, 0.9266651272773743, 0.9231956005096436, 0.916445791721344, 0.9136248230934143, 0.9164186716079712, 0.9207624197006226, 0.9270039200782776, 0.9348961710929871, 0.9267930388450623, 0.9183735847473145, 0.9581054449081421, 0.923728883266449, 0.9138874411582947, 0.9147392511367798, 0.9278088808059692, 0.9181557297706604, 0.9357128143310547, 0.9112833142280579, 0.9149609208106995, 0.9043055772781372, 0.9306192398071289, 0.9138653874397278, 0.9112111330032349, 0.9099366068840027]
[2025-05-21 18:48:13,299]: Mean: 0.92279828
[2025-05-21 18:48:13,299]: Min: 0.90081328
[2025-05-21 18:48:13,299]: Max: 0.97352356
[2025-05-21 18:48:13,300]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 18:48:13,301]: Sample Values (25 elements): [-0.011983520351350307, 0.011983520351350307, -0.011983520351350307, 0.023967040702700615, 0.023967040702700615, -0.03595056012272835, 0.0, -0.011983520351350307, 0.03595056012272835, 0.0, 0.011983520351350307, -0.011983520351350307, -0.023967040702700615, -0.023967040702700615, 0.03595056012272835, -0.011983520351350307, -0.011983520351350307, -0.011983520351350307, -0.03595056012272835, -0.04793408140540123, -0.04793408140540123, -0.011983520351350307, -0.023967040702700615, 0.0, -0.03595056012272835]
[2025-05-21 18:48:13,301]: Mean: 0.00011443
[2025-05-21 18:48:13,302]: Min: -0.08388464
[2025-05-21 18:48:13,302]: Max: 0.09586816
[2025-05-21 18:48:13,302]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-21 18:48:13,302]: Sample Values (25 elements): [0.9301494359970093, 0.9004194140434265, 0.9279760718345642, 0.8886033296585083, 0.9311663508415222, 0.8993189334869385, 0.9498064517974854, 0.9166589379310608, 0.9296107292175293, 0.9345048069953918, 0.9211739301681519, 0.9032070636749268, 0.9191185235977173, 0.9063543081283569, 0.9448738694190979, 0.9007494449615479, 0.954309344291687, 0.9104208946228027, 0.9215237498283386, 0.9412200450897217, 0.8899355530738831, 0.9223097562789917, 0.9130039811134338, 0.9497454762458801, 0.8965525031089783]
[2025-05-21 18:48:13,302]: Mean: 0.91930109
[2025-05-21 18:48:13,302]: Min: 0.86846954
[2025-05-21 18:48:13,302]: Max: 0.97994918
[2025-05-21 18:48:13,303]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-21 18:48:13,307]: Sample Values (25 elements): [0.020698241889476776, 0.020698241889476776, 0.020698241889476776, -0.020698241889476776, 0.031047362834215164, 0.010349120944738388, -0.010349120944738388, -0.031047362834215164, -0.010349120944738388, -0.020698241889476776, 0.010349120944738388, -0.010349120944738388, 0.020698241889476776, 0.020698241889476776, -0.010349120944738388, -0.04139648377895355, -0.031047362834215164, -0.020698241889476776, -0.010349120944738388, 0.0, 0.0, 0.020698241889476776, 0.0, -0.031047362834215164, -0.020698241889476776]
[2025-05-21 18:48:13,307]: Mean: 0.00013167
[2025-05-21 18:48:13,307]: Min: -0.08279297
[2025-05-21 18:48:13,307]: Max: 0.07244384
[2025-05-21 18:48:13,307]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-21 18:48:13,307]: Sample Values (25 elements): [0.9268971681594849, 0.9212392568588257, 0.9129820466041565, 0.9117910861968994, 0.9218705296516418, 0.9108263850212097, 0.9103613495826721, 0.9238284230232239, 0.9187204241752625, 0.9334853291511536, 0.9164942502975464, 0.9235360622406006, 0.9175888895988464, 0.9128597378730774, 0.9105240106582642, 0.8989460468292236, 0.9144415855407715, 0.9046326279640198, 0.9134877920150757, 0.9121981263160706, 0.9197743535041809, 0.9203242659568787, 0.9110050201416016, 0.9207596778869629, 0.9120416045188904]
[2025-05-21 18:48:13,308]: Mean: 0.91559482
[2025-05-21 18:48:13,308]: Min: 0.89825225
[2025-05-21 18:48:13,308]: Max: 0.93913674
[2025-05-21 18:48:13,309]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 18:48:13,315]: Sample Values (25 elements): [-0.017110366374254227, -0.008555183187127113, -0.017110366374254227, 0.0, 0.0, 0.0, -0.03422073274850845, 0.008555183187127113, 0.0, 0.0, 0.008555183187127113, 0.017110366374254227, -0.008555183187127113, -0.02566554956138134, 0.008555183187127113, 0.0, 0.008555183187127113, 0.0, 0.0, 0.008555183187127113, -0.008555183187127113, 0.0, -0.008555183187127113, 0.008555183187127113, -0.017110366374254227]
[2025-05-21 18:48:13,315]: Mean: 0.00003124
[2025-05-21 18:48:13,316]: Min: -0.05988628
[2025-05-21 18:48:13,316]: Max: 0.06844147
[2025-05-21 18:48:13,316]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-21 18:48:13,316]: Sample Values (25 elements): [0.9471370577812195, 0.9381685853004456, 0.9417116641998291, 0.9023447632789612, 0.9205867052078247, 0.9258154034614563, 0.9256195425987244, 0.9083683490753174, 0.9137123227119446, 0.9415668249130249, 0.935243546962738, 0.9198009371757507, 0.9368661642074585, 0.9316574335098267, 0.9263015985488892, 0.9201520681381226, 0.928722620010376, 0.9308261275291443, 0.9497931003570557, 0.91976398229599, 0.9242892265319824, 0.9235494136810303, 0.9342965483665466, 0.9179782867431641, 0.9104976654052734]
[2025-05-21 18:48:13,316]: Mean: 0.92707908
[2025-05-21 18:48:13,316]: Min: 0.89684093
[2025-05-21 18:48:13,317]: Max: 0.96597499
[2025-05-21 18:48:13,317]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-21 18:48:13,318]: Sample Values (25 elements): [0.09591075778007507, -0.03836430236697197, -0.057546451687812805, 0.019182151183485985, -0.03836430236697197, -0.07672860473394394, -0.057546451687812805, 0.0, 0.019182151183485985, -0.019182151183485985, 0.019182151183485985, -0.03836430236697197, 0.03836430236697197, 0.057546451687812805, 0.07672860473394394, 0.03836430236697197, -0.057546451687812805, -0.03836430236697197, -0.019182151183485985, -0.03836430236697197, 0.0, -0.03836430236697197, 0.03836430236697197, 0.07672860473394394, -0.07672860473394394]
[2025-05-21 18:48:13,318]: Mean: -0.00022830
[2025-05-21 18:48:13,318]: Min: -0.13427506
[2025-05-21 18:48:13,318]: Max: 0.15345721
[2025-05-21 18:48:13,319]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-21 18:48:13,319]: Sample Values (25 elements): [0.8793118000030518, 0.9012843370437622, 0.8986883163452148, 0.8972752690315247, 0.9169982075691223, 0.872861385345459, 0.8998568058013916, 0.9078969955444336, 0.9004335403442383, 0.8884912133216858, 0.8893505930900574, 0.880359411239624, 0.8963688015937805, 0.8697527050971985, 0.8987773060798645, 0.9217678904533386, 0.8952333331108093, 0.9011651873588562, 0.8926296234130859, 0.8794885277748108, 0.9014208316802979, 0.8925574421882629, 0.8896164298057556, 0.8832785487174988, 0.8999945521354675]
[2025-05-21 18:48:13,319]: Mean: 0.89242578
[2025-05-21 18:48:13,319]: Min: 0.86670142
[2025-05-21 18:48:13,319]: Max: 0.94447112
[2025-05-21 18:48:13,320]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 18:48:13,328]: Sample Values (25 elements): [-0.00945697259157896, 0.00945697259157896, 0.01891394518315792, 0.00945697259157896, -0.01891394518315792, 0.0, -0.00945697259157896, 0.01891394518315792, 0.00945697259157896, 0.0, -0.00945697259157896, -0.00945697259157896, 0.00945697259157896, 0.0, 0.028370916843414307, -0.028370916843414307, -0.00945697259157896, -0.01891394518315792, -0.00945697259157896, -0.00945697259157896, -0.00945697259157896, -0.00945697259157896, -0.00945697259157896, -0.01891394518315792, 0.00945697259157896]
[2025-05-21 18:48:13,328]: Mean: -0.00005936
[2025-05-21 18:48:13,328]: Min: -0.06619881
[2025-05-21 18:48:13,328]: Max: 0.07565578
[2025-05-21 18:48:13,328]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-21 18:48:13,329]: Sample Values (25 elements): [0.9219232201576233, 0.9201526641845703, 0.9095883965492249, 0.918226420879364, 0.918975830078125, 0.9157627820968628, 0.9290808439254761, 0.917629063129425, 0.9436599016189575, 0.915046215057373, 0.9136641621589661, 0.9176526665687561, 0.9158896803855896, 0.9121275544166565, 0.9352210760116577, 0.9118093848228455, 0.9155672788619995, 0.9212092757225037, 0.9200976490974426, 0.9257783889770508, 0.9358629584312439, 0.9174441695213318, 0.9203882217407227, 0.9205808639526367, 0.9276531338691711]
[2025-05-21 18:48:13,329]: Mean: 0.92004406
[2025-05-21 18:48:13,329]: Min: 0.90361559
[2025-05-21 18:48:13,329]: Max: 0.94997406
[2025-05-21 18:48:13,330]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 18:48:13,337]: Sample Values (25 elements): [0.0, -0.008584153838455677, -0.025752462446689606, -0.017168307676911354, -0.017168307676911354, -0.017168307676911354, 0.0, -0.025752462446689606, 0.008584153838455677, -0.008584153838455677, -0.008584153838455677, -0.025752462446689606, -0.008584153838455677, 0.008584153838455677, -0.008584153838455677, -0.008584153838455677, 0.0, 0.008584153838455677, 0.008584153838455677, 0.0, -0.017168307676911354, -0.017168307676911354, 0.017168307676911354, -0.025752462446689606, -0.017168307676911354]
[2025-05-21 18:48:13,337]: Mean: -0.00000314
[2025-05-21 18:48:13,338]: Min: -0.06867323
[2025-05-21 18:48:13,338]: Max: 0.06008908
[2025-05-21 18:48:13,338]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-21 18:48:13,338]: Sample Values (25 elements): [0.9118853807449341, 0.9242600202560425, 0.9038709998130798, 0.9335172772407532, 0.9442984461784363, 0.9143504500389099, 0.9031959772109985, 0.9175962209701538, 0.906989336013794, 0.9232497215270996, 0.9174301028251648, 0.8987089395523071, 0.926510214805603, 0.9445531964302063, 0.903169572353363, 0.914423942565918, 0.917545735836029, 0.9103408455848694, 0.9292395710945129, 0.929225742816925, 0.9097153544425964, 0.9366164803504944, 0.915752649307251, 0.9136201739311218, 0.9128741025924683]
[2025-05-21 18:48:13,338]: Mean: 0.91936970
[2025-05-21 18:48:13,338]: Min: 0.88081533
[2025-05-21 18:48:13,339]: Max: 0.95781696
[2025-05-21 18:48:13,340]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-21 18:48:13,356]: Sample Values (25 elements): [-0.007426380645483732, -0.007426380645483732, -0.007426380645483732, -0.007426380645483732, 0.02970552258193493, -0.014852761290967464, 0.0, 0.014852761290967464, -0.007426380645483732, 0.0, 0.02227914147078991, -0.007426380645483732, 0.0, -0.02227914147078991, 0.02227914147078991, 0.007426380645483732, 0.0, 0.007426380645483732, 0.014852761290967464, -0.02227914147078991, 0.007426380645483732, 0.007426380645483732, 0.014852761290967464, -0.014852761290967464, -0.007426380645483732]
[2025-05-21 18:48:13,356]: Mean: -0.00002798
[2025-05-21 18:48:13,357]: Min: -0.05941105
[2025-05-21 18:48:13,357]: Max: 0.05198466
[2025-05-21 18:48:13,357]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-21 18:48:13,358]: Sample Values (25 elements): [0.9124131798744202, 0.9085519313812256, 0.9104320406913757, 0.9151488542556763, 0.910472571849823, 0.9074559807777405, 0.9105824828147888, 0.9124231934547424, 0.9146366715431213, 0.9077785015106201, 0.9173481464385986, 0.909469723701477, 0.9098362922668457, 0.9179190993309021, 0.9171500205993652, 0.9093896150588989, 0.9115164875984192, 0.9103719592094421, 0.9257354140281677, 0.9186574220657349, 0.9167009592056274, 0.9185198545455933, 0.9187757968902588, 0.9166102409362793, 0.9194698333740234]
[2025-05-21 18:48:13,358]: Mean: 0.91396916
[2025-05-21 18:48:13,358]: Min: 0.90294802
[2025-05-21 18:48:13,358]: Max: 0.93083161
[2025-05-21 18:48:13,361]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 18:48:13,440]: Sample Values (25 elements): [0.01753188669681549, -0.01753188669681549, 0.011687925085425377, 0.011687925085425377, 0.0, 0.011687925085425377, 0.011687925085425377, -0.0058439625427126884, 0.0, -0.011687925085425377, 0.0058439625427126884, -0.0058439625427126884, 0.0, 0.011687925085425377, -0.01753188669681549, 0.0058439625427126884, -0.011687925085425377, 0.0, -0.011687925085425377, -0.011687925085425377, 0.0058439625427126884, 0.011687925085425377, 0.0, -0.0058439625427126884, 0.0058439625427126884]
[2025-05-21 18:48:13,441]: Mean: -0.00000419
[2025-05-21 18:48:13,441]: Min: -0.04090774
[2025-05-21 18:48:13,441]: Max: 0.04675170
[2025-05-21 18:48:13,441]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-21 18:48:13,443]: Sample Values (25 elements): [0.9180735945701599, 0.9242089986801147, 0.916060209274292, 0.932564377784729, 0.9253097772598267, 0.9243627190589905, 0.9245463609695435, 0.9185823202133179, 0.91889488697052, 0.9396138191223145, 0.9277107119560242, 0.9245999455451965, 0.9211560487747192, 0.9426022171974182, 0.9253430366516113, 0.9204186201095581, 0.9169688820838928, 0.9253097176551819, 0.9314074516296387, 0.9214680194854736, 0.9186100959777832, 0.9298445582389832, 0.9168053269386292, 0.9150679707527161, 0.9224740862846375]
[2025-05-21 18:48:13,443]: Mean: 0.92268306
[2025-05-21 18:48:13,443]: Min: 0.90889382
[2025-05-21 18:48:13,443]: Max: 0.95183295
[2025-05-21 18:48:13,444]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-21 18:48:13,446]: Sample Values (25 elements): [0.0, 0.0, -0.050318874418735504, -0.03773915767669678, 0.025159437209367752, -0.03773915767669678, -0.03773915767669678, 0.0, -0.012579718604683876, -0.025159437209367752, 0.025159437209367752, -0.012579718604683876, 0.012579718604683876, 0.025159437209367752, -0.012579718604683876, 0.06289859116077423, -0.050318874418735504, 0.03773915767669678, 0.06289859116077423, 0.025159437209367752, 0.050318874418735504, -0.07547831535339355, 0.012579718604683876, -0.03773915767669678, 0.03773915767669678]
[2025-05-21 18:48:13,446]: Mean: -0.00002486
[2025-05-21 18:48:13,446]: Min: -0.08805803
[2025-05-21 18:48:13,446]: Max: 0.10063775
[2025-05-21 18:48:13,446]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-21 18:48:13,446]: Sample Values (25 elements): [0.9022473096847534, 0.9103488326072693, 0.9082797765731812, 0.9048038125038147, 0.9075597524642944, 0.9037737250328064, 0.9071982502937317, 0.9090818166732788, 0.9085216522216797, 0.9044435620307922, 0.9037389159202576, 0.9057223796844482, 0.9095003604888916, 0.9049826264381409, 0.9106864929199219, 0.9017239809036255, 0.9070326089859009, 0.9077246189117432, 0.8999651670455933, 0.9020809531211853, 0.8997063040733337, 0.9093770980834961, 0.9069910645484924, 0.9047829508781433, 0.8959679007530212]
[2025-05-21 18:48:13,447]: Mean: 0.90584862
[2025-05-21 18:48:13,447]: Min: 0.89018708
[2025-05-21 18:48:13,447]: Max: 0.92577004
[2025-05-21 18:48:13,448]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 18:48:13,486]: Sample Values (25 elements): [0.017400745302438736, 0.011600497178733349, -0.005800248589366674, -0.011600497178733349, 0.0, 0.0, -0.017400745302438736, 0.005800248589366674, 0.011600497178733349, 0.005800248589366674, -0.011600497178733349, 0.0, -0.017400745302438736, -0.005800248589366674, 0.0, -0.017400745302438736, 0.005800248589366674, 0.011600497178733349, 0.0, -0.005800248589366674, -0.011600497178733349, -0.011600497178733349, -0.005800248589366674, -0.005800248589366674, 0.005800248589366674]
[2025-05-21 18:48:13,486]: Mean: -0.00000055
[2025-05-21 18:48:13,486]: Min: -0.04640199
[2025-05-21 18:48:13,487]: Max: 0.04060174
[2025-05-21 18:48:13,487]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-21 18:48:13,487]: Sample Values (25 elements): [0.9125255942344666, 0.9154418110847473, 0.9196900725364685, 0.9128332734107971, 0.9140940308570862, 0.9183028340339661, 0.9108617901802063, 0.9139178395271301, 0.9100425839424133, 0.9228334426879883, 0.9067362546920776, 0.9129776358604431, 0.9127309322357178, 0.9122918844223022, 0.9130547046661377, 0.9139071702957153, 0.9244145750999451, 0.9131035208702087, 0.9138860702514648, 0.9142704010009766, 0.9243197441101074, 0.9132408499717712, 0.9124311804771423, 0.915319561958313, 0.9135634303092957]
[2025-05-21 18:48:13,487]: Mean: 0.91486764
[2025-05-21 18:48:13,487]: Min: 0.90617180
[2025-05-21 18:48:13,488]: Max: 0.93073332
[2025-05-21 18:48:13,489]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 18:48:13,539]: Sample Values (25 elements): [-0.004093437921255827, -0.012280313298106194, -0.016373751685023308, 0.0, 0.0, 0.0, -0.004093437921255827, -0.008186875842511654, 0.0, 0.012280313298106194, 0.008186875842511654, 0.008186875842511654, -0.012280313298106194, 0.008186875842511654, 0.016373751685023308, 0.008186875842511654, 0.0, 0.004093437921255827, -0.008186875842511654, 0.004093437921255827, -0.004093437921255827, 0.008186875842511654, -0.004093437921255827, -0.008186875842511654, 0.008186875842511654]
[2025-05-21 18:48:13,539]: Mean: -0.00000439
[2025-05-21 18:48:13,540]: Min: -0.02865406
[2025-05-21 18:48:13,540]: Max: 0.03274750
[2025-05-21 18:48:13,540]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-21 18:48:13,541]: Sample Values (25 elements): [0.9173250794410706, 0.9143999218940735, 0.9153624773025513, 0.9174679517745972, 0.9164260625839233, 0.9103657007217407, 0.9123889803886414, 0.9181960225105286, 0.919421911239624, 0.91749107837677, 0.9134182333946228, 0.9130062460899353, 0.9129505753517151, 0.9208736419677734, 0.9232320785522461, 0.9177425503730774, 0.9154342412948608, 0.918337881565094, 0.9161580204963684, 0.9183328747749329, 0.91885906457901, 0.9200258255004883, 0.9180504679679871, 0.9146372079849243, 0.9081863760948181]
[2025-05-21 18:48:13,541]: Mean: 0.91632050
[2025-05-21 18:48:13,541]: Min: 0.90376186
[2025-05-21 18:48:13,541]: Max: 0.92877966
[2025-05-21 18:48:13,541]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-21 18:48:13,541]: Sample Values (25 elements): [-0.05670515447854996, -0.08390186727046967, -0.06818536669015884, 0.050831835716962814, -0.11244331300258636, -0.07719357311725616, -0.04462246224284172, -0.04605355113744736, 0.06421390175819397, 0.01593451201915741, -0.07526644319295883, 0.02101818658411503, 0.11202314496040344, 0.06859417259693146, 0.10540079325437546, -0.08566153049468994, 0.049939800053834915, -0.1071292906999588, 0.05669749528169632, -0.054641760885715485, -0.04424046352505684, 0.05846894904971123, 0.0453394278883934, -0.08814265578985214, -0.05813674256205559]
[2025-05-21 18:48:13,542]: Mean: 0.00007421
[2025-05-21 18:48:13,542]: Min: -0.19380637
[2025-05-21 18:48:13,542]: Max: 0.18286397
