[2025-05-26 18:26:48,408]: 
Training ResNet20 with hardtanh
[2025-05-26 18:27:53,443]: [ResNet20_hardtanh] Epoch: 001 Train Loss: 1.7980 Train Acc: 0.3292 Eval Loss: 1.5469 Eval Acc: 0.4204 (LR: 0.00100000)
[2025-05-26 18:28:41,075]: [ResNet20_hardtanh] Epoch: 002 Train Loss: 1.4408 Train Acc: 0.4755 Eval Loss: 1.3021 Eval Acc: 0.5208 (LR: 0.00100000)
[2025-05-26 18:29:46,231]: [ResNet20_hardtanh] Epoch: 003 Train Loss: 1.2712 Train Acc: 0.5431 Eval Loss: 1.1427 Eval Acc: 0.5834 (LR: 0.00100000)
[2025-05-26 18:30:26,533]: [ResNet20_hardtanh] Epoch: 004 Train Loss: 1.1656 Train Acc: 0.5819 Eval Loss: 1.1147 Eval Acc: 0.5985 (LR: 0.00100000)
[2025-05-26 18:31:31,111]: [ResNet20_hardtanh] Epoch: 005 Train Loss: 1.0787 Train Acc: 0.6153 Eval Loss: 1.0516 Eval Acc: 0.6322 (LR: 0.00100000)
[2025-05-26 18:32:15,169]: [ResNet20_hardtanh] Epoch: 006 Train Loss: 1.0190 Train Acc: 0.6342 Eval Loss: 0.9920 Eval Acc: 0.6421 (LR: 0.00100000)
[2025-05-26 18:33:19,224]: [ResNet20_hardtanh] Epoch: 007 Train Loss: 0.9720 Train Acc: 0.6505 Eval Loss: 1.0635 Eval Acc: 0.6169 (LR: 0.00100000)
[2025-05-26 18:34:09,225]: [ResNet20_hardtanh] Epoch: 008 Train Loss: 0.9262 Train Acc: 0.6699 Eval Loss: 0.8916 Eval Acc: 0.6858 (LR: 0.00100000)
[2025-05-26 18:35:12,085]: [ResNet20_hardtanh] Epoch: 009 Train Loss: 0.8889 Train Acc: 0.6824 Eval Loss: 1.2014 Eval Acc: 0.6026 (LR: 0.00100000)
[2025-05-26 18:36:10,595]: [ResNet20_hardtanh] Epoch: 010 Train Loss: 0.8614 Train Acc: 0.6943 Eval Loss: 0.8935 Eval Acc: 0.6911 (LR: 0.00100000)
[2025-05-26 18:37:03,993]: [ResNet20_hardtanh] Epoch: 011 Train Loss: 0.8327 Train Acc: 0.7045 Eval Loss: 0.8783 Eval Acc: 0.6921 (LR: 0.00100000)
[2025-05-26 18:38:08,678]: [ResNet20_hardtanh] Epoch: 012 Train Loss: 0.8045 Train Acc: 0.7165 Eval Loss: 0.8499 Eval Acc: 0.7041 (LR: 0.00100000)
[2025-05-26 18:38:58,741]: [ResNet20_hardtanh] Epoch: 013 Train Loss: 0.7860 Train Acc: 0.7230 Eval Loss: 0.8092 Eval Acc: 0.7221 (LR: 0.00100000)
[2025-05-26 18:40:03,411]: [ResNet20_hardtanh] Epoch: 014 Train Loss: 0.7625 Train Acc: 0.7278 Eval Loss: 0.8157 Eval Acc: 0.7236 (LR: 0.00100000)
[2025-05-26 18:40:44,425]: [ResNet20_hardtanh] Epoch: 015 Train Loss: 0.7373 Train Acc: 0.7391 Eval Loss: 0.7664 Eval Acc: 0.7357 (LR: 0.00100000)
[2025-05-26 18:41:21,984]: [ResNet20_hardtanh] Epoch: 016 Train Loss: 0.7144 Train Acc: 0.7484 Eval Loss: 0.7532 Eval Acc: 0.7422 (LR: 0.00100000)
[2025-05-26 18:41:58,729]: [ResNet20_hardtanh] Epoch: 017 Train Loss: 0.6967 Train Acc: 0.7546 Eval Loss: 0.7189 Eval Acc: 0.7562 (LR: 0.00100000)
[2025-05-26 18:42:32,910]: [ResNet20_hardtanh] Epoch: 018 Train Loss: 0.6756 Train Acc: 0.7638 Eval Loss: 0.7438 Eval Acc: 0.7444 (LR: 0.00100000)
[2025-05-26 18:43:06,630]: [ResNet20_hardtanh] Epoch: 019 Train Loss: 0.6712 Train Acc: 0.7641 Eval Loss: 0.7980 Eval Acc: 0.7321 (LR: 0.00100000)
[2025-05-26 18:43:40,746]: [ResNet20_hardtanh] Epoch: 020 Train Loss: 0.6501 Train Acc: 0.7704 Eval Loss: 0.6854 Eval Acc: 0.7720 (LR: 0.00100000)
[2025-05-26 18:44:14,444]: [ResNet20_hardtanh] Epoch: 021 Train Loss: 0.6352 Train Acc: 0.7778 Eval Loss: 0.6811 Eval Acc: 0.7705 (LR: 0.00100000)
[2025-05-26 18:44:48,133]: [ResNet20_hardtanh] Epoch: 022 Train Loss: 0.6221 Train Acc: 0.7819 Eval Loss: 0.8380 Eval Acc: 0.7261 (LR: 0.00100000)
[2025-05-26 18:45:21,856]: [ResNet20_hardtanh] Epoch: 023 Train Loss: 0.6094 Train Acc: 0.7867 Eval Loss: 0.6726 Eval Acc: 0.7736 (LR: 0.00100000)
[2025-05-26 18:45:55,548]: [ResNet20_hardtanh] Epoch: 024 Train Loss: 0.5991 Train Acc: 0.7885 Eval Loss: 0.6622 Eval Acc: 0.7739 (LR: 0.00100000)
[2025-05-26 18:46:29,247]: [ResNet20_hardtanh] Epoch: 025 Train Loss: 0.5874 Train Acc: 0.7947 Eval Loss: 0.8704 Eval Acc: 0.7141 (LR: 0.00100000)
[2025-05-26 18:47:02,955]: [ResNet20_hardtanh] Epoch: 026 Train Loss: 0.5762 Train Acc: 0.8001 Eval Loss: 0.7434 Eval Acc: 0.7568 (LR: 0.00100000)
[2025-05-26 18:47:36,672]: [ResNet20_hardtanh] Epoch: 027 Train Loss: 0.5680 Train Acc: 0.8022 Eval Loss: 0.6417 Eval Acc: 0.7825 (LR: 0.00100000)
[2025-05-26 18:48:10,348]: [ResNet20_hardtanh] Epoch: 028 Train Loss: 0.5577 Train Acc: 0.8077 Eval Loss: 0.6664 Eval Acc: 0.7736 (LR: 0.00100000)
[2025-05-26 18:48:44,059]: [ResNet20_hardtanh] Epoch: 029 Train Loss: 0.5537 Train Acc: 0.8061 Eval Loss: 0.6309 Eval Acc: 0.7855 (LR: 0.00100000)
[2025-05-26 18:49:17,775]: [ResNet20_hardtanh] Epoch: 030 Train Loss: 0.5393 Train Acc: 0.8123 Eval Loss: 0.6287 Eval Acc: 0.7910 (LR: 0.00100000)
[2025-05-26 18:49:51,485]: [ResNet20_hardtanh] Epoch: 031 Train Loss: 0.5349 Train Acc: 0.8129 Eval Loss: 0.6918 Eval Acc: 0.7728 (LR: 0.00100000)
[2025-05-26 18:50:25,185]: [ResNet20_hardtanh] Epoch: 032 Train Loss: 0.5254 Train Acc: 0.8163 Eval Loss: 0.7356 Eval Acc: 0.7609 (LR: 0.00100000)
[2025-05-26 18:50:58,891]: [ResNet20_hardtanh] Epoch: 033 Train Loss: 0.5188 Train Acc: 0.8186 Eval Loss: 0.6209 Eval Acc: 0.7891 (LR: 0.00100000)
[2025-05-26 18:51:32,621]: [ResNet20_hardtanh] Epoch: 034 Train Loss: 0.5117 Train Acc: 0.8231 Eval Loss: 0.6965 Eval Acc: 0.7666 (LR: 0.00100000)
[2025-05-26 18:52:06,342]: [ResNet20_hardtanh] Epoch: 035 Train Loss: 0.5069 Train Acc: 0.8228 Eval Loss: 0.5796 Eval Acc: 0.8061 (LR: 0.00100000)
[2025-05-26 18:52:40,045]: [ResNet20_hardtanh] Epoch: 036 Train Loss: 0.4990 Train Acc: 0.8257 Eval Loss: 0.5478 Eval Acc: 0.8114 (LR: 0.00100000)
[2025-05-26 18:53:13,741]: [ResNet20_hardtanh] Epoch: 037 Train Loss: 0.4886 Train Acc: 0.8302 Eval Loss: 0.5409 Eval Acc: 0.8160 (LR: 0.00100000)
[2025-05-26 18:53:47,458]: [ResNet20_hardtanh] Epoch: 038 Train Loss: 0.4866 Train Acc: 0.8300 Eval Loss: 0.7829 Eval Acc: 0.7553 (LR: 0.00100000)
[2025-05-26 18:54:21,147]: [ResNet20_hardtanh] Epoch: 039 Train Loss: 0.4792 Train Acc: 0.8318 Eval Loss: 0.5488 Eval Acc: 0.8104 (LR: 0.00100000)
[2025-05-26 18:54:55,273]: [ResNet20_hardtanh] Epoch: 040 Train Loss: 0.4763 Train Acc: 0.8349 Eval Loss: 0.7516 Eval Acc: 0.7589 (LR: 0.00100000)
[2025-05-26 18:55:28,972]: [ResNet20_hardtanh] Epoch: 041 Train Loss: 0.4669 Train Acc: 0.8364 Eval Loss: 0.5815 Eval Acc: 0.8075 (LR: 0.00100000)
[2025-05-26 18:56:02,678]: [ResNet20_hardtanh] Epoch: 042 Train Loss: 0.4664 Train Acc: 0.8368 Eval Loss: 0.6803 Eval Acc: 0.7838 (LR: 0.00100000)
[2025-05-26 18:56:36,387]: [ResNet20_hardtanh] Epoch: 043 Train Loss: 0.4656 Train Acc: 0.8377 Eval Loss: 0.7051 Eval Acc: 0.7851 (LR: 0.00010000)
[2025-05-26 18:57:10,105]: [ResNet20_hardtanh] Epoch: 044 Train Loss: 0.3830 Train Acc: 0.8707 Eval Loss: 0.4277 Eval Acc: 0.8571 (LR: 0.00010000)
[2025-05-26 18:57:43,807]: [ResNet20_hardtanh] Epoch: 045 Train Loss: 0.3619 Train Acc: 0.8739 Eval Loss: 0.4234 Eval Acc: 0.8574 (LR: 0.00010000)
[2025-05-26 18:58:17,518]: [ResNet20_hardtanh] Epoch: 046 Train Loss: 0.3569 Train Acc: 0.8759 Eval Loss: 0.4224 Eval Acc: 0.8590 (LR: 0.00010000)
[2025-05-26 18:58:51,238]: [ResNet20_hardtanh] Epoch: 047 Train Loss: 0.3485 Train Acc: 0.8788 Eval Loss: 0.4266 Eval Acc: 0.8581 (LR: 0.00010000)
[2025-05-26 18:59:24,956]: [ResNet20_hardtanh] Epoch: 048 Train Loss: 0.3452 Train Acc: 0.8804 Eval Loss: 0.4281 Eval Acc: 0.8577 (LR: 0.00010000)
[2025-05-26 18:59:58,664]: [ResNet20_hardtanh] Epoch: 049 Train Loss: 0.3412 Train Acc: 0.8801 Eval Loss: 0.4206 Eval Acc: 0.8594 (LR: 0.00010000)
[2025-05-26 19:00:32,375]: [ResNet20_hardtanh] Epoch: 050 Train Loss: 0.3372 Train Acc: 0.8828 Eval Loss: 0.4304 Eval Acc: 0.8577 (LR: 0.00010000)
[2025-05-26 19:01:06,072]: [ResNet20_hardtanh] Epoch: 051 Train Loss: 0.3365 Train Acc: 0.8833 Eval Loss: 0.4255 Eval Acc: 0.8587 (LR: 0.00010000)
[2025-05-26 19:01:39,794]: [ResNet20_hardtanh] Epoch: 052 Train Loss: 0.3356 Train Acc: 0.8830 Eval Loss: 0.4246 Eval Acc: 0.8598 (LR: 0.00010000)
[2025-05-26 19:02:13,498]: [ResNet20_hardtanh] Epoch: 053 Train Loss: 0.3299 Train Acc: 0.8855 Eval Loss: 0.4169 Eval Acc: 0.8626 (LR: 0.00010000)
[2025-05-26 19:02:47,208]: [ResNet20_hardtanh] Epoch: 054 Train Loss: 0.3263 Train Acc: 0.8862 Eval Loss: 0.4214 Eval Acc: 0.8610 (LR: 0.00010000)
[2025-05-26 19:03:20,927]: [ResNet20_hardtanh] Epoch: 055 Train Loss: 0.3237 Train Acc: 0.8880 Eval Loss: 0.4230 Eval Acc: 0.8600 (LR: 0.00010000)
[2025-05-26 19:03:54,694]: [ResNet20_hardtanh] Epoch: 056 Train Loss: 0.3232 Train Acc: 0.8877 Eval Loss: 0.4293 Eval Acc: 0.8580 (LR: 0.00010000)
[2025-05-26 19:04:28,340]: [ResNet20_hardtanh] Epoch: 057 Train Loss: 0.3224 Train Acc: 0.8886 Eval Loss: 0.4143 Eval Acc: 0.8668 (LR: 0.00010000)
[2025-05-26 19:05:02,060]: [ResNet20_hardtanh] Epoch: 058 Train Loss: 0.3200 Train Acc: 0.8893 Eval Loss: 0.4131 Eval Acc: 0.8625 (LR: 0.00010000)
[2025-05-26 19:05:35,559]: [ResNet20_hardtanh] Epoch: 059 Train Loss: 0.3180 Train Acc: 0.8886 Eval Loss: 0.4181 Eval Acc: 0.8614 (LR: 0.00010000)
[2025-05-26 19:06:09,496]: [ResNet20_hardtanh] Epoch: 060 Train Loss: 0.3182 Train Acc: 0.8890 Eval Loss: 0.4230 Eval Acc: 0.8630 (LR: 0.00010000)
[2025-05-26 19:06:43,202]: [ResNet20_hardtanh] Epoch: 061 Train Loss: 0.3164 Train Acc: 0.8903 Eval Loss: 0.4159 Eval Acc: 0.8641 (LR: 0.00010000)
[2025-05-26 19:07:16,893]: [ResNet20_hardtanh] Epoch: 062 Train Loss: 0.3096 Train Acc: 0.8924 Eval Loss: 0.4161 Eval Acc: 0.8645 (LR: 0.00010000)
[2025-05-26 19:07:50,643]: [ResNet20_hardtanh] Epoch: 063 Train Loss: 0.3116 Train Acc: 0.8916 Eval Loss: 0.4262 Eval Acc: 0.8622 (LR: 0.00010000)
[2025-05-26 19:08:24,327]: [ResNet20_hardtanh] Epoch: 064 Train Loss: 0.3098 Train Acc: 0.8922 Eval Loss: 0.4183 Eval Acc: 0.8638 (LR: 0.00001000)
[2025-05-26 19:08:58,022]: [ResNet20_hardtanh] Epoch: 065 Train Loss: 0.3019 Train Acc: 0.8937 Eval Loss: 0.4120 Eval Acc: 0.8657 (LR: 0.00001000)
[2025-05-26 19:09:31,741]: [ResNet20_hardtanh] Epoch: 066 Train Loss: 0.2975 Train Acc: 0.8970 Eval Loss: 0.4115 Eval Acc: 0.8658 (LR: 0.00001000)
[2025-05-26 19:10:05,455]: [ResNet20_hardtanh] Epoch: 067 Train Loss: 0.2957 Train Acc: 0.8976 Eval Loss: 0.4097 Eval Acc: 0.8665 (LR: 0.00001000)
[2025-05-26 19:10:39,160]: [ResNet20_hardtanh] Epoch: 068 Train Loss: 0.2952 Train Acc: 0.8974 Eval Loss: 0.4136 Eval Acc: 0.8662 (LR: 0.00001000)
[2025-05-26 19:11:12,668]: [ResNet20_hardtanh] Epoch: 069 Train Loss: 0.2967 Train Acc: 0.8967 Eval Loss: 0.4099 Eval Acc: 0.8673 (LR: 0.00001000)
[2025-05-26 19:11:46,361]: [ResNet20_hardtanh] Epoch: 070 Train Loss: 0.3009 Train Acc: 0.8961 Eval Loss: 0.4107 Eval Acc: 0.8663 (LR: 0.00001000)
[2025-05-26 19:12:20,091]: [ResNet20_hardtanh] Epoch: 071 Train Loss: 0.2955 Train Acc: 0.8976 Eval Loss: 0.4095 Eval Acc: 0.8673 (LR: 0.00001000)
[2025-05-26 19:12:53,820]: [ResNet20_hardtanh] Epoch: 072 Train Loss: 0.2942 Train Acc: 0.8977 Eval Loss: 0.4099 Eval Acc: 0.8666 (LR: 0.00001000)
[2025-05-26 19:13:27,526]: [ResNet20_hardtanh] Epoch: 073 Train Loss: 0.2970 Train Acc: 0.8960 Eval Loss: 0.4093 Eval Acc: 0.8670 (LR: 0.00001000)
[2025-05-26 19:14:01,240]: [ResNet20_hardtanh] Epoch: 074 Train Loss: 0.2919 Train Acc: 0.8998 Eval Loss: 0.4084 Eval Acc: 0.8665 (LR: 0.00001000)
[2025-05-26 19:14:34,957]: [ResNet20_hardtanh] Epoch: 075 Train Loss: 0.2983 Train Acc: 0.8967 Eval Loss: 0.4083 Eval Acc: 0.8690 (LR: 0.00001000)
[2025-05-26 19:15:08,657]: [ResNet20_hardtanh] Epoch: 076 Train Loss: 0.2925 Train Acc: 0.8977 Eval Loss: 0.4089 Eval Acc: 0.8684 (LR: 0.00001000)
[2025-05-26 19:15:42,367]: [ResNet20_hardtanh] Epoch: 077 Train Loss: 0.2920 Train Acc: 0.8980 Eval Loss: 0.4070 Eval Acc: 0.8678 (LR: 0.00001000)
[2025-05-26 19:16:16,069]: [ResNet20_hardtanh] Epoch: 078 Train Loss: 0.2949 Train Acc: 0.8974 Eval Loss: 0.4110 Eval Acc: 0.8645 (LR: 0.00001000)
[2025-05-26 19:16:49,592]: [ResNet20_hardtanh] Epoch: 079 Train Loss: 0.2936 Train Acc: 0.8959 Eval Loss: 0.4088 Eval Acc: 0.8682 (LR: 0.00001000)
[2025-05-26 19:17:23,709]: [ResNet20_hardtanh] Epoch: 080 Train Loss: 0.2959 Train Acc: 0.8973 Eval Loss: 0.4123 Eval Acc: 0.8654 (LR: 0.00001000)
[2025-05-26 19:17:57,450]: [ResNet20_hardtanh] Epoch: 081 Train Loss: 0.2914 Train Acc: 0.8993 Eval Loss: 0.4089 Eval Acc: 0.8671 (LR: 0.00001000)
[2025-05-26 19:18:31,118]: [ResNet20_hardtanh] Epoch: 082 Train Loss: 0.2921 Train Acc: 0.8978 Eval Loss: 0.4105 Eval Acc: 0.8661 (LR: 0.00001000)
[2025-05-26 19:19:04,825]: [ResNet20_hardtanh] Epoch: 083 Train Loss: 0.2907 Train Acc: 0.8976 Eval Loss: 0.4114 Eval Acc: 0.8676 (LR: 0.00000100)
[2025-05-26 19:19:38,533]: [ResNet20_hardtanh] Epoch: 084 Train Loss: 0.2930 Train Acc: 0.8968 Eval Loss: 0.4105 Eval Acc: 0.8666 (LR: 0.00000100)
[2025-05-26 19:20:12,250]: [ResNet20_hardtanh] Epoch: 085 Train Loss: 0.2881 Train Acc: 0.9003 Eval Loss: 0.4104 Eval Acc: 0.8671 (LR: 0.00000100)
[2025-05-26 19:20:45,960]: [ResNet20_hardtanh] Epoch: 086 Train Loss: 0.2928 Train Acc: 0.8978 Eval Loss: 0.4087 Eval Acc: 0.8683 (LR: 0.00000100)
[2025-05-26 19:21:19,481]: [ResNet20_hardtanh] Epoch: 087 Train Loss: 0.2930 Train Acc: 0.8979 Eval Loss: 0.4119 Eval Acc: 0.8669 (LR: 0.00000100)
[2025-05-26 19:21:53,206]: [ResNet20_hardtanh] Epoch: 088 Train Loss: 0.2907 Train Acc: 0.8982 Eval Loss: 0.4100 Eval Acc: 0.8681 (LR: 0.00000100)
[2025-05-26 19:22:26,917]: [ResNet20_hardtanh] Epoch: 089 Train Loss: 0.2907 Train Acc: 0.8992 Eval Loss: 0.4106 Eval Acc: 0.8674 (LR: 0.00000010)
[2025-05-26 19:23:00,624]: [ResNet20_hardtanh] Epoch: 090 Train Loss: 0.2912 Train Acc: 0.8980 Eval Loss: 0.4089 Eval Acc: 0.8673 (LR: 0.00000010)
[2025-05-26 19:23:34,334]: [ResNet20_hardtanh] Epoch: 091 Train Loss: 0.2881 Train Acc: 0.8998 Eval Loss: 0.4094 Eval Acc: 0.8672 (LR: 0.00000010)
[2025-05-26 19:24:08,065]: [ResNet20_hardtanh] Epoch: 092 Train Loss: 0.2941 Train Acc: 0.8967 Eval Loss: 0.4104 Eval Acc: 0.8672 (LR: 0.00000010)
[2025-05-26 19:24:41,797]: [ResNet20_hardtanh] Epoch: 093 Train Loss: 0.2888 Train Acc: 0.8995 Eval Loss: 0.4098 Eval Acc: 0.8678 (LR: 0.00000010)
[2025-05-26 19:25:15,467]: [ResNet20_hardtanh] Epoch: 094 Train Loss: 0.2927 Train Acc: 0.8987 Eval Loss: 0.4120 Eval Acc: 0.8677 (LR: 0.00000010)
[2025-05-26 19:25:49,171]: [ResNet20_hardtanh] Epoch: 095 Train Loss: 0.2888 Train Acc: 0.8995 Eval Loss: 0.4109 Eval Acc: 0.8672 (LR: 0.00000010)
[2025-05-26 19:25:49,171]: Early stopping was triggered!
[2025-05-26 19:25:49,171]: [ResNet20_hardtanh] Best Eval Accuracy: 0.8690
[2025-05-26 19:25:49,203]: 
Training of full-precision model finished!
[2025-05-26 19:25:49,203]: Model Architecture:
[2025-05-26 19:25:49,204]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-26 19:25:49,204]: 
Model Weights:
[2025-05-26 19:25:49,205]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-26 19:25:49,205]: Sample Values (25 elements): [0.11429435014724731, 0.13401462137699127, 0.12380234152078629, 0.08972249925136566, 0.1557314097881317, 0.20428411662578583, -0.08649393171072006, 0.020801445469260216, -0.07717002928256989, 0.02229599840939045, 0.05476944521069527, -0.09259052574634552, 0.23377461731433868, -0.1427975296974182, 0.033015232533216476, -0.10604818910360336, 0.2985577881336212, -0.015088686719536781, 0.2628355622291565, 0.0009012791560962796, 0.10755578428506851, -0.19856677949428558, 0.09366193413734436, 0.0924876481294632, 0.03850790113210678]
[2025-05-26 19:25:49,205]: Mean: 0.00067067
[2025-05-26 19:25:49,205]: Min: -0.59812289
[2025-05-26 19:25:49,205]: Max: 0.34461883
[2025-05-26 19:25:49,205]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,206]: Sample Values (16 elements): [0.5502116084098816, 0.668010950088501, 0.8586013317108154, 0.7805400490760803, 0.6322060227394104, 0.644192636013031, 0.6316308379173279, 0.9070236086845398, 0.6369194984436035, 0.7387666702270508, 1.1098291873931885, 0.8136717081069946, 0.8170953392982483, 0.5320959091186523, 0.7520093321800232, 0.7269629240036011]
[2025-05-26 19:25:49,206]: Mean: 0.73748547
[2025-05-26 19:25:49,206]: Min: 0.53209591
[2025-05-26 19:25:49,206]: Max: 1.10982919
[2025-05-26 19:25:49,206]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,206]: Sample Values (25 elements): [0.03572211414575577, -0.06424879282712936, -0.09860416501760483, -0.08759094029664993, 0.12966471910476685, 0.09789237380027771, -0.1310362070798874, 0.021751470863819122, -0.08302688598632812, -0.056198857724666595, -0.12711744010448456, -0.012240425683557987, -0.04471903666853905, 0.18227888643741608, 0.015606286004185677, -0.13038721680641174, 0.021318344399333, -0.0576324462890625, 0.07405475527048111, -0.16154134273529053, 0.006188387516885996, -0.04357253387570381, -0.020922165364027023, -0.17759950459003448, -0.24880069494247437]
[2025-05-26 19:25:49,207]: Mean: -0.00189196
[2025-05-26 19:25:49,207]: Min: -0.49130526
[2025-05-26 19:25:49,207]: Max: 0.59045911
[2025-05-26 19:25:49,207]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,207]: Sample Values (16 elements): [1.159720778465271, 0.6315287947654724, 0.8420218229293823, 0.936284065246582, 0.8593540787696838, 0.6250782012939453, 0.7656907439231873, 0.8702704906463623, 0.9505681991577148, 0.9118705987930298, 0.7736178040504456, 0.8487583994865417, 1.0461220741271973, 0.8934905529022217, 1.0480709075927734, 1.1816296577453613]
[2025-05-26 19:25:49,207]: Mean: 0.89650482
[2025-05-26 19:25:49,207]: Min: 0.62507820
[2025-05-26 19:25:49,208]: Max: 1.18162966
[2025-05-26 19:25:49,208]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,208]: Sample Values (25 elements): [-0.004710650071501732, -0.23553799092769623, 0.06768837571144104, 0.05795392021536827, 0.0496041439473629, 0.0912245362997055, 0.12559400498867035, 0.18709655106067657, 0.0045632352121174335, -0.04584398865699768, 0.08081304281949997, -0.09788379073143005, -0.00981881469488144, -0.03689245507121086, 0.07819468528032303, -0.07189401239156723, 0.11307567358016968, -0.18822550773620605, -0.2029915302991867, 0.15510042011737823, -0.028925174847245216, -0.08618440479040146, -0.008591711521148682, 0.001659130910411477, -0.002436526818200946]
[2025-05-26 19:25:49,208]: Mean: 0.00207706
[2025-05-26 19:25:49,208]: Min: -0.43365645
[2025-05-26 19:25:49,208]: Max: 0.45334584
[2025-05-26 19:25:49,208]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,209]: Sample Values (16 elements): [0.5574577450752258, 0.6937311291694641, 0.6015516519546509, 0.7072291374206543, 0.8055368661880493, 0.8745425343513489, 0.8813281655311584, 0.7555713057518005, 0.9174238443374634, 0.9757425785064697, 0.9265540838241577, 0.6187164187431335, 0.9175874590873718, 0.6465546488761902, 0.8841875791549683, 0.6287380456924438]
[2025-05-26 19:25:49,209]: Mean: 0.77452832
[2025-05-26 19:25:49,209]: Min: 0.55745775
[2025-05-26 19:25:49,209]: Max: 0.97574258
[2025-05-26 19:25:49,209]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,209]: Sample Values (25 elements): [0.06035811826586723, -0.13889148831367493, -0.10005894303321838, -0.3388187885284424, 0.04442336782813072, -0.0945204347372055, 0.0475122295320034, -0.08537094295024872, -0.1472414880990982, 0.12994007766246796, 0.0003471667878329754, 0.02699613757431507, 0.08981398493051529, -0.15319810807704926, 0.1202571764588356, 0.08661158382892609, 0.2743806540966034, 0.04930520057678223, -0.017849594354629517, -0.07705120742321014, -0.13328817486763, -0.1570587307214737, 0.0720413401722908, -0.0035949668381363153, -0.20152482390403748]
[2025-05-26 19:25:49,210]: Mean: -0.00140711
[2025-05-26 19:25:49,210]: Min: -0.43368351
[2025-05-26 19:25:49,210]: Max: 0.47730064
[2025-05-26 19:25:49,210]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,210]: Sample Values (16 elements): [1.0230270624160767, 1.086357831954956, 0.767203152179718, 1.148169994354248, 1.0023902654647827, 0.9663600325584412, 1.091180443763733, 0.6984784603118896, 0.6949113011360168, 1.1556416749954224, 0.8268964290618896, 1.3163074254989624, 0.8870227932929993, 0.8841089010238647, 0.8793299198150635, 1.259594440460205]
[2025-05-26 19:25:49,210]: Mean: 0.98043627
[2025-05-26 19:25:49,211]: Min: 0.69491130
[2025-05-26 19:25:49,211]: Max: 1.31630743
[2025-05-26 19:25:49,211]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,211]: Sample Values (25 elements): [-0.027401810511946678, 0.10567864030599594, -0.019861645996570587, -0.020195430144667625, 0.07351521402597427, -0.05034923553466797, 0.09794334322214127, 0.17479659616947174, -0.11275702714920044, 0.007578512188047171, 0.06773246824741364, 0.05339161306619644, 0.12487907707691193, -0.1694294959306717, 0.1048644408583641, 0.05285624787211418, -0.00884645152837038, -0.10722599178552628, -0.042574480175971985, -0.2215823084115982, -0.07272809743881226, 0.29376184940338135, -0.12062326818704605, -0.035512275993824005, -0.03039667010307312]
[2025-05-26 19:25:49,211]: Mean: -0.00469146
[2025-05-26 19:25:49,211]: Min: -0.44131535
[2025-05-26 19:25:49,211]: Max: 0.52563852
[2025-05-26 19:25:49,212]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,212]: Sample Values (16 elements): [0.5922549962997437, 0.9779398441314697, 0.48222821950912476, 0.6803516149520874, 0.6031842827796936, 0.5652828812599182, 0.6451655626296997, 0.5718948841094971, 0.9111365675926208, 0.6330024600028992, 0.8687618374824524, 0.5225087404251099, 0.7340036630630493, 0.8212340474128723, 0.7303474545478821, 0.6211785078048706]
[2025-05-26 19:25:49,212]: Mean: 0.68502975
[2025-05-26 19:25:49,212]: Min: 0.48222822
[2025-05-26 19:25:49,212]: Max: 0.97793984
[2025-05-26 19:25:49,212]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,213]: Sample Values (25 elements): [-0.005656318739056587, 0.11177985370159149, 0.06343848258256912, 0.15181246399879456, -0.011197860352694988, 0.08701693266630173, 0.21527552604675293, -0.036205023527145386, -0.20970846712589264, 0.02360670082271099, 0.010465352796018124, -0.4709031283855438, 0.06508287042379379, 0.018697531893849373, 0.12256968766450882, 0.02526140958070755, -0.009280984289944172, -0.00798158347606659, 0.04189521074295044, 0.10303228348493576, -0.09854106605052948, -0.0038842863868921995, -0.13819952309131622, -0.10511018335819244, -0.0952562540769577]
[2025-05-26 19:25:49,213]: Mean: -0.00623599
[2025-05-26 19:25:49,213]: Min: -0.53331852
[2025-05-26 19:25:49,213]: Max: 0.39245892
[2025-05-26 19:25:49,213]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,213]: Sample Values (16 elements): [0.8557426333427429, 0.9770027995109558, 1.0159744024276733, 1.1442762613296509, 0.8613601922988892, 1.0642571449279785, 1.1199239492416382, 1.058964490890503, 0.8534445762634277, 0.9861290454864502, 0.9574756622314453, 0.8688041567802429, 0.953551173210144, 0.9228023886680603, 0.9130203723907471, 0.8815149664878845]
[2025-05-26 19:25:49,213]: Mean: 0.96464032
[2025-05-26 19:25:49,214]: Min: 0.85344458
[2025-05-26 19:25:49,214]: Max: 1.14427626
[2025-05-26 19:25:49,214]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,214]: Sample Values (25 elements): [-0.03852413222193718, -0.1995713710784912, 0.014351973310112953, 0.009096859954297543, 0.05980584770441055, 0.0233291182667017, -0.04531698673963547, 0.16504469513893127, -0.11169897764921188, -0.055638354271650314, -0.01919659599661827, -0.005638116039335728, 0.13465099036693573, -0.18788668513298035, 0.044566892087459564, -0.19448518753051758, -0.05516471713781357, 0.11627577990293503, 0.02598283439874649, 0.04739837720990181, 0.010608718730509281, 0.10517926514148712, -0.0981869027018547, -0.17225955426692963, -0.052974533289670944]
[2025-05-26 19:25:49,214]: Mean: -0.00134037
[2025-05-26 19:25:49,214]: Min: -0.39483452
[2025-05-26 19:25:49,215]: Max: 0.51383978
[2025-05-26 19:25:49,215]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,215]: Sample Values (16 elements): [0.8965025544166565, 0.7409180998802185, 0.6697055101394653, 1.0061177015304565, 0.666359543800354, 0.651939332485199, 0.7458117008209229, 0.704752504825592, 0.7410255074501038, 0.5052545070648193, 0.6561073660850525, 0.714324414730072, 0.6325165629386902, 0.7474067211151123, 0.6094486713409424, 0.6692317128181458]
[2025-05-26 19:25:49,215]: Mean: 0.70983887
[2025-05-26 19:25:49,215]: Min: 0.50525451
[2025-05-26 19:25:49,215]: Max: 1.00611770
[2025-05-26 19:25:49,215]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-26 19:25:49,216]: Sample Values (25 elements): [0.026370737701654434, -0.04071004316210747, 0.04643953964114189, -0.019180692732334137, 0.06393350660800934, -0.16093847155570984, 0.03873011842370033, 0.08083350956439972, 0.14464357495307922, -0.013904710300266743, -0.051033832132816315, -0.09273488819599152, 0.06626807898283005, -0.13894926011562347, -0.34902432560920715, -0.009333133697509766, -0.12955252826213837, -0.09876944869756699, -0.028589623048901558, -0.03630964085459709, -0.003869861364364624, 0.07645491510629654, -0.006146468687802553, 0.1214042454957962, -0.04686209559440613]
[2025-05-26 19:25:49,216]: Mean: 0.00432576
[2025-05-26 19:25:49,216]: Min: -0.39039880
[2025-05-26 19:25:49,216]: Max: 0.39065492
[2025-05-26 19:25:49,216]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,216]: Sample Values (25 elements): [0.7071413993835449, 0.9045945405960083, 0.7970846891403198, 0.7875382900238037, 0.8797261118888855, 0.7757295966148376, 0.9917125105857849, 0.6416102051734924, 0.7632288932800293, 0.8214387893676758, 0.815360426902771, 0.8191978931427002, 0.8888670802116394, 0.7405829429626465, 0.8045385479927063, 0.642770528793335, 0.7261314392089844, 0.8163524866104126, 0.7560022473335266, 0.666067361831665, 0.7352674603462219, 0.7405836582183838, 0.8734428882598877, 0.5965235829353333, 0.8037998676300049]
[2025-05-26 19:25:49,217]: Mean: 0.77595186
[2025-05-26 19:25:49,217]: Min: 0.59652358
[2025-05-26 19:25:49,217]: Max: 0.99171251
[2025-05-26 19:25:49,217]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,217]: Sample Values (25 elements): [0.09904912859201431, -0.058279696851968765, 0.07338474690914154, 0.033208541572093964, -0.05935130640864372, 0.12794256210327148, 0.10296925157308578, -0.12065883725881577, -0.016924679279327393, -0.11623601615428925, 0.15046852827072144, 0.14383666217327118, -0.03829078748822212, 0.0945587009191513, 0.048246581107378006, 0.055026598274707794, -0.04818687587976456, 0.04557844623923302, -0.07473605871200562, 0.009342152625322342, 0.02333683893084526, -0.001647584605962038, 0.004861939698457718, -0.004338250029832125, -0.021295655518770218]
[2025-05-26 19:25:49,217]: Mean: 0.00201934
[2025-05-26 19:25:49,218]: Min: -0.38586891
[2025-05-26 19:25:49,218]: Max: 0.35906258
[2025-05-26 19:25:49,218]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,218]: Sample Values (25 elements): [0.7301149964332581, 0.7851240038871765, 0.6987364888191223, 0.7856627106666565, 0.8286901116371155, 0.8562981486320496, 0.70313960313797, 0.6978512406349182, 0.7770841717720032, 0.7994734048843384, 0.7844347953796387, 0.7512884736061096, 0.7212440371513367, 0.8367453813552856, 0.9971225261688232, 0.7099695801734924, 0.6737432479858398, 0.9069218635559082, 0.8647176027297974, 0.7548388838768005, 0.6060072779655457, 0.6606361865997314, 0.8608936667442322, 0.7777382135391235, 0.6759841442108154]
[2025-05-26 19:25:49,218]: Mean: 0.76221943
[2025-05-26 19:25:49,218]: Min: 0.60600728
[2025-05-26 19:25:49,218]: Max: 0.99712253
[2025-05-26 19:25:49,218]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-26 19:25:49,219]: Sample Values (25 elements): [-0.18909025192260742, -0.3092363178730011, 0.25615403056144714, -0.18504254519939423, 0.2746959626674652, 0.22914251685142517, 0.054044030606746674, 0.032857004553079605, 0.2541870176792145, 0.0887182280421257, 0.23837438225746155, 0.13668103516101837, 0.1689571589231491, 0.23350588977336884, -0.08582182973623276, -0.3825324475765228, -0.16574403643608093, -0.02991069294512272, -0.17111708223819733, -0.05394980311393738, 0.050734613090753555, 0.11724565923213959, 0.2143636792898178, -0.15304675698280334, 0.1489449143409729]
[2025-05-26 19:25:49,219]: Mean: 0.00849727
[2025-05-26 19:25:49,219]: Min: -0.45839590
[2025-05-26 19:25:49,219]: Max: 0.44234228
[2025-05-26 19:25:49,219]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,219]: Sample Values (25 elements): [0.6761747598648071, 0.27178114652633667, 0.6283966898918152, 0.3518148958683014, 0.4983835220336914, 0.49365073442459106, 0.5602676272392273, 0.5441693067550659, 0.4707086980342865, 0.5621576309204102, 0.49860888719558716, 0.5082482099533081, 0.3316601514816284, 0.2919485867023468, 0.4618634879589081, 0.56379234790802, 0.5742371678352356, 0.3930882215499878, 0.6465232968330383, 0.6130110025405884, 0.49887675046920776, 0.46561408042907715, 0.5083709359169006, 0.4443615972995758, 0.380880206823349]
[2025-05-26 19:25:49,220]: Mean: 0.51025653
[2025-05-26 19:25:49,220]: Min: 0.27178115
[2025-05-26 19:25:49,220]: Max: 0.75144649
[2025-05-26 19:25:49,220]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,220]: Sample Values (25 elements): [-0.0601474829018116, -0.0069289435632526875, -0.0022445828653872013, -0.08328285068273544, -0.03316056355834007, -0.031791072338819504, 0.058137089014053345, 0.014275330118834972, 0.09790065884590149, 0.002693051937967539, 0.08842738717794418, 0.08527719974517822, 0.07956398278474808, -0.03913864865899086, -0.08043190091848373, -0.11793248355388641, 0.04569394513964653, -0.13888581097126007, -0.07371541112661362, -0.06835071742534637, 0.014166177250444889, -0.1003216803073883, 0.06870705634355545, -0.12995180487632751, -0.019445450976490974]
[2025-05-26 19:25:49,220]: Mean: 0.00233807
[2025-05-26 19:25:49,221]: Min: -0.38896734
[2025-05-26 19:25:49,221]: Max: 0.33344868
[2025-05-26 19:25:49,221]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,221]: Sample Values (25 elements): [0.7776239514350891, 0.9381120204925537, 1.014744758605957, 0.6608483195304871, 0.859599232673645, 1.0504993200302124, 0.8076260089874268, 0.7562715411186218, 0.9032034277915955, 0.8192076086997986, 0.8095394968986511, 1.078836441040039, 0.7184111475944519, 0.8352881669998169, 1.1136575937271118, 0.7611797451972961, 0.832094132900238, 0.81430584192276, 0.921656608581543, 0.8019354939460754, 0.8402772545814514, 0.7657175064086914, 0.9752669930458069, 0.8839296102523804, 1.0380939245224]
[2025-05-26 19:25:49,221]: Mean: 0.87508619
[2025-05-26 19:25:49,221]: Min: 0.66084832
[2025-05-26 19:25:49,221]: Max: 1.11365759
[2025-05-26 19:25:49,222]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,222]: Sample Values (25 elements): [0.0680566132068634, -0.1766384094953537, 0.22105732560157776, -0.014877039939165115, 0.08417858928442001, 0.07616490125656128, 0.10172432661056519, 0.03229811042547226, -0.07450374215841293, -0.07545376569032669, -0.08882465213537216, 0.018908722326159477, 0.03197181969881058, -0.1632203906774521, 0.04205596074461937, -0.1475296914577484, -0.027814587578177452, 0.0027336920611560345, 0.038710713386535645, -0.1372668445110321, 0.09751251339912415, 0.0042809066362679005, 0.07960900664329529, -0.15695050358772278, 0.04016825556755066]
[2025-05-26 19:25:49,222]: Mean: 0.00016429
[2025-05-26 19:25:49,222]: Min: -0.33932337
[2025-05-26 19:25:49,222]: Max: 0.37259525
[2025-05-26 19:25:49,222]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,223]: Sample Values (25 elements): [0.6158192753791809, 0.574986457824707, 0.6105727553367615, 0.7156373858451843, 0.6745414733886719, 0.6473590731620789, 0.8160945773124695, 0.7644526362419128, 0.7136293053627014, 0.5350351333618164, 0.7642601728439331, 0.720725417137146, 0.6208803057670593, 0.6959929466247559, 0.9008598923683167, 0.6733432412147522, 0.6859773993492126, 0.7251498699188232, 0.6683773398399353, 0.7942367196083069, 0.6200363636016846, 0.7874156832695007, 0.698259174823761, 0.3635953962802887, 0.5420039296150208]
[2025-05-26 19:25:49,223]: Mean: 0.68213159
[2025-05-26 19:25:49,223]: Min: 0.36359540
[2025-05-26 19:25:49,223]: Max: 0.90085989
[2025-05-26 19:25:49,223]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,223]: Sample Values (25 elements): [0.1218869686126709, -0.1034475639462471, -0.05186251178383827, -0.17074470221996307, -0.015953153371810913, -0.057090334594249725, 0.07706116884946823, -0.04420347884297371, 0.06782472878694534, 0.018679482862353325, -0.1112808957695961, -0.22781291604042053, 0.2079666703939438, -0.07741934061050415, 0.014249846339225769, -0.05212904140353203, -0.012994000688195229, 0.039518292993307114, -0.0953638106584549, -0.09648954123258591, -0.07626567780971527, -0.25100943446159363, 0.04519112408161163, -0.04135190695524216, 0.09554464370012283]
[2025-05-26 19:25:49,224]: Mean: -0.00005613
[2025-05-26 19:25:49,224]: Min: -0.38320661
[2025-05-26 19:25:49,224]: Max: 0.42978662
[2025-05-26 19:25:49,224]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,224]: Sample Values (25 elements): [1.0279077291488647, 0.8736992478370667, 1.089428424835205, 0.8671848177909851, 0.8531743884086609, 0.9641868472099304, 1.1810895204544067, 0.9737184643745422, 0.9717438220977783, 1.067350149154663, 0.8427587151527405, 1.0137786865234375, 1.0606212615966797, 0.9673487544059753, 0.8253228068351746, 0.8748511075973511, 0.9576467275619507, 0.8490889072418213, 0.9072620868682861, 1.1921378374099731, 0.6950575113296509, 0.9068790674209595, 0.9881873726844788, 0.9554229974746704, 1.1203831434249878]
[2025-05-26 19:25:49,224]: Mean: 0.97886026
[2025-05-26 19:25:49,224]: Min: 0.69505751
[2025-05-26 19:25:49,225]: Max: 1.24778652
[2025-05-26 19:25:49,225]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,225]: Sample Values (25 elements): [0.028730880469083786, 0.054887160658836365, 0.059820957481861115, 0.06257103383541107, -0.1887228935956955, -0.023003198206424713, 0.11422443389892578, 0.09448191523551941, -0.023881804198026657, -0.0007510299328714609, -0.029263507574796677, -0.059206340461969376, -0.06258893013000488, 0.009591532871127129, 0.0094985980540514, 0.10835520923137665, -0.03481405973434448, 0.015740422531962395, -0.23733465373516083, 0.05616893246769905, -0.016278229653835297, 0.019520355388522148, 0.18700407445430756, 0.06040974333882332, -0.06725309789180756]
[2025-05-26 19:25:49,225]: Mean: -0.00276972
[2025-05-26 19:25:49,225]: Min: -0.35541722
[2025-05-26 19:25:49,225]: Max: 0.39067468
[2025-05-26 19:25:49,225]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,226]: Sample Values (25 elements): [0.7484176754951477, 0.6856278777122498, 0.6592332124710083, 0.8423532843589783, 0.5410075187683105, 0.7619869112968445, 0.7143532037734985, 0.6050164103507996, 0.687147319316864, 0.7717726826667786, 0.7312003374099731, 0.6960811614990234, 0.9285094141960144, 0.6091731190681458, 0.7554932236671448, 0.7532341480255127, 0.7161076068878174, 0.7410926222801208, 0.6251361966133118, 0.6648226976394653, 0.7410304546356201, 0.7821645140647888, 0.7959353923797607, 0.6855247616767883, 0.6721224188804626]
[2025-05-26 19:25:49,226]: Mean: 0.72466850
[2025-05-26 19:25:49,226]: Min: 0.54100752
[2025-05-26 19:25:49,226]: Max: 0.92850941
[2025-05-26 19:25:49,226]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-26 19:25:49,227]: Sample Values (25 elements): [0.06859387457370758, 0.26428869366645813, -0.03526131436228752, 0.029984623193740845, 0.05842458829283714, 0.05893673747777939, 0.038436632603406906, 0.07853853702545166, 0.054007142782211304, 0.018935317173600197, 0.011455805972218513, 0.012892463244497776, -0.015270154923200607, -0.04664624109864235, 0.011240709573030472, 0.07610319554805756, -0.06060526520013809, 0.024551697075366974, 0.2695280611515045, -0.17007403075695038, 0.017204629257321358, -0.08247186243534088, 0.0008313370635733008, 0.03941980004310608, 0.08578094840049744]
[2025-05-26 19:25:49,227]: Mean: -0.00175143
[2025-05-26 19:25:49,227]: Min: -0.31890848
[2025-05-26 19:25:49,227]: Max: 0.35801673
[2025-05-26 19:25:49,227]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,227]: Sample Values (25 elements): [0.91752028465271, 0.660677969455719, 0.6792107224464417, 0.649118185043335, 0.8805438280105591, 0.6918087005615234, 0.5499494075775146, 0.558104932308197, 0.9317766427993774, 1.0161499977111816, 0.7313655614852905, 0.8048710823059082, 0.7893196940422058, 0.49660158157348633, 0.9020243287086487, 0.7419540286064148, 0.7471784353256226, 0.5825760960578918, 0.7045317888259888, 0.59757000207901, 0.8969983458518982, 0.6642112731933594, 0.826509416103363, 0.12727788090705872, 0.6899623274803162]
[2025-05-26 19:25:49,228]: Mean: 0.78184640
[2025-05-26 19:25:49,228]: Min: 0.12727788
[2025-05-26 19:25:49,228]: Max: 1.01615000
[2025-05-26 19:25:49,228]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,228]: Sample Values (25 elements): [0.06260176748037338, -0.03313252329826355, 0.03797793388366699, 0.010048609226942062, 0.12885475158691406, 0.10150988399982452, -0.08843549340963364, 0.052842509001493454, 0.10479474812746048, 0.06481720507144928, 0.10054350644350052, 0.13527479767799377, -0.01976862922310829, 0.028135647997260094, -0.018662715330719948, -0.0492699109017849, -0.09187444299459457, 0.11318039894104004, 0.035133033990859985, -0.19772864878177643, -0.02239350415766239, 0.05873878300189972, -0.1258222609758377, 0.050508130341768265, -0.10903935134410858]
[2025-05-26 19:25:49,229]: Mean: 0.00083670
[2025-05-26 19:25:49,229]: Min: -0.33010036
[2025-05-26 19:25:49,229]: Max: 0.40634722
[2025-05-26 19:25:49,229]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,229]: Sample Values (25 elements): [0.8572869896888733, 0.9663800597190857, 0.7578933238983154, 0.8267126083374023, 0.8015729188919067, 0.6833119988441467, 0.801382303237915, 0.9152363538742065, 0.8314493298530579, 0.565557599067688, 0.7475906610488892, 0.7479345202445984, 0.6946561932563782, 0.8344639539718628, 0.6803284883499146, 0.8169525265693665, 0.7832987904548645, 0.7847591638565063, 0.891495943069458, 0.6318332552909851, 0.7886202335357666, 0.7874906659126282, 0.7319915890693665, 0.7958405613899231, 0.8230616450309753]
[2025-05-26 19:25:49,229]: Mean: 0.78900903
[2025-05-26 19:25:49,229]: Min: 0.55508131
[2025-05-26 19:25:49,230]: Max: 0.96638006
[2025-05-26 19:25:49,230]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-26 19:25:49,230]: Sample Values (25 elements): [0.052361708134412766, 0.18395762145519257, 0.20864969491958618, -0.01977970078587532, 0.05551696941256523, 0.02352883480489254, -0.04069014638662338, -0.06741443276405334, 0.18389910459518433, 0.09010924398899078, -0.2021322101354599, 0.08336222171783447, 0.08664087951183319, 0.40351271629333496, -0.09757648408412933, 0.0924091786146164, -0.12375335395336151, 0.09978058934211731, -0.2656565308570862, -0.06948740035295486, 0.0701228454709053, -0.12264305353164673, -0.04571028798818588, 0.15725894272327423, 0.10699167102575302]
[2025-05-26 19:25:49,230]: Mean: 0.00200642
[2025-05-26 19:25:49,230]: Min: -0.36108875
[2025-05-26 19:25:49,230]: Max: 0.40351272
[2025-05-26 19:25:49,230]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,231]: Sample Values (25 elements): [0.5862607359886169, 0.4137505888938904, 0.6273230314254761, 0.5661307573318481, 0.5390627384185791, 0.4840429723262787, 0.566608726978302, 0.4213906526565552, 0.7783549427986145, 0.4642132818698883, 0.5385741591453552, 0.6202110052108765, 0.6070090532302856, 0.5699092745780945, 0.4703812301158905, 0.5615012645721436, 0.44163426756858826, 0.7200524806976318, 0.5561913847923279, 0.6368650794029236, 0.41944047808647156, 0.515289306640625, 0.6306380033493042, 0.6617869138717651, 0.6395826935768127]
[2025-05-26 19:25:49,231]: Mean: 0.57039630
[2025-05-26 19:25:49,231]: Min: 0.29883018
[2025-05-26 19:25:49,231]: Max: 0.83254170
[2025-05-26 19:25:49,231]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,232]: Sample Values (25 elements): [-0.09091401100158691, -0.1423485279083252, 0.047228146344423294, -0.027119606733322144, 0.015889877453446388, -0.017009897157549858, 0.0021843407303094864, 0.1581466794013977, -0.09219294041395187, 0.09169994294643402, -0.13774727284908295, 0.03308899328112602, 0.058292511850595474, 0.026041634380817413, 0.0550784170627594, 0.01751887984573841, 0.1303550899028778, 0.045887209475040436, 0.1683134287595749, -0.24681925773620605, 0.04130978882312775, 0.0474642850458622, -0.031255193054676056, 0.1490822434425354, 0.004622669890522957]
[2025-05-26 19:25:49,232]: Mean: -0.00046814
[2025-05-26 19:25:49,232]: Min: -0.41842383
[2025-05-26 19:25:49,232]: Max: 0.38220432
[2025-05-26 19:25:49,232]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,232]: Sample Values (25 elements): [0.9379630088806152, 1.018417239189148, 0.7356694936752319, 1.1831963062286377, 0.8195781111717224, 0.931477427482605, 0.9846059679985046, 1.0843842029571533, 0.7146393656730652, 1.132835865020752, 1.0260753631591797, 1.0573025941848755, 0.5827283263206482, 0.7987610101699829, 1.1808334589004517, 1.0247660875320435, 0.9871360063552856, 0.9487981796264648, 0.9817982912063599, 0.6742023825645447, 1.2453160285949707, 0.8485867381095886, 1.029895305633545, 0.963786780834198, 1.210842490196228]
[2025-05-26 19:25:49,233]: Mean: 0.93083328
[2025-05-26 19:25:49,233]: Min: 0.27766928
[2025-05-26 19:25:49,233]: Max: 1.24531603
[2025-05-26 19:25:49,233]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,233]: Sample Values (25 elements): [-0.0015571418916806579, 0.03520526736974716, 0.021356990560889244, -0.06784307956695557, -0.02369275502860546, -0.01451900228857994, 0.006772765889763832, 0.09145644307136536, -0.009437517262995243, -0.005004816222935915, 0.16588102281093597, -0.04112320393323898, 0.06685656309127808, -0.014923393726348877, 0.0007537382189184427, -0.02198530174791813, 0.023143915459513664, 0.056166794151067734, -0.05324063077569008, 0.10294993966817856, -0.0495576336979866, 0.06515204906463623, -0.04375927150249481, -0.006720127072185278, 0.06306461244821548]
[2025-05-26 19:25:49,234]: Mean: 0.00016800
[2025-05-26 19:25:49,234]: Min: -0.33734655
[2025-05-26 19:25:49,234]: Max: 0.29873466
[2025-05-26 19:25:49,234]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,234]: Sample Values (25 elements): [0.5636031627655029, 0.9831992983818054, 0.8258786201477051, 0.8482595682144165, 0.9195650815963745, 0.8284926414489746, 0.8693817853927612, 0.8509705066680908, 0.7851013541221619, 0.8884161710739136, 0.9572181105613708, 0.8648001551628113, 0.565464973449707, 1.073982834815979, 0.8601230978965759, 0.9222607612609863, 0.8927865028381348, 0.868232786655426, 0.7988631725311279, 1.0247725248336792, 0.7776561975479126, 0.9128149151802063, 0.8539445996284485, 0.7773576974868774, 0.8099766969680786]
[2025-05-26 19:25:49,234]: Mean: 0.84853935
[2025-05-26 19:25:49,235]: Min: 0.53343213
[2025-05-26 19:25:49,235]: Max: 1.07487082
[2025-05-26 19:25:49,235]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,235]: Sample Values (25 elements): [0.007055334281176329, 2.3555245109313245e-15, 0.08110348135232925, -0.010314165614545345, 0.004885584581643343, -0.02363702282309532, -0.0251008253544569, 0.00557229109108448, 1.566093366101029e-10, 0.024137580767273903, -0.07568727433681488, -0.0335458368062973, -2.0797275460015126e-20, 0.03726132959127426, -0.0757635086774826, 0.002043792512267828, 0.11724317818880081, -0.03688366711139679, -0.0030550144147127867, -0.05723237246274948, 0.00549090001732111, 0.02464599721133709, 0.08142583817243576, 0.03096548654139042, -0.012338814325630665]
[2025-05-26 19:25:49,235]: Mean: -0.00114370
[2025-05-26 19:25:49,236]: Min: -0.30356410
[2025-05-26 19:25:49,236]: Max: 0.33977669
[2025-05-26 19:25:49,236]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,236]: Sample Values (25 elements): [1.029191017150879, 0.9601432085037231, 1.070718765258789, 0.25210824608802795, -4.1194562072632834e-06, 1.1457246706925162e-08, 0.6841678023338318, 0.8024532794952393, 1.0067239999771118, 1.287534475326538, 0.9756051301956177, 1.0376389026641846, 1.162787914276123, 0.8285731077194214, 0.9289435148239136, 0.5977728962898254, 0.8403424024581909, 0.8189029097557068, 1.0172781944274902, 9.766091579876957e-07, 0.7575250267982483, 0.7297480702400208, 0.9023464918136597, 1.1113135814666748, 1.041532039642334]
[2025-05-26 19:25:49,236]: Mean: 0.84565842
[2025-05-26 19:25:49,236]: Min: -0.00000412
[2025-05-26 19:25:49,236]: Max: 1.28753448
[2025-05-26 19:25:49,237]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,237]: Sample Values (25 elements): [0.0753449872136116, 0.0322544239461422, -0.042386461049318314, -0.06231702119112015, 0.036645788699388504, -0.003335181623697281, 0.07511647790670395, 0.016618240624666214, 0.008469179272651672, 0.04022154211997986, -0.10193488746881485, 0.06301049143075943, 0.062421079725027084, -0.050742942839860916, -0.08990135788917542, 0.03504370525479317, -0.025361888110637665, -0.0038629204500466585, 0.1470761001110077, -0.016421303153038025, -0.02896963618695736, -1.5409114956232983e-14, 0.0009333493653684855, -0.034533120691776276, -0.04211699962615967]
[2025-05-26 19:25:49,237]: Mean: -0.00067969
[2025-05-26 19:25:49,237]: Min: -0.23955834
[2025-05-26 19:25:49,237]: Max: 0.27092022
[2025-05-26 19:25:49,238]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,238]: Sample Values (25 elements): [1.1031378507614136, 1.0399597883224487, 1.3095059394836426, 0.9989219903945923, 0.8600848317146301, 1.2050906419754028, 0.9504304528236389, 1.0214667320251465, 0.9047538638114929, 1.014860987663269, 1.0168102979660034, 1.0596795082092285, 1.120157241821289, 1.1246250867843628, 1.0246623754501343, 1.0041230916976929, 0.9411888122558594, 1.1662544012069702, 1.199916958808899, 1.0152226686477661, 1.1036466360092163, 1.0628058910369873, 1.1137555837631226, 0.9881245493888855, 0.955162763595581]
[2025-05-26 19:25:49,238]: Mean: 1.04044986
[2025-05-26 19:25:49,238]: Min: 0.86008483
[2025-05-26 19:25:49,238]: Max: 1.30950594
[2025-05-26 19:25:49,238]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-26 19:25:49,239]: Sample Values (25 elements): [-0.5250959992408752, -0.5971916317939758, 0.12744519114494324, 0.24236413836479187, -0.02008187398314476, 0.2830040454864502, -0.33084893226623535, 0.48247459530830383, 0.43079084157943726, 0.34557825326919556, 0.7152265310287476, -0.17511063814163208, 0.4879647195339203, -0.31080684065818787, -0.5322420001029968, 0.09984149783849716, -0.7686123847961426, 0.39935821294784546, 0.20633244514465332, 0.5011131167411804, -0.1563509851694107, 0.27460452914237976, 0.6331163644790649, -0.6754001379013062, 0.7102773189544678]
[2025-05-26 19:25:49,239]: Mean: -0.00754639
[2025-05-26 19:25:49,239]: Min: -1.01073802
[2025-05-26 19:25:49,239]: Max: 1.14522386
[2025-05-26 19:25:49,239]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT

[2025-05-27 22:21:30,892]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT
[2025-05-27 22:21:30,892]: 


QAT of ResNet20 with hardtanh down to 4 bits...
[2025-05-27 22:21:31,162]: [ResNet20_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-27 22:21:31,428]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 22:22:30,280]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.4575 Train Acc: 0.8396 Eval Loss: 0.6946 Eval Acc: 0.7751 (LR: 0.00100000)
[2025-05-27 22:23:31,349]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.4566 Train Acc: 0.8403 Eval Loss: 0.6583 Eval Acc: 0.7960 (LR: 0.00100000)
[2025-05-27 22:25:01,351]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.4581 Train Acc: 0.8414 Eval Loss: 0.6501 Eval Acc: 0.7888 (LR: 0.00100000)
[2025-05-27 22:25:59,370]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.4513 Train Acc: 0.8420 Eval Loss: 0.5737 Eval Acc: 0.8144 (LR: 0.00100000)
[2025-05-27 22:26:57,140]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.4573 Train Acc: 0.8422 Eval Loss: 0.7688 Eval Acc: 0.7587 (LR: 0.00100000)
[2025-05-27 22:27:54,645]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.4532 Train Acc: 0.8423 Eval Loss: 0.5277 Eval Acc: 0.8284 (LR: 0.00100000)
[2025-05-27 22:28:51,990]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.4554 Train Acc: 0.8411 Eval Loss: 0.6280 Eval Acc: 0.7976 (LR: 0.00100000)
[2025-05-27 22:29:51,751]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.4496 Train Acc: 0.8436 Eval Loss: 0.6491 Eval Acc: 0.7996 (LR: 0.00100000)
[2025-05-27 22:30:48,999]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.4463 Train Acc: 0.8440 Eval Loss: 0.7480 Eval Acc: 0.7619 (LR: 0.00100000)
[2025-05-27 22:31:46,812]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.4432 Train Acc: 0.8462 Eval Loss: 0.5513 Eval Acc: 0.8161 (LR: 0.00100000)
[2025-05-27 22:32:42,168]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.4419 Train Acc: 0.8464 Eval Loss: 0.7998 Eval Acc: 0.7523 (LR: 0.00100000)
[2025-05-27 22:33:39,177]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.4392 Train Acc: 0.8472 Eval Loss: 0.6290 Eval Acc: 0.7959 (LR: 0.00010000)
[2025-05-27 22:34:41,264]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.3525 Train Acc: 0.8787 Eval Loss: 0.4084 Eval Acc: 0.8639 (LR: 0.00010000)
[2025-05-27 22:35:40,772]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.3324 Train Acc: 0.8845 Eval Loss: 0.3988 Eval Acc: 0.8684 (LR: 0.00010000)
[2025-05-27 22:36:39,053]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.3232 Train Acc: 0.8887 Eval Loss: 0.4043 Eval Acc: 0.8665 (LR: 0.00010000)
[2025-05-27 22:37:40,641]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.3195 Train Acc: 0.8890 Eval Loss: 0.4279 Eval Acc: 0.8595 (LR: 0.00010000)
[2025-05-27 22:38:39,877]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.3174 Train Acc: 0.8886 Eval Loss: 0.4064 Eval Acc: 0.8667 (LR: 0.00010000)
[2025-05-27 22:39:38,793]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.3119 Train Acc: 0.8918 Eval Loss: 0.4074 Eval Acc: 0.8633 (LR: 0.00010000)
[2025-05-27 22:40:38,771]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.3128 Train Acc: 0.8915 Eval Loss: 0.4103 Eval Acc: 0.8653 (LR: 0.00010000)
[2025-05-27 22:41:36,008]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.3064 Train Acc: 0.8930 Eval Loss: 0.4027 Eval Acc: 0.8679 (LR: 0.00001000)
[2025-05-27 22:42:33,373]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.2948 Train Acc: 0.8975 Eval Loss: 0.3927 Eval Acc: 0.8718 (LR: 0.00001000)
[2025-05-27 22:43:32,086]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.2905 Train Acc: 0.8994 Eval Loss: 0.3910 Eval Acc: 0.8743 (LR: 0.00001000)
[2025-05-27 22:44:35,635]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.2882 Train Acc: 0.9004 Eval Loss: 0.3906 Eval Acc: 0.8719 (LR: 0.00001000)
[2025-05-27 22:45:36,788]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.2886 Train Acc: 0.9004 Eval Loss: 0.3916 Eval Acc: 0.8711 (LR: 0.00001000)
[2025-05-27 22:46:32,837]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.2854 Train Acc: 0.9008 Eval Loss: 0.3857 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-05-27 22:47:28,735]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.2845 Train Acc: 0.9011 Eval Loss: 0.3885 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-05-27 22:48:28,855]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.2871 Train Acc: 0.9013 Eval Loss: 0.3893 Eval Acc: 0.8725 (LR: 0.00001000)
[2025-05-27 22:49:23,804]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.2833 Train Acc: 0.9017 Eval Loss: 0.3899 Eval Acc: 0.8738 (LR: 0.00001000)
[2025-05-27 22:50:18,777]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.2832 Train Acc: 0.9012 Eval Loss: 0.3946 Eval Acc: 0.8707 (LR: 0.00001000)
[2025-05-27 22:51:12,941]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.2856 Train Acc: 0.9010 Eval Loss: 0.3930 Eval Acc: 0.8707 (LR: 0.00001000)
[2025-05-27 22:52:11,112]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.2854 Train Acc: 0.9006 Eval Loss: 0.3921 Eval Acc: 0.8726 (LR: 0.00000100)
[2025-05-27 22:53:09,257]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.2805 Train Acc: 0.9029 Eval Loss: 0.3877 Eval Acc: 0.8758 (LR: 0.00000100)
[2025-05-27 22:54:05,538]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.2855 Train Acc: 0.9009 Eval Loss: 0.3887 Eval Acc: 0.8730 (LR: 0.00000100)
[2025-05-27 22:55:05,908]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.2808 Train Acc: 0.9034 Eval Loss: 0.3851 Eval Acc: 0.8741 (LR: 0.00000100)
[2025-05-27 22:56:05,877]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.2835 Train Acc: 0.9025 Eval Loss: 0.3896 Eval Acc: 0.8725 (LR: 0.00000100)
[2025-05-27 22:57:05,893]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.2828 Train Acc: 0.9028 Eval Loss: 0.3884 Eval Acc: 0.8726 (LR: 0.00000100)
[2025-05-27 22:58:06,967]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.2843 Train Acc: 0.9012 Eval Loss: 0.3892 Eval Acc: 0.8727 (LR: 0.00000100)
[2025-05-27 22:59:04,934]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.2812 Train Acc: 0.9027 Eval Loss: 0.3896 Eval Acc: 0.8732 (LR: 0.00000100)
[2025-05-27 23:00:02,399]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.2832 Train Acc: 0.9024 Eval Loss: 0.3860 Eval Acc: 0.8741 (LR: 0.00000100)
[2025-05-27 23:00:58,782]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.2847 Train Acc: 0.9016 Eval Loss: 0.3904 Eval Acc: 0.8715 (LR: 0.00000010)
[2025-05-27 23:01:54,136]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.2812 Train Acc: 0.9021 Eval Loss: 0.3882 Eval Acc: 0.8735 (LR: 0.00000010)
[2025-05-27 23:02:48,447]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.2824 Train Acc: 0.9025 Eval Loss: 0.3870 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:03:43,626]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.2815 Train Acc: 0.9008 Eval Loss: 0.3909 Eval Acc: 0.8732 (LR: 0.00000010)
[2025-05-27 23:04:39,717]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.2827 Train Acc: 0.9031 Eval Loss: 0.3886 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:05:39,122]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.2828 Train Acc: 0.9020 Eval Loss: 0.3883 Eval Acc: 0.8716 (LR: 0.00000010)
[2025-05-27 23:06:33,148]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.2810 Train Acc: 0.9023 Eval Loss: 0.3881 Eval Acc: 0.8748 (LR: 0.00000010)
[2025-05-27 23:07:26,132]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.2816 Train Acc: 0.9011 Eval Loss: 0.3879 Eval Acc: 0.8731 (LR: 0.00000010)
[2025-05-27 23:08:19,829]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.2836 Train Acc: 0.9014 Eval Loss: 0.3881 Eval Acc: 0.8721 (LR: 0.00000010)
[2025-05-27 23:09:15,192]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.2803 Train Acc: 0.9019 Eval Loss: 0.3868 Eval Acc: 0.8724 (LR: 0.00000010)
[2025-05-27 23:10:10,602]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.2800 Train Acc: 0.9051 Eval Loss: 0.3881 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:11:08,284]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.2820 Train Acc: 0.9010 Eval Loss: 0.3884 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:12:04,909]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.2825 Train Acc: 0.9009 Eval Loss: 0.3887 Eval Acc: 0.8732 (LR: 0.00000010)
[2025-05-27 23:13:02,858]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.2801 Train Acc: 0.9021 Eval Loss: 0.3883 Eval Acc: 0.8733 (LR: 0.00000010)
[2025-05-27 23:13:56,621]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.2777 Train Acc: 0.9031 Eval Loss: 0.3892 Eval Acc: 0.8736 (LR: 0.00000010)
[2025-05-27 23:14:51,046]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.2795 Train Acc: 0.9033 Eval Loss: 0.3871 Eval Acc: 0.8752 (LR: 0.00000010)
[2025-05-27 23:16:05,197]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.2807 Train Acc: 0.9022 Eval Loss: 0.3881 Eval Acc: 0.8739 (LR: 0.00000010)
[2025-05-27 23:17:06,758]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.2821 Train Acc: 0.9031 Eval Loss: 0.3887 Eval Acc: 0.8748 (LR: 0.00000010)
[2025-05-27 23:18:01,942]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.2802 Train Acc: 0.9030 Eval Loss: 0.3874 Eval Acc: 0.8746 (LR: 0.00000010)
[2025-05-27 23:18:57,124]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.2804 Train Acc: 0.9026 Eval Loss: 0.3894 Eval Acc: 0.8730 (LR: 0.00000010)
[2025-05-27 23:19:53,865]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.2819 Train Acc: 0.9016 Eval Loss: 0.3884 Eval Acc: 0.8739 (LR: 0.00000010)
[2025-05-27 23:19:53,865]: [ResNet20_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8758
[2025-05-27 23:19:53,942]: 


Quantization of model down to 4 bits finished
[2025-05-27 23:19:53,942]: Model Architecture:
[2025-05-27 23:19:54,009]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0849], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6143192052841187, max_val=0.6598900556564331)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0674], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.522510290145874, max_val=0.4878910183906555)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0659], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4825107455253601, max_val=0.5058311223983765)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0723], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.463073194026947, max_val=0.6220464706420898)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0663], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5766500234603882, max_val=0.41732466220855713)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0710], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4599286913871765, max_val=0.6046261787414551)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4759485125541687, max_val=0.45402586460113525)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.44458848237991333, max_val=0.42457979917526245)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0626], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5297294855117798, max_val=0.4092288017272949)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0541], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4395017623901367, max_val=0.37151724100112915)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0508], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3854938745498657, max_val=0.3761533498764038)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3882233500480652, max_val=0.3930966258049011)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39195799827575684, max_val=0.3915663957595825)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3756554126739502, max_val=0.40544217824935913)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0500], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35337257385253906, max_val=0.3960513472557068)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0525], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3757522702217102, max_val=0.4115116000175476)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0516], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40120452642440796, max_val=0.3724977970123291)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0444], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3335478901863098, max_val=0.33220237493515015)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0432], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.298761248588562, max_val=0.3498861789703369)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.29387879371643066, max_val=0.2833215594291687)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 23:19:54,010]: 
Model Weights:
[2025-05-27 23:19:54,010]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 23:19:54,014]: Sample Values (25 elements): [-0.6426541209220886, 0.24111595749855042, 0.016744570806622505, -0.11762793362140656, 0.0823795273900032, 0.3330398201942444, -0.058072254061698914, -0.057918839156627655, 0.11025778949260712, -0.13092070817947388, -0.20155860483646393, 0.07468586415052414, -0.10484182834625244, -0.009911373257637024, -0.06779824942350388, -0.35034871101379395, 0.18598516285419464, 0.12728391587734222, -0.109271340072155, 0.054927680641412735, -0.3412022590637207, -0.058659348636865616, 0.22740860283374786, -0.08660665154457092, 0.16899335384368896]
[2025-05-27 23:19:54,026]: Mean: 0.00053960
[2025-05-27 23:19:54,027]: Min: -0.64265412
[2025-05-27 23:19:54,028]: Max: 0.37439919
[2025-05-27 23:19:54,028]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,028]: Sample Values (16 elements): [0.7199600338935852, 0.881969153881073, 0.6384759545326233, 0.5620710849761963, 0.5421256422996521, 0.7594660520553589, 0.8202391266822815, 0.8054354190826416, 0.8234019875526428, 1.205202341079712, 0.6614911556243896, 0.6598211526870728, 0.6744239926338196, 0.721225917339325, 0.8854226469993591, 0.74123615026474]
[2025-05-27 23:19:54,028]: Mean: 0.75637299
[2025-05-27 23:19:54,028]: Min: 0.54212564
[2025-05-27 23:19:54,029]: Max: 1.20520234
[2025-05-27 23:19:54,030]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,030]: Sample Values (25 elements): [-0.2548418641090393, -0.2548418641090393, -0.16989457607269287, 0.0, -0.16989457607269287, 0.08494728803634644, 0.0, 0.08494728803634644, -0.08494728803634644, 0.0, 0.0, 0.08494728803634644, 0.0, -0.2548418641090393, 0.0, -0.08494728803634644, -0.2548418641090393, -0.08494728803634644, 0.2548418641090393, 0.08494728803634644, 0.0, -0.33978915214538574, 0.0, 0.08494728803634644, 0.08494728803634644]
[2025-05-27 23:19:54,030]: Mean: -0.00184347
[2025-05-27 23:19:54,030]: Min: -0.59463102
[2025-05-27 23:19:54,030]: Max: 0.67957830
[2025-05-27 23:19:54,030]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,031]: Sample Values (16 elements): [0.8899332284927368, 0.7601288557052612, 0.712221086025238, 0.8277907371520996, 0.962030291557312, 0.9623211026191711, 0.8248394131660461, 0.8549318909645081, 1.0059703588485718, 0.9106495976448059, 0.6172178983688354, 0.6069960594177246, 0.8379213809967041, 1.1514277458190918, 1.121148705482483, 1.0220801830291748]
[2025-05-27 23:19:54,031]: Mean: 0.87922549
[2025-05-27 23:19:54,031]: Min: 0.60699606
[2025-05-27 23:19:54,031]: Max: 1.15142775
[2025-05-27 23:19:54,033]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,033]: Sample Values (25 elements): [0.0, -0.06736008822917938, 0.0, 0.0, -0.06736008822917938, 0.0, 0.20208026468753815, -0.06736008822917938, -0.13472017645835876, 0.06736008822917938, 0.13472017645835876, 0.06736008822917938, 0.26944035291671753, 0.06736008822917938, 0.06736008822917938, 0.13472017645835876, 0.0, 0.06736008822917938, 0.0, -0.06736008822917938, 0.20208026468753815, 0.0, 0.13472017645835876, 0.0, 0.06736008822917938]
[2025-05-27 23:19:54,033]: Mean: 0.00230966
[2025-05-27 23:19:54,034]: Min: -0.53888071
[2025-05-27 23:19:54,034]: Max: 0.47152060
[2025-05-27 23:19:54,034]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,034]: Sample Values (16 elements): [0.7521250247955322, 0.6249299645423889, 0.5161903500556946, 0.9358468651771545, 0.8913898468017578, 0.6669401526451111, 0.6192550659179688, 0.8743323683738708, 0.5826666951179504, 0.6606810688972473, 0.8357117772102356, 0.845569372177124, 0.7029781937599182, 0.5743013620376587, 0.801015317440033, 0.8751368522644043]
[2025-05-27 23:19:54,034]: Mean: 0.73494184
[2025-05-27 23:19:54,034]: Min: 0.51619035
[2025-05-27 23:19:54,035]: Max: 0.93584687
[2025-05-27 23:19:54,036]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,036]: Sample Values (25 elements): [0.0, -0.06588946282863617, 0.2635578513145447, -0.1976683884859085, -0.13177892565727234, -0.06588946282863617, -0.06588946282863617, 0.0, 0.1976683884859085, -0.13177892565727234, 0.1976683884859085, 0.13177892565727234, -0.06588946282863617, 0.06588946282863617, 0.1976683884859085, 0.13177892565727234, -0.13177892565727234, 0.13177892565727234, 0.13177892565727234, 0.06588946282863617, -0.06588946282863617, 0.13177892565727234, -0.06588946282863617, -0.1976683884859085, -0.13177892565727234]
[2025-05-27 23:19:54,036]: Mean: -0.00280259
[2025-05-27 23:19:54,036]: Min: -0.46122622
[2025-05-27 23:19:54,036]: Max: 0.52711570
[2025-05-27 23:19:54,036]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,037]: Sample Values (16 elements): [0.8616681098937988, 1.1402956247329712, 1.3249397277832031, 1.0682018995285034, 1.0368916988372803, 0.7879756689071655, 0.8057215213775635, 0.8601382970809937, 0.8772050142288208, 1.0859421491622925, 1.218174695968628, 0.6953684091567993, 0.9671945571899414, 1.2275135517120361, 0.8333852887153625, 0.6855931282043457]
[2025-05-27 23:19:54,037]: Mean: 0.96726310
[2025-05-27 23:19:54,037]: Min: 0.68559313
[2025-05-27 23:19:54,037]: Max: 1.32493973
[2025-05-27 23:19:54,038]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,039]: Sample Values (25 elements): [-0.1446826308965683, 0.07234131544828415, -0.07234131544828415, 0.07234131544828415, -0.07234131544828415, 0.2893652617931366, -0.07234131544828415, 0.1446826308965683, 0.0, -0.21702393889427185, 0.1446826308965683, -0.21702393889427185, -0.07234131544828415, -0.21702393889427185, 0.21702393889427185, 0.0, 0.07234131544828415, -0.07234131544828415, 0.21702393889427185, -0.21702393889427185, 0.07234131544828415, -0.21702393889427185, 0.07234131544828415, 0.2893652617931366, 0.21702393889427185]
[2025-05-27 23:19:54,039]: Mean: -0.00405036
[2025-05-27 23:19:54,039]: Min: -0.43404788
[2025-05-27 23:19:54,039]: Max: 0.65107185
[2025-05-27 23:19:54,039]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,039]: Sample Values (16 elements): [0.5682204365730286, 0.7829757332801819, 0.5287565588951111, 0.6091507077217102, 0.7302096486091614, 0.659342885017395, 0.5385982394218445, 0.5695311427116394, 0.4540361166000366, 0.5341546535491943, 0.8589759469032288, 0.4923173189163208, 0.6041142344474792, 0.8115857839584351, 0.6709096431732178, 1.0032650232315063]
[2025-05-27 23:19:54,040]: Mean: 0.65100902
[2025-05-27 23:19:54,040]: Min: 0.45403612
[2025-05-27 23:19:54,040]: Max: 1.00326502
[2025-05-27 23:19:54,041]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,041]: Sample Values (25 elements): [-0.06626497954130173, -0.13252995908260345, 0.06626497954130173, -0.06626497954130173, 0.0, -0.19879493117332458, 0.0, -0.33132490515708923, -0.06626497954130173, 0.06626497954130173, -0.06626497954130173, -0.13252995908260345, -0.06626497954130173, -0.19879493117332458, 0.0, 0.13252995908260345, -0.13252995908260345, 0.0, -0.06626497954130173, 0.06626497954130173, -0.19879493117332458, 0.06626497954130173, 0.0, -0.13252995908260345, 0.0]
[2025-05-27 23:19:54,042]: Mean: -0.00839817
[2025-05-27 23:19:54,042]: Min: -0.59638482
[2025-05-27 23:19:54,042]: Max: 0.39758986
[2025-05-27 23:19:54,042]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,042]: Sample Values (16 elements): [0.774094820022583, 0.9024201035499573, 0.8568373322486877, 0.9513039588928223, 1.1291375160217285, 0.7718190550804138, 1.0559113025665283, 0.8585480451583862, 0.9195536971092224, 0.948539137840271, 1.0159072875976562, 1.0982182025909424, 0.972059428691864, 0.8692214488983154, 0.8295823931694031, 1.0011394023895264]
[2025-05-27 23:19:54,042]: Mean: 0.93464333
[2025-05-27 23:19:54,042]: Min: 0.77181906
[2025-05-27 23:19:54,043]: Max: 1.12913752
[2025-05-27 23:19:54,044]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,044]: Sample Values (25 elements): [0.0709703266620636, -0.1419406533241272, 0.1419406533241272, -0.0709703266620636, -0.0709703266620636, 0.0709703266620636, 0.2129109799861908, -0.0709703266620636, -0.0709703266620636, -0.2129109799861908, 0.2838813066482544, -0.1419406533241272, 0.1419406533241272, -0.354851633310318, 0.0, -0.0709703266620636, 0.0, 0.0, 0.0709703266620636, 0.0709703266620636, -0.2129109799861908, 0.0709703266620636, -0.1419406533241272, -0.2129109799861908, 0.1419406533241272]
[2025-05-27 23:19:54,044]: Mean: -0.00135534
[2025-05-27 23:19:54,044]: Min: -0.42582196
[2025-05-27 23:19:54,044]: Max: 0.63873291
[2025-05-27 23:19:54,044]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,045]: Sample Values (16 elements): [0.4638417661190033, 0.7178043127059937, 0.9891815185546875, 0.5783552527427673, 0.6532819271087646, 0.701474666595459, 0.5944343209266663, 0.6689743995666504, 0.5444214344024658, 0.7103608250617981, 0.6213151216506958, 0.8959854245185852, 0.7364267110824585, 0.6333847641944885, 0.7398566007614136, 0.6179550290107727]
[2025-05-27 23:19:54,045]: Mean: 0.67919087
[2025-05-27 23:19:54,045]: Min: 0.46384177
[2025-05-27 23:19:54,045]: Max: 0.98918152
[2025-05-27 23:19:54,046]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 23:19:54,047]: Sample Values (25 elements): [0.18599487841129303, -0.18599487841129303, 0.0, -0.12399658560752869, -0.24799317121505737, 0.18599487841129303, -0.06199829280376434, -0.06199829280376434, -0.06199829280376434, 0.06199829280376434, 0.3099914789199829, -0.12399658560752869, -0.24799317121505737, -0.06199829280376434, 0.0, -0.06199829280376434, 0.0, 0.06199829280376434, -0.24799317121505737, -0.12399658560752869, 0.12399658560752869, 0.18599487841129303, 0.12399658560752869, -0.12399658560752869, -0.12399658560752869]
[2025-05-27 23:19:54,047]: Mean: 0.00527416
[2025-05-27 23:19:54,047]: Min: -0.49598634
[2025-05-27 23:19:54,047]: Max: 0.43398803
[2025-05-27 23:19:54,047]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,048]: Sample Values (25 elements): [0.8766804337501526, 0.727696418762207, 0.9548017382621765, 0.712350070476532, 0.7960881590843201, 0.839598536491394, 0.7350967526435852, 0.8040611743927002, 0.8831131458282471, 0.7531800866127014, 0.8227335810661316, 0.8062945008277893, 0.7801333069801331, 0.7187216877937317, 0.8984827399253845, 0.7931334376335144, 0.8789087533950806, 0.6573391556739807, 0.6477357745170593, 0.6769729256629944, 0.7792772054672241, 0.7470419406890869, 0.7024821043014526, 0.6224122047424316, 0.645995020866394]
[2025-05-27 23:19:54,048]: Mean: 0.75939178
[2025-05-27 23:19:54,048]: Min: 0.56548613
[2025-05-27 23:19:54,048]: Max: 0.95480174
[2025-05-27 23:19:54,049]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,049]: Sample Values (25 elements): [-0.05794455483555794, 0.05794455483555794, 0.0, 0.0, 0.05794455483555794, -0.23177821934223175, 0.0, 0.05794455483555794, -0.1738336682319641, -0.05794455483555794, 0.11588910967111588, 0.0, -0.05794455483555794, 0.05794455483555794, 0.0, 0.0, 0.05794455483555794, -0.05794455483555794, 0.05794455483555794, -0.11588910967111588, -0.11588910967111588, -0.05794455483555794, -0.05794455483555794, 0.0, -0.11588910967111588]
[2025-05-27 23:19:54,049]: Mean: 0.00180448
[2025-05-27 23:19:54,050]: Min: -0.46355644
[2025-05-27 23:19:54,050]: Max: 0.40561187
[2025-05-27 23:19:54,050]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,050]: Sample Values (25 elements): [0.651753306388855, 0.7912667989730835, 0.8367494344711304, 0.8184898495674133, 0.6650214791297913, 0.6676897406578064, 0.6289339065551758, 0.784439742565155, 0.6856152415275574, 0.6865620017051697, 0.7459314465522766, 0.6132388114929199, 0.6334460973739624, 0.6744452714920044, 0.6377683281898499, 0.8980578780174255, 0.693994402885437, 0.7520151138305664, 0.7556484341621399, 0.8107056021690369, 0.7498130798339844, 0.7519232630729675, 0.8186742663383484, 0.6517654657363892, 0.737139880657196]
[2025-05-27 23:19:54,050]: Mean: 0.73543894
[2025-05-27 23:19:54,050]: Min: 0.57592297
[2025-05-27 23:19:54,051]: Max: 1.00900102
[2025-05-27 23:19:54,052]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 23:19:54,052]: Sample Values (25 elements): [-0.18779167532920837, 0.0, -0.12519444525241852, 0.12519444525241852, -0.06259722262620926, -0.06259722262620926, 0.12519444525241852, -0.06259722262620926, 0.37558335065841675, 0.25038889050483704, -0.06259722262620926, 0.06259722262620926, 0.0, 0.12519444525241852, 0.18779167532920837, -0.06259722262620926, 0.37558335065841675, 0.18779167532920837, 0.0, -0.06259722262620926, 0.3129861056804657, -0.06259722262620926, 0.37558335065841675, -0.5007777810096741, 0.0]
[2025-05-27 23:19:54,052]: Mean: 0.00978082
[2025-05-27 23:19:54,052]: Min: -0.50077778
[2025-05-27 23:19:54,053]: Max: 0.43818057
[2025-05-27 23:19:54,053]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,053]: Sample Values (25 elements): [0.40467333793640137, 0.3222782015800476, 0.5021571516990662, 0.4087270200252533, 0.4922991991043091, 0.570088267326355, 0.46277356147766113, 0.7077815532684326, 0.24762405455112457, 0.49636736512184143, 0.4645204544067383, 0.30882537364959717, 0.45357707142829895, 0.4215966761112213, 0.5498287677764893, 0.3778231143951416, 0.503854513168335, 0.39936137199401855, 0.6223498582839966, 0.33328622579574585, 0.5821948051452637, 0.37810003757476807, 0.40591734647750854, 0.5688435435295105, 0.6026618480682373]
[2025-05-27 23:19:54,053]: Mean: 0.45248979
[2025-05-27 23:19:54,053]: Min: 0.18908732
[2025-05-27 23:19:54,054]: Max: 0.70778155
[2025-05-27 23:19:54,055]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,055]: Sample Values (25 elements): [0.0, -0.3244076073169708, 0.1622038036584854, -0.054067935794591904, -0.1622038036584854, 0.0, 0.0, -0.10813587158918381, 0.054067935794591904, -0.10813587158918381, 0.1622038036584854, -0.054067935794591904, -0.1622038036584854, 0.054067935794591904, 0.0, -0.054067935794591904, 0.054067935794591904, 0.10813587158918381, 0.1622038036584854, -0.054067935794591904, -0.1622038036584854, 0.10813587158918381, 0.0, 0.054067935794591904, 0.10813587158918381]
[2025-05-27 23:19:54,055]: Mean: 0.00222350
[2025-05-27 23:19:54,055]: Min: -0.43254349
[2025-05-27 23:19:54,055]: Max: 0.37847555
[2025-05-27 23:19:54,055]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,056]: Sample Values (25 elements): [0.6966106295585632, 1.05405592918396, 0.9146935939788818, 0.8274841904640198, 0.9573546051979065, 0.9360926151275635, 0.9816900491714478, 0.7152800559997559, 0.9442707896232605, 0.8961313366889954, 0.8428031802177429, 0.8093924522399902, 0.7748339772224426, 0.7056224346160889, 0.8171281814575195, 0.8141465187072754, 1.0559183359146118, 0.6963238716125488, 0.7825970649719238, 0.8672521114349365, 1.0350563526153564, 1.11383056640625, 0.8549882769584656, 0.8005373477935791, 0.8851532340049744]
[2025-05-27 23:19:54,056]: Mean: 0.86529732
[2025-05-27 23:19:54,056]: Min: 0.69632387
[2025-05-27 23:19:54,056]: Max: 1.11383057
[2025-05-27 23:19:54,057]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,058]: Sample Values (25 elements): [0.1523294597864151, -0.05077648535370827, 0.1523294597864151, 0.0, 0.0, 0.3046589195728302, -0.10155297070741653, 0.1523294597864151, -0.05077648535370827, -0.10155297070741653, 0.05077648535370827, 0.1523294597864151, -0.05077648535370827, -0.1523294597864151, 0.05077648535370827, 0.05077648535370827, 0.0, 0.10155297070741653, 0.10155297070741653, 0.0, -0.10155297070741653, 0.0, -0.10155297070741653, -0.05077648535370827, -0.1523294597864151]
[2025-05-27 23:19:54,058]: Mean: 0.00091459
[2025-05-27 23:19:54,058]: Min: -0.40621188
[2025-05-27 23:19:54,058]: Max: 0.35543540
[2025-05-27 23:19:54,058]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,058]: Sample Values (25 elements): [0.8891102075576782, 0.6821486949920654, 0.7443974614143372, 0.5714499354362488, 0.48448213934898376, 0.7316824793815613, 0.6816507577896118, 0.5808258056640625, 0.5397822260856628, 0.6322325468063354, 0.762410044670105, 0.6724870800971985, 0.6292170882225037, 0.509261965751648, 0.5566011071205139, 0.7884519100189209, 0.817934513092041, 0.6877077221870422, 0.7665836215019226, 0.6611663699150085, 0.6399352550506592, 0.7318851351737976, 0.7181316018104553, 0.8122629523277283, 0.3406904637813568]
[2025-05-27 23:19:54,058]: Mean: 0.65841293
[2025-05-27 23:19:54,059]: Min: 0.34069046
[2025-05-27 23:19:54,059]: Max: 0.88911021
[2025-05-27 23:19:54,060]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,060]: Sample Values (25 elements): [0.10417599976062775, -0.05208799988031387, 0.0, -0.15626400709152222, 0.05208799988031387, 0.05208799988031387, -0.05208799988031387, 0.0, -0.15626400709152222, -0.15626400709152222, -0.10417599976062775, 0.05208799988031387, 0.0, 0.0, -0.05208799988031387, 0.05208799988031387, 0.0, 0.15626400709152222, 0.2083519995212555, 0.10417599976062775, 0.0, 0.0, -0.05208799988031387, 0.10417599976062775, 0.0]
[2025-05-27 23:19:54,060]: Mean: 0.00003391
[2025-05-27 23:19:54,061]: Min: -0.36461601
[2025-05-27 23:19:54,061]: Max: 0.41670400
[2025-05-27 23:19:54,061]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,061]: Sample Values (25 elements): [0.8089678883552551, 0.8977544903755188, 1.003676414489746, 0.9558132886886597, 0.7295958399772644, 0.8922988772392273, 0.8717154264450073, 0.8763931393623352, 0.8163840174674988, 0.9346815347671509, 0.9750272631645203, 0.8413995504379272, 0.979736864566803, 1.2382982969284058, 0.9550454616546631, 0.842961847782135, 0.9794014096260071, 0.8949662446975708, 1.0497022867202759, 1.0617640018463135, 1.0035626888275146, 1.0286775827407837, 1.3124303817749023, 0.9973174333572388, 1.160744547843933]
[2025-05-27 23:19:54,061]: Mean: 0.98241794
[2025-05-27 23:19:54,061]: Min: 0.72959584
[2025-05-27 23:19:54,062]: Max: 1.31243038
[2025-05-27 23:19:54,063]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,063]: Sample Values (25 elements): [-0.052234962582588196, -0.052234962582588196, 0.0, -0.10446992516517639, 0.20893985033035278, 0.052234962582588196, 0.1567048877477646, 0.1567048877477646, -0.052234962582588196, 0.052234962582588196, 0.10446992516517639, -0.052234962582588196, -0.052234962582588196, -0.10446992516517639, -0.1567048877477646, 0.0, 0.10446992516517639, -0.10446992516517639, 0.1567048877477646, 0.10446992516517639, 0.0, -0.1567048877477646, 0.1567048877477646, 0.10446992516517639, -0.20893985033035278]
[2025-05-27 23:19:54,063]: Mean: -0.00264122
[2025-05-27 23:19:54,063]: Min: -0.41787970
[2025-05-27 23:19:54,063]: Max: 0.36564475
[2025-05-27 23:19:54,063]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,064]: Sample Values (25 elements): [0.7371126413345337, 0.6676781177520752, 0.7907653450965881, 0.6601159572601318, 0.6695426106452942, 0.7352165579795837, 0.7242993712425232, 0.842538595199585, 0.6749321818351746, 0.5772538781166077, 0.7543803453445435, 0.6510277986526489, 0.643217146396637, 0.7170736789703369, 0.6842182874679565, 0.6522552967071533, 0.9282348155975342, 0.7512209415435791, 0.6162936091423035, 0.7056678533554077, 0.6557865738868713, 0.8441653847694397, 0.6716113686561584, 0.5930519700050354, 0.5293178558349609]
[2025-05-27 23:19:54,064]: Mean: 0.69585884
[2025-05-27 23:19:54,064]: Min: 0.52931786
[2025-05-27 23:19:54,064]: Max: 0.92823482
[2025-05-27 23:19:54,065]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 23:19:54,066]: Sample Values (25 elements): [0.10414635390043259, 0.10414635390043259, -0.05207317695021629, -0.10414635390043259, -0.20829270780086517, 0.10414635390043259, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20829270780086517, 0.0, -0.10414635390043259, 0.05207317695021629, -0.10414635390043259, -0.05207317695021629, -0.05207317695021629, 0.15621952712535858, -0.05207317695021629, 0.05207317695021629, 0.0, 0.05207317695021629, -0.05207317695021629, 0.05207317695021629]
[2025-05-27 23:19:54,066]: Mean: -0.00218102
[2025-05-27 23:19:54,066]: Min: -0.36451223
[2025-05-27 23:19:54,066]: Max: 0.41658542
[2025-05-27 23:19:54,066]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,067]: Sample Values (25 elements): [0.9838815331459045, 0.547332227230072, 0.7798359394073486, 0.6834213137626648, 0.7784376740455627, 0.8635265827178955, 1.0005815029144287, 0.6682615876197815, 0.9010177254676819, 0.570982038974762, 0.8330108523368835, 0.804256021976471, 0.6676335334777832, 0.7657063007354736, 0.8915756344795227, 0.42705899477005005, 0.7534747123718262, 0.9701852202415466, 0.8571138978004456, 0.7816236019134521, 0.8786206841468811, 0.9923309683799744, 0.6413992047309875, 0.6854091286659241, 0.8703923225402832]
[2025-05-27 23:19:54,067]: Mean: 0.77300596
[2025-05-27 23:19:54,067]: Min: 0.00483258
[2025-05-27 23:19:54,067]: Max: 1.00058150
[2025-05-27 23:19:54,068]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,069]: Sample Values (25 elements): [0.14988479018211365, 0.14988479018211365, -0.09992319345474243, 0.0, 0.049961596727371216, 0.0, -0.049961596727371216, 0.14988479018211365, -0.049961596727371216, -0.09992319345474243, 0.14988479018211365, 0.09992319345474243, 0.14988479018211365, 0.0, -0.049961596727371216, 0.0, -0.09992319345474243, 0.049961596727371216, 0.049961596727371216, -0.09992319345474243, 0.0, -0.049961596727371216, -0.09992319345474243, 0.049961596727371216, 0.09992319345474243]
[2025-05-27 23:19:54,069]: Mean: 0.00063970
[2025-05-27 23:19:54,069]: Min: -0.34973118
[2025-05-27 23:19:54,069]: Max: 0.39969277
[2025-05-27 23:19:54,069]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,070]: Sample Values (25 elements): [0.7883546948432922, 0.7073838114738464, 0.6684300899505615, 0.9317591190338135, 0.795074462890625, 0.7838124632835388, 0.6992064714431763, 0.7416681051254272, 0.8002946376800537, 0.7166070342063904, 0.7023223638534546, 0.905210018157959, 0.8780856728553772, 0.8019479513168335, 0.8261373043060303, 0.7679257988929749, 0.5146444439888, 0.9184287190437317, 0.8467696309089661, 0.6799672842025757, 0.7086241245269775, 0.7049883604049683, 0.7562990188598633, 0.6713235974311829, 0.8222271800041199]
[2025-05-27 23:19:54,070]: Mean: 0.77510244
[2025-05-27 23:19:54,070]: Min: 0.51464444
[2025-05-27 23:19:54,070]: Max: 0.97108799
[2025-05-27 23:19:54,071]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 23:19:54,071]: Sample Values (25 elements): [0.05248425900936127, -0.1574527770280838, 0.05248425900936127, 0.0, -0.10496851801872253, 0.0, -0.10496851801872253, -0.10496851801872253, 0.10496851801872253, 0.0, 0.10496851801872253, -0.05248425900936127, -0.10496851801872253, -0.10496851801872253, 0.0, -0.10496851801872253, 0.0, 0.05248425900936127, -0.05248425900936127, -0.1574527770280838, 0.0, 0.10496851801872253, 0.20993703603744507, 0.05248425900936127, 0.3149055540561676]
[2025-05-27 23:19:54,071]: Mean: 0.00292149
[2025-05-27 23:19:54,072]: Min: -0.36738980
[2025-05-27 23:19:54,072]: Max: 0.41987407
[2025-05-27 23:19:54,072]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,072]: Sample Values (25 elements): [0.4044421315193176, 0.45941436290740967, 0.6027922034263611, 0.5781317949295044, 0.5116778612136841, 0.7284967303276062, 0.5247259140014648, 0.47221216559410095, 0.4057075083255768, 0.5934829115867615, 0.42161455750465393, 0.6643372774124146, 0.42493051290512085, 0.5014469027519226, 0.5930007100105286, 0.5147568583488464, 0.5194451212882996, 0.5800130367279053, 0.41520968079566956, 0.456794798374176, 0.5228508114814758, 0.4984164237976074, 0.36170804500579834, 0.339314341545105, 0.6859971880912781]
[2025-05-27 23:19:54,072]: Mean: 0.51195645
[2025-05-27 23:19:54,072]: Min: 0.27283946
[2025-05-27 23:19:54,073]: Max: 0.80480933
[2025-05-27 23:19:54,074]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,074]: Sample Values (25 elements): [-0.05158015713095665, -0.05158015713095665, 0.05158015713095665, 0.05158015713095665, 0.0, -0.05158015713095665, 0.1031603142619133, -0.05158015713095665, -0.1031603142619133, -0.2063206285238266, 0.05158015713095665, 0.05158015713095665, 0.1031603142619133, 0.0, 0.05158015713095665, 0.05158015713095665, 0.05158015713095665, 0.1031603142619133, -0.05158015713095665, -0.05158015713095665, -0.15474046766757965, -0.05158015713095665, 0.0, 0.0, 0.05158015713095665]
[2025-05-27 23:19:54,075]: Mean: -0.00073598
[2025-05-27 23:19:54,075]: Min: -0.41264126
[2025-05-27 23:19:54,075]: Max: 0.36106110
[2025-05-27 23:19:54,075]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,075]: Sample Values (25 elements): [1.0804654359817505, 0.6318374276161194, 1.0155926942825317, 0.8192660212516785, 1.0705831050872803, 0.8164719343185425, 0.16418103873729706, 1.0483007431030273, 1.0030792951583862, 1.132487177848816, 0.9804920554161072, 0.9863764643669128, 0.8233485221862793, 1.1569116115570068, 0.9837790131568909, 1.087405800819397, 1.060770034790039, 1.0172396898269653, 0.8810496926307678, 0.44543299078941345, 1.093314290046692, 0.9981855154037476, 1.074906349182129, 0.7774664163589478, 1.1773655414581299]
[2025-05-27 23:19:54,075]: Mean: 0.93683958
[2025-05-27 23:19:54,076]: Min: 0.16418104
[2025-05-27 23:19:54,076]: Max: 1.22385383
[2025-05-27 23:19:54,077]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,077]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.04438335448503494, 0.0, -0.08876670897006989, 0.0, -0.08876670897006989, 0.04438335448503494, 0.04438335448503494, 0.0, -0.08876670897006989, -0.04438335448503494, -0.08876670897006989, -0.04438335448503494, 0.0, 0.0, -0.04438335448503494, 0.0, 0.0, -0.04438335448503494, -0.08876670897006989, -0.04438335448503494, 0.08876670897006989, 0.13315007090568542]
[2025-05-27 23:19:54,077]: Mean: -0.00002769
[2025-05-27 23:19:54,078]: Min: -0.35506684
[2025-05-27 23:19:54,078]: Max: 0.31068349
[2025-05-27 23:19:54,078]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,078]: Sample Values (25 elements): [0.9150764346122742, 0.8667334914207458, 0.7415573596954346, 1.0034372806549072, 0.8865050673484802, 0.8451539874076843, 0.8453237414360046, 0.7458606362342834, 0.68619304895401, 0.9333871603012085, 0.8522279262542725, 0.9410866498947144, 1.0869415998458862, 0.9895835518836975, 0.8770179152488708, 0.6870368123054504, 0.7851063013076782, 0.9629465937614441, 0.9354592561721802, 0.7890484929084778, 0.8943039774894714, 0.7790056467056274, 0.8828809261322021, 0.8183607459068298, 0.7947760820388794]
[2025-05-27 23:19:54,078]: Mean: 0.84251010
[2025-05-27 23:19:54,078]: Min: 0.51757538
[2025-05-27 23:19:54,079]: Max: 1.11055732
[2025-05-27 23:19:54,080]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,080]: Sample Values (25 elements): [0.0, 0.0432431623339653, 0.0432431623339653, -0.0432431623339653, -0.0864863246679306, 0.0, 0.0432431623339653, -0.0432431623339653, 0.0432431623339653, 0.0, 0.0, -0.0432431623339653, 0.0864863246679306, -0.0864863246679306, 0.1297294795513153, 0.0432431623339653, 0.0432431623339653, -0.1729726493358612, -0.0864863246679306, 0.2162158191204071, 0.0432431623339653, -0.1297294795513153, 0.0864863246679306, -0.0432431623339653, 0.0]
[2025-05-27 23:19:54,081]: Mean: -0.00103228
[2025-05-27 23:19:54,081]: Min: -0.30270213
[2025-05-27 23:19:54,081]: Max: 0.34594530
[2025-05-27 23:19:54,081]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,081]: Sample Values (25 elements): [0.5841192603111267, 0.9577358365058899, 1.1978871822357178, 1.0794408321380615, 0.9942445755004883, 0.8073200583457947, 0.6832258105278015, 1.1155389547348022, 6.209714014808994e-41, 1.2905081510543823, 0.7319194674491882, 5.080547712256057e-41, 0.9910449385643005, 1.018579363822937, 0.7572420239448547, 0.9987689852714539, 0.9997023344039917, 0.5742692947387695, 1.0670888423919678, 0.8173263669013977, 1.15804922580719, 1.0335661172866821, 0.8502309322357178, 0.9623343348503113, 0.37332311272621155]
[2025-05-27 23:19:54,081]: Mean: 0.83970749
[2025-05-27 23:19:54,082]: Min: 0.00000000
[2025-05-27 23:19:54,082]: Max: 1.29050815
[2025-05-27 23:19:54,083]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,083]: Sample Values (25 elements): [-0.07696004956960678, 0.03848002478480339, 0.11544007062911987, 0.0, 0.03848002478480339, 0.0, 0.0, 0.0, 0.0, 0.07696004956960678, 0.0, 0.07696004956960678, 0.0, 0.0, 0.07696004956960678, -0.07696004956960678, -0.11544007062911987, -0.07696004956960678, -0.03848002478480339, -0.15392009913921356, 0.03848002478480339, 0.07696004956960678, 0.07696004956960678, 0.15392009913921356, -0.03848002478480339]
[2025-05-27 23:19:54,083]: Mean: -0.00058455
[2025-05-27 23:19:54,084]: Min: -0.30784020
[2025-05-27 23:19:54,084]: Max: 0.26936018
[2025-05-27 23:19:54,084]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,084]: Sample Values (25 elements): [1.0055742263793945, 1.0583033561706543, 1.0093821287155151, 0.9218530654907227, 1.0623267889022827, 1.0975815057754517, 0.9984928965568542, 1.015810489654541, 0.948951005935669, 0.9736665487289429, 1.1900631189346313, 1.0926525592803955, 0.9422924518585205, 1.0330703258514404, 0.9575133919715881, 1.0755020380020142, 1.1029974222183228, 1.2629973888397217, 0.9415900111198425, 1.138209581375122, 1.0206646919250488, 1.102783203125, 0.9917560815811157, 0.8761133551597595, 0.985011637210846]
[2025-05-27 23:19:54,084]: Mean: 1.01513863
[2025-05-27 23:19:54,084]: Min: 0.82803351
[2025-05-27 23:19:54,085]: Max: 1.26299739
[2025-05-27 23:19:54,085]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 23:19:54,085]: Sample Values (25 elements): [-0.344186395406723, -0.3131575584411621, 0.30628857016563416, 0.7052105665206909, 0.2880244851112366, -0.28712838888168335, -0.18181142210960388, -0.3256983160972595, 0.1652415692806244, -0.4332570731639862, 0.5540663599967957, -0.12367850542068481, 0.5025562047958374, -0.45604774355888367, 0.7971539497375488, -0.2657325863838196, 0.4372327923774719, -0.2819989323616028, 0.12590208649635315, -0.3948247730731964, -0.4716801941394806, 0.47795572876930237, -0.1658487617969513, -0.30977094173431396, -0.5038914680480957]
[2025-05-27 23:19:54,085]: Mean: -0.00667225
[2025-05-27 23:19:54,085]: Min: -1.02350676
[2025-05-27 23:19:54,086]: Max: 1.23685789
[2025-05-27 23:19:54,086]: 

[2025-05-28 01:29:32,449]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT
[2025-05-28 01:29:32,454]: 


QAT of ResNet20 with hardtanh down to 3 bits...
[2025-05-28 01:29:32,832]: [ResNet20_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-28 01:29:33,322]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-28 01:30:29,273]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.5186 Train Acc: 0.8192 Eval Loss: 0.7521 Eval Acc: 0.7667 (LR: 0.00100000)
[2025-05-28 01:31:23,579]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.5243 Train Acc: 0.8190 Eval Loss: 0.6567 Eval Acc: 0.7805 (LR: 0.00100000)
[2025-05-28 01:32:17,977]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.5257 Train Acc: 0.8177 Eval Loss: 0.7967 Eval Acc: 0.7464 (LR: 0.00100000)
[2025-05-28 01:33:12,368]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.5274 Train Acc: 0.8158 Eval Loss: 0.7942 Eval Acc: 0.7410 (LR: 0.00100000)
[2025-05-28 01:34:06,808]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.5264 Train Acc: 0.8182 Eval Loss: 0.6945 Eval Acc: 0.7718 (LR: 0.00100000)
[2025-05-28 01:35:00,966]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.5240 Train Acc: 0.8182 Eval Loss: 0.6518 Eval Acc: 0.7833 (LR: 0.00100000)
[2025-05-28 01:35:55,537]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.5237 Train Acc: 0.8187 Eval Loss: 0.5848 Eval Acc: 0.8054 (LR: 0.00100000)
[2025-05-28 01:36:50,366]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.5185 Train Acc: 0.8185 Eval Loss: 0.7315 Eval Acc: 0.7618 (LR: 0.00100000)
[2025-05-28 01:37:44,782]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.5170 Train Acc: 0.8207 Eval Loss: 0.7654 Eval Acc: 0.7572 (LR: 0.00100000)
[2025-05-28 01:38:39,115]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.5200 Train Acc: 0.8180 Eval Loss: 0.6104 Eval Acc: 0.7978 (LR: 0.00100000)
[2025-05-28 01:39:33,913]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.5094 Train Acc: 0.8228 Eval Loss: 0.6519 Eval Acc: 0.7828 (LR: 0.00100000)
[2025-05-28 01:40:28,134]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.5120 Train Acc: 0.8208 Eval Loss: 0.9313 Eval Acc: 0.7194 (LR: 0.00100000)
[2025-05-28 01:41:22,303]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.5167 Train Acc: 0.8190 Eval Loss: 0.7758 Eval Acc: 0.7453 (LR: 0.00010000)
[2025-05-28 01:42:16,315]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.4167 Train Acc: 0.8555 Eval Loss: 0.4549 Eval Acc: 0.8469 (LR: 0.00010000)
[2025-05-28 01:43:10,532]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.3935 Train Acc: 0.8632 Eval Loss: 0.4357 Eval Acc: 0.8537 (LR: 0.00010000)
[2025-05-28 01:44:04,496]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.3835 Train Acc: 0.8677 Eval Loss: 0.4682 Eval Acc: 0.8453 (LR: 0.00010000)
[2025-05-28 01:44:58,573]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.3790 Train Acc: 0.8685 Eval Loss: 0.4587 Eval Acc: 0.8467 (LR: 0.00010000)
[2025-05-28 01:45:52,691]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.3717 Train Acc: 0.8712 Eval Loss: 0.4834 Eval Acc: 0.8426 (LR: 0.00010000)
[2025-05-28 01:46:47,140]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.3732 Train Acc: 0.8697 Eval Loss: 0.4395 Eval Acc: 0.8517 (LR: 0.00010000)
[2025-05-28 01:47:41,891]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.3716 Train Acc: 0.8695 Eval Loss: 0.4363 Eval Acc: 0.8561 (LR: 0.00010000)
[2025-05-28 01:48:36,272]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.3681 Train Acc: 0.8723 Eval Loss: 0.4543 Eval Acc: 0.8505 (LR: 0.00001000)
[2025-05-28 01:49:30,512]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.3496 Train Acc: 0.8770 Eval Loss: 0.4200 Eval Acc: 0.8629 (LR: 0.00001000)
[2025-05-28 01:50:24,710]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.3465 Train Acc: 0.8798 Eval Loss: 0.4148 Eval Acc: 0.8611 (LR: 0.00001000)
[2025-05-28 01:51:19,054]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.3421 Train Acc: 0.8812 Eval Loss: 0.4211 Eval Acc: 0.8596 (LR: 0.00001000)
[2025-05-28 01:52:13,466]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.3437 Train Acc: 0.8794 Eval Loss: 0.4182 Eval Acc: 0.8586 (LR: 0.00001000)
[2025-05-28 01:53:07,852]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.3443 Train Acc: 0.8804 Eval Loss: 0.4157 Eval Acc: 0.8611 (LR: 0.00001000)
[2025-05-28 01:54:02,111]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.3460 Train Acc: 0.8793 Eval Loss: 0.4215 Eval Acc: 0.8595 (LR: 0.00001000)
[2025-05-28 01:54:56,468]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.3428 Train Acc: 0.8813 Eval Loss: 0.4176 Eval Acc: 0.8604 (LR: 0.00001000)
[2025-05-28 01:55:51,007]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.3411 Train Acc: 0.8810 Eval Loss: 0.4152 Eval Acc: 0.8630 (LR: 0.00000100)
[2025-05-28 01:56:45,221]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.3385 Train Acc: 0.8835 Eval Loss: 0.4125 Eval Acc: 0.8606 (LR: 0.00000100)
[2025-05-28 01:57:39,420]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.3388 Train Acc: 0.8821 Eval Loss: 0.4120 Eval Acc: 0.8620 (LR: 0.00000100)
[2025-05-28 01:58:33,666]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.3321 Train Acc: 0.8839 Eval Loss: 0.4138 Eval Acc: 0.8610 (LR: 0.00000100)
[2025-05-28 01:59:27,870]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.3423 Train Acc: 0.8804 Eval Loss: 0.4115 Eval Acc: 0.8629 (LR: 0.00000100)
[2025-05-28 02:00:22,388]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.3379 Train Acc: 0.8841 Eval Loss: 0.4155 Eval Acc: 0.8635 (LR: 0.00000100)
[2025-05-28 02:01:16,575]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.3359 Train Acc: 0.8833 Eval Loss: 0.4106 Eval Acc: 0.8629 (LR: 0.00000100)
[2025-05-28 02:02:10,565]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.3371 Train Acc: 0.8829 Eval Loss: 0.4158 Eval Acc: 0.8645 (LR: 0.00000100)
[2025-05-28 02:03:04,804]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.3385 Train Acc: 0.8830 Eval Loss: 0.4090 Eval Acc: 0.8670 (LR: 0.00000100)
[2025-05-28 02:03:59,011]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.3397 Train Acc: 0.8823 Eval Loss: 0.4125 Eval Acc: 0.8644 (LR: 0.00000100)
[2025-05-28 02:04:53,422]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.3370 Train Acc: 0.8825 Eval Loss: 0.4176 Eval Acc: 0.8615 (LR: 0.00000100)
[2025-05-28 02:05:48,125]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.3362 Train Acc: 0.8813 Eval Loss: 0.4119 Eval Acc: 0.8649 (LR: 0.00000100)
[2025-05-28 02:06:44,303]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.3369 Train Acc: 0.8824 Eval Loss: 0.4136 Eval Acc: 0.8622 (LR: 0.00000100)
[2025-05-28 02:07:45,443]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.3336 Train Acc: 0.8835 Eval Loss: 0.4116 Eval Acc: 0.8620 (LR: 0.00000100)
[2025-05-28 02:08:46,057]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.3304 Train Acc: 0.8856 Eval Loss: 0.4150 Eval Acc: 0.8631 (LR: 0.00000010)
[2025-05-28 02:09:47,063]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.3364 Train Acc: 0.8827 Eval Loss: 0.4112 Eval Acc: 0.8643 (LR: 0.00000010)
[2025-05-28 02:10:47,855]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.3355 Train Acc: 0.8842 Eval Loss: 0.4149 Eval Acc: 0.8622 (LR: 0.00000010)
[2025-05-28 02:11:48,886]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.3333 Train Acc: 0.8846 Eval Loss: 0.4103 Eval Acc: 0.8645 (LR: 0.00000010)
[2025-05-28 02:12:50,156]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.3298 Train Acc: 0.8859 Eval Loss: 0.4116 Eval Acc: 0.8639 (LR: 0.00000010)
[2025-05-28 02:13:51,136]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.3348 Train Acc: 0.8831 Eval Loss: 0.4131 Eval Acc: 0.8626 (LR: 0.00000010)
[2025-05-28 02:14:51,941]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.3325 Train Acc: 0.8865 Eval Loss: 0.4105 Eval Acc: 0.8613 (LR: 0.00000010)
[2025-05-28 02:15:52,775]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.3320 Train Acc: 0.8838 Eval Loss: 0.4174 Eval Acc: 0.8606 (LR: 0.00000010)
[2025-05-28 02:16:53,910]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.3341 Train Acc: 0.8826 Eval Loss: 0.4068 Eval Acc: 0.8619 (LR: 0.00000010)
[2025-05-28 02:17:54,848]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.3339 Train Acc: 0.8822 Eval Loss: 0.4123 Eval Acc: 0.8625 (LR: 0.00000010)
[2025-05-28 02:18:55,808]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.3335 Train Acc: 0.8845 Eval Loss: 0.4119 Eval Acc: 0.8639 (LR: 0.00000010)
[2025-05-28 02:19:57,658]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.3389 Train Acc: 0.8841 Eval Loss: 0.4118 Eval Acc: 0.8643 (LR: 0.00000010)
[2025-05-28 02:20:58,656]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.3324 Train Acc: 0.8848 Eval Loss: 0.4144 Eval Acc: 0.8623 (LR: 0.00000010)
[2025-05-28 02:21:59,445]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.3335 Train Acc: 0.8849 Eval Loss: 0.4120 Eval Acc: 0.8612 (LR: 0.00000010)
[2025-05-28 02:23:00,288]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.3361 Train Acc: 0.8835 Eval Loss: 0.4143 Eval Acc: 0.8618 (LR: 0.00000010)
[2025-05-28 02:24:00,258]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.3312 Train Acc: 0.8849 Eval Loss: 0.4129 Eval Acc: 0.8641 (LR: 0.00000010)
[2025-05-28 02:25:01,909]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.3341 Train Acc: 0.8848 Eval Loss: 0.4135 Eval Acc: 0.8627 (LR: 0.00000010)
[2025-05-28 02:25:55,011]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.3337 Train Acc: 0.8828 Eval Loss: 0.4126 Eval Acc: 0.8624 (LR: 0.00000010)
[2025-05-28 02:25:55,011]: [ResNet20_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8670
[2025-05-28 02:25:55,070]: 


Quantization of model down to 3 bits finished
[2025-05-28 02:25:55,070]: Model Architecture:
[2025-05-28 02:25:55,123]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2101], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6697229146957397, max_val=0.8009130954742432)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1591], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5649782419204712, max_val=0.548621416091919)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1514], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5365430116653442, max_val=0.5231335163116455)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1668], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5449584722518921, max_val=0.6225415468215942)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1586], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6295100450515747, max_val=0.48066049814224243)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1628], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49039554595947266, max_val=0.6495349407196045)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1394], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5005074739456177, max_val=0.4749515652656555)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1319], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4748920798301697, max_val=0.44870781898498535)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1490], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5215785503387451, max_val=0.5215318202972412)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1200], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4360434412956238, max_val=0.40374115109443665)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1134], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39324140548706055, max_val=0.4005874991416931)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1169], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4093770384788513, max_val=0.40917539596557617)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1175], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4111122488975525, max_val=0.41119325160980225)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1175], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41224926710128784, max_val=0.41050612926483154)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1143], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3591172695159912, max_val=0.4410126805305481)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1296], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.453443706035614, max_val=0.4534432888031006)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1253], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4578039050102234, max_val=0.41957688331604004)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1052], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3721088767051697, max_val=0.3644782304763794)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1062], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3591262102127075, max_val=0.3845975995063782)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0784], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27172499895095825, max_val=0.2767793536186218)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-28 02:25:55,124]: 
Model Weights:
[2025-05-28 02:25:55,124]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-28 02:25:55,127]: Sample Values (25 elements): [0.34370991587638855, 0.17263725399971008, 0.2056685835123062, 0.0832485482096672, -0.3077431321144104, 0.1863238662481308, 0.06030620262026787, -0.17406433820724487, 0.3669530153274536, 0.2661249339580536, -0.02617013268172741, 9.163453069049865e-05, 0.08499139547348022, 0.20026196539402008, -0.15182311832904816, 0.17423281073570251, -0.10659005492925644, -0.1298324018716812, 0.21712344884872437, -0.04748096689581871, -0.18822839856147766, -0.05846583843231201, 0.20426760613918304, 0.3194216191768646, 0.03240438550710678]
[2025-05-28 02:25:55,137]: Mean: 0.00038215
[2025-05-28 02:25:55,137]: Min: -0.62313497
[2025-05-28 02:25:55,137]: Max: 0.40120149
[2025-05-28 02:25:55,137]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,138]: Sample Values (16 elements): [0.8915883898735046, 0.6545968055725098, 0.9369418025016785, 0.6722307801246643, 0.7902864813804626, 0.9878193140029907, 0.7203459739685059, 0.9153258204460144, 0.8961492776870728, 1.421970009803772, 0.8635890483856201, 0.7504258751869202, 0.6750190854072571, 0.8029582500457764, 0.7350221872329712, 0.8355352878570557]
[2025-05-28 02:25:55,138]: Mean: 0.84686273
[2025-05-28 02:25:55,138]: Min: 0.65459681
[2025-05-28 02:25:55,138]: Max: 1.42197001
[2025-05-28 02:25:55,139]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,139]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.21009086072444916, -0.21009086072444916, 0.0, 0.0, 0.0, -0.21009086072444916, -0.21009086072444916, -0.21009086072444916, -0.21009086072444916, -0.21009086072444916, 0.0, -0.21009086072444916, 0.0, 0.21009086072444916, 0.0, 0.21009086072444916, 0.0, -0.21009086072444916, 0.21009086072444916, 0.0, 0.0, -0.21009086072444916]
[2025-05-28 02:25:55,140]: Mean: -0.00009119
[2025-05-28 02:25:55,140]: Min: -0.63027257
[2025-05-28 02:25:55,140]: Max: 0.84036344
[2025-05-28 02:25:55,140]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,140]: Sample Values (16 elements): [0.8176381587982178, 1.0438272953033447, 0.861684262752533, 1.0990748405456543, 0.9903313517570496, 0.696682870388031, 0.9815497994422913, 0.8288586735725403, 0.7058954238891602, 1.0938868522644043, 0.7675558924674988, 0.9912732243537903, 0.8663222193717957, 0.8667140603065491, 0.7745399475097656, 0.6258865594863892]
[2025-05-28 02:25:55,140]: Mean: 0.87573260
[2025-05-28 02:25:55,141]: Min: 0.62588656
[2025-05-28 02:25:55,141]: Max: 1.09907484
[2025-05-28 02:25:55,142]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,142]: Sample Values (25 elements): [0.15908567607402802, 0.0, -0.15908567607402802, 0.0, -0.15908567607402802, 0.0, 0.15908567607402802, 0.15908567607402802, 0.31817135214805603, -0.15908567607402802, -0.15908567607402802, 0.0, 0.0, 0.15908567607402802, 0.15908567607402802, 0.0, 0.0, -0.15908567607402802, -0.31817135214805603, 0.0, 0.15908567607402802, -0.15908567607402802, -0.15908567607402802, -0.15908567607402802, 0.0]
[2025-05-28 02:25:55,142]: Mean: 0.00317619
[2025-05-28 02:25:55,142]: Min: -0.63634270
[2025-05-28 02:25:55,142]: Max: 0.47725701
[2025-05-28 02:25:55,142]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,143]: Sample Values (16 elements): [0.6119037866592407, 0.6378180980682373, 0.5762261152267456, 0.6308867931365967, 0.7875370383262634, 0.4741102159023285, 0.9286661148071289, 0.7528268098831177, 0.8105657696723938, 0.8871376514434814, 0.6109964847564697, 0.9383946657180786, 0.7362233996391296, 0.7369725108146667, 0.881726086139679, 0.5814177989959717]
[2025-05-28 02:25:55,143]: Mean: 0.72396314
[2025-05-28 02:25:55,143]: Min: 0.47411022
[2025-05-28 02:25:55,143]: Max: 0.93839467
[2025-05-28 02:25:55,144]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,144]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.15138237178325653, 0.15138237178325653, 0.0, -0.15138237178325653, 0.15138237178325653, 0.0, 0.30276474356651306, -0.30276474356651306, 0.0, -0.15138237178325653, 0.0, 0.0, 0.15138237178325653, 0.15138237178325653, 0.0, 0.0, 0.0, -0.15138237178325653, 0.15138237178325653, 0.0, -0.15138237178325653]
[2025-05-28 02:25:55,145]: Mean: -0.00105127
[2025-05-28 02:25:55,145]: Min: -0.60552949
[2025-05-28 02:25:55,145]: Max: 0.45414710
[2025-05-28 02:25:55,145]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,145]: Sample Values (16 elements): [0.7479534149169922, 0.8732932209968567, 0.7734206914901733, 1.0138909816741943, 1.082748293876648, 0.764386773109436, 0.9832496643066406, 1.214929223060608, 0.6545184254646301, 0.8214617967605591, 0.8541505932807922, 0.8194344639778137, 1.1876871585845947, 1.0434012413024902, 0.970274031162262, 1.2960816621780396]
[2025-05-28 02:25:55,145]: Mean: 0.94380510
[2025-05-28 02:25:55,146]: Min: 0.65451843
[2025-05-28 02:25:55,146]: Max: 1.29608166
[2025-05-28 02:25:55,147]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,147]: Sample Values (25 elements): [-0.16678573191165924, -0.3335714638233185, 0.0, 0.16678573191165924, -0.3335714638233185, 0.0, 0.16678573191165924, 0.0, -0.16678573191165924, 0.16678573191165924, 0.16678573191165924, -0.16678573191165924, 0.16678573191165924, -0.16678573191165924, 0.16678573191165924, 0.0, 0.0, 0.0, -0.16678573191165924, 0.16678573191165924, 0.0, 0.3335714638233185, 0.0, 0.3335714638233185, -0.16678573191165924]
[2025-05-28 02:25:55,147]: Mean: -0.00419860
[2025-05-28 02:25:55,147]: Min: -0.50035721
[2025-05-28 02:25:55,147]: Max: 0.66714293
[2025-05-28 02:25:55,147]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,148]: Sample Values (16 elements): [0.8202404975891113, 0.5049985647201538, 0.7896280288696289, 0.810437023639679, 1.0269724130630493, 0.5417959094047546, 0.7269442677497864, 0.5555393695831299, 0.5462889075279236, 0.5965890288352966, 0.5517818331718445, 0.40444332361221313, 0.6508734822273254, 0.6445609331130981, 0.5329882502555847, 0.5943019390106201]
[2025-05-28 02:25:55,148]: Mean: 0.64364898
[2025-05-28 02:25:55,148]: Min: 0.40444332
[2025-05-28 02:25:55,148]: Max: 1.02697241
[2025-05-28 02:25:55,149]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,149]: Sample Values (25 elements): [-0.15859580039978027, -0.15859580039978027, -0.15859580039978027, 0.0, 0.15859580039978027, 0.0, -0.15859580039978027, 0.0, 0.15859580039978027, 0.15859580039978027, 0.0, 0.15859580039978027, 0.0, -0.15859580039978027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.15859580039978027, 0.0, 0.0, -0.4757874011993408]
[2025-05-28 02:25:55,149]: Mean: -0.00943039
[2025-05-28 02:25:55,150]: Min: -0.63438320
[2025-05-28 02:25:55,150]: Max: 0.47578740
[2025-05-28 02:25:55,150]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,150]: Sample Values (16 elements): [0.9059003591537476, 0.9484075307846069, 0.9359661340713501, 0.7397465705871582, 0.922544538974762, 0.9441477656364441, 0.8697326183319092, 0.8956549167633057, 1.0405128002166748, 0.8515046834945679, 0.9824320673942566, 0.8557999134063721, 0.9448953866958618, 1.1906695365905762, 1.1000394821166992, 0.8669593930244446]
[2025-05-28 02:25:55,150]: Mean: 0.93718207
[2025-05-28 02:25:55,150]: Min: 0.73974657
[2025-05-28 02:25:55,151]: Max: 1.19066954
[2025-05-28 02:25:55,152]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,152]: Sample Values (25 elements): [0.0, 0.0, 0.3256944417953491, 0.0, -0.16284722089767456, 0.16284722089767456, 0.0, -0.16284722089767456, 0.0, 0.0, -0.16284722089767456, 0.0, -0.16284722089767456, 0.0, 0.0, -0.16284722089767456, 0.0, -0.16284722089767456, -0.16284722089767456, 0.0, 0.16284722089767456, 0.16284722089767456, -0.16284722089767456, 0.0, -0.16284722089767456]
[2025-05-28 02:25:55,152]: Mean: -0.00212041
[2025-05-28 02:25:55,152]: Min: -0.48854166
[2025-05-28 02:25:55,152]: Max: 0.65138888
[2025-05-28 02:25:55,152]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,153]: Sample Values (16 elements): [0.6755640506744385, 0.5542540550231934, 0.7338504791259766, 0.6320100426673889, 0.5922788977622986, 0.6827216148376465, 0.7410493493080139, 0.5896729230880737, 0.6247391104698181, 0.8908321857452393, 0.6475778222084045, 0.6892907023429871, 0.9947946071624756, 0.639370858669281, 0.6839615702629089, 0.47836068272590637]
[2025-05-28 02:25:55,153]: Mean: 0.67814553
[2025-05-28 02:25:55,153]: Min: 0.47836068
[2025-05-28 02:25:55,153]: Max: 0.99479461
[2025-05-28 02:25:55,154]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-28 02:25:55,154]: Sample Values (25 elements): [0.0, -0.13935129344463348, -0.13935129344463348, 0.0, 0.13935129344463348, 0.27870258688926697, 0.13935129344463348, -0.27870258688926697, -0.13935129344463348, 0.0, -0.13935129344463348, 0.0, 0.0, -0.13935129344463348, 0.13935129344463348, 0.0, 0.0, -0.13935129344463348, -0.13935129344463348, 0.13935129344463348, 0.0, 0.13935129344463348, 0.0, -0.27870258688926697, 0.0]
[2025-05-28 02:25:55,154]: Mean: 0.00480835
[2025-05-28 02:25:55,155]: Min: -0.55740517
[2025-05-28 02:25:55,155]: Max: 0.41805387
[2025-05-28 02:25:55,155]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,155]: Sample Values (25 elements): [0.6715849041938782, 0.6181246638298035, 0.6380940675735474, 0.7468742728233337, 0.8042741417884827, 0.8353410959243774, 0.7797037363052368, 0.9028513431549072, 0.7298941612243652, 0.7493135333061218, 0.9508644938468933, 0.9113667011260986, 0.9806722402572632, 0.7165733575820923, 0.6352713704109192, 0.7714037299156189, 0.7542346119880676, 0.7321503162384033, 0.8074820637702942, 0.6598275303840637, 0.8476630449295044, 0.6454087495803833, 0.916163980960846, 0.739037036895752, 0.8192486763000488]
[2025-05-28 02:25:55,155]: Mean: 0.77109253
[2025-05-28 02:25:55,155]: Min: 0.61812466
[2025-05-28 02:25:55,155]: Max: 0.98067224
[2025-05-28 02:25:55,156]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,157]: Sample Values (25 elements): [-0.13194285333156586, 0.13194285333156586, -0.3958285450935364, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13194285333156586, 0.13194285333156586, 0.0, 0.0, -0.13194285333156586, 0.13194285333156586, 0.13194285333156586, 0.13194285333156586, 0.2638857066631317, -0.13194285333156586, -0.13194285333156586, 0.0, 0.0, 0.0, 0.13194285333156586, -0.13194285333156586, 0.13194285333156586]
[2025-05-28 02:25:55,157]: Mean: 0.00153189
[2025-05-28 02:25:55,157]: Min: -0.52777141
[2025-05-28 02:25:55,157]: Max: 0.39582855
[2025-05-28 02:25:55,157]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,157]: Sample Values (25 elements): [0.738884687423706, 0.8683633208274841, 0.6374307870864868, 0.7826436161994934, 0.7593419551849365, 0.6323198676109314, 0.619491696357727, 0.7433492541313171, 0.67511385679245, 0.6406503915786743, 0.909334123134613, 0.7149009704589844, 0.8253446817398071, 0.7070591449737549, 0.760409414768219, 0.8299437761306763, 0.6426486372947693, 0.8259216547012329, 0.6799277663230896, 0.673331618309021, 1.033713936805725, 0.8831403851509094, 0.6509718298912048, 0.7141711115837097, 0.6833220720291138]
[2025-05-28 02:25:55,158]: Mean: 0.74177849
[2025-05-28 02:25:55,158]: Min: 0.59243816
[2025-05-28 02:25:55,158]: Max: 1.03371394
[2025-05-28 02:25:55,159]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-28 02:25:55,159]: Sample Values (25 elements): [-0.14901576936244965, 0.14901576936244965, 0.0, -0.14901576936244965, 0.0, 0.0, -0.2980315387248993, -0.14901576936244965, -0.14901576936244965, 0.2980315387248993, -0.14901576936244965, -0.2980315387248993, 0.44704729318618774, 0.2980315387248993, 0.14901576936244965, 0.14901576936244965, 0.14901576936244965, -0.14901576936244965, -0.2980315387248993, -0.14901576936244965, 0.0, 0.14901576936244965, 0.14901576936244965, 0.14901576936244965, -0.14901576936244965]
[2025-05-28 02:25:55,159]: Mean: 0.00611197
[2025-05-28 02:25:55,159]: Min: -0.59606308
[2025-05-28 02:25:55,160]: Max: 0.44704729
[2025-05-28 02:25:55,160]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,160]: Sample Values (25 elements): [0.16969051957130432, 0.4756442606449127, 0.3448815941810608, 0.3855862319469452, 0.23456892371177673, 0.5569912195205688, 0.5487092733383179, 0.4995564818382263, 0.28240975737571716, 0.37574443221092224, 0.4244999587535858, 0.6287470459938049, 0.36421531438827515, 0.43853846192359924, 0.37955451011657715, 0.3939521908760071, 0.4116326570510864, 0.3482111990451813, 0.46458274126052856, 0.5620684027671814, 0.3337269425392151, 0.4061790704727173, 0.4655732214450836, 0.505101203918457, 0.434029757976532]
[2025-05-28 02:25:55,160]: Mean: 0.42778704
[2025-05-28 02:25:55,160]: Min: 0.16969052
[2025-05-28 02:25:55,160]: Max: 0.72930366
[2025-05-28 02:25:55,161]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,162]: Sample Values (25 elements): [0.0, 0.11996922641992569, -0.11996922641992569, 0.0, 0.0, -0.11996922641992569, 0.23993845283985138, 0.0, 0.0, 0.23993845283985138, 0.0, 0.0, 0.11996922641992569, 0.11996922641992569, 0.11996922641992569, 0.11996922641992569, -0.11996922641992569, 0.0, 0.11996922641992569, -0.23993845283985138, 0.11996922641992569, 0.11996922641992569, 0.0, 0.11996922641992569, -0.11996922641992569]
[2025-05-28 02:25:55,162]: Mean: 0.00177038
[2025-05-28 02:25:55,162]: Min: -0.47987691
[2025-05-28 02:25:55,162]: Max: 0.35990769
[2025-05-28 02:25:55,162]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,162]: Sample Values (25 elements): [0.7414290904998779, 0.7709028124809265, 0.7725483179092407, 0.8449530601501465, 0.616449773311615, 0.7529852986335754, 0.7428526878356934, 0.7558305263519287, 0.8335225582122803, 0.8114389181137085, 1.0263360738754272, 0.720385730266571, 0.8262020945549011, 0.9603818655014038, 0.6849712133407593, 0.8912324905395508, 1.0308833122253418, 0.8958356976509094, 0.8007423877716064, 0.809155285358429, 0.8432257771492004, 1.0421137809753418, 0.9008339643478394, 0.9137608408927917, 0.9869116544723511]
[2025-05-28 02:25:55,163]: Mean: 0.85922635
[2025-05-28 02:25:55,163]: Min: 0.61644977
[2025-05-28 02:25:55,163]: Max: 1.10569501
[2025-05-28 02:25:55,164]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,164]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11340412497520447, 0.11340412497520447, 0.0, 0.11340412497520447, -0.11340412497520447, 0.0, 0.0, 0.0, 0.11340412497520447, 0.11340412497520447, 0.11340412497520447, 0.0, 0.0, 0.0, -0.11340412497520447, 0.0, 0.0, 0.11340412497520447, 0.11340412497520447]
[2025-05-28 02:25:55,164]: Mean: 0.00051682
[2025-05-28 02:25:55,164]: Min: -0.34021237
[2025-05-28 02:25:55,165]: Max: 0.45361650
[2025-05-28 02:25:55,165]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,165]: Sample Values (25 elements): [0.7375550270080566, 0.560502827167511, 0.6952448487281799, 0.569843590259552, 0.527451753616333, 0.7165865898132324, 0.6981063485145569, 0.7479019165039062, 0.6249059438705444, 0.6784762144088745, 0.5198546051979065, 0.3292802572250366, 0.4815060794353485, 0.6760161519050598, 0.6936061382293701, 0.5625332593917847, 0.6223956942558289, 0.8161596655845642, 0.8413675427436829, 0.6564188599586487, 0.695729672908783, 0.6097182631492615, 0.735832691192627, 0.7635492086410522, 0.6755493879318237]
[2025-05-28 02:25:55,165]: Mean: 0.65745527
[2025-05-28 02:25:55,165]: Min: 0.32928026
[2025-05-28 02:25:55,165]: Max: 0.84136754
[2025-05-28 02:25:55,166]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,167]: Sample Values (25 elements): [0.0, 0.23387211561203003, 0.11693605780601501, -0.11693605780601501, -0.11693605780601501, 0.0, 0.0, -0.11693605780601501, 0.11693605780601501, 0.11693605780601501, 0.11693605780601501, 0.0, -0.11693605780601501, 0.23387211561203003, 0.11693605780601501, -0.11693605780601501, 0.0, 0.23387211561203003, -0.11693605780601501, -0.23387211561203003, -0.11693605780601501, -0.23387211561203003, 0.0, 0.0, 0.0]
[2025-05-28 02:25:55,167]: Mean: -0.00076130
[2025-05-28 02:25:55,167]: Min: -0.46774423
[2025-05-28 02:25:55,167]: Max: 0.35080817
[2025-05-28 02:25:55,167]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,167]: Sample Values (25 elements): [1.1849414110183716, 0.8634557723999023, 1.017977237701416, 1.0438498258590698, 1.1924022436141968, 0.9874029159545898, 0.8410245180130005, 1.1872915029525757, 0.8089337944984436, 0.6689110994338989, 0.8595011234283447, 0.8178206086158752, 1.012863039970398, 0.934752881526947, 1.1199678182601929, 0.8997020721435547, 0.8510603904724121, 0.9535173773765564, 0.9474779367446899, 1.0199027061462402, 1.1285815238952637, 1.0615979433059692, 0.8441683650016785, 0.8550753593444824, 0.9800628423690796]
[2025-05-28 02:25:55,167]: Mean: 0.97372025
[2025-05-28 02:25:55,168]: Min: 0.66891110
[2025-05-28 02:25:55,168]: Max: 1.23818147
[2025-05-28 02:25:55,169]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,169]: Sample Values (25 elements): [0.0, 0.0, -0.11747221648693085, 0.0, 0.0, 0.0, -0.11747221648693085, 0.0, 0.11747221648693085, 0.0, 0.0, -0.11747221648693085, -0.11747221648693085, -0.2349444329738617, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11747221648693085, 0.11747221648693085, -0.11747221648693085, 0.0, 0.0, 0.0]
[2025-05-28 02:25:55,169]: Mean: -0.00224339
[2025-05-28 02:25:55,169]: Min: -0.35241663
[2025-05-28 02:25:55,170]: Max: 0.46988887
[2025-05-28 02:25:55,170]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,170]: Sample Values (25 elements): [0.8575956225395203, 0.6526444554328918, 0.6931635737419128, 0.6915566921234131, 0.9363316297531128, 0.5908023118972778, 0.7786667346954346, 0.8015625476837158, 0.7710341215133667, 0.7877042889595032, 0.6194020509719849, 0.6301397085189819, 0.6293533444404602, 0.6140267252922058, 0.627839207649231, 0.6654895544052124, 0.7506932616233826, 0.6863690614700317, 0.5953887104988098, 0.5656394362449646, 0.7173781394958496, 0.7457585334777832, 0.8060193657875061, 0.528866708278656, 0.6853885054588318]
[2025-05-28 02:25:55,170]: Mean: 0.69510823
[2025-05-28 02:25:55,170]: Min: 0.52886671
[2025-05-28 02:25:55,170]: Max: 0.93633163
[2025-05-28 02:25:55,171]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-28 02:25:55,172]: Sample Values (25 elements): [-0.11753649264574051, -0.11753649264574051, 0.0, 0.0, 0.0, 0.0, 0.11753649264574051, 0.0, 0.0, -0.11753649264574051, 0.0, 0.0, 0.11753649264574051, -0.11753649264574051, 0.0, -0.11753649264574051, 0.0, -0.11753649264574051, 0.11753649264574051, -0.11753649264574051, 0.0, -0.11753649264574051, -0.11753649264574051, 0.0, 0.11753649264574051]
[2025-05-28 02:25:55,172]: Mean: -0.00265273
[2025-05-28 02:25:55,172]: Min: -0.47014597
[2025-05-28 02:25:55,172]: Max: 0.35260949
[2025-05-28 02:25:55,172]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,173]: Sample Values (25 elements): [0.9624865651130676, 0.5734809637069702, 0.9499053359031677, 1.0155253410339355, 0.8466586470603943, 0.501490592956543, 0.8444669246673584, 0.696139395236969, 0.7943884134292603, 0.9215104579925537, 0.8017493486404419, 0.8844654560089111, 0.8418026566505432, 0.6337157487869263, 1.046267032623291, 0.7903183102607727, 0.7215834259986877, 0.6641829013824463, 0.7388055324554443, 0.6468116044998169, 0.949694037437439, 0.8856381177902222, 0.8403341770172119, 0.8547196388244629, 0.9926234483718872]
[2025-05-28 02:25:55,173]: Mean: 0.78026688
[2025-05-28 02:25:55,173]: Min: 0.03086193
[2025-05-28 02:25:55,173]: Max: 1.04626703
[2025-05-28 02:25:55,174]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,175]: Sample Values (25 elements): [-0.22860854864120483, 0.0, 0.11430427432060242, -0.11430427432060242, 0.0, 0.0, 0.0, 0.0, -0.11430427432060242, 0.0, 0.0, 0.11430427432060242, -0.11430427432060242, -0.11430427432060242, 0.0, 0.0, 0.11430427432060242, 0.0, -0.11430427432060242, 0.11430427432060242, 0.0, 0.0, 0.0, 0.0, -0.11430427432060242]
[2025-05-28 02:25:55,175]: Mean: 0.00078138
[2025-05-28 02:25:55,175]: Min: -0.34291282
[2025-05-28 02:25:55,175]: Max: 0.45721710
[2025-05-28 02:25:55,175]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,175]: Sample Values (25 elements): [0.888995349407196, 0.7995696663856506, 0.7765844464302063, 0.7600758075714111, 0.5325523018836975, 0.7105078101158142, 0.7978779673576355, 0.7861396074295044, 0.7247591018676758, 0.718950092792511, 0.9695340394973755, 0.6795875430107117, 0.7829974293708801, 0.8956905007362366, 0.6893831491470337, 0.5480595827102661, 0.8487086892127991, 0.6883662343025208, 0.7975068688392639, 0.7804485559463501, 0.9070018529891968, 0.6935685276985168, 0.7908574342727661, 0.7159339189529419, 0.8925947546958923]
[2025-05-28 02:25:55,176]: Mean: 0.78231573
[2025-05-28 02:25:55,176]: Min: 0.53255230
[2025-05-28 02:25:55,176]: Max: 0.99538207
[2025-05-28 02:25:55,177]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-28 02:25:55,177]: Sample Values (25 elements): [-0.12955528497695923, 0.12955528497695923, 0.0, 0.0, 0.25911056995391846, 0.0, 0.0, 0.12955528497695923, 0.0, 0.0, 0.0, 0.0, -0.12955528497695923, 0.25911056995391846, 0.12955528497695923, -0.12955528497695923, -0.25911056995391846, 0.0, 0.12955528497695923, 0.0, 0.0, -0.12955528497695923, 0.0, -0.12955528497695923, 0.0]
[2025-05-28 02:25:55,177]: Mean: 0.00145497
[2025-05-28 02:25:55,177]: Min: -0.38866585
[2025-05-28 02:25:55,178]: Max: 0.38866585
[2025-05-28 02:25:55,178]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,178]: Sample Values (25 elements): [0.4765385389328003, 0.39690035581588745, 0.46569254994392395, 0.41462570428848267, 0.479436993598938, 0.3787315785884857, 0.42883747816085815, 0.7115005850791931, 0.43111175298690796, 0.4306192100048065, 0.6373619437217712, 0.5293716788291931, 0.5551925897598267, 0.7558866143226624, 0.486683189868927, 0.4413706958293915, 0.4907870292663574, 0.5340591669082642, 0.4255567491054535, 0.5430988669395447, 0.5235261917114258, 0.6796747446060181, 0.3630857467651367, 0.5076599717140198, 0.62054044008255]
[2025-05-28 02:25:55,178]: Mean: 0.48930529
[2025-05-28 02:25:55,178]: Min: 0.24191412
[2025-05-28 02:25:55,178]: Max: 0.75588661
[2025-05-28 02:25:55,179]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,180]: Sample Values (25 elements): [0.0, 0.0, -0.12534011900424957, 0.0, 0.0, 0.0, 0.0, -0.12534011900424957, 0.0, 0.0, 0.0, 0.0, -0.12534011900424957, 0.0, 0.0, 0.0, 0.12534011900424957, -0.12534011900424957, 0.12534011900424957, 0.0, -0.12534011900424957, 0.0, 0.12534011900424957, 0.0, -0.12534011900424957]
[2025-05-28 02:25:55,180]: Mean: -0.00062221
[2025-05-28 02:25:55,180]: Min: -0.50136048
[2025-05-28 02:25:55,181]: Max: 0.37602037
[2025-05-28 02:25:55,181]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,181]: Sample Values (25 elements): [1.0593513250350952, 0.6552159786224365, 0.8491916060447693, 0.7790395617485046, 0.931839644908905, 0.8494516611099243, 1.0417896509170532, 0.9749351143836975, 0.9808566570281982, 0.9611501097679138, 0.850631594657898, 0.9375057816505432, 0.18294458091259003, 0.7249884009361267, 1.0827621221542358, 1.0396740436553955, 1.0800104141235352, 0.7824307680130005, 0.9322637915611267, 0.8078113198280334, 0.8082084059715271, 0.8556857109069824, 0.8572257161140442, 0.9928857684135437, 1.0725829601287842]
[2025-05-28 02:25:55,181]: Mean: 0.92450035
[2025-05-28 02:25:55,181]: Min: 0.18294458
[2025-05-28 02:25:55,181]: Max: 1.26355469
[2025-05-28 02:25:55,182]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,183]: Sample Values (25 elements): [0.10522673279047012, 0.0, -0.10522673279047012, 0.10522673279047012, -0.10522673279047012, 0.0, 0.0, 0.0, 0.0, -0.10522673279047012, 0.0, -0.10522673279047012, 0.0, 0.10522673279047012, -0.10522673279047012, -0.10522673279047012, -0.10522673279047012, -0.21045346558094025, -0.10522673279047012, 0.0, 0.10522673279047012, 0.21045346558094025, 0.10522673279047012, 0.0, 0.0]
[2025-05-28 02:25:55,183]: Mean: -0.00003711
[2025-05-28 02:25:55,183]: Min: -0.42090693
[2025-05-28 02:25:55,183]: Max: 0.31568021
[2025-05-28 02:25:55,183]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,184]: Sample Values (25 elements): [0.8212926983833313, 0.7827681303024292, 0.7129367589950562, 0.9520220756530762, 0.7122600674629211, 0.76169753074646, 1.1493176221847534, 0.8803352117538452, 0.5816574096679688, 0.9595238566398621, 0.9569537043571472, 0.870025098323822, 0.7614462375640869, 0.8515357375144958, 0.942300021648407, 0.7436751127243042, 0.969732940196991, 0.8419330716133118, 0.8790997862815857, 1.0611913204193115, 0.9287850260734558, 0.48612725734710693, 0.9236570596694946, 0.8374102115631104, 0.9967639446258545]
[2025-05-28 02:25:55,184]: Mean: 0.84833229
[2025-05-28 02:25:55,184]: Min: 0.48612726
[2025-05-28 02:25:55,184]: Max: 1.14931762
[2025-05-28 02:25:55,185]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,186]: Sample Values (25 elements): [0.0, -0.10624626278877258, 0.0, 0.10624626278877258, -0.10624626278877258, 0.0, -0.10624626278877258, -0.10624626278877258, 0.0, 0.10624626278877258, 0.0, -0.10624626278877258, 0.0, 0.0, 0.10624626278877258, 0.10624626278877258, -0.10624626278877258, 0.0, 0.0, 0.10624626278877258, -0.10624626278877258, 0.0, 0.0, 0.0, -0.10624626278877258]
[2025-05-28 02:25:55,186]: Mean: -0.00101450
[2025-05-28 02:25:55,186]: Min: -0.31873879
[2025-05-28 02:25:55,186]: Max: 0.42498505
[2025-05-28 02:25:55,186]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,186]: Sample Values (25 elements): [1.1327742338180542, 1.2138915061950684, 1.0868273973464966, 0.8810564279556274, 0.7926161885261536, 5.080547712256057e-41, 1.1502816677093506, 1.144344449043274, 0.0022012919653207064, 0.8081487417221069, 0.8236158490180969, 1.0470889806747437, 0.7267525792121887, 0.6352288126945496, 5.993914051302973e-41, 0.8316540122032166, 1.1178882122039795, 0.5895190238952637, 0.776425838470459, 0.8358080983161926, 1.0093201398849487, 1.0910314321517944, 0.8837454915046692, 1.065826654434204, 1.002544641494751]
[2025-05-28 02:25:55,186]: Mean: 0.85025811
[2025-05-28 02:25:55,187]: Min: 0.00000000
[2025-05-28 02:25:55,187]: Max: 1.33133101
[2025-05-28 02:25:55,188]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,188]: Sample Values (25 elements): [0.0, 0.0, 0.0783577710390091, 0.0, -0.0783577710390091, 0.0, 0.0, -0.0783577710390091, 0.0, -0.0783577710390091, 0.0783577710390091, 0.0783577710390091, 0.0783577710390091, 0.0, 0.0, 0.0783577710390091, -0.0783577710390091, 0.0, 0.0, 0.0783577710390091, -0.0783577710390091, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 02:25:55,189]: Mean: -0.00082260
[2025-05-28 02:25:55,189]: Min: -0.23507331
[2025-05-28 02:25:55,189]: Max: 0.31343108
[2025-05-28 02:25:55,189]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,190]: Sample Values (25 elements): [1.1113182306289673, 0.9165340662002563, 0.9587891697883606, 1.0070017576217651, 1.0005155801773071, 0.9557517170906067, 1.0123579502105713, 0.9278162717819214, 1.065739393234253, 0.8902961611747742, 1.0196563005447388, 0.9145594239234924, 1.0145611763000488, 1.05124032497406, 0.9783657193183899, 0.9154394865036011, 0.9328198432922363, 0.8857706785202026, 1.0707321166992188, 1.0104278326034546, 1.0870904922485352, 1.0048884153366089, 1.188826322555542, 0.8093259930610657, 1.2791123390197754]
[2025-05-28 02:25:55,190]: Mean: 0.99818385
[2025-05-28 02:25:55,190]: Min: 0.80932599
[2025-05-28 02:25:55,190]: Max: 1.27911234
[2025-05-28 02:25:55,190]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-28 02:25:55,191]: Sample Values (25 elements): [0.644254744052887, -0.4218088686466217, 0.23435530066490173, 0.08130688965320587, 0.19336001574993134, 0.40748897194862366, 0.638317346572876, -0.2617209851741791, 0.2630358338356018, -0.4138370752334595, -0.2356204241514206, -0.01589428074657917, 0.36018654704093933, 0.3097194731235504, -0.18278810381889343, -0.18749020993709564, 0.09316593408584595, -0.16930772364139557, -0.5754337906837463, 0.43946129083633423, -0.32419711351394653, 0.6749430298805237, -0.2676546275615692, 0.23565654456615448, 0.1459595113992691]
[2025-05-28 02:25:55,191]: Mean: -0.00658744
[2025-05-28 02:25:55,191]: Min: -1.00892770
[2025-05-28 02:25:55,191]: Max: 1.18766701
[2025-05-28 02:25:55,191]: 


QAT of ResNet20 with hardtanh down to 2 bits...
[2025-05-28 02:25:55,306]: [ResNet20_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-28 02:25:55,355]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-28 02:26:47,747]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.0570 Train Acc: 0.6300 Eval Loss: 1.0449 Eval Acc: 0.6474 (LR: 0.00100000)
[2025-05-28 02:27:40,142]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.8882 Train Acc: 0.6873 Eval Loss: 1.3002 Eval Acc: 0.6029 (LR: 0.00100000)
[2025-05-28 02:28:32,717]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.8440 Train Acc: 0.7015 Eval Loss: 1.0149 Eval Acc: 0.6665 (LR: 0.00100000)
[2025-05-28 02:29:27,664]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.8309 Train Acc: 0.7076 Eval Loss: 1.0107 Eval Acc: 0.6617 (LR: 0.00100000)
[2025-05-28 02:30:23,186]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.8204 Train Acc: 0.7124 Eval Loss: 0.8725 Eval Acc: 0.7081 (LR: 0.00100000)
[2025-05-28 02:31:15,565]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.8131 Train Acc: 0.7147 Eval Loss: 1.0346 Eval Acc: 0.6657 (LR: 0.00100000)
[2025-05-28 02:32:07,930]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.8003 Train Acc: 0.7195 Eval Loss: 0.9620 Eval Acc: 0.6712 (LR: 0.00100000)
[2025-05-28 02:33:00,307]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.7953 Train Acc: 0.7213 Eval Loss: 0.9718 Eval Acc: 0.6744 (LR: 0.00100000)
[2025-05-28 02:33:52,821]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.7891 Train Acc: 0.7209 Eval Loss: 1.0783 Eval Acc: 0.6485 (LR: 0.00100000)
[2025-05-28 02:34:45,206]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.7863 Train Acc: 0.7266 Eval Loss: 0.9467 Eval Acc: 0.6793 (LR: 0.00100000)
[2025-05-28 02:35:37,535]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.7843 Train Acc: 0.7250 Eval Loss: 1.0066 Eval Acc: 0.6643 (LR: 0.00010000)
[2025-05-28 02:36:29,743]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.6869 Train Acc: 0.7605 Eval Loss: 0.7047 Eval Acc: 0.7601 (LR: 0.00010000)
[2025-05-28 02:37:22,102]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.6589 Train Acc: 0.7710 Eval Loss: 0.7016 Eval Acc: 0.7671 (LR: 0.00010000)
[2025-05-28 02:38:14,466]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.6557 Train Acc: 0.7708 Eval Loss: 0.6835 Eval Acc: 0.7649 (LR: 0.00010000)
[2025-05-28 02:39:06,802]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.6494 Train Acc: 0.7734 Eval Loss: 0.6451 Eval Acc: 0.7783 (LR: 0.00010000)
[2025-05-28 02:39:59,177]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.6474 Train Acc: 0.7730 Eval Loss: 0.6982 Eval Acc: 0.7579 (LR: 0.00010000)
[2025-05-28 02:40:58,117]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.6451 Train Acc: 0.7758 Eval Loss: 0.6664 Eval Acc: 0.7719 (LR: 0.00010000)
[2025-05-28 02:41:55,636]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.6502 Train Acc: 0.7743 Eval Loss: 0.6918 Eval Acc: 0.7631 (LR: 0.00010000)
[2025-05-28 02:42:50,241]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.6438 Train Acc: 0.7741 Eval Loss: 0.6437 Eval Acc: 0.7794 (LR: 0.00010000)
[2025-05-28 02:43:45,302]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.6477 Train Acc: 0.7728 Eval Loss: 0.6778 Eval Acc: 0.7711 (LR: 0.00010000)
[2025-05-28 02:44:37,924]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.6485 Train Acc: 0.7734 Eval Loss: 0.7192 Eval Acc: 0.7610 (LR: 0.00010000)
[2025-05-28 02:45:30,466]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.6432 Train Acc: 0.7767 Eval Loss: 0.7029 Eval Acc: 0.7621 (LR: 0.00010000)
[2025-05-28 02:46:23,004]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.6448 Train Acc: 0.7744 Eval Loss: 0.7012 Eval Acc: 0.7663 (LR: 0.00010000)
[2025-05-28 02:47:15,583]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.6449 Train Acc: 0.7747 Eval Loss: 0.6637 Eval Acc: 0.7749 (LR: 0.00010000)
[2025-05-28 02:48:08,145]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.6446 Train Acc: 0.7755 Eval Loss: 0.6782 Eval Acc: 0.7672 (LR: 0.00001000)
[2025-05-28 02:49:00,725]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.6124 Train Acc: 0.7858 Eval Loss: 0.6056 Eval Acc: 0.7915 (LR: 0.00001000)
[2025-05-28 02:49:53,334]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.5998 Train Acc: 0.7920 Eval Loss: 0.6066 Eval Acc: 0.7866 (LR: 0.00001000)
[2025-05-28 02:50:45,839]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.5973 Train Acc: 0.7911 Eval Loss: 0.6020 Eval Acc: 0.7947 (LR: 0.00001000)
[2025-05-28 02:51:38,201]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.5986 Train Acc: 0.7905 Eval Loss: 0.6108 Eval Acc: 0.7923 (LR: 0.00001000)
[2025-05-28 02:52:30,395]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.5949 Train Acc: 0.7937 Eval Loss: 0.6122 Eval Acc: 0.7881 (LR: 0.00001000)
[2025-05-28 02:53:22,530]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.6016 Train Acc: 0.7898 Eval Loss: 0.5965 Eval Acc: 0.7938 (LR: 0.00001000)
[2025-05-28 02:54:14,874]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.5972 Train Acc: 0.7924 Eval Loss: 0.6545 Eval Acc: 0.7806 (LR: 0.00001000)
[2025-05-28 02:55:07,321]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.5976 Train Acc: 0.7929 Eval Loss: 0.5987 Eval Acc: 0.7936 (LR: 0.00001000)
[2025-05-28 02:55:59,634]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.5956 Train Acc: 0.7928 Eval Loss: 0.6137 Eval Acc: 0.7894 (LR: 0.00001000)
[2025-05-28 02:56:52,009]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.5985 Train Acc: 0.7909 Eval Loss: 0.6133 Eval Acc: 0.7888 (LR: 0.00001000)
[2025-05-28 02:57:44,327]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.5988 Train Acc: 0.7906 Eval Loss: 0.5951 Eval Acc: 0.7929 (LR: 0.00001000)
[2025-05-28 02:58:36,735]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.6038 Train Acc: 0.7877 Eval Loss: 0.6508 Eval Acc: 0.7777 (LR: 0.00001000)
[2025-05-28 02:59:32,331]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.5970 Train Acc: 0.7906 Eval Loss: 0.6165 Eval Acc: 0.7886 (LR: 0.00001000)
[2025-05-28 03:00:28,577]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.5995 Train Acc: 0.7912 Eval Loss: 0.6310 Eval Acc: 0.7828 (LR: 0.00001000)
[2025-05-28 03:01:24,148]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.6025 Train Acc: 0.7893 Eval Loss: 0.6211 Eval Acc: 0.7872 (LR: 0.00001000)
[2025-05-28 03:02:17,435]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.6025 Train Acc: 0.7895 Eval Loss: 0.6229 Eval Acc: 0.7864 (LR: 0.00001000)
[2025-05-28 03:03:11,510]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.6002 Train Acc: 0.7918 Eval Loss: 0.6047 Eval Acc: 0.7935 (LR: 0.00000100)
[2025-05-28 03:04:05,890]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.5853 Train Acc: 0.7956 Eval Loss: 0.5870 Eval Acc: 0.7969 (LR: 0.00000100)
[2025-05-28 03:05:01,833]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.5804 Train Acc: 0.7988 Eval Loss: 0.5882 Eval Acc: 0.7966 (LR: 0.00000100)
[2025-05-28 03:05:56,448]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.5823 Train Acc: 0.7967 Eval Loss: 0.6117 Eval Acc: 0.7921 (LR: 0.00000100)
[2025-05-28 03:06:56,859]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.5845 Train Acc: 0.7953 Eval Loss: 0.5992 Eval Acc: 0.7963 (LR: 0.00000100)
[2025-05-28 03:07:54,402]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.5830 Train Acc: 0.7967 Eval Loss: 0.5840 Eval Acc: 0.7974 (LR: 0.00000100)
[2025-05-28 03:09:01,122]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.5797 Train Acc: 0.7981 Eval Loss: 0.5831 Eval Acc: 0.7939 (LR: 0.00000100)
[2025-05-28 03:10:01,448]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.5821 Train Acc: 0.7959 Eval Loss: 0.6006 Eval Acc: 0.7926 (LR: 0.00000100)
[2025-05-28 03:10:56,347]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.5811 Train Acc: 0.7968 Eval Loss: 0.5863 Eval Acc: 0.7991 (LR: 0.00000100)
[2025-05-28 03:11:50,366]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.5882 Train Acc: 0.7948 Eval Loss: 0.5912 Eval Acc: 0.7980 (LR: 0.00000100)
[2025-05-28 03:12:47,154]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.5840 Train Acc: 0.7961 Eval Loss: 0.5858 Eval Acc: 0.8011 (LR: 0.00000100)
[2025-05-28 03:13:45,283]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.5879 Train Acc: 0.7944 Eval Loss: 0.6005 Eval Acc: 0.7928 (LR: 0.00000100)
[2025-05-28 03:14:41,349]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.5843 Train Acc: 0.7949 Eval Loss: 0.5939 Eval Acc: 0.7968 (LR: 0.00000010)
[2025-05-28 03:15:35,414]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.5779 Train Acc: 0.7985 Eval Loss: 0.5817 Eval Acc: 0.8029 (LR: 0.00000010)
[2025-05-28 03:16:31,501]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.5681 Train Acc: 0.8017 Eval Loss: 0.5784 Eval Acc: 0.8057 (LR: 0.00000010)
[2025-05-28 03:17:25,985]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.5805 Train Acc: 0.7977 Eval Loss: 0.5764 Eval Acc: 0.8022 (LR: 0.00000010)
[2025-05-28 03:18:20,671]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.5718 Train Acc: 0.8009 Eval Loss: 0.5978 Eval Acc: 0.7987 (LR: 0.00000010)
[2025-05-28 03:19:17,065]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.5716 Train Acc: 0.7997 Eval Loss: 0.5837 Eval Acc: 0.7964 (LR: 0.00000010)
[2025-05-28 03:20:17,585]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.5755 Train Acc: 0.7991 Eval Loss: 0.5815 Eval Acc: 0.7997 (LR: 0.00000010)
[2025-05-28 03:20:17,585]: [ResNet20_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8057
[2025-05-28 03:20:17,633]: 


Quantization of model down to 2 bits finished
[2025-05-28 03:20:17,633]: Model Architecture:
[2025-05-28 03:20:17,689]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4851], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7205580472946167, max_val=0.734716534614563)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3761], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5641546249389648, max_val=0.56415855884552)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3553], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5524898767471313, max_val=0.5133013725280762)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3613], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5033931732177734, max_val=0.5805621147155762)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4159], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6896791458129883, max_val=0.5579859018325806)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4119], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5354851484298706, max_val=0.7001368999481201)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3441], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5161285400390625, max_val=0.5161341428756714)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3198], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48208779096603394, max_val=0.4773271679878235)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3792], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5687311887741089, max_val=0.5687358379364014)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2763], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4143645763397217, max_val=0.4143887758255005)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2827], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4226089119911194, max_val=0.42536211013793945)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2863], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43009859323501587, max_val=0.42883139848709106)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2892], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43375295400619507, max_val=0.433752179145813)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2713], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40710073709487915, max_val=0.4068604111671448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3138], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4671328663825989, max_val=0.47417473793029785)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3358], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5037088394165039, max_val=0.5037050247192383)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2929], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4650043249130249, max_val=0.41377002000808716)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2414], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37476640939712524, max_val=0.3495364785194397)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2410], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3689987063407898, max_val=0.35411983728408813)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1918], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28538620471954346, max_val=0.2899147868156433)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-28 03:20:17,689]: 
Model Weights:
[2025-05-28 03:20:17,689]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-28 03:20:17,690]: Sample Values (25 elements): [0.26088252663612366, -0.018119078129529953, -0.07160340249538422, 0.1019570380449295, -0.22229944169521332, -0.06831502169370651, -0.2891807556152344, -0.008580866269767284, -0.14420408010482788, 0.12773102521896362, -0.16464143991470337, -0.06430812925100327, 0.02213216759264469, -0.3375847637653351, 0.015473133884370327, 0.0658787190914154, 0.07014276832342148, -0.24969349801540375, 0.020067935809493065, -0.049569450318813324, -0.20269101858139038, -0.03743064031004906, -0.06896166503429413, -0.1963791847229004, 0.09982821345329285]
[2025-05-28 03:20:17,690]: Mean: 0.00060594
[2025-05-28 03:20:17,690]: Min: -0.53880769
[2025-05-28 03:20:17,690]: Max: 0.41461828
[2025-05-28 03:20:17,690]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,691]: Sample Values (16 elements): [0.9742780923843384, 1.3543182611465454, 1.025742530822754, 1.0738520622253418, 2.0337352752685547, 1.0459359884262085, 1.0260727405548096, 1.2618181705474854, 1.0639171600341797, 1.2116073369979858, 1.1903884410858154, 1.0183357000350952, 1.1216580867767334, 1.1581043004989624, 1.2351171970367432, 1.3250654935836792]
[2025-05-28 03:20:17,691]: Mean: 1.19499660
[2025-05-28 03:20:17,691]: Min: 0.97427809
[2025-05-28 03:20:17,691]: Max: 2.03373528
[2025-05-28 03:20:17,693]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 03:20:17,693]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.48509153723716736, 0.0, 0.0, 0.0, 0.0, 0.48509153723716736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48509153723716736, 0.0]
[2025-05-28 03:20:17,693]: Mean: 0.00042109
[2025-05-28 03:20:17,694]: Min: -0.48509154
[2025-05-28 03:20:17,694]: Max: 0.97018307
[2025-05-28 03:20:17,694]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,694]: Sample Values (16 elements): [1.1530884504318237, 0.8512058258056641, 0.7918034195899963, 0.6777244210243225, 0.9279040694236755, 0.7360314726829529, 0.9701733589172363, 0.8940212726593018, 0.5928334593772888, 0.8900293707847595, 0.7999462485313416, 0.9877285957336426, 0.8929016590118408, 1.0097923278808594, 0.8467249870300293, 1.0097805261611938]
[2025-05-28 03:20:17,694]: Mean: 0.87698054
[2025-05-28 03:20:17,695]: Min: 0.59283346
[2025-05-28 03:20:17,695]: Max: 1.15308845
[2025-05-28 03:20:17,696]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 03:20:17,696]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.3761044144630432, 0.0, 0.0, 0.0, 0.0, -0.3761044144630432, 0.0, 0.0, 0.3761044144630432, 0.0, 0.0, 0.0, 0.0, 0.3761044144630432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,697]: Mean: 0.00603987
[2025-05-28 03:20:17,697]: Min: -0.37610441
[2025-05-28 03:20:17,697]: Max: 0.75220883
[2025-05-28 03:20:17,697]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,697]: Sample Values (16 elements): [0.6565384864807129, 0.8345956206321716, 0.5147429704666138, 0.7390639185905457, 0.6601319909095764, 0.8557490110397339, 0.44435128569602966, 0.7237035632133484, 0.7888414263725281, 0.527068555355072, 0.872574508190155, 0.5002356171607971, 1.0854774713516235, 0.6340820789337158, 0.7112991809844971, 0.771996021270752]
[2025-05-28 03:20:17,698]: Mean: 0.70752823
[2025-05-28 03:20:17,698]: Min: 0.44435129
[2025-05-28 03:20:17,698]: Max: 1.08547747
[2025-05-28 03:20:17,700]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 03:20:17,700]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.35526376962661743, 0.0, 0.0, 0.35526376962661743, 0.0, 0.0, 0.0, 0.0, 0.0, -0.35526376962661743, 0.0, -0.35526376962661743, -0.35526376962661743, 0.0, 0.0, 0.0, 0.0, 0.35526376962661743, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,700]: Mean: -0.00632197
[2025-05-28 03:20:17,701]: Min: -0.71052754
[2025-05-28 03:20:17,701]: Max: 0.35526377
[2025-05-28 03:20:17,701]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,701]: Sample Values (16 elements): [0.7836899757385254, 0.737565815448761, 1.0552210807800293, 0.9587256908416748, 0.8453034162521362, 0.9538276791572571, 0.8613192439079285, 0.7061148881912231, 0.7935039401054382, 0.9383644461631775, 1.0529086589813232, 1.0359035730361938, 0.6897061467170715, 0.9440649151802063, 0.9684206247329712, 0.8328909277915955]
[2025-05-28 03:20:17,701]: Mean: 0.88484573
[2025-05-28 03:20:17,702]: Min: 0.68970615
[2025-05-28 03:20:17,702]: Max: 1.05522108
[2025-05-28 03:20:17,703]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 03:20:17,703]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.361318439245224, -0.361318439245224, 0.361318439245224, 0.0, -0.361318439245224, 0.0, 0.0, 0.0, -0.361318439245224, -0.361318439245224, 0.0, 0.0, 0.0, 0.0, 0.0, -0.361318439245224, 0.0, 0.0, 0.0, 0.361318439245224, 0.361318439245224, 0.0]
[2025-05-28 03:20:17,704]: Mean: -0.00109776
[2025-05-28 03:20:17,704]: Min: -0.36131844
[2025-05-28 03:20:17,704]: Max: 0.72263688
[2025-05-28 03:20:17,704]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,704]: Sample Values (16 elements): [0.6160545945167542, 0.5102027058601379, 0.7752965688705444, 0.687491238117218, 0.40038982033729553, 1.1780447959899902, 0.8188787698745728, 0.5789657831192017, 0.5677548050880432, 0.7000464200973511, 0.662180483341217, 0.6808182597160339, 0.3875318169593811, 0.4731864929199219, 0.8460072875022888, 0.7032327055931091]
[2025-05-28 03:20:17,705]: Mean: 0.66163015
[2025-05-28 03:20:17,705]: Min: 0.38753182
[2025-05-28 03:20:17,705]: Max: 1.17804480
[2025-05-28 03:20:17,706]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 03:20:17,707]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.41588836908340454, -0.41588836908340454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41588836908340454, -0.41588836908340454, 0.0, 0.0, -0.41588836908340454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41588836908340454]
[2025-05-28 03:20:17,707]: Mean: -0.00451268
[2025-05-28 03:20:17,707]: Min: -0.83177674
[2025-05-28 03:20:17,707]: Max: 0.41588837
[2025-05-28 03:20:17,708]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,708]: Sample Values (16 elements): [1.1205817461013794, 0.8605762124061584, 0.963782548904419, 0.8802133798599243, 0.8490746021270752, 1.007507562637329, 0.7782711386680603, 0.8722544312477112, 0.8089200258255005, 0.82266765832901, 0.8370177745819092, 0.7536553144454956, 0.7653711438179016, 1.0823407173156738, 0.9427277445793152, 0.7968989014625549]
[2025-05-28 03:20:17,708]: Mean: 0.88386631
[2025-05-28 03:20:17,708]: Min: 0.75365531
[2025-05-28 03:20:17,708]: Max: 1.12058175
[2025-05-28 03:20:17,709]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 03:20:17,710]: Sample Values (25 elements): [0.41187402606010437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41187402606010437, 0.41187402606010437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.41187402606010437, -0.41187402606010437, -0.41187402606010437, 0.0, 0.41187402606010437, 0.0, 0.0]
[2025-05-28 03:20:17,710]: Mean: 0.00107259
[2025-05-28 03:20:17,710]: Min: -0.41187403
[2025-05-28 03:20:17,710]: Max: 0.82374805
[2025-05-28 03:20:17,710]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-28 03:20:17,711]: Sample Values (16 elements): [0.7086930274963379, 0.9833898544311523, 0.7123962640762329, 0.5810571312904358, 0.5817826390266418, 0.4634619355201721, 0.6789636611938477, 0.8097521066665649, 0.43765750527381897, 0.6623520851135254, 0.6886700987815857, 0.8390066623687744, 0.8413040041923523, 0.7479035258293152, 0.4168068766593933, 0.7683112621307373]
[2025-05-28 03:20:17,711]: Mean: 0.68259430
[2025-05-28 03:20:17,711]: Min: 0.41680688
[2025-05-28 03:20:17,711]: Max: 0.98338985
[2025-05-28 03:20:17,712]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-28 03:20:17,713]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3440875709056854, 0.0, 0.3440875709056854, -0.3440875709056854, 0.3440875709056854, 0.0, 0.0, 0.0, 0.3440875709056854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,713]: Mean: 0.00403228
[2025-05-28 03:20:17,713]: Min: -0.34408757
[2025-05-28 03:20:17,713]: Max: 0.68817514
[2025-05-28 03:20:17,713]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,714]: Sample Values (25 elements): [0.6942645907402039, 0.7977513670921326, 0.9759038686752319, 0.8046412467956543, 0.701641321182251, 0.7990477085113525, 0.8446565270423889, 0.8184966444969177, 0.6974496245384216, 0.9024884700775146, 0.9595326781272888, 0.8501246571540833, 0.7419718503952026, 0.9543066620826721, 0.8078853487968445, 0.8121962547302246, 0.6977166533470154, 0.9846393465995789, 0.7115758061408997, 0.7706501483917236, 0.7616389393806458, 0.7031654119491577, 0.8221858143806458, 0.8860979676246643, 0.802191972732544]
[2025-05-28 03:20:17,714]: Mean: 0.80957568
[2025-05-28 03:20:17,714]: Min: 0.68893868
[2025-05-28 03:20:17,714]: Max: 0.98463935
[2025-05-28 03:20:17,715]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 03:20:17,716]: Sample Values (25 elements): [0.0, 0.31980499625205994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.31980499625205994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31980499625205994, 0.0, 0.31980499625205994, 0.0, 0.31980499625205994, -0.31980499625205994]
[2025-05-28 03:20:17,716]: Mean: 0.00156155
[2025-05-28 03:20:17,716]: Min: -0.63960999
[2025-05-28 03:20:17,716]: Max: 0.31980500
[2025-05-28 03:20:17,716]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,716]: Sample Values (25 elements): [0.9111282825469971, 0.7324476838111877, 0.8131417036056519, 0.7094683647155762, 0.8540157675743103, 0.7917959094047546, 0.7332010865211487, 0.7222696542739868, 0.828619122505188, 0.9015600085258484, 0.7079848051071167, 0.6763367056846619, 0.7927482724189758, 0.7401716113090515, 0.8975740671157837, 0.6056933999061584, 0.8368151187896729, 0.8247376680374146, 0.7030626535415649, 0.8519348502159119, 0.7449027299880981, 0.723422110080719, 0.7199428081512451, 0.7403810024261475, 0.6772724390029907]
[2025-05-28 03:20:17,717]: Mean: 0.77040541
[2025-05-28 03:20:17,717]: Min: 0.59589946
[2025-05-28 03:20:17,717]: Max: 1.03243077
[2025-05-28 03:20:17,718]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-28 03:20:17,718]: Sample Values (25 elements): [0.379155695438385, 0.0, 0.0, 0.0, 0.379155695438385, 0.0, 0.0, 0.0, 0.0, 0.75831139087677, 0.379155695438385, 0.0, -0.379155695438385, 0.0, 0.379155695438385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.379155695438385, 0.379155695438385]
[2025-05-28 03:20:17,719]: Mean: 0.01555131
[2025-05-28 03:20:17,719]: Min: -0.37915570
[2025-05-28 03:20:17,719]: Max: 0.75831139
[2025-05-28 03:20:17,719]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,720]: Sample Values (25 elements): [0.2952617406845093, 0.43068814277648926, 0.38970646262168884, 0.400372177362442, 0.4452435076236725, 0.3906525671482086, 0.4051908254623413, 0.16025148332118988, 0.4604950249195099, 0.584598958492279, 0.37497052550315857, 0.39637094736099243, 0.4448961615562439, 0.3684294819831848, 0.4305439293384552, 0.5749386548995972, 0.487113893032074, 0.2629838287830353, 0.35767409205436707, 0.24957165122032166, 0.4664318859577179, 0.25571832060813904, 0.26845017075538635, 0.22157996892929077, 0.5017665028572083]
[2025-05-28 03:20:17,720]: Mean: 0.38774222
[2025-05-28 03:20:17,720]: Min: 0.16025148
[2025-05-28 03:20:17,721]: Max: 0.58459896
[2025-05-28 03:20:17,722]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 03:20:17,722]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2762511372566223, 0.0, 0.0, 0.0, 0.2762511372566223, 0.0, 0.0, 0.0, 0.0, -0.2762511372566223, 0.0, -0.2762511372566223, 0.2762511372566223, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,723]: Mean: 0.00368695
[2025-05-28 03:20:17,723]: Min: -0.27625114
[2025-05-28 03:20:17,723]: Max: 0.55250227
[2025-05-28 03:20:17,723]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,724]: Sample Values (25 elements): [0.9278214573860168, 0.751173198223114, 0.749713122844696, 0.8931928277015686, 0.983524739742279, 0.8612440228462219, 1.0149457454681396, 0.9675891399383545, 0.7648167610168457, 0.8019767999649048, 0.7266814708709717, 0.8209233283996582, 1.1154392957687378, 0.793950080871582, 0.7532897591590881, 0.954493522644043, 0.9362090826034546, 0.8095797896385193, 0.8760526776313782, 0.8042587637901306, 0.8749783635139465, 0.8283088207244873, 0.7525383234024048, 0.8535431027412415, 0.6822329163551331]
[2025-05-28 03:20:17,724]: Mean: 0.84875774
[2025-05-28 03:20:17,724]: Min: 0.68223292
[2025-05-28 03:20:17,725]: Max: 1.11543930
[2025-05-28 03:20:17,726]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 03:20:17,727]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.28265702724456787, 0.0, 0.0, 0.0, 0.0, 0.28265702724456787, -0.28265702724456787, 0.0, 0.0, 0.28265702724456787, 0.0, 0.28265702724456787, 0.0, 0.0, -0.28265702724456787, 0.0, 0.0, 0.28265702724456787, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,727]: Mean: 0.00144150
[2025-05-28 03:20:17,728]: Min: -0.28265703
[2025-05-28 03:20:17,728]: Max: 0.56531405
[2025-05-28 03:20:17,728]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,729]: Sample Values (25 elements): [0.7635889053344727, 0.727310836315155, 0.7557267546653748, 0.547036349773407, 0.7999176979064941, 0.28158947825431824, 0.6685187220573425, 0.7392148375511169, 0.7683736085891724, 0.7386782765388489, 0.870373547077179, 0.6206279993057251, 0.6323264241218567, 0.5898392200469971, 0.6303021907806396, 0.7690261006355286, 0.7468978762626648, 0.7400938868522644, 0.7947078943252563, 0.6221864223480225, 0.6868722438812256, 0.543245255947113, 0.6110413670539856, 0.9033419489860535, 0.7337372303009033]
[2025-05-28 03:20:17,729]: Mean: 0.70434737
[2025-05-28 03:20:17,729]: Min: 0.28158948
[2025-05-28 03:20:17,729]: Max: 0.96325213
[2025-05-28 03:20:17,730]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 03:20:17,731]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.28631001710891724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.28631001710891724, 0.0, -0.28631001710891724]
[2025-05-28 03:20:17,731]: Mean: -0.00133586
[2025-05-28 03:20:17,731]: Min: -0.57262003
[2025-05-28 03:20:17,731]: Max: 0.28631002
[2025-05-28 03:20:17,731]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,732]: Sample Values (25 elements): [1.0991138219833374, 0.9880059957504272, 1.1909985542297363, 0.9778245091438293, 1.1989487409591675, 0.7888019680976868, 0.8738195300102234, 0.9824878573417664, 1.0057724714279175, 0.7678120136260986, 0.8709720373153687, 0.8492860198020935, 1.1764487028121948, 0.8932703733444214, 0.9286688566207886, 0.9935970902442932, 0.6628158092498779, 0.9360024333000183, 0.9357982277870178, 0.9597386121749878, 1.0116034746170044, 0.7997187376022339, 0.9166990518569946, 0.9811529517173767, 0.9422853589057922]
[2025-05-28 03:20:17,732]: Mean: 0.95049953
[2025-05-28 03:20:17,732]: Min: 0.66281581
[2025-05-28 03:20:17,732]: Max: 1.19894874
[2025-05-28 03:20:17,734]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 03:20:17,735]: Sample Values (25 elements): [0.0, -0.2891683876514435, 0.0, 0.0, 0.0, -0.2891683876514435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2891683876514435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2891683876514435, -0.2891683876514435, -0.2891683876514435]
[2025-05-28 03:20:17,735]: Mean: -0.00423587
[2025-05-28 03:20:17,735]: Min: -0.28916839
[2025-05-28 03:20:17,735]: Max: 0.28916839
[2025-05-28 03:20:17,735]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-28 03:20:17,735]: Sample Values (25 elements): [0.7680487036705017, 0.8458757996559143, 0.7715505957603455, 0.7911171913146973, 0.6199227571487427, 0.6728548407554626, 0.6813622117042542, 0.7106121778488159, 0.8729782104492188, 0.772200882434845, 0.5981513857841492, 0.6612172722816467, 0.8129412531852722, 0.8923853039741516, 0.7307056188583374, 0.7146726250648499, 0.6412687301635742, 0.8829347491264343, 0.5900352597236633, 0.7449188828468323, 0.6713544130325317, 0.8071072101593018, 0.6909164786338806, 0.726120114326477, 0.7155565023422241]
[2025-05-28 03:20:17,736]: Mean: 0.72163689
[2025-05-28 03:20:17,736]: Min: 0.55714256
[2025-05-28 03:20:17,736]: Max: 0.94480634
[2025-05-28 03:20:17,737]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-28 03:20:17,738]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.2713204026222229, -0.2713204026222229, 0.0, 0.0, 0.0, 0.0, -0.2713204026222229, 0.0, 0.0, 0.0, 0.2713204026222229, 0.0, 0.2713204026222229, -0.2713204026222229, 0.0, 0.0, 0.0, 0.0, 0.2713204026222229, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,738]: Mean: -0.00111873
[2025-05-28 03:20:17,738]: Min: -0.54264081
[2025-05-28 03:20:17,738]: Max: 0.27132040
[2025-05-28 03:20:17,738]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,738]: Sample Values (25 elements): [0.8674991130828857, 0.7187908291816711, 0.849435567855835, 0.8297317624092102, 0.6996648907661438, 0.8110858798027039, 0.8559404611587524, 0.9520589113235474, 1.0200583934783936, 0.8271054029464722, 0.7734571695327759, 0.8123235702514648, 0.9193576574325562, -5.016087982897115e-41, 1.030789852142334, 0.8497825264930725, 0.7904304265975952, 0.8021615743637085, 0.8807435035705566, 0.5237835645675659, 0.5597469806671143, 0.8885607123374939, 0.9136165380477905, 0.6314594745635986, 0.8033458590507507]
[2025-05-28 03:20:17,739]: Mean: 0.79458445
[2025-05-28 03:20:17,739]: Min: -0.00000000
[2025-05-28 03:20:17,739]: Max: 1.07199383
[2025-05-28 03:20:17,741]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 03:20:17,742]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31376922130584717, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.31376922130584717]
[2025-05-28 03:20:17,743]: Mean: -0.00062985
[2025-05-28 03:20:17,743]: Min: -0.31376922
[2025-05-28 03:20:17,743]: Max: 0.62753844
[2025-05-28 03:20:17,743]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,744]: Sample Values (25 elements): [0.765037477016449, 0.7970475554466248, 0.8083771467208862, 0.7962774038314819, 0.7792121767997742, 0.8084792494773865, 0.90000319480896, 0.6933847665786743, 0.6204037070274353, 0.7449374198913574, 0.6802950501441956, 0.7943242788314819, 0.82566237449646, 0.7337960004806519, 0.8757035136222839, 0.6938302516937256, 0.8884010910987854, 0.7250521779060364, 0.6770308017730713, 0.8318043351173401, 0.6242954730987549, 0.6810304522514343, 0.6949785947799683, 0.8660507202148438, 0.8417463898658752]
[2025-05-28 03:20:17,744]: Mean: 0.76966906
[2025-05-28 03:20:17,744]: Min: 0.55862498
[2025-05-28 03:20:17,744]: Max: 0.99816942
[2025-05-28 03:20:17,746]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-28 03:20:17,746]: Sample Values (25 elements): [0.33580464124679565, 0.0, 0.0, 0.0, 0.33580464124679565, 0.0, 0.0, 0.0, 0.0, 0.33580464124679565, 0.0, 0.33580464124679565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33580464124679565, 0.0, 0.0, 0.33580464124679565, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,747]: Mean: 0.00245951
[2025-05-28 03:20:17,747]: Min: -0.33580464
[2025-05-28 03:20:17,747]: Max: 0.33580464
[2025-05-28 03:20:17,748]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,748]: Sample Values (25 elements): [0.4998289942741394, 0.6072049140930176, 0.34505629539489746, 0.5046223998069763, 0.3012200593948364, 0.44608381390571594, 0.5121236443519592, 0.48196718096733093, 0.41021642088890076, 0.333130419254303, 0.3974948227405548, 0.5552773475646973, 0.727526843547821, 0.4200174808502197, 0.622869610786438, 0.6294432282447815, 0.46364665031433105, 0.5656529664993286, 0.31079205870628357, 0.456903338432312, 0.5887514352798462, 0.5348052978515625, 0.42630553245544434, 0.4493049085140228, 0.39472952485084534]
[2025-05-28 03:20:17,748]: Mean: 0.46284240
[2025-05-28 03:20:17,749]: Min: 0.23484439
[2025-05-28 03:20:17,749]: Max: 0.72752684
[2025-05-28 03:20:17,750]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 03:20:17,751]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29292479157447815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,751]: Mean: -0.00039730
[2025-05-28 03:20:17,752]: Min: -0.58584958
[2025-05-28 03:20:17,752]: Max: 0.29292479
[2025-05-28 03:20:17,752]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,752]: Sample Values (25 elements): [1.0674470663070679, 1.004652500152588, 0.8210289478302002, 1.0017006397247314, 0.9276474118232727, 0.9268904328346252, 1.1315054893493652, 0.49874037504196167, 0.9766461253166199, 0.7575670480728149, 1.0646721124649048, 0.9497199654579163, 0.7747195959091187, 1.0778799057006836, 0.7651938796043396, 0.24370482563972473, 0.8148209452629089, 1.0678552389144897, 1.0231959819793701, 1.0341746807098389, 1.1553786993026733, 0.8138130307197571, 0.9409525990486145, 4.925564102101732e-41, 1.1876513957977295]
[2025-05-28 03:20:17,753]: Mean: 0.88694310
[2025-05-28 03:20:17,753]: Min: 0.00000000
[2025-05-28 03:20:17,753]: Max: 1.23067379
[2025-05-28 03:20:17,755]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 03:20:17,756]: Sample Values (25 elements): [0.0, -0.24143430590629578, 0.0, 0.0, 0.0, 0.0, 0.24143430590629578, 0.0, 0.0, 0.0, 0.24143430590629578, 0.0, 0.0, 0.0, -0.24143430590629578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,756]: Mean: 0.00039951
[2025-05-28 03:20:17,756]: Min: -0.48286861
[2025-05-28 03:20:17,756]: Max: 0.24143431
[2025-05-28 03:20:17,757]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,757]: Sample Values (25 elements): [0.8201987147331238, 0.8949007391929626, 0.7241681814193726, 0.7944005727767944, 1.092560052871704, 0.8951107859611511, 0.7256624102592468, 0.8393974900245667, 0.4967222511768341, 0.6439622044563293, 0.8273346424102783, 0.8958738446235657, 0.967459499835968, 0.8591598272323608, 0.8470463752746582, 0.8535400032997131, 0.8881548047065735, 0.7919098734855652, 1.0800542831420898, 0.925597071647644, 0.8566142916679382, 1.029543399810791, 0.8280571103096008, 0.8060069680213928, 0.42999184131622314]
[2025-05-28 03:20:17,757]: Mean: 0.85963941
[2025-05-28 03:20:17,757]: Min: 0.42999184
[2025-05-28 03:20:17,758]: Max: 1.18170309
[2025-05-28 03:20:17,759]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 03:20:17,760]: Sample Values (25 elements): [-0.24103951454162598, 0.0, 0.0, -0.24103951454162598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24103951454162598, 0.0, 0.0, 0.0, -0.24103951454162598, 0.0, -0.24103951454162598]
[2025-05-28 03:20:17,760]: Mean: -0.00098079
[2025-05-28 03:20:17,760]: Min: -0.48207903
[2025-05-28 03:20:17,761]: Max: 0.24103951
[2025-05-28 03:20:17,761]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,761]: Sample Values (25 elements): [0.9900476336479187, 1.257172703742981, 0.7570404410362244, 0.8578441739082336, 0.33941054344177246, 0.9723919034004211, 0.996931254863739, 0.8122145533561707, 0.6789882183074951, 0.7561990022659302, 0.9054875373840332, 0.794812798500061, 0.7572816610336304, 0.9243310689926147, 0.9693729281425476, 1.0395395755767822, 0.8644344210624695, 0.9412987232208252, 0.7835652828216553, 0.8791770935058594, 0.7529282569885254, 0.9723745584487915, 0.7277759313583374, 0.9963737726211548, 0.9692572355270386]
[2025-05-28 03:20:17,762]: Mean: 0.82970226
[2025-05-28 03:20:17,762]: Min: -0.00000000
[2025-05-28 03:20:17,762]: Max: 1.35998082
[2025-05-28 03:20:17,764]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 03:20:17,765]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.19176700711250305, -0.19176700711250305, 0.0, 0.0, 0.0, 0.0, 0.19176700711250305, 0.0, 0.0, 0.0, 0.0, 0.0, -0.19176700711250305, 0.0, 0.0, -0.19176700711250305, 0.19176700711250305, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 03:20:17,765]: Mean: -0.00029651
[2025-05-28 03:20:17,765]: Min: -0.19176701
[2025-05-28 03:20:17,765]: Max: 0.38353401
[2025-05-28 03:20:17,765]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-28 03:20:17,766]: Sample Values (25 elements): [0.787352442741394, 0.9453858137130737, 0.9601491689682007, 1.0455894470214844, 0.9319577217102051, 1.0175708532333374, 1.1015946865081787, 0.9371146559715271, 0.9453375935554504, 0.8875648379325867, 0.9611401557922363, 0.88960862159729, 0.9075896739959717, 0.9547771215438843, 0.9859797358512878, 1.022555947303772, 0.9950098395347595, 0.9984300136566162, 0.9211510419845581, 0.9276853203773499, 1.0022451877593994, 1.06003999710083, 0.9428962469100952, 0.9134469032287598, 1.0539377927780151]
[2025-05-28 03:20:17,766]: Mean: 0.96834922
[2025-05-28 03:20:17,766]: Min: 0.75063533
[2025-05-28 03:20:17,766]: Max: 1.15200162
[2025-05-28 03:20:17,766]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-28 03:20:17,767]: Sample Values (25 elements): [0.08214631676673889, -0.40295183658599854, -0.15942329168319702, -0.38705092668533325, 0.45077216625213623, -0.4333467185497284, -0.2518016993999481, -0.02349221706390381, -0.1806660145521164, 0.268943190574646, 0.37146830558776855, -0.7513157725334167, -0.4607301950454712, 0.30538424849510193, 0.36402010917663574, 0.08173222839832306, -0.5694994926452637, -0.33825770020484924, -0.6392268538475037, -0.02898864820599556, 0.2681536078453064, 0.21554380655288696, 0.33396390080451965, -0.07664477080106735, -0.24070245027542114]
[2025-05-28 03:20:17,767]: Mean: -0.00720717
[2025-05-28 03:20:17,767]: Min: -0.97507918
[2025-05-28 03:20:17,767]: Max: 1.04101646
