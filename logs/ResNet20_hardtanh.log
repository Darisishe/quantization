[2025-05-04 05:28:33,777]: 
Training ResNet20 with hardtanh
[2025-05-04 05:29:46,541]: [ResNet20_hardtanh] Epoch: 001 Train Loss: 1.9894 Train Acc: 0.2569 Eval Loss: 1.8282 Eval Acc: 0.3348 (LR: 0.001000)
[2025-05-04 05:30:56,009]: [ResNet20_hardtanh] Epoch: 002 Train Loss: 1.7681 Train Acc: 0.3490 Eval Loss: 1.6633 Eval Acc: 0.3760 (LR: 0.001000)
[2025-05-04 05:32:07,483]: [ResNet20_hardtanh] Epoch: 003 Train Loss: 1.6389 Train Acc: 0.3980 Eval Loss: 1.5507 Eval Acc: 0.4252 (LR: 0.001000)
[2025-05-04 05:33:15,466]: [ResNet20_hardtanh] Epoch: 004 Train Loss: 1.5374 Train Acc: 0.4355 Eval Loss: 1.4722 Eval Acc: 0.4605 (LR: 0.001000)
[2025-05-04 05:34:24,773]: [ResNet20_hardtanh] Epoch: 005 Train Loss: 1.4519 Train Acc: 0.4716 Eval Loss: 1.3994 Eval Acc: 0.4886 (LR: 0.001000)
[2025-05-04 05:35:33,706]: [ResNet20_hardtanh] Epoch: 006 Train Loss: 1.3882 Train Acc: 0.4957 Eval Loss: 1.4483 Eval Acc: 0.4724 (LR: 0.001000)
[2025-05-04 05:36:41,628]: [ResNet20_hardtanh] Epoch: 007 Train Loss: 1.3312 Train Acc: 0.5207 Eval Loss: 1.2654 Eval Acc: 0.5373 (LR: 0.001000)
[2025-05-04 05:37:50,339]: [ResNet20_hardtanh] Epoch: 008 Train Loss: 1.2854 Train Acc: 0.5359 Eval Loss: 1.2290 Eval Acc: 0.5535 (LR: 0.001000)
[2025-05-04 05:38:58,694]: [ResNet20_hardtanh] Epoch: 009 Train Loss: 1.2419 Train Acc: 0.5528 Eval Loss: 1.2116 Eval Acc: 0.5539 (LR: 0.001000)
[2025-05-04 05:40:08,392]: [ResNet20_hardtanh] Epoch: 010 Train Loss: 1.2073 Train Acc: 0.5655 Eval Loss: 1.2008 Eval Acc: 0.5604 (LR: 0.001000)
[2025-05-04 05:41:18,022]: [ResNet20_hardtanh] Epoch: 011 Train Loss: 1.1777 Train Acc: 0.5800 Eval Loss: 1.1246 Eval Acc: 0.5984 (LR: 0.001000)
[2025-05-04 05:42:27,848]: [ResNet20_hardtanh] Epoch: 012 Train Loss: 1.1434 Train Acc: 0.5912 Eval Loss: 1.1999 Eval Acc: 0.5668 (LR: 0.001000)
[2025-05-04 05:43:39,770]: [ResNet20_hardtanh] Epoch: 013 Train Loss: 1.1237 Train Acc: 0.5986 Eval Loss: 1.0999 Eval Acc: 0.6078 (LR: 0.001000)
[2025-05-04 05:44:49,652]: [ResNet20_hardtanh] Epoch: 014 Train Loss: 1.0927 Train Acc: 0.6093 Eval Loss: 1.0829 Eval Acc: 0.6091 (LR: 0.001000)
[2025-05-04 05:45:59,387]: [ResNet20_hardtanh] Epoch: 015 Train Loss: 1.0715 Train Acc: 0.6183 Eval Loss: 1.0677 Eval Acc: 0.6233 (LR: 0.001000)
[2025-05-04 05:47:09,364]: [ResNet20_hardtanh] Epoch: 016 Train Loss: 1.0493 Train Acc: 0.6245 Eval Loss: 1.0096 Eval Acc: 0.6383 (LR: 0.001000)
[2025-05-04 05:48:18,293]: [ResNet20_hardtanh] Epoch: 017 Train Loss: 1.0305 Train Acc: 0.6331 Eval Loss: 1.0070 Eval Acc: 0.6431 (LR: 0.001000)
[2025-05-04 05:49:29,039]: [ResNet20_hardtanh] Epoch: 018 Train Loss: 1.0092 Train Acc: 0.6407 Eval Loss: 0.9804 Eval Acc: 0.6576 (LR: 0.001000)
[2025-05-04 05:50:39,204]: [ResNet20_hardtanh] Epoch: 019 Train Loss: 0.9933 Train Acc: 0.6471 Eval Loss: 0.9568 Eval Acc: 0.6622 (LR: 0.001000)
[2025-05-04 05:51:49,735]: [ResNet20_hardtanh] Epoch: 020 Train Loss: 0.9757 Train Acc: 0.6542 Eval Loss: 0.9624 Eval Acc: 0.6601 (LR: 0.001000)
[2025-05-04 05:53:00,694]: [ResNet20_hardtanh] Epoch: 021 Train Loss: 0.9578 Train Acc: 0.6597 Eval Loss: 0.9798 Eval Acc: 0.6554 (LR: 0.001000)
[2025-05-04 05:54:12,229]: [ResNet20_hardtanh] Epoch: 022 Train Loss: 0.9425 Train Acc: 0.6660 Eval Loss: 0.9129 Eval Acc: 0.6775 (LR: 0.001000)
[2025-05-04 05:55:24,328]: [ResNet20_hardtanh] Epoch: 023 Train Loss: 0.9293 Train Acc: 0.6707 Eval Loss: 0.9154 Eval Acc: 0.6752 (LR: 0.001000)
[2025-05-04 05:56:33,887]: [ResNet20_hardtanh] Epoch: 024 Train Loss: 0.9136 Train Acc: 0.6760 Eval Loss: 0.9078 Eval Acc: 0.6808 (LR: 0.001000)
[2025-05-04 05:57:43,464]: [ResNet20_hardtanh] Epoch: 025 Train Loss: 0.8993 Train Acc: 0.6809 Eval Loss: 0.8855 Eval Acc: 0.6855 (LR: 0.001000)
[2025-05-04 05:58:57,165]: [ResNet20_hardtanh] Epoch: 026 Train Loss: 0.8892 Train Acc: 0.6861 Eval Loss: 0.8872 Eval Acc: 0.6897 (LR: 0.001000)
[2025-05-04 06:00:07,680]: [ResNet20_hardtanh] Epoch: 027 Train Loss: 0.8762 Train Acc: 0.6901 Eval Loss: 0.8608 Eval Acc: 0.6965 (LR: 0.001000)
[2025-05-04 06:01:15,902]: [ResNet20_hardtanh] Epoch: 028 Train Loss: 0.8655 Train Acc: 0.6940 Eval Loss: 0.8218 Eval Acc: 0.7119 (LR: 0.001000)
[2025-05-04 06:02:25,289]: [ResNet20_hardtanh] Epoch: 029 Train Loss: 0.8554 Train Acc: 0.6971 Eval Loss: 0.8420 Eval Acc: 0.7034 (LR: 0.001000)
[2025-05-04 06:03:32,585]: [ResNet20_hardtanh] Epoch: 030 Train Loss: 0.8398 Train Acc: 0.7029 Eval Loss: 0.8261 Eval Acc: 0.7109 (LR: 0.001000)
[2025-05-04 06:04:36,377]: [ResNet20_hardtanh] Epoch: 031 Train Loss: 0.8355 Train Acc: 0.7032 Eval Loss: 0.8195 Eval Acc: 0.7132 (LR: 0.001000)
[2025-05-04 06:05:37,804]: [ResNet20_hardtanh] Epoch: 032 Train Loss: 0.8276 Train Acc: 0.7078 Eval Loss: 0.8211 Eval Acc: 0.7084 (LR: 0.001000)
[2025-05-04 06:06:38,579]: [ResNet20_hardtanh] Epoch: 033 Train Loss: 0.8197 Train Acc: 0.7106 Eval Loss: 0.8093 Eval Acc: 0.7138 (LR: 0.001000)
[2025-05-04 06:07:45,659]: [ResNet20_hardtanh] Epoch: 034 Train Loss: 0.8037 Train Acc: 0.7157 Eval Loss: 0.8072 Eval Acc: 0.7142 (LR: 0.001000)
[2025-05-04 06:08:47,914]: [ResNet20_hardtanh] Epoch: 035 Train Loss: 0.7934 Train Acc: 0.7205 Eval Loss: 0.7895 Eval Acc: 0.7253 (LR: 0.001000)
[2025-05-04 06:09:55,397]: [ResNet20_hardtanh] Epoch: 036 Train Loss: 0.7925 Train Acc: 0.7212 Eval Loss: 0.7991 Eval Acc: 0.7222 (LR: 0.001000)
[2025-05-04 06:10:55,175]: [ResNet20_hardtanh] Epoch: 037 Train Loss: 0.7794 Train Acc: 0.7238 Eval Loss: 0.8003 Eval Acc: 0.7212 (LR: 0.001000)
[2025-05-04 06:11:54,683]: [ResNet20_hardtanh] Epoch: 038 Train Loss: 0.7702 Train Acc: 0.7283 Eval Loss: 0.7786 Eval Acc: 0.7259 (LR: 0.001000)
[2025-05-04 06:12:54,939]: [ResNet20_hardtanh] Epoch: 039 Train Loss: 0.7647 Train Acc: 0.7292 Eval Loss: 0.7889 Eval Acc: 0.7260 (LR: 0.001000)
[2025-05-04 06:13:54,479]: [ResNet20_hardtanh] Epoch: 040 Train Loss: 0.7577 Train Acc: 0.7331 Eval Loss: 0.7888 Eval Acc: 0.7209 (LR: 0.001000)
[2025-05-04 06:14:54,011]: [ResNet20_hardtanh] Epoch: 041 Train Loss: 0.7524 Train Acc: 0.7338 Eval Loss: 0.8079 Eval Acc: 0.7248 (LR: 0.001000)
[2025-05-04 06:15:53,348]: [ResNet20_hardtanh] Epoch: 042 Train Loss: 0.7407 Train Acc: 0.7392 Eval Loss: 0.8057 Eval Acc: 0.7230 (LR: 0.001000)
[2025-05-04 06:16:57,196]: [ResNet20_hardtanh] Epoch: 043 Train Loss: 0.7321 Train Acc: 0.7445 Eval Loss: 0.7254 Eval Acc: 0.7438 (LR: 0.001000)
[2025-05-04 06:18:02,434]: [ResNet20_hardtanh] Epoch: 044 Train Loss: 0.7289 Train Acc: 0.7431 Eval Loss: 0.7198 Eval Acc: 0.7463 (LR: 0.001000)
[2025-05-04 06:19:06,449]: [ResNet20_hardtanh] Epoch: 045 Train Loss: 0.7213 Train Acc: 0.7479 Eval Loss: 0.7488 Eval Acc: 0.7356 (LR: 0.001000)
[2025-05-04 06:20:09,827]: [ResNet20_hardtanh] Epoch: 046 Train Loss: 0.7165 Train Acc: 0.7493 Eval Loss: 0.7189 Eval Acc: 0.7525 (LR: 0.001000)
[2025-05-04 06:21:10,317]: [ResNet20_hardtanh] Epoch: 047 Train Loss: 0.7052 Train Acc: 0.7536 Eval Loss: 0.7729 Eval Acc: 0.7290 (LR: 0.001000)
[2025-05-04 06:22:10,160]: [ResNet20_hardtanh] Epoch: 048 Train Loss: 0.6969 Train Acc: 0.7543 Eval Loss: 0.7080 Eval Acc: 0.7512 (LR: 0.001000)
[2025-05-04 06:23:17,070]: [ResNet20_hardtanh] Epoch: 049 Train Loss: 0.6963 Train Acc: 0.7540 Eval Loss: 0.7430 Eval Acc: 0.7390 (LR: 0.001000)
[2025-05-04 06:24:18,588]: [ResNet20_hardtanh] Epoch: 050 Train Loss: 0.6921 Train Acc: 0.7557 Eval Loss: 0.7698 Eval Acc: 0.7333 (LR: 0.001000)
[2025-05-04 06:25:18,663]: [ResNet20_hardtanh] Epoch: 051 Train Loss: 0.6852 Train Acc: 0.7591 Eval Loss: 0.7000 Eval Acc: 0.7544 (LR: 0.001000)
[2025-05-04 06:26:19,172]: [ResNet20_hardtanh] Epoch: 052 Train Loss: 0.6811 Train Acc: 0.7601 Eval Loss: 0.7131 Eval Acc: 0.7551 (LR: 0.001000)
[2025-05-04 06:27:19,922]: [ResNet20_hardtanh] Epoch: 053 Train Loss: 0.6740 Train Acc: 0.7637 Eval Loss: 0.6925 Eval Acc: 0.7553 (LR: 0.001000)
[2025-05-04 06:28:21,958]: [ResNet20_hardtanh] Epoch: 054 Train Loss: 0.6639 Train Acc: 0.7675 Eval Loss: 0.7236 Eval Acc: 0.7513 (LR: 0.001000)
[2025-05-04 06:29:31,781]: [ResNet20_hardtanh] Epoch: 055 Train Loss: 0.6612 Train Acc: 0.7695 Eval Loss: 0.6842 Eval Acc: 0.7634 (LR: 0.001000)
[2025-05-04 06:30:43,523]: [ResNet20_hardtanh] Epoch: 056 Train Loss: 0.6585 Train Acc: 0.7680 Eval Loss: 0.6991 Eval Acc: 0.7605 (LR: 0.001000)
[2025-05-04 06:31:54,395]: [ResNet20_hardtanh] Epoch: 057 Train Loss: 0.6518 Train Acc: 0.7717 Eval Loss: 0.6633 Eval Acc: 0.7694 (LR: 0.001000)
[2025-05-04 06:33:05,152]: [ResNet20_hardtanh] Epoch: 058 Train Loss: 0.6424 Train Acc: 0.7760 Eval Loss: 0.6542 Eval Acc: 0.7704 (LR: 0.001000)
[2025-05-04 06:34:16,560]: [ResNet20_hardtanh] Epoch: 059 Train Loss: 0.6444 Train Acc: 0.7745 Eval Loss: 0.6565 Eval Acc: 0.7729 (LR: 0.001000)
[2025-05-04 06:35:27,827]: [ResNet20_hardtanh] Epoch: 060 Train Loss: 0.6358 Train Acc: 0.7774 Eval Loss: 0.6464 Eval Acc: 0.7765 (LR: 0.001000)
[2025-05-04 06:36:39,327]: [ResNet20_hardtanh] Epoch: 061 Train Loss: 0.6267 Train Acc: 0.7794 Eval Loss: 0.6671 Eval Acc: 0.7687 (LR: 0.001000)
[2025-05-04 06:37:50,341]: [ResNet20_hardtanh] Epoch: 062 Train Loss: 0.6264 Train Acc: 0.7812 Eval Loss: 0.6434 Eval Acc: 0.7762 (LR: 0.001000)
[2025-05-04 06:39:00,873]: [ResNet20_hardtanh] Epoch: 063 Train Loss: 0.6185 Train Acc: 0.7858 Eval Loss: 0.6593 Eval Acc: 0.7750 (LR: 0.001000)
[2025-05-04 06:40:11,795]: [ResNet20_hardtanh] Epoch: 064 Train Loss: 0.6087 Train Acc: 0.7891 Eval Loss: 0.6255 Eval Acc: 0.7820 (LR: 0.001000)
[2025-05-04 06:41:22,940]: [ResNet20_hardtanh] Epoch: 065 Train Loss: 0.6147 Train Acc: 0.7849 Eval Loss: 0.6415 Eval Acc: 0.7782 (LR: 0.001000)
[2025-05-04 06:42:34,994]: [ResNet20_hardtanh] Epoch: 066 Train Loss: 0.6062 Train Acc: 0.7875 Eval Loss: 0.6662 Eval Acc: 0.7700 (LR: 0.001000)
[2025-05-04 06:43:46,334]: [ResNet20_hardtanh] Epoch: 067 Train Loss: 0.6017 Train Acc: 0.7893 Eval Loss: 0.6605 Eval Acc: 0.7765 (LR: 0.001000)
[2025-05-04 06:44:58,078]: [ResNet20_hardtanh] Epoch: 068 Train Loss: 0.5962 Train Acc: 0.7907 Eval Loss: 0.6551 Eval Acc: 0.7770 (LR: 0.001000)
[2025-05-04 06:46:11,471]: [ResNet20_hardtanh] Epoch: 069 Train Loss: 0.5896 Train Acc: 0.7922 Eval Loss: 0.6460 Eval Acc: 0.7797 (LR: 0.001000)
[2025-05-04 06:47:31,076]: [ResNet20_hardtanh] Epoch: 070 Train Loss: 0.5919 Train Acc: 0.7931 Eval Loss: 0.6391 Eval Acc: 0.7840 (LR: 0.000100)
[2025-05-04 06:48:40,755]: [ResNet20_hardtanh] Epoch: 071 Train Loss: 0.5328 Train Acc: 0.8131 Eval Loss: 0.5741 Eval Acc: 0.8033 (LR: 0.000100)
[2025-05-04 06:49:52,137]: [ResNet20_hardtanh] Epoch: 072 Train Loss: 0.5229 Train Acc: 0.8182 Eval Loss: 0.5719 Eval Acc: 0.8019 (LR: 0.000100)
[2025-05-04 06:51:03,534]: [ResNet20_hardtanh] Epoch: 073 Train Loss: 0.5226 Train Acc: 0.8196 Eval Loss: 0.5730 Eval Acc: 0.8039 (LR: 0.000100)
[2025-05-04 06:52:14,217]: [ResNet20_hardtanh] Epoch: 074 Train Loss: 0.5223 Train Acc: 0.8194 Eval Loss: 0.5677 Eval Acc: 0.8073 (LR: 0.000100)
[2025-05-04 06:53:25,948]: [ResNet20_hardtanh] Epoch: 075 Train Loss: 0.5185 Train Acc: 0.8199 Eval Loss: 0.5665 Eval Acc: 0.8076 (LR: 0.000100)
[2025-05-04 06:54:37,484]: [ResNet20_hardtanh] Epoch: 076 Train Loss: 0.5153 Train Acc: 0.8215 Eval Loss: 0.5666 Eval Acc: 0.8089 (LR: 0.000100)
[2025-05-04 06:55:48,841]: [ResNet20_hardtanh] Epoch: 077 Train Loss: 0.5173 Train Acc: 0.8214 Eval Loss: 0.5715 Eval Acc: 0.8038 (LR: 0.000100)
[2025-05-04 06:56:55,323]: [ResNet20_hardtanh] Epoch: 078 Train Loss: 0.5182 Train Acc: 0.8197 Eval Loss: 0.5694 Eval Acc: 0.8043 (LR: 0.000100)
[2025-05-04 06:58:02,335]: [ResNet20_hardtanh] Epoch: 079 Train Loss: 0.5132 Train Acc: 0.8200 Eval Loss: 0.5665 Eval Acc: 0.8044 (LR: 0.000100)
[2025-05-04 06:59:13,579]: [ResNet20_hardtanh] Epoch: 080 Train Loss: 0.5133 Train Acc: 0.8205 Eval Loss: 0.5623 Eval Acc: 0.8068 (LR: 0.000100)
[2025-05-04 07:00:25,041]: [ResNet20_hardtanh] Epoch: 081 Train Loss: 0.5091 Train Acc: 0.8233 Eval Loss: 0.5604 Eval Acc: 0.8096 (LR: 0.000100)
[2025-05-04 07:01:36,506]: [ResNet20_hardtanh] Epoch: 082 Train Loss: 0.5085 Train Acc: 0.8240 Eval Loss: 0.5638 Eval Acc: 0.8059 (LR: 0.000100)
[2025-05-04 07:02:47,746]: [ResNet20_hardtanh] Epoch: 083 Train Loss: 0.5081 Train Acc: 0.8237 Eval Loss: 0.5613 Eval Acc: 0.8060 (LR: 0.000100)
[2025-05-04 07:03:58,822]: [ResNet20_hardtanh] Epoch: 084 Train Loss: 0.5083 Train Acc: 0.8233 Eval Loss: 0.5647 Eval Acc: 0.8047 (LR: 0.000100)
[2025-05-04 07:05:02,593]: [ResNet20_hardtanh] Epoch: 085 Train Loss: 0.5059 Train Acc: 0.8249 Eval Loss: 0.5628 Eval Acc: 0.8069 (LR: 0.000100)
[2025-05-04 07:06:03,831]: [ResNet20_hardtanh] Epoch: 086 Train Loss: 0.5037 Train Acc: 0.8266 Eval Loss: 0.5640 Eval Acc: 0.8100 (LR: 0.000100)
[2025-05-04 07:07:14,162]: [ResNet20_hardtanh] Epoch: 087 Train Loss: 0.5033 Train Acc: 0.8246 Eval Loss: 0.5607 Eval Acc: 0.8070 (LR: 0.000100)
[2025-05-04 07:08:25,862]: [ResNet20_hardtanh] Epoch: 088 Train Loss: 0.5018 Train Acc: 0.8247 Eval Loss: 0.5635 Eval Acc: 0.8076 (LR: 0.000100)
[2025-05-04 07:09:36,470]: [ResNet20_hardtanh] Epoch: 089 Train Loss: 0.5045 Train Acc: 0.8240 Eval Loss: 0.5647 Eval Acc: 0.8078 (LR: 0.000100)
[2025-05-04 07:10:47,008]: [ResNet20_hardtanh] Epoch: 090 Train Loss: 0.5029 Train Acc: 0.8259 Eval Loss: 0.5558 Eval Acc: 0.8091 (LR: 0.000100)
[2025-05-04 07:11:58,228]: [ResNet20_hardtanh] Epoch: 091 Train Loss: 0.4997 Train Acc: 0.8268 Eval Loss: 0.5588 Eval Acc: 0.8091 (LR: 0.000100)
[2025-05-04 07:13:09,593]: [ResNet20_hardtanh] Epoch: 092 Train Loss: 0.5066 Train Acc: 0.8249 Eval Loss: 0.5624 Eval Acc: 0.8079 (LR: 0.000100)
[2025-05-04 07:14:21,005]: [ResNet20_hardtanh] Epoch: 093 Train Loss: 0.5017 Train Acc: 0.8264 Eval Loss: 0.5629 Eval Acc: 0.8078 (LR: 0.000100)
[2025-05-04 07:15:32,034]: [ResNet20_hardtanh] Epoch: 094 Train Loss: 0.5001 Train Acc: 0.8263 Eval Loss: 0.5620 Eval Acc: 0.8052 (LR: 0.000100)
[2025-05-04 07:16:43,347]: [ResNet20_hardtanh] Epoch: 095 Train Loss: 0.5005 Train Acc: 0.8271 Eval Loss: 0.5632 Eval Acc: 0.8076 (LR: 0.000100)
[2025-05-04 07:17:46,970]: [ResNet20_hardtanh] Epoch: 096 Train Loss: 0.5030 Train Acc: 0.8250 Eval Loss: 0.5564 Eval Acc: 0.8103 (LR: 0.000100)
[2025-05-04 07:18:50,431]: [ResNet20_hardtanh] Epoch: 097 Train Loss: 0.4986 Train Acc: 0.8282 Eval Loss: 0.5555 Eval Acc: 0.8096 (LR: 0.000100)
[2025-05-04 07:19:52,361]: [ResNet20_hardtanh] Epoch: 098 Train Loss: 0.4950 Train Acc: 0.8283 Eval Loss: 0.5598 Eval Acc: 0.8076 (LR: 0.000100)
[2025-05-04 07:20:54,813]: [ResNet20_hardtanh] Epoch: 099 Train Loss: 0.4993 Train Acc: 0.8273 Eval Loss: 0.5567 Eval Acc: 0.8091 (LR: 0.000100)
[2025-05-04 07:21:56,730]: [ResNet20_hardtanh] Epoch: 100 Train Loss: 0.4967 Train Acc: 0.8280 Eval Loss: 0.5685 Eval Acc: 0.8060 (LR: 0.000010)
[2025-05-04 07:22:58,628]: [ResNet20_hardtanh] Epoch: 101 Train Loss: 0.4924 Train Acc: 0.8284 Eval Loss: 0.5526 Eval Acc: 0.8105 (LR: 0.000010)
[2025-05-04 07:24:01,064]: [ResNet20_hardtanh] Epoch: 102 Train Loss: 0.4887 Train Acc: 0.8293 Eval Loss: 0.5559 Eval Acc: 0.8074 (LR: 0.000010)
[2025-05-04 07:25:03,241]: [ResNet20_hardtanh] Epoch: 103 Train Loss: 0.4879 Train Acc: 0.8316 Eval Loss: 0.5512 Eval Acc: 0.8117 (LR: 0.000010)
[2025-05-04 07:26:05,906]: [ResNet20_hardtanh] Epoch: 104 Train Loss: 0.4888 Train Acc: 0.8301 Eval Loss: 0.5551 Eval Acc: 0.8097 (LR: 0.000010)
[2025-05-04 07:27:06,224]: [ResNet20_hardtanh] Epoch: 105 Train Loss: 0.4879 Train Acc: 0.8291 Eval Loss: 0.5543 Eval Acc: 0.8100 (LR: 0.000010)
[2025-05-04 07:28:06,177]: [ResNet20_hardtanh] Epoch: 106 Train Loss: 0.4872 Train Acc: 0.8292 Eval Loss: 0.5524 Eval Acc: 0.8090 (LR: 0.000010)
[2025-05-04 07:29:02,871]: [ResNet20_hardtanh] Epoch: 107 Train Loss: 0.4864 Train Acc: 0.8332 Eval Loss: 0.5556 Eval Acc: 0.8098 (LR: 0.000010)
[2025-05-04 07:29:48,553]: [ResNet20_hardtanh] Epoch: 108 Train Loss: 0.4879 Train Acc: 0.8306 Eval Loss: 0.5522 Eval Acc: 0.8088 (LR: 0.000010)
[2025-05-04 07:30:33,585]: [ResNet20_hardtanh] Epoch: 109 Train Loss: 0.4846 Train Acc: 0.8313 Eval Loss: 0.5505 Eval Acc: 0.8089 (LR: 0.000010)
[2025-05-04 07:31:19,327]: [ResNet20_hardtanh] Epoch: 110 Train Loss: 0.4878 Train Acc: 0.8287 Eval Loss: 0.5498 Eval Acc: 0.8118 (LR: 0.000010)
[2025-05-04 07:32:04,675]: [ResNet20_hardtanh] Epoch: 111 Train Loss: 0.4830 Train Acc: 0.8326 Eval Loss: 0.5516 Eval Acc: 0.8107 (LR: 0.000010)
[2025-05-04 07:32:50,624]: [ResNet20_hardtanh] Epoch: 112 Train Loss: 0.4838 Train Acc: 0.8321 Eval Loss: 0.5534 Eval Acc: 0.8099 (LR: 0.000010)
[2025-05-04 07:33:37,058]: [ResNet20_hardtanh] Epoch: 113 Train Loss: 0.4857 Train Acc: 0.8316 Eval Loss: 0.5541 Eval Acc: 0.8100 (LR: 0.000010)
[2025-05-04 07:34:26,635]: [ResNet20_hardtanh] Epoch: 114 Train Loss: 0.4877 Train Acc: 0.8298 Eval Loss: 0.5520 Eval Acc: 0.8105 (LR: 0.000010)
[2025-05-04 07:35:12,111]: [ResNet20_hardtanh] Epoch: 115 Train Loss: 0.4848 Train Acc: 0.8317 Eval Loss: 0.5534 Eval Acc: 0.8095 (LR: 0.000010)
[2025-05-04 07:35:57,238]: [ResNet20_hardtanh] Epoch: 116 Train Loss: 0.4854 Train Acc: 0.8315 Eval Loss: 0.5527 Eval Acc: 0.8113 (LR: 0.000010)
[2025-05-04 07:36:38,743]: [ResNet20_hardtanh] Epoch: 117 Train Loss: 0.4849 Train Acc: 0.8325 Eval Loss: 0.5532 Eval Acc: 0.8092 (LR: 0.000010)
[2025-05-04 07:37:09,725]: [ResNet20_hardtanh] Epoch: 118 Train Loss: 0.4854 Train Acc: 0.8304 Eval Loss: 0.5545 Eval Acc: 0.8106 (LR: 0.000010)
[2025-05-04 07:37:39,636]: [ResNet20_hardtanh] Epoch: 119 Train Loss: 0.4882 Train Acc: 0.8307 Eval Loss: 0.5542 Eval Acc: 0.8106 (LR: 0.000010)
[2025-05-04 07:38:09,439]: [ResNet20_hardtanh] Epoch: 120 Train Loss: 0.4871 Train Acc: 0.8312 Eval Loss: 0.5489 Eval Acc: 0.8112 (LR: 0.000010)
[2025-05-04 07:38:09,476]: 
Training of full-precision model finished!
[2025-05-04 07:38:09,477]: Model Architecture:
[2025-05-04 07:38:09,478]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-04 07:38:09,479]: 
Model Parameters with Weights:
[2025-05-04 07:38:09,479]: 
Parameter: initial_layer.0.weight
[2025-05-04 07:38:09,479]: Shape: torch.Size([16, 3, 3, 3])
[2025-05-04 07:38:09,532]: Sample Values (16 elements): [0.11910194903612137, 0.08572356402873993, 0.09863922744989395, 0.014608236961066723, 0.0228639617562294, -0.17502254247665405, 0.2789076268672943, -0.2866423726081848, 0.05861940607428551, 0.006008229684084654, 0.29140493273735046, 0.025968628004193306, -0.10076332092285156, -0.16703666746616364, -0.09408342093229294, -0.10728638619184494]
[2025-05-04 07:38:09,542]: Mean: -0.0010
[2025-05-04 07:38:09,544]: Std: 0.1667
[2025-05-04 07:38:09,557]: Min: -0.4197
[2025-05-04 07:38:09,559]: Max: 0.3623
[2025-05-04 07:38:09,559]: 
Parameter: initial_layer.1.weight
[2025-05-04 07:38:09,559]: Shape: torch.Size([16])
[2025-05-04 07:38:09,561]: Sample Values (16 elements): [0.8448085784912109, 0.8935974836349487, 0.8241094350814819, 0.807264506816864, 0.668906033039093, 0.92348313331604, 0.9000720977783203, 0.7897629737854004, 0.8110707998275757, 1.003731608390808, 0.9056439399719238, 0.8267901539802551, 0.8877019286155701, 0.7009708285331726, 0.9112521409988403, 0.905559241771698]
[2025-05-04 07:38:09,561]: Mean: 0.8503
[2025-05-04 07:38:09,562]: Std: 0.0849
[2025-05-04 07:38:09,563]: Min: 0.6689
[2025-05-04 07:38:09,563]: Max: 1.0037
[2025-05-04 07:38:09,563]: 
Parameter: initial_layer.1.bias
[2025-05-04 07:38:09,564]: Shape: torch.Size([16])
[2025-05-04 07:38:09,565]: Sample Values (16 elements): [0.13334347307682037, -0.005246381741017103, -0.20504595339298248, -0.28562989830970764, 0.342777818441391, 0.35627833008766174, 0.17277419567108154, -0.5009250640869141, 0.23399348556995392, 0.7560157775878906, 0.054993461817502975, -0.5773707032203674, -0.36027541756629944, 0.03296956047415733, 0.12977005541324615, 0.035711467266082764]
[2025-05-04 07:38:09,565]: Mean: 0.0196
[2025-05-04 07:38:09,568]: Std: 0.3437
[2025-05-04 07:38:09,568]: Min: -0.5774
[2025-05-04 07:38:09,569]: Max: 0.7560
[2025-05-04 07:38:09,569]: 
Parameter: layer1.0.conv1.weight
[2025-05-04 07:38:09,569]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:38:09,570]: Sample Values (16 elements): [0.09604618698358536, 0.035153042525053024, -0.04099820926785469, 0.030563972890377045, 0.01732909493148327, 0.019023166969418526, 0.13274870812892914, -0.020478369668126106, -0.04503714665770531, -0.03203215077519417, 0.041065387427806854, -0.028595181182026863, 0.0598207451403141, 0.0364270955324173, 0.05642491579055786, 0.009279007092118263]
[2025-05-04 07:38:09,571]: Mean: -0.0004
[2025-05-04 07:38:09,572]: Std: 0.0687
[2025-05-04 07:38:09,572]: Min: -0.2231
[2025-05-04 07:38:09,573]: Max: 0.2570
[2025-05-04 07:38:09,573]: 
Parameter: layer1.0.bn1.weight
[2025-05-04 07:38:09,573]: Shape: torch.Size([16])
[2025-05-04 07:38:09,576]: Sample Values (16 elements): [0.971222996711731, 0.896797239780426, 1.0256867408752441, 0.9452223777770996, 1.0365856885910034, 0.89382004737854, 0.9395608901977539, 0.8210445046424866, 0.9862409234046936, 0.9525456428527832, 1.0146198272705078, 1.0624312162399292, 0.8874598145484924, 0.9721184372901917, 0.6924431920051575, 1.0548317432403564]
[2025-05-04 07:38:09,576]: Mean: 0.9470
[2025-05-04 07:38:09,577]: Std: 0.0953
[2025-05-04 07:38:09,578]: Min: 0.6924
[2025-05-04 07:38:09,579]: Max: 1.0624
[2025-05-04 07:38:09,579]: 
Parameter: layer1.0.bn1.bias
[2025-05-04 07:38:09,579]: Shape: torch.Size([16])
[2025-05-04 07:38:09,582]: Sample Values (16 elements): [-0.16031700372695923, -0.1200743243098259, -0.15851570665836334, -0.06801912933588028, -0.20527145266532898, -0.26254892349243164, -0.1470302790403366, -0.0964815616607666, -0.1349366009235382, -0.42221319675445557, -0.010031492449343204, -0.39748844504356384, 0.03321976587176323, 0.07783783972263336, 0.2432970553636551, -0.0550633929669857]
[2025-05-04 07:38:09,583]: Mean: -0.1177
[2025-05-04 07:38:09,584]: Std: 0.1661
[2025-05-04 07:38:09,584]: Min: -0.4222
[2025-05-04 07:38:09,585]: Max: 0.2433
[2025-05-04 07:38:09,585]: 
Parameter: layer1.0.conv2.weight
[2025-05-04 07:38:09,585]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:38:09,587]: Sample Values (16 elements): [-0.03702661767601967, -0.09238934516906738, 0.07511351257562637, -0.034953150898218155, -0.002853227313607931, 0.031341634690761566, 0.007723927032202482, 0.08779852837324142, -0.062089115381240845, -0.16601033508777618, -0.1228870302438736, 0.07618273049592972, 0.058205731213092804, -0.01891375333070755, 0.11969495564699173, 0.0045010969042778015]
[2025-05-04 07:38:09,587]: Mean: -0.0007
[2025-05-04 07:38:09,588]: Std: 0.0666
[2025-05-04 07:38:09,589]: Min: -0.2319
[2025-05-04 07:38:09,589]: Max: 0.2427
[2025-05-04 07:38:09,589]: 
Parameter: layer1.0.bn2.weight
[2025-05-04 07:38:09,589]: Shape: torch.Size([16])
[2025-05-04 07:38:09,591]: Sample Values (16 elements): [0.9018180966377258, 0.9742838740348816, 0.8759545087814331, 0.9588026404380798, 0.9408332705497742, 0.8489014506340027, 1.003838300704956, 0.9628807306289673, 0.9830116629600525, 0.7721073627471924, 0.7690764665603638, 0.8518243432044983, 0.9226018190383911, 0.6533128619194031, 0.7966732382774353, 0.7827265858650208]
[2025-05-04 07:38:09,593]: Mean: 0.8749
[2025-05-04 07:38:09,597]: Std: 0.0990
[2025-05-04 07:38:09,598]: Min: 0.6533
[2025-05-04 07:38:09,598]: Max: 1.0038
[2025-05-04 07:38:09,598]: 
Parameter: layer1.0.bn2.bias
[2025-05-04 07:38:09,598]: Shape: torch.Size([16])
[2025-05-04 07:38:09,599]: Sample Values (16 elements): [-0.424323707818985, -0.0377548485994339, 0.21476201713085175, -0.0484234057366848, -0.38795042037963867, -0.15192323923110962, -0.16247247159481049, -0.34468695521354675, -0.1144656091928482, 0.35235196352005005, 0.24195124208927155, 0.40540811419487, -0.45751112699508667, 0.3061440587043762, 0.09957375377416611, -0.004383510909974575]
[2025-05-04 07:38:09,600]: Mean: -0.0321
[2025-05-04 07:38:09,601]: Std: 0.2835
[2025-05-04 07:38:09,601]: Min: -0.4575
[2025-05-04 07:38:09,602]: Max: 0.4054
[2025-05-04 07:38:09,602]: 
Parameter: layer1.1.conv1.weight
[2025-05-04 07:38:09,602]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:38:09,603]: Sample Values (16 elements): [-0.023395629599690437, -0.07310345768928528, 0.012676202692091465, -0.09960055351257324, -0.013223143294453621, 0.10638885200023651, 0.023467041552066803, 0.038444243371486664, -0.05671887472271919, -0.020394600927829742, -0.085150346159935, -0.04882773756980896, 0.029339775443077087, -0.03337925300002098, 0.08049395680427551, 0.047333959490060806]
[2025-05-04 07:38:09,603]: Mean: 0.0020
[2025-05-04 07:38:09,604]: Std: 0.0652
[2025-05-04 07:38:09,604]: Min: -0.1931
[2025-05-04 07:38:09,605]: Max: 0.2366
[2025-05-04 07:38:09,605]: 
Parameter: layer1.1.bn1.weight
[2025-05-04 07:38:09,605]: Shape: torch.Size([16])
[2025-05-04 07:38:09,605]: Sample Values (16 elements): [0.9135109186172485, 1.0371211767196655, 0.9392065405845642, 1.075406551361084, 0.9467574954032898, 0.8386598825454712, 1.019573450088501, 0.9521734714508057, 1.0176150798797607, 0.9226016402244568, 0.9631439447402954, 1.0077388286590576, 1.0397852659225464, 0.9496399760246277, 1.0036051273345947, 0.9247892498970032]
[2025-05-04 07:38:09,610]: Mean: 0.9720
[2025-05-04 07:38:09,612]: Std: 0.0604
[2025-05-04 07:38:09,612]: Min: 0.8387
[2025-05-04 07:38:09,613]: Max: 1.0754
[2025-05-04 07:38:09,613]: 
Parameter: layer1.1.bn1.bias
[2025-05-04 07:38:09,613]: Shape: torch.Size([16])
[2025-05-04 07:38:09,615]: Sample Values (16 elements): [0.38588619232177734, 0.048249468207359314, -0.12331860512495041, -0.23834960162639618, 0.21802544593811035, -0.09872982650995255, -0.020866742357611656, 0.08104202151298523, 0.02782764844596386, 0.018870558589696884, -0.2065359503030777, 0.23278267681598663, -0.04015392065048218, 0.005129343364387751, 0.14296196401119232, 0.04345301166176796]
[2025-05-04 07:38:09,615]: Mean: 0.0298
[2025-05-04 07:38:09,616]: Std: 0.1623
[2025-05-04 07:38:09,616]: Min: -0.2383
[2025-05-04 07:38:09,617]: Max: 0.3859
[2025-05-04 07:38:09,617]: 
Parameter: layer1.1.conv2.weight
[2025-05-04 07:38:09,617]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:38:09,618]: Sample Values (16 elements): [-0.026066351681947708, -0.05921073630452156, 0.019419196993112564, 0.08468936383724213, 0.05658808350563049, 0.000922893697861582, 0.04321381822228432, 0.05601215362548828, -0.08788355439901352, -0.0029599026311188936, -0.013113167136907578, -0.05604979768395424, 0.08522245287895203, -0.009122271090745926, -0.02300218865275383, -0.012739439494907856]
[2025-05-04 07:38:09,619]: Mean: 0.0010
[2025-05-04 07:38:09,619]: Std: 0.0642
[2025-05-04 07:38:09,620]: Min: -0.2142
[2025-05-04 07:38:09,620]: Max: 0.1795
[2025-05-04 07:38:09,621]: 
Parameter: layer1.1.bn2.weight
[2025-05-04 07:38:09,621]: Shape: torch.Size([16])
[2025-05-04 07:38:09,621]: Sample Values (16 elements): [0.9040297269821167, 0.9168917536735535, 0.9217929244041443, 0.8006374835968018, 0.9506651163101196, 0.9237089157104492, 0.7681174874305725, 0.7844699621200562, 0.8339121341705322, 0.8646860122680664, 0.8490106463432312, 0.8104944825172424, 0.9426876306533813, 1.0067325830459595, 0.9539005160331726, 0.9552648663520813]
[2025-05-04 07:38:09,622]: Mean: 0.8867
[2025-05-04 07:38:09,622]: Std: 0.0717
[2025-05-04 07:38:09,622]: Min: 0.7681
[2025-05-04 07:38:09,623]: Max: 1.0067
[2025-05-04 07:38:09,623]: 
Parameter: layer1.1.bn2.bias
[2025-05-04 07:38:09,623]: Shape: torch.Size([16])
[2025-05-04 07:38:09,624]: Sample Values (16 elements): [-0.18846744298934937, -0.5031610727310181, -0.4197200536727905, 0.26918449997901917, 0.14007195830345154, -0.08932358026504517, 0.16232700645923615, 0.08524183183908463, 0.03852178901433945, -0.15441076457500458, 0.007814557291567326, 0.06989093124866486, 0.055700212717056274, -0.25201526284217834, -0.053875479847192764, 0.25719907879829407]
[2025-05-04 07:38:09,624]: Mean: -0.0359
[2025-05-04 07:38:09,624]: Std: 0.2227
[2025-05-04 07:38:09,625]: Min: -0.5032
[2025-05-04 07:38:09,626]: Max: 0.2692
[2025-05-04 07:38:09,626]: 
Parameter: layer1.2.conv1.weight
[2025-05-04 07:38:09,626]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:38:09,627]: Sample Values (16 elements): [0.041937556117773056, 0.08889580518007278, 0.040940169245004654, -0.025828860700130463, -0.01971968077123165, -0.05885889753699303, 0.06341622024774551, 0.04141531512141228, 0.04947831854224205, -0.08519395440816879, 0.09501062333583832, -0.12102573364973068, -0.057897280901670456, 0.09789550304412842, 0.0020569730550050735, -0.11678104847669601]
[2025-05-04 07:38:09,628]: Mean: 0.0002
[2025-05-04 07:38:09,628]: Std: 0.0616
[2025-05-04 07:38:09,628]: Min: -0.1857
[2025-05-04 07:38:09,629]: Max: 0.2068
[2025-05-04 07:38:09,629]: 
Parameter: layer1.2.bn1.weight
[2025-05-04 07:38:09,629]: Shape: torch.Size([16])
[2025-05-04 07:38:09,630]: Sample Values (16 elements): [0.9065953493118286, 0.9546288251876831, 1.0031466484069824, 0.9592845439910889, 0.9800357222557068, 1.0660490989685059, 0.9169701933860779, 0.9348292350769043, 0.9378352165222168, 0.9348431825637817, 1.0162124633789062, 1.0987578630447388, 0.9203130006790161, 0.9464387893676758, 0.9397425055503845, 0.9188605546951294]
[2025-05-04 07:38:09,631]: Mean: 0.9647
[2025-05-04 07:38:09,632]: Std: 0.0553
[2025-05-04 07:38:09,632]: Min: 0.9066
[2025-05-04 07:38:09,633]: Max: 1.0988
[2025-05-04 07:38:09,634]: 
Parameter: layer1.2.bn1.bias
[2025-05-04 07:38:09,634]: Shape: torch.Size([16])
[2025-05-04 07:38:09,637]: Sample Values (16 elements): [0.058939315378665924, 0.08270132541656494, 0.012152289040386677, -0.05349399149417877, -0.03948495164513588, -0.23792479932308197, -0.030798425897955894, 0.14362101256847382, -0.02713812328875065, -0.010739509016275406, -0.025610631331801414, 0.1530483514070511, -0.032354820519685745, -0.12987986207008362, -0.021861908957362175, 0.0740031749010086]
[2025-05-04 07:38:09,638]: Mean: -0.0053
[2025-05-04 07:38:09,639]: Std: 0.0971
[2025-05-04 07:38:09,639]: Min: -0.2379
[2025-05-04 07:38:09,641]: Max: 0.1530
[2025-05-04 07:38:09,641]: 
Parameter: layer1.2.conv2.weight
[2025-05-04 07:38:09,641]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:38:09,642]: Sample Values (16 elements): [-0.03285490348935127, -0.03176061064004898, 0.0023400885984301567, 0.07915487885475159, 0.05175254866480827, -0.048609551042318344, -0.04263798147439957, 0.055111486464738846, -0.07229097932577133, -0.07804898917675018, 0.05948592722415924, -0.00753523875027895, 0.045049674808979034, -0.02378068119287491, -0.03595733642578125, 0.0522611141204834]
[2025-05-04 07:38:09,642]: Mean: -0.0023
[2025-05-04 07:38:09,643]: Std: 0.0586
[2025-05-04 07:38:09,643]: Min: -0.1902
[2025-05-04 07:38:09,644]: Max: 0.2000
[2025-05-04 07:38:09,645]: 
Parameter: layer1.2.bn2.weight
[2025-05-04 07:38:09,645]: Shape: torch.Size([16])
[2025-05-04 07:38:09,646]: Sample Values (16 elements): [0.9227866530418396, 1.0072095394134521, 0.9291865229606628, 0.8015544414520264, 0.9133197069168091, 0.9571477174758911, 0.9195935726165771, 0.8548124432563782, 0.9151181578636169, 0.8879567980766296, 0.9401468634605408, 0.9168280959129333, 0.9098582863807678, 0.8321149945259094, 0.8846169710159302, 0.9956488013267517]
[2025-05-04 07:38:09,646]: Mean: 0.9117
[2025-05-04 07:38:09,647]: Std: 0.0533
[2025-05-04 07:38:09,647]: Min: 0.8016
[2025-05-04 07:38:09,648]: Max: 1.0072
[2025-05-04 07:38:09,648]: 
Parameter: layer1.2.bn2.bias
[2025-05-04 07:38:09,648]: Shape: torch.Size([16])
[2025-05-04 07:38:09,648]: Sample Values (16 elements): [-0.11301661282777786, 0.023368366062641144, 0.4748448431491852, -0.038479842245578766, -0.3006572723388672, -0.07180269062519073, -0.2101556956768036, -0.10977081954479218, -0.0966276153922081, 0.05709216371178627, 0.11487892270088196, 0.08532372117042542, 0.05897674337029457, 0.0522846020758152, -0.14641475677490234, -0.029179751873016357]
[2025-05-04 07:38:09,649]: Mean: -0.0156
[2025-05-04 07:38:09,650]: Std: 0.1732
[2025-05-04 07:38:09,650]: Min: -0.3007
[2025-05-04 07:38:09,651]: Max: 0.4748
[2025-05-04 07:38:09,651]: 
Parameter: layer2.0.conv1.weight
[2025-05-04 07:38:09,651]: Shape: torch.Size([32, 16, 3, 3])
[2025-05-04 07:38:09,651]: Sample Values (16 elements): [0.060070645064115524, 0.022037098184227943, 0.021559063345193863, -0.07533230632543564, -0.031726375222206116, -0.08401578664779663, -0.09243892878293991, -0.002675989642739296, -0.05586456134915352, -0.0262571033090353, -0.07774554193019867, -0.015068842098116875, 0.129826158285141, -0.05251901596784592, 0.054778803139925, 0.007075246423482895]
[2025-05-04 07:38:09,652]: Mean: -0.0011
[2025-05-04 07:38:09,653]: Std: 0.0530
[2025-05-04 07:38:09,653]: Min: -0.1852
[2025-05-04 07:38:09,653]: Max: 0.1972
[2025-05-04 07:38:09,653]: 
Parameter: layer2.0.bn1.weight
[2025-05-04 07:38:09,654]: Shape: torch.Size([32])
[2025-05-04 07:38:09,654]: Sample Values (16 elements): [0.9463641047477722, 0.9629955887794495, 1.0010244846343994, 0.9349864721298218, 0.9426910877227783, 0.9611742496490479, 0.9537556171417236, 0.9432932138442993, 0.9648909568786621, 0.9547024369239807, 0.9405048489570618, 1.0041093826293945, 0.9761766195297241, 0.9635685086250305, 0.9646649956703186, 0.9663172364234924]
[2025-05-04 07:38:09,654]: Mean: 0.9595
[2025-05-04 07:38:09,655]: Std: 0.0208
[2025-05-04 07:38:09,655]: Min: 0.9092
[2025-05-04 07:38:09,655]: Max: 1.0041
[2025-05-04 07:38:09,656]: 
Parameter: layer2.0.bn1.bias
[2025-05-04 07:38:09,656]: Shape: torch.Size([32])
[2025-05-04 07:38:09,657]: Sample Values (16 elements): [0.008503689430654049, -0.196516215801239, 0.0069880192168056965, 0.004154198803007603, -0.005862592253834009, -0.0746561661362648, -0.027651991695165634, 0.05169861391186714, -0.04208410531282425, -0.03577519208192825, 0.023042170330882072, 0.018985208123922348, -0.024946684017777443, 0.026880713179707527, 0.014506441541016102, -0.12268007546663284]
[2025-05-04 07:38:09,657]: Mean: -0.0122
[2025-05-04 07:38:09,657]: Std: 0.0497
[2025-05-04 07:38:09,658]: Min: -0.1965
[2025-05-04 07:38:09,658]: Max: 0.0517
[2025-05-04 07:38:09,659]: 
Parameter: layer2.0.conv2.weight
[2025-05-04 07:38:09,659]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:38:09,659]: Sample Values (16 elements): [-0.00911465473473072, 0.08589118719100952, -0.021902086213231087, 0.021809030324220657, -0.022261843085289, 0.014633108861744404, 0.029075125232338905, -0.0001025121528073214, 0.01779649406671524, 0.019751260057091713, 0.027514459565281868, 0.049689970910549164, -0.01877826824784279, -0.013838846236467361, 0.011255709454417229, 0.004789426922798157]
[2025-05-04 07:38:09,660]: Mean: 0.0007
[2025-05-04 07:38:09,661]: Std: 0.0390
[2025-05-04 07:38:09,663]: Min: -0.1339
[2025-05-04 07:38:09,664]: Max: 0.1250
[2025-05-04 07:38:09,665]: 
Parameter: layer2.0.bn2.weight
[2025-05-04 07:38:09,665]: Shape: torch.Size([32])
[2025-05-04 07:38:09,672]: Sample Values (16 elements): [1.0092705488204956, 1.0255424976348877, 0.980170726776123, 0.9737483263015747, 0.9280173182487488, 1.0189496278762817, 0.8752965331077576, 1.017971396446228, 0.9688839316368103, 0.957393229007721, 0.9492350816726685, 0.9590592384338379, 1.0128225088119507, 1.007418155670166, 0.9799937605857849, 0.900501012802124]
[2025-05-04 07:38:09,679]: Mean: 0.9725
[2025-05-04 07:38:09,695]: Std: 0.0346
[2025-05-04 07:38:09,698]: Min: 0.8753
[2025-05-04 07:38:09,699]: Max: 1.0255
[2025-05-04 07:38:09,699]: 
Parameter: layer2.0.bn2.bias
[2025-05-04 07:38:09,699]: Shape: torch.Size([32])
[2025-05-04 07:38:09,700]: Sample Values (16 elements): [0.1692247986793518, -0.024105018004775047, -0.06395148485898972, -0.08798892796039581, 0.04279228299856186, -0.10482704639434814, 0.028414087370038033, -0.11539140343666077, -0.14136086404323578, 0.09399072825908661, 0.05464870482683182, -0.09383153915405273, 0.056730423122644424, -0.011584596708416939, 0.03706906735897064, 0.029404837638139725]
[2025-05-04 07:38:09,700]: Mean: -0.0067
[2025-05-04 07:38:09,701]: Std: 0.0744
[2025-05-04 07:38:09,701]: Min: -0.1414
[2025-05-04 07:38:09,701]: Max: 0.1692
[2025-05-04 07:38:09,701]: 
Parameter: layer2.0.downsample.0.weight
[2025-05-04 07:38:09,701]: Shape: torch.Size([32, 16, 1, 1])
[2025-05-04 07:38:09,702]: Sample Values (16 elements): [0.1538315862417221, 0.07913292199373245, 0.0649554580450058, 0.04051569104194641, -0.17465470731258392, 0.21675708889961243, -0.11387793719768524, 0.018470285460352898, 0.1433975249528885, 0.02137039415538311, -0.09805711358785629, -0.105788454413414, -0.06603320688009262, 0.17010140419006348, -0.09702207148075104, -0.11087320744991302]
[2025-05-04 07:38:09,703]: Mean: 0.0044
[2025-05-04 07:38:09,703]: Std: 0.1456
[2025-05-04 07:38:09,703]: Min: -0.3031
[2025-05-04 07:38:09,704]: Max: 0.2926
[2025-05-04 07:38:09,704]: 
Parameter: layer2.0.downsample.1.weight
[2025-05-04 07:38:09,705]: Shape: torch.Size([32])
[2025-05-04 07:38:09,705]: Sample Values (16 elements): [0.9154910445213318, 0.9302188158035278, 0.8636959195137024, 0.862976610660553, 0.895574152469635, 0.8467128872871399, 0.8821594715118408, 0.8630729913711548, 0.8879614472389221, 0.8809073567390442, 0.9164570569992065, 0.9207144379615784, 0.9308814406394958, 0.8700525760650635, 0.8484508991241455, 0.9093711972236633]
[2025-05-04 07:38:09,706]: Mean: 0.8885
[2025-05-04 07:38:09,706]: Std: 0.0300
[2025-05-04 07:38:09,707]: Min: 0.8296
[2025-05-04 07:38:09,707]: Max: 0.9518
[2025-05-04 07:38:09,707]: 
Parameter: layer2.0.downsample.1.bias
[2025-05-04 07:38:09,708]: Shape: torch.Size([32])
[2025-05-04 07:38:09,708]: Sample Values (16 elements): [-0.04560494422912598, -0.06395148485898972, 0.04279228299856186, 0.019209954887628555, -0.13289926946163177, -0.10482704639434814, -0.08798892796039581, -0.14136086404323578, 0.07716092467308044, -0.02761339768767357, -0.057556986808776855, -0.054345130920410156, 0.016034800559282303, -0.09383153915405273, -0.024105018004775047, 0.06797194480895996]
[2025-05-04 07:38:09,709]: Mean: -0.0067
[2025-05-04 07:38:09,709]: Std: 0.0744
[2025-05-04 07:38:09,710]: Min: -0.1414
[2025-05-04 07:38:09,713]: Max: 0.1692
[2025-05-04 07:38:09,713]: 
Parameter: layer2.1.conv1.weight
[2025-05-04 07:38:09,713]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:38:09,713]: Sample Values (16 elements): [-0.047336358577013016, 0.03744157403707504, -0.004404297098517418, -0.06447813659906387, 0.052025485783815384, -0.03715325891971588, -0.033773645758628845, 0.056587137281894684, -0.026491982862353325, 0.05827606841921806, 0.048836126923561096, -0.04551399126648903, 0.0012642624787986279, -0.007508983835577965, -0.04590757191181183, 0.04009369760751724]
[2025-05-04 07:38:09,714]: Mean: 0.0009
[2025-05-04 07:38:09,714]: Std: 0.0394
[2025-05-04 07:38:09,715]: Min: -0.1240
[2025-05-04 07:38:09,715]: Max: 0.1281
[2025-05-04 07:38:09,715]: 
Parameter: layer2.1.bn1.weight
[2025-05-04 07:38:09,715]: Shape: torch.Size([32])
[2025-05-04 07:38:09,717]: Sample Values (16 elements): [0.9921507239341736, 0.9950735569000244, 0.9830793738365173, 0.9602081775665283, 0.9748103022575378, 0.973136305809021, 0.9766969680786133, 1.0036277770996094, 0.995372474193573, 0.979939877986908, 0.9750842452049255, 1.007246971130371, 0.9818835854530334, 0.9646102786064148, 0.9795235395431519, 0.9768348336219788]
[2025-05-04 07:38:09,719]: Mean: 0.9829
[2025-05-04 07:38:09,720]: Std: 0.0174
[2025-05-04 07:38:09,720]: Min: 0.9290
[2025-05-04 07:38:09,720]: Max: 1.0072
[2025-05-04 07:38:09,720]: 
Parameter: layer2.1.bn1.bias
[2025-05-04 07:38:09,721]: Shape: torch.Size([32])
[2025-05-04 07:38:09,722]: Sample Values (16 elements): [0.11325539648532867, 0.059040218591690063, -0.055563852190971375, 0.04094704985618591, 0.010687240399420261, 0.07449802756309509, -0.008322942070662975, -0.04240555316209793, 0.02193012461066246, -0.09227103739976883, 0.01683497056365013, 0.017698800191283226, 0.042908500880002975, -0.010999217629432678, 0.051883455365896225, 0.031228816136717796]
[2025-05-04 07:38:09,724]: Mean: 0.0165
[2025-05-04 07:38:09,728]: Std: 0.0445
[2025-05-04 07:38:09,732]: Min: -0.0923
[2025-05-04 07:38:09,734]: Max: 0.1133
[2025-05-04 07:38:09,734]: 
Parameter: layer2.1.conv2.weight
[2025-05-04 07:38:09,734]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:38:09,756]: Sample Values (16 elements): [-0.0009948360966518521, -0.059223927557468414, -0.0338289774954319, 0.032047513872385025, -0.02157316356897354, -0.047020670026540756, 0.05158787593245506, -0.037731561809778214, -0.05301743745803833, 0.059599924832582474, -0.025258583948016167, -0.04780765622854233, 0.004235147964209318, 0.0038454008754342794, -0.005517277866601944, -0.023290134966373444]
[2025-05-04 07:38:09,762]: Mean: 0.0001
[2025-05-04 07:38:09,763]: Std: 0.0394
[2025-05-04 07:38:09,763]: Min: -0.1357
[2025-05-04 07:38:09,764]: Max: 0.1427
[2025-05-04 07:38:09,764]: 
Parameter: layer2.1.bn2.weight
[2025-05-04 07:38:09,764]: Shape: torch.Size([32])
[2025-05-04 07:38:09,769]: Sample Values (16 elements): [0.9610368609428406, 0.9741219282150269, 0.9797606468200684, 0.9650355577468872, 0.9520919322967529, 0.9873034954071045, 0.9508793950080872, 1.000386357307434, 1.003822922706604, 0.955024242401123, 0.9622713923454285, 0.9735686182975769, 0.971574604511261, 0.9558878540992737, 0.9626231789588928, 0.9299069046974182]
[2025-05-04 07:38:09,770]: Mean: 0.9643
[2025-05-04 07:38:09,770]: Std: 0.0208
[2025-05-04 07:38:09,771]: Min: 0.9251
[2025-05-04 07:38:09,771]: Max: 1.0038
[2025-05-04 07:38:09,771]: 
Parameter: layer2.1.bn2.bias
[2025-05-04 07:38:09,771]: Shape: torch.Size([32])
[2025-05-04 07:38:09,772]: Sample Values (16 elements): [-0.02655530720949173, -0.07905177026987076, -0.043600261211395264, 0.02159736678004265, 0.07103845477104187, 0.04895761236548424, 0.06842576712369919, -0.07876482605934143, -0.06425492465496063, -0.01750866509974003, 0.03705572709441185, -0.007211893331259489, 0.035426728427410126, -0.04081222787499428, 0.060572534799575806, -0.019714202731847763]
[2025-05-04 07:38:09,772]: Mean: -0.0045
[2025-05-04 07:38:09,773]: Std: 0.0592
[2025-05-04 07:38:09,773]: Min: -0.1377
[2025-05-04 07:38:09,774]: Max: 0.1260
[2025-05-04 07:38:09,774]: 
Parameter: layer2.2.conv1.weight
[2025-05-04 07:38:09,774]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:38:09,776]: Sample Values (16 elements): [0.0036002234555780888, 0.07165917754173279, 0.05013080686330795, -0.03432450443506241, 0.029151087626814842, -0.04050449654459953, 0.06547752767801285, 0.011187884025275707, -0.02440362423658371, 0.01583118923008442, 0.017326924949884415, -0.037450481206178665, -0.027362417429685593, -0.04208715260028839, -0.027510011568665504, 0.013570173643529415]
[2025-05-04 07:38:09,776]: Mean: 0.0001
[2025-05-04 07:38:09,777]: Std: 0.0386
[2025-05-04 07:38:09,778]: Min: -0.1174
[2025-05-04 07:38:09,778]: Max: 0.1179
[2025-05-04 07:38:09,778]: 
Parameter: layer2.2.bn1.weight
[2025-05-04 07:38:09,778]: Shape: torch.Size([32])
[2025-05-04 07:38:09,779]: Sample Values (16 elements): [0.9775418043136597, 0.9637635946273804, 0.9873231053352356, 1.0036358833312988, 0.9501227736473083, 1.0199679136276245, 0.9833123683929443, 0.9829193353652954, 0.9546064138412476, 0.9802411198616028, 1.0039712190628052, 0.9880717396736145, 0.9458475112915039, 0.963280200958252, 0.9989442825317383, 0.9865382313728333]
[2025-05-04 07:38:09,779]: Mean: 0.9799
[2025-05-04 07:38:09,780]: Std: 0.0190
[2025-05-04 07:38:09,780]: Min: 0.9429
[2025-05-04 07:38:09,781]: Max: 1.0200
[2025-05-04 07:38:09,781]: 
Parameter: layer2.2.bn1.bias
[2025-05-04 07:38:09,781]: Shape: torch.Size([32])
[2025-05-04 07:38:09,784]: Sample Values (16 elements): [-0.03806755691766739, 0.02345416508615017, -0.10106457024812698, -0.004206487908959389, 0.0039002597332000732, 0.02800678461790085, 0.02275608293712139, 0.11353441327810287, -0.008414683863520622, -0.020523784682154655, 0.0035982702393084764, 0.0492120124399662, 0.032784562557935715, 0.005198200698941946, 0.0017897430807352066, 0.0013211615150794387]
[2025-05-04 07:38:09,784]: Mean: 0.0071
[2025-05-04 07:38:09,784]: Std: 0.0417
[2025-05-04 07:38:09,785]: Min: -0.1011
[2025-05-04 07:38:09,786]: Max: 0.1135
[2025-05-04 07:38:09,786]: 
Parameter: layer2.2.conv2.weight
[2025-05-04 07:38:09,786]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:38:09,787]: Sample Values (16 elements): [-0.03637746348977089, -0.024072716012597084, -0.045197002589702606, -0.030751673504710197, 0.02903790958225727, -0.023349139839410782, 0.010847816243767738, 0.018689678981900215, 0.0363120436668396, 0.045302048325538635, -0.007522914092987776, 0.05408768728375435, 0.013048234395682812, 0.02250088006258011, 0.02542736753821373, 0.025413474068045616]
[2025-05-04 07:38:09,787]: Mean: 0.0009
[2025-05-04 07:38:09,788]: Std: 0.0385
[2025-05-04 07:38:09,788]: Min: -0.1242
[2025-05-04 07:38:09,789]: Max: 0.1157
[2025-05-04 07:38:09,789]: 
Parameter: layer2.2.bn2.weight
[2025-05-04 07:38:09,789]: Shape: torch.Size([32])
[2025-05-04 07:38:09,789]: Sample Values (16 elements): [1.0252281427383423, 0.9546666145324707, 0.9664266705513, 0.9579535126686096, 0.9119513034820557, 0.9738904237747192, 0.9284756779670715, 0.9880728721618652, 0.9789150357246399, 0.9425769448280334, 0.9868152737617493, 0.9765066504478455, 0.994056761264801, 1.0317893028259277, 1.0027337074279785, 0.9859936833381653]
[2025-05-04 07:38:09,789]: Mean: 0.9780
[2025-05-04 07:38:09,790]: Std: 0.0324
[2025-05-04 07:38:09,793]: Min: 0.9120
[2025-05-04 07:38:09,794]: Max: 1.0459
[2025-05-04 07:38:09,794]: 
Parameter: layer2.2.bn2.bias
[2025-05-04 07:38:09,794]: Shape: torch.Size([32])
[2025-05-04 07:38:09,795]: Sample Values (16 elements): [0.0117527199909091, -0.06297726929187775, -0.030550966039299965, 0.14011988043785095, -0.004838146734982729, 0.013913435861468315, -0.005114338360726833, 0.06548430025577545, 0.050457123667001724, 0.1801002472639084, 0.0008311777492053807, -0.04073277488350868, -0.026135534048080444, 0.02280639298260212, 0.029557861387729645, -0.052245963364839554]
[2025-05-04 07:38:09,796]: Mean: -0.0037
[2025-05-04 07:38:09,797]: Std: 0.0635
[2025-05-04 07:38:09,797]: Min: -0.1290
[2025-05-04 07:38:09,798]: Max: 0.1801
[2025-05-04 07:38:09,798]: 
Parameter: layer3.0.conv1.weight
[2025-05-04 07:38:09,798]: Shape: torch.Size([64, 32, 3, 3])
[2025-05-04 07:38:09,800]: Sample Values (16 elements): [-0.0022318728733807802, 0.061734430491924286, -0.03388705477118492, 0.03407808020710945, 0.05534961074590683, -0.055058110505342484, 0.0436684750020504, -0.03212742134928703, 0.021238921210169792, 0.022347653284668922, -0.05185714364051819, 0.005553313996642828, 0.018894007429480553, 0.017080429941415787, -0.018643252551555634, -0.025443607941269875]
[2025-05-04 07:38:09,800]: Mean: 0.0001
[2025-05-04 07:38:09,800]: Std: 0.0355
[2025-05-04 07:38:09,801]: Min: -0.1117
[2025-05-04 07:38:09,801]: Max: 0.1102
[2025-05-04 07:38:09,801]: 
Parameter: layer3.0.bn1.weight
[2025-05-04 07:38:09,801]: Shape: torch.Size([64])
[2025-05-04 07:38:09,802]: Sample Values (16 elements): [1.0056034326553345, 0.9672949314117432, 0.9594818949699402, 0.9810194969177246, 0.9561256170272827, 0.9739323258399963, 0.9895634055137634, 1.0062317848205566, 0.9844292402267456, 0.9647937417030334, 0.971209704875946, 0.9589673280715942, 0.983555257320404, 0.9915583729743958, 0.9833886027336121, 0.9918636679649353]
[2025-05-04 07:38:09,805]: Mean: 0.9761
[2025-05-04 07:38:09,806]: Std: 0.0118
[2025-05-04 07:38:09,807]: Min: 0.9549
[2025-05-04 07:38:09,808]: Max: 1.0088
[2025-05-04 07:38:09,808]: 
Parameter: layer3.0.bn1.bias
[2025-05-04 07:38:09,808]: Shape: torch.Size([64])
[2025-05-04 07:38:09,809]: Sample Values (16 elements): [0.04771842062473297, 0.002276928396895528, -0.025521788746118546, -0.021674562245607376, 0.03118554688990116, -0.008460883051156998, 0.038432586938142776, 0.0813167467713356, -0.007636127062141895, 0.022079654037952423, -0.03860244154930115, -0.003462341148406267, 0.002163305412977934, 0.004956363700330257, -0.017193522304296494, -0.01029829028993845]
[2025-05-04 07:38:09,809]: Mean: -0.0025
[2025-05-04 07:38:09,810]: Std: 0.0344
[2025-05-04 07:38:09,812]: Min: -0.0900
[2025-05-04 07:38:09,812]: Max: 0.0920
[2025-05-04 07:38:09,812]: 
Parameter: layer3.0.conv2.weight
[2025-05-04 07:38:09,812]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:38:09,825]: Sample Values (16 elements): [0.05945310741662979, -0.038221295922994614, -0.012601749040186405, -0.02269730716943741, 0.03748392313718796, 0.044258132576942444, 0.013892349787056446, 0.01930820196866989, -0.045074403285980225, 0.051641639322042465, -0.00036095757968723774, 0.053744446486234665, 0.017253803089261055, 0.03269261494278908, 0.016650650650262833, -0.03156763315200806]
[2025-05-04 07:38:09,826]: Mean: 0.0003
[2025-05-04 07:38:09,827]: Std: 0.0264
[2025-05-04 07:38:09,827]: Min: -0.0907
[2025-05-04 07:38:09,828]: Max: 0.0842
[2025-05-04 07:38:09,828]: 
Parameter: layer3.0.bn2.weight
[2025-05-04 07:38:09,828]: Shape: torch.Size([64])
[2025-05-04 07:38:09,829]: Sample Values (16 elements): [1.0015586614608765, 0.9785628914833069, 1.0004674196243286, 0.9868454337120056, 0.9957654476165771, 0.9895889759063721, 0.9830487966537476, 0.9779565334320068, 0.9883415699005127, 0.9976363778114319, 0.9985863566398621, 0.9872353672981262, 0.9489892721176147, 0.9875345826148987, 0.9982420206069946, 0.9870241284370422]
[2025-05-04 07:38:09,830]: Mean: 0.9872
[2025-05-04 07:38:09,831]: Std: 0.0124
[2025-05-04 07:38:09,831]: Min: 0.9490
[2025-05-04 07:38:09,832]: Max: 1.0193
[2025-05-04 07:38:09,832]: 
Parameter: layer3.0.bn2.bias
[2025-05-04 07:38:09,832]: Shape: torch.Size([64])
[2025-05-04 07:38:09,833]: Sample Values (16 elements): [0.06235398352146149, 0.025510134175419807, 0.02213863469660282, -0.03713785484433174, 0.021766774356365204, 0.04126458615064621, -0.07754068076610565, 0.046680811792612076, -0.0076499804854393005, -0.04736451432108879, -0.01820841059088707, -0.0016927416436374187, 0.043603330850601196, 0.04036491736769676, -0.05868123471736908, -0.028152303770184517]
[2025-05-04 07:38:09,833]: Mean: 0.0031
[2025-05-04 07:38:09,834]: Std: 0.0423
[2025-05-04 07:38:09,834]: Min: -0.0850
[2025-05-04 07:38:09,835]: Max: 0.1025
[2025-05-04 07:38:09,835]: 
Parameter: layer3.0.downsample.0.weight
[2025-05-04 07:38:09,835]: Shape: torch.Size([64, 32, 1, 1])
[2025-05-04 07:38:09,837]: Sample Values (16 elements): [0.0029848460108041763, 0.026025189086794853, -0.06579483300447464, -0.14147141575813293, -0.07227811217308044, 0.14662493765354156, -0.0710507333278656, -0.0869298204779625, -0.14652949571609497, 0.1289432942867279, -0.032121047377586365, 0.07466278970241547, 0.07609900087118149, 0.003016871865838766, -0.12230800837278366, 0.13950033485889435]
[2025-05-04 07:38:09,838]: Mean: 0.0002
[2025-05-04 07:38:09,838]: Std: 0.1005
[2025-05-04 07:38:09,838]: Min: -0.2133
[2025-05-04 07:38:09,839]: Max: 0.2220
[2025-05-04 07:38:09,839]: 
Parameter: layer3.0.downsample.1.weight
[2025-05-04 07:38:09,839]: Shape: torch.Size([64])
[2025-05-04 07:38:09,840]: Sample Values (16 elements): [0.9360101222991943, 0.9186092615127563, 0.9380577802658081, 0.936430811882019, 0.9336204528808594, 0.9243227243423462, 0.9419951438903809, 0.9382912516593933, 0.9248048067092896, 0.9439042806625366, 0.903412938117981, 0.9175377488136292, 0.9240585565567017, 0.9262884259223938, 0.9158788919448853, 0.9310224056243896]
[2025-05-04 07:38:09,840]: Mean: 0.9318
[2025-05-04 07:38:09,841]: Std: 0.0099
[2025-05-04 07:38:09,841]: Min: 0.9034
[2025-05-04 07:38:09,842]: Max: 0.9509
[2025-05-04 07:38:09,842]: 
Parameter: layer3.0.downsample.1.bias
[2025-05-04 07:38:09,842]: Shape: torch.Size([64])
[2025-05-04 07:38:09,843]: Sample Values (16 elements): [-0.028152303770184517, -0.028025928884744644, 0.06235398352146149, 0.046680811792612076, 0.06918976455926895, -0.007455508690327406, -0.043157659471035004, 0.03303002193570137, -0.03295541927218437, 0.07275106012821198, 0.031190069392323494, 0.02213863469660282, 0.020557429641485214, 0.042653050273656845, 0.028489192947745323, -0.022001655772328377]
[2025-05-04 07:38:09,843]: Mean: 0.0031
[2025-05-04 07:38:09,843]: Std: 0.0423
[2025-05-04 07:38:09,844]: Min: -0.0850
[2025-05-04 07:38:09,844]: Max: 0.1025
[2025-05-04 07:38:09,844]: 
Parameter: layer3.1.conv1.weight
[2025-05-04 07:38:09,844]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:38:09,857]: Sample Values (16 elements): [-0.012904220260679722, -0.026774568483233452, 0.048756178468465805, -0.0004303071473259479, -0.013838335871696472, 0.010480678640305996, -0.008583616465330124, -0.007944569922983646, -0.006581434514373541, 0.0361408106982708, -0.030656393617391586, -0.020319290459156036, 0.03515145182609558, -0.05230331048369408, 0.017492718994617462, -0.05467398837208748]
[2025-05-04 07:38:09,858]: Mean: 0.0002
[2025-05-04 07:38:09,858]: Std: 0.0269
[2025-05-04 07:38:09,858]: Min: -0.0846
[2025-05-04 07:38:09,859]: Max: 0.1001
[2025-05-04 07:38:09,859]: 
Parameter: layer3.1.bn1.weight
[2025-05-04 07:38:09,859]: Shape: torch.Size([64])
[2025-05-04 07:38:09,861]: Sample Values (16 elements): [1.0030726194381714, 0.9797917008399963, 0.9831719398498535, 0.9781444072723389, 0.9869756698608398, 0.9853845834732056, 0.9924059510231018, 0.9795078039169312, 0.9705963134765625, 0.9748284816741943, 0.9782365560531616, 0.9770973920822144, 0.9897026419639587, 1.0144370794296265, 1.0332378149032593, 0.991835355758667]
[2025-05-04 07:38:09,861]: Mean: 0.9918
[2025-05-04 07:38:09,862]: Std: 0.0163
[2025-05-04 07:38:09,862]: Min: 0.9667
[2025-05-04 07:38:09,862]: Max: 1.0332
[2025-05-04 07:38:09,862]: 
Parameter: layer3.1.bn1.bias
[2025-05-04 07:38:09,862]: Shape: torch.Size([64])
[2025-05-04 07:38:09,863]: Sample Values (16 elements): [0.020579079166054726, -0.01990596391260624, -0.036949727684259415, 0.0314243920147419, 0.010076481848955154, -0.006272373721003532, -0.04419417306780815, -0.0199087243527174, 0.009745932184159756, 0.03217843919992447, -0.017327379435300827, 0.007349432911723852, -0.03322454169392586, 0.014037148095667362, 0.01690099574625492, 0.025830576196312904]
[2025-05-04 07:38:09,864]: Mean: -0.0014
[2025-05-04 07:38:09,864]: Std: 0.0392
[2025-05-04 07:38:09,865]: Min: -0.1197
[2025-05-04 07:38:09,865]: Max: 0.1273
[2025-05-04 07:38:09,865]: 
Parameter: layer3.1.conv2.weight
[2025-05-04 07:38:09,865]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:38:09,893]: Sample Values (16 elements): [0.013705147430300713, 0.015604031272232533, -0.041809406131505966, 0.027667297050356865, -0.006305144168436527, -0.0038809608668088913, -0.07064241915941238, -0.015470344573259354, -0.016968606039881706, -0.038103118538856506, -0.011117668822407722, 0.011112871579825878, 0.012672226876020432, 0.0029762391932308674, 0.018153268843889236, -0.026836860924959183]
[2025-05-04 07:38:09,893]: Mean: -0.0002
[2025-05-04 07:38:09,894]: Std: 0.0259
[2025-05-04 07:38:09,894]: Min: -0.0876
[2025-05-04 07:38:09,894]: Max: 0.0825
[2025-05-04 07:38:09,894]: 
Parameter: layer3.1.bn2.weight
[2025-05-04 07:38:09,894]: Shape: torch.Size([64])
[2025-05-04 07:38:09,895]: Sample Values (16 elements): [0.9740825891494751, 0.9879742860794067, 0.9875893592834473, 0.9922260642051697, 0.9950773119926453, 0.9763675332069397, 1.0125914812088013, 1.0125713348388672, 0.9979055523872375, 0.9996429085731506, 0.9950529932975769, 0.9758317470550537, 1.006340742111206, 0.9820002913475037, 0.9984275102615356, 1.0038965940475464]
[2025-05-04 07:38:09,896]: Mean: 0.9911
[2025-05-04 07:38:09,896]: Std: 0.0114
[2025-05-04 07:38:09,896]: Min: 0.9678
[2025-05-04 07:38:09,897]: Max: 1.0133
[2025-05-04 07:38:09,897]: 
Parameter: layer3.1.bn2.bias
[2025-05-04 07:38:09,897]: Shape: torch.Size([64])
[2025-05-04 07:38:09,897]: Sample Values (16 elements): [-0.01388623472303152, 0.04283575341105461, -0.05109703168272972, -0.00896554533392191, -0.03505173698067665, -0.01632198877632618, 0.027502821758389473, 0.07272130995988846, 0.05062296614050865, -0.032716862857341766, -0.014907431788742542, 0.018268689513206482, 0.03172772005200386, 0.000119771808385849, 0.0053480807691812515, 0.026379354298114777]
[2025-05-04 07:38:09,898]: Mean: 0.0049
[2025-05-04 07:38:09,898]: Std: 0.0389
[2025-05-04 07:38:09,898]: Min: -0.0876
[2025-05-04 07:38:09,899]: Max: 0.0768
[2025-05-04 07:38:09,899]: 
Parameter: layer3.2.conv1.weight
[2025-05-04 07:38:09,899]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:38:09,954]: Sample Values (16 elements): [-0.003883401397615671, 0.031180521473288536, 0.030803067609667778, 0.003093235194683075, 0.006589238531887531, 0.013364431448280811, -0.012338208965957165, -0.012431567534804344, 0.018420886248350143, -0.041645098477602005, 0.02265380136668682, 0.0041748699732124805, 0.0043267663568258286, -0.02196001261472702, -0.007120200898498297, 0.0026079483795911074]
[2025-05-04 07:38:09,958]: Mean: -0.0000
[2025-05-04 07:38:09,959]: Std: 0.0251
[2025-05-04 07:38:09,959]: Min: -0.0911
[2025-05-04 07:38:09,960]: Max: 0.0838
[2025-05-04 07:38:09,960]: 
Parameter: layer3.2.bn1.weight
[2025-05-04 07:38:09,960]: Shape: torch.Size([64])
[2025-05-04 07:38:09,961]: Sample Values (16 elements): [0.9849346876144409, 0.9698091745376587, 0.980477511882782, 0.9871460795402527, 0.9762731194496155, 0.9822962284088135, 0.9750763773918152, 0.9794443845748901, 0.9772241115570068, 0.9984651803970337, 0.9831852912902832, 0.9797125458717346, 0.9834531545639038, 0.9910104274749756, 0.9806479811668396, 0.9757443070411682]
[2025-05-04 07:38:09,961]: Mean: 0.9807
[2025-05-04 07:38:09,962]: Std: 0.0088
[2025-05-04 07:38:09,963]: Min: 0.9650
[2025-05-04 07:38:09,964]: Max: 1.0050
[2025-05-04 07:38:09,964]: 
Parameter: layer3.2.bn1.bias
[2025-05-04 07:38:09,964]: Shape: torch.Size([64])
[2025-05-04 07:38:09,965]: Sample Values (16 elements): [0.01814902387559414, -0.014413236640393734, -0.0038815364241600037, -0.013655397109687328, 0.017082294449210167, -0.004912720061838627, -0.035870376974344254, -0.00688865315169096, -0.0015216843457892537, -0.050234828144311905, -0.02487003244459629, 0.011793631128966808, -0.013589339330792427, 0.02497432380914688, -0.010180873796343803, 0.014104041270911694]
[2025-05-04 07:38:09,966]: Mean: -0.0024
[2025-05-04 07:38:09,966]: Std: 0.0184
[2025-05-04 07:38:09,967]: Min: -0.0502
[2025-05-04 07:38:09,967]: Max: 0.0480
[2025-05-04 07:38:09,967]: 
Parameter: layer3.2.conv2.weight
[2025-05-04 07:38:09,967]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:38:09,997]: Sample Values (16 elements): [0.03307189419865608, -0.01670856401324272, 0.04017443209886551, 0.0073168626986444, 0.009809075854718685, 0.011580977588891983, -0.03456663340330124, -0.0005931409541517496, 0.0002742133801802993, -0.02396349608898163, -0.03111603483557701, -0.006419554352760315, 0.01962091028690338, -0.040705908089876175, 0.003223198000341654, 0.003309054998680949]
[2025-05-04 07:38:09,998]: Mean: -0.0002
[2025-05-04 07:38:09,998]: Std: 0.0242
[2025-05-04 07:38:09,999]: Min: -0.0747
[2025-05-04 07:38:09,999]: Max: 0.0709
[2025-05-04 07:38:09,999]: 
Parameter: layer3.2.bn2.weight
[2025-05-04 07:38:10,000]: Shape: torch.Size([64])
[2025-05-04 07:38:10,000]: Sample Values (16 elements): [1.043564796447754, 1.0292541980743408, 1.0262680053710938, 1.0108455419540405, 1.0322389602661133, 1.0246808528900146, 1.0310248136520386, 1.0372281074523926, 1.044644832611084, 1.0511841773986816, 1.0365279912948608, 1.0329954624176025, 1.0200004577636719, 1.0090670585632324, 1.025708556175232, 1.032118320465088]
[2025-05-04 07:38:10,001]: Mean: 1.0319
[2025-05-04 07:38:10,001]: Std: 0.0109
[2025-05-04 07:38:10,002]: Min: 1.0034
[2025-05-04 07:38:10,002]: Max: 1.0531
[2025-05-04 07:38:10,003]: 
Parameter: layer3.2.bn2.bias
[2025-05-04 07:38:10,003]: Shape: torch.Size([64])
[2025-05-04 07:38:10,003]: Sample Values (16 elements): [0.01486234925687313, 0.03223418444395065, 0.09082326292991638, -0.07560113817453384, 0.03200472891330719, 0.05379252880811691, 0.08573828637599945, -0.03489859402179718, -0.050507813692092896, 0.022291069850325584, -0.03385338559746742, -0.02050120197236538, -0.008032852783799171, 0.051707737147808075, 0.05452946573495865, -0.047658342868089676]
[2025-05-04 07:38:10,004]: Mean: 0.0058
[2025-05-04 07:38:10,007]: Std: 0.0418
[2025-05-04 07:38:10,007]: Min: -0.0987
[2025-05-04 07:38:10,008]: Max: 0.0908
[2025-05-04 07:38:10,008]: 
Parameter: fc.weight
[2025-05-04 07:38:10,008]: Shape: torch.Size([10, 64])
[2025-05-04 07:38:10,009]: Sample Values (16 elements): [-0.2946303188800812, 0.24193623661994934, -0.33405017852783203, 0.20639051496982574, -0.1864587366580963, 0.2827949821949005, 0.2906413972377777, -0.219810351729393, -0.0626726821064949, 0.2761569321155548, 0.3747428357601166, -0.14288832247257233, 0.06157820299267769, -0.3683554232120514, 0.3760252296924591, -0.303697407245636]
[2025-05-04 07:38:10,010]: Mean: -0.0024
[2025-05-04 07:38:10,011]: Std: 0.2316
[2025-05-04 07:38:10,015]: Min: -0.4546
[2025-05-04 07:38:10,020]: Max: 0.5041
[2025-05-04 07:38:10,020]: 
Parameter: fc.bias
[2025-05-04 07:38:10,021]: Shape: torch.Size([10])
[2025-05-04 07:38:10,035]: Sample Values (10 elements): [0.17165391147136688, -0.20044748485088348, 0.0656646117568016, -0.25547271966934204, -0.22856022417545319, -0.03272764012217522, 0.4432232081890106, 0.14475974440574646, 0.2544611394405365, 0.09833972901105881]
[2025-05-04 07:38:10,048]: Mean: 0.0461
[2025-05-04 07:38:10,049]: Std: 0.2268
[2025-05-04 07:38:10,049]: Min: -0.2555
[2025-05-04 07:38:10,050]: Max: 0.4432
[2025-05-04 07:38:10,050]: 


QAT of ResNet20 with hardtanh down to 4 bits...
[2025-05-04 07:38:10,700]: [ResNet20_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-04 07:38:11,091]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-04 07:40:18,782]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.6561 Train Acc: 0.7711 Eval Loss: 0.7275 Eval Acc: 0.7488 (LR: 0.001000)
[2025-05-04 07:42:35,055]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.6371 Train Acc: 0.7761 Eval Loss: 0.7300 Eval Acc: 0.7538 (LR: 0.001000)
[2025-05-04 07:44:49,291]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.6335 Train Acc: 0.7767 Eval Loss: 0.7011 Eval Acc: 0.7545 (LR: 0.001000)
[2025-05-04 07:46:54,291]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.6243 Train Acc: 0.7835 Eval Loss: 0.7552 Eval Acc: 0.7454 (LR: 0.001000)
[2025-05-04 07:49:03,107]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.6151 Train Acc: 0.7837 Eval Loss: 0.7547 Eval Acc: 0.7485 (LR: 0.001000)
[2025-05-04 07:51:11,755]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.6085 Train Acc: 0.7871 Eval Loss: 0.7128 Eval Acc: 0.7535 (LR: 0.001000)
[2025-05-04 07:53:23,712]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.6057 Train Acc: 0.7879 Eval Loss: 0.7118 Eval Acc: 0.7585 (LR: 0.001000)
[2025-05-04 07:55:36,870]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.6102 Train Acc: 0.7879 Eval Loss: 0.7773 Eval Acc: 0.7396 (LR: 0.001000)
[2025-05-04 07:57:50,052]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.6010 Train Acc: 0.7895 Eval Loss: 0.6876 Eval Acc: 0.7656 (LR: 0.001000)
[2025-05-04 08:00:01,803]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.5935 Train Acc: 0.7910 Eval Loss: 0.6872 Eval Acc: 0.7643 (LR: 0.001000)
[2025-05-04 08:02:14,275]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.5895 Train Acc: 0.7927 Eval Loss: 0.6523 Eval Acc: 0.7795 (LR: 0.001000)
[2025-05-04 08:04:27,252]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.5801 Train Acc: 0.7964 Eval Loss: 0.6341 Eval Acc: 0.7796 (LR: 0.001000)
[2025-05-04 08:06:39,308]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.5755 Train Acc: 0.8005 Eval Loss: 0.6585 Eval Acc: 0.7719 (LR: 0.001000)
[2025-05-04 08:08:45,647]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.5772 Train Acc: 0.7998 Eval Loss: 0.5970 Eval Acc: 0.7962 (LR: 0.001000)
[2025-05-04 08:12:33,145]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.5642 Train Acc: 0.8038 Eval Loss: 0.6651 Eval Acc: 0.7696 (LR: 0.001000)
[2025-05-04 08:19:46,942]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.5709 Train Acc: 0.8014 Eval Loss: 0.6342 Eval Acc: 0.7865 (LR: 0.001000)
