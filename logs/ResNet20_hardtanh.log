[2025-05-12 08:36:47,145]: 
Training ResNet20 with hardtanh
[2025-05-12 08:38:21,803]: [ResNet20_hardtanh] Epoch: 001 Train Loss: 2.0436 Train Acc: 0.2254 Eval Loss: 1.9208 Eval Acc: 0.2860 (LR: 0.001000)
[2025-05-12 08:39:54,045]: [ResNet20_hardtanh] Epoch: 002 Train Loss: 1.8142 Train Acc: 0.3288 Eval Loss: 1.8112 Eval Acc: 0.3438 (LR: 0.001000)
[2025-05-12 08:41:17,702]: [ResNet20_hardtanh] Epoch: 003 Train Loss: 1.6697 Train Acc: 0.3819 Eval Loss: 1.5670 Eval Acc: 0.4237 (LR: 0.001000)
[2025-05-12 08:42:33,194]: [ResNet20_hardtanh] Epoch: 004 Train Loss: 1.5584 Train Acc: 0.4285 Eval Loss: 1.4867 Eval Acc: 0.4531 (LR: 0.001000)
[2025-05-12 08:44:00,973]: [ResNet20_hardtanh] Epoch: 005 Train Loss: 1.4640 Train Acc: 0.4645 Eval Loss: 1.3906 Eval Acc: 0.4845 (LR: 0.001000)
[2025-05-12 08:45:24,880]: [ResNet20_hardtanh] Epoch: 006 Train Loss: 1.3902 Train Acc: 0.4941 Eval Loss: 1.3615 Eval Acc: 0.5008 (LR: 0.001000)
[2025-05-12 08:46:49,382]: [ResNet20_hardtanh] Epoch: 007 Train Loss: 1.3317 Train Acc: 0.5192 Eval Loss: 1.2979 Eval Acc: 0.5310 (LR: 0.001000)
[2025-05-12 08:48:16,542]: [ResNet20_hardtanh] Epoch: 008 Train Loss: 1.2831 Train Acc: 0.5378 Eval Loss: 1.2235 Eval Acc: 0.5579 (LR: 0.001000)
[2025-05-12 08:49:46,708]: [ResNet20_hardtanh] Epoch: 009 Train Loss: 1.2364 Train Acc: 0.5556 Eval Loss: 1.2727 Eval Acc: 0.5454 (LR: 0.001000)
[2025-05-12 08:51:16,968]: [ResNet20_hardtanh] Epoch: 010 Train Loss: 1.1988 Train Acc: 0.5696 Eval Loss: 1.2193 Eval Acc: 0.5590 (LR: 0.001000)
[2025-05-12 08:52:44,318]: [ResNet20_hardtanh] Epoch: 011 Train Loss: 1.1662 Train Acc: 0.5793 Eval Loss: 1.2293 Eval Acc: 0.5605 (LR: 0.001000)
[2025-05-12 08:54:09,883]: [ResNet20_hardtanh] Epoch: 012 Train Loss: 1.1367 Train Acc: 0.5924 Eval Loss: 1.1437 Eval Acc: 0.5894 (LR: 0.001000)
[2025-05-12 08:55:25,539]: [ResNet20_hardtanh] Epoch: 013 Train Loss: 1.1103 Train Acc: 0.6027 Eval Loss: 1.0942 Eval Acc: 0.6116 (LR: 0.001000)
[2025-05-12 08:56:41,188]: [ResNet20_hardtanh] Epoch: 014 Train Loss: 1.0890 Train Acc: 0.6106 Eval Loss: 1.0993 Eval Acc: 0.6031 (LR: 0.001000)
[2025-05-12 08:58:08,519]: [ResNet20_hardtanh] Epoch: 015 Train Loss: 1.0629 Train Acc: 0.6210 Eval Loss: 1.1155 Eval Acc: 0.5953 (LR: 0.001000)
[2025-05-12 08:59:26,257]: [ResNet20_hardtanh] Epoch: 016 Train Loss: 1.0431 Train Acc: 0.6287 Eval Loss: 1.0728 Eval Acc: 0.6249 (LR: 0.001000)
[2025-05-12 09:00:24,066]: [ResNet20_hardtanh] Epoch: 017 Train Loss: 1.0280 Train Acc: 0.6338 Eval Loss: 1.0033 Eval Acc: 0.6448 (LR: 0.001000)
[2025-05-12 09:01:47,394]: [ResNet20_hardtanh] Epoch: 018 Train Loss: 1.0038 Train Acc: 0.6421 Eval Loss: 0.9871 Eval Acc: 0.6567 (LR: 0.001000)
[2025-05-12 09:03:10,033]: [ResNet20_hardtanh] Epoch: 019 Train Loss: 0.9877 Train Acc: 0.6478 Eval Loss: 0.9701 Eval Acc: 0.6514 (LR: 0.001000)
[2025-05-12 09:04:37,178]: [ResNet20_hardtanh] Epoch: 020 Train Loss: 0.9794 Train Acc: 0.6512 Eval Loss: 0.9844 Eval Acc: 0.6512 (LR: 0.001000)
[2025-05-12 09:06:09,913]: [ResNet20_hardtanh] Epoch: 021 Train Loss: 0.9624 Train Acc: 0.6588 Eval Loss: 0.9306 Eval Acc: 0.6707 (LR: 0.001000)
[2025-05-12 09:07:39,343]: [ResNet20_hardtanh] Epoch: 022 Train Loss: 0.9463 Train Acc: 0.6644 Eval Loss: 0.9360 Eval Acc: 0.6709 (LR: 0.001000)
[2025-05-12 09:09:09,747]: [ResNet20_hardtanh] Epoch: 023 Train Loss: 0.9315 Train Acc: 0.6690 Eval Loss: 0.9205 Eval Acc: 0.6778 (LR: 0.001000)
[2025-05-12 09:10:42,043]: [ResNet20_hardtanh] Epoch: 024 Train Loss: 0.9149 Train Acc: 0.6772 Eval Loss: 0.9322 Eval Acc: 0.6711 (LR: 0.001000)
[2025-05-12 09:12:07,850]: [ResNet20_hardtanh] Epoch: 025 Train Loss: 0.9055 Train Acc: 0.6813 Eval Loss: 0.9212 Eval Acc: 0.6773 (LR: 0.001000)
[2025-05-12 09:13:42,517]: [ResNet20_hardtanh] Epoch: 026 Train Loss: 0.8916 Train Acc: 0.6841 Eval Loss: 0.8856 Eval Acc: 0.6895 (LR: 0.001000)
[2025-05-12 09:15:09,524]: [ResNet20_hardtanh] Epoch: 027 Train Loss: 0.8797 Train Acc: 0.6878 Eval Loss: 0.8634 Eval Acc: 0.6967 (LR: 0.001000)
[2025-05-12 09:16:35,666]: [ResNet20_hardtanh] Epoch: 028 Train Loss: 0.8742 Train Acc: 0.6915 Eval Loss: 0.9015 Eval Acc: 0.6851 (LR: 0.001000)
[2025-05-12 09:18:09,300]: [ResNet20_hardtanh] Epoch: 029 Train Loss: 0.8584 Train Acc: 0.6980 Eval Loss: 0.8616 Eval Acc: 0.7007 (LR: 0.001000)
[2025-05-12 09:19:43,669]: [ResNet20_hardtanh] Epoch: 030 Train Loss: 0.8508 Train Acc: 0.7002 Eval Loss: 0.8451 Eval Acc: 0.7045 (LR: 0.001000)
[2025-05-12 09:21:11,354]: [ResNet20_hardtanh] Epoch: 031 Train Loss: 0.8421 Train Acc: 0.7025 Eval Loss: 0.8803 Eval Acc: 0.6929 (LR: 0.001000)
[2025-05-12 09:22:44,480]: [ResNet20_hardtanh] Epoch: 032 Train Loss: 0.8284 Train Acc: 0.7092 Eval Loss: 0.8829 Eval Acc: 0.6876 (LR: 0.001000)
[2025-05-12 09:24:11,428]: [ResNet20_hardtanh] Epoch: 033 Train Loss: 0.8162 Train Acc: 0.7118 Eval Loss: 0.8815 Eval Acc: 0.6937 (LR: 0.001000)
[2025-05-12 09:25:42,419]: [ResNet20_hardtanh] Epoch: 034 Train Loss: 0.8108 Train Acc: 0.7130 Eval Loss: 0.8423 Eval Acc: 0.7096 (LR: 0.001000)
[2025-05-12 09:27:17,169]: [ResNet20_hardtanh] Epoch: 035 Train Loss: 0.8013 Train Acc: 0.7168 Eval Loss: 0.8483 Eval Acc: 0.7055 (LR: 0.001000)
[2025-05-12 09:28:47,895]: [ResNet20_hardtanh] Epoch: 036 Train Loss: 0.7943 Train Acc: 0.7212 Eval Loss: 0.8220 Eval Acc: 0.7141 (LR: 0.001000)
[2025-05-12 09:30:20,113]: [ResNet20_hardtanh] Epoch: 037 Train Loss: 0.7842 Train Acc: 0.7252 Eval Loss: 0.8418 Eval Acc: 0.7118 (LR: 0.001000)
[2025-05-12 09:31:56,669]: [ResNet20_hardtanh] Epoch: 038 Train Loss: 0.7750 Train Acc: 0.7269 Eval Loss: 0.8257 Eval Acc: 0.7158 (LR: 0.001000)
[2025-05-12 09:33:26,729]: [ResNet20_hardtanh] Epoch: 039 Train Loss: 0.7730 Train Acc: 0.7293 Eval Loss: 0.7958 Eval Acc: 0.7246 (LR: 0.001000)
[2025-05-12 09:35:01,626]: [ResNet20_hardtanh] Epoch: 040 Train Loss: 0.7670 Train Acc: 0.7289 Eval Loss: 0.7899 Eval Acc: 0.7251 (LR: 0.001000)
[2025-05-12 09:36:36,432]: [ResNet20_hardtanh] Epoch: 041 Train Loss: 0.7515 Train Acc: 0.7360 Eval Loss: 0.7705 Eval Acc: 0.7331 (LR: 0.001000)
[2025-05-12 09:38:08,918]: [ResNet20_hardtanh] Epoch: 042 Train Loss: 0.7493 Train Acc: 0.7395 Eval Loss: 0.7684 Eval Acc: 0.7318 (LR: 0.001000)
[2025-05-12 09:39:45,694]: [ResNet20_hardtanh] Epoch: 043 Train Loss: 0.7457 Train Acc: 0.7384 Eval Loss: 0.7570 Eval Acc: 0.7382 (LR: 0.001000)
[2025-05-12 09:41:26,426]: [ResNet20_hardtanh] Epoch: 044 Train Loss: 0.7339 Train Acc: 0.7426 Eval Loss: 0.7633 Eval Acc: 0.7365 (LR: 0.001000)
[2025-05-12 09:43:07,871]: [ResNet20_hardtanh] Epoch: 045 Train Loss: 0.7256 Train Acc: 0.7458 Eval Loss: 0.8026 Eval Acc: 0.7237 (LR: 0.001000)
[2025-05-12 09:44:36,458]: [ResNet20_hardtanh] Epoch: 046 Train Loss: 0.7246 Train Acc: 0.7458 Eval Loss: 0.7718 Eval Acc: 0.7351 (LR: 0.001000)
[2025-05-12 09:45:55,213]: [ResNet20_hardtanh] Epoch: 047 Train Loss: 0.7181 Train Acc: 0.7476 Eval Loss: 0.7574 Eval Acc: 0.7397 (LR: 0.001000)
[2025-05-12 09:47:11,154]: [ResNet20_hardtanh] Epoch: 048 Train Loss: 0.7101 Train Acc: 0.7509 Eval Loss: 0.7264 Eval Acc: 0.7490 (LR: 0.001000)
[2025-05-12 09:48:28,170]: [ResNet20_hardtanh] Epoch: 049 Train Loss: 0.7008 Train Acc: 0.7541 Eval Loss: 0.7672 Eval Acc: 0.7374 (LR: 0.001000)
[2025-05-12 09:49:47,084]: [ResNet20_hardtanh] Epoch: 050 Train Loss: 0.6919 Train Acc: 0.7561 Eval Loss: 0.7586 Eval Acc: 0.7378 (LR: 0.001000)
[2025-05-12 09:51:01,890]: [ResNet20_hardtanh] Epoch: 051 Train Loss: 0.6942 Train Acc: 0.7565 Eval Loss: 0.7941 Eval Acc: 0.7348 (LR: 0.001000)
[2025-05-12 09:52:17,818]: [ResNet20_hardtanh] Epoch: 052 Train Loss: 0.6895 Train Acc: 0.7584 Eval Loss: 0.7252 Eval Acc: 0.7530 (LR: 0.001000)
[2025-05-12 09:53:35,307]: [ResNet20_hardtanh] Epoch: 053 Train Loss: 0.6825 Train Acc: 0.7636 Eval Loss: 0.7433 Eval Acc: 0.7418 (LR: 0.001000)
[2025-05-12 09:54:53,550]: [ResNet20_hardtanh] Epoch: 054 Train Loss: 0.6778 Train Acc: 0.7628 Eval Loss: 0.7095 Eval Acc: 0.7496 (LR: 0.001000)
[2025-05-12 09:56:06,000]: [ResNet20_hardtanh] Epoch: 055 Train Loss: 0.6666 Train Acc: 0.7670 Eval Loss: 0.6912 Eval Acc: 0.7654 (LR: 0.001000)
[2025-05-12 09:57:16,982]: [ResNet20_hardtanh] Epoch: 056 Train Loss: 0.6683 Train Acc: 0.7649 Eval Loss: 0.7054 Eval Acc: 0.7541 (LR: 0.001000)
[2025-05-12 09:58:30,671]: [ResNet20_hardtanh] Epoch: 057 Train Loss: 0.6614 Train Acc: 0.7695 Eval Loss: 0.7076 Eval Acc: 0.7544 (LR: 0.001000)
[2025-05-12 09:59:30,474]: [ResNet20_hardtanh] Epoch: 058 Train Loss: 0.6556 Train Acc: 0.7696 Eval Loss: 0.6924 Eval Acc: 0.7584 (LR: 0.001000)
[2025-05-12 10:00:29,243]: [ResNet20_hardtanh] Epoch: 059 Train Loss: 0.6509 Train Acc: 0.7723 Eval Loss: 0.7350 Eval Acc: 0.7484 (LR: 0.001000)
[2025-05-12 10:01:22,492]: [ResNet20_hardtanh] Epoch: 060 Train Loss: 0.6468 Train Acc: 0.7754 Eval Loss: 0.7041 Eval Acc: 0.7595 (LR: 0.001000)
[2025-05-12 10:02:28,424]: [ResNet20_hardtanh] Epoch: 061 Train Loss: 0.6397 Train Acc: 0.7744 Eval Loss: 0.6869 Eval Acc: 0.7625 (LR: 0.001000)
[2025-05-12 10:03:35,683]: [ResNet20_hardtanh] Epoch: 062 Train Loss: 0.6385 Train Acc: 0.7773 Eval Loss: 0.7375 Eval Acc: 0.7502 (LR: 0.001000)
[2025-05-12 10:04:45,100]: [ResNet20_hardtanh] Epoch: 063 Train Loss: 0.6327 Train Acc: 0.7804 Eval Loss: 0.6880 Eval Acc: 0.7619 (LR: 0.001000)
[2025-05-12 10:05:54,490]: [ResNet20_hardtanh] Epoch: 064 Train Loss: 0.6227 Train Acc: 0.7841 Eval Loss: 0.6825 Eval Acc: 0.7676 (LR: 0.001000)
[2025-05-12 10:07:00,672]: [ResNet20_hardtanh] Epoch: 065 Train Loss: 0.6240 Train Acc: 0.7828 Eval Loss: 0.7021 Eval Acc: 0.7660 (LR: 0.001000)
[2025-05-12 10:08:10,839]: [ResNet20_hardtanh] Epoch: 066 Train Loss: 0.6250 Train Acc: 0.7843 Eval Loss: 0.6903 Eval Acc: 0.7622 (LR: 0.001000)
[2025-05-12 10:09:13,676]: [ResNet20_hardtanh] Epoch: 067 Train Loss: 0.6178 Train Acc: 0.7833 Eval Loss: 0.6653 Eval Acc: 0.7731 (LR: 0.001000)
[2025-05-12 10:10:21,309]: [ResNet20_hardtanh] Epoch: 068 Train Loss: 0.6140 Train Acc: 0.7868 Eval Loss: 0.7331 Eval Acc: 0.7502 (LR: 0.001000)
[2025-05-12 10:11:20,488]: [ResNet20_hardtanh] Epoch: 069 Train Loss: 0.6065 Train Acc: 0.7884 Eval Loss: 0.7270 Eval Acc: 0.7503 (LR: 0.001000)
[2025-05-12 10:12:23,522]: [ResNet20_hardtanh] Epoch: 070 Train Loss: 0.6021 Train Acc: 0.7915 Eval Loss: 0.6667 Eval Acc: 0.7758 (LR: 0.000100)
[2025-05-12 10:13:13,008]: [ResNet20_hardtanh] Epoch: 071 Train Loss: 0.5490 Train Acc: 0.8085 Eval Loss: 0.5958 Eval Acc: 0.7974 (LR: 0.000100)
[2025-05-12 10:14:16,374]: [ResNet20_hardtanh] Epoch: 072 Train Loss: 0.5369 Train Acc: 0.8140 Eval Loss: 0.5981 Eval Acc: 0.7977 (LR: 0.000100)
[2025-05-12 10:15:15,646]: [ResNet20_hardtanh] Epoch: 073 Train Loss: 0.5292 Train Acc: 0.8159 Eval Loss: 0.5968 Eval Acc: 0.7981 (LR: 0.000100)
[2025-05-12 10:16:26,240]: [ResNet20_hardtanh] Epoch: 074 Train Loss: 0.5303 Train Acc: 0.8157 Eval Loss: 0.6019 Eval Acc: 0.7942 (LR: 0.000100)
[2025-05-12 10:17:31,917]: [ResNet20_hardtanh] Epoch: 075 Train Loss: 0.5264 Train Acc: 0.8187 Eval Loss: 0.5943 Eval Acc: 0.7977 (LR: 0.000100)
[2025-05-12 10:18:41,679]: [ResNet20_hardtanh] Epoch: 076 Train Loss: 0.5259 Train Acc: 0.8178 Eval Loss: 0.5929 Eval Acc: 0.8009 (LR: 0.000100)
[2025-05-12 10:19:51,628]: [ResNet20_hardtanh] Epoch: 077 Train Loss: 0.5263 Train Acc: 0.8175 Eval Loss: 0.5959 Eval Acc: 0.7980 (LR: 0.000100)
[2025-05-12 10:20:58,716]: [ResNet20_hardtanh] Epoch: 078 Train Loss: 0.5228 Train Acc: 0.8189 Eval Loss: 0.5930 Eval Acc: 0.7977 (LR: 0.000100)
[2025-05-12 10:22:11,681]: [ResNet20_hardtanh] Epoch: 079 Train Loss: 0.5226 Train Acc: 0.8183 Eval Loss: 0.5915 Eval Acc: 0.8003 (LR: 0.000100)
[2025-05-12 10:23:21,292]: [ResNet20_hardtanh] Epoch: 080 Train Loss: 0.5199 Train Acc: 0.8201 Eval Loss: 0.5944 Eval Acc: 0.7968 (LR: 0.000100)
[2025-05-12 10:24:31,146]: [ResNet20_hardtanh] Epoch: 081 Train Loss: 0.5201 Train Acc: 0.8191 Eval Loss: 0.5912 Eval Acc: 0.7972 (LR: 0.000100)
[2025-05-12 10:25:45,973]: [ResNet20_hardtanh] Epoch: 082 Train Loss: 0.5220 Train Acc: 0.8174 Eval Loss: 0.5933 Eval Acc: 0.7988 (LR: 0.000100)
[2025-05-12 10:27:03,487]: [ResNet20_hardtanh] Epoch: 083 Train Loss: 0.5163 Train Acc: 0.8209 Eval Loss: 0.5878 Eval Acc: 0.8016 (LR: 0.000100)
[2025-05-12 10:28:18,772]: [ResNet20_hardtanh] Epoch: 084 Train Loss: 0.5200 Train Acc: 0.8186 Eval Loss: 0.5944 Eval Acc: 0.7976 (LR: 0.000100)
[2025-05-12 10:29:30,693]: [ResNet20_hardtanh] Epoch: 085 Train Loss: 0.5147 Train Acc: 0.8230 Eval Loss: 0.5889 Eval Acc: 0.8007 (LR: 0.000100)
[2025-05-12 10:30:40,962]: [ResNet20_hardtanh] Epoch: 086 Train Loss: 0.5150 Train Acc: 0.8208 Eval Loss: 0.5924 Eval Acc: 0.7981 (LR: 0.000100)
[2025-05-12 10:31:55,077]: [ResNet20_hardtanh] Epoch: 087 Train Loss: 0.5194 Train Acc: 0.8191 Eval Loss: 0.5911 Eval Acc: 0.7991 (LR: 0.000100)
[2025-05-12 10:33:04,873]: [ResNet20_hardtanh] Epoch: 088 Train Loss: 0.5162 Train Acc: 0.8199 Eval Loss: 0.5888 Eval Acc: 0.7976 (LR: 0.000100)
[2025-05-12 10:34:16,185]: [ResNet20_hardtanh] Epoch: 089 Train Loss: 0.5154 Train Acc: 0.8213 Eval Loss: 0.5993 Eval Acc: 0.7967 (LR: 0.000100)
[2025-05-12 10:35:28,952]: [ResNet20_hardtanh] Epoch: 090 Train Loss: 0.5159 Train Acc: 0.8208 Eval Loss: 0.5898 Eval Acc: 0.8001 (LR: 0.000100)
[2025-05-12 10:36:43,606]: [ResNet20_hardtanh] Epoch: 091 Train Loss: 0.5118 Train Acc: 0.8219 Eval Loss: 0.5923 Eval Acc: 0.7999 (LR: 0.000100)
[2025-05-12 10:37:54,042]: [ResNet20_hardtanh] Epoch: 092 Train Loss: 0.5074 Train Acc: 0.8248 Eval Loss: 0.5911 Eval Acc: 0.8000 (LR: 0.000100)
[2025-05-12 10:39:02,324]: [ResNet20_hardtanh] Epoch: 093 Train Loss: 0.5121 Train Acc: 0.8220 Eval Loss: 0.5902 Eval Acc: 0.8011 (LR: 0.000100)
[2025-05-12 10:40:12,716]: [ResNet20_hardtanh] Epoch: 094 Train Loss: 0.5085 Train Acc: 0.8228 Eval Loss: 0.5876 Eval Acc: 0.7992 (LR: 0.000100)
[2025-05-12 10:41:23,536]: [ResNet20_hardtanh] Epoch: 095 Train Loss: 0.5122 Train Acc: 0.8206 Eval Loss: 0.5907 Eval Acc: 0.7997 (LR: 0.000100)
[2025-05-12 10:42:26,419]: [ResNet20_hardtanh] Epoch: 096 Train Loss: 0.5085 Train Acc: 0.8240 Eval Loss: 0.5871 Eval Acc: 0.8005 (LR: 0.000100)
[2025-05-12 10:43:31,355]: [ResNet20_hardtanh] Epoch: 097 Train Loss: 0.5053 Train Acc: 0.8258 Eval Loss: 0.5886 Eval Acc: 0.8001 (LR: 0.000100)
[2025-05-12 10:44:17,879]: [ResNet20_hardtanh] Epoch: 098 Train Loss: 0.5061 Train Acc: 0.8250 Eval Loss: 0.5962 Eval Acc: 0.7989 (LR: 0.000100)
[2025-05-12 10:45:18,712]: [ResNet20_hardtanh] Epoch: 099 Train Loss: 0.5052 Train Acc: 0.8252 Eval Loss: 0.5891 Eval Acc: 0.7995 (LR: 0.000100)
[2025-05-12 10:46:18,458]: [ResNet20_hardtanh] Epoch: 100 Train Loss: 0.5072 Train Acc: 0.8237 Eval Loss: 0.5938 Eval Acc: 0.7959 (LR: 0.000010)
[2025-05-12 10:47:26,280]: [ResNet20_hardtanh] Epoch: 101 Train Loss: 0.5004 Train Acc: 0.8260 Eval Loss: 0.5805 Eval Acc: 0.8025 (LR: 0.000010)
[2025-05-12 10:48:35,344]: [ResNet20_hardtanh] Epoch: 102 Train Loss: 0.5003 Train Acc: 0.8274 Eval Loss: 0.5797 Eval Acc: 0.8028 (LR: 0.000010)
[2025-05-12 10:49:43,536]: [ResNet20_hardtanh] Epoch: 103 Train Loss: 0.5005 Train Acc: 0.8278 Eval Loss: 0.5833 Eval Acc: 0.8031 (LR: 0.000010)
[2025-05-12 10:50:57,117]: [ResNet20_hardtanh] Epoch: 104 Train Loss: 0.5002 Train Acc: 0.8271 Eval Loss: 0.5811 Eval Acc: 0.8024 (LR: 0.000010)
[2025-05-12 10:52:06,402]: [ResNet20_hardtanh] Epoch: 105 Train Loss: 0.4981 Train Acc: 0.8283 Eval Loss: 0.5795 Eval Acc: 0.8028 (LR: 0.000010)
[2025-05-12 10:53:18,361]: [ResNet20_hardtanh] Epoch: 106 Train Loss: 0.4967 Train Acc: 0.8280 Eval Loss: 0.5785 Eval Acc: 0.8017 (LR: 0.000010)
[2025-05-12 10:54:31,068]: [ResNet20_hardtanh] Epoch: 107 Train Loss: 0.4984 Train Acc: 0.8274 Eval Loss: 0.5818 Eval Acc: 0.8025 (LR: 0.000010)
[2025-05-12 10:55:36,967]: [ResNet20_hardtanh] Epoch: 108 Train Loss: 0.4971 Train Acc: 0.8282 Eval Loss: 0.5790 Eval Acc: 0.8018 (LR: 0.000010)
[2025-05-12 10:56:48,162]: [ResNet20_hardtanh] Epoch: 109 Train Loss: 0.4977 Train Acc: 0.8275 Eval Loss: 0.5812 Eval Acc: 0.8013 (LR: 0.000010)
[2025-05-12 10:57:54,658]: [ResNet20_hardtanh] Epoch: 110 Train Loss: 0.4974 Train Acc: 0.8264 Eval Loss: 0.5805 Eval Acc: 0.8026 (LR: 0.000010)
[2025-05-12 10:59:03,280]: [ResNet20_hardtanh] Epoch: 111 Train Loss: 0.4969 Train Acc: 0.8276 Eval Loss: 0.5823 Eval Acc: 0.8023 (LR: 0.000010)
[2025-05-12 11:00:09,450]: [ResNet20_hardtanh] Epoch: 112 Train Loss: 0.5015 Train Acc: 0.8265 Eval Loss: 0.5840 Eval Acc: 0.8019 (LR: 0.000010)
[2025-05-12 11:01:10,327]: [ResNet20_hardtanh] Epoch: 113 Train Loss: 0.4946 Train Acc: 0.8284 Eval Loss: 0.5804 Eval Acc: 0.8020 (LR: 0.000010)
[2025-05-12 11:02:12,065]: [ResNet20_hardtanh] Epoch: 114 Train Loss: 0.5008 Train Acc: 0.8256 Eval Loss: 0.5803 Eval Acc: 0.8024 (LR: 0.000010)
[2025-05-12 11:03:06,122]: [ResNet20_hardtanh] Epoch: 115 Train Loss: 0.4952 Train Acc: 0.8287 Eval Loss: 0.5801 Eval Acc: 0.8023 (LR: 0.000010)
[2025-05-12 11:04:06,246]: [ResNet20_hardtanh] Epoch: 116 Train Loss: 0.4949 Train Acc: 0.8298 Eval Loss: 0.5776 Eval Acc: 0.8022 (LR: 0.000010)
[2025-05-12 11:04:54,984]: [ResNet20_hardtanh] Epoch: 117 Train Loss: 0.4966 Train Acc: 0.8274 Eval Loss: 0.5798 Eval Acc: 0.8019 (LR: 0.000010)
[2025-05-12 11:05:47,311]: [ResNet20_hardtanh] Epoch: 118 Train Loss: 0.4944 Train Acc: 0.8298 Eval Loss: 0.5803 Eval Acc: 0.8027 (LR: 0.000010)
[2025-05-12 11:06:29,676]: [ResNet20_hardtanh] Epoch: 119 Train Loss: 0.4935 Train Acc: 0.8289 Eval Loss: 0.5832 Eval Acc: 0.8022 (LR: 0.000010)
[2025-05-12 11:07:15,948]: [ResNet20_hardtanh] Epoch: 120 Train Loss: 0.4933 Train Acc: 0.8290 Eval Loss: 0.5829 Eval Acc: 0.8010 (LR: 0.000010)
[2025-05-12 11:07:15,952]: [ResNet20_hardtanh] Best Eval Accuracy: 0.8031
[2025-05-12 11:07:15,978]: 
Training of full-precision model finished!
[2025-05-12 11:07:15,978]: Model Architecture:
[2025-05-12 11:07:15,980]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:07:15,980]: 
Model Weights:
[2025-05-12 11:07:15,980]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 11:07:15,992]: Sample Values (25 elements): [-0.06710654497146606, 0.014120065607130527, 0.08181387931108475, 0.07811412960290909, 0.017640141770243645, 0.07140887528657913, 0.1550607532262802, 0.09721076488494873, 0.15519823133945465, 0.034271303564310074, 0.14903442561626434, 0.19512979686260223, -0.056871216744184494, -0.2977442443370819, -0.07626011967658997, 0.13996651768684387, -0.007265271153301001, -0.17960277199745178, -0.0658833384513855, -0.08466539531946182, -0.17047704756259918, 0.15013353526592255, 0.3471350073814392, -0.05460178479552269, -0.09969425946474075]
[2025-05-12 11:07:15,997]: Mean: -0.00559618
[2025-05-12 11:07:16,002]: Min: -0.40589619
[2025-05-12 11:07:16,003]: Max: 0.61920595
[2025-05-12 11:07:16,003]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,003]: Sample Values (16 elements): [0.7387691736221313, 1.0191501379013062, 0.8758656978607178, 0.7765699028968811, 1.1721898317337036, 0.7499456405639648, 0.7222785353660583, 1.0378423929214478, 0.6163099408149719, 0.9542068243026733, 0.7952638864517212, 0.8052526116371155, 0.9735123515129089, 0.921705961227417, 0.8783934712409973, 0.8505398035049438]
[2025-05-12 11:07:16,005]: Mean: 0.86798728
[2025-05-12 11:07:16,005]: Min: 0.61630994
[2025-05-12 11:07:16,005]: Max: 1.17218983
[2025-05-12 11:07:16,005]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:07:16,005]: Sample Values (25 elements): [-0.041953738778829575, 0.03767836466431618, -0.039562128484249115, 0.11880449950695038, 0.04349863901734352, 0.12141961604356766, -0.001141968066804111, -0.038271673023700714, 0.039088647812604904, -0.05425785481929779, 0.06063266471028328, 0.006970527116209269, 0.05564228817820549, -0.061911698430776596, 0.09812946617603302, -0.05405111610889435, -0.06876551359891891, 0.01630837470293045, -0.024482110515236855, 0.037610381841659546, 0.01118421833962202, 0.01099899411201477, -0.06691291183233261, 0.02116885408759117, 0.028559047728776932]
[2025-05-12 11:07:16,006]: Mean: 0.00130292
[2025-05-12 11:07:16,006]: Min: -0.20857435
[2025-05-12 11:07:16,007]: Max: 0.21743931
[2025-05-12 11:07:16,007]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,007]: Sample Values (16 elements): [0.9380256533622742, 0.7897098064422607, 0.8850247859954834, 0.9758754968643188, 0.89500492811203, 0.9613305926322937, 0.8172377347946167, 1.126208782196045, 1.003354787826538, 1.0040758848190308, 0.982144832611084, 1.1166857481002808, 1.0597176551818848, 0.8995134234428406, 1.0317952632904053, 1.0371112823486328]
[2025-05-12 11:07:16,007]: Mean: 0.97017604
[2025-05-12 11:07:16,008]: Min: 0.78970981
[2025-05-12 11:07:16,008]: Max: 1.12620878
[2025-05-12 11:07:16,008]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:07:16,008]: Sample Values (25 elements): [0.00766789261251688, 0.0265814159065485, 0.006737684831023216, 0.10094130039215088, -0.021256353706121445, -0.044462114572525024, 0.06662257015705109, 0.05238475278019905, 0.01951221004128456, 0.20596228539943695, -0.034323371946811676, -0.02398691698908806, -0.03363746032118797, -0.13187170028686523, -0.048013560473918915, 0.06047647073864937, -0.11606746166944504, 0.07388392090797424, -0.02054368145763874, 0.09470652788877487, -0.09949416667222977, 0.0630473867058754, -0.05324719846248627, 0.0945596694946289, -0.12940284609794617]
[2025-05-12 11:07:16,008]: Mean: -0.00214027
[2025-05-12 11:07:16,010]: Min: -0.35861132
[2025-05-12 11:07:16,010]: Max: 0.36197340
[2025-05-12 11:07:16,010]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,011]: Sample Values (16 elements): [0.8926534652709961, 0.8748024702072144, 0.9570059776306152, 0.8902842998504639, 0.9797749519348145, 0.982847273349762, 0.935391366481781, 1.1166075468063354, 0.9312354326248169, 0.847939133644104, 1.1284655332565308, 0.8591601848602295, 0.7591565251350403, 0.8183475136756897, 0.732628583908081, 1.0185537338256836]
[2025-05-12 11:07:16,011]: Mean: 0.92030334
[2025-05-12 11:07:16,011]: Min: 0.73262858
[2025-05-12 11:07:16,011]: Max: 1.12846553
[2025-05-12 11:07:16,012]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:07:16,012]: Sample Values (25 elements): [-0.0013364609330892563, 0.07464289665222168, -0.07014096528291702, 0.03474809601902962, -0.01336650364100933, 0.05874421447515488, 0.058932527899742126, 0.015564145520329475, 0.08448901027441025, -0.08659625053405762, 0.020003188401460648, -0.013232991099357605, 0.05611739307641983, 0.024476096034049988, 0.06382571160793304, -0.003991972655057907, 0.0341777428984642, 0.14606855809688568, -0.09484781324863434, -0.07099544256925583, -0.05550394579768181, -0.1264544278383255, 0.0888124629855156, 0.013738838955760002, -0.08353124558925629]
[2025-05-12 11:07:16,012]: Mean: -0.00087144
[2025-05-12 11:07:16,013]: Min: -0.30022040
[2025-05-12 11:07:16,013]: Max: 0.29175600
[2025-05-12 11:07:16,013]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,013]: Sample Values (16 elements): [0.9704026579856873, 0.986193060874939, 1.0450185537338257, 1.0450172424316406, 0.9689418077468872, 0.9228833913803101, 0.9321334958076477, 1.0774943828582764, 0.9383089542388916, 0.973828136920929, 0.9516894221305847, 1.1083868741989136, 0.9668499827384949, 1.0565719604492188, 1.023725152015686, 0.9541776180267334]
[2025-05-12 11:07:16,013]: Mean: 0.99510145
[2025-05-12 11:07:16,014]: Min: 0.92288339
[2025-05-12 11:07:16,014]: Max: 1.10838687
[2025-05-12 11:07:16,014]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:07:16,015]: Sample Values (25 elements): [-0.028879767283797264, 0.058799684047698975, 0.06202739477157593, 0.03837296739220619, -0.06080026552081108, -0.03169906139373779, -0.13770076632499695, 0.002040096092969179, 0.019692417234182358, -0.07414120435714722, -0.022486774250864983, 0.07731771469116211, 0.022489158436655998, -0.05885132774710655, 0.06773374229669571, 0.012911040335893631, 0.005624863784760237, -0.030231570824980736, 0.0558510348200798, 0.01477106288075447, 0.00039220252074301243, 0.10753122717142105, -0.03251158818602562, -0.058934830129146576, 0.032429568469524384]
[2025-05-12 11:07:16,015]: Mean: 0.00068176
[2025-05-12 11:07:16,016]: Min: -0.23154797
[2025-05-12 11:07:16,016]: Max: 0.20009813
[2025-05-12 11:07:16,016]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,017]: Sample Values (16 elements): [0.8848782181739807, 0.8587960600852966, 0.8778522610664368, 0.859171450138092, 0.7834091186523438, 0.8788039684295654, 1.0326753854751587, 0.9363625645637512, 0.8831413388252258, 0.8679351806640625, 0.932420551776886, 0.9012235403060913, 0.8404868245124817, 0.8568863868713379, 0.9611141681671143, 0.9483618140220642]
[2025-05-12 11:07:16,017]: Mean: 0.89396989
[2025-05-12 11:07:16,017]: Min: 0.78340912
[2025-05-12 11:07:16,017]: Max: 1.03267539
[2025-05-12 11:07:16,017]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:07:16,018]: Sample Values (25 elements): [-0.006630023010075092, -0.01394590549170971, -0.06367875635623932, 0.008754902519285679, 0.04018358141183853, -0.022037459537386894, -0.040425315499305725, 0.0203886516392231, -0.057245660573244095, 0.01661662757396698, 0.016129720956087112, 0.013496605679392815, -0.011038322933018208, -0.0456467866897583, 0.031375713646411896, 0.03247404471039772, -0.056933771818876266, 0.021646840497851372, -0.09245473146438599, -0.10244064033031464, 0.04719245806336403, 0.03327255696058273, 0.07511090487241745, -0.0038223545998334885, -0.004915713332593441]
[2025-05-12 11:07:16,018]: Mean: 0.00339626
[2025-05-12 11:07:16,018]: Min: -0.17473345
[2025-05-12 11:07:16,019]: Max: 0.19472346
[2025-05-12 11:07:16,019]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,020]: Sample Values (16 elements): [1.044682502746582, 0.9501034021377563, 0.9415319561958313, 0.9089486598968506, 0.9727333784103394, 1.0183583498001099, 1.0038927793502808, 0.9263918399810791, 1.0041784048080444, 0.8886692523956299, 0.9241079688072205, 0.9575228691101074, 0.902671217918396, 0.936408519744873, 0.9308249354362488, 1.0401208400726318]
[2025-05-12 11:07:16,020]: Mean: 0.95944667
[2025-05-12 11:07:16,020]: Min: 0.88866925
[2025-05-12 11:07:16,021]: Max: 1.04468250
[2025-05-12 11:07:16,021]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:07:16,021]: Sample Values (25 elements): [-0.06866908818483353, 0.1516125351190567, 0.004956506192684174, 0.11598022282123566, 0.09581217169761658, -0.02632090449333191, -0.04019582271575928, -0.027502261102199554, 0.02779509872198105, 0.03269648924469948, -0.06474778056144714, 0.062214333564043045, -0.04957571253180504, 0.030894437804818153, -0.05016940459609032, 0.020695485174655914, 0.07029683887958527, -0.013617102988064289, -0.017812464386224747, -0.02181580662727356, -0.038333259522914886, 0.029534773901104927, 0.036287859082221985, -0.08270476013422012, -0.016648394986987114]
[2025-05-12 11:07:16,021]: Mean: -0.00080513
[2025-05-12 11:07:16,022]: Min: -0.18831533
[2025-05-12 11:07:16,022]: Max: 0.20721830
[2025-05-12 11:07:16,022]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:07:16,022]: Sample Values (16 elements): [0.8848997950553894, 0.9090638160705566, 0.934060275554657, 0.9042726159095764, 0.8636787533760071, 0.882529079914093, 0.8909710049629211, 0.8370172381401062, 0.9212539792060852, 0.909453272819519, 0.9242406487464905, 0.8921130299568176, 0.9082831144332886, 0.8892034888267517, 0.9267365336418152, 0.9423611164093018]
[2025-05-12 11:07:16,023]: Mean: 0.90125859
[2025-05-12 11:07:16,023]: Min: 0.83701724
[2025-05-12 11:07:16,023]: Max: 0.94236112
[2025-05-12 11:07:16,023]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 11:07:16,024]: Sample Values (25 elements): [0.0835830569267273, -0.07365289330482483, -0.04199571907520294, 0.049512118101119995, 0.052039772272109985, 0.005026055499911308, 0.030900325626134872, -0.0011919569224119186, -0.0005566107574850321, -0.04654494673013687, -0.025460155680775642, -0.01892564445734024, -0.015791691839694977, 0.008459548465907574, -0.02255111001431942, 0.031587231904268265, -0.05121488496661186, 0.06564998626708984, -0.01775370165705681, 0.12493152171373367, 0.0115372808650136, 0.09122560918331146, -0.052528202533721924, -0.04264596104621887, 0.06737072020769119]
[2025-05-12 11:07:16,024]: Mean: 0.00006360
[2025-05-12 11:07:16,025]: Min: -0.15253815
[2025-05-12 11:07:16,025]: Max: 0.15384626
[2025-05-12 11:07:16,025]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,025]: Sample Values (25 elements): [0.9659788608551025, 0.9555211067199707, 0.9430919289588928, 0.999999463558197, 0.9647893309593201, 0.9463250041007996, 0.968024492263794, 0.9427034854888916, 1.0010164976119995, 0.9642444252967834, 0.9708285331726074, 0.9815582036972046, 0.9373685121536255, 0.993942141532898, 0.9681532382965088, 0.9500492215156555, 0.9416123628616333, 0.9683364033699036, 0.9484237432479858, 0.9864459037780762, 0.9668054580688477, 0.9476452469825745, 0.9757165312767029, 0.9638987183570862, 0.9845760464668274]
[2025-05-12 11:07:16,025]: Mean: 0.96622384
[2025-05-12 11:07:16,026]: Min: 0.93736851
[2025-05-12 11:07:16,026]: Max: 1.00101650
[2025-05-12 11:07:16,026]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:07:16,027]: Sample Values (25 elements): [0.05795185640454292, -0.05773850902915001, -0.013379974290728569, 0.025526827201247215, -0.015116781927645206, -0.040470026433467865, -0.043009933084249496, 0.003977031446993351, 0.06392277777194977, 0.029408609494566917, -0.06068001687526703, -0.08797873556613922, -0.04169934242963791, -0.0505966916680336, 0.022145414724946022, -0.013160577975213528, -0.02062581107020378, 0.029937341809272766, 0.03509857505559921, -0.011620331555604935, 0.04718444496393204, -0.016921037808060646, -0.0037623404059559107, -0.03312549740076065, -0.014157066121697426]
[2025-05-12 11:07:16,027]: Mean: -0.00095472
[2025-05-12 11:07:16,028]: Min: -0.14877227
[2025-05-12 11:07:16,028]: Max: 0.12499410
[2025-05-12 11:07:16,028]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,028]: Sample Values (25 elements): [0.97178053855896, 0.9908244013786316, 0.9800589680671692, 0.9530459046363831, 0.9517461061477661, 0.9801832437515259, 1.0077362060546875, 0.904439389705658, 0.9846673607826233, 0.9551506638526917, 0.9994195699691772, 0.9574856758117676, 0.9412936568260193, 0.9697948694229126, 1.0035178661346436, 0.9985844492912292, 0.9906709790229797, 0.9592735767364502, 1.0089466571807861, 0.9703232645988464, 0.9693865180015564, 0.9288597702980042, 0.9688223004341125, 0.9824315905570984, 0.9836997389793396]
[2025-05-12 11:07:16,029]: Mean: 0.97061050
[2025-05-12 11:07:16,029]: Min: 0.90443939
[2025-05-12 11:07:16,029]: Max: 1.00894666
[2025-05-12 11:07:16,029]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 11:07:16,030]: Sample Values (25 elements): [-0.09369304031133652, 0.03475959971547127, -0.11618754267692566, 0.3037668764591217, 0.11045723408460617, 0.13496413826942444, 0.27504345774650574, -0.11333496868610382, 0.2594890296459198, 0.09068860858678818, 0.1055721566081047, 0.11226718872785568, -0.0298923347145319, 0.16917552053928375, 0.086458221077919, -0.21563562750816345, 0.07628165185451508, 0.009239538572728634, 0.18808682262897491, -0.18442532420158386, 0.08214682340621948, 0.026733703911304474, -0.000549035205040127, 0.023142952471971512, 0.09337973594665527]
[2025-05-12 11:07:16,030]: Mean: 0.00600446
[2025-05-12 11:07:16,030]: Min: -0.32686004
[2025-05-12 11:07:16,031]: Max: 0.30376688
[2025-05-12 11:07:16,031]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,031]: Sample Values (25 elements): [0.8779181838035583, 0.8891977667808533, 0.8830876350402832, 0.9138947129249573, 0.8815047144889832, 0.858059823513031, 0.8661209940910339, 0.9162993431091309, 0.8591514825820923, 0.8944228291511536, 0.8887106776237488, 0.8779824376106262, 0.8980060815811157, 0.9053494334220886, 0.897734522819519, 0.8884351849555969, 0.9431425333023071, 0.9119202494621277, 0.8934754133224487, 0.8957933783531189, 0.9351178407669067, 0.8491969108581543, 0.8828606009483337, 0.8985528349876404, 0.8682188987731934]
[2025-05-12 11:07:16,032]: Mean: 0.89439940
[2025-05-12 11:07:16,032]: Min: 0.84919691
[2025-05-12 11:07:16,032]: Max: 0.94740218
[2025-05-12 11:07:16,032]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:07:16,033]: Sample Values (25 elements): [-0.009575547650456429, 0.07142014056444168, 0.02384580485522747, -0.04562486708164215, -0.028572579845786095, 0.01540948823094368, -0.0354292131960392, -0.01353427954018116, -0.07411006093025208, -0.012413186021149158, 0.09334732592105865, -0.04301317036151886, 0.009079226292669773, -0.07851553708314896, 0.05790669098496437, -0.0010831363033503294, -0.02893281728029251, -0.01258398499339819, -0.015056004747748375, 0.08018004149198532, -0.06964095681905746, 0.05055282264947891, -0.04465622454881668, 0.04474753141403198, 0.06105444207787514]
[2025-05-12 11:07:16,033]: Mean: -0.00044330
[2025-05-12 11:07:16,034]: Min: -0.14385231
[2025-05-12 11:07:16,034]: Max: 0.12948371
[2025-05-12 11:07:16,034]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,034]: Sample Values (25 elements): [0.9958018660545349, 0.9502882361412048, 0.9945901036262512, 0.9631354212760925, 0.9879146814346313, 0.9970264434814453, 0.983138382434845, 1.0033786296844482, 0.9772312045097351, 1.001474142074585, 0.9813277721405029, 0.9651829600334167, 0.9823903441429138, 1.0064691305160522, 0.9782145023345947, 1.0002219676971436, 0.9454243779182434, 0.975816547870636, 0.9638772010803223, 0.9805909395217896, 0.9901580810546875, 1.0001635551452637, 0.9733566045761108, 0.9981900453567505, 0.9967503547668457]
[2025-05-12 11:07:16,035]: Mean: 0.98329532
[2025-05-12 11:07:16,035]: Min: 0.94542438
[2025-05-12 11:07:16,036]: Max: 1.00646913
[2025-05-12 11:07:16,036]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:07:16,036]: Sample Values (25 elements): [-0.03650442510843277, -0.01134511735290289, 0.01655433513224125, -0.02028433233499527, 0.022789575159549713, -0.10264382511377335, -0.03601537272334099, -0.0012836586683988571, -0.023010604083538055, 0.01667468063533306, -0.028797980397939682, -0.025740643963217735, -0.008391723036766052, -0.0507122166454792, 0.0726454108953476, -0.033565063029527664, -0.04745417833328247, 0.03673063963651657, -0.03824283555150032, 0.01276353094726801, 0.009879468008875847, -0.06459232419729233, 0.03499990701675415, -0.036611542105674744, 0.0471184179186821]
[2025-05-12 11:07:16,036]: Mean: 0.00066491
[2025-05-12 11:07:16,037]: Min: -0.13376336
[2025-05-12 11:07:16,037]: Max: 0.12678942
[2025-05-12 11:07:16,037]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,038]: Sample Values (25 elements): [0.9710970520973206, 1.0088610649108887, 0.9513573050498962, 0.9421125054359436, 0.9535993933677673, 0.9934736490249634, 0.9536833167076111, 0.9593080878257751, 0.9788340926170349, 0.9827685952186584, 0.99578458070755, 0.9642691612243652, 0.927056610584259, 0.9341568946838379, 0.9263777136802673, 0.9274951219558716, 0.9252982139587402, 0.9724839925765991, 0.9215243458747864, 0.9510171413421631, 0.9489039778709412, 1.0046067237854004, 0.9833122491836548, 0.9779326915740967, 0.9196605086326599]
[2025-05-12 11:07:16,038]: Mean: 0.95965457
[2025-05-12 11:07:16,038]: Min: 0.89025605
[2025-05-12 11:07:16,039]: Max: 1.00886106
[2025-05-12 11:07:16,039]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:07:16,039]: Sample Values (25 elements): [-0.03709862753748894, -0.028936676681041718, -0.01339737232774496, -0.03769233450293541, 0.04275915399193764, -0.10422524064779282, -0.06293541193008423, -0.003048897488042712, -0.0029503877740353346, 0.036448460072278976, 0.03199388459324837, 0.027192287147045135, -0.06008199602365494, 0.05482454597949982, -0.004390242509543896, -0.005680990405380726, 0.005745395086705685, -0.013295471668243408, 0.03254862502217293, -0.05973619967699051, 0.00261890422552824, -0.07424654811620712, -0.0193872582167387, 0.030375367030501366, -0.02855796180665493]
[2025-05-12 11:07:16,040]: Mean: -0.00027821
[2025-05-12 11:07:16,040]: Min: -0.11600928
[2025-05-12 11:07:16,040]: Max: 0.13822031
[2025-05-12 11:07:16,040]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,041]: Sample Values (25 elements): [0.9820966124534607, 0.9675243496894836, 1.0176689624786377, 0.9659000635147095, 0.9611701369285583, 1.0006541013717651, 0.9900438189506531, 0.964919924736023, 0.9831848740577698, 0.9921808242797852, 0.97563636302948, 0.9864400625228882, 0.9841402769088745, 0.9999167919158936, 0.9931212663650513, 0.971798300743103, 0.9597493410110474, 0.9929682016372681, 0.986581563949585, 0.9642810225486755, 0.9808698296546936, 0.964040994644165, 0.9804199934005737, 0.9894980788230896, 1.002569317817688]
[2025-05-12 11:07:16,041]: Mean: 0.98176867
[2025-05-12 11:07:16,041]: Min: 0.95974934
[2025-05-12 11:07:16,042]: Max: 1.01766896
[2025-05-12 11:07:16,042]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:07:16,042]: Sample Values (25 elements): [-0.05402626842260361, 0.012377102859318256, -0.013144932687282562, -0.053814977407455444, -0.013505457900464535, 0.06967901438474655, 0.04345971718430519, -0.009110726416110992, 0.011297029443085194, 0.02074006199836731, 0.013028419576585293, 0.025746775791049004, -0.04343600943684578, -0.024478452280163765, -0.018086375668644905, -0.034747254103422165, -0.02095569297671318, 0.039106208831071854, -0.06396321952342987, 0.032542724162340164, -0.016257259994745255, 0.048339784145355225, 0.0538996197283268, 0.044497422873973846, -0.06989976018667221]
[2025-05-12 11:07:16,043]: Mean: -0.00061738
[2025-05-12 11:07:16,043]: Min: -0.11833904
[2025-05-12 11:07:16,043]: Max: 0.11324416
[2025-05-12 11:07:16,043]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:07:16,044]: Sample Values (25 elements): [0.9837600588798523, 0.9639961123466492, 1.0130398273468018, 0.9743921756744385, 0.9385219812393188, 0.958286464214325, 0.9927345514297485, 1.0245171785354614, 0.9621343016624451, 0.9686293005943298, 0.9953714609146118, 0.9550560712814331, 0.9750943779945374, 1.0032652616500854, 0.9621630311012268, 0.957561194896698, 0.9898091554641724, 0.9180887341499329, 0.9546605348587036, 1.0065107345581055, 0.961048424243927, 0.9315930604934692, 0.9916171431541443, 0.9810779094696045, 0.9674359560012817]
[2025-05-12 11:07:16,044]: Mean: 0.97300690
[2025-05-12 11:07:16,044]: Min: 0.91808873
[2025-05-12 11:07:16,045]: Max: 1.02451718
[2025-05-12 11:07:16,045]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 11:07:16,045]: Sample Values (25 elements): [-0.028806297108530998, -0.014917991124093533, -0.012538067065179348, -0.03246743232011795, -0.04915745556354523, 0.04798390716314316, 0.0401788055896759, -0.023659244179725647, -0.023749081417918205, -0.06585730612277985, 0.01657298393547535, 0.029576918110251427, 0.01992136612534523, -0.011886881664395332, -0.057413097470998764, -0.04548950493335724, -0.029991421848535538, -0.012787400744855404, -0.003072635503485799, -0.030886420980095863, 0.04860125482082367, 0.017812324687838554, 0.06192498281598091, 0.007228074129670858, -0.04937824234366417]
[2025-05-12 11:07:16,046]: Mean: -0.00011634
[2025-05-12 11:07:16,046]: Min: -0.10254122
[2025-05-12 11:07:16,046]: Max: 0.10255671
[2025-05-12 11:07:16,046]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,047]: Sample Values (25 elements): [0.9623168706893921, 0.9816873669624329, 0.9714811444282532, 0.9503332376480103, 0.9772085547447205, 0.9938414096832275, 0.9518464803695679, 0.9837871193885803, 0.9993846416473389, 0.9763246774673462, 0.9748814702033997, 1.001796841621399, 0.9623635411262512, 0.9620324373245239, 0.9638859033584595, 0.9769515991210938, 0.9639261960983276, 0.9942388534545898, 0.973365843296051, 0.97515469789505, 0.9809342622756958, 0.9615731239318848, 0.9848254323005676, 0.9816635251045227, 0.9849402904510498]
[2025-05-12 11:07:16,049]: Mean: 0.97668773
[2025-05-12 11:07:16,052]: Min: 0.94973063
[2025-05-12 11:07:16,057]: Max: 1.01149404
[2025-05-12 11:07:16,057]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:07:16,072]: Sample Values (25 elements): [0.0018963932525366545, 0.011210808530449867, -0.01734265312552452, 0.0441967211663723, -0.006339426152408123, 0.01461811363697052, 0.020890774205327034, -0.019589705392718315, -0.04805264249444008, 0.006613734643906355, 0.014222736470401287, 0.012025713920593262, -0.03886261209845543, -0.008844082243740559, -0.023602034896612167, -0.00439831055700779, -0.01341839600354433, 0.03863096982240677, -0.02686796523630619, 0.012463977560400963, 0.022896600887179375, -0.007751000579446554, 0.01537858322262764, 0.016075771301984787, -0.00450317794457078]
[2025-05-12 11:07:16,080]: Mean: 0.00004227
[2025-05-12 11:07:16,083]: Min: -0.09248824
[2025-05-12 11:07:16,083]: Max: 0.09347939
[2025-05-12 11:07:16,083]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,084]: Sample Values (25 elements): [0.9994516372680664, 0.9798868894577026, 1.0017637014389038, 0.9771512150764465, 0.9829608798027039, 0.966312825679779, 0.9602412581443787, 0.9756050109863281, 0.9834465384483337, 0.9899205565452576, 0.992031455039978, 0.9740645289421082, 0.9838982224464417, 0.9722694754600525, 1.001496434211731, 1.029536485671997, 0.9892725944519043, 1.0220304727554321, 1.0309730768203735, 0.9823590517044067, 0.9875994920730591, 0.997542679309845, 0.9777259230613708, 0.9802283644676208, 0.9922153949737549]
[2025-05-12 11:07:16,084]: Mean: 0.98610020
[2025-05-12 11:07:16,084]: Min: 0.94863641
[2025-05-12 11:07:16,085]: Max: 1.03097308
[2025-05-12 11:07:16,085]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 11:07:16,086]: Sample Values (25 elements): [0.14269092679023743, -0.050200723111629486, -0.14366081357002258, 0.12573963403701782, 0.08814498782157898, -0.04215957224369049, -0.09813980013132095, 0.07068198174238205, -0.14815615117549896, -0.08162857592105865, -0.08511542528867722, 0.08853548020124435, -0.13413779437541962, 0.021025268360972404, -0.017290104180574417, 0.07636897265911102, 0.16870462894439697, 0.10484721511602402, -0.030425962060689926, 0.10630140453577042, -0.12290885299444199, -0.15615206956863403, 0.031142476946115494, -0.028781607747077942, 0.06275397539138794]
[2025-05-12 11:07:16,086]: Mean: 0.00105080
[2025-05-12 11:07:16,086]: Min: -0.22087625
[2025-05-12 11:07:16,087]: Max: 0.19815850
[2025-05-12 11:07:16,087]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,087]: Sample Values (25 elements): [0.9400131106376648, 0.9230440258979797, 0.931010901927948, 0.897499680519104, 0.9303720593452454, 0.9568955302238464, 0.9368566870689392, 0.9216005802154541, 0.9215908646583557, 0.9256731867790222, 0.931350827217102, 0.9338822960853577, 0.952694833278656, 0.9560728073120117, 0.951928436756134, 0.9203115701675415, 0.9518961906433105, 0.9311651587486267, 0.932794988155365, 0.9014183282852173, 0.8854414820671082, 0.9333941340446472, 0.9412907958030701, 0.9414293169975281, 0.9343026876449585]
[2025-05-12 11:07:16,087]: Mean: 0.93245757
[2025-05-12 11:07:16,088]: Min: 0.88487077
[2025-05-12 11:07:16,088]: Max: 0.96027815
[2025-05-12 11:07:16,088]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:07:16,089]: Sample Values (25 elements): [-0.06589598208665848, -0.029914185404777527, 0.000960326346103102, 0.022751959040760994, 0.010262916795909405, 0.013763240538537502, -0.0070710089057683945, -0.009991135448217392, 0.020457325503230095, 0.028544457629323006, -0.02882944606244564, 0.024266738444566727, -0.040184274315834045, -0.029295790940523148, -0.009426182135939598, 0.011973830871284008, -0.03884052485227585, -0.015596535988152027, -0.00037651642924174666, -0.02009126916527748, 0.00500842509791255, -0.017323467880487442, -0.004103749059140682, -0.03301764652132988, 0.03315483033657074]
[2025-05-12 11:07:16,090]: Mean: 0.00014577
[2025-05-12 11:07:16,090]: Min: -0.09079982
[2025-05-12 11:07:16,090]: Max: 0.09611505
[2025-05-12 11:07:16,090]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,091]: Sample Values (25 elements): [1.0018340349197388, 0.9889484643936157, 0.9885165095329285, 0.9866015911102295, 1.0011281967163086, 0.97760009765625, 0.9958323240280151, 0.977888822555542, 0.9854300022125244, 0.9974901676177979, 0.9637472629547119, 0.9874154925346375, 0.9930815100669861, 1.0139994621276855, 0.989223837852478, 0.9865907430648804, 1.0035927295684814, 1.027376413345337, 1.0115596055984497, 1.0081480741500854, 0.987764298915863, 0.9934774041175842, 0.9869031310081482, 0.9941695332527161, 1.0011413097381592]
[2025-05-12 11:07:16,091]: Mean: 0.99471474
[2025-05-12 11:07:16,091]: Min: 0.96374726
[2025-05-12 11:07:16,092]: Max: 1.03476250
[2025-05-12 11:07:16,092]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:07:16,093]: Sample Values (25 elements): [0.031812552362680435, 0.020667169243097305, -0.015367413870990276, -0.024344749748706818, -0.012738463468849659, -0.040792979300022125, -0.01877599023282528, -0.04051350802183151, -0.017507076263427734, -0.0013459071051329374, -0.003376936074346304, -0.003912228625267744, 0.004814819432795048, -0.008047143928706646, 0.023611390963196754, 0.01222783513367176, 0.03530076518654823, 0.02761014550924301, 0.0049500977620482445, -0.012572021223604679, -0.014420129358768463, -0.003976154141128063, -0.008208842016756535, 0.0038987724110484123, 0.024218715727329254]
[2025-05-12 11:07:16,094]: Mean: 0.00004190
[2025-05-12 11:07:16,094]: Min: -0.09278718
[2025-05-12 11:07:16,094]: Max: 0.08847722
[2025-05-12 11:07:16,094]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,095]: Sample Values (25 elements): [0.9949563145637512, 0.9748198986053467, 0.9987133145332336, 0.9974299669265747, 0.9907856583595276, 0.9904345273971558, 1.004669189453125, 1.0015047788619995, 0.9767366051673889, 0.9942441582679749, 1.0170013904571533, 0.9997042417526245, 0.9979076981544495, 0.9829830527305603, 0.977785587310791, 0.9852211475372314, 0.971860408782959, 1.008742332458496, 1.0054720640182495, 0.9961896538734436, 0.9878805875778198, 0.9800800681114197, 0.9879623055458069, 0.9977317452430725, 0.9815301299095154]
[2025-05-12 11:07:16,095]: Mean: 0.99050939
[2025-05-12 11:07:16,095]: Min: 0.96849948
[2025-05-12 11:07:16,096]: Max: 1.03757548
[2025-05-12 11:07:16,096]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:07:16,099]: Sample Values (25 elements): [-0.026857202872633934, -0.011354267597198486, -0.011258102022111416, -0.015387807972729206, 0.03928753361105919, -0.030878029763698578, 0.03276273235678673, 0.01733912155032158, -0.03251422941684723, 0.02563079260289669, -0.018892699852585793, 0.0038381745107471943, -0.027872715145349503, 0.015514682978391647, -0.0015487472992390394, 0.019816549494862556, -0.020446477457880974, 0.006550423800945282, 0.021293018013238907, -0.0277751125395298, -0.016144393011927605, 0.014130095019936562, -0.01165646594017744, -0.039142489433288574, -0.011761399917304516]
[2025-05-12 11:07:16,101]: Mean: -0.00012535
[2025-05-12 11:07:16,105]: Min: -0.08415398
[2025-05-12 11:07:16,110]: Max: 0.08338951
[2025-05-12 11:07:16,110]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,133]: Sample Values (25 elements): [0.9789844155311584, 0.9905281662940979, 0.9672814607620239, 0.9841182827949524, 0.9795821905136108, 0.9754820466041565, 0.987623929977417, 0.9837608337402344, 0.9749050140380859, 0.9817367196083069, 0.99949049949646, 0.9788249731063843, 0.986811637878418, 0.979057788848877, 0.9865171909332275, 0.9778776168823242, 0.9709933996200562, 0.9737735390663147, 0.9674890637397766, 0.978083074092865, 0.9635568261146545, 0.9718021154403687, 0.9928702116012573, 0.9792320728302002, 0.9645695090293884]
[2025-05-12 11:07:16,133]: Mean: 0.97900057
[2025-05-12 11:07:16,134]: Min: 0.96149832
[2025-05-12 11:07:16,134]: Max: 1.02484000
[2025-05-12 11:07:16,134]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:07:16,136]: Sample Values (25 elements): [0.005303396377712488, -0.027673892676830292, -0.005029600113630295, 0.000690483080688864, 0.024702122434973717, -0.028758099302649498, 0.04204072058200836, -0.0023927967995405197, 0.010184825398027897, 0.003947085700929165, 0.03248387202620506, -0.011624584905803204, 0.015890488401055336, 0.02032194659113884, -0.012784414924681187, -0.023539720103144646, -0.009175596758723259, 0.026269709691405296, 0.03300498425960541, -0.038732513785362244, -0.011227053590118885, 0.01182203646749258, -0.008389425463974476, 0.031993694603443146, -0.015657661482691765]
[2025-05-12 11:07:16,137]: Mean: -0.00015971
[2025-05-12 11:07:16,137]: Min: -0.07416751
[2025-05-12 11:07:16,137]: Max: 0.07107043
[2025-05-12 11:07:16,137]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:07:16,138]: Sample Values (25 elements): [1.029212236404419, 1.055196762084961, 1.0638941526412964, 1.0377287864685059, 1.0492658615112305, 1.0698896646499634, 1.020727515220642, 1.0280472040176392, 1.029396891593933, 1.0308767557144165, 1.021349549293518, 1.0442800521850586, 1.0440266132354736, 1.0214104652404785, 1.0235027074813843, 1.0302343368530273, 1.0384986400604248, 1.0386966466903687, 1.0152537822723389, 1.0529814958572388, 1.0547893047332764, 1.0302492380142212, 1.029700517654419, 1.0508991479873657, 1.0193036794662476]
[2025-05-12 11:07:16,138]: Mean: 1.03614593
[2025-05-12 11:07:16,139]: Min: 1.00165236
[2025-05-12 11:07:16,139]: Max: 1.07128561
[2025-05-12 11:07:16,139]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 11:07:16,140]: Sample Values (25 elements): [-0.1880926489830017, 0.35802748799324036, -0.09541632980108261, 0.3532482087612152, -0.11045738309621811, -0.26858165860176086, -0.012909633107483387, 0.3551398813724518, -0.08386287093162537, 0.23751245439052582, 0.2566467821598053, -0.031960878521203995, -0.23521849513053894, 0.1133589893579483, -0.22766241431236267, 0.17853336036205292, 0.11079946905374527, -0.00010475794988451526, -0.11214232444763184, 0.14507894217967987, -0.3080267012119293, 0.2693871557712555, -0.3529704213142395, -0.10156910121440887, -0.2267906367778778]
[2025-05-12 11:07:16,140]: Mean: 0.00308437
[2025-05-12 11:07:16,141]: Min: -0.44636309
[2025-05-12 11:07:16,142]: Max: 0.43676478
[2025-05-12 11:07:16,142]: 


QAT of ResNet20 with hardtanh down to 4 bits...
[2025-05-12 11:07:16,485]: [ResNet20_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-12 11:07:16,696]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:09:37,527]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.6856 Train Acc: 0.7607 Eval Loss: 0.7496 Eval Acc: 0.7451 (LR: 0.001000)
[2025-05-12 11:11:55,705]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.6672 Train Acc: 0.7667 Eval Loss: 0.7904 Eval Acc: 0.7344 (LR: 0.001000)
[2025-05-12 11:13:59,953]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.6595 Train Acc: 0.7694 Eval Loss: 0.6946 Eval Acc: 0.7631 (LR: 0.001000)
[2025-05-12 11:16:14,570]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.6519 Train Acc: 0.7709 Eval Loss: 0.7097 Eval Acc: 0.7556 (LR: 0.001000)
[2025-05-12 11:18:17,611]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.6441 Train Acc: 0.7740 Eval Loss: 0.7963 Eval Acc: 0.7325 (LR: 0.001000)
[2025-05-12 11:20:22,689]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.6405 Train Acc: 0.7756 Eval Loss: 0.7725 Eval Acc: 0.7398 (LR: 0.001000)
[2025-05-12 11:22:37,994]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.6299 Train Acc: 0.7807 Eval Loss: 0.7543 Eval Acc: 0.7461 (LR: 0.001000)
[2025-05-12 11:24:47,478]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.6257 Train Acc: 0.7824 Eval Loss: 0.9477 Eval Acc: 0.6881 (LR: 0.001000)
[2025-05-12 11:27:00,483]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.6228 Train Acc: 0.7808 Eval Loss: 0.7034 Eval Acc: 0.7631 (LR: 0.001000)
[2025-05-12 11:29:10,164]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.6086 Train Acc: 0.7868 Eval Loss: 0.7104 Eval Acc: 0.7523 (LR: 0.001000)
[2025-05-12 11:31:18,495]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.6092 Train Acc: 0.7865 Eval Loss: 0.7065 Eval Acc: 0.7594 (LR: 0.001000)
[2025-05-12 11:33:34,167]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.6115 Train Acc: 0.7850 Eval Loss: 0.7292 Eval Acc: 0.7537 (LR: 0.001000)
[2025-05-12 11:35:38,672]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.5979 Train Acc: 0.7910 Eval Loss: 0.6549 Eval Acc: 0.7766 (LR: 0.001000)
[2025-05-12 11:37:54,146]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.6010 Train Acc: 0.7900 Eval Loss: 0.6770 Eval Acc: 0.7709 (LR: 0.001000)
[2025-05-12 11:39:57,670]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.5924 Train Acc: 0.7921 Eval Loss: 0.7412 Eval Acc: 0.7483 (LR: 0.001000)
[2025-05-12 11:42:04,411]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.5893 Train Acc: 0.7961 Eval Loss: 0.6456 Eval Acc: 0.7824 (LR: 0.001000)
[2025-05-12 11:43:55,555]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.5834 Train Acc: 0.7971 Eval Loss: 0.6579 Eval Acc: 0.7722 (LR: 0.001000)
[2025-05-12 11:45:48,247]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.5824 Train Acc: 0.7985 Eval Loss: 0.6593 Eval Acc: 0.7765 (LR: 0.001000)
[2025-05-12 11:47:39,644]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.5825 Train Acc: 0.7971 Eval Loss: 0.7078 Eval Acc: 0.7621 (LR: 0.001000)
[2025-05-12 11:49:31,376]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.5712 Train Acc: 0.8005 Eval Loss: 0.7020 Eval Acc: 0.7666 (LR: 0.001000)
[2025-05-12 11:51:20,952]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.5678 Train Acc: 0.8023 Eval Loss: 0.6907 Eval Acc: 0.7668 (LR: 0.001000)
[2025-05-12 11:53:07,510]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.5643 Train Acc: 0.8029 Eval Loss: 0.6992 Eval Acc: 0.7647 (LR: 0.001000)
[2025-05-12 11:54:52,679]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.5601 Train Acc: 0.8033 Eval Loss: 0.6827 Eval Acc: 0.7717 (LR: 0.001000)
[2025-05-12 11:56:38,721]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.5594 Train Acc: 0.8038 Eval Loss: 0.6983 Eval Acc: 0.7626 (LR: 0.001000)
[2025-05-12 11:58:23,841]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.5555 Train Acc: 0.8071 Eval Loss: 0.6596 Eval Acc: 0.7786 (LR: 0.001000)
[2025-05-12 12:00:09,857]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.5490 Train Acc: 0.8086 Eval Loss: 0.6759 Eval Acc: 0.7720 (LR: 0.001000)
[2025-05-12 12:01:56,434]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.5543 Train Acc: 0.8049 Eval Loss: 0.6416 Eval Acc: 0.7808 (LR: 0.001000)
[2025-05-12 12:03:45,118]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.5406 Train Acc: 0.8111 Eval Loss: 0.6359 Eval Acc: 0.7846 (LR: 0.001000)
[2025-05-12 12:05:35,331]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.5409 Train Acc: 0.8122 Eval Loss: 0.6037 Eval Acc: 0.7983 (LR: 0.001000)
[2025-05-12 12:07:27,397]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.5369 Train Acc: 0.8129 Eval Loss: 0.7340 Eval Acc: 0.7581 (LR: 0.000250)
[2025-05-12 12:09:19,766]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.4802 Train Acc: 0.8325 Eval Loss: 0.5490 Eval Acc: 0.8104 (LR: 0.000250)
[2025-05-12 12:11:11,753]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.4706 Train Acc: 0.8366 Eval Loss: 0.5881 Eval Acc: 0.8004 (LR: 0.000250)
[2025-05-12 12:13:02,336]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.4696 Train Acc: 0.8380 Eval Loss: 0.5669 Eval Acc: 0.8082 (LR: 0.000250)
[2025-05-12 12:14:50,699]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.4673 Train Acc: 0.8364 Eval Loss: 0.5570 Eval Acc: 0.8111 (LR: 0.000250)
[2025-05-12 12:16:37,699]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.4666 Train Acc: 0.8374 Eval Loss: 0.5487 Eval Acc: 0.8149 (LR: 0.000250)
[2025-05-12 12:18:25,184]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.4676 Train Acc: 0.8389 Eval Loss: 0.5638 Eval Acc: 0.8045 (LR: 0.000250)
[2025-05-12 12:20:11,011]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.4621 Train Acc: 0.8404 Eval Loss: 0.5535 Eval Acc: 0.8151 (LR: 0.000250)
[2025-05-12 12:21:58,052]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.4611 Train Acc: 0.8400 Eval Loss: 0.5488 Eval Acc: 0.8152 (LR: 0.000250)
[2025-05-12 12:23:45,692]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.4602 Train Acc: 0.8401 Eval Loss: 0.5410 Eval Acc: 0.8172 (LR: 0.000250)
[2025-05-12 12:25:34,743]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.4579 Train Acc: 0.8411 Eval Loss: 0.5630 Eval Acc: 0.8105 (LR: 0.000250)
[2025-05-12 12:27:23,982]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.4588 Train Acc: 0.8410 Eval Loss: 0.5599 Eval Acc: 0.8114 (LR: 0.000250)
[2025-05-12 12:29:14,361]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.4539 Train Acc: 0.8432 Eval Loss: 0.5692 Eval Acc: 0.8112 (LR: 0.000250)
[2025-05-12 12:31:06,479]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.4612 Train Acc: 0.8410 Eval Loss: 0.5848 Eval Acc: 0.8031 (LR: 0.000250)
[2025-05-12 12:32:58,655]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.4526 Train Acc: 0.8440 Eval Loss: 0.5394 Eval Acc: 0.8206 (LR: 0.000250)
[2025-05-12 12:34:52,136]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.4597 Train Acc: 0.8404 Eval Loss: 0.5507 Eval Acc: 0.8157 (LR: 0.000063)
[2025-05-12 12:36:46,830]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.4377 Train Acc: 0.8481 Eval Loss: 0.5312 Eval Acc: 0.8177 (LR: 0.000063)
[2025-05-12 12:38:42,432]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.4342 Train Acc: 0.8498 Eval Loss: 0.5206 Eval Acc: 0.8196 (LR: 0.000063)
[2025-05-12 12:40:37,612]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.4311 Train Acc: 0.8509 Eval Loss: 0.5245 Eval Acc: 0.8223 (LR: 0.000063)
[2025-05-12 12:42:32,871]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.4291 Train Acc: 0.8511 Eval Loss: 0.5379 Eval Acc: 0.8195 (LR: 0.000063)
[2025-05-12 12:44:28,087]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.4255 Train Acc: 0.8521 Eval Loss: 0.5215 Eval Acc: 0.8244 (LR: 0.000063)
[2025-05-12 12:46:23,373]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.4256 Train Acc: 0.8516 Eval Loss: 0.5262 Eval Acc: 0.8236 (LR: 0.000063)
[2025-05-12 12:48:19,862]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.4308 Train Acc: 0.8514 Eval Loss: 0.5239 Eval Acc: 0.8216 (LR: 0.000063)
[2025-05-12 12:50:16,568]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.4271 Train Acc: 0.8513 Eval Loss: 0.5185 Eval Acc: 0.8220 (LR: 0.000063)
[2025-05-12 12:52:13,193]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.4225 Train Acc: 0.8554 Eval Loss: 0.5314 Eval Acc: 0.8203 (LR: 0.000063)
[2025-05-12 12:54:09,033]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.4256 Train Acc: 0.8527 Eval Loss: 0.5324 Eval Acc: 0.8205 (LR: 0.000063)
[2025-05-12 12:56:03,469]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.4280 Train Acc: 0.8526 Eval Loss: 0.5359 Eval Acc: 0.8163 (LR: 0.000063)
[2025-05-12 12:57:57,934]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.4267 Train Acc: 0.8518 Eval Loss: 0.5230 Eval Acc: 0.8241 (LR: 0.000063)
[2025-05-12 12:59:51,629]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.4273 Train Acc: 0.8524 Eval Loss: 0.5288 Eval Acc: 0.8204 (LR: 0.000063)
[2025-05-12 13:01:43,765]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.4255 Train Acc: 0.8519 Eval Loss: 0.5241 Eval Acc: 0.8220 (LR: 0.000063)
[2025-05-12 13:03:35,408]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.4305 Train Acc: 0.8510 Eval Loss: 0.5244 Eval Acc: 0.8215 (LR: 0.000063)
[2025-05-12 13:03:35,409]: [ResNet20_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8244
[2025-05-12 13:03:35,470]: 


Quantization of model down to 4 bits finished
[2025-05-12 13:03:35,470]: Model Architecture:
[2025-05-12 13:03:35,642]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0383], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2999310791492462, max_val=0.2744261622428894)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0677], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5079823136329651, max_val=0.5073425769805908)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0545], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39169779419898987, max_val=0.4257650375366211)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0407], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32486793398857117, max_val=0.28585323691368103)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0326], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23774857819080353, max_val=0.25088876485824585)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0319], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2177990972995758, max_val=0.2600552439689636)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0255], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1888166069984436, max_val=0.19406484067440033)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18206824362277985, max_val=0.14622752368450165)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0464], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3547787070274353, max_val=0.34115922451019287)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0220], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16463898122310638, max_val=0.16482676565647125)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0215], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1668703854084015, max_val=0.1559121012687683)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0202], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14795677363872528, max_val=0.15502287447452545)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0189], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14328043162822723, max_val=0.14060567319393158)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12353907525539398, max_val=0.1242070347070694)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0148], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11194485425949097, max_val=0.11028474569320679)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0302], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2336786538362503, max_val=0.21941539645195007)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11460266262292862, max_val=0.11580654233694077)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0137], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1126975566148758, max_val=0.09329192340373993)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09342028200626373, max_val=0.09586736559867859)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08000089973211288, max_val=0.08079478144645691)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:03:35,642]: 
Model Weights:
[2025-05-12 13:03:35,643]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 13:03:35,643]: Sample Values (25 elements): [0.16103102266788483, 0.20476935803890228, -0.306258887052536, -0.2777477204799652, 0.23133204877376556, 0.15236493945121765, -0.06188236176967621, -0.11080285161733627, -0.3336528539657593, -0.21825240552425385, 0.0620400495827198, 0.5805980563163757, -0.10676909983158112, -0.035127073526382446, -0.01991872303187847, 0.12724468111991882, -0.008432464674115181, 0.24467039108276367, 0.013735471293330193, 0.02218177542090416, -0.25228241086006165, 0.04777161777019501, -0.2681375741958618, 0.15229801833629608, -0.26139402389526367]
[2025-05-12 13:03:35,643]: Mean: -0.00496878
[2025-05-12 13:03:35,644]: Min: -0.50702578
[2025-05-12 13:03:35,644]: Max: 0.71288669
[2025-05-12 13:03:35,644]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,645]: Sample Values (16 elements): [0.5406114459037781, 0.5112253427505493, 0.600835382938385, 1.0060677528381348, 0.783126711845398, 0.8298618197441101, 0.7804523706436157, 0.7448822855949402, 0.5504748821258545, 0.8618637323379517, 0.636521577835083, 0.6814838647842407, 0.9139947295188904, 0.8121491074562073, 0.9667893648147583, 1.1750330924987793]
[2025-05-12 13:03:35,645]: Mean: 0.77471089
[2025-05-12 13:03:35,646]: Min: 0.51122534
[2025-05-12 13:03:35,646]: Max: 1.17503309
[2025-05-12 13:03:35,649]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:03:35,650]: Sample Values (25 elements): [-0.038290414959192276, 0.0, -0.11487124860286713, -0.038290414959192276, -0.038290414959192276, -0.07658082991838455, -0.038290414959192276, -0.038290414959192276, -0.07658082991838455, 0.0, 0.0, 0.11487124860286713, 0.0, 0.038290414959192276, 0.07658082991838455, 0.038290414959192276, -0.11487124860286713, 0.19145207107067108, 0.038290414959192276, 0.0, 0.0, 0.07658082991838455, 0.07658082991838455, 0.038290414959192276, 0.11487124860286713]
[2025-05-12 13:03:35,650]: Mean: 0.00305792
[2025-05-12 13:03:35,650]: Min: -0.30632332
[2025-05-12 13:03:35,651]: Max: 0.26803291
[2025-05-12 13:03:35,651]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,651]: Sample Values (16 elements): [0.9709801077842712, 0.6826882362365723, 1.0533509254455566, 0.8940607905387878, 0.7564743161201477, 0.9701243042945862, 1.1782410144805908, 1.037400722503662, 0.9682180285453796, 0.7770339250564575, 0.8080976605415344, 0.8537801504135132, 0.9376657009124756, 1.0771592855453491, 1.1596198081970215, 0.7600460052490234]
[2025-05-12 13:03:35,652]: Mean: 0.93030882
[2025-05-12 13:03:35,652]: Min: 0.68268824
[2025-05-12 13:03:35,653]: Max: 1.17824101
[2025-05-12 13:03:35,654]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:03:35,655]: Sample Values (25 elements): [0.2030649185180664, -0.2030649185180664, 0.06768830865621567, -0.06768830865621567, 0.06768830865621567, 0.06768830865621567, 0.06768830865621567, 0.06768830865621567, 0.13537661731243134, -0.13537661731243134, 0.0, -0.06768830865621567, 0.13537661731243134, -0.06768830865621567, 0.0, 0.0, 0.0, -0.06768830865621567, 0.0, 0.0, 0.0, -0.13537661731243134, 0.06768830865621567, 0.06768830865621567, 0.06768830865621567]
[2025-05-12 13:03:35,655]: Mean: -0.00408363
[2025-05-12 13:03:35,656]: Min: -0.54150647
[2025-05-12 13:03:35,656]: Max: 0.47381815
[2025-05-12 13:03:35,656]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,657]: Sample Values (16 elements): [0.7631725072860718, 0.7264557480812073, 0.8763688206672668, 0.772484302520752, 0.7443649172782898, 0.6065894365310669, 0.5477449297904968, 1.02006196975708, 0.6703143119812012, 0.8949587941169739, 0.7705414295196533, 0.8650652170181274, 0.7955824136734009, 1.0547075271606445, 1.046526551246643, 0.8251785039901733]
[2025-05-12 13:03:35,657]: Mean: 0.81125736
[2025-05-12 13:03:35,657]: Min: 0.54774493
[2025-05-12 13:03:35,657]: Max: 1.05470753
[2025-05-12 13:03:35,660]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:03:35,661]: Sample Values (25 elements): [0.0, -0.05449753627181053, 0.05449753627181053, 0.0, -0.05449753627181053, 0.0, -0.05449753627181053, -0.05449753627181053, -0.05449753627181053, 0.0, -0.05449753627181053, 0.0, 0.0, 0.10899507254362106, 0.0, 0.0, 0.0, 0.10899507254362106, 0.10899507254362106, -0.05449753627181053, -0.05449753627181053, 0.0, -0.10899507254362106, 0.0, -0.05449753627181053]
[2025-05-12 13:03:35,661]: Mean: -0.00096979
[2025-05-12 13:03:35,661]: Min: -0.38148275
[2025-05-12 13:03:35,661]: Max: 0.43598029
[2025-05-12 13:03:35,661]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,662]: Sample Values (16 elements): [0.9051822423934937, 1.1323481798171997, 1.0814684629440308, 0.8506507873535156, 0.9080017805099487, 1.1050130128860474, 0.9312611818313599, 1.0738635063171387, 0.9257566332817078, 1.0066416263580322, 0.9937461018562317, 0.859402060508728, 0.9305424094200134, 0.8535889387130737, 0.8883750438690186, 0.922875702381134]
[2025-05-12 13:03:35,662]: Mean: 0.96054482
[2025-05-12 13:03:35,662]: Min: 0.85065079
[2025-05-12 13:03:35,663]: Max: 1.13234818
[2025-05-12 13:03:35,665]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:03:35,665]: Sample Values (25 elements): [0.04071473330259323, 0.1221441999077797, 0.1221441999077797, 0.04071473330259323, -0.08142946660518646, -0.08142946660518646, 0.08142946660518646, -0.04071473330259323, -0.04071473330259323, 0.0, 0.16285893321037292, -0.04071473330259323, -0.1221441999077797, -0.04071473330259323, 0.04071473330259323, -0.1221441999077797, 0.1221441999077797, -0.08142946660518646, 0.04071473330259323, 0.08142946660518646, 0.0, -0.04071473330259323, 0.0, -0.16285893321037292, -0.04071473330259323]
[2025-05-12 13:03:35,665]: Mean: 0.00150206
[2025-05-12 13:03:35,666]: Min: -0.32571787
[2025-05-12 13:03:35,666]: Max: 0.28500313
[2025-05-12 13:03:35,666]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,667]: Sample Values (16 elements): [0.7510651350021362, 0.7383370399475098, 0.686947226524353, 0.6983736753463745, 0.7152187824249268, 0.8445647954940796, 0.6574940085411072, 0.8158320784568787, 0.6439087986946106, 0.788930356502533, 0.724155604839325, 1.0289208889007568, 0.924820601940155, 0.8076769113540649, 0.7358608245849609, 0.7657575607299805]
[2025-05-12 13:03:35,667]: Mean: 0.77049154
[2025-05-12 13:03:35,667]: Min: 0.64390880
[2025-05-12 13:03:35,667]: Max: 1.02892089
[2025-05-12 13:03:35,669]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:03:35,670]: Sample Values (25 elements): [0.03257592022418976, 0.06515184044837952, -0.09772776067256927, -0.03257592022418976, 0.03257592022418976, -0.06515184044837952, -0.03257592022418976, -0.03257592022418976, 0.03257592022418976, 0.06515184044837952, -0.06515184044837952, -0.03257592022418976, 0.09772776067256927, 0.03257592022418976, -0.03257592022418976, -0.03257592022418976, -0.03257592022418976, -0.03257592022418976, 0.1628796011209488, -0.09772776067256927, 0.09772776067256927, -0.03257592022418976, 0.0, -0.06515184044837952, 0.13030368089675903]
[2025-05-12 13:03:35,670]: Mean: 0.00335091
[2025-05-12 13:03:35,671]: Min: -0.22803144
[2025-05-12 13:03:35,671]: Max: 0.26060736
[2025-05-12 13:03:35,671]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,671]: Sample Values (16 elements): [0.9496446251869202, 1.046218752861023, 0.8862932324409485, 0.8903125524520874, 0.9877442121505737, 1.010912299156189, 0.8472455143928528, 0.9085351824760437, 1.0655769109725952, 0.9176738262176514, 0.9866757988929749, 0.7978401184082031, 0.892372190952301, 0.8704333305358887, 0.8587270379066467, 0.8386082649230957]
[2025-05-12 13:03:35,672]: Mean: 0.92217588
[2025-05-12 13:03:35,672]: Min: 0.79784012
[2025-05-12 13:03:35,672]: Max: 1.06557691
[2025-05-12 13:03:35,674]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:03:35,674]: Sample Values (25 elements): [0.09557124972343445, -0.1911424994468689, 0.1911424994468689, 0.0, 0.0, -0.03185708448290825, 0.09557124972343445, -0.15928542613983154, -0.03185708448290825, 0.0, 0.03185708448290825, 0.0, 0.09557124972343445, 0.03185708448290825, 0.03185708448290825, 0.0637141689658165, -0.127428337931633, 0.09557124972343445, -0.09557124972343445, 0.0637141689658165, 0.03185708448290825, -0.03185708448290825, -0.09557124972343445, 0.0, -0.09557124972343445]
[2025-05-12 13:03:35,675]: Mean: -0.00178366
[2025-05-12 13:03:35,675]: Min: -0.22299959
[2025-05-12 13:03:35,675]: Max: 0.25485668
[2025-05-12 13:03:35,675]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:03:35,676]: Sample Values (16 elements): [0.7554274797439575, 0.8304473161697388, 0.6664247512817383, 0.8706679940223694, 0.7468643188476562, 0.7843610048294067, 0.8015563488006592, 0.8338001370429993, 0.76590496301651, 0.7644858956336975, 0.7730358242988586, 0.9067003726959229, 0.8127331733703613, 0.8105825781822205, 0.7622870206832886, 0.7245290279388428]
[2025-05-12 13:03:35,676]: Mean: 0.78811300
[2025-05-12 13:03:35,676]: Min: 0.66642475
[2025-05-12 13:03:35,677]: Max: 0.90670037
[2025-05-12 13:03:35,678]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 13:03:35,679]: Sample Values (25 elements): [-0.12762713432312012, 0.051050856709480286, -0.12762713432312012, -0.025525428354740143, 0.0, 0.051050856709480286, -0.051050856709480286, -0.051050856709480286, 0.0, -0.10210171341896057, -0.10210171341896057, -0.051050856709480286, 0.025525428354740143, -0.051050856709480286, -0.07657628506422043, 0.025525428354740143, 0.051050856709480286, -0.051050856709480286, 0.051050856709480286, -0.10210171341896057, 0.051050856709480286, 0.0, 0.0, 0.025525428354740143, -0.051050856709480286]
[2025-05-12 13:03:35,679]: Mean: 0.00004431
[2025-05-12 13:03:35,680]: Min: -0.17867801
[2025-05-12 13:03:35,680]: Max: 0.20420343
[2025-05-12 13:03:35,680]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,680]: Sample Values (25 elements): [0.9632390737533569, 0.952540397644043, 0.9525154232978821, 0.9082449078559875, 0.9439828991889954, 0.922910749912262, 0.9828351140022278, 0.952461838722229, 0.9667450785636902, 0.918980062007904, 0.8840963840484619, 0.909397542476654, 0.8922324180603027, 0.9762845039367676, 0.927772581577301, 0.9571994543075562, 0.9161018133163452, 0.997983455657959, 0.986078679561615, 0.9226065874099731, 0.9951680302619934, 0.9443765878677368, 0.9435369372367859, 0.9393264651298523, 0.9690579175949097]
[2025-05-12 13:03:35,681]: Mean: 0.94469750
[2025-05-12 13:03:35,681]: Min: 0.88228333
[2025-05-12 13:03:35,681]: Max: 1.02655113
[2025-05-12 13:03:35,683]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:03:35,683]: Sample Values (25 elements): [0.0656590461730957, 0.0, 0.0, 0.021886348724365234, 0.0, 0.0, 0.0656590461730957, 0.0656590461730957, -0.0656590461730957, 0.0656590461730957, 0.021886348724365234, 0.04377269744873047, 0.04377269744873047, 0.021886348724365234, 0.021886348724365234, -0.08754539489746094, -0.021886348724365234, -0.021886348724365234, -0.021886348724365234, 0.021886348724365234, 0.0, -0.021886348724365234, -0.04377269744873047, 0.0, 0.0]
[2025-05-12 13:03:35,684]: Mean: -0.00118979
[2025-05-12 13:03:35,684]: Min: -0.17509079
[2025-05-12 13:03:35,684]: Max: 0.15320444
[2025-05-12 13:03:35,684]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,685]: Sample Values (25 elements): [0.942581057548523, 0.9619185924530029, 0.9687207341194153, 0.9390081763267517, 0.9213130474090576, 0.8906335234642029, 0.9346688389778137, 0.9003890752792358, 0.9349322319030762, 0.9353756308555603, 1.0024595260620117, 0.896239697933197, 0.9279355406761169, 0.9330374598503113, 0.934736430644989, 0.9508588910102844, 0.9349720478057861, 0.8936566710472107, 1.0245628356933594, 0.9405954480171204, 0.9557806253433228, 0.9640587568283081, 0.9707468748092651, 1.0086270570755005, 0.8944928646087646]
[2025-05-12 13:03:35,685]: Mean: 0.93635452
[2025-05-12 13:03:35,685]: Min: 0.82719016
[2025-05-12 13:03:35,686]: Max: 1.02456284
[2025-05-12 13:03:35,687]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 13:03:35,687]: Sample Values (25 elements): [0.09279193729162216, -0.04639596864581108, -0.2319798469543457, 0.13918790221214294, 0.18558387458324432, -0.2783758044242859, -0.18558387458324432, 0.04639596864581108, 0.09279193729162216, 0.18558387458324432, -0.04639596864581108, 0.04639596864581108, 0.13918790221214294, 0.13918790221214294, -0.13918790221214294, 0.04639596864581108, 0.0, -0.18558387458324432, 0.18558387458324432, 0.04639596864581108, -0.32477179169654846, -0.13918790221214294, 0.09279193729162216, -0.2783758044242859, 0.18558387458324432]
[2025-05-12 13:03:35,688]: Mean: 0.00634320
[2025-05-12 13:03:35,688]: Min: -0.37116775
[2025-05-12 13:03:35,688]: Max: 0.32477179
[2025-05-12 13:03:35,688]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,689]: Sample Values (25 elements): [0.8480120897293091, 0.8409008383750916, 0.8271620273590088, 0.903170108795166, 0.7617250680923462, 0.790663480758667, 0.8243982791900635, 0.7893868684768677, 0.8048105239868164, 0.7747688293457031, 0.8140811324119568, 0.8931601643562317, 0.7919090390205383, 0.7793270349502563, 0.8541241884231567, 0.7800034880638123, 0.7910138964653015, 0.8366519212722778, 0.8136393427848816, 0.7871679663658142, 0.8882440328598022, 0.8615107536315918, 0.7956384420394897, 0.8239598870277405, 0.8422908782958984]
[2025-05-12 13:03:35,689]: Mean: 0.82403338
[2025-05-12 13:03:35,689]: Min: 0.75288099
[2025-05-12 13:03:35,690]: Max: 0.90317011
[2025-05-12 13:03:35,691]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:03:35,692]: Sample Values (25 elements): [0.0, 0.08785761892795563, 0.06589321792125702, 0.043928809463977814, -0.021964404731988907, 0.0, -0.021964404731988907, 0.021964404731988907, -0.021964404731988907, 0.0, 0.021964404731988907, -0.06589321792125702, -0.021964404731988907, -0.08785761892795563, -0.06589321792125702, 0.043928809463977814, 0.0, -0.06589321792125702, 0.0, -0.06589321792125702, 0.0, 0.043928809463977814, 0.0, 0.0, 0.021964404731988907]
[2025-05-12 13:03:35,692]: Mean: -0.00050764
[2025-05-12 13:03:35,692]: Min: -0.15375084
[2025-05-12 13:03:35,692]: Max: 0.17571524
[2025-05-12 13:03:35,693]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,693]: Sample Values (25 elements): [0.9446877241134644, 1.0302232503890991, 0.9140809178352356, 0.9960781931877136, 0.982518196105957, 0.9801825284957886, 0.9222237467765808, 0.9873851537704468, 0.9415686726570129, 1.019654393196106, 0.9598335027694702, 0.95854252576828, 0.9707499742507935, 0.9979234933853149, 0.9862827062606812, 0.9792999029159546, 0.9658133387565613, 0.9796009659767151, 0.9486218690872192, 0.9632723331451416, 1.012555718421936, 0.9740217328071594, 0.9779431819915771, 0.9696856737136841, 0.9951611161231995]
[2025-05-12 13:03:35,693]: Mean: 0.97497791
[2025-05-12 13:03:35,700]: Min: 0.91408092
[2025-05-12 13:03:35,702]: Max: 1.03022325
[2025-05-12 13:03:35,718]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:03:35,727]: Sample Values (25 elements): [-0.04303771257400513, -0.06455656886100769, -0.021518856287002563, 0.06455656886100769, 0.0, 0.08607542514801025, 0.0, 0.04303771257400513, 0.0, 0.0, -0.04303771257400513, 0.0, -0.08607542514801025, -0.021518856287002563, -0.021518856287002563, 0.021518856287002563, 0.04303771257400513, 0.08607542514801025, -0.04303771257400513, 0.04303771257400513, -0.04303771257400513, 0.04303771257400513, -0.021518856287002563, 0.04303771257400513, -0.04303771257400513]
[2025-05-12 13:03:35,735]: Mean: 0.00054638
[2025-05-12 13:03:35,736]: Min: -0.17215085
[2025-05-12 13:03:35,736]: Max: 0.15063199
[2025-05-12 13:03:35,736]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,737]: Sample Values (25 elements): [0.879106879234314, 0.877471387386322, 0.8968967199325562, 0.8114804625511169, 0.9972499012947083, 0.8945160508155823, 0.9418661594390869, 0.9124605059623718, 0.9355984330177307, 0.9382362961769104, 0.9813665151596069, 0.9706003069877625, 0.9165827631950378, 0.9361647963523865, 0.8924443125724792, 0.8708340525627136, 0.8947283029556274, 0.8823733329772949, 0.9504806995391846, 0.8847585320472717, 0.9742027521133423, 0.9318256378173828, 0.9565402865409851, 0.9118748903274536, 0.868607759475708]
[2025-05-12 13:03:35,737]: Mean: 0.91266870
[2025-05-12 13:03:35,737]: Min: 0.81148046
[2025-05-12 13:03:35,737]: Max: 0.99724990
[2025-05-12 13:03:35,739]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:03:35,747]: Sample Values (25 elements): [-0.04039721190929413, 0.06059581786394119, 0.0, -0.06059581786394119, 0.0, 0.06059581786394119, -0.04039721190929413, -0.08079442381858826, 0.04039721190929413, 0.020198605954647064, -0.06059581786394119, 0.020198605954647064, -0.020198605954647064, -0.020198605954647064, 0.020198605954647064, -0.020198605954647064, 0.0, -0.04039721190929413, -0.06059581786394119, 0.04039721190929413, 0.0, 0.06059581786394119, -0.06059581786394119, -0.020198605954647064, 0.04039721190929413]
[2025-05-12 13:03:35,751]: Mean: -0.00010301
[2025-05-12 13:03:35,763]: Min: -0.14139023
[2025-05-12 13:03:35,774]: Max: 0.16158885
[2025-05-12 13:03:35,775]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,775]: Sample Values (25 elements): [0.9419182538986206, 0.9693862795829773, 0.9532493948936462, 0.9704568982124329, 0.9286012053489685, 0.9427013993263245, 0.9624052047729492, 0.9849691987037659, 0.9655262231826782, 0.9629155397415161, 0.9450579285621643, 0.984946072101593, 1.0261034965515137, 0.9789344072341919, 0.9863329529762268, 0.9715610146522522, 0.9617630839347839, 0.9671961665153503, 1.0001713037490845, 0.9556558728218079, 0.9756628274917603, 0.9478460550308228, 0.9832457304000854, 0.9827572703361511, 1.0103156566619873]
[2025-05-12 13:03:35,775]: Mean: 0.97159290
[2025-05-12 13:03:35,776]: Min: 0.92860121
[2025-05-12 13:03:35,776]: Max: 1.02610350
[2025-05-12 13:03:35,779]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:03:35,780]: Sample Values (25 elements): [-0.018925799056887627, 0.018925799056887627, 0.03785159811377525, -0.018925799056887627, -0.151406392455101, 0.0, 0.018925799056887627, -0.03785159811377525, -0.05677739530801773, -0.018925799056887627, -0.05677739530801773, 0.0, 0.03785159811377525, -0.03785159811377525, 0.0, -0.03785159811377525, 0.018925799056887627, 0.0, -0.018925799056887627, -0.05677739530801773, 0.018925799056887627, 0.0, 0.03785159811377525, -0.03785159811377525, -0.018925799056887627]
[2025-05-12 13:03:35,780]: Mean: -0.00071054
[2025-05-12 13:03:35,780]: Min: -0.15140639
[2025-05-12 13:03:35,780]: Max: 0.13248059
[2025-05-12 13:03:35,781]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:03:35,781]: Sample Values (25 elements): [0.9544139504432678, 0.9602514505386353, 0.9794096946716309, 0.9469590187072754, 0.9207329750061035, 0.9487024545669556, 0.9104976654052734, 0.9252264499664307, 0.9359837174415588, 0.9090175628662109, 0.9474181532859802, 0.943008303642273, 0.937183141708374, 0.9828406572341919, 0.9375783801078796, 0.833103597164154, 0.9230003952980042, 1.0144838094711304, 0.8940728306770325, 0.980665385723114, 0.9224193692207336, 0.9278870224952698, 0.93181312084198, 0.9602053165435791, 0.9310606122016907]
[2025-05-12 13:03:35,781]: Mean: 0.93649894
[2025-05-12 13:03:35,782]: Min: 0.83310360
[2025-05-12 13:03:35,782]: Max: 1.01448381
[2025-05-12 13:03:35,785]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 13:03:35,786]: Sample Values (25 elements): [0.049549274146556854, 0.0, -0.0330328494310379, 0.0, 0.0660656988620758, 0.049549274146556854, 0.0660656988620758, 0.049549274146556854, -0.0660656988620758, 0.049549274146556854, 0.049549274146556854, 0.01651642471551895, 0.0330328494310379, 0.0, 0.01651642471551895, 0.049549274146556854, 0.01651642471551895, -0.049549274146556854, -0.01651642471551895, -0.049549274146556854, 0.0, 0.0330328494310379, -0.0330328494310379, -0.01651642471551895, 0.01651642471551895]
[2025-05-12 13:03:35,786]: Mean: -0.00007437
[2025-05-12 13:03:35,787]: Min: -0.11561497
[2025-05-12 13:03:35,787]: Max: 0.13213140
[2025-05-12 13:03:35,787]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,787]: Sample Values (25 elements): [0.9779682159423828, 0.9587385058403015, 0.946779727935791, 0.9546079039573669, 0.9546356797218323, 0.9356614351272583, 0.9751899838447571, 0.9886648654937744, 0.9777601957321167, 0.9838556051254272, 0.9779480695724487, 0.9672818183898926, 0.9417617321014404, 0.979630172252655, 0.966106653213501, 0.9674739241600037, 0.9589774012565613, 0.9554128050804138, 0.9688394665718079, 0.9867401719093323, 0.9671755433082581, 0.959035336971283, 0.9681365489959717, 0.9214698672294617, 0.9722661972045898]
[2025-05-12 13:03:35,788]: Mean: 0.96425080
[2025-05-12 13:03:35,788]: Min: 0.92146987
[2025-05-12 13:03:35,789]: Max: 1.00743783
[2025-05-12 13:03:35,792]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:03:35,793]: Sample Values (25 elements): [0.08889149129390717, 0.04444574564695358, 0.0, 0.0, 0.0, 0.04444574564695358, 0.04444574564695358, -0.04444574564695358, 0.014815248548984528, -0.07407624274492264, 0.04444574564695358, -0.04444574564695358, 0.0, 0.04444574564695358, 0.0, 0.0, 0.0, -0.014815248548984528, 0.029630497097969055, -0.014815248548984528, 0.0, -0.014815248548984528, 0.014815248548984528, 0.0, 0.029630497097969055]
[2025-05-12 13:03:35,793]: Mean: 0.00010369
[2025-05-12 13:03:35,793]: Min: -0.11852199
[2025-05-12 13:03:35,793]: Max: 0.10370674
[2025-05-12 13:03:35,794]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,796]: Sample Values (25 elements): [0.9092562794685364, 0.9928414821624756, 0.9347352385520935, 0.960390567779541, 1.0418858528137207, 0.9813480377197266, 0.9643517136573792, 0.960548996925354, 0.9880884885787964, 0.9752991199493408, 0.9796804785728455, 0.9824267029762268, 0.9707796573638916, 0.9736637473106384, 0.9743483066558838, 0.9368005990982056, 0.9929111003875732, 0.9552836418151855, 0.9852699041366577, 0.9564962387084961, 0.989834725856781, 1.0017385482788086, 0.9540619254112244, 0.989777684211731, 0.9816364049911499]
[2025-05-12 13:03:35,796]: Mean: 0.97414041
[2025-05-12 13:03:35,796]: Min: 0.90925628
[2025-05-12 13:03:35,796]: Max: 1.04371274
[2025-05-12 13:03:35,798]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 13:03:35,799]: Sample Values (25 elements): [-0.0302062276750803, -0.15103113651275635, -0.1812373697757721, -0.09061868488788605, 0.0302062276750803, -0.1208249107003212, -0.15103113651275635, 0.09061868488788605, 0.0604124553501606, -0.0604124553501606, 0.0604124553501606, -0.15103113651275635, -0.0302062276750803, 0.1812373697757721, 0.0302062276750803, -0.0302062276750803, 0.0302062276750803, 0.09061868488788605, 0.0604124553501606, 0.0604124553501606, 0.1208249107003212, 0.09061868488788605, -0.0604124553501606, 0.15103113651275635, 0.09061868488788605]
[2025-05-12 13:03:35,800]: Mean: 0.00104719
[2025-05-12 13:03:35,801]: Min: -0.24164982
[2025-05-12 13:03:35,801]: Max: 0.21144359
[2025-05-12 13:03:35,801]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,802]: Sample Values (25 elements): [0.8763207197189331, 0.8407381176948547, 0.8853471875190735, 0.8891217708587646, 0.8710260987281799, 0.9155276417732239, 0.9029349088668823, 0.9042314291000366, 0.905574381351471, 0.9056637287139893, 0.9220864176750183, 0.888698935508728, 0.9016723036766052, 0.886346697807312, 0.8974393010139465, 0.9131826162338257, 0.91783607006073, 0.8979116678237915, 0.8625147342681885, 0.8256063461303711, 0.8956965804100037, 0.9059332013130188, 0.9032766222953796, 0.8946162462234497, 0.9020479321479797]
[2025-05-12 13:03:35,802]: Mean: 0.89579976
[2025-05-12 13:03:35,803]: Min: 0.81911093
[2025-05-12 13:03:35,803]: Max: 0.93738425
[2025-05-12 13:03:35,804]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:03:35,806]: Sample Values (25 elements): [0.015360645949840546, 0.015360645949840546, 0.0, -0.015360645949840546, -0.04608193784952164, -0.015360645949840546, -0.03072129189968109, -0.015360645949840546, 0.015360645949840546, 0.015360645949840546, -0.015360645949840546, 0.03072129189968109, 0.04608193784952164, 0.0, -0.015360645949840546, 0.04608193784952164, -0.015360645949840546, 0.015360645949840546, -0.015360645949840546, -0.03072129189968109, -0.015360645949840546, 0.0, 0.03072129189968109, 0.0, 0.0]
[2025-05-12 13:03:35,807]: Mean: 0.00006542
[2025-05-12 13:03:35,807]: Min: -0.10752452
[2025-05-12 13:03:35,807]: Max: 0.12288517
[2025-05-12 13:03:35,807]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,808]: Sample Values (25 elements): [1.0354381799697876, 0.9949456453323364, 1.0026049613952637, 0.9829933643341064, 1.0478785037994385, 1.0087050199508667, 1.0106291770935059, 1.020055890083313, 0.9807624220848083, 0.9788827896118164, 0.9925581812858582, 1.006880760192871, 1.0154290199279785, 0.9946342706680298, 0.9897361397743225, 0.9783527851104736, 0.9837653636932373, 1.0114928483963013, 1.0191974639892578, 0.996337354183197, 0.9962011575698853, 0.9652005434036255, 0.9785127639770508, 0.9794979691505432, 0.9619961380958557]
[2025-05-12 13:03:35,808]: Mean: 0.99530149
[2025-05-12 13:03:35,808]: Min: 0.96199614
[2025-05-12 13:03:35,809]: Max: 1.04787850
[2025-05-12 13:03:35,811]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:03:35,812]: Sample Values (25 elements): [-0.013732556253671646, -0.027465112507343292, 0.04119766876101494, -0.013732556253671646, -0.027465112507343292, 0.027465112507343292, -0.04119766876101494, -0.013732556253671646, -0.013732556253671646, 0.0, 0.013732556253671646, 0.013732556253671646, 0.027465112507343292, 0.013732556253671646, -0.013732556253671646, 0.04119766876101494, -0.013732556253671646, 0.027465112507343292, -0.013732556253671646, 0.0, 0.0, -0.04119766876101494, -0.027465112507343292, -0.04119766876101494, 0.027465112507343292]
[2025-05-12 13:03:35,812]: Mean: 0.00006258
[2025-05-12 13:03:35,812]: Min: -0.10986045
[2025-05-12 13:03:35,813]: Max: 0.09612790
[2025-05-12 13:03:35,813]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,813]: Sample Values (25 elements): [0.9681779146194458, 0.9478988647460938, 0.9598835110664368, 0.998081386089325, 0.9627042412757874, 0.9755363464355469, 1.0425045490264893, 0.9902602434158325, 0.9957872033119202, 0.9980354309082031, 0.9824861884117126, 0.981368362903595, 0.9815259575843811, 1.000718355178833, 0.9517743587493896, 1.008765459060669, 0.9793692231178284, 0.9847195148468018, 0.9593345522880554, 0.9862338900566101, 0.9862274527549744, 1.0082324743270874, 0.9609963297843933, 1.0077753067016602, 1.006931185722351]
[2025-05-12 13:03:35,814]: Mean: 0.98300254
[2025-05-12 13:03:35,814]: Min: 0.94223499
[2025-05-12 13:03:35,814]: Max: 1.04250455
[2025-05-12 13:03:35,816]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:03:35,817]: Sample Values (25 elements): [-0.025238342583179474, 0.025238342583179474, -0.025238342583179474, 0.012619171291589737, 0.012619171291589737, -0.03785751387476921, 0.012619171291589737, -0.03785751387476921, 0.012619171291589737, -0.012619171291589737, -0.012619171291589737, -0.012619171291589737, 0.0, -0.025238342583179474, 0.0, 0.025238342583179474, -0.03785751387476921, 0.012619171291589737, 0.0, 0.025238342583179474, 0.0, -0.012619171291589737, -0.05047668516635895, 0.0, 0.0]
[2025-05-12 13:03:35,817]: Mean: -0.00012084
[2025-05-12 13:03:35,817]: Min: -0.08833420
[2025-05-12 13:03:35,817]: Max: 0.10095337
[2025-05-12 13:03:35,817]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,818]: Sample Values (25 elements): [0.9720859527587891, 0.9663223028182983, 0.9866110682487488, 0.9616851806640625, 0.9751640558242798, 0.9727948307991028, 0.9756231904029846, 0.9837297797203064, 0.9677775502204895, 0.979512631893158, 0.9745721817016602, 0.9557456374168396, 0.9748601913452148, 0.9851904511451721, 0.9881502985954285, 0.9708683490753174, 0.9710363149642944, 0.973445475101471, 0.9919558167457581, 1.002663254737854, 0.9669660329818726, 0.9772465825080872, 0.968512773513794, 0.9559103846549988, 0.9668002128601074]
[2025-05-12 13:03:35,818]: Mean: 0.97417545
[2025-05-12 13:03:35,818]: Min: 0.94320428
[2025-05-12 13:03:35,819]: Max: 1.03946757
[2025-05-12 13:03:35,820]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:03:35,821]: Sample Values (25 elements): [0.021439475938677788, -0.010719737969338894, -0.03215921297669411, -0.010719737969338894, -0.010719737969338894, 0.010719737969338894, 0.010719737969338894, 0.010719737969338894, -0.021439475938677788, -0.010719737969338894, 0.021439475938677788, 0.021439475938677788, 0.03215921297669411, 0.021439475938677788, 0.021439475938677788, 0.010719737969338894, 0.010719737969338894, -0.021439475938677788, -0.053598690778017044, -0.03215921297669411, 0.010719737969338894, -0.010719737969338894, 0.010719737969338894, 0.021439475938677788, 0.0]
[2025-05-12 13:03:35,821]: Mean: -0.00013842
[2025-05-12 13:03:35,822]: Min: -0.07503816
[2025-05-12 13:03:35,822]: Max: 0.08575790
[2025-05-12 13:03:35,822]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:03:35,823]: Sample Values (25 elements): [1.036428451538086, 1.0056997537612915, 1.0635478496551514, 1.0294952392578125, 1.036369800567627, 1.058687686920166, 1.023575782775879, 1.0513185262680054, 1.0271309614181519, 1.0634649991989136, 1.0711126327514648, 1.0263537168502808, 1.0267008543014526, 1.0593972206115723, 1.0384278297424316, 1.035609483718872, 1.0368282794952393, 1.0526573657989502, 1.0766284465789795, 1.0763540267944336, 1.0499309301376343, 1.0510826110839844, 1.0255249738693237, 1.0369517803192139, 1.0254079103469849]
[2025-05-12 13:03:35,823]: Mean: 1.04666686
[2025-05-12 13:03:35,823]: Min: 1.00490105
[2025-05-12 13:03:35,824]: Max: 1.08841312
[2025-05-12 13:03:35,824]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 13:03:35,824]: Sample Values (25 elements): [-0.08852001279592514, 0.34148019552230835, -0.4188498556613922, -0.16244903206825256, -0.3036057651042938, 0.2008017748594284, 0.48991841077804565, 0.30743837356567383, 0.05154605209827423, -0.029798133298754692, 0.1335751861333847, -0.152299702167511, -0.28576382994651794, -0.37101998925209045, 0.417097806930542, 0.2700038552284241, -0.28305956721305847, -0.3837774097919464, -0.4151275157928467, -0.23806412518024445, -0.15122509002685547, -0.18940377235412598, -0.20328283309936523, 0.35854172706604004, 0.31824004650115967]
[2025-05-12 13:03:35,825]: Mean: 0.00304284
[2025-05-12 13:03:35,825]: Min: -0.48848873
[2025-05-12 13:03:35,825]: Max: 0.49601281
[2025-05-12 13:03:35,825]: 


QAT of ResNet20 with hardtanh down to 3 bits...
[2025-05-12 13:03:36,191]: [ResNet20_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-12 13:03:36,326]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:05:26,366]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.8617 Train Acc: 0.6976 Eval Loss: 0.9895 Eval Acc: 0.6673 (LR: 0.001000)
[2025-05-12 13:07:16,076]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.7999 Train Acc: 0.7216 Eval Loss: 0.9658 Eval Acc: 0.6690 (LR: 0.001000)
[2025-05-12 13:09:05,119]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.7826 Train Acc: 0.7250 Eval Loss: 0.8918 Eval Acc: 0.6936 (LR: 0.001000)
[2025-05-12 13:10:54,533]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.7598 Train Acc: 0.7360 Eval Loss: 0.8789 Eval Acc: 0.7003 (LR: 0.001000)
[2025-05-12 13:12:43,762]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.7534 Train Acc: 0.7362 Eval Loss: 0.8523 Eval Acc: 0.7094 (LR: 0.001000)
[2025-05-12 13:14:33,989]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.7369 Train Acc: 0.7445 Eval Loss: 0.8746 Eval Acc: 0.7027 (LR: 0.001000)
[2025-05-12 13:16:24,816]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.7254 Train Acc: 0.7461 Eval Loss: 0.7338 Eval Acc: 0.7429 (LR: 0.001000)
[2025-05-12 13:18:16,095]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.7184 Train Acc: 0.7496 Eval Loss: 0.7741 Eval Acc: 0.7342 (LR: 0.001000)
[2025-05-12 13:20:06,410]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.7129 Train Acc: 0.7514 Eval Loss: 0.8354 Eval Acc: 0.7166 (LR: 0.001000)
[2025-05-12 13:21:56,192]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.7033 Train Acc: 0.7549 Eval Loss: 0.8545 Eval Acc: 0.7092 (LR: 0.001000)
[2025-05-12 13:23:45,388]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.7027 Train Acc: 0.7540 Eval Loss: 0.8161 Eval Acc: 0.7163 (LR: 0.001000)
[2025-05-12 13:25:34,847]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.6861 Train Acc: 0.7621 Eval Loss: 0.8218 Eval Acc: 0.7210 (LR: 0.001000)
[2025-05-12 13:27:24,604]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.6823 Train Acc: 0.7625 Eval Loss: 0.7288 Eval Acc: 0.7444 (LR: 0.001000)
[2025-05-12 13:29:15,354]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.6713 Train Acc: 0.7672 Eval Loss: 0.8127 Eval Acc: 0.7204 (LR: 0.001000)
[2025-05-12 13:31:07,173]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.6694 Train Acc: 0.7656 Eval Loss: 0.8239 Eval Acc: 0.7282 (LR: 0.001000)
[2025-05-12 13:32:58,312]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.6612 Train Acc: 0.7682 Eval Loss: 0.7356 Eval Acc: 0.7477 (LR: 0.001000)
[2025-05-12 13:34:49,070]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.6630 Train Acc: 0.7696 Eval Loss: 0.7350 Eval Acc: 0.7497 (LR: 0.001000)
[2025-05-12 13:36:39,091]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.6550 Train Acc: 0.7709 Eval Loss: 0.7080 Eval Acc: 0.7594 (LR: 0.001000)
[2025-05-12 13:38:28,694]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.6421 Train Acc: 0.7771 Eval Loss: 0.7107 Eval Acc: 0.7558 (LR: 0.001000)
[2025-05-12 13:40:17,174]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.6432 Train Acc: 0.7761 Eval Loss: 0.6750 Eval Acc: 0.7724 (LR: 0.001000)
[2025-05-12 13:42:06,455]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.6366 Train Acc: 0.7780 Eval Loss: 0.7812 Eval Acc: 0.7369 (LR: 0.001000)
[2025-05-12 13:43:55,969]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.6315 Train Acc: 0.7806 Eval Loss: 0.6809 Eval Acc: 0.7668 (LR: 0.001000)
[2025-05-12 13:45:47,946]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.6231 Train Acc: 0.7821 Eval Loss: 0.7042 Eval Acc: 0.7699 (LR: 0.001000)
[2025-05-12 13:47:38,558]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.6218 Train Acc: 0.7839 Eval Loss: 0.6891 Eval Acc: 0.7615 (LR: 0.001000)
[2025-05-12 13:49:31,847]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.6272 Train Acc: 0.7809 Eval Loss: 0.8346 Eval Acc: 0.7157 (LR: 0.001000)
[2025-05-12 13:51:26,824]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.6191 Train Acc: 0.7837 Eval Loss: 0.6860 Eval Acc: 0.7619 (LR: 0.001000)
[2025-05-12 13:53:20,321]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.6155 Train Acc: 0.7862 Eval Loss: 0.6771 Eval Acc: 0.7685 (LR: 0.001000)
[2025-05-12 13:55:13,193]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.6082 Train Acc: 0.7885 Eval Loss: 0.7187 Eval Acc: 0.7566 (LR: 0.001000)
[2025-05-12 13:57:04,015]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.6102 Train Acc: 0.7875 Eval Loss: 0.7078 Eval Acc: 0.7547 (LR: 0.001000)
[2025-05-12 13:58:53,390]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.6053 Train Acc: 0.7898 Eval Loss: 0.7498 Eval Acc: 0.7470 (LR: 0.000250)
[2025-05-12 14:00:42,257]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.5387 Train Acc: 0.8132 Eval Loss: 0.5728 Eval Acc: 0.8034 (LR: 0.000250)
[2025-05-12 14:02:31,275]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.5266 Train Acc: 0.8175 Eval Loss: 0.5795 Eval Acc: 0.7995 (LR: 0.000250)
[2025-05-12 14:04:20,245]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.5259 Train Acc: 0.8198 Eval Loss: 0.5883 Eval Acc: 0.7968 (LR: 0.000250)
[2025-05-12 14:06:08,629]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.5245 Train Acc: 0.8160 Eval Loss: 0.6033 Eval Acc: 0.7932 (LR: 0.000250)
[2025-05-12 14:07:56,953]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.5261 Train Acc: 0.8172 Eval Loss: 0.5686 Eval Acc: 0.8063 (LR: 0.000250)
[2025-05-12 14:09:48,500]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.5221 Train Acc: 0.8189 Eval Loss: 0.5844 Eval Acc: 0.8026 (LR: 0.000250)
[2025-05-12 14:11:38,926]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.5221 Train Acc: 0.8172 Eval Loss: 0.5605 Eval Acc: 0.8075 (LR: 0.000250)
[2025-05-12 14:13:29,787]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.5228 Train Acc: 0.8190 Eval Loss: 0.6124 Eval Acc: 0.7944 (LR: 0.000250)
[2025-05-12 14:15:16,397]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.5251 Train Acc: 0.8186 Eval Loss: 0.5680 Eval Acc: 0.8070 (LR: 0.000250)
[2025-05-12 14:17:06,187]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.5168 Train Acc: 0.8196 Eval Loss: 0.5769 Eval Acc: 0.8003 (LR: 0.000250)
[2025-05-12 14:18:56,920]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.5189 Train Acc: 0.8201 Eval Loss: 0.6061 Eval Acc: 0.7956 (LR: 0.000250)
[2025-05-12 14:20:49,575]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.5223 Train Acc: 0.8170 Eval Loss: 0.5780 Eval Acc: 0.8037 (LR: 0.000250)
[2025-05-12 14:22:44,575]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.5193 Train Acc: 0.8208 Eval Loss: 0.5651 Eval Acc: 0.8106 (LR: 0.000250)
[2025-05-12 14:24:53,097]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.5222 Train Acc: 0.8178 Eval Loss: 0.5581 Eval Acc: 0.8090 (LR: 0.000250)
[2025-05-12 14:26:48,772]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.5109 Train Acc: 0.8232 Eval Loss: 0.5790 Eval Acc: 0.8007 (LR: 0.000063)
[2025-05-12 14:28:47,183]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.4916 Train Acc: 0.8291 Eval Loss: 0.5377 Eval Acc: 0.8163 (LR: 0.000063)
[2025-05-12 14:30:52,775]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.4875 Train Acc: 0.8308 Eval Loss: 0.5456 Eval Acc: 0.8148 (LR: 0.000063)
[2025-05-12 14:32:53,978]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.4832 Train Acc: 0.8332 Eval Loss: 0.5422 Eval Acc: 0.8147 (LR: 0.000063)
[2025-05-12 14:34:50,452]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.4855 Train Acc: 0.8324 Eval Loss: 0.5413 Eval Acc: 0.8154 (LR: 0.000063)
[2025-05-12 14:36:49,408]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.4851 Train Acc: 0.8328 Eval Loss: 0.5335 Eval Acc: 0.8165 (LR: 0.000063)
[2025-05-12 14:39:02,880]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.4854 Train Acc: 0.8320 Eval Loss: 0.5449 Eval Acc: 0.8111 (LR: 0.000063)
[2025-05-12 14:41:28,168]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.4863 Train Acc: 0.8324 Eval Loss: 0.5429 Eval Acc: 0.8165 (LR: 0.000063)
[2025-05-12 14:43:51,919]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.4847 Train Acc: 0.8330 Eval Loss: 0.5920 Eval Acc: 0.7951 (LR: 0.000063)
[2025-05-12 14:46:14,963]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.4841 Train Acc: 0.8334 Eval Loss: 0.5309 Eval Acc: 0.8175 (LR: 0.000063)
[2025-05-12 14:48:37,703]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.4844 Train Acc: 0.8329 Eval Loss: 0.5563 Eval Acc: 0.8078 (LR: 0.000063)
[2025-05-12 14:50:59,053]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.4869 Train Acc: 0.8296 Eval Loss: 0.5386 Eval Acc: 0.8162 (LR: 0.000063)
[2025-05-12 14:53:19,308]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.4842 Train Acc: 0.8327 Eval Loss: 0.5442 Eval Acc: 0.8163 (LR: 0.000063)
[2025-05-12 14:55:21,607]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.4838 Train Acc: 0.8330 Eval Loss: 0.5552 Eval Acc: 0.8110 (LR: 0.000063)
[2025-05-12 14:57:15,655]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.4829 Train Acc: 0.8322 Eval Loss: 0.5609 Eval Acc: 0.8122 (LR: 0.000063)
[2025-05-12 14:59:11,687]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.4866 Train Acc: 0.8310 Eval Loss: 0.5704 Eval Acc: 0.8093 (LR: 0.000063)
[2025-05-12 14:59:11,687]: [ResNet20_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8175
[2025-05-12 14:59:11,748]: 


Quantization of model down to 3 bits finished
[2025-05-12 14:59:11,748]: Model Architecture:
[2025-05-12 14:59:11,926]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1237], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4394969344139099, max_val=0.42623409628868103)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2148], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7461519837379456, max_val=0.7572817206382751)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1563], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49360063672065735, max_val=0.6008085012435913)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1050], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3676516115665436, max_val=0.36755117774009705)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1011], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.29477566480636597, max_val=0.4130558967590332)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0875], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2932508885860443, max_val=0.31917038559913635)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0666], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23368880152702332, max_val=0.23258472979068756)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0545], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20092417299747467, max_val=0.18063701689243317)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1059], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.370726078748703, max_val=0.37068113684654236)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0539], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18933308124542236, max_val=0.18827982246875763)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0534], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18639135360717773, max_val=0.18747557699680328)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0502], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17548809945583344, max_val=0.17600612342357635)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0466], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16320060193538666, max_val=0.1633279174566269)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13894276320934296, max_val=0.13127294182777405)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0354], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12605884671211243, max_val=0.12189185619354248)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0725], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26635801792144775, max_val=0.24096445739269257)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0341], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1194150522351265, max_val=0.11931271106004715)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0306], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10909030586481094, max_val=0.10531242936849594)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0276], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09426268190145493, max_val=0.09906543046236038)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08154021948575974, max_val=0.08258640021085739)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 14:59:11,926]: 
Model Weights:
[2025-05-12 14:59:11,926]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 14:59:11,927]: Sample Values (25 elements): [0.14617182314395905, 0.10597553849220276, 0.0036176880821585655, -0.1630147397518158, 0.11932379007339478, 0.5205503106117249, -0.028179757297039032, -0.34869223833084106, -0.08892713487148285, -0.06714479625225067, 0.5896676778793335, -0.3594164252281189, 0.20291703939437866, 0.10287603735923767, -0.3186171054840088, -0.014349572360515594, -0.06589435786008835, 0.3392382860183716, 0.15888355672359467, -0.18651118874549866, 0.7472912073135376, 0.3171018660068512, -0.7249082922935486, 0.44320690631866455, -0.06924530118703842]
[2025-05-12 14:59:11,927]: Mean: -0.00269505
[2025-05-12 14:59:11,928]: Min: -0.72490829
[2025-05-12 14:59:11,928]: Max: 0.89616227
[2025-05-12 14:59:11,928]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,928]: Sample Values (16 elements): [0.7226004600524902, 0.704150915145874, 0.9267656207084656, 0.5639141201972961, 0.7304942011833191, 1.1344939470291138, 0.58332359790802, 0.45733386278152466, 0.997023344039917, 0.6720291376113892, 0.8478069305419922, 0.5394625663757324, 0.8285465836524963, 0.54665607213974, 1.092198371887207, 1.21695077419281]
[2025-05-12 14:59:11,928]: Mean: 0.78523439
[2025-05-12 14:59:11,929]: Min: 0.45733386
[2025-05-12 14:59:11,929]: Max: 1.21695077
[2025-05-12 14:59:11,931]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:59:11,932]: Sample Values (25 elements): [0.0, 0.12367591261863708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12367591261863708, 0.12367591261863708, 0.12367591261863708, 0.0, 0.0, 0.0, -0.12367591261863708, 0.12367591261863708, 0.0, 0.0, 0.0, 0.12367591261863708, -0.12367591261863708, -0.24735182523727417, 0.12367591261863708, 0.0, 0.12367591261863708, 0.0]
[2025-05-12 14:59:11,932]: Mean: 0.00440166
[2025-05-12 14:59:11,932]: Min: -0.49470365
[2025-05-12 14:59:11,933]: Max: 0.37102774
[2025-05-12 14:59:11,933]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,933]: Sample Values (16 elements): [0.6426257491111755, 0.6594325304031372, 1.0149378776550293, 0.873217761516571, 0.886811375617981, 1.1691035032272339, 1.168717861175537, 1.0665686130523682, 0.8332399129867554, 0.6336302757263184, 1.0494565963745117, 0.7394927144050598, 0.8946630358695984, 0.7231274843215942, 0.5544455051422119, 0.861997127532959]
[2025-05-12 14:59:11,933]: Mean: 0.86071670
[2025-05-12 14:59:11,934]: Min: 0.55444551
[2025-05-12 14:59:11,934]: Max: 1.16910350
[2025-05-12 14:59:11,938]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:59:11,938]: Sample Values (25 elements): [0.21477654576301575, 0.0, 0.21477654576301575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21477654576301575, 0.0, 0.0, 0.0, 0.21477654576301575, 0.0, 0.0, 0.0, 0.21477654576301575, 0.0, 0.21477654576301575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 14:59:11,939]: Mean: -0.00615245
[2025-05-12 14:59:11,939]: Min: -0.64432967
[2025-05-12 14:59:11,939]: Max: 0.85910618
[2025-05-12 14:59:11,939]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,940]: Sample Values (16 elements): [0.8209967017173767, 0.5757780075073242, 1.03077232837677, 0.4051356613636017, 0.5925484299659729, 0.4982944130897522, 1.0333373546600342, 0.7518355250358582, 0.7252944111824036, 1.1061280965805054, 0.8650764226913452, 0.699144721031189, 0.6657204627990723, 0.5869109034538269, 0.47713279724121094, 0.5950267314910889]
[2025-05-12 14:59:11,940]: Mean: 0.71432078
[2025-05-12 14:59:11,941]: Min: 0.40513566
[2025-05-12 14:59:11,941]: Max: 1.10612810
[2025-05-12 14:59:11,943]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:59:11,943]: Sample Values (25 elements): [-0.15634411573410034, 0.0, 0.0, -0.3126882314682007, 0.15634411573410034, -0.15634411573410034, 0.15634411573410034, 0.0, 0.0, 0.0, 0.469032347202301, 0.0, 0.0, -0.469032347202301, 0.15634411573410034, 0.0, 0.0, -0.15634411573410034, -0.15634411573410034, 0.0, 0.0, 0.0, -0.15634411573410034, 0.0, 0.15634411573410034]
[2025-05-12 14:59:11,943]: Mean: -0.00047500
[2025-05-12 14:59:11,944]: Min: -0.46903235
[2025-05-12 14:59:11,944]: Max: 0.62537646
[2025-05-12 14:59:11,944]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,945]: Sample Values (16 elements): [0.983947217464447, 0.7637668251991272, 0.8403852581977844, 0.7907768487930298, 0.781522810459137, 0.9086762070655823, 0.8489304780960083, 0.8985351920127869, 0.9954900741577148, 0.9483512043952942, 0.8016842007637024, 1.166195273399353, 1.0341928005218506, 1.1756364107131958, 0.8957482576370239, 0.7883700728416443]
[2025-05-12 14:59:11,945]: Mean: 0.91388810
[2025-05-12 14:59:11,945]: Min: 0.76376683
[2025-05-12 14:59:11,946]: Max: 1.17563641
[2025-05-12 14:59:11,947]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:59:11,948]: Sample Values (25 elements): [-0.10502902418375015, 0.0, -0.10502902418375015, 0.0, 0.2100580483675003, 0.10502902418375015, 0.10502902418375015, 0.0, -0.10502902418375015, -0.10502902418375015, -0.10502902418375015, -0.10502902418375015, -0.2100580483675003, -0.10502902418375015, 0.0, 0.0, 0.10502902418375015, -0.10502902418375015, 0.0, 0.2100580483675003, -0.2100580483675003, -0.10502902418375015, -0.2100580483675003, -0.10502902418375015, 0.10502902418375015]
[2025-05-12 14:59:11,948]: Mean: 0.00109405
[2025-05-12 14:59:11,949]: Min: -0.31508708
[2025-05-12 14:59:11,949]: Max: 0.31508708
[2025-05-12 14:59:11,949]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,949]: Sample Values (16 elements): [0.5893484354019165, 0.8968868851661682, 0.7243678569793701, 0.4703855812549591, 0.6162408590316772, 1.089058756828308, 0.6610236763954163, 0.7493987679481506, 0.6421636343002319, 0.6547799706459045, 0.5713799595832825, 0.6574214696884155, 0.7140097618103027, 0.5704641938209534, 0.7813547253608704, 0.5727937817573547]
[2025-05-12 14:59:11,950]: Mean: 0.68506742
[2025-05-12 14:59:11,950]: Min: 0.47038558
[2025-05-12 14:59:11,950]: Max: 1.08905876
[2025-05-12 14:59:11,952]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:59:11,952]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.10111870616674423, 0.0, -0.10111870616674423, -0.10111870616674423, -0.10111870616674423, -0.10111870616674423, 0.0, 0.20223741233348846, 0.0, 0.0, -0.10111870616674423, 0.0, 0.0, 0.3033561110496521, -0.20223741233348846, 0.10111870616674423, -0.10111870616674423, 0.10111870616674423, 0.0, 0.0, 0.0]
[2025-05-12 14:59:11,952]: Mean: 0.00513493
[2025-05-12 14:59:11,953]: Min: -0.30335611
[2025-05-12 14:59:11,953]: Max: 0.40447482
[2025-05-12 14:59:11,953]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,954]: Sample Values (16 elements): [1.0651404857635498, 0.8387547135353088, 1.0425530672073364, 1.008484125137329, 0.8286833763122559, 0.763405978679657, 0.7142139077186584, 0.9927135705947876, 1.0573492050170898, 0.8832901120185852, 0.8948978185653687, 0.831497311592102, 0.841676652431488, 0.8582483530044556, 0.8332189321517944, 1.062997579574585]
[2025-05-12 14:59:11,954]: Mean: 0.90732032
[2025-05-12 14:59:11,954]: Min: 0.71421391
[2025-05-12 14:59:11,954]: Max: 1.06514049
[2025-05-12 14:59:11,956]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:59:11,956]: Sample Values (25 elements): [0.08748847991228104, 0.0, 0.08748847991228104, -0.08748847991228104, 0.17497695982456207, -0.08748847991228104, -0.17497695982456207, 0.08748847991228104, -0.08748847991228104, 0.0, -0.08748847991228104, -0.08748847991228104, 0.0, -0.08748847991228104, 0.0, 0.0, -0.17497695982456207, 0.08748847991228104, 0.08748847991228104, -0.08748847991228104, -0.08748847991228104, -0.17497695982456207, -0.17497695982456207, -0.08748847991228104, 0.0]
[2025-05-12 14:59:11,957]: Mean: -0.00303779
[2025-05-12 14:59:11,957]: Min: -0.26246545
[2025-05-12 14:59:11,957]: Max: 0.34995392
[2025-05-12 14:59:11,957]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:59:11,958]: Sample Values (16 elements): [0.8324103951454163, 0.634050726890564, 0.6498992443084717, 0.7565147280693054, 0.6129772067070007, 0.5807526707649231, 0.8282848596572876, 0.7960458397865295, 0.7485033273696899, 0.7684236764907837, 0.6471390128135681, 0.718773365020752, 0.7017184495925903, 0.6902261972427368, 0.6535862684249878, 0.5965227484703064]
[2025-05-12 14:59:11,958]: Mean: 0.70098931
[2025-05-12 14:59:11,958]: Min: 0.58075267
[2025-05-12 14:59:11,959]: Max: 0.83241040
[2025-05-12 14:59:11,960]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 14:59:11,960]: Sample Values (25 elements): [0.06661055982112885, -0.06661055982112885, 0.0, 0.0, 0.1332211196422577, -0.06661055982112885, 0.1332211196422577, 0.0, 0.0, 0.0, -0.1332211196422577, 0.06661055982112885, 0.0, 0.06661055982112885, -0.06661055982112885, 0.1332211196422577, 0.06661055982112885, 0.06661055982112885, 0.06661055982112885, 0.0, 0.0, -0.06661055982112885, -0.06661055982112885, 0.06661055982112885, 0.0]
[2025-05-12 14:59:11,961]: Mean: 0.00021683
[2025-05-12 14:59:11,961]: Min: -0.26644224
[2025-05-12 14:59:11,961]: Max: 0.19983168
[2025-05-12 14:59:11,961]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:59:11,962]: Sample Values (25 elements): [0.9464864134788513, 0.9151040315628052, 0.9429464340209961, 0.9636522531509399, 0.8956997394561768, 0.9022157192230225, 0.9448603391647339, 0.9153450131416321, 0.9062764048576355, 0.8546369075775146, 0.9669367074966431, 0.9287676215171814, 0.9493726491928101, 0.9333981871604919, 0.9905285835266113, 0.9531832933425903, 1.0101642608642578, 0.8708868622779846, 1.038590908050537, 0.897050678730011, 0.9177082777023315, 0.9823135733604431, 0.8359092473983765, 0.8775902986526489, 0.9167373776435852]
[2025-05-12 14:59:11,962]: Mean: 0.93296194
[2025-05-12 14:59:11,962]: Min: 0.83590925
[2025-05-12 14:59:11,963]: Max: 1.03859091
[2025-05-12 14:59:11,964]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:59:11,965]: Sample Values (25 elements): [-0.05450878292322159, 0.0, -0.05450878292322159, -0.05450878292322159, 0.0, 0.0, 0.0, -0.05450878292322159, 0.05450878292322159, 0.05450878292322159, 0.0, -0.05450878292322159, 0.0, -0.05450878292322159, 0.0, 0.0, 0.0, 0.0, 0.05450878292322159, 0.0, 0.05450878292322159, 0.05450878292322159, 0.0, 0.05450878292322159, 0.05450878292322159]
[2025-05-12 14:59:11,965]: Mean: -0.00145499
[2025-05-12 14:59:11,965]: Min: -0.21803513
[2025-05-12 14:59:11,966]: Max: 0.16352636
[2025-05-12 14:59:11,966]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:59:11,966]: Sample Values (25 elements): [0.9116598963737488, 0.9261081218719482, 0.9489229917526245, 0.9077144265174866, 0.8836988806724548, 0.9637576937675476, 1.0089696645736694, 0.9324536919593811, 0.913273274898529, 0.9630195498466492, 0.9648409485816956, 0.860047459602356, 0.9578328728675842, 0.8243731260299683, 0.924625039100647, 0.8828337788581848, 0.847837507724762, 0.9099200963973999, 1.04507315158844, 0.9306492805480957, 0.9497948884963989, 0.8922474980354309, 0.9391030669212341, 0.9115417003631592, 0.9114912152290344]
[2025-05-12 14:59:11,966]: Mean: 0.92027795
[2025-05-12 14:59:11,967]: Min: 0.78870326
[2025-05-12 14:59:11,967]: Max: 1.04507315
[2025-05-12 14:59:11,971]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 14:59:11,973]: Sample Values (25 elements): [0.0, -0.1059153750538826, 0.2118307501077652, 0.2118307501077652, -0.1059153750538826, -0.2118307501077652, 0.3177461326122284, -0.1059153750538826, 0.3177461326122284, -0.1059153750538826, 0.1059153750538826, 0.0, -0.1059153750538826, 0.1059153750538826, 0.0, 0.1059153750538826, -0.2118307501077652, 0.0, 0.2118307501077652, 0.1059153750538826, 0.1059153750538826, 0.0, 0.1059153750538826, -0.1059153750538826, -0.1059153750538826]
[2025-05-12 14:59:11,978]: Mean: 0.00496478
[2025-05-12 14:59:11,980]: Min: -0.42366150
[2025-05-12 14:59:11,983]: Max: 0.31774613
[2025-05-12 14:59:11,983]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 14:59:11,987]: Sample Values (25 elements): [0.7387641668319702, 0.8012365698814392, 0.7522703409194946, 0.7947377562522888, 0.7312096953392029, 0.8294816613197327, 0.8234190940856934, 0.8160552382469177, 0.7428672313690186, 0.7318131327629089, 0.7830246686935425, 0.7677276730537415, 0.7390960454940796, 0.8570775985717773, 0.7303032279014587, 0.8349038362503052, 0.7633784413337708, 0.7802408337593079, 0.7428877949714661, 0.8823797702789307, 0.8728230595588684, 0.7557681202888489, 0.6610874533653259, 0.8229050636291504, 0.8472092151641846]
[2025-05-12 14:59:11,992]: Mean: 0.78399086
[2025-05-12 14:59:11,999]: Min: 0.66108745
[2025-05-12 14:59:12,004]: Max: 0.88237977
[2025-05-12 14:59:12,014]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:59:12,018]: Sample Values (25 elements): [0.0, 0.0539446696639061, 0.0539446696639061, 0.0539446696639061, 0.1078893393278122, 0.0, 0.0539446696639061, -0.0539446696639061, 0.0, 0.0, 0.0539446696639061, 0.0539446696639061, 0.0539446696639061, 0.0539446696639061, -0.0539446696639061, 0.0539446696639061, 0.0539446696639061, -0.0539446696639061, 0.1078893393278122, 0.0, -0.0539446696639061, 0.0539446696639061, 0.0539446696639061, 0.0, -0.1078893393278122]
[2025-05-12 14:59:12,019]: Mean: -0.00064387
[2025-05-12 14:59:12,021]: Min: -0.21577868
[2025-05-12 14:59:12,025]: Max: 0.16183400
[2025-05-12 14:59:12,025]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:59:12,042]: Sample Values (25 elements): [0.9918503165245056, 0.9904929399490356, 0.9562332034111023, 0.9961177110671997, 0.990376889705658, 0.9389297366142273, 0.9690067172050476, 0.9843437075614929, 0.9734872579574585, 0.9548876881599426, 0.9589813351631165, 0.9626411199569702, 0.9673255681991577, 1.044716238975525, 1.0090253353118896, 0.95399010181427, 0.9150338172912598, 0.9430549740791321, 0.9867709279060364, 1.000579595565796, 1.0107330083847046, 0.9054225087165833, 0.9071911573410034, 0.9280073046684265, 0.9588100910186768]
[2025-05-12 14:59:12,052]: Mean: 0.97099721
[2025-05-12 14:59:12,053]: Min: 0.90542251
[2025-05-12 14:59:12,054]: Max: 1.04471624
[2025-05-12 14:59:12,059]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:59:12,059]: Sample Values (25 elements): [0.1068190187215805, 0.05340950936079025, 0.05340950936079025, 0.0, 0.05340950936079025, 0.05340950936079025, -0.1068190187215805, 0.05340950936079025, 0.05340950936079025, 0.0, 0.0, 0.05340950936079025, 0.0, 0.0, 0.0, -0.05340950936079025, -0.05340950936079025, -0.05340950936079025, -0.05340950936079025, -0.05340950936079025, 0.0, -0.05340950936079025, 0.0, 0.0, -0.05340950936079025]
[2025-05-12 14:59:12,060]: Mean: 0.00037669
[2025-05-12 14:59:12,060]: Min: -0.16022852
[2025-05-12 14:59:12,060]: Max: 0.21363804
[2025-05-12 14:59:12,060]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:59:12,061]: Sample Values (25 elements): [0.7985347509384155, 0.9561054110527039, 0.8488823771476746, 0.9209759831428528, 0.8650125861167908, 0.8880732655525208, 0.8711476922035217, 0.8267626762390137, 0.8146700859069824, 0.9034001231193542, 0.8902837038040161, 0.9502431750297546, 0.9230383038520813, 0.7754034996032715, 0.9713343977928162, 0.8928850889205933, 0.7769911885261536, 0.9406012892723083, 0.9411722421646118, 0.9680226445198059, 0.8706606030464172, 0.8649534583091736, 0.8154385089874268, 0.8326120972633362, 0.7245844602584839]
[2025-05-12 14:59:12,061]: Mean: 0.87351841
[2025-05-12 14:59:12,061]: Min: 0.72458446
[2025-05-12 14:59:12,062]: Max: 0.97133440
[2025-05-12 14:59:12,065]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:59:12,065]: Sample Values (25 elements): [0.0, -0.05021344870328903, 0.05021344870328903, 0.0, 0.05021344870328903, -0.05021344870328903, 0.05021344870328903, 0.0, 0.0, -0.05021344870328903, -0.10042689740657806, 0.0, 0.05021344870328903, 0.0, 0.05021344870328903, 0.0, 0.0, 0.10042689740657806, 0.0, 0.0, 0.0, 0.0, -0.05021344870328903, 0.0, -0.10042689740657806]
[2025-05-12 14:59:12,066]: Mean: -0.00031056
[2025-05-12 14:59:12,066]: Min: -0.15064034
[2025-05-12 14:59:12,067]: Max: 0.20085379
[2025-05-12 14:59:12,067]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:59:12,068]: Sample Values (25 elements): [0.9587953090667725, 0.9333333969116211, 0.9663787484169006, 0.9596427083015442, 1.0108296871185303, 0.9602380394935608, 0.9706616997718811, 0.941148042678833, 0.9617267847061157, 0.9539529085159302, 1.0253026485443115, 0.9720233082771301, 0.9064239263534546, 0.9617003202438354, 0.9368656277656555, 0.950610876083374, 0.97404545545578, 0.9931814670562744, 0.977969765663147, 1.0287907123565674, 0.9950434565544128, 0.9463191032409668, 0.9535702466964722, 0.9628433585166931, 0.9321338534355164]
[2025-05-12 14:59:12,068]: Mean: 0.96775556
[2025-05-12 14:59:12,068]: Min: 0.90642393
[2025-05-12 14:59:12,069]: Max: 1.02879071
[2025-05-12 14:59:12,072]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:59:12,072]: Sample Values (25 elements): [0.0, -0.04664712771773338, -0.04664712771773338, -0.04664712771773338, 0.04664712771773338, -0.04664712771773338, -0.04664712771773338, -0.09329425543546677, -0.09329425543546677, -0.04664712771773338, -0.04664712771773338, -0.04664712771773338, 0.09329425543546677, -0.09329425543546677, -0.04664712771773338, 0.04664712771773338, 0.0, 0.04664712771773338, 0.0, 0.04664712771773338, 0.04664712771773338, 0.0, -0.04664712771773338, 0.0, 0.09329425543546677]
[2025-05-12 14:59:12,072]: Mean: -0.00071368
[2025-05-12 14:59:12,073]: Min: -0.13994138
[2025-05-12 14:59:12,073]: Max: 0.18658851
[2025-05-12 14:59:12,073]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:59:12,074]: Sample Values (25 elements): [0.8768355846405029, 0.9174280762672424, 0.7860937714576721, 0.9269421100616455, 0.9233911037445068, 0.9006263613700867, 0.9342570900917053, 0.9055365920066833, 0.8801575303077698, 0.9161003828048706, 0.8811081647872925, 0.965968132019043, 0.8490294814109802, 0.9995867013931274, 0.9413586258888245, 0.9004014730453491, 0.9270651936531067, 0.9028922915458679, 0.8746269941329956, 0.9172531962394714, 0.8071125745773315, 0.9339601397514343, 0.902157723903656, 0.9534287452697754, 0.9442379474639893]
[2025-05-12 14:59:12,074]: Mean: 0.90489346
[2025-05-12 14:59:12,075]: Min: 0.78609377
[2025-05-12 14:59:12,075]: Max: 0.99958670
[2025-05-12 14:59:12,078]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 14:59:12,079]: Sample Values (25 elements): [0.03860216587781906, 0.03860216587781906, -0.07720433175563812, -0.07720433175563812, 0.03860216587781906, 0.0, 0.03860216587781906, 0.03860216587781906, 0.03860216587781906, -0.03860216587781906, 0.0, 0.0, 0.03860216587781906, -0.03860216587781906, 0.03860216587781906, -0.11580649763345718, 0.0, -0.03860216587781906, 0.03860216587781906, 0.0, -0.03860216587781906, 0.03860216587781906, 0.0, 0.07720433175563812, 0.0]
[2025-05-12 14:59:12,079]: Mean: 0.00015917
[2025-05-12 14:59:12,080]: Min: -0.15440866
[2025-05-12 14:59:12,080]: Max: 0.11580650
[2025-05-12 14:59:12,080]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,081]: Sample Values (25 elements): [0.9623057246208191, 0.9545364379882812, 0.9737243056297302, 0.9130666255950928, 0.9452030062675476, 1.0208972692489624, 0.9884651899337769, 0.9803879261016846, 0.9335525035858154, 0.9495588541030884, 0.967275083065033, 0.9716605544090271, 0.9812242984771729, 0.9691904783248901, 0.9792540073394775, 0.9368459582328796, 0.9606592655181885, 0.9644868969917297, 0.9909748435020447, 0.9740964770317078, 0.973547637462616, 0.9583030939102173, 0.9317251443862915, 0.9427129030227661, 0.9571450352668762]
[2025-05-12 14:59:12,081]: Mean: 0.96085155
[2025-05-12 14:59:12,081]: Min: 0.91212833
[2025-05-12 14:59:12,082]: Max: 1.02089727
[2025-05-12 14:59:12,084]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:59:12,085]: Sample Values (25 elements): [-0.0354214571416378, 0.0, -0.0354214571416378, 0.0, 0.0, 0.0, -0.0354214571416378, 0.0, 0.0, 0.0, -0.0354214571416378, -0.0354214571416378, -0.0354214571416378, -0.0354214571416378, 0.0354214571416378, 0.0, 0.0, -0.0354214571416378, 0.0708429142832756, 0.0, -0.0354214571416378, 0.0, -0.0708429142832756, 0.0, 0.0]
[2025-05-12 14:59:12,085]: Mean: 0.00015182
[2025-05-12 14:59:12,086]: Min: -0.14168583
[2025-05-12 14:59:12,086]: Max: 0.10626437
[2025-05-12 14:59:12,086]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,086]: Sample Values (25 elements): [0.9849277138710022, 0.9593572616577148, 0.9975418448448181, 0.947691798210144, 0.9869876503944397, 0.9809252619743347, 1.0010836124420166, 0.9800272583961487, 0.9437749981880188, 0.968429684638977, 0.9720602035522461, 0.9740687012672424, 1.0229618549346924, 1.0304818153381348, 0.9429064989089966, 0.896550178527832, 0.9092110395431519, 0.9602677226066589, 1.0025465488433838, 0.9706264734268188, 0.9682749509811401, 1.0487362146377563, 0.9871374368667603, 0.9628731608390808, 0.9976770877838135]
[2025-05-12 14:59:12,087]: Mean: 0.97437948
[2025-05-12 14:59:12,087]: Min: 0.89655018
[2025-05-12 14:59:12,087]: Max: 1.04873621
[2025-05-12 14:59:12,089]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 14:59:12,089]: Sample Values (25 elements): [0.07247467339038849, 0.07247467339038849, -0.14494934678077698, -0.07247467339038849, -0.14494934678077698, 0.07247467339038849, 0.14494934678077698, 0.14494934678077698, 0.0, -0.07247467339038849, 0.07247467339038849, 0.0, 0.0, 0.07247467339038849, -0.07247467339038849, 0.0, 0.07247467339038849, 0.14494934678077698, -0.07247467339038849, 0.07247467339038849, -0.14494934678077698, 0.0, 0.07247467339038849, -0.07247467339038849, 0.0]
[2025-05-12 14:59:12,089]: Mean: -0.00010616
[2025-05-12 14:59:12,090]: Min: -0.28989869
[2025-05-12 14:59:12,090]: Max: 0.21742402
[2025-05-12 14:59:12,090]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,090]: Sample Values (25 elements): [0.8996096849441528, 0.8801100254058838, 0.9057958722114563, 0.8987709283828735, 0.7995356917381287, 0.8135800957679749, 0.8947718739509583, 0.9094439744949341, 0.9011766910552979, 0.8526351451873779, 0.8811336159706116, 0.9086606502532959, 0.8458895683288574, 0.8713741898536682, 0.8850616216659546, 0.8799680471420288, 0.8898066282272339, 0.8665169477462769, 0.8921260833740234, 0.8988860845565796, 0.8775151968002319, 0.9038059115409851, 0.8924806118011475, 0.9319092035293579, 0.8962972164154053]
[2025-05-12 14:59:12,090]: Mean: 0.88355690
[2025-05-12 14:59:12,091]: Min: 0.79953569
[2025-05-12 14:59:12,092]: Max: 0.93777442
[2025-05-12 14:59:12,093]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:59:12,094]: Sample Values (25 elements): [0.03410398215055466, 0.0, -0.03410398215055466, 0.0, 0.0, 0.03410398215055466, 0.0, -0.03410398215055466, 0.06820796430110931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03410398215055466, 0.03410398215055466, 0.03410398215055466, -0.03410398215055466, 0.0, -0.03410398215055466, -0.03410398215055466, 0.0, 0.0, -0.03410398215055466]
[2025-05-12 14:59:12,094]: Mean: 0.00017577
[2025-05-12 14:59:12,095]: Min: -0.13641593
[2025-05-12 14:59:12,096]: Max: 0.10231195
[2025-05-12 14:59:12,096]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,097]: Sample Values (25 elements): [0.9969863891601562, 1.003303050994873, 0.9829118251800537, 1.0069992542266846, 0.9977111220359802, 0.9825863838195801, 1.0109453201293945, 1.0093271732330322, 0.9623972177505493, 0.9772549271583557, 0.9741475582122803, 0.9927874207496643, 1.0051451921463013, 0.9938385486602783, 0.9968011379241943, 0.9820355772972107, 1.0076899528503418, 1.0229588747024536, 1.0312964916229248, 1.0096684694290161, 0.9629019498825073, 1.0030326843261719, 0.9811234474182129, 0.9971290826797485, 0.9750295281410217]
[2025-05-12 14:59:12,098]: Mean: 0.99375856
[2025-05-12 14:59:12,098]: Min: 0.95267445
[2025-05-12 14:59:12,098]: Max: 1.06278193
[2025-05-12 14:59:12,101]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:59:12,101]: Sample Values (25 elements): [-0.030629010871052742, -0.030629010871052742, 0.0, -0.030629010871052742, 0.030629010871052742, -0.030629010871052742, 0.0, 0.0, 0.0, 0.0, 0.030629010871052742, 0.0, 0.030629010871052742, 0.030629010871052742, 0.030629010871052742, -0.030629010871052742, 0.0, 0.0, -0.030629010871052742, 0.030629010871052742, 0.0, 0.030629010871052742, 0.0, -0.030629010871052742, 0.030629010871052742]
[2025-05-12 14:59:12,102]: Mean: 0.00001246
[2025-05-12 14:59:12,102]: Min: -0.12251604
[2025-05-12 14:59:12,102]: Max: 0.09188703
[2025-05-12 14:59:12,102]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,103]: Sample Values (25 elements): [0.9846729636192322, 0.9446091651916504, 0.983794093132019, 1.0014299154281616, 0.9851394891738892, 0.9872369170188904, 0.9824495315551758, 0.9787836670875549, 0.9647022485733032, 0.9592553973197937, 0.9600298404693604, 0.9816840291023254, 0.9796609282493591, 0.987046480178833, 0.9849564433097839, 1.0306825637817383, 1.0042517185211182, 0.9637141823768616, 0.9653499126434326, 0.9972717761993408, 0.9862370491027832, 1.0014524459838867, 0.9845978021621704, 0.975895345211029, 0.9678769707679749]
[2025-05-12 14:59:12,103]: Mean: 0.97906798
[2025-05-12 14:59:12,103]: Min: 0.92774433
[2025-05-12 14:59:12,104]: Max: 1.05305505
[2025-05-12 14:59:12,105]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:59:12,106]: Sample Values (25 elements): [-0.027618279680609703, 0.027618279680609703, 0.027618279680609703, -0.027618279680609703, 0.027618279680609703, 0.027618279680609703, 0.027618279680609703, 0.0, 0.0, -0.027618279680609703, 0.027618279680609703, 0.0, 0.027618279680609703, 0.027618279680609703, 0.055236559361219406, -0.027618279680609703, 0.0, -0.055236559361219406, -0.027618279680609703, 0.0, -0.027618279680609703, 0.055236559361219406, 0.0, 0.0, 0.027618279680609703]
[2025-05-12 14:59:12,106]: Mean: -0.00008466
[2025-05-12 14:59:12,107]: Min: -0.08285484
[2025-05-12 14:59:12,107]: Max: 0.11047312
[2025-05-12 14:59:12,107]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,108]: Sample Values (25 elements): [0.9829471707344055, 0.9939439296722412, 0.9653531908988953, 0.9701501727104187, 0.9781016111373901, 0.9746797680854797, 0.9875789880752563, 1.0075531005859375, 0.970582902431488, 0.9835920929908752, 0.9721572399139404, 0.9581793546676636, 1.00271737575531, 0.9634073376655579, 0.9631422162055969, 0.9648613333702087, 0.9616054892539978, 0.9640116095542908, 0.9830847978591919, 0.9677378535270691, 0.9583809971809387, 0.9708501100540161, 0.9860190153121948, 0.9831954836845398, 0.9701216220855713]
[2025-05-12 14:59:12,108]: Mean: 0.97179222
[2025-05-12 14:59:12,108]: Min: 0.94153047
[2025-05-12 14:59:12,109]: Max: 1.03903520
[2025-05-12 14:59:12,110]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:59:12,111]: Sample Values (25 elements): [0.023446673527359962, -0.023446673527359962, 0.0, 0.046893347054719925, 0.023446673527359962, 0.023446673527359962, -0.046893347054719925, 0.023446673527359962, 0.0, 0.023446673527359962, 0.046893347054719925, -0.023446673527359962, 0.0, -0.023446673527359962, 0.023446673527359962, 0.0, 0.023446673527359962, 0.023446673527359962, -0.023446673527359962, -0.023446673527359962, 0.0, 0.0, -0.023446673527359962, 0.023446673527359962, 0.0]
[2025-05-12 14:59:12,112]: Mean: -0.00019081
[2025-05-12 14:59:12,112]: Min: -0.07034002
[2025-05-12 14:59:12,112]: Max: 0.09378669
[2025-05-12 14:59:12,112]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:59:12,112]: Sample Values (25 elements): [1.033346176147461, 1.0678231716156006, 1.0484356880187988, 1.0541828870773315, 1.0439351797103882, 1.0606096982955933, 1.0256390571594238, 1.041204810142517, 1.0863381624221802, 1.0856475830078125, 1.0624827146530151, 1.0422675609588623, 1.0369549989700317, 1.0345383882522583, 1.027668833732605, 1.043601632118225, 1.0765904188156128, 1.0611785650253296, 1.0755035877227783, 1.0501728057861328, 1.0246371030807495, 1.0100055932998657, 1.033939242362976, 1.06064772605896, 1.030604362487793]
[2025-05-12 14:59:12,113]: Mean: 1.04571390
[2025-05-12 14:59:12,113]: Min: 1.00377250
[2025-05-12 14:59:12,114]: Max: 1.08633816
[2025-05-12 14:59:12,114]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 14:59:12,114]: Sample Values (25 elements): [-0.3231790363788605, -0.2790571451187134, -0.4485992193222046, -0.03312954306602478, -0.37949636578559875, -0.023742368444800377, 0.35741689801216125, -0.4346294701099396, 0.14650259912014008, -0.20413218438625336, -0.4794037938117981, 0.2871735095977783, 0.38862666487693787, 0.3809266984462738, 0.3573479950428009, -0.2907862961292267, 0.06333448737859726, 0.0809536725282669, -0.3788343369960785, -0.15907856822013855, 0.20103143155574799, -0.36449265480041504, -0.47439438104629517, -0.26886531710624695, 0.17163285613059998]
[2025-05-12 14:59:12,115]: Mean: 0.00304282
[2025-05-12 14:59:12,115]: Min: -0.49025491
[2025-05-12 14:59:12,115]: Max: 0.50022864
[2025-05-12 14:59:12,115]: 


QAT of ResNet20 with hardtanh down to 2 bits...
[2025-05-12 14:59:12,514]: [ResNet20_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-12 14:59:12,685]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 15:01:09,364]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.3165 Train Acc: 0.5447 Eval Loss: 1.2859 Eval Acc: 0.5558 (LR: 0.001000)
[2025-05-12 15:03:06,000]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 1.0497 Train Acc: 0.6308 Eval Loss: 1.0735 Eval Acc: 0.6303 (LR: 0.001000)
[2025-05-12 15:05:02,135]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.9958 Train Acc: 0.6507 Eval Loss: 1.0350 Eval Acc: 0.6349 (LR: 0.001000)
[2025-05-12 15:06:58,612]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.9691 Train Acc: 0.6588 Eval Loss: 1.2004 Eval Acc: 0.5841 (LR: 0.001000)
[2025-05-12 15:08:54,469]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.9482 Train Acc: 0.6665 Eval Loss: 1.0922 Eval Acc: 0.6311 (LR: 0.001000)
[2025-05-12 15:10:49,067]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.9399 Train Acc: 0.6676 Eval Loss: 1.1159 Eval Acc: 0.6212 (LR: 0.001000)
[2025-05-12 15:12:45,195]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.9266 Train Acc: 0.6743 Eval Loss: 1.0454 Eval Acc: 0.6400 (LR: 0.001000)
[2025-05-12 15:14:36,858]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.9155 Train Acc: 0.6766 Eval Loss: 1.0576 Eval Acc: 0.6440 (LR: 0.001000)
[2025-05-12 15:16:28,546]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.9053 Train Acc: 0.6811 Eval Loss: 1.0182 Eval Acc: 0.6425 (LR: 0.001000)
[2025-05-12 15:18:22,957]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.9089 Train Acc: 0.6799 Eval Loss: 0.9692 Eval Acc: 0.6686 (LR: 0.001000)
[2025-05-12 15:20:17,734]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.8945 Train Acc: 0.6874 Eval Loss: 1.1803 Eval Acc: 0.6154 (LR: 0.001000)
[2025-05-12 15:22:10,751]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.8926 Train Acc: 0.6873 Eval Loss: 1.0065 Eval Acc: 0.6550 (LR: 0.001000)
[2025-05-12 15:24:03,525]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.8806 Train Acc: 0.6917 Eval Loss: 1.2830 Eval Acc: 0.5776 (LR: 0.001000)
[2025-05-12 15:25:54,909]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.8726 Train Acc: 0.6948 Eval Loss: 1.1431 Eval Acc: 0.6176 (LR: 0.001000)
[2025-05-12 15:27:45,079]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.8660 Train Acc: 0.6948 Eval Loss: 0.9369 Eval Acc: 0.6762 (LR: 0.001000)
[2025-05-12 15:29:34,791]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.8591 Train Acc: 0.6979 Eval Loss: 0.9134 Eval Acc: 0.6852 (LR: 0.001000)
[2025-05-12 15:31:25,251]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.8525 Train Acc: 0.7020 Eval Loss: 0.8571 Eval Acc: 0.7007 (LR: 0.001000)
[2025-05-12 15:33:13,076]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.8487 Train Acc: 0.7029 Eval Loss: 1.1277 Eval Acc: 0.6316 (LR: 0.001000)
[2025-05-12 15:35:02,582]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.8384 Train Acc: 0.7043 Eval Loss: 0.8901 Eval Acc: 0.6852 (LR: 0.001000)
[2025-05-12 15:36:51,490]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.8391 Train Acc: 0.7049 Eval Loss: 1.2495 Eval Acc: 0.5919 (LR: 0.001000)
[2025-05-12 15:38:41,013]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.8394 Train Acc: 0.7050 Eval Loss: 0.9510 Eval Acc: 0.6755 (LR: 0.001000)
[2025-05-12 15:40:30,455]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.8387 Train Acc: 0.7057 Eval Loss: 0.9857 Eval Acc: 0.6571 (LR: 0.001000)
[2025-05-12 15:42:20,129]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.8307 Train Acc: 0.7092 Eval Loss: 0.9124 Eval Acc: 0.6826 (LR: 0.001000)
[2025-05-12 15:44:09,443]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.8302 Train Acc: 0.7102 Eval Loss: 0.9150 Eval Acc: 0.6889 (LR: 0.001000)
[2025-05-12 15:45:59,388]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.8255 Train Acc: 0.7103 Eval Loss: 0.8959 Eval Acc: 0.6963 (LR: 0.001000)
[2025-05-12 15:47:48,374]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.8235 Train Acc: 0.7091 Eval Loss: 0.9946 Eval Acc: 0.6642 (LR: 0.001000)
[2025-05-12 15:49:42,604]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.8196 Train Acc: 0.7133 Eval Loss: 0.9715 Eval Acc: 0.6648 (LR: 0.001000)
[2025-05-12 15:51:39,728]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.8210 Train Acc: 0.7115 Eval Loss: 1.0930 Eval Acc: 0.6404 (LR: 0.001000)
[2025-05-12 15:53:39,506]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.8155 Train Acc: 0.7138 Eval Loss: 1.1961 Eval Acc: 0.6161 (LR: 0.001000)
[2025-05-12 15:55:39,924]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.8106 Train Acc: 0.7174 Eval Loss: 0.9509 Eval Acc: 0.6775 (LR: 0.000250)
[2025-05-12 15:57:39,751]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.7451 Train Acc: 0.7412 Eval Loss: 0.7817 Eval Acc: 0.7294 (LR: 0.000250)
[2025-05-12 15:59:42,516]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.7445 Train Acc: 0.7404 Eval Loss: 0.7386 Eval Acc: 0.7404 (LR: 0.000250)
[2025-05-12 16:01:43,173]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.7458 Train Acc: 0.7388 Eval Loss: 0.8972 Eval Acc: 0.7007 (LR: 0.000250)
[2025-05-12 16:03:43,007]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.7497 Train Acc: 0.7373 Eval Loss: 0.8684 Eval Acc: 0.7033 (LR: 0.000250)
[2025-05-12 16:05:40,202]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.7448 Train Acc: 0.7391 Eval Loss: 0.7785 Eval Acc: 0.7314 (LR: 0.000250)
[2025-05-12 16:07:35,339]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.7420 Train Acc: 0.7426 Eval Loss: 0.7966 Eval Acc: 0.7308 (LR: 0.000250)
[2025-05-12 16:09:28,319]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.7462 Train Acc: 0.7398 Eval Loss: 0.9569 Eval Acc: 0.6807 (LR: 0.000250)
[2025-05-12 16:11:21,685]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.7398 Train Acc: 0.7418 Eval Loss: 0.7612 Eval Acc: 0.7389 (LR: 0.000250)
[2025-05-12 16:13:15,905]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.7492 Train Acc: 0.7363 Eval Loss: 0.8536 Eval Acc: 0.7071 (LR: 0.000250)
[2025-05-12 16:15:11,679]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.7445 Train Acc: 0.7396 Eval Loss: 0.7948 Eval Acc: 0.7268 (LR: 0.000250)
[2025-05-12 16:17:04,886]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.7466 Train Acc: 0.7388 Eval Loss: 0.7948 Eval Acc: 0.7244 (LR: 0.000250)
[2025-05-12 16:18:54,898]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.7491 Train Acc: 0.7354 Eval Loss: 0.8152 Eval Acc: 0.7169 (LR: 0.000250)
[2025-05-12 16:20:44,465]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.7458 Train Acc: 0.7393 Eval Loss: 0.8143 Eval Acc: 0.7214 (LR: 0.000250)
[2025-05-12 16:22:32,552]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.7451 Train Acc: 0.7395 Eval Loss: 0.7924 Eval Acc: 0.7274 (LR: 0.000250)
[2025-05-12 16:24:21,788]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.7455 Train Acc: 0.7392 Eval Loss: 0.9071 Eval Acc: 0.6938 (LR: 0.000063)
[2025-05-12 16:26:13,341]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.7114 Train Acc: 0.7527 Eval Loss: 0.7193 Eval Acc: 0.7498 (LR: 0.000063)
[2025-05-12 16:28:07,124]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.7091 Train Acc: 0.7533 Eval Loss: 0.7299 Eval Acc: 0.7460 (LR: 0.000063)
[2025-05-12 16:30:00,993]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.7115 Train Acc: 0.7535 Eval Loss: 0.7488 Eval Acc: 0.7398 (LR: 0.000063)
[2025-05-12 16:31:54,624]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.7155 Train Acc: 0.7507 Eval Loss: 0.7675 Eval Acc: 0.7312 (LR: 0.000063)
[2025-05-12 16:33:48,117]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.7112 Train Acc: 0.7536 Eval Loss: 0.7103 Eval Acc: 0.7514 (LR: 0.000063)
[2025-05-12 16:35:43,668]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.7105 Train Acc: 0.7530 Eval Loss: 0.8081 Eval Acc: 0.7284 (LR: 0.000063)
[2025-05-12 16:37:39,053]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.7126 Train Acc: 0.7526 Eval Loss: 0.8402 Eval Acc: 0.7139 (LR: 0.000063)
[2025-05-12 16:39:33,690]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.7089 Train Acc: 0.7514 Eval Loss: 0.8563 Eval Acc: 0.7160 (LR: 0.000063)
[2025-05-12 16:42:16,876]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.7066 Train Acc: 0.7547 Eval Loss: 0.8300 Eval Acc: 0.7155 (LR: 0.000063)
[2025-05-12 16:44:48,590]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.7145 Train Acc: 0.7513 Eval Loss: 0.7716 Eval Acc: 0.7377 (LR: 0.000063)
[2025-05-12 16:46:49,346]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.7147 Train Acc: 0.7498 Eval Loss: 0.8653 Eval Acc: 0.7049 (LR: 0.000063)
[2025-05-12 16:48:53,113]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.7123 Train Acc: 0.7518 Eval Loss: 0.8547 Eval Acc: 0.7113 (LR: 0.000063)
[2025-05-12 16:50:45,629]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.7162 Train Acc: 0.7506 Eval Loss: 0.8233 Eval Acc: 0.7183 (LR: 0.000063)
[2025-05-12 16:52:46,846]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.7183 Train Acc: 0.7505 Eval Loss: 0.8387 Eval Acc: 0.7128 (LR: 0.000063)
[2025-05-12 16:54:50,857]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.7191 Train Acc: 0.7506 Eval Loss: 0.7353 Eval Acc: 0.7485 (LR: 0.000063)
[2025-05-12 16:54:50,859]: [ResNet20_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.7514
[2025-05-12 16:54:50,905]: 


Quantization of model down to 2 bits finished
[2025-05-12 16:54:50,905]: Model Architecture:
[2025-05-12 16:54:51,328]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2159], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32097864151000977, max_val=0.326620489358902)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3215], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48223766684532166, max_val=0.482122004032135)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2380], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3332904875278473, max_val=0.3807760775089264)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1769], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26537415385246277, max_val=0.2653237283229828)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1738], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24641293287277222, max_val=0.2749367654323578)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1739], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24472013115882874, max_val=0.27709847688674927)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1220], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18275536596775055, max_val=0.18312016129493713)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1073], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16180972754955292, max_val=0.16002590954303741)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2395], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3592694401741028, max_val=0.35929954051971436)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1048], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15661749243736267, max_val=0.15764956176280975)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1061], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1565425544977188, max_val=0.1617422252893448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0992], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14882397651672363, max_val=0.14885473251342773)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1036], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15627451241016388, max_val=0.15457795560359955)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0835], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12513312697410583, max_val=0.125304713845253)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0717], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11078573763370514, max_val=0.10436592996120453)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1599], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24832384288311005, max_val=0.23141764104366302)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10970909148454666, max_val=0.11997699737548828)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0703], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10545840859413147, max_val=0.10542170703411102)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0616], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09240242093801498, max_val=0.09247596561908722)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0533], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08012311160564423, max_val=0.0796479880809784)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 16:54:51,328]: 
Model Weights:
[2025-05-12 16:54:51,328]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 16:54:51,338]: Sample Values (25 elements): [0.28053197264671326, 0.13172204792499542, -0.24865925312042236, -0.049971986562013626, 0.10391097515821457, 0.37474751472473145, -0.11898744851350784, 0.21934370696544647, 0.11743789911270142, 0.1279652863740921, 0.041199877858161926, 0.07012464851140976, 0.29321229457855225, 0.025088634341955185, -0.12658679485321045, -0.10852128267288208, 0.11069777607917786, 0.09593591839075089, 0.12515275180339813, 0.1730635017156601, 0.11085574328899384, -0.14386135339736938, -0.2368401437997818, 0.04594939574599266, -0.373169481754303]
[2025-05-12 16:54:51,342]: Mean: -0.00342728
[2025-05-12 16:54:51,349]: Min: -0.45523608
[2025-05-12 16:54:51,350]: Max: 0.60562181
[2025-05-12 16:54:51,351]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,352]: Sample Values (16 elements): [0.9260684251785278, 0.7098086476325989, 1.0576001405715942, 0.8985550403594971, 1.2587895393371582, 1.2774779796600342, 0.9807219505310059, 1.058302640914917, 1.0409784317016602, 1.0887346267700195, 0.8636408448219299, 0.7137815356254578, 0.9621039032936096, 1.4633508920669556, 1.1286803483963013, 0.8125845789909363]
[2025-05-12 16:54:51,353]: Mean: 1.01507378
[2025-05-12 16:54:51,353]: Min: 0.70980865
[2025-05-12 16:54:51,353]: Max: 1.46335089
[2025-05-12 16:54:51,363]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:54:51,365]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21586677432060242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21586677432060242, 0.0, 0.0, 0.0, 0.21586677432060242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21586677432060242]
[2025-05-12 16:54:51,366]: Mean: 0.00168646
[2025-05-12 16:54:51,367]: Min: -0.21586677
[2025-05-12 16:54:51,367]: Max: 0.43173355
[2025-05-12 16:54:51,367]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,369]: Sample Values (16 elements): [1.0737377405166626, 0.8396254181861877, 1.052399754524231, 1.0138449668884277, 1.0001083612442017, 0.9753391742706299, 0.9630717635154724, 1.1510083675384521, 1.056236982345581, 1.0826576948165894, 0.910476803779602, 0.8411164283752441, 0.8184739947319031, 0.8901497721672058, 0.8903902173042297, 1.1252175569534302]
[2025-05-12 16:54:51,370]: Mean: 0.98024094
[2025-05-12 16:54:51,373]: Min: 0.81847399
[2025-05-12 16:54:51,376]: Max: 1.15100837
[2025-05-12 16:54:51,385]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:54:51,394]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3214535713195801, 0.0, 0.0, 0.0, 0.3214535713195801, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3214535713195801, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,402]: Mean: -0.00446463
[2025-05-12 16:54:51,406]: Min: -0.64290714
[2025-05-12 16:54:51,412]: Max: 0.32145357
[2025-05-12 16:54:51,412]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,413]: Sample Values (16 elements): [0.9184654951095581, 0.9442341327667236, 1.1808412075042725, 0.8262363076210022, 1.192805290222168, 1.1225382089614868, 0.6393318176269531, 0.8770884871482849, 0.6040710806846619, 0.7482358813285828, 0.8093769550323486, 0.6474668383598328, 0.7238456010818481, 0.723106324672699, 0.6553202271461487, 0.8622855544090271]
[2025-05-12 16:54:51,413]: Mean: 0.84220308
[2025-05-12 16:54:51,414]: Min: 0.60407108
[2025-05-12 16:54:51,414]: Max: 1.19280529
[2025-05-12 16:54:51,423]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:54:51,426]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23802237212657928, 0.0, 0.0, 0.0, -0.23802237212657928, 0.0, 0.0, -0.23802237212657928, 0.0]
[2025-05-12 16:54:51,426]: Mean: 0.00237609
[2025-05-12 16:54:51,427]: Min: -0.23802237
[2025-05-12 16:54:51,429]: Max: 0.47604474
[2025-05-12 16:54:51,429]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,430]: Sample Values (16 elements): [1.119987964630127, 0.9503121972084045, 1.0199309587478638, 0.9653679132461548, 0.9481907486915588, 1.040834903717041, 1.14609694480896, 0.9786710143089294, 1.009506344795227, 0.9330796599388123, 1.0872745513916016, 0.9977151155471802, 1.1109163761138916, 0.9479442238807678, 1.0312246084213257, 1.0754655599594116]
[2025-05-12 16:54:51,431]: Mean: 1.02265739
[2025-05-12 16:54:51,433]: Min: 0.93307966
[2025-05-12 16:54:51,436]: Max: 1.14609694
[2025-05-12 16:54:51,444]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:54:51,459]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.17689940333366394, -0.17689940333366394, 0.17689940333366394, 0.0, 0.17689940333366394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17689940333366394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17689940333366394, 0.17689940333366394, 0.17689940333366394, 0.17689940333366394, 0.0, 0.0]
[2025-05-12 16:54:51,466]: Mean: 0.00314795
[2025-05-12 16:54:51,472]: Min: -0.35379881
[2025-05-12 16:54:51,474]: Max: 0.17689940
[2025-05-12 16:54:51,474]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,474]: Sample Values (16 elements): [1.0800385475158691, 1.0449298620224, 0.9243561029434204, 0.8940896987915039, 0.8009995222091675, 0.7969807386398315, 0.8465238213539124, 0.8235105872154236, 1.0019034147262573, 0.615502655506134, 0.8127151131629944, 0.9167937636375427, 0.8398486375808716, 0.7135183215141296, 0.8080657124519348, 0.9941572546958923]
[2025-05-12 16:54:51,474]: Mean: 0.86962086
[2025-05-12 16:54:51,475]: Min: 0.61550266
[2025-05-12 16:54:51,475]: Max: 1.08003855
[2025-05-12 16:54:51,486]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:54:51,488]: Sample Values (25 elements): [-0.1737832874059677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1737832874059677, 0.1737832874059677, 0.0, 0.1737832874059677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1737832874059677, 0.0, 0.0]
[2025-05-12 16:54:51,489]: Mean: 0.00573243
[2025-05-12 16:54:51,490]: Min: -0.17378329
[2025-05-12 16:54:51,491]: Max: 0.34756657
[2025-05-12 16:54:51,491]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,493]: Sample Values (16 elements): [0.9507681131362915, 0.9769986867904663, 0.9461479783058167, 0.9808817505836487, 0.8849419951438904, 1.0462509393692017, 1.0691533088684082, 0.9336168169975281, 0.9020470380783081, 0.9466335773468018, 1.0505893230438232, 1.0701566934585571, 1.064308762550354, 0.9115874767303467, 1.00593101978302, 0.9222590327262878]
[2025-05-12 16:54:51,495]: Mean: 0.97889203
[2025-05-12 16:54:51,498]: Min: 0.88494200
[2025-05-12 16:54:51,500]: Max: 1.07015669
[2025-05-12 16:54:51,517]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:54:51,526]: Sample Values (25 elements): [0.17393958568572998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17393958568572998, 0.0, 0.0, 0.0, 0.17393958568572998, 0.0, -0.17393958568572998, 0.0, 0.0, -0.17393958568572998, 0.17393958568572998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,532]: Mean: 0.00158539
[2025-05-12 16:54:51,537]: Min: -0.17393959
[2025-05-12 16:54:51,538]: Max: 0.34787917
[2025-05-12 16:54:51,539]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:54:51,539]: Sample Values (16 elements): [0.8670734167098999, 0.9196046590805054, 0.9034306406974792, 0.8679407835006714, 0.8705118298530579, 0.8894628882408142, 0.8892400860786438, 0.8482442498207092, 0.9714058041572571, 0.9375013113021851, 0.7754012942314148, 0.9010114073753357, 0.980151891708374, 0.8231704831123352, 0.8421815037727356, 0.8431071043014526]
[2025-05-12 16:54:51,539]: Mean: 0.88309002
[2025-05-12 16:54:51,539]: Min: 0.77540129
[2025-05-12 16:54:51,540]: Max: 0.98015189
[2025-05-12 16:54:51,547]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 16:54:51,551]: Sample Values (25 elements): [0.0, 0.0, 0.12195852398872375, 0.12195852398872375, -0.12195852398872375, 0.0, 0.12195852398872375, 0.12195852398872375, 0.12195852398872375, -0.12195852398872375, -0.12195852398872375, -0.12195852398872375, 0.0, 0.0, 0.0, -0.12195852398872375, 0.0, 0.0, 0.0, 0.0, 0.0, -0.12195852398872375, 0.12195852398872375, 0.0, 0.0]
[2025-05-12 16:54:51,552]: Mean: 0.00140273
[2025-05-12 16:54:51,553]: Min: -0.12195852
[2025-05-12 16:54:51,554]: Max: 0.24391705
[2025-05-12 16:54:51,554]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,556]: Sample Values (25 elements): [0.9757612347602844, 1.0409413576126099, 0.9674537181854248, 0.9704944491386414, 0.9558881521224976, 0.9543964266777039, 0.9321513772010803, 0.9730972647666931, 1.0084099769592285, 0.9813823699951172, 1.0612738132476807, 0.9562616944313049, 1.003743052482605, 0.9714847803115845, 0.9514838457107544, 0.9431825876235962, 0.9457882642745972, 0.9843741655349731, 0.9636247158050537, 0.9993340969085693, 0.964566707611084, 0.9764572381973267, 0.9951555132865906, 0.9206213355064392, 0.9717416167259216]
[2025-05-12 16:54:51,557]: Mean: 0.97306037
[2025-05-12 16:54:51,558]: Min: 0.92062134
[2025-05-12 16:54:51,559]: Max: 1.06127381
[2025-05-12 16:54:51,571]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:54:51,578]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.10727843642234802, 0.0, 0.0, -0.10727843642234802, 0.0, 0.0, 0.0, 0.0, -0.10727843642234802, 0.10727843642234802, -0.10727843642234802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10727843642234802, 0.0]
[2025-05-12 16:54:51,582]: Mean: -0.00119897
[2025-05-12 16:54:51,587]: Min: -0.21455687
[2025-05-12 16:54:51,592]: Max: 0.10727844
[2025-05-12 16:54:51,592]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,600]: Sample Values (25 elements): [0.9813316464424133, 1.022336483001709, 1.0500751733779907, 0.991628885269165, 0.9415284991264343, 1.05207359790802, 0.9140796661376953, 1.0436855554580688, 0.9647455215454102, 0.8302924036979675, 0.9620747566223145, 0.9740056991577148, 0.9818123579025269, 0.9760769605636597, 0.9326271414756775, 0.988422691822052, 0.9509556889533997, 0.96624356508255, 1.002058982849121, 0.9172102212905884, 0.9777591824531555, 0.9575623869895935, 1.007062554359436, 1.0235037803649902, 0.9624766707420349]
[2025-05-12 16:54:51,602]: Mean: 0.97530270
[2025-05-12 16:54:51,602]: Min: 0.83029240
[2025-05-12 16:54:51,602]: Max: 1.05911565
[2025-05-12 16:54:51,603]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 16:54:51,604]: Sample Values (25 elements): [0.0, 0.0, -0.23952317237854004, 0.0, -0.23952317237854004, 0.0, 0.0, 0.0, 0.0, -0.23952317237854004, 0.0, 0.0, 0.23952317237854004, 0.0, 0.0, 0.0, -0.23952317237854004, 0.0, 0.23952317237854004, 0.0, 0.0, -0.23952317237854004, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,604]: Mean: 0.00514601
[2025-05-12 16:54:51,605]: Min: -0.23952317
[2025-05-12 16:54:51,606]: Max: 0.47904634
[2025-05-12 16:54:51,607]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,616]: Sample Values (25 elements): [0.7713936567306519, 0.8343436121940613, 0.8867996335029602, 0.8105154633522034, 0.85014408826828, 0.8913351893424988, 0.8513799905776978, 0.864091694355011, 0.8594980239868164, 0.8177903890609741, 0.8136035203933716, 0.8546696305274963, 0.8180053234100342, 0.8673256635665894, 0.890612781047821, 0.8380402326583862, 0.8357293009757996, 0.8363015651702881, 0.9050966501235962, 0.876474142074585, 0.8672307729721069, 0.8292993903160095, 0.8370833396911621, 0.8783047199249268, 0.8056609630584717]
[2025-05-12 16:54:51,617]: Mean: 0.83496600
[2025-05-12 16:54:51,618]: Min: 0.73391759
[2025-05-12 16:54:51,619]: Max: 0.90509665
[2025-05-12 16:54:51,630]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:54:51,633]: Sample Values (25 elements): [-0.10475587844848633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10475587844848633, 0.0, 0.0, 0.0, 0.10475587844848633, -0.10475587844848633, -0.10475587844848633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,639]: Mean: 0.00061380
[2025-05-12 16:54:51,644]: Min: -0.10475588
[2025-05-12 16:54:51,648]: Max: 0.20951176
[2025-05-12 16:54:51,648]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,657]: Sample Values (25 elements): [0.9848929643630981, 1.004071593284607, 1.043139100074768, 0.9788318276405334, 0.9858342409133911, 1.0052118301391602, 0.9648955464363098, 1.0268313884735107, 0.9856777787208557, 0.9955922961235046, 1.032367467880249, 0.9759873747825623, 1.0001300573349, 1.0276799201965332, 1.0517114400863647, 1.0319873094558716, 1.021004557609558, 1.012305498123169, 0.9389632344245911, 0.9266286492347717, 1.0096031427383423, 1.0527080297470093, 1.0086642503738403, 0.9998539090156555, 0.9950200319290161]
[2025-05-12 16:54:51,662]: Mean: 1.00180268
[2025-05-12 16:54:51,663]: Min: 0.92662865
[2025-05-12 16:54:51,663]: Max: 1.05517673
[2025-05-12 16:54:51,664]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:54:51,668]: Sample Values (25 elements): [0.10609477013349533, 0.0, 0.0, 0.10609477013349533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10609477013349533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,671]: Mean: 0.00065619
[2025-05-12 16:54:51,673]: Min: -0.10609477
[2025-05-12 16:54:51,675]: Max: 0.21218954
[2025-05-12 16:54:51,675]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,676]: Sample Values (25 elements): [0.9677553772926331, 0.9656670093536377, 0.8888967037200928, 0.9853641390800476, 0.8550728559494019, 1.0052608251571655, 0.8902027010917664, 0.9323870539665222, 0.9806176424026489, 0.9339066743850708, 0.9527279734611511, 0.9481087327003479, 0.9973620176315308, 0.895489513874054, 0.9625666737556458, 1.0413252115249634, 0.8885402679443359, 0.9683938026428223, 0.9466301798820496, 0.923730194568634, 0.9664828777313232, 0.9476984143257141, 1.0201233625411987, 0.9018372297286987, 0.9713663458824158]
[2025-05-12 16:54:51,677]: Mean: 0.94802177
[2025-05-12 16:54:51,678]: Min: 0.85507286
[2025-05-12 16:54:51,679]: Max: 1.04132521
[2025-05-12 16:54:51,688]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:54:51,692]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09922634065151215, 0.0, 0.0, -0.09922634065151215, 0.0, 0.0, 0.09922634065151215, 0.0, 0.09922634065151215, 0.0]
[2025-05-12 16:54:51,695]: Mean: 0.00024764
[2025-05-12 16:54:51,698]: Min: -0.09922634
[2025-05-12 16:54:51,703]: Max: 0.19845268
[2025-05-12 16:54:51,703]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,711]: Sample Values (25 elements): [0.96205073595047, 1.0400311946868896, 0.9908013939857483, 1.0078296661376953, 0.9968785643577576, 1.016719937324524, 0.9635919332504272, 0.9614982008934021, 0.9719276428222656, 0.9970947504043579, 0.9984824657440186, 0.9707107543945312, 0.953819215297699, 0.9804037809371948, 1.0127815008163452, 0.996808648109436, 0.9734736084938049, 0.991949200630188, 0.9884217381477356, 0.9874484539031982, 0.9922935962677002, 1.034284234046936, 1.0115973949432373, 0.9926411509513855, 1.0169380903244019]
[2025-05-12 16:54:51,716]: Mean: 0.99558842
[2025-05-12 16:54:51,721]: Min: 0.95381922
[2025-05-12 16:54:51,722]: Max: 1.05951703
[2025-05-12 16:54:51,723]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:54:51,727]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.10361765325069427, 0.0, 0.10361765325069427, 0.0, 0.0, 0.0, 0.0, 0.10361765325069427, 0.0, -0.10361765325069427, 0.10361765325069427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,730]: Mean: -0.00053968
[2025-05-12 16:54:51,737]: Min: -0.20723531
[2025-05-12 16:54:51,739]: Max: 0.10361765
[2025-05-12 16:54:51,739]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:54:51,740]: Sample Values (25 elements): [0.9871906638145447, 1.0117555856704712, 0.9647657871246338, 1.0040489435195923, 0.9653238654136658, 0.8860368728637695, 0.9555765390396118, 0.9262872338294983, 0.9845312237739563, 1.0186011791229248, 0.9468146562576294, 0.9799671769142151, 0.9478564262390137, 1.0089516639709473, 0.9837835431098938, 0.9786823391914368, 0.9809222221374512, 0.9740889072418213, 1.0839799642562866, 0.9515937566757202, 0.9828163385391235, 0.9225478172302246, 1.0267163515090942, 1.008734107017517, 0.944471538066864]
[2025-05-12 16:54:51,741]: Mean: 0.97313559
[2025-05-12 16:54:51,742]: Min: 0.88603687
[2025-05-12 16:54:51,743]: Max: 1.08397996
[2025-05-12 16:54:51,754]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 16:54:51,759]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.08347932994365692, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08347932994365692, 0.0, 0.0, 0.0, 0.08347932994365692, 0.0, -0.08347932994365692, -0.08347932994365692, 0.0, -0.08347932994365692, -0.08347932994365692, 0.0, -0.08347932994365692, 0.0, 0.0]
[2025-05-12 16:54:51,761]: Mean: 0.00049819
[2025-05-12 16:54:51,763]: Min: -0.08347933
[2025-05-12 16:54:51,766]: Max: 0.16695866
[2025-05-12 16:54:51,766]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,778]: Sample Values (25 elements): [0.9963292479515076, 0.9785407781600952, 0.9433410167694092, 0.9783247113227844, 0.9778218865394592, 0.980754554271698, 0.9512700438499451, 0.9824206829071045, 0.9665530920028687, 0.970410943031311, 0.9939502477645874, 0.9997262954711914, 0.960286557674408, 0.9856835603713989, 0.9820681810379028, 0.9619610905647278, 0.9945937395095825, 0.9683921337127686, 0.9940655827522278, 1.0033962726593018, 0.9891467094421387, 0.9367272853851318, 0.977306067943573, 1.0026336908340454, 1.0040671825408936]
[2025-05-12 16:54:51,780]: Mean: 0.98218805
[2025-05-12 16:54:51,797]: Min: 0.93672729
[2025-05-12 16:54:51,802]: Max: 1.03849626
[2025-05-12 16:54:51,821]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:54:51,822]: Sample Values (25 elements): [-0.07171715050935745, 0.0, -0.07171715050935745, 0.0, 0.07171715050935745, -0.07171715050935745, -0.07171715050935745, 0.0, 0.0, 0.0, 0.0, 0.07171715050935745, 0.0, 0.0, 0.07171715050935745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,822]: Mean: 0.00030544
[2025-05-12 16:54:51,822]: Min: -0.14343430
[2025-05-12 16:54:51,824]: Max: 0.07171715
[2025-05-12 16:54:51,824]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,829]: Sample Values (25 elements): [0.9720685482025146, 1.0182790756225586, 0.998889148235321, 0.9904823899269104, 1.0004650354385376, 1.0011446475982666, 1.0196412801742554, 0.9823885560035706, 1.0064595937728882, 1.0238325595855713, 1.0149568319320679, 1.020276427268982, 0.9901790022850037, 1.0131274461746216, 1.0248794555664062, 0.9473415017127991, 0.9772573709487915, 0.9916965365409851, 1.061644196510315, 0.98868328332901, 1.0028034448623657, 1.01132071018219, 0.9964072108268738, 1.0187923908233643, 1.0058460235595703]
[2025-05-12 16:54:51,832]: Mean: 1.00165701
[2025-05-12 16:54:51,833]: Min: 0.94373065
[2025-05-12 16:54:51,834]: Max: 1.07550609
[2025-05-12 16:54:51,838]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 16:54:51,840]: Sample Values (25 elements): [-0.1599138081073761, 0.1599138081073761, 0.0, 0.1599138081073761, 0.0, 0.1599138081073761, 0.1599138081073761, -0.1599138081073761, -0.1599138081073761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1599138081073761, 0.0, 0.1599138081073761, 0.0, 0.0, 0.1599138081073761, 0.1599138081073761, 0.0, 0.1599138081073761, -0.1599138081073761, 0.0]
[2025-05-12 16:54:51,843]: Mean: 0.00281098
[2025-05-12 16:54:51,846]: Min: -0.31982762
[2025-05-12 16:54:51,847]: Max: 0.15991381
[2025-05-12 16:54:51,848]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,852]: Sample Values (25 elements): [0.9037330746650696, 0.8933594822883606, 0.8869817852973938, 0.8802148699760437, 0.8987594842910767, 0.8769081234931946, 0.9236533641815186, 0.9041351079940796, 0.8951179385185242, 0.9224097728729248, 0.8184477686882019, 0.8905857801437378, 0.919910192489624, 0.9152430891990662, 0.9034806489944458, 0.8768496513366699, 0.8810494542121887, 0.8618082404136658, 0.8945246934890747, 0.8603301644325256, 0.8913626074790955, 0.9161584973335266, 0.874260663986206, 0.9026318788528442, 0.9437571167945862]
[2025-05-12 16:54:51,854]: Mean: 0.89293230
[2025-05-12 16:54:51,859]: Min: 0.81169093
[2025-05-12 16:54:51,862]: Max: 0.94375712
[2025-05-12 16:54:51,880]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:54:51,882]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07656192779541016, 0.0, 0.0, 0.0, 0.0, 0.07656192779541016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07656192779541016, 0.0, 0.0, 0.07656192779541016, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,882]: Mean: 0.00032815
[2025-05-12 16:54:51,883]: Min: -0.07656193
[2025-05-12 16:54:51,883]: Max: 0.15312386
[2025-05-12 16:54:51,883]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,889]: Sample Values (25 elements): [0.9908449649810791, 1.0171573162078857, 1.0571541786193848, 0.9987694621086121, 0.9855130314826965, 1.0397528409957886, 0.9882286190986633, 1.007038950920105, 1.0610567331314087, 1.0136301517486572, 0.991294264793396, 1.1066604852676392, 1.028056025505066, 0.9989806413650513, 1.0003336668014526, 1.0630931854248047, 0.9958365559577942, 0.9943879842758179, 0.999793291091919, 1.0392683744430542, 1.0037531852722168, 1.0055749416351318, 1.0004057884216309, 0.9761547446250916, 1.0060440301895142]
[2025-05-12 16:54:51,890]: Mean: 1.01945221
[2025-05-12 16:54:51,892]: Min: 0.96090436
[2025-05-12 16:54:51,893]: Max: 1.10666049
[2025-05-12 16:54:51,897]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:54:51,900]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07029321789741516, 0.0, 0.0, 0.0, 0.0, -0.07029321789741516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07029321789741516]
[2025-05-12 16:54:51,901]: Mean: 0.00000000
[2025-05-12 16:54:51,903]: Min: -0.07029322
[2025-05-12 16:54:51,905]: Max: 0.07029322
[2025-05-12 16:54:51,905]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,909]: Sample Values (25 elements): [0.9921467900276184, 0.9960933327674866, 0.9870991706848145, 0.9976165294647217, 1.0103110074996948, 0.9927160739898682, 0.9768620729446411, 0.9764977693557739, 1.0460240840911865, 0.9446209669113159, 0.9884775280952454, 0.9990289807319641, 0.9890440106391907, 1.008781909942627, 0.9891167879104614, 1.0047205686569214, 0.9854778051376343, 1.011216163635254, 0.9840463399887085, 1.0019887685775757, 0.9755668044090271, 1.0006929636001587, 0.9971495866775513, 0.9955460429191589, 1.0290182828903198]
[2025-05-12 16:54:51,912]: Mean: 0.99345720
[2025-05-12 16:54:51,912]: Min: 0.94462097
[2025-05-12 16:54:51,918]: Max: 1.07399690
[2025-05-12 16:54:51,942]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:54:51,942]: Sample Values (25 elements): [0.0616263821721077, 0.0, 0.0, 0.0, 0.0616263821721077, 0.0616263821721077, 0.0, -0.0616263821721077, 0.0, 0.0, 0.0, 0.0616263821721077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,943]: Mean: 0.00019058
[2025-05-12 16:54:51,943]: Min: -0.06162638
[2025-05-12 16:54:51,943]: Max: 0.12325276
[2025-05-12 16:54:51,943]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,944]: Sample Values (25 elements): [0.9663709998130798, 1.0152355432510376, 0.9861418008804321, 0.976594090461731, 0.9898806810379028, 0.986534833908081, 0.9733308553695679, 0.9645516872406006, 0.9857167601585388, 1.0004098415374756, 0.9947007894515991, 0.9768241047859192, 0.9650068283081055, 0.9765207171440125, 0.9763180613517761, 0.9898542165756226, 0.9656931161880493, 1.0542963743209839, 0.9809877872467041, 0.9619781374931335, 0.9817266464233398, 1.0149180889129639, 1.006075382232666, 1.0092767477035522, 0.993839681148529]
[2025-05-12 16:54:51,946]: Mean: 0.98198152
[2025-05-12 16:54:51,949]: Min: 0.94730604
[2025-05-12 16:54:51,952]: Max: 1.05429637
[2025-05-12 16:54:51,966]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:54:51,970]: Sample Values (25 elements): [0.0, 0.05325707420706749, 0.0, -0.05325707420706749, -0.05325707420706749, 0.0, 0.0, -0.05325707420706749, -0.05325707420706749, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05325707420706749, 0.05325707420706749, 0.0, 0.0, 0.0, 0.05325707420706749, 0.0, 0.0, 0.0]
[2025-05-12 16:54:51,972]: Mean: -0.00013002
[2025-05-12 16:54:51,974]: Min: -0.10651415
[2025-05-12 16:54:51,978]: Max: 0.05325707
[2025-05-12 16:54:51,978]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:54:51,984]: Sample Values (25 elements): [1.050060749053955, 1.0386348962783813, 1.023541808128357, 1.0636688470840454, 1.0246061086654663, 1.0198676586151123, 1.0196850299835205, 1.027255892753601, 1.027171015739441, 1.0339293479919434, 1.0129199028015137, 1.0410380363464355, 1.019123911857605, 1.053972840309143, 1.0486841201782227, 1.0264641046524048, 1.0256989002227783, 1.0193077325820923, 1.0649446249008179, 1.051029920578003, 1.038314938545227, 1.0318599939346313, 1.0162203311920166, 1.0340241193771362, 1.0741502046585083]
[2025-05-12 16:54:51,991]: Mean: 1.03411782
[2025-05-12 16:54:51,996]: Min: 0.99216491
[2025-05-12 16:54:52,001]: Max: 1.07415020
[2025-05-12 16:54:52,001]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 16:54:52,002]: Sample Values (25 elements): [0.24065759778022766, -0.248776376247406, 0.33021190762519836, 0.0343305841088295, 0.2796953320503235, 0.12113587558269501, -0.3564550280570984, -0.08604566007852554, -0.2563493251800537, 0.037038806825876236, 0.2001829892396927, 0.3424331545829773, -0.3194316029548645, -0.33486825227737427, 0.14612996578216553, 0.04686445742845535, -0.24662573635578156, -0.3378857672214508, 0.3398178219795227, 0.3598080575466156, 0.21989117562770844, -0.3209289610385895, 0.3502563536167145, 0.46480458974838257, -0.046941228210926056]
[2025-05-12 16:54:52,002]: Mean: 0.00304281
[2025-05-12 16:54:52,003]: Min: -0.47741798
[2025-05-12 16:54:52,003]: Max: 0.46480459
