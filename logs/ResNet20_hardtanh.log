[2025-05-26 18:26:48,408]: 
Training ResNet20 with hardtanh
[2025-05-26 18:27:53,443]: [ResNet20_hardtanh] Epoch: 001 Train Loss: 1.7980 Train Acc: 0.3292 Eval Loss: 1.5469 Eval Acc: 0.4204 (LR: 0.00100000)
[2025-05-26 18:28:41,075]: [ResNet20_hardtanh] Epoch: 002 Train Loss: 1.4408 Train Acc: 0.4755 Eval Loss: 1.3021 Eval Acc: 0.5208 (LR: 0.00100000)
[2025-05-26 18:29:46,231]: [ResNet20_hardtanh] Epoch: 003 Train Loss: 1.2712 Train Acc: 0.5431 Eval Loss: 1.1427 Eval Acc: 0.5834 (LR: 0.00100000)
[2025-05-26 18:30:26,533]: [ResNet20_hardtanh] Epoch: 004 Train Loss: 1.1656 Train Acc: 0.5819 Eval Loss: 1.1147 Eval Acc: 0.5985 (LR: 0.00100000)
[2025-05-26 18:31:31,111]: [ResNet20_hardtanh] Epoch: 005 Train Loss: 1.0787 Train Acc: 0.6153 Eval Loss: 1.0516 Eval Acc: 0.6322 (LR: 0.00100000)
[2025-05-26 18:32:15,169]: [ResNet20_hardtanh] Epoch: 006 Train Loss: 1.0190 Train Acc: 0.6342 Eval Loss: 0.9920 Eval Acc: 0.6421 (LR: 0.00100000)
[2025-05-26 18:33:19,224]: [ResNet20_hardtanh] Epoch: 007 Train Loss: 0.9720 Train Acc: 0.6505 Eval Loss: 1.0635 Eval Acc: 0.6169 (LR: 0.00100000)
[2025-05-26 18:34:09,225]: [ResNet20_hardtanh] Epoch: 008 Train Loss: 0.9262 Train Acc: 0.6699 Eval Loss: 0.8916 Eval Acc: 0.6858 (LR: 0.00100000)
[2025-05-26 18:35:12,085]: [ResNet20_hardtanh] Epoch: 009 Train Loss: 0.8889 Train Acc: 0.6824 Eval Loss: 1.2014 Eval Acc: 0.6026 (LR: 0.00100000)
[2025-05-26 18:36:10,595]: [ResNet20_hardtanh] Epoch: 010 Train Loss: 0.8614 Train Acc: 0.6943 Eval Loss: 0.8935 Eval Acc: 0.6911 (LR: 0.00100000)
[2025-05-26 18:37:03,993]: [ResNet20_hardtanh] Epoch: 011 Train Loss: 0.8327 Train Acc: 0.7045 Eval Loss: 0.8783 Eval Acc: 0.6921 (LR: 0.00100000)
[2025-05-26 18:38:08,678]: [ResNet20_hardtanh] Epoch: 012 Train Loss: 0.8045 Train Acc: 0.7165 Eval Loss: 0.8499 Eval Acc: 0.7041 (LR: 0.00100000)
[2025-05-26 18:38:58,741]: [ResNet20_hardtanh] Epoch: 013 Train Loss: 0.7860 Train Acc: 0.7230 Eval Loss: 0.8092 Eval Acc: 0.7221 (LR: 0.00100000)
[2025-05-26 18:40:03,411]: [ResNet20_hardtanh] Epoch: 014 Train Loss: 0.7625 Train Acc: 0.7278 Eval Loss: 0.8157 Eval Acc: 0.7236 (LR: 0.00100000)
[2025-05-26 18:40:44,425]: [ResNet20_hardtanh] Epoch: 015 Train Loss: 0.7373 Train Acc: 0.7391 Eval Loss: 0.7664 Eval Acc: 0.7357 (LR: 0.00100000)
[2025-05-26 18:41:21,984]: [ResNet20_hardtanh] Epoch: 016 Train Loss: 0.7144 Train Acc: 0.7484 Eval Loss: 0.7532 Eval Acc: 0.7422 (LR: 0.00100000)
[2025-05-26 18:41:58,729]: [ResNet20_hardtanh] Epoch: 017 Train Loss: 0.6967 Train Acc: 0.7546 Eval Loss: 0.7189 Eval Acc: 0.7562 (LR: 0.00100000)
[2025-05-26 18:42:32,910]: [ResNet20_hardtanh] Epoch: 018 Train Loss: 0.6756 Train Acc: 0.7638 Eval Loss: 0.7438 Eval Acc: 0.7444 (LR: 0.00100000)
[2025-05-26 18:43:06,630]: [ResNet20_hardtanh] Epoch: 019 Train Loss: 0.6712 Train Acc: 0.7641 Eval Loss: 0.7980 Eval Acc: 0.7321 (LR: 0.00100000)
[2025-05-26 18:43:40,746]: [ResNet20_hardtanh] Epoch: 020 Train Loss: 0.6501 Train Acc: 0.7704 Eval Loss: 0.6854 Eval Acc: 0.7720 (LR: 0.00100000)
[2025-05-26 18:44:14,444]: [ResNet20_hardtanh] Epoch: 021 Train Loss: 0.6352 Train Acc: 0.7778 Eval Loss: 0.6811 Eval Acc: 0.7705 (LR: 0.00100000)
[2025-05-26 18:44:48,133]: [ResNet20_hardtanh] Epoch: 022 Train Loss: 0.6221 Train Acc: 0.7819 Eval Loss: 0.8380 Eval Acc: 0.7261 (LR: 0.00100000)
[2025-05-26 18:45:21,856]: [ResNet20_hardtanh] Epoch: 023 Train Loss: 0.6094 Train Acc: 0.7867 Eval Loss: 0.6726 Eval Acc: 0.7736 (LR: 0.00100000)
[2025-05-26 18:45:55,548]: [ResNet20_hardtanh] Epoch: 024 Train Loss: 0.5991 Train Acc: 0.7885 Eval Loss: 0.6622 Eval Acc: 0.7739 (LR: 0.00100000)
[2025-05-26 18:46:29,247]: [ResNet20_hardtanh] Epoch: 025 Train Loss: 0.5874 Train Acc: 0.7947 Eval Loss: 0.8704 Eval Acc: 0.7141 (LR: 0.00100000)
[2025-05-26 18:47:02,955]: [ResNet20_hardtanh] Epoch: 026 Train Loss: 0.5762 Train Acc: 0.8001 Eval Loss: 0.7434 Eval Acc: 0.7568 (LR: 0.00100000)
[2025-05-26 18:47:36,672]: [ResNet20_hardtanh] Epoch: 027 Train Loss: 0.5680 Train Acc: 0.8022 Eval Loss: 0.6417 Eval Acc: 0.7825 (LR: 0.00100000)
[2025-05-26 18:48:10,348]: [ResNet20_hardtanh] Epoch: 028 Train Loss: 0.5577 Train Acc: 0.8077 Eval Loss: 0.6664 Eval Acc: 0.7736 (LR: 0.00100000)
[2025-05-26 18:48:44,059]: [ResNet20_hardtanh] Epoch: 029 Train Loss: 0.5537 Train Acc: 0.8061 Eval Loss: 0.6309 Eval Acc: 0.7855 (LR: 0.00100000)
[2025-05-26 18:49:17,775]: [ResNet20_hardtanh] Epoch: 030 Train Loss: 0.5393 Train Acc: 0.8123 Eval Loss: 0.6287 Eval Acc: 0.7910 (LR: 0.00100000)
[2025-05-26 18:49:51,485]: [ResNet20_hardtanh] Epoch: 031 Train Loss: 0.5349 Train Acc: 0.8129 Eval Loss: 0.6918 Eval Acc: 0.7728 (LR: 0.00100000)
[2025-05-26 18:50:25,185]: [ResNet20_hardtanh] Epoch: 032 Train Loss: 0.5254 Train Acc: 0.8163 Eval Loss: 0.7356 Eval Acc: 0.7609 (LR: 0.00100000)
[2025-05-26 18:50:58,891]: [ResNet20_hardtanh] Epoch: 033 Train Loss: 0.5188 Train Acc: 0.8186 Eval Loss: 0.6209 Eval Acc: 0.7891 (LR: 0.00100000)
[2025-05-26 18:51:32,621]: [ResNet20_hardtanh] Epoch: 034 Train Loss: 0.5117 Train Acc: 0.8231 Eval Loss: 0.6965 Eval Acc: 0.7666 (LR: 0.00100000)
[2025-05-26 18:52:06,342]: [ResNet20_hardtanh] Epoch: 035 Train Loss: 0.5069 Train Acc: 0.8228 Eval Loss: 0.5796 Eval Acc: 0.8061 (LR: 0.00100000)
[2025-05-26 18:52:40,045]: [ResNet20_hardtanh] Epoch: 036 Train Loss: 0.4990 Train Acc: 0.8257 Eval Loss: 0.5478 Eval Acc: 0.8114 (LR: 0.00100000)
[2025-05-26 18:53:13,741]: [ResNet20_hardtanh] Epoch: 037 Train Loss: 0.4886 Train Acc: 0.8302 Eval Loss: 0.5409 Eval Acc: 0.8160 (LR: 0.00100000)
[2025-05-26 18:53:47,458]: [ResNet20_hardtanh] Epoch: 038 Train Loss: 0.4866 Train Acc: 0.8300 Eval Loss: 0.7829 Eval Acc: 0.7553 (LR: 0.00100000)
[2025-05-26 18:54:21,147]: [ResNet20_hardtanh] Epoch: 039 Train Loss: 0.4792 Train Acc: 0.8318 Eval Loss: 0.5488 Eval Acc: 0.8104 (LR: 0.00100000)
[2025-05-26 18:54:55,273]: [ResNet20_hardtanh] Epoch: 040 Train Loss: 0.4763 Train Acc: 0.8349 Eval Loss: 0.7516 Eval Acc: 0.7589 (LR: 0.00100000)
[2025-05-26 18:55:28,972]: [ResNet20_hardtanh] Epoch: 041 Train Loss: 0.4669 Train Acc: 0.8364 Eval Loss: 0.5815 Eval Acc: 0.8075 (LR: 0.00100000)
[2025-05-26 18:56:02,678]: [ResNet20_hardtanh] Epoch: 042 Train Loss: 0.4664 Train Acc: 0.8368 Eval Loss: 0.6803 Eval Acc: 0.7838 (LR: 0.00100000)
[2025-05-26 18:56:36,387]: [ResNet20_hardtanh] Epoch: 043 Train Loss: 0.4656 Train Acc: 0.8377 Eval Loss: 0.7051 Eval Acc: 0.7851 (LR: 0.00010000)
[2025-05-26 18:57:10,105]: [ResNet20_hardtanh] Epoch: 044 Train Loss: 0.3830 Train Acc: 0.8707 Eval Loss: 0.4277 Eval Acc: 0.8571 (LR: 0.00010000)
[2025-05-26 18:57:43,807]: [ResNet20_hardtanh] Epoch: 045 Train Loss: 0.3619 Train Acc: 0.8739 Eval Loss: 0.4234 Eval Acc: 0.8574 (LR: 0.00010000)
[2025-05-26 18:58:17,518]: [ResNet20_hardtanh] Epoch: 046 Train Loss: 0.3569 Train Acc: 0.8759 Eval Loss: 0.4224 Eval Acc: 0.8590 (LR: 0.00010000)
[2025-05-26 18:58:51,238]: [ResNet20_hardtanh] Epoch: 047 Train Loss: 0.3485 Train Acc: 0.8788 Eval Loss: 0.4266 Eval Acc: 0.8581 (LR: 0.00010000)
[2025-05-26 18:59:24,956]: [ResNet20_hardtanh] Epoch: 048 Train Loss: 0.3452 Train Acc: 0.8804 Eval Loss: 0.4281 Eval Acc: 0.8577 (LR: 0.00010000)
[2025-05-26 18:59:58,664]: [ResNet20_hardtanh] Epoch: 049 Train Loss: 0.3412 Train Acc: 0.8801 Eval Loss: 0.4206 Eval Acc: 0.8594 (LR: 0.00010000)
[2025-05-26 19:00:32,375]: [ResNet20_hardtanh] Epoch: 050 Train Loss: 0.3372 Train Acc: 0.8828 Eval Loss: 0.4304 Eval Acc: 0.8577 (LR: 0.00010000)
[2025-05-26 19:01:06,072]: [ResNet20_hardtanh] Epoch: 051 Train Loss: 0.3365 Train Acc: 0.8833 Eval Loss: 0.4255 Eval Acc: 0.8587 (LR: 0.00010000)
[2025-05-26 19:01:39,794]: [ResNet20_hardtanh] Epoch: 052 Train Loss: 0.3356 Train Acc: 0.8830 Eval Loss: 0.4246 Eval Acc: 0.8598 (LR: 0.00010000)
[2025-05-26 19:02:13,498]: [ResNet20_hardtanh] Epoch: 053 Train Loss: 0.3299 Train Acc: 0.8855 Eval Loss: 0.4169 Eval Acc: 0.8626 (LR: 0.00010000)
[2025-05-26 19:02:47,208]: [ResNet20_hardtanh] Epoch: 054 Train Loss: 0.3263 Train Acc: 0.8862 Eval Loss: 0.4214 Eval Acc: 0.8610 (LR: 0.00010000)
[2025-05-26 19:03:20,927]: [ResNet20_hardtanh] Epoch: 055 Train Loss: 0.3237 Train Acc: 0.8880 Eval Loss: 0.4230 Eval Acc: 0.8600 (LR: 0.00010000)
[2025-05-26 19:03:54,694]: [ResNet20_hardtanh] Epoch: 056 Train Loss: 0.3232 Train Acc: 0.8877 Eval Loss: 0.4293 Eval Acc: 0.8580 (LR: 0.00010000)
[2025-05-26 19:04:28,340]: [ResNet20_hardtanh] Epoch: 057 Train Loss: 0.3224 Train Acc: 0.8886 Eval Loss: 0.4143 Eval Acc: 0.8668 (LR: 0.00010000)
[2025-05-26 19:05:02,060]: [ResNet20_hardtanh] Epoch: 058 Train Loss: 0.3200 Train Acc: 0.8893 Eval Loss: 0.4131 Eval Acc: 0.8625 (LR: 0.00010000)
[2025-05-26 19:05:35,559]: [ResNet20_hardtanh] Epoch: 059 Train Loss: 0.3180 Train Acc: 0.8886 Eval Loss: 0.4181 Eval Acc: 0.8614 (LR: 0.00010000)
[2025-05-26 19:06:09,496]: [ResNet20_hardtanh] Epoch: 060 Train Loss: 0.3182 Train Acc: 0.8890 Eval Loss: 0.4230 Eval Acc: 0.8630 (LR: 0.00010000)
[2025-05-26 19:06:43,202]: [ResNet20_hardtanh] Epoch: 061 Train Loss: 0.3164 Train Acc: 0.8903 Eval Loss: 0.4159 Eval Acc: 0.8641 (LR: 0.00010000)
[2025-05-26 19:07:16,893]: [ResNet20_hardtanh] Epoch: 062 Train Loss: 0.3096 Train Acc: 0.8924 Eval Loss: 0.4161 Eval Acc: 0.8645 (LR: 0.00010000)
[2025-05-26 19:07:50,643]: [ResNet20_hardtanh] Epoch: 063 Train Loss: 0.3116 Train Acc: 0.8916 Eval Loss: 0.4262 Eval Acc: 0.8622 (LR: 0.00010000)
[2025-05-26 19:08:24,327]: [ResNet20_hardtanh] Epoch: 064 Train Loss: 0.3098 Train Acc: 0.8922 Eval Loss: 0.4183 Eval Acc: 0.8638 (LR: 0.00001000)
[2025-05-26 19:08:58,022]: [ResNet20_hardtanh] Epoch: 065 Train Loss: 0.3019 Train Acc: 0.8937 Eval Loss: 0.4120 Eval Acc: 0.8657 (LR: 0.00001000)
[2025-05-26 19:09:31,741]: [ResNet20_hardtanh] Epoch: 066 Train Loss: 0.2975 Train Acc: 0.8970 Eval Loss: 0.4115 Eval Acc: 0.8658 (LR: 0.00001000)
[2025-05-26 19:10:05,455]: [ResNet20_hardtanh] Epoch: 067 Train Loss: 0.2957 Train Acc: 0.8976 Eval Loss: 0.4097 Eval Acc: 0.8665 (LR: 0.00001000)
[2025-05-26 19:10:39,160]: [ResNet20_hardtanh] Epoch: 068 Train Loss: 0.2952 Train Acc: 0.8974 Eval Loss: 0.4136 Eval Acc: 0.8662 (LR: 0.00001000)
[2025-05-26 19:11:12,668]: [ResNet20_hardtanh] Epoch: 069 Train Loss: 0.2967 Train Acc: 0.8967 Eval Loss: 0.4099 Eval Acc: 0.8673 (LR: 0.00001000)
[2025-05-26 19:11:46,361]: [ResNet20_hardtanh] Epoch: 070 Train Loss: 0.3009 Train Acc: 0.8961 Eval Loss: 0.4107 Eval Acc: 0.8663 (LR: 0.00001000)
[2025-05-26 19:12:20,091]: [ResNet20_hardtanh] Epoch: 071 Train Loss: 0.2955 Train Acc: 0.8976 Eval Loss: 0.4095 Eval Acc: 0.8673 (LR: 0.00001000)
[2025-05-26 19:12:53,820]: [ResNet20_hardtanh] Epoch: 072 Train Loss: 0.2942 Train Acc: 0.8977 Eval Loss: 0.4099 Eval Acc: 0.8666 (LR: 0.00001000)
[2025-05-26 19:13:27,526]: [ResNet20_hardtanh] Epoch: 073 Train Loss: 0.2970 Train Acc: 0.8960 Eval Loss: 0.4093 Eval Acc: 0.8670 (LR: 0.00001000)
[2025-05-26 19:14:01,240]: [ResNet20_hardtanh] Epoch: 074 Train Loss: 0.2919 Train Acc: 0.8998 Eval Loss: 0.4084 Eval Acc: 0.8665 (LR: 0.00001000)
[2025-05-26 19:14:34,957]: [ResNet20_hardtanh] Epoch: 075 Train Loss: 0.2983 Train Acc: 0.8967 Eval Loss: 0.4083 Eval Acc: 0.8690 (LR: 0.00001000)
[2025-05-26 19:15:08,657]: [ResNet20_hardtanh] Epoch: 076 Train Loss: 0.2925 Train Acc: 0.8977 Eval Loss: 0.4089 Eval Acc: 0.8684 (LR: 0.00001000)
[2025-05-26 19:15:42,367]: [ResNet20_hardtanh] Epoch: 077 Train Loss: 0.2920 Train Acc: 0.8980 Eval Loss: 0.4070 Eval Acc: 0.8678 (LR: 0.00001000)
[2025-05-26 19:16:16,069]: [ResNet20_hardtanh] Epoch: 078 Train Loss: 0.2949 Train Acc: 0.8974 Eval Loss: 0.4110 Eval Acc: 0.8645 (LR: 0.00001000)
[2025-05-26 19:16:49,592]: [ResNet20_hardtanh] Epoch: 079 Train Loss: 0.2936 Train Acc: 0.8959 Eval Loss: 0.4088 Eval Acc: 0.8682 (LR: 0.00001000)
[2025-05-26 19:17:23,709]: [ResNet20_hardtanh] Epoch: 080 Train Loss: 0.2959 Train Acc: 0.8973 Eval Loss: 0.4123 Eval Acc: 0.8654 (LR: 0.00001000)
[2025-05-26 19:17:57,450]: [ResNet20_hardtanh] Epoch: 081 Train Loss: 0.2914 Train Acc: 0.8993 Eval Loss: 0.4089 Eval Acc: 0.8671 (LR: 0.00001000)
[2025-05-26 19:18:31,118]: [ResNet20_hardtanh] Epoch: 082 Train Loss: 0.2921 Train Acc: 0.8978 Eval Loss: 0.4105 Eval Acc: 0.8661 (LR: 0.00001000)
[2025-05-26 19:19:04,825]: [ResNet20_hardtanh] Epoch: 083 Train Loss: 0.2907 Train Acc: 0.8976 Eval Loss: 0.4114 Eval Acc: 0.8676 (LR: 0.00000100)
[2025-05-26 19:19:38,533]: [ResNet20_hardtanh] Epoch: 084 Train Loss: 0.2930 Train Acc: 0.8968 Eval Loss: 0.4105 Eval Acc: 0.8666 (LR: 0.00000100)
[2025-05-26 19:20:12,250]: [ResNet20_hardtanh] Epoch: 085 Train Loss: 0.2881 Train Acc: 0.9003 Eval Loss: 0.4104 Eval Acc: 0.8671 (LR: 0.00000100)
[2025-05-26 19:20:45,960]: [ResNet20_hardtanh] Epoch: 086 Train Loss: 0.2928 Train Acc: 0.8978 Eval Loss: 0.4087 Eval Acc: 0.8683 (LR: 0.00000100)
[2025-05-26 19:21:19,481]: [ResNet20_hardtanh] Epoch: 087 Train Loss: 0.2930 Train Acc: 0.8979 Eval Loss: 0.4119 Eval Acc: 0.8669 (LR: 0.00000100)
[2025-05-26 19:21:53,206]: [ResNet20_hardtanh] Epoch: 088 Train Loss: 0.2907 Train Acc: 0.8982 Eval Loss: 0.4100 Eval Acc: 0.8681 (LR: 0.00000100)
[2025-05-26 19:22:26,917]: [ResNet20_hardtanh] Epoch: 089 Train Loss: 0.2907 Train Acc: 0.8992 Eval Loss: 0.4106 Eval Acc: 0.8674 (LR: 0.00000010)
[2025-05-26 19:23:00,624]: [ResNet20_hardtanh] Epoch: 090 Train Loss: 0.2912 Train Acc: 0.8980 Eval Loss: 0.4089 Eval Acc: 0.8673 (LR: 0.00000010)
[2025-05-26 19:23:34,334]: [ResNet20_hardtanh] Epoch: 091 Train Loss: 0.2881 Train Acc: 0.8998 Eval Loss: 0.4094 Eval Acc: 0.8672 (LR: 0.00000010)
[2025-05-26 19:24:08,065]: [ResNet20_hardtanh] Epoch: 092 Train Loss: 0.2941 Train Acc: 0.8967 Eval Loss: 0.4104 Eval Acc: 0.8672 (LR: 0.00000010)
[2025-05-26 19:24:41,797]: [ResNet20_hardtanh] Epoch: 093 Train Loss: 0.2888 Train Acc: 0.8995 Eval Loss: 0.4098 Eval Acc: 0.8678 (LR: 0.00000010)
[2025-05-26 19:25:15,467]: [ResNet20_hardtanh] Epoch: 094 Train Loss: 0.2927 Train Acc: 0.8987 Eval Loss: 0.4120 Eval Acc: 0.8677 (LR: 0.00000010)
[2025-05-26 19:25:49,171]: [ResNet20_hardtanh] Epoch: 095 Train Loss: 0.2888 Train Acc: 0.8995 Eval Loss: 0.4109 Eval Acc: 0.8672 (LR: 0.00000010)
[2025-05-26 19:25:49,171]: Early stopping was triggered!
[2025-05-26 19:25:49,171]: [ResNet20_hardtanh] Best Eval Accuracy: 0.8690
[2025-05-26 19:25:49,203]: 
Training of full-precision model finished!
[2025-05-26 19:25:49,203]: Model Architecture:
[2025-05-26 19:25:49,204]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-26 19:25:49,204]: 
Model Weights:
[2025-05-26 19:25:49,205]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-26 19:25:49,205]: Sample Values (25 elements): [0.11429435014724731, 0.13401462137699127, 0.12380234152078629, 0.08972249925136566, 0.1557314097881317, 0.20428411662578583, -0.08649393171072006, 0.020801445469260216, -0.07717002928256989, 0.02229599840939045, 0.05476944521069527, -0.09259052574634552, 0.23377461731433868, -0.1427975296974182, 0.033015232533216476, -0.10604818910360336, 0.2985577881336212, -0.015088686719536781, 0.2628355622291565, 0.0009012791560962796, 0.10755578428506851, -0.19856677949428558, 0.09366193413734436, 0.0924876481294632, 0.03850790113210678]
[2025-05-26 19:25:49,205]: Mean: 0.00067067
[2025-05-26 19:25:49,205]: Min: -0.59812289
[2025-05-26 19:25:49,205]: Max: 0.34461883
[2025-05-26 19:25:49,205]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,206]: Sample Values (16 elements): [0.5502116084098816, 0.668010950088501, 0.8586013317108154, 0.7805400490760803, 0.6322060227394104, 0.644192636013031, 0.6316308379173279, 0.9070236086845398, 0.6369194984436035, 0.7387666702270508, 1.1098291873931885, 0.8136717081069946, 0.8170953392982483, 0.5320959091186523, 0.7520093321800232, 0.7269629240036011]
[2025-05-26 19:25:49,206]: Mean: 0.73748547
[2025-05-26 19:25:49,206]: Min: 0.53209591
[2025-05-26 19:25:49,206]: Max: 1.10982919
[2025-05-26 19:25:49,206]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,206]: Sample Values (25 elements): [0.03572211414575577, -0.06424879282712936, -0.09860416501760483, -0.08759094029664993, 0.12966471910476685, 0.09789237380027771, -0.1310362070798874, 0.021751470863819122, -0.08302688598632812, -0.056198857724666595, -0.12711744010448456, -0.012240425683557987, -0.04471903666853905, 0.18227888643741608, 0.015606286004185677, -0.13038721680641174, 0.021318344399333, -0.0576324462890625, 0.07405475527048111, -0.16154134273529053, 0.006188387516885996, -0.04357253387570381, -0.020922165364027023, -0.17759950459003448, -0.24880069494247437]
[2025-05-26 19:25:49,207]: Mean: -0.00189196
[2025-05-26 19:25:49,207]: Min: -0.49130526
[2025-05-26 19:25:49,207]: Max: 0.59045911
[2025-05-26 19:25:49,207]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,207]: Sample Values (16 elements): [1.159720778465271, 0.6315287947654724, 0.8420218229293823, 0.936284065246582, 0.8593540787696838, 0.6250782012939453, 0.7656907439231873, 0.8702704906463623, 0.9505681991577148, 0.9118705987930298, 0.7736178040504456, 0.8487583994865417, 1.0461220741271973, 0.8934905529022217, 1.0480709075927734, 1.1816296577453613]
[2025-05-26 19:25:49,207]: Mean: 0.89650482
[2025-05-26 19:25:49,207]: Min: 0.62507820
[2025-05-26 19:25:49,208]: Max: 1.18162966
[2025-05-26 19:25:49,208]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,208]: Sample Values (25 elements): [-0.004710650071501732, -0.23553799092769623, 0.06768837571144104, 0.05795392021536827, 0.0496041439473629, 0.0912245362997055, 0.12559400498867035, 0.18709655106067657, 0.0045632352121174335, -0.04584398865699768, 0.08081304281949997, -0.09788379073143005, -0.00981881469488144, -0.03689245507121086, 0.07819468528032303, -0.07189401239156723, 0.11307567358016968, -0.18822550773620605, -0.2029915302991867, 0.15510042011737823, -0.028925174847245216, -0.08618440479040146, -0.008591711521148682, 0.001659130910411477, -0.002436526818200946]
[2025-05-26 19:25:49,208]: Mean: 0.00207706
[2025-05-26 19:25:49,208]: Min: -0.43365645
[2025-05-26 19:25:49,208]: Max: 0.45334584
[2025-05-26 19:25:49,208]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,209]: Sample Values (16 elements): [0.5574577450752258, 0.6937311291694641, 0.6015516519546509, 0.7072291374206543, 0.8055368661880493, 0.8745425343513489, 0.8813281655311584, 0.7555713057518005, 0.9174238443374634, 0.9757425785064697, 0.9265540838241577, 0.6187164187431335, 0.9175874590873718, 0.6465546488761902, 0.8841875791549683, 0.6287380456924438]
[2025-05-26 19:25:49,209]: Mean: 0.77452832
[2025-05-26 19:25:49,209]: Min: 0.55745775
[2025-05-26 19:25:49,209]: Max: 0.97574258
[2025-05-26 19:25:49,209]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,209]: Sample Values (25 elements): [0.06035811826586723, -0.13889148831367493, -0.10005894303321838, -0.3388187885284424, 0.04442336782813072, -0.0945204347372055, 0.0475122295320034, -0.08537094295024872, -0.1472414880990982, 0.12994007766246796, 0.0003471667878329754, 0.02699613757431507, 0.08981398493051529, -0.15319810807704926, 0.1202571764588356, 0.08661158382892609, 0.2743806540966034, 0.04930520057678223, -0.017849594354629517, -0.07705120742321014, -0.13328817486763, -0.1570587307214737, 0.0720413401722908, -0.0035949668381363153, -0.20152482390403748]
[2025-05-26 19:25:49,210]: Mean: -0.00140711
[2025-05-26 19:25:49,210]: Min: -0.43368351
[2025-05-26 19:25:49,210]: Max: 0.47730064
[2025-05-26 19:25:49,210]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,210]: Sample Values (16 elements): [1.0230270624160767, 1.086357831954956, 0.767203152179718, 1.148169994354248, 1.0023902654647827, 0.9663600325584412, 1.091180443763733, 0.6984784603118896, 0.6949113011360168, 1.1556416749954224, 0.8268964290618896, 1.3163074254989624, 0.8870227932929993, 0.8841089010238647, 0.8793299198150635, 1.259594440460205]
[2025-05-26 19:25:49,210]: Mean: 0.98043627
[2025-05-26 19:25:49,211]: Min: 0.69491130
[2025-05-26 19:25:49,211]: Max: 1.31630743
[2025-05-26 19:25:49,211]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,211]: Sample Values (25 elements): [-0.027401810511946678, 0.10567864030599594, -0.019861645996570587, -0.020195430144667625, 0.07351521402597427, -0.05034923553466797, 0.09794334322214127, 0.17479659616947174, -0.11275702714920044, 0.007578512188047171, 0.06773246824741364, 0.05339161306619644, 0.12487907707691193, -0.1694294959306717, 0.1048644408583641, 0.05285624787211418, -0.00884645152837038, -0.10722599178552628, -0.042574480175971985, -0.2215823084115982, -0.07272809743881226, 0.29376184940338135, -0.12062326818704605, -0.035512275993824005, -0.03039667010307312]
[2025-05-26 19:25:49,211]: Mean: -0.00469146
[2025-05-26 19:25:49,211]: Min: -0.44131535
[2025-05-26 19:25:49,211]: Max: 0.52563852
[2025-05-26 19:25:49,212]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,212]: Sample Values (16 elements): [0.5922549962997437, 0.9779398441314697, 0.48222821950912476, 0.6803516149520874, 0.6031842827796936, 0.5652828812599182, 0.6451655626296997, 0.5718948841094971, 0.9111365675926208, 0.6330024600028992, 0.8687618374824524, 0.5225087404251099, 0.7340036630630493, 0.8212340474128723, 0.7303474545478821, 0.6211785078048706]
[2025-05-26 19:25:49,212]: Mean: 0.68502975
[2025-05-26 19:25:49,212]: Min: 0.48222822
[2025-05-26 19:25:49,212]: Max: 0.97793984
[2025-05-26 19:25:49,212]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,213]: Sample Values (25 elements): [-0.005656318739056587, 0.11177985370159149, 0.06343848258256912, 0.15181246399879456, -0.011197860352694988, 0.08701693266630173, 0.21527552604675293, -0.036205023527145386, -0.20970846712589264, 0.02360670082271099, 0.010465352796018124, -0.4709031283855438, 0.06508287042379379, 0.018697531893849373, 0.12256968766450882, 0.02526140958070755, -0.009280984289944172, -0.00798158347606659, 0.04189521074295044, 0.10303228348493576, -0.09854106605052948, -0.0038842863868921995, -0.13819952309131622, -0.10511018335819244, -0.0952562540769577]
[2025-05-26 19:25:49,213]: Mean: -0.00623599
[2025-05-26 19:25:49,213]: Min: -0.53331852
[2025-05-26 19:25:49,213]: Max: 0.39245892
[2025-05-26 19:25:49,213]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,213]: Sample Values (16 elements): [0.8557426333427429, 0.9770027995109558, 1.0159744024276733, 1.1442762613296509, 0.8613601922988892, 1.0642571449279785, 1.1199239492416382, 1.058964490890503, 0.8534445762634277, 0.9861290454864502, 0.9574756622314453, 0.8688041567802429, 0.953551173210144, 0.9228023886680603, 0.9130203723907471, 0.8815149664878845]
[2025-05-26 19:25:49,213]: Mean: 0.96464032
[2025-05-26 19:25:49,214]: Min: 0.85344458
[2025-05-26 19:25:49,214]: Max: 1.14427626
[2025-05-26 19:25:49,214]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 19:25:49,214]: Sample Values (25 elements): [-0.03852413222193718, -0.1995713710784912, 0.014351973310112953, 0.009096859954297543, 0.05980584770441055, 0.0233291182667017, -0.04531698673963547, 0.16504469513893127, -0.11169897764921188, -0.055638354271650314, -0.01919659599661827, -0.005638116039335728, 0.13465099036693573, -0.18788668513298035, 0.044566892087459564, -0.19448518753051758, -0.05516471713781357, 0.11627577990293503, 0.02598283439874649, 0.04739837720990181, 0.010608718730509281, 0.10517926514148712, -0.0981869027018547, -0.17225955426692963, -0.052974533289670944]
[2025-05-26 19:25:49,214]: Mean: -0.00134037
[2025-05-26 19:25:49,214]: Min: -0.39483452
[2025-05-26 19:25:49,215]: Max: 0.51383978
[2025-05-26 19:25:49,215]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-26 19:25:49,215]: Sample Values (16 elements): [0.8965025544166565, 0.7409180998802185, 0.6697055101394653, 1.0061177015304565, 0.666359543800354, 0.651939332485199, 0.7458117008209229, 0.704752504825592, 0.7410255074501038, 0.5052545070648193, 0.6561073660850525, 0.714324414730072, 0.6325165629386902, 0.7474067211151123, 0.6094486713409424, 0.6692317128181458]
[2025-05-26 19:25:49,215]: Mean: 0.70983887
[2025-05-26 19:25:49,215]: Min: 0.50525451
[2025-05-26 19:25:49,215]: Max: 1.00611770
[2025-05-26 19:25:49,215]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-26 19:25:49,216]: Sample Values (25 elements): [0.026370737701654434, -0.04071004316210747, 0.04643953964114189, -0.019180692732334137, 0.06393350660800934, -0.16093847155570984, 0.03873011842370033, 0.08083350956439972, 0.14464357495307922, -0.013904710300266743, -0.051033832132816315, -0.09273488819599152, 0.06626807898283005, -0.13894926011562347, -0.34902432560920715, -0.009333133697509766, -0.12955252826213837, -0.09876944869756699, -0.028589623048901558, -0.03630964085459709, -0.003869861364364624, 0.07645491510629654, -0.006146468687802553, 0.1214042454957962, -0.04686209559440613]
[2025-05-26 19:25:49,216]: Mean: 0.00432576
[2025-05-26 19:25:49,216]: Min: -0.39039880
[2025-05-26 19:25:49,216]: Max: 0.39065492
[2025-05-26 19:25:49,216]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,216]: Sample Values (25 elements): [0.7071413993835449, 0.9045945405960083, 0.7970846891403198, 0.7875382900238037, 0.8797261118888855, 0.7757295966148376, 0.9917125105857849, 0.6416102051734924, 0.7632288932800293, 0.8214387893676758, 0.815360426902771, 0.8191978931427002, 0.8888670802116394, 0.7405829429626465, 0.8045385479927063, 0.642770528793335, 0.7261314392089844, 0.8163524866104126, 0.7560022473335266, 0.666067361831665, 0.7352674603462219, 0.7405836582183838, 0.8734428882598877, 0.5965235829353333, 0.8037998676300049]
[2025-05-26 19:25:49,217]: Mean: 0.77595186
[2025-05-26 19:25:49,217]: Min: 0.59652358
[2025-05-26 19:25:49,217]: Max: 0.99171251
[2025-05-26 19:25:49,217]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,217]: Sample Values (25 elements): [0.09904912859201431, -0.058279696851968765, 0.07338474690914154, 0.033208541572093964, -0.05935130640864372, 0.12794256210327148, 0.10296925157308578, -0.12065883725881577, -0.016924679279327393, -0.11623601615428925, 0.15046852827072144, 0.14383666217327118, -0.03829078748822212, 0.0945587009191513, 0.048246581107378006, 0.055026598274707794, -0.04818687587976456, 0.04557844623923302, -0.07473605871200562, 0.009342152625322342, 0.02333683893084526, -0.001647584605962038, 0.004861939698457718, -0.004338250029832125, -0.021295655518770218]
[2025-05-26 19:25:49,217]: Mean: 0.00201934
[2025-05-26 19:25:49,218]: Min: -0.38586891
[2025-05-26 19:25:49,218]: Max: 0.35906258
[2025-05-26 19:25:49,218]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,218]: Sample Values (25 elements): [0.7301149964332581, 0.7851240038871765, 0.6987364888191223, 0.7856627106666565, 0.8286901116371155, 0.8562981486320496, 0.70313960313797, 0.6978512406349182, 0.7770841717720032, 0.7994734048843384, 0.7844347953796387, 0.7512884736061096, 0.7212440371513367, 0.8367453813552856, 0.9971225261688232, 0.7099695801734924, 0.6737432479858398, 0.9069218635559082, 0.8647176027297974, 0.7548388838768005, 0.6060072779655457, 0.6606361865997314, 0.8608936667442322, 0.7777382135391235, 0.6759841442108154]
[2025-05-26 19:25:49,218]: Mean: 0.76221943
[2025-05-26 19:25:49,218]: Min: 0.60600728
[2025-05-26 19:25:49,218]: Max: 0.99712253
[2025-05-26 19:25:49,218]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-26 19:25:49,219]: Sample Values (25 elements): [-0.18909025192260742, -0.3092363178730011, 0.25615403056144714, -0.18504254519939423, 0.2746959626674652, 0.22914251685142517, 0.054044030606746674, 0.032857004553079605, 0.2541870176792145, 0.0887182280421257, 0.23837438225746155, 0.13668103516101837, 0.1689571589231491, 0.23350588977336884, -0.08582182973623276, -0.3825324475765228, -0.16574403643608093, -0.02991069294512272, -0.17111708223819733, -0.05394980311393738, 0.050734613090753555, 0.11724565923213959, 0.2143636792898178, -0.15304675698280334, 0.1489449143409729]
[2025-05-26 19:25:49,219]: Mean: 0.00849727
[2025-05-26 19:25:49,219]: Min: -0.45839590
[2025-05-26 19:25:49,219]: Max: 0.44234228
[2025-05-26 19:25:49,219]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,219]: Sample Values (25 elements): [0.6761747598648071, 0.27178114652633667, 0.6283966898918152, 0.3518148958683014, 0.4983835220336914, 0.49365073442459106, 0.5602676272392273, 0.5441693067550659, 0.4707086980342865, 0.5621576309204102, 0.49860888719558716, 0.5082482099533081, 0.3316601514816284, 0.2919485867023468, 0.4618634879589081, 0.56379234790802, 0.5742371678352356, 0.3930882215499878, 0.6465232968330383, 0.6130110025405884, 0.49887675046920776, 0.46561408042907715, 0.5083709359169006, 0.4443615972995758, 0.380880206823349]
[2025-05-26 19:25:49,220]: Mean: 0.51025653
[2025-05-26 19:25:49,220]: Min: 0.27178115
[2025-05-26 19:25:49,220]: Max: 0.75144649
[2025-05-26 19:25:49,220]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,220]: Sample Values (25 elements): [-0.0601474829018116, -0.0069289435632526875, -0.0022445828653872013, -0.08328285068273544, -0.03316056355834007, -0.031791072338819504, 0.058137089014053345, 0.014275330118834972, 0.09790065884590149, 0.002693051937967539, 0.08842738717794418, 0.08527719974517822, 0.07956398278474808, -0.03913864865899086, -0.08043190091848373, -0.11793248355388641, 0.04569394513964653, -0.13888581097126007, -0.07371541112661362, -0.06835071742534637, 0.014166177250444889, -0.1003216803073883, 0.06870705634355545, -0.12995180487632751, -0.019445450976490974]
[2025-05-26 19:25:49,220]: Mean: 0.00233807
[2025-05-26 19:25:49,221]: Min: -0.38896734
[2025-05-26 19:25:49,221]: Max: 0.33344868
[2025-05-26 19:25:49,221]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,221]: Sample Values (25 elements): [0.7776239514350891, 0.9381120204925537, 1.014744758605957, 0.6608483195304871, 0.859599232673645, 1.0504993200302124, 0.8076260089874268, 0.7562715411186218, 0.9032034277915955, 0.8192076086997986, 0.8095394968986511, 1.078836441040039, 0.7184111475944519, 0.8352881669998169, 1.1136575937271118, 0.7611797451972961, 0.832094132900238, 0.81430584192276, 0.921656608581543, 0.8019354939460754, 0.8402772545814514, 0.7657175064086914, 0.9752669930458069, 0.8839296102523804, 1.0380939245224]
[2025-05-26 19:25:49,221]: Mean: 0.87508619
[2025-05-26 19:25:49,221]: Min: 0.66084832
[2025-05-26 19:25:49,221]: Max: 1.11365759
[2025-05-26 19:25:49,222]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,222]: Sample Values (25 elements): [0.0680566132068634, -0.1766384094953537, 0.22105732560157776, -0.014877039939165115, 0.08417858928442001, 0.07616490125656128, 0.10172432661056519, 0.03229811042547226, -0.07450374215841293, -0.07545376569032669, -0.08882465213537216, 0.018908722326159477, 0.03197181969881058, -0.1632203906774521, 0.04205596074461937, -0.1475296914577484, -0.027814587578177452, 0.0027336920611560345, 0.038710713386535645, -0.1372668445110321, 0.09751251339912415, 0.0042809066362679005, 0.07960900664329529, -0.15695050358772278, 0.04016825556755066]
[2025-05-26 19:25:49,222]: Mean: 0.00016429
[2025-05-26 19:25:49,222]: Min: -0.33932337
[2025-05-26 19:25:49,222]: Max: 0.37259525
[2025-05-26 19:25:49,222]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,223]: Sample Values (25 elements): [0.6158192753791809, 0.574986457824707, 0.6105727553367615, 0.7156373858451843, 0.6745414733886719, 0.6473590731620789, 0.8160945773124695, 0.7644526362419128, 0.7136293053627014, 0.5350351333618164, 0.7642601728439331, 0.720725417137146, 0.6208803057670593, 0.6959929466247559, 0.9008598923683167, 0.6733432412147522, 0.6859773993492126, 0.7251498699188232, 0.6683773398399353, 0.7942367196083069, 0.6200363636016846, 0.7874156832695007, 0.698259174823761, 0.3635953962802887, 0.5420039296150208]
[2025-05-26 19:25:49,223]: Mean: 0.68213159
[2025-05-26 19:25:49,223]: Min: 0.36359540
[2025-05-26 19:25:49,223]: Max: 0.90085989
[2025-05-26 19:25:49,223]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,223]: Sample Values (25 elements): [0.1218869686126709, -0.1034475639462471, -0.05186251178383827, -0.17074470221996307, -0.015953153371810913, -0.057090334594249725, 0.07706116884946823, -0.04420347884297371, 0.06782472878694534, 0.018679482862353325, -0.1112808957695961, -0.22781291604042053, 0.2079666703939438, -0.07741934061050415, 0.014249846339225769, -0.05212904140353203, -0.012994000688195229, 0.039518292993307114, -0.0953638106584549, -0.09648954123258591, -0.07626567780971527, -0.25100943446159363, 0.04519112408161163, -0.04135190695524216, 0.09554464370012283]
[2025-05-26 19:25:49,224]: Mean: -0.00005613
[2025-05-26 19:25:49,224]: Min: -0.38320661
[2025-05-26 19:25:49,224]: Max: 0.42978662
[2025-05-26 19:25:49,224]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,224]: Sample Values (25 elements): [1.0279077291488647, 0.8736992478370667, 1.089428424835205, 0.8671848177909851, 0.8531743884086609, 0.9641868472099304, 1.1810895204544067, 0.9737184643745422, 0.9717438220977783, 1.067350149154663, 0.8427587151527405, 1.0137786865234375, 1.0606212615966797, 0.9673487544059753, 0.8253228068351746, 0.8748511075973511, 0.9576467275619507, 0.8490889072418213, 0.9072620868682861, 1.1921378374099731, 0.6950575113296509, 0.9068790674209595, 0.9881873726844788, 0.9554229974746704, 1.1203831434249878]
[2025-05-26 19:25:49,224]: Mean: 0.97886026
[2025-05-26 19:25:49,224]: Min: 0.69505751
[2025-05-26 19:25:49,225]: Max: 1.24778652
[2025-05-26 19:25:49,225]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 19:25:49,225]: Sample Values (25 elements): [0.028730880469083786, 0.054887160658836365, 0.059820957481861115, 0.06257103383541107, -0.1887228935956955, -0.023003198206424713, 0.11422443389892578, 0.09448191523551941, -0.023881804198026657, -0.0007510299328714609, -0.029263507574796677, -0.059206340461969376, -0.06258893013000488, 0.009591532871127129, 0.0094985980540514, 0.10835520923137665, -0.03481405973434448, 0.015740422531962395, -0.23733465373516083, 0.05616893246769905, -0.016278229653835297, 0.019520355388522148, 0.18700407445430756, 0.06040974333882332, -0.06725309789180756]
[2025-05-26 19:25:49,225]: Mean: -0.00276972
[2025-05-26 19:25:49,225]: Min: -0.35541722
[2025-05-26 19:25:49,225]: Max: 0.39067468
[2025-05-26 19:25:49,225]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-26 19:25:49,226]: Sample Values (25 elements): [0.7484176754951477, 0.6856278777122498, 0.6592332124710083, 0.8423532843589783, 0.5410075187683105, 0.7619869112968445, 0.7143532037734985, 0.6050164103507996, 0.687147319316864, 0.7717726826667786, 0.7312003374099731, 0.6960811614990234, 0.9285094141960144, 0.6091731190681458, 0.7554932236671448, 0.7532341480255127, 0.7161076068878174, 0.7410926222801208, 0.6251361966133118, 0.6648226976394653, 0.7410304546356201, 0.7821645140647888, 0.7959353923797607, 0.6855247616767883, 0.6721224188804626]
[2025-05-26 19:25:49,226]: Mean: 0.72466850
[2025-05-26 19:25:49,226]: Min: 0.54100752
[2025-05-26 19:25:49,226]: Max: 0.92850941
[2025-05-26 19:25:49,226]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-26 19:25:49,227]: Sample Values (25 elements): [0.06859387457370758, 0.26428869366645813, -0.03526131436228752, 0.029984623193740845, 0.05842458829283714, 0.05893673747777939, 0.038436632603406906, 0.07853853702545166, 0.054007142782211304, 0.018935317173600197, 0.011455805972218513, 0.012892463244497776, -0.015270154923200607, -0.04664624109864235, 0.011240709573030472, 0.07610319554805756, -0.06060526520013809, 0.024551697075366974, 0.2695280611515045, -0.17007403075695038, 0.017204629257321358, -0.08247186243534088, 0.0008313370635733008, 0.03941980004310608, 0.08578094840049744]
[2025-05-26 19:25:49,227]: Mean: -0.00175143
[2025-05-26 19:25:49,227]: Min: -0.31890848
[2025-05-26 19:25:49,227]: Max: 0.35801673
[2025-05-26 19:25:49,227]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,227]: Sample Values (25 elements): [0.91752028465271, 0.660677969455719, 0.6792107224464417, 0.649118185043335, 0.8805438280105591, 0.6918087005615234, 0.5499494075775146, 0.558104932308197, 0.9317766427993774, 1.0161499977111816, 0.7313655614852905, 0.8048710823059082, 0.7893196940422058, 0.49660158157348633, 0.9020243287086487, 0.7419540286064148, 0.7471784353256226, 0.5825760960578918, 0.7045317888259888, 0.59757000207901, 0.8969983458518982, 0.6642112731933594, 0.826509416103363, 0.12727788090705872, 0.6899623274803162]
[2025-05-26 19:25:49,228]: Mean: 0.78184640
[2025-05-26 19:25:49,228]: Min: 0.12727788
[2025-05-26 19:25:49,228]: Max: 1.01615000
[2025-05-26 19:25:49,228]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,228]: Sample Values (25 elements): [0.06260176748037338, -0.03313252329826355, 0.03797793388366699, 0.010048609226942062, 0.12885475158691406, 0.10150988399982452, -0.08843549340963364, 0.052842509001493454, 0.10479474812746048, 0.06481720507144928, 0.10054350644350052, 0.13527479767799377, -0.01976862922310829, 0.028135647997260094, -0.018662715330719948, -0.0492699109017849, -0.09187444299459457, 0.11318039894104004, 0.035133033990859985, -0.19772864878177643, -0.02239350415766239, 0.05873878300189972, -0.1258222609758377, 0.050508130341768265, -0.10903935134410858]
[2025-05-26 19:25:49,229]: Mean: 0.00083670
[2025-05-26 19:25:49,229]: Min: -0.33010036
[2025-05-26 19:25:49,229]: Max: 0.40634722
[2025-05-26 19:25:49,229]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,229]: Sample Values (25 elements): [0.8572869896888733, 0.9663800597190857, 0.7578933238983154, 0.8267126083374023, 0.8015729188919067, 0.6833119988441467, 0.801382303237915, 0.9152363538742065, 0.8314493298530579, 0.565557599067688, 0.7475906610488892, 0.7479345202445984, 0.6946561932563782, 0.8344639539718628, 0.6803284883499146, 0.8169525265693665, 0.7832987904548645, 0.7847591638565063, 0.891495943069458, 0.6318332552909851, 0.7886202335357666, 0.7874906659126282, 0.7319915890693665, 0.7958405613899231, 0.8230616450309753]
[2025-05-26 19:25:49,229]: Mean: 0.78900903
[2025-05-26 19:25:49,229]: Min: 0.55508131
[2025-05-26 19:25:49,230]: Max: 0.96638006
[2025-05-26 19:25:49,230]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-26 19:25:49,230]: Sample Values (25 elements): [0.052361708134412766, 0.18395762145519257, 0.20864969491958618, -0.01977970078587532, 0.05551696941256523, 0.02352883480489254, -0.04069014638662338, -0.06741443276405334, 0.18389910459518433, 0.09010924398899078, -0.2021322101354599, 0.08336222171783447, 0.08664087951183319, 0.40351271629333496, -0.09757648408412933, 0.0924091786146164, -0.12375335395336151, 0.09978058934211731, -0.2656565308570862, -0.06948740035295486, 0.0701228454709053, -0.12264305353164673, -0.04571028798818588, 0.15725894272327423, 0.10699167102575302]
[2025-05-26 19:25:49,230]: Mean: 0.00200642
[2025-05-26 19:25:49,230]: Min: -0.36108875
[2025-05-26 19:25:49,230]: Max: 0.40351272
[2025-05-26 19:25:49,230]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,231]: Sample Values (25 elements): [0.5862607359886169, 0.4137505888938904, 0.6273230314254761, 0.5661307573318481, 0.5390627384185791, 0.4840429723262787, 0.566608726978302, 0.4213906526565552, 0.7783549427986145, 0.4642132818698883, 0.5385741591453552, 0.6202110052108765, 0.6070090532302856, 0.5699092745780945, 0.4703812301158905, 0.5615012645721436, 0.44163426756858826, 0.7200524806976318, 0.5561913847923279, 0.6368650794029236, 0.41944047808647156, 0.515289306640625, 0.6306380033493042, 0.6617869138717651, 0.6395826935768127]
[2025-05-26 19:25:49,231]: Mean: 0.57039630
[2025-05-26 19:25:49,231]: Min: 0.29883018
[2025-05-26 19:25:49,231]: Max: 0.83254170
[2025-05-26 19:25:49,231]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,232]: Sample Values (25 elements): [-0.09091401100158691, -0.1423485279083252, 0.047228146344423294, -0.027119606733322144, 0.015889877453446388, -0.017009897157549858, 0.0021843407303094864, 0.1581466794013977, -0.09219294041395187, 0.09169994294643402, -0.13774727284908295, 0.03308899328112602, 0.058292511850595474, 0.026041634380817413, 0.0550784170627594, 0.01751887984573841, 0.1303550899028778, 0.045887209475040436, 0.1683134287595749, -0.24681925773620605, 0.04130978882312775, 0.0474642850458622, -0.031255193054676056, 0.1490822434425354, 0.004622669890522957]
[2025-05-26 19:25:49,232]: Mean: -0.00046814
[2025-05-26 19:25:49,232]: Min: -0.41842383
[2025-05-26 19:25:49,232]: Max: 0.38220432
[2025-05-26 19:25:49,232]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,232]: Sample Values (25 elements): [0.9379630088806152, 1.018417239189148, 0.7356694936752319, 1.1831963062286377, 0.8195781111717224, 0.931477427482605, 0.9846059679985046, 1.0843842029571533, 0.7146393656730652, 1.132835865020752, 1.0260753631591797, 1.0573025941848755, 0.5827283263206482, 0.7987610101699829, 1.1808334589004517, 1.0247660875320435, 0.9871360063552856, 0.9487981796264648, 0.9817982912063599, 0.6742023825645447, 1.2453160285949707, 0.8485867381095886, 1.029895305633545, 0.963786780834198, 1.210842490196228]
[2025-05-26 19:25:49,233]: Mean: 0.93083328
[2025-05-26 19:25:49,233]: Min: 0.27766928
[2025-05-26 19:25:49,233]: Max: 1.24531603
[2025-05-26 19:25:49,233]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,233]: Sample Values (25 elements): [-0.0015571418916806579, 0.03520526736974716, 0.021356990560889244, -0.06784307956695557, -0.02369275502860546, -0.01451900228857994, 0.006772765889763832, 0.09145644307136536, -0.009437517262995243, -0.005004816222935915, 0.16588102281093597, -0.04112320393323898, 0.06685656309127808, -0.014923393726348877, 0.0007537382189184427, -0.02198530174791813, 0.023143915459513664, 0.056166794151067734, -0.05324063077569008, 0.10294993966817856, -0.0495576336979866, 0.06515204906463623, -0.04375927150249481, -0.006720127072185278, 0.06306461244821548]
[2025-05-26 19:25:49,234]: Mean: 0.00016800
[2025-05-26 19:25:49,234]: Min: -0.33734655
[2025-05-26 19:25:49,234]: Max: 0.29873466
[2025-05-26 19:25:49,234]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,234]: Sample Values (25 elements): [0.5636031627655029, 0.9831992983818054, 0.8258786201477051, 0.8482595682144165, 0.9195650815963745, 0.8284926414489746, 0.8693817853927612, 0.8509705066680908, 0.7851013541221619, 0.8884161710739136, 0.9572181105613708, 0.8648001551628113, 0.565464973449707, 1.073982834815979, 0.8601230978965759, 0.9222607612609863, 0.8927865028381348, 0.868232786655426, 0.7988631725311279, 1.0247725248336792, 0.7776561975479126, 0.9128149151802063, 0.8539445996284485, 0.7773576974868774, 0.8099766969680786]
[2025-05-26 19:25:49,234]: Mean: 0.84853935
[2025-05-26 19:25:49,235]: Min: 0.53343213
[2025-05-26 19:25:49,235]: Max: 1.07487082
[2025-05-26 19:25:49,235]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,235]: Sample Values (25 elements): [0.007055334281176329, 2.3555245109313245e-15, 0.08110348135232925, -0.010314165614545345, 0.004885584581643343, -0.02363702282309532, -0.0251008253544569, 0.00557229109108448, 1.566093366101029e-10, 0.024137580767273903, -0.07568727433681488, -0.0335458368062973, -2.0797275460015126e-20, 0.03726132959127426, -0.0757635086774826, 0.002043792512267828, 0.11724317818880081, -0.03688366711139679, -0.0030550144147127867, -0.05723237246274948, 0.00549090001732111, 0.02464599721133709, 0.08142583817243576, 0.03096548654139042, -0.012338814325630665]
[2025-05-26 19:25:49,235]: Mean: -0.00114370
[2025-05-26 19:25:49,236]: Min: -0.30356410
[2025-05-26 19:25:49,236]: Max: 0.33977669
[2025-05-26 19:25:49,236]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,236]: Sample Values (25 elements): [1.029191017150879, 0.9601432085037231, 1.070718765258789, 0.25210824608802795, -4.1194562072632834e-06, 1.1457246706925162e-08, 0.6841678023338318, 0.8024532794952393, 1.0067239999771118, 1.287534475326538, 0.9756051301956177, 1.0376389026641846, 1.162787914276123, 0.8285731077194214, 0.9289435148239136, 0.5977728962898254, 0.8403424024581909, 0.8189029097557068, 1.0172781944274902, 9.766091579876957e-07, 0.7575250267982483, 0.7297480702400208, 0.9023464918136597, 1.1113135814666748, 1.041532039642334]
[2025-05-26 19:25:49,236]: Mean: 0.84565842
[2025-05-26 19:25:49,236]: Min: -0.00000412
[2025-05-26 19:25:49,236]: Max: 1.28753448
[2025-05-26 19:25:49,237]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 19:25:49,237]: Sample Values (25 elements): [0.0753449872136116, 0.0322544239461422, -0.042386461049318314, -0.06231702119112015, 0.036645788699388504, -0.003335181623697281, 0.07511647790670395, 0.016618240624666214, 0.008469179272651672, 0.04022154211997986, -0.10193488746881485, 0.06301049143075943, 0.062421079725027084, -0.050742942839860916, -0.08990135788917542, 0.03504370525479317, -0.025361888110637665, -0.0038629204500466585, 0.1470761001110077, -0.016421303153038025, -0.02896963618695736, -1.5409114956232983e-14, 0.0009333493653684855, -0.034533120691776276, -0.04211699962615967]
[2025-05-26 19:25:49,237]: Mean: -0.00067969
[2025-05-26 19:25:49,237]: Min: -0.23955834
[2025-05-26 19:25:49,237]: Max: 0.27092022
[2025-05-26 19:25:49,238]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-26 19:25:49,238]: Sample Values (25 elements): [1.1031378507614136, 1.0399597883224487, 1.3095059394836426, 0.9989219903945923, 0.8600848317146301, 1.2050906419754028, 0.9504304528236389, 1.0214667320251465, 0.9047538638114929, 1.014860987663269, 1.0168102979660034, 1.0596795082092285, 1.120157241821289, 1.1246250867843628, 1.0246623754501343, 1.0041230916976929, 0.9411888122558594, 1.1662544012069702, 1.199916958808899, 1.0152226686477661, 1.1036466360092163, 1.0628058910369873, 1.1137555837631226, 0.9881245493888855, 0.955162763595581]
[2025-05-26 19:25:49,238]: Mean: 1.04044986
[2025-05-26 19:25:49,238]: Min: 0.86008483
[2025-05-26 19:25:49,238]: Max: 1.30950594
[2025-05-26 19:25:49,238]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-26 19:25:49,239]: Sample Values (25 elements): [-0.5250959992408752, -0.5971916317939758, 0.12744519114494324, 0.24236413836479187, -0.02008187398314476, 0.2830040454864502, -0.33084893226623535, 0.48247459530830383, 0.43079084157943726, 0.34557825326919556, 0.7152265310287476, -0.17511063814163208, 0.4879647195339203, -0.31080684065818787, -0.5322420001029968, 0.09984149783849716, -0.7686123847961426, 0.39935821294784546, 0.20633244514465332, 0.5011131167411804, -0.1563509851694107, 0.27460452914237976, 0.6331163644790649, -0.6754001379013062, 0.7102773189544678]
[2025-05-26 19:25:49,239]: Mean: -0.00754639
[2025-05-26 19:25:49,239]: Min: -1.01073802
[2025-05-26 19:25:49,239]: Max: 1.14522386
[2025-05-26 19:25:49,239]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT

[2025-05-27 22:21:30,892]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT
[2025-05-27 22:21:30,892]: 


QAT of ResNet20 with hardtanh down to 4 bits...
[2025-05-27 22:21:31,162]: [ResNet20_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-27 22:21:31,428]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 22:22:30,280]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.4575 Train Acc: 0.8396 Eval Loss: 0.6946 Eval Acc: 0.7751 (LR: 0.00100000)
[2025-05-27 22:23:31,349]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.4566 Train Acc: 0.8403 Eval Loss: 0.6583 Eval Acc: 0.7960 (LR: 0.00100000)
[2025-05-27 22:25:01,351]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.4581 Train Acc: 0.8414 Eval Loss: 0.6501 Eval Acc: 0.7888 (LR: 0.00100000)
[2025-05-27 22:25:59,370]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.4513 Train Acc: 0.8420 Eval Loss: 0.5737 Eval Acc: 0.8144 (LR: 0.00100000)
[2025-05-27 22:26:57,140]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.4573 Train Acc: 0.8422 Eval Loss: 0.7688 Eval Acc: 0.7587 (LR: 0.00100000)
[2025-05-27 22:27:54,645]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.4532 Train Acc: 0.8423 Eval Loss: 0.5277 Eval Acc: 0.8284 (LR: 0.00100000)
[2025-05-27 22:28:51,990]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.4554 Train Acc: 0.8411 Eval Loss: 0.6280 Eval Acc: 0.7976 (LR: 0.00100000)
[2025-05-27 22:29:51,751]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.4496 Train Acc: 0.8436 Eval Loss: 0.6491 Eval Acc: 0.7996 (LR: 0.00100000)
[2025-05-27 22:30:48,999]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.4463 Train Acc: 0.8440 Eval Loss: 0.7480 Eval Acc: 0.7619 (LR: 0.00100000)
[2025-05-27 22:31:46,812]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.4432 Train Acc: 0.8462 Eval Loss: 0.5513 Eval Acc: 0.8161 (LR: 0.00100000)
[2025-05-27 22:32:42,168]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.4419 Train Acc: 0.8464 Eval Loss: 0.7998 Eval Acc: 0.7523 (LR: 0.00100000)
[2025-05-27 22:33:39,177]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.4392 Train Acc: 0.8472 Eval Loss: 0.6290 Eval Acc: 0.7959 (LR: 0.00010000)
[2025-05-27 22:34:41,264]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.3525 Train Acc: 0.8787 Eval Loss: 0.4084 Eval Acc: 0.8639 (LR: 0.00010000)
[2025-05-27 22:35:40,772]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.3324 Train Acc: 0.8845 Eval Loss: 0.3988 Eval Acc: 0.8684 (LR: 0.00010000)
[2025-05-27 22:36:39,053]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.3232 Train Acc: 0.8887 Eval Loss: 0.4043 Eval Acc: 0.8665 (LR: 0.00010000)
[2025-05-27 22:37:40,641]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.3195 Train Acc: 0.8890 Eval Loss: 0.4279 Eval Acc: 0.8595 (LR: 0.00010000)
[2025-05-27 22:38:39,877]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.3174 Train Acc: 0.8886 Eval Loss: 0.4064 Eval Acc: 0.8667 (LR: 0.00010000)
[2025-05-27 22:39:38,793]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.3119 Train Acc: 0.8918 Eval Loss: 0.4074 Eval Acc: 0.8633 (LR: 0.00010000)
[2025-05-27 22:40:38,771]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.3128 Train Acc: 0.8915 Eval Loss: 0.4103 Eval Acc: 0.8653 (LR: 0.00010000)
[2025-05-27 22:41:36,008]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.3064 Train Acc: 0.8930 Eval Loss: 0.4027 Eval Acc: 0.8679 (LR: 0.00001000)
[2025-05-27 22:42:33,373]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.2948 Train Acc: 0.8975 Eval Loss: 0.3927 Eval Acc: 0.8718 (LR: 0.00001000)
[2025-05-27 22:43:32,086]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.2905 Train Acc: 0.8994 Eval Loss: 0.3910 Eval Acc: 0.8743 (LR: 0.00001000)
[2025-05-27 22:44:35,635]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.2882 Train Acc: 0.9004 Eval Loss: 0.3906 Eval Acc: 0.8719 (LR: 0.00001000)
[2025-05-27 22:45:36,788]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.2886 Train Acc: 0.9004 Eval Loss: 0.3916 Eval Acc: 0.8711 (LR: 0.00001000)
[2025-05-27 22:46:32,837]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.2854 Train Acc: 0.9008 Eval Loss: 0.3857 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-05-27 22:47:28,735]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.2845 Train Acc: 0.9011 Eval Loss: 0.3885 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-05-27 22:48:28,855]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.2871 Train Acc: 0.9013 Eval Loss: 0.3893 Eval Acc: 0.8725 (LR: 0.00001000)
[2025-05-27 22:49:23,804]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.2833 Train Acc: 0.9017 Eval Loss: 0.3899 Eval Acc: 0.8738 (LR: 0.00001000)
[2025-05-27 22:50:18,777]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.2832 Train Acc: 0.9012 Eval Loss: 0.3946 Eval Acc: 0.8707 (LR: 0.00001000)
[2025-05-27 22:51:12,941]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.2856 Train Acc: 0.9010 Eval Loss: 0.3930 Eval Acc: 0.8707 (LR: 0.00001000)
[2025-05-27 22:52:11,112]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.2854 Train Acc: 0.9006 Eval Loss: 0.3921 Eval Acc: 0.8726 (LR: 0.00000100)
[2025-05-27 22:53:09,257]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.2805 Train Acc: 0.9029 Eval Loss: 0.3877 Eval Acc: 0.8758 (LR: 0.00000100)
[2025-05-27 22:54:05,538]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.2855 Train Acc: 0.9009 Eval Loss: 0.3887 Eval Acc: 0.8730 (LR: 0.00000100)
[2025-05-27 22:55:05,908]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.2808 Train Acc: 0.9034 Eval Loss: 0.3851 Eval Acc: 0.8741 (LR: 0.00000100)
[2025-05-27 22:56:05,877]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.2835 Train Acc: 0.9025 Eval Loss: 0.3896 Eval Acc: 0.8725 (LR: 0.00000100)
[2025-05-27 22:57:05,893]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.2828 Train Acc: 0.9028 Eval Loss: 0.3884 Eval Acc: 0.8726 (LR: 0.00000100)
[2025-05-27 22:58:06,967]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.2843 Train Acc: 0.9012 Eval Loss: 0.3892 Eval Acc: 0.8727 (LR: 0.00000100)
[2025-05-27 22:59:04,934]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.2812 Train Acc: 0.9027 Eval Loss: 0.3896 Eval Acc: 0.8732 (LR: 0.00000100)
[2025-05-27 23:00:02,399]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.2832 Train Acc: 0.9024 Eval Loss: 0.3860 Eval Acc: 0.8741 (LR: 0.00000100)
[2025-05-27 23:00:58,782]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.2847 Train Acc: 0.9016 Eval Loss: 0.3904 Eval Acc: 0.8715 (LR: 0.00000010)
[2025-05-27 23:01:54,136]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.2812 Train Acc: 0.9021 Eval Loss: 0.3882 Eval Acc: 0.8735 (LR: 0.00000010)
[2025-05-27 23:02:48,447]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.2824 Train Acc: 0.9025 Eval Loss: 0.3870 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:03:43,626]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.2815 Train Acc: 0.9008 Eval Loss: 0.3909 Eval Acc: 0.8732 (LR: 0.00000010)
[2025-05-27 23:04:39,717]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.2827 Train Acc: 0.9031 Eval Loss: 0.3886 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:05:39,122]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.2828 Train Acc: 0.9020 Eval Loss: 0.3883 Eval Acc: 0.8716 (LR: 0.00000010)
[2025-05-27 23:06:33,148]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.2810 Train Acc: 0.9023 Eval Loss: 0.3881 Eval Acc: 0.8748 (LR: 0.00000010)
[2025-05-27 23:07:26,132]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.2816 Train Acc: 0.9011 Eval Loss: 0.3879 Eval Acc: 0.8731 (LR: 0.00000010)
[2025-05-27 23:08:19,829]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.2836 Train Acc: 0.9014 Eval Loss: 0.3881 Eval Acc: 0.8721 (LR: 0.00000010)
[2025-05-27 23:09:15,192]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.2803 Train Acc: 0.9019 Eval Loss: 0.3868 Eval Acc: 0.8724 (LR: 0.00000010)
[2025-05-27 23:10:10,602]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.2800 Train Acc: 0.9051 Eval Loss: 0.3881 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:11:08,284]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.2820 Train Acc: 0.9010 Eval Loss: 0.3884 Eval Acc: 0.8727 (LR: 0.00000010)
[2025-05-27 23:12:04,909]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.2825 Train Acc: 0.9009 Eval Loss: 0.3887 Eval Acc: 0.8732 (LR: 0.00000010)
[2025-05-27 23:13:02,858]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.2801 Train Acc: 0.9021 Eval Loss: 0.3883 Eval Acc: 0.8733 (LR: 0.00000010)
[2025-05-27 23:13:56,621]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.2777 Train Acc: 0.9031 Eval Loss: 0.3892 Eval Acc: 0.8736 (LR: 0.00000010)
[2025-05-27 23:14:51,046]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.2795 Train Acc: 0.9033 Eval Loss: 0.3871 Eval Acc: 0.8752 (LR: 0.00000010)
[2025-05-27 23:16:05,197]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.2807 Train Acc: 0.9022 Eval Loss: 0.3881 Eval Acc: 0.8739 (LR: 0.00000010)
[2025-05-27 23:17:06,758]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.2821 Train Acc: 0.9031 Eval Loss: 0.3887 Eval Acc: 0.8748 (LR: 0.00000010)
[2025-05-27 23:18:01,942]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.2802 Train Acc: 0.9030 Eval Loss: 0.3874 Eval Acc: 0.8746 (LR: 0.00000010)
[2025-05-27 23:18:57,124]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.2804 Train Acc: 0.9026 Eval Loss: 0.3894 Eval Acc: 0.8730 (LR: 0.00000010)
[2025-05-27 23:19:53,865]: [ResNet20_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.2819 Train Acc: 0.9016 Eval Loss: 0.3884 Eval Acc: 0.8739 (LR: 0.00000010)
[2025-05-27 23:19:53,865]: [ResNet20_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8758
[2025-05-27 23:19:53,942]: 


Quantization of model down to 4 bits finished
[2025-05-27 23:19:53,942]: Model Architecture:
[2025-05-27 23:19:54,009]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0849], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6143192052841187, max_val=0.6598900556564331)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0674], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.522510290145874, max_val=0.4878910183906555)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0659], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4825107455253601, max_val=0.5058311223983765)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0723], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.463073194026947, max_val=0.6220464706420898)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0663], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5766500234603882, max_val=0.41732466220855713)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0710], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4599286913871765, max_val=0.6046261787414551)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4759485125541687, max_val=0.45402586460113525)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.44458848237991333, max_val=0.42457979917526245)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0626], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5297294855117798, max_val=0.4092288017272949)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0541], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4395017623901367, max_val=0.37151724100112915)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0508], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3854938745498657, max_val=0.3761533498764038)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3882233500480652, max_val=0.3930966258049011)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39195799827575684, max_val=0.3915663957595825)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3756554126739502, max_val=0.40544217824935913)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0500], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35337257385253906, max_val=0.3960513472557068)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0525], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3757522702217102, max_val=0.4115116000175476)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0516], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40120452642440796, max_val=0.3724977970123291)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0444], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3335478901863098, max_val=0.33220237493515015)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0432], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.298761248588562, max_val=0.3498861789703369)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.29387879371643066, max_val=0.2833215594291687)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=14, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1429], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 23:19:54,010]: 
Model Weights:
[2025-05-27 23:19:54,010]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 23:19:54,014]: Sample Values (25 elements): [-0.6426541209220886, 0.24111595749855042, 0.016744570806622505, -0.11762793362140656, 0.0823795273900032, 0.3330398201942444, -0.058072254061698914, -0.057918839156627655, 0.11025778949260712, -0.13092070817947388, -0.20155860483646393, 0.07468586415052414, -0.10484182834625244, -0.009911373257637024, -0.06779824942350388, -0.35034871101379395, 0.18598516285419464, 0.12728391587734222, -0.109271340072155, 0.054927680641412735, -0.3412022590637207, -0.058659348636865616, 0.22740860283374786, -0.08660665154457092, 0.16899335384368896]
[2025-05-27 23:19:54,026]: Mean: 0.00053960
[2025-05-27 23:19:54,027]: Min: -0.64265412
[2025-05-27 23:19:54,028]: Max: 0.37439919
[2025-05-27 23:19:54,028]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,028]: Sample Values (16 elements): [0.7199600338935852, 0.881969153881073, 0.6384759545326233, 0.5620710849761963, 0.5421256422996521, 0.7594660520553589, 0.8202391266822815, 0.8054354190826416, 0.8234019875526428, 1.205202341079712, 0.6614911556243896, 0.6598211526870728, 0.6744239926338196, 0.721225917339325, 0.8854226469993591, 0.74123615026474]
[2025-05-27 23:19:54,028]: Mean: 0.75637299
[2025-05-27 23:19:54,028]: Min: 0.54212564
[2025-05-27 23:19:54,029]: Max: 1.20520234
[2025-05-27 23:19:54,030]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,030]: Sample Values (25 elements): [-0.2548418641090393, -0.2548418641090393, -0.16989457607269287, 0.0, -0.16989457607269287, 0.08494728803634644, 0.0, 0.08494728803634644, -0.08494728803634644, 0.0, 0.0, 0.08494728803634644, 0.0, -0.2548418641090393, 0.0, -0.08494728803634644, -0.2548418641090393, -0.08494728803634644, 0.2548418641090393, 0.08494728803634644, 0.0, -0.33978915214538574, 0.0, 0.08494728803634644, 0.08494728803634644]
[2025-05-27 23:19:54,030]: Mean: -0.00184347
[2025-05-27 23:19:54,030]: Min: -0.59463102
[2025-05-27 23:19:54,030]: Max: 0.67957830
[2025-05-27 23:19:54,030]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,031]: Sample Values (16 elements): [0.8899332284927368, 0.7601288557052612, 0.712221086025238, 0.8277907371520996, 0.962030291557312, 0.9623211026191711, 0.8248394131660461, 0.8549318909645081, 1.0059703588485718, 0.9106495976448059, 0.6172178983688354, 0.6069960594177246, 0.8379213809967041, 1.1514277458190918, 1.121148705482483, 1.0220801830291748]
[2025-05-27 23:19:54,031]: Mean: 0.87922549
[2025-05-27 23:19:54,031]: Min: 0.60699606
[2025-05-27 23:19:54,031]: Max: 1.15142775
[2025-05-27 23:19:54,033]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,033]: Sample Values (25 elements): [0.0, -0.06736008822917938, 0.0, 0.0, -0.06736008822917938, 0.0, 0.20208026468753815, -0.06736008822917938, -0.13472017645835876, 0.06736008822917938, 0.13472017645835876, 0.06736008822917938, 0.26944035291671753, 0.06736008822917938, 0.06736008822917938, 0.13472017645835876, 0.0, 0.06736008822917938, 0.0, -0.06736008822917938, 0.20208026468753815, 0.0, 0.13472017645835876, 0.0, 0.06736008822917938]
[2025-05-27 23:19:54,033]: Mean: 0.00230966
[2025-05-27 23:19:54,034]: Min: -0.53888071
[2025-05-27 23:19:54,034]: Max: 0.47152060
[2025-05-27 23:19:54,034]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,034]: Sample Values (16 elements): [0.7521250247955322, 0.6249299645423889, 0.5161903500556946, 0.9358468651771545, 0.8913898468017578, 0.6669401526451111, 0.6192550659179688, 0.8743323683738708, 0.5826666951179504, 0.6606810688972473, 0.8357117772102356, 0.845569372177124, 0.7029781937599182, 0.5743013620376587, 0.801015317440033, 0.8751368522644043]
[2025-05-27 23:19:54,034]: Mean: 0.73494184
[2025-05-27 23:19:54,034]: Min: 0.51619035
[2025-05-27 23:19:54,035]: Max: 0.93584687
[2025-05-27 23:19:54,036]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,036]: Sample Values (25 elements): [0.0, -0.06588946282863617, 0.2635578513145447, -0.1976683884859085, -0.13177892565727234, -0.06588946282863617, -0.06588946282863617, 0.0, 0.1976683884859085, -0.13177892565727234, 0.1976683884859085, 0.13177892565727234, -0.06588946282863617, 0.06588946282863617, 0.1976683884859085, 0.13177892565727234, -0.13177892565727234, 0.13177892565727234, 0.13177892565727234, 0.06588946282863617, -0.06588946282863617, 0.13177892565727234, -0.06588946282863617, -0.1976683884859085, -0.13177892565727234]
[2025-05-27 23:19:54,036]: Mean: -0.00280259
[2025-05-27 23:19:54,036]: Min: -0.46122622
[2025-05-27 23:19:54,036]: Max: 0.52711570
[2025-05-27 23:19:54,036]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,037]: Sample Values (16 elements): [0.8616681098937988, 1.1402956247329712, 1.3249397277832031, 1.0682018995285034, 1.0368916988372803, 0.7879756689071655, 0.8057215213775635, 0.8601382970809937, 0.8772050142288208, 1.0859421491622925, 1.218174695968628, 0.6953684091567993, 0.9671945571899414, 1.2275135517120361, 0.8333852887153625, 0.6855931282043457]
[2025-05-27 23:19:54,037]: Mean: 0.96726310
[2025-05-27 23:19:54,037]: Min: 0.68559313
[2025-05-27 23:19:54,037]: Max: 1.32493973
[2025-05-27 23:19:54,038]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,039]: Sample Values (25 elements): [-0.1446826308965683, 0.07234131544828415, -0.07234131544828415, 0.07234131544828415, -0.07234131544828415, 0.2893652617931366, -0.07234131544828415, 0.1446826308965683, 0.0, -0.21702393889427185, 0.1446826308965683, -0.21702393889427185, -0.07234131544828415, -0.21702393889427185, 0.21702393889427185, 0.0, 0.07234131544828415, -0.07234131544828415, 0.21702393889427185, -0.21702393889427185, 0.07234131544828415, -0.21702393889427185, 0.07234131544828415, 0.2893652617931366, 0.21702393889427185]
[2025-05-27 23:19:54,039]: Mean: -0.00405036
[2025-05-27 23:19:54,039]: Min: -0.43404788
[2025-05-27 23:19:54,039]: Max: 0.65107185
[2025-05-27 23:19:54,039]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,039]: Sample Values (16 elements): [0.5682204365730286, 0.7829757332801819, 0.5287565588951111, 0.6091507077217102, 0.7302096486091614, 0.659342885017395, 0.5385982394218445, 0.5695311427116394, 0.4540361166000366, 0.5341546535491943, 0.8589759469032288, 0.4923173189163208, 0.6041142344474792, 0.8115857839584351, 0.6709096431732178, 1.0032650232315063]
[2025-05-27 23:19:54,040]: Mean: 0.65100902
[2025-05-27 23:19:54,040]: Min: 0.45403612
[2025-05-27 23:19:54,040]: Max: 1.00326502
[2025-05-27 23:19:54,041]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,041]: Sample Values (25 elements): [-0.06626497954130173, -0.13252995908260345, 0.06626497954130173, -0.06626497954130173, 0.0, -0.19879493117332458, 0.0, -0.33132490515708923, -0.06626497954130173, 0.06626497954130173, -0.06626497954130173, -0.13252995908260345, -0.06626497954130173, -0.19879493117332458, 0.0, 0.13252995908260345, -0.13252995908260345, 0.0, -0.06626497954130173, 0.06626497954130173, -0.19879493117332458, 0.06626497954130173, 0.0, -0.13252995908260345, 0.0]
[2025-05-27 23:19:54,042]: Mean: -0.00839817
[2025-05-27 23:19:54,042]: Min: -0.59638482
[2025-05-27 23:19:54,042]: Max: 0.39758986
[2025-05-27 23:19:54,042]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,042]: Sample Values (16 elements): [0.774094820022583, 0.9024201035499573, 0.8568373322486877, 0.9513039588928223, 1.1291375160217285, 0.7718190550804138, 1.0559113025665283, 0.8585480451583862, 0.9195536971092224, 0.948539137840271, 1.0159072875976562, 1.0982182025909424, 0.972059428691864, 0.8692214488983154, 0.8295823931694031, 1.0011394023895264]
[2025-05-27 23:19:54,042]: Mean: 0.93464333
[2025-05-27 23:19:54,042]: Min: 0.77181906
[2025-05-27 23:19:54,043]: Max: 1.12913752
[2025-05-27 23:19:54,044]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 23:19:54,044]: Sample Values (25 elements): [0.0709703266620636, -0.1419406533241272, 0.1419406533241272, -0.0709703266620636, -0.0709703266620636, 0.0709703266620636, 0.2129109799861908, -0.0709703266620636, -0.0709703266620636, -0.2129109799861908, 0.2838813066482544, -0.1419406533241272, 0.1419406533241272, -0.354851633310318, 0.0, -0.0709703266620636, 0.0, 0.0, 0.0709703266620636, 0.0709703266620636, -0.2129109799861908, 0.0709703266620636, -0.1419406533241272, -0.2129109799861908, 0.1419406533241272]
[2025-05-27 23:19:54,044]: Mean: -0.00135534
[2025-05-27 23:19:54,044]: Min: -0.42582196
[2025-05-27 23:19:54,044]: Max: 0.63873291
[2025-05-27 23:19:54,044]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 23:19:54,045]: Sample Values (16 elements): [0.4638417661190033, 0.7178043127059937, 0.9891815185546875, 0.5783552527427673, 0.6532819271087646, 0.701474666595459, 0.5944343209266663, 0.6689743995666504, 0.5444214344024658, 0.7103608250617981, 0.6213151216506958, 0.8959854245185852, 0.7364267110824585, 0.6333847641944885, 0.7398566007614136, 0.6179550290107727]
[2025-05-27 23:19:54,045]: Mean: 0.67919087
[2025-05-27 23:19:54,045]: Min: 0.46384177
[2025-05-27 23:19:54,045]: Max: 0.98918152
[2025-05-27 23:19:54,046]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 23:19:54,047]: Sample Values (25 elements): [0.18599487841129303, -0.18599487841129303, 0.0, -0.12399658560752869, -0.24799317121505737, 0.18599487841129303, -0.06199829280376434, -0.06199829280376434, -0.06199829280376434, 0.06199829280376434, 0.3099914789199829, -0.12399658560752869, -0.24799317121505737, -0.06199829280376434, 0.0, -0.06199829280376434, 0.0, 0.06199829280376434, -0.24799317121505737, -0.12399658560752869, 0.12399658560752869, 0.18599487841129303, 0.12399658560752869, -0.12399658560752869, -0.12399658560752869]
[2025-05-27 23:19:54,047]: Mean: 0.00527416
[2025-05-27 23:19:54,047]: Min: -0.49598634
[2025-05-27 23:19:54,047]: Max: 0.43398803
[2025-05-27 23:19:54,047]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,048]: Sample Values (25 elements): [0.8766804337501526, 0.727696418762207, 0.9548017382621765, 0.712350070476532, 0.7960881590843201, 0.839598536491394, 0.7350967526435852, 0.8040611743927002, 0.8831131458282471, 0.7531800866127014, 0.8227335810661316, 0.8062945008277893, 0.7801333069801331, 0.7187216877937317, 0.8984827399253845, 0.7931334376335144, 0.8789087533950806, 0.6573391556739807, 0.6477357745170593, 0.6769729256629944, 0.7792772054672241, 0.7470419406890869, 0.7024821043014526, 0.6224122047424316, 0.645995020866394]
[2025-05-27 23:19:54,048]: Mean: 0.75939178
[2025-05-27 23:19:54,048]: Min: 0.56548613
[2025-05-27 23:19:54,048]: Max: 0.95480174
[2025-05-27 23:19:54,049]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,049]: Sample Values (25 elements): [-0.05794455483555794, 0.05794455483555794, 0.0, 0.0, 0.05794455483555794, -0.23177821934223175, 0.0, 0.05794455483555794, -0.1738336682319641, -0.05794455483555794, 0.11588910967111588, 0.0, -0.05794455483555794, 0.05794455483555794, 0.0, 0.0, 0.05794455483555794, -0.05794455483555794, 0.05794455483555794, -0.11588910967111588, -0.11588910967111588, -0.05794455483555794, -0.05794455483555794, 0.0, -0.11588910967111588]
[2025-05-27 23:19:54,049]: Mean: 0.00180448
[2025-05-27 23:19:54,050]: Min: -0.46355644
[2025-05-27 23:19:54,050]: Max: 0.40561187
[2025-05-27 23:19:54,050]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,050]: Sample Values (25 elements): [0.651753306388855, 0.7912667989730835, 0.8367494344711304, 0.8184898495674133, 0.6650214791297913, 0.6676897406578064, 0.6289339065551758, 0.784439742565155, 0.6856152415275574, 0.6865620017051697, 0.7459314465522766, 0.6132388114929199, 0.6334460973739624, 0.6744452714920044, 0.6377683281898499, 0.8980578780174255, 0.693994402885437, 0.7520151138305664, 0.7556484341621399, 0.8107056021690369, 0.7498130798339844, 0.7519232630729675, 0.8186742663383484, 0.6517654657363892, 0.737139880657196]
[2025-05-27 23:19:54,050]: Mean: 0.73543894
[2025-05-27 23:19:54,050]: Min: 0.57592297
[2025-05-27 23:19:54,051]: Max: 1.00900102
[2025-05-27 23:19:54,052]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 23:19:54,052]: Sample Values (25 elements): [-0.18779167532920837, 0.0, -0.12519444525241852, 0.12519444525241852, -0.06259722262620926, -0.06259722262620926, 0.12519444525241852, -0.06259722262620926, 0.37558335065841675, 0.25038889050483704, -0.06259722262620926, 0.06259722262620926, 0.0, 0.12519444525241852, 0.18779167532920837, -0.06259722262620926, 0.37558335065841675, 0.18779167532920837, 0.0, -0.06259722262620926, 0.3129861056804657, -0.06259722262620926, 0.37558335065841675, -0.5007777810096741, 0.0]
[2025-05-27 23:19:54,052]: Mean: 0.00978082
[2025-05-27 23:19:54,052]: Min: -0.50077778
[2025-05-27 23:19:54,053]: Max: 0.43818057
[2025-05-27 23:19:54,053]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,053]: Sample Values (25 elements): [0.40467333793640137, 0.3222782015800476, 0.5021571516990662, 0.4087270200252533, 0.4922991991043091, 0.570088267326355, 0.46277356147766113, 0.7077815532684326, 0.24762405455112457, 0.49636736512184143, 0.4645204544067383, 0.30882537364959717, 0.45357707142829895, 0.4215966761112213, 0.5498287677764893, 0.3778231143951416, 0.503854513168335, 0.39936137199401855, 0.6223498582839966, 0.33328622579574585, 0.5821948051452637, 0.37810003757476807, 0.40591734647750854, 0.5688435435295105, 0.6026618480682373]
[2025-05-27 23:19:54,053]: Mean: 0.45248979
[2025-05-27 23:19:54,053]: Min: 0.18908732
[2025-05-27 23:19:54,054]: Max: 0.70778155
[2025-05-27 23:19:54,055]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,055]: Sample Values (25 elements): [0.0, -0.3244076073169708, 0.1622038036584854, -0.054067935794591904, -0.1622038036584854, 0.0, 0.0, -0.10813587158918381, 0.054067935794591904, -0.10813587158918381, 0.1622038036584854, -0.054067935794591904, -0.1622038036584854, 0.054067935794591904, 0.0, -0.054067935794591904, 0.054067935794591904, 0.10813587158918381, 0.1622038036584854, -0.054067935794591904, -0.1622038036584854, 0.10813587158918381, 0.0, 0.054067935794591904, 0.10813587158918381]
[2025-05-27 23:19:54,055]: Mean: 0.00222350
[2025-05-27 23:19:54,055]: Min: -0.43254349
[2025-05-27 23:19:54,055]: Max: 0.37847555
[2025-05-27 23:19:54,055]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,056]: Sample Values (25 elements): [0.6966106295585632, 1.05405592918396, 0.9146935939788818, 0.8274841904640198, 0.9573546051979065, 0.9360926151275635, 0.9816900491714478, 0.7152800559997559, 0.9442707896232605, 0.8961313366889954, 0.8428031802177429, 0.8093924522399902, 0.7748339772224426, 0.7056224346160889, 0.8171281814575195, 0.8141465187072754, 1.0559183359146118, 0.6963238716125488, 0.7825970649719238, 0.8672521114349365, 1.0350563526153564, 1.11383056640625, 0.8549882769584656, 0.8005373477935791, 0.8851532340049744]
[2025-05-27 23:19:54,056]: Mean: 0.86529732
[2025-05-27 23:19:54,056]: Min: 0.69632387
[2025-05-27 23:19:54,056]: Max: 1.11383057
[2025-05-27 23:19:54,057]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,058]: Sample Values (25 elements): [0.1523294597864151, -0.05077648535370827, 0.1523294597864151, 0.0, 0.0, 0.3046589195728302, -0.10155297070741653, 0.1523294597864151, -0.05077648535370827, -0.10155297070741653, 0.05077648535370827, 0.1523294597864151, -0.05077648535370827, -0.1523294597864151, 0.05077648535370827, 0.05077648535370827, 0.0, 0.10155297070741653, 0.10155297070741653, 0.0, -0.10155297070741653, 0.0, -0.10155297070741653, -0.05077648535370827, -0.1523294597864151]
[2025-05-27 23:19:54,058]: Mean: 0.00091459
[2025-05-27 23:19:54,058]: Min: -0.40621188
[2025-05-27 23:19:54,058]: Max: 0.35543540
[2025-05-27 23:19:54,058]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,058]: Sample Values (25 elements): [0.8891102075576782, 0.6821486949920654, 0.7443974614143372, 0.5714499354362488, 0.48448213934898376, 0.7316824793815613, 0.6816507577896118, 0.5808258056640625, 0.5397822260856628, 0.6322325468063354, 0.762410044670105, 0.6724870800971985, 0.6292170882225037, 0.509261965751648, 0.5566011071205139, 0.7884519100189209, 0.817934513092041, 0.6877077221870422, 0.7665836215019226, 0.6611663699150085, 0.6399352550506592, 0.7318851351737976, 0.7181316018104553, 0.8122629523277283, 0.3406904637813568]
[2025-05-27 23:19:54,058]: Mean: 0.65841293
[2025-05-27 23:19:54,059]: Min: 0.34069046
[2025-05-27 23:19:54,059]: Max: 0.88911021
[2025-05-27 23:19:54,060]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,060]: Sample Values (25 elements): [0.10417599976062775, -0.05208799988031387, 0.0, -0.15626400709152222, 0.05208799988031387, 0.05208799988031387, -0.05208799988031387, 0.0, -0.15626400709152222, -0.15626400709152222, -0.10417599976062775, 0.05208799988031387, 0.0, 0.0, -0.05208799988031387, 0.05208799988031387, 0.0, 0.15626400709152222, 0.2083519995212555, 0.10417599976062775, 0.0, 0.0, -0.05208799988031387, 0.10417599976062775, 0.0]
[2025-05-27 23:19:54,060]: Mean: 0.00003391
[2025-05-27 23:19:54,061]: Min: -0.36461601
[2025-05-27 23:19:54,061]: Max: 0.41670400
[2025-05-27 23:19:54,061]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,061]: Sample Values (25 elements): [0.8089678883552551, 0.8977544903755188, 1.003676414489746, 0.9558132886886597, 0.7295958399772644, 0.8922988772392273, 0.8717154264450073, 0.8763931393623352, 0.8163840174674988, 0.9346815347671509, 0.9750272631645203, 0.8413995504379272, 0.979736864566803, 1.2382982969284058, 0.9550454616546631, 0.842961847782135, 0.9794014096260071, 0.8949662446975708, 1.0497022867202759, 1.0617640018463135, 1.0035626888275146, 1.0286775827407837, 1.3124303817749023, 0.9973174333572388, 1.160744547843933]
[2025-05-27 23:19:54,061]: Mean: 0.98241794
[2025-05-27 23:19:54,061]: Min: 0.72959584
[2025-05-27 23:19:54,062]: Max: 1.31243038
[2025-05-27 23:19:54,063]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 23:19:54,063]: Sample Values (25 elements): [-0.052234962582588196, -0.052234962582588196, 0.0, -0.10446992516517639, 0.20893985033035278, 0.052234962582588196, 0.1567048877477646, 0.1567048877477646, -0.052234962582588196, 0.052234962582588196, 0.10446992516517639, -0.052234962582588196, -0.052234962582588196, -0.10446992516517639, -0.1567048877477646, 0.0, 0.10446992516517639, -0.10446992516517639, 0.1567048877477646, 0.10446992516517639, 0.0, -0.1567048877477646, 0.1567048877477646, 0.10446992516517639, -0.20893985033035278]
[2025-05-27 23:19:54,063]: Mean: -0.00264122
[2025-05-27 23:19:54,063]: Min: -0.41787970
[2025-05-27 23:19:54,063]: Max: 0.36564475
[2025-05-27 23:19:54,063]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 23:19:54,064]: Sample Values (25 elements): [0.7371126413345337, 0.6676781177520752, 0.7907653450965881, 0.6601159572601318, 0.6695426106452942, 0.7352165579795837, 0.7242993712425232, 0.842538595199585, 0.6749321818351746, 0.5772538781166077, 0.7543803453445435, 0.6510277986526489, 0.643217146396637, 0.7170736789703369, 0.6842182874679565, 0.6522552967071533, 0.9282348155975342, 0.7512209415435791, 0.6162936091423035, 0.7056678533554077, 0.6557865738868713, 0.8441653847694397, 0.6716113686561584, 0.5930519700050354, 0.5293178558349609]
[2025-05-27 23:19:54,064]: Mean: 0.69585884
[2025-05-27 23:19:54,064]: Min: 0.52931786
[2025-05-27 23:19:54,064]: Max: 0.92823482
[2025-05-27 23:19:54,065]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 23:19:54,066]: Sample Values (25 elements): [0.10414635390043259, 0.10414635390043259, -0.05207317695021629, -0.10414635390043259, -0.20829270780086517, 0.10414635390043259, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20829270780086517, 0.0, -0.10414635390043259, 0.05207317695021629, -0.10414635390043259, -0.05207317695021629, -0.05207317695021629, 0.15621952712535858, -0.05207317695021629, 0.05207317695021629, 0.0, 0.05207317695021629, -0.05207317695021629, 0.05207317695021629]
[2025-05-27 23:19:54,066]: Mean: -0.00218102
[2025-05-27 23:19:54,066]: Min: -0.36451223
[2025-05-27 23:19:54,066]: Max: 0.41658542
[2025-05-27 23:19:54,066]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,067]: Sample Values (25 elements): [0.9838815331459045, 0.547332227230072, 0.7798359394073486, 0.6834213137626648, 0.7784376740455627, 0.8635265827178955, 1.0005815029144287, 0.6682615876197815, 0.9010177254676819, 0.570982038974762, 0.8330108523368835, 0.804256021976471, 0.6676335334777832, 0.7657063007354736, 0.8915756344795227, 0.42705899477005005, 0.7534747123718262, 0.9701852202415466, 0.8571138978004456, 0.7816236019134521, 0.8786206841468811, 0.9923309683799744, 0.6413992047309875, 0.6854091286659241, 0.8703923225402832]
[2025-05-27 23:19:54,067]: Mean: 0.77300596
[2025-05-27 23:19:54,067]: Min: 0.00483258
[2025-05-27 23:19:54,067]: Max: 1.00058150
[2025-05-27 23:19:54,068]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,069]: Sample Values (25 elements): [0.14988479018211365, 0.14988479018211365, -0.09992319345474243, 0.0, 0.049961596727371216, 0.0, -0.049961596727371216, 0.14988479018211365, -0.049961596727371216, -0.09992319345474243, 0.14988479018211365, 0.09992319345474243, 0.14988479018211365, 0.0, -0.049961596727371216, 0.0, -0.09992319345474243, 0.049961596727371216, 0.049961596727371216, -0.09992319345474243, 0.0, -0.049961596727371216, -0.09992319345474243, 0.049961596727371216, 0.09992319345474243]
[2025-05-27 23:19:54,069]: Mean: 0.00063970
[2025-05-27 23:19:54,069]: Min: -0.34973118
[2025-05-27 23:19:54,069]: Max: 0.39969277
[2025-05-27 23:19:54,069]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,070]: Sample Values (25 elements): [0.7883546948432922, 0.7073838114738464, 0.6684300899505615, 0.9317591190338135, 0.795074462890625, 0.7838124632835388, 0.6992064714431763, 0.7416681051254272, 0.8002946376800537, 0.7166070342063904, 0.7023223638534546, 0.905210018157959, 0.8780856728553772, 0.8019479513168335, 0.8261373043060303, 0.7679257988929749, 0.5146444439888, 0.9184287190437317, 0.8467696309089661, 0.6799672842025757, 0.7086241245269775, 0.7049883604049683, 0.7562990188598633, 0.6713235974311829, 0.8222271800041199]
[2025-05-27 23:19:54,070]: Mean: 0.77510244
[2025-05-27 23:19:54,070]: Min: 0.51464444
[2025-05-27 23:19:54,070]: Max: 0.97108799
[2025-05-27 23:19:54,071]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 23:19:54,071]: Sample Values (25 elements): [0.05248425900936127, -0.1574527770280838, 0.05248425900936127, 0.0, -0.10496851801872253, 0.0, -0.10496851801872253, -0.10496851801872253, 0.10496851801872253, 0.0, 0.10496851801872253, -0.05248425900936127, -0.10496851801872253, -0.10496851801872253, 0.0, -0.10496851801872253, 0.0, 0.05248425900936127, -0.05248425900936127, -0.1574527770280838, 0.0, 0.10496851801872253, 0.20993703603744507, 0.05248425900936127, 0.3149055540561676]
[2025-05-27 23:19:54,071]: Mean: 0.00292149
[2025-05-27 23:19:54,072]: Min: -0.36738980
[2025-05-27 23:19:54,072]: Max: 0.41987407
[2025-05-27 23:19:54,072]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,072]: Sample Values (25 elements): [0.4044421315193176, 0.45941436290740967, 0.6027922034263611, 0.5781317949295044, 0.5116778612136841, 0.7284967303276062, 0.5247259140014648, 0.47221216559410095, 0.4057075083255768, 0.5934829115867615, 0.42161455750465393, 0.6643372774124146, 0.42493051290512085, 0.5014469027519226, 0.5930007100105286, 0.5147568583488464, 0.5194451212882996, 0.5800130367279053, 0.41520968079566956, 0.456794798374176, 0.5228508114814758, 0.4984164237976074, 0.36170804500579834, 0.339314341545105, 0.6859971880912781]
[2025-05-27 23:19:54,072]: Mean: 0.51195645
[2025-05-27 23:19:54,072]: Min: 0.27283946
[2025-05-27 23:19:54,073]: Max: 0.80480933
[2025-05-27 23:19:54,074]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,074]: Sample Values (25 elements): [-0.05158015713095665, -0.05158015713095665, 0.05158015713095665, 0.05158015713095665, 0.0, -0.05158015713095665, 0.1031603142619133, -0.05158015713095665, -0.1031603142619133, -0.2063206285238266, 0.05158015713095665, 0.05158015713095665, 0.1031603142619133, 0.0, 0.05158015713095665, 0.05158015713095665, 0.05158015713095665, 0.1031603142619133, -0.05158015713095665, -0.05158015713095665, -0.15474046766757965, -0.05158015713095665, 0.0, 0.0, 0.05158015713095665]
[2025-05-27 23:19:54,075]: Mean: -0.00073598
[2025-05-27 23:19:54,075]: Min: -0.41264126
[2025-05-27 23:19:54,075]: Max: 0.36106110
[2025-05-27 23:19:54,075]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,075]: Sample Values (25 elements): [1.0804654359817505, 0.6318374276161194, 1.0155926942825317, 0.8192660212516785, 1.0705831050872803, 0.8164719343185425, 0.16418103873729706, 1.0483007431030273, 1.0030792951583862, 1.132487177848816, 0.9804920554161072, 0.9863764643669128, 0.8233485221862793, 1.1569116115570068, 0.9837790131568909, 1.087405800819397, 1.060770034790039, 1.0172396898269653, 0.8810496926307678, 0.44543299078941345, 1.093314290046692, 0.9981855154037476, 1.074906349182129, 0.7774664163589478, 1.1773655414581299]
[2025-05-27 23:19:54,075]: Mean: 0.93683958
[2025-05-27 23:19:54,076]: Min: 0.16418104
[2025-05-27 23:19:54,076]: Max: 1.22385383
[2025-05-27 23:19:54,077]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,077]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.04438335448503494, 0.0, -0.08876670897006989, 0.0, -0.08876670897006989, 0.04438335448503494, 0.04438335448503494, 0.0, -0.08876670897006989, -0.04438335448503494, -0.08876670897006989, -0.04438335448503494, 0.0, 0.0, -0.04438335448503494, 0.0, 0.0, -0.04438335448503494, -0.08876670897006989, -0.04438335448503494, 0.08876670897006989, 0.13315007090568542]
[2025-05-27 23:19:54,077]: Mean: -0.00002769
[2025-05-27 23:19:54,078]: Min: -0.35506684
[2025-05-27 23:19:54,078]: Max: 0.31068349
[2025-05-27 23:19:54,078]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,078]: Sample Values (25 elements): [0.9150764346122742, 0.8667334914207458, 0.7415573596954346, 1.0034372806549072, 0.8865050673484802, 0.8451539874076843, 0.8453237414360046, 0.7458606362342834, 0.68619304895401, 0.9333871603012085, 0.8522279262542725, 0.9410866498947144, 1.0869415998458862, 0.9895835518836975, 0.8770179152488708, 0.6870368123054504, 0.7851063013076782, 0.9629465937614441, 0.9354592561721802, 0.7890484929084778, 0.8943039774894714, 0.7790056467056274, 0.8828809261322021, 0.8183607459068298, 0.7947760820388794]
[2025-05-27 23:19:54,078]: Mean: 0.84251010
[2025-05-27 23:19:54,078]: Min: 0.51757538
[2025-05-27 23:19:54,079]: Max: 1.11055732
[2025-05-27 23:19:54,080]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,080]: Sample Values (25 elements): [0.0, 0.0432431623339653, 0.0432431623339653, -0.0432431623339653, -0.0864863246679306, 0.0, 0.0432431623339653, -0.0432431623339653, 0.0432431623339653, 0.0, 0.0, -0.0432431623339653, 0.0864863246679306, -0.0864863246679306, 0.1297294795513153, 0.0432431623339653, 0.0432431623339653, -0.1729726493358612, -0.0864863246679306, 0.2162158191204071, 0.0432431623339653, -0.1297294795513153, 0.0864863246679306, -0.0432431623339653, 0.0]
[2025-05-27 23:19:54,081]: Mean: -0.00103228
[2025-05-27 23:19:54,081]: Min: -0.30270213
[2025-05-27 23:19:54,081]: Max: 0.34594530
[2025-05-27 23:19:54,081]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,081]: Sample Values (25 elements): [0.5841192603111267, 0.9577358365058899, 1.1978871822357178, 1.0794408321380615, 0.9942445755004883, 0.8073200583457947, 0.6832258105278015, 1.1155389547348022, 6.209714014808994e-41, 1.2905081510543823, 0.7319194674491882, 5.080547712256057e-41, 0.9910449385643005, 1.018579363822937, 0.7572420239448547, 0.9987689852714539, 0.9997023344039917, 0.5742692947387695, 1.0670888423919678, 0.8173263669013977, 1.15804922580719, 1.0335661172866821, 0.8502309322357178, 0.9623343348503113, 0.37332311272621155]
[2025-05-27 23:19:54,081]: Mean: 0.83970749
[2025-05-27 23:19:54,082]: Min: 0.00000000
[2025-05-27 23:19:54,082]: Max: 1.29050815
[2025-05-27 23:19:54,083]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 23:19:54,083]: Sample Values (25 elements): [-0.07696004956960678, 0.03848002478480339, 0.11544007062911987, 0.0, 0.03848002478480339, 0.0, 0.0, 0.0, 0.0, 0.07696004956960678, 0.0, 0.07696004956960678, 0.0, 0.0, 0.07696004956960678, -0.07696004956960678, -0.11544007062911987, -0.07696004956960678, -0.03848002478480339, -0.15392009913921356, 0.03848002478480339, 0.07696004956960678, 0.07696004956960678, 0.15392009913921356, -0.03848002478480339]
[2025-05-27 23:19:54,083]: Mean: -0.00058455
[2025-05-27 23:19:54,084]: Min: -0.30784020
[2025-05-27 23:19:54,084]: Max: 0.26936018
[2025-05-27 23:19:54,084]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 23:19:54,084]: Sample Values (25 elements): [1.0055742263793945, 1.0583033561706543, 1.0093821287155151, 0.9218530654907227, 1.0623267889022827, 1.0975815057754517, 0.9984928965568542, 1.015810489654541, 0.948951005935669, 0.9736665487289429, 1.1900631189346313, 1.0926525592803955, 0.9422924518585205, 1.0330703258514404, 0.9575133919715881, 1.0755020380020142, 1.1029974222183228, 1.2629973888397217, 0.9415900111198425, 1.138209581375122, 1.0206646919250488, 1.102783203125, 0.9917560815811157, 0.8761133551597595, 0.985011637210846]
[2025-05-27 23:19:54,084]: Mean: 1.01513863
[2025-05-27 23:19:54,084]: Min: 0.82803351
[2025-05-27 23:19:54,085]: Max: 1.26299739
[2025-05-27 23:19:54,085]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 23:19:54,085]: Sample Values (25 elements): [-0.344186395406723, -0.3131575584411621, 0.30628857016563416, 0.7052105665206909, 0.2880244851112366, -0.28712838888168335, -0.18181142210960388, -0.3256983160972595, 0.1652415692806244, -0.4332570731639862, 0.5540663599967957, -0.12367850542068481, 0.5025562047958374, -0.45604774355888367, 0.7971539497375488, -0.2657325863838196, 0.4372327923774719, -0.2819989323616028, 0.12590208649635315, -0.3948247730731964, -0.4716801941394806, 0.47795572876930237, -0.1658487617969513, -0.30977094173431396, -0.5038914680480957]
[2025-05-27 23:19:54,085]: Mean: -0.00667225
[2025-05-27 23:19:54,085]: Min: -1.02350676
[2025-05-27 23:19:54,086]: Max: 1.23685789
[2025-05-27 23:19:54,086]: 

[2025-05-28 01:29:32,449]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT
[2025-05-28 01:29:32,454]: 


QAT of ResNet20 with hardtanh down to 3 bits...
[2025-05-28 01:29:32,832]: [ResNet20_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-28 01:29:33,322]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-28 01:30:29,273]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.5186 Train Acc: 0.8192 Eval Loss: 0.7521 Eval Acc: 0.7667 (LR: 0.00100000)
[2025-05-28 01:31:23,579]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.5243 Train Acc: 0.8190 Eval Loss: 0.6567 Eval Acc: 0.7805 (LR: 0.00100000)
[2025-05-28 01:32:17,977]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.5257 Train Acc: 0.8177 Eval Loss: 0.7967 Eval Acc: 0.7464 (LR: 0.00100000)
[2025-05-28 01:33:12,368]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.5274 Train Acc: 0.8158 Eval Loss: 0.7942 Eval Acc: 0.7410 (LR: 0.00100000)
[2025-05-28 01:34:06,808]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.5264 Train Acc: 0.8182 Eval Loss: 0.6945 Eval Acc: 0.7718 (LR: 0.00100000)
[2025-05-28 01:35:00,966]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.5240 Train Acc: 0.8182 Eval Loss: 0.6518 Eval Acc: 0.7833 (LR: 0.00100000)
[2025-05-28 01:35:55,537]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.5237 Train Acc: 0.8187 Eval Loss: 0.5848 Eval Acc: 0.8054 (LR: 0.00100000)
[2025-05-28 01:36:50,366]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.5185 Train Acc: 0.8185 Eval Loss: 0.7315 Eval Acc: 0.7618 (LR: 0.00100000)
[2025-05-28 01:37:44,782]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.5170 Train Acc: 0.8207 Eval Loss: 0.7654 Eval Acc: 0.7572 (LR: 0.00100000)
[2025-05-28 01:38:39,115]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.5200 Train Acc: 0.8180 Eval Loss: 0.6104 Eval Acc: 0.7978 (LR: 0.00100000)
[2025-05-28 01:39:33,913]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.5094 Train Acc: 0.8228 Eval Loss: 0.6519 Eval Acc: 0.7828 (LR: 0.00100000)
[2025-05-28 01:40:28,134]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.5120 Train Acc: 0.8208 Eval Loss: 0.9313 Eval Acc: 0.7194 (LR: 0.00100000)
[2025-05-28 01:41:22,303]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.5167 Train Acc: 0.8190 Eval Loss: 0.7758 Eval Acc: 0.7453 (LR: 0.00010000)
[2025-05-28 01:42:16,315]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.4167 Train Acc: 0.8555 Eval Loss: 0.4549 Eval Acc: 0.8469 (LR: 0.00010000)
[2025-05-28 01:43:10,532]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.3935 Train Acc: 0.8632 Eval Loss: 0.4357 Eval Acc: 0.8537 (LR: 0.00010000)
[2025-05-28 01:44:04,496]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.3835 Train Acc: 0.8677 Eval Loss: 0.4682 Eval Acc: 0.8453 (LR: 0.00010000)
[2025-05-28 01:44:58,573]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.3790 Train Acc: 0.8685 Eval Loss: 0.4587 Eval Acc: 0.8467 (LR: 0.00010000)
[2025-05-28 01:45:52,691]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.3717 Train Acc: 0.8712 Eval Loss: 0.4834 Eval Acc: 0.8426 (LR: 0.00010000)
[2025-05-28 01:46:47,140]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.3732 Train Acc: 0.8697 Eval Loss: 0.4395 Eval Acc: 0.8517 (LR: 0.00010000)
[2025-05-28 01:47:41,891]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.3716 Train Acc: 0.8695 Eval Loss: 0.4363 Eval Acc: 0.8561 (LR: 0.00010000)
[2025-05-28 01:48:36,272]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.3681 Train Acc: 0.8723 Eval Loss: 0.4543 Eval Acc: 0.8505 (LR: 0.00001000)
[2025-05-28 01:49:30,512]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.3496 Train Acc: 0.8770 Eval Loss: 0.4200 Eval Acc: 0.8629 (LR: 0.00001000)
[2025-05-28 01:50:24,710]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.3465 Train Acc: 0.8798 Eval Loss: 0.4148 Eval Acc: 0.8611 (LR: 0.00001000)
[2025-05-28 01:51:19,054]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.3421 Train Acc: 0.8812 Eval Loss: 0.4211 Eval Acc: 0.8596 (LR: 0.00001000)
[2025-05-28 01:52:13,466]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.3437 Train Acc: 0.8794 Eval Loss: 0.4182 Eval Acc: 0.8586 (LR: 0.00001000)
[2025-05-28 01:53:07,852]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.3443 Train Acc: 0.8804 Eval Loss: 0.4157 Eval Acc: 0.8611 (LR: 0.00001000)
[2025-05-28 01:54:02,111]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.3460 Train Acc: 0.8793 Eval Loss: 0.4215 Eval Acc: 0.8595 (LR: 0.00001000)
[2025-05-28 01:54:56,468]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.3428 Train Acc: 0.8813 Eval Loss: 0.4176 Eval Acc: 0.8604 (LR: 0.00001000)
[2025-05-28 01:55:51,007]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.3411 Train Acc: 0.8810 Eval Loss: 0.4152 Eval Acc: 0.8630 (LR: 0.00000100)
[2025-05-28 01:56:45,221]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.3385 Train Acc: 0.8835 Eval Loss: 0.4125 Eval Acc: 0.8606 (LR: 0.00000100)
[2025-05-28 01:57:39,420]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.3388 Train Acc: 0.8821 Eval Loss: 0.4120 Eval Acc: 0.8620 (LR: 0.00000100)
[2025-05-28 01:58:33,666]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.3321 Train Acc: 0.8839 Eval Loss: 0.4138 Eval Acc: 0.8610 (LR: 0.00000100)
[2025-05-28 01:59:27,870]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.3423 Train Acc: 0.8804 Eval Loss: 0.4115 Eval Acc: 0.8629 (LR: 0.00000100)
[2025-05-28 02:00:22,388]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.3379 Train Acc: 0.8841 Eval Loss: 0.4155 Eval Acc: 0.8635 (LR: 0.00000100)
[2025-05-28 02:01:16,575]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.3359 Train Acc: 0.8833 Eval Loss: 0.4106 Eval Acc: 0.8629 (LR: 0.00000100)
[2025-05-28 02:02:10,565]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.3371 Train Acc: 0.8829 Eval Loss: 0.4158 Eval Acc: 0.8645 (LR: 0.00000100)
[2025-05-28 02:03:04,804]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.3385 Train Acc: 0.8830 Eval Loss: 0.4090 Eval Acc: 0.8670 (LR: 0.00000100)
[2025-05-28 02:03:59,011]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.3397 Train Acc: 0.8823 Eval Loss: 0.4125 Eval Acc: 0.8644 (LR: 0.00000100)
[2025-05-28 02:04:53,422]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.3370 Train Acc: 0.8825 Eval Loss: 0.4176 Eval Acc: 0.8615 (LR: 0.00000100)
[2025-05-28 02:05:48,125]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.3362 Train Acc: 0.8813 Eval Loss: 0.4119 Eval Acc: 0.8649 (LR: 0.00000100)
[2025-05-28 02:06:44,303]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.3369 Train Acc: 0.8824 Eval Loss: 0.4136 Eval Acc: 0.8622 (LR: 0.00000100)
[2025-05-28 02:07:45,443]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.3336 Train Acc: 0.8835 Eval Loss: 0.4116 Eval Acc: 0.8620 (LR: 0.00000100)
[2025-05-28 02:08:46,057]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.3304 Train Acc: 0.8856 Eval Loss: 0.4150 Eval Acc: 0.8631 (LR: 0.00000010)
[2025-05-28 02:09:47,063]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.3364 Train Acc: 0.8827 Eval Loss: 0.4112 Eval Acc: 0.8643 (LR: 0.00000010)
[2025-05-28 02:10:47,855]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.3355 Train Acc: 0.8842 Eval Loss: 0.4149 Eval Acc: 0.8622 (LR: 0.00000010)
[2025-05-28 02:11:48,886]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.3333 Train Acc: 0.8846 Eval Loss: 0.4103 Eval Acc: 0.8645 (LR: 0.00000010)
[2025-05-28 02:12:50,156]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.3298 Train Acc: 0.8859 Eval Loss: 0.4116 Eval Acc: 0.8639 (LR: 0.00000010)
[2025-05-28 02:13:51,136]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.3348 Train Acc: 0.8831 Eval Loss: 0.4131 Eval Acc: 0.8626 (LR: 0.00000010)
[2025-05-28 02:14:51,941]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.3325 Train Acc: 0.8865 Eval Loss: 0.4105 Eval Acc: 0.8613 (LR: 0.00000010)
[2025-05-28 02:15:52,775]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.3320 Train Acc: 0.8838 Eval Loss: 0.4174 Eval Acc: 0.8606 (LR: 0.00000010)
[2025-05-28 02:16:53,910]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.3341 Train Acc: 0.8826 Eval Loss: 0.4068 Eval Acc: 0.8619 (LR: 0.00000010)
[2025-05-28 02:17:54,848]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.3339 Train Acc: 0.8822 Eval Loss: 0.4123 Eval Acc: 0.8625 (LR: 0.00000010)
[2025-05-28 02:18:55,808]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.3335 Train Acc: 0.8845 Eval Loss: 0.4119 Eval Acc: 0.8639 (LR: 0.00000010)
[2025-05-28 02:19:57,658]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.3389 Train Acc: 0.8841 Eval Loss: 0.4118 Eval Acc: 0.8643 (LR: 0.00000010)
[2025-05-28 02:20:58,656]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.3324 Train Acc: 0.8848 Eval Loss: 0.4144 Eval Acc: 0.8623 (LR: 0.00000010)
[2025-05-28 02:21:59,445]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.3335 Train Acc: 0.8849 Eval Loss: 0.4120 Eval Acc: 0.8612 (LR: 0.00000010)
[2025-05-28 02:23:00,288]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.3361 Train Acc: 0.8835 Eval Loss: 0.4143 Eval Acc: 0.8618 (LR: 0.00000010)
[2025-05-28 02:24:00,258]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.3312 Train Acc: 0.8849 Eval Loss: 0.4129 Eval Acc: 0.8641 (LR: 0.00000010)
[2025-05-28 02:25:01,909]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.3341 Train Acc: 0.8848 Eval Loss: 0.4135 Eval Acc: 0.8627 (LR: 0.00000010)
[2025-05-28 02:25:55,011]: [ResNet20_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.3337 Train Acc: 0.8828 Eval Loss: 0.4126 Eval Acc: 0.8624 (LR: 0.00000010)
[2025-05-28 02:25:55,011]: [ResNet20_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8670
[2025-05-28 02:25:55,070]: 


Quantization of model down to 3 bits finished
[2025-05-28 02:25:55,070]: Model Architecture:
[2025-05-28 02:25:55,123]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2101], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6697229146957397, max_val=0.8009130954742432)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1591], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5649782419204712, max_val=0.548621416091919)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1514], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5365430116653442, max_val=0.5231335163116455)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1668], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5449584722518921, max_val=0.6225415468215942)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1586], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6295100450515747, max_val=0.48066049814224243)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1628], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49039554595947266, max_val=0.6495349407196045)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1394], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5005074739456177, max_val=0.4749515652656555)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1319], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4748920798301697, max_val=0.44870781898498535)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1490], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5215785503387451, max_val=0.5215318202972412)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1200], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4360434412956238, max_val=0.40374115109443665)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1134], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39324140548706055, max_val=0.4005874991416931)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1169], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4093770384788513, max_val=0.40917539596557617)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1175], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4111122488975525, max_val=0.41119325160980225)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1175], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41224926710128784, max_val=0.41050612926483154)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1143], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3591172695159912, max_val=0.4410126805305481)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1296], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.453443706035614, max_val=0.4534432888031006)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1253], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4578039050102234, max_val=0.41957688331604004)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1052], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3721088767051697, max_val=0.3644782304763794)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1062], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3591262102127075, max_val=0.3845975995063782)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0784], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27172499895095825, max_val=0.2767793536186218)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=6, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3333], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-28 02:25:55,124]: 
Model Weights:
[2025-05-28 02:25:55,124]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-28 02:25:55,127]: Sample Values (25 elements): [0.34370991587638855, 0.17263725399971008, 0.2056685835123062, 0.0832485482096672, -0.3077431321144104, 0.1863238662481308, 0.06030620262026787, -0.17406433820724487, 0.3669530153274536, 0.2661249339580536, -0.02617013268172741, 9.163453069049865e-05, 0.08499139547348022, 0.20026196539402008, -0.15182311832904816, 0.17423281073570251, -0.10659005492925644, -0.1298324018716812, 0.21712344884872437, -0.04748096689581871, -0.18822839856147766, -0.05846583843231201, 0.20426760613918304, 0.3194216191768646, 0.03240438550710678]
[2025-05-28 02:25:55,137]: Mean: 0.00038215
[2025-05-28 02:25:55,137]: Min: -0.62313497
[2025-05-28 02:25:55,137]: Max: 0.40120149
[2025-05-28 02:25:55,137]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,138]: Sample Values (16 elements): [0.8915883898735046, 0.6545968055725098, 0.9369418025016785, 0.6722307801246643, 0.7902864813804626, 0.9878193140029907, 0.7203459739685059, 0.9153258204460144, 0.8961492776870728, 1.421970009803772, 0.8635890483856201, 0.7504258751869202, 0.6750190854072571, 0.8029582500457764, 0.7350221872329712, 0.8355352878570557]
[2025-05-28 02:25:55,138]: Mean: 0.84686273
[2025-05-28 02:25:55,138]: Min: 0.65459681
[2025-05-28 02:25:55,138]: Max: 1.42197001
[2025-05-28 02:25:55,139]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,139]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.21009086072444916, -0.21009086072444916, 0.0, 0.0, 0.0, -0.21009086072444916, -0.21009086072444916, -0.21009086072444916, -0.21009086072444916, -0.21009086072444916, 0.0, -0.21009086072444916, 0.0, 0.21009086072444916, 0.0, 0.21009086072444916, 0.0, -0.21009086072444916, 0.21009086072444916, 0.0, 0.0, -0.21009086072444916]
[2025-05-28 02:25:55,140]: Mean: -0.00009119
[2025-05-28 02:25:55,140]: Min: -0.63027257
[2025-05-28 02:25:55,140]: Max: 0.84036344
[2025-05-28 02:25:55,140]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,140]: Sample Values (16 elements): [0.8176381587982178, 1.0438272953033447, 0.861684262752533, 1.0990748405456543, 0.9903313517570496, 0.696682870388031, 0.9815497994422913, 0.8288586735725403, 0.7058954238891602, 1.0938868522644043, 0.7675558924674988, 0.9912732243537903, 0.8663222193717957, 0.8667140603065491, 0.7745399475097656, 0.6258865594863892]
[2025-05-28 02:25:55,140]: Mean: 0.87573260
[2025-05-28 02:25:55,141]: Min: 0.62588656
[2025-05-28 02:25:55,141]: Max: 1.09907484
[2025-05-28 02:25:55,142]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,142]: Sample Values (25 elements): [0.15908567607402802, 0.0, -0.15908567607402802, 0.0, -0.15908567607402802, 0.0, 0.15908567607402802, 0.15908567607402802, 0.31817135214805603, -0.15908567607402802, -0.15908567607402802, 0.0, 0.0, 0.15908567607402802, 0.15908567607402802, 0.0, 0.0, -0.15908567607402802, -0.31817135214805603, 0.0, 0.15908567607402802, -0.15908567607402802, -0.15908567607402802, -0.15908567607402802, 0.0]
[2025-05-28 02:25:55,142]: Mean: 0.00317619
[2025-05-28 02:25:55,142]: Min: -0.63634270
[2025-05-28 02:25:55,142]: Max: 0.47725701
[2025-05-28 02:25:55,142]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,143]: Sample Values (16 elements): [0.6119037866592407, 0.6378180980682373, 0.5762261152267456, 0.6308867931365967, 0.7875370383262634, 0.4741102159023285, 0.9286661148071289, 0.7528268098831177, 0.8105657696723938, 0.8871376514434814, 0.6109964847564697, 0.9383946657180786, 0.7362233996391296, 0.7369725108146667, 0.881726086139679, 0.5814177989959717]
[2025-05-28 02:25:55,143]: Mean: 0.72396314
[2025-05-28 02:25:55,143]: Min: 0.47411022
[2025-05-28 02:25:55,143]: Max: 0.93839467
[2025-05-28 02:25:55,144]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,144]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.15138237178325653, 0.15138237178325653, 0.0, -0.15138237178325653, 0.15138237178325653, 0.0, 0.30276474356651306, -0.30276474356651306, 0.0, -0.15138237178325653, 0.0, 0.0, 0.15138237178325653, 0.15138237178325653, 0.0, 0.0, 0.0, -0.15138237178325653, 0.15138237178325653, 0.0, -0.15138237178325653]
[2025-05-28 02:25:55,145]: Mean: -0.00105127
[2025-05-28 02:25:55,145]: Min: -0.60552949
[2025-05-28 02:25:55,145]: Max: 0.45414710
[2025-05-28 02:25:55,145]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,145]: Sample Values (16 elements): [0.7479534149169922, 0.8732932209968567, 0.7734206914901733, 1.0138909816741943, 1.082748293876648, 0.764386773109436, 0.9832496643066406, 1.214929223060608, 0.6545184254646301, 0.8214617967605591, 0.8541505932807922, 0.8194344639778137, 1.1876871585845947, 1.0434012413024902, 0.970274031162262, 1.2960816621780396]
[2025-05-28 02:25:55,145]: Mean: 0.94380510
[2025-05-28 02:25:55,146]: Min: 0.65451843
[2025-05-28 02:25:55,146]: Max: 1.29608166
[2025-05-28 02:25:55,147]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,147]: Sample Values (25 elements): [-0.16678573191165924, -0.3335714638233185, 0.0, 0.16678573191165924, -0.3335714638233185, 0.0, 0.16678573191165924, 0.0, -0.16678573191165924, 0.16678573191165924, 0.16678573191165924, -0.16678573191165924, 0.16678573191165924, -0.16678573191165924, 0.16678573191165924, 0.0, 0.0, 0.0, -0.16678573191165924, 0.16678573191165924, 0.0, 0.3335714638233185, 0.0, 0.3335714638233185, -0.16678573191165924]
[2025-05-28 02:25:55,147]: Mean: -0.00419860
[2025-05-28 02:25:55,147]: Min: -0.50035721
[2025-05-28 02:25:55,147]: Max: 0.66714293
[2025-05-28 02:25:55,147]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,148]: Sample Values (16 elements): [0.8202404975891113, 0.5049985647201538, 0.7896280288696289, 0.810437023639679, 1.0269724130630493, 0.5417959094047546, 0.7269442677497864, 0.5555393695831299, 0.5462889075279236, 0.5965890288352966, 0.5517818331718445, 0.40444332361221313, 0.6508734822273254, 0.6445609331130981, 0.5329882502555847, 0.5943019390106201]
[2025-05-28 02:25:55,148]: Mean: 0.64364898
[2025-05-28 02:25:55,148]: Min: 0.40444332
[2025-05-28 02:25:55,148]: Max: 1.02697241
[2025-05-28 02:25:55,149]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,149]: Sample Values (25 elements): [-0.15859580039978027, -0.15859580039978027, -0.15859580039978027, 0.0, 0.15859580039978027, 0.0, -0.15859580039978027, 0.0, 0.15859580039978027, 0.15859580039978027, 0.0, 0.15859580039978027, 0.0, -0.15859580039978027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.15859580039978027, 0.0, 0.0, -0.4757874011993408]
[2025-05-28 02:25:55,149]: Mean: -0.00943039
[2025-05-28 02:25:55,150]: Min: -0.63438320
[2025-05-28 02:25:55,150]: Max: 0.47578740
[2025-05-28 02:25:55,150]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,150]: Sample Values (16 elements): [0.9059003591537476, 0.9484075307846069, 0.9359661340713501, 0.7397465705871582, 0.922544538974762, 0.9441477656364441, 0.8697326183319092, 0.8956549167633057, 1.0405128002166748, 0.8515046834945679, 0.9824320673942566, 0.8557999134063721, 0.9448953866958618, 1.1906695365905762, 1.1000394821166992, 0.8669593930244446]
[2025-05-28 02:25:55,150]: Mean: 0.93718207
[2025-05-28 02:25:55,150]: Min: 0.73974657
[2025-05-28 02:25:55,151]: Max: 1.19066954
[2025-05-28 02:25:55,152]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-28 02:25:55,152]: Sample Values (25 elements): [0.0, 0.0, 0.3256944417953491, 0.0, -0.16284722089767456, 0.16284722089767456, 0.0, -0.16284722089767456, 0.0, 0.0, -0.16284722089767456, 0.0, -0.16284722089767456, 0.0, 0.0, -0.16284722089767456, 0.0, -0.16284722089767456, -0.16284722089767456, 0.0, 0.16284722089767456, 0.16284722089767456, -0.16284722089767456, 0.0, -0.16284722089767456]
[2025-05-28 02:25:55,152]: Mean: -0.00212041
[2025-05-28 02:25:55,152]: Min: -0.48854166
[2025-05-28 02:25:55,152]: Max: 0.65138888
[2025-05-28 02:25:55,152]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-28 02:25:55,153]: Sample Values (16 elements): [0.6755640506744385, 0.5542540550231934, 0.7338504791259766, 0.6320100426673889, 0.5922788977622986, 0.6827216148376465, 0.7410493493080139, 0.5896729230880737, 0.6247391104698181, 0.8908321857452393, 0.6475778222084045, 0.6892907023429871, 0.9947946071624756, 0.639370858669281, 0.6839615702629089, 0.47836068272590637]
[2025-05-28 02:25:55,153]: Mean: 0.67814553
[2025-05-28 02:25:55,153]: Min: 0.47836068
[2025-05-28 02:25:55,153]: Max: 0.99479461
[2025-05-28 02:25:55,154]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-28 02:25:55,154]: Sample Values (25 elements): [0.0, -0.13935129344463348, -0.13935129344463348, 0.0, 0.13935129344463348, 0.27870258688926697, 0.13935129344463348, -0.27870258688926697, -0.13935129344463348, 0.0, -0.13935129344463348, 0.0, 0.0, -0.13935129344463348, 0.13935129344463348, 0.0, 0.0, -0.13935129344463348, -0.13935129344463348, 0.13935129344463348, 0.0, 0.13935129344463348, 0.0, -0.27870258688926697, 0.0]
[2025-05-28 02:25:55,154]: Mean: 0.00480835
[2025-05-28 02:25:55,155]: Min: -0.55740517
[2025-05-28 02:25:55,155]: Max: 0.41805387
[2025-05-28 02:25:55,155]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,155]: Sample Values (25 elements): [0.6715849041938782, 0.6181246638298035, 0.6380940675735474, 0.7468742728233337, 0.8042741417884827, 0.8353410959243774, 0.7797037363052368, 0.9028513431549072, 0.7298941612243652, 0.7493135333061218, 0.9508644938468933, 0.9113667011260986, 0.9806722402572632, 0.7165733575820923, 0.6352713704109192, 0.7714037299156189, 0.7542346119880676, 0.7321503162384033, 0.8074820637702942, 0.6598275303840637, 0.8476630449295044, 0.6454087495803833, 0.916163980960846, 0.739037036895752, 0.8192486763000488]
[2025-05-28 02:25:55,155]: Mean: 0.77109253
[2025-05-28 02:25:55,155]: Min: 0.61812466
[2025-05-28 02:25:55,155]: Max: 0.98067224
[2025-05-28 02:25:55,156]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,157]: Sample Values (25 elements): [-0.13194285333156586, 0.13194285333156586, -0.3958285450935364, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13194285333156586, 0.13194285333156586, 0.0, 0.0, -0.13194285333156586, 0.13194285333156586, 0.13194285333156586, 0.13194285333156586, 0.2638857066631317, -0.13194285333156586, -0.13194285333156586, 0.0, 0.0, 0.0, 0.13194285333156586, -0.13194285333156586, 0.13194285333156586]
[2025-05-28 02:25:55,157]: Mean: 0.00153189
[2025-05-28 02:25:55,157]: Min: -0.52777141
[2025-05-28 02:25:55,157]: Max: 0.39582855
[2025-05-28 02:25:55,157]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,157]: Sample Values (25 elements): [0.738884687423706, 0.8683633208274841, 0.6374307870864868, 0.7826436161994934, 0.7593419551849365, 0.6323198676109314, 0.619491696357727, 0.7433492541313171, 0.67511385679245, 0.6406503915786743, 0.909334123134613, 0.7149009704589844, 0.8253446817398071, 0.7070591449737549, 0.760409414768219, 0.8299437761306763, 0.6426486372947693, 0.8259216547012329, 0.6799277663230896, 0.673331618309021, 1.033713936805725, 0.8831403851509094, 0.6509718298912048, 0.7141711115837097, 0.6833220720291138]
[2025-05-28 02:25:55,158]: Mean: 0.74177849
[2025-05-28 02:25:55,158]: Min: 0.59243816
[2025-05-28 02:25:55,158]: Max: 1.03371394
[2025-05-28 02:25:55,159]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-28 02:25:55,159]: Sample Values (25 elements): [-0.14901576936244965, 0.14901576936244965, 0.0, -0.14901576936244965, 0.0, 0.0, -0.2980315387248993, -0.14901576936244965, -0.14901576936244965, 0.2980315387248993, -0.14901576936244965, -0.2980315387248993, 0.44704729318618774, 0.2980315387248993, 0.14901576936244965, 0.14901576936244965, 0.14901576936244965, -0.14901576936244965, -0.2980315387248993, -0.14901576936244965, 0.0, 0.14901576936244965, 0.14901576936244965, 0.14901576936244965, -0.14901576936244965]
[2025-05-28 02:25:55,159]: Mean: 0.00611197
[2025-05-28 02:25:55,159]: Min: -0.59606308
[2025-05-28 02:25:55,160]: Max: 0.44704729
[2025-05-28 02:25:55,160]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,160]: Sample Values (25 elements): [0.16969051957130432, 0.4756442606449127, 0.3448815941810608, 0.3855862319469452, 0.23456892371177673, 0.5569912195205688, 0.5487092733383179, 0.4995564818382263, 0.28240975737571716, 0.37574443221092224, 0.4244999587535858, 0.6287470459938049, 0.36421531438827515, 0.43853846192359924, 0.37955451011657715, 0.3939521908760071, 0.4116326570510864, 0.3482111990451813, 0.46458274126052856, 0.5620684027671814, 0.3337269425392151, 0.4061790704727173, 0.4655732214450836, 0.505101203918457, 0.434029757976532]
[2025-05-28 02:25:55,160]: Mean: 0.42778704
[2025-05-28 02:25:55,160]: Min: 0.16969052
[2025-05-28 02:25:55,160]: Max: 0.72930366
[2025-05-28 02:25:55,161]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,162]: Sample Values (25 elements): [0.0, 0.11996922641992569, -0.11996922641992569, 0.0, 0.0, -0.11996922641992569, 0.23993845283985138, 0.0, 0.0, 0.23993845283985138, 0.0, 0.0, 0.11996922641992569, 0.11996922641992569, 0.11996922641992569, 0.11996922641992569, -0.11996922641992569, 0.0, 0.11996922641992569, -0.23993845283985138, 0.11996922641992569, 0.11996922641992569, 0.0, 0.11996922641992569, -0.11996922641992569]
[2025-05-28 02:25:55,162]: Mean: 0.00177038
[2025-05-28 02:25:55,162]: Min: -0.47987691
[2025-05-28 02:25:55,162]: Max: 0.35990769
[2025-05-28 02:25:55,162]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,162]: Sample Values (25 elements): [0.7414290904998779, 0.7709028124809265, 0.7725483179092407, 0.8449530601501465, 0.616449773311615, 0.7529852986335754, 0.7428526878356934, 0.7558305263519287, 0.8335225582122803, 0.8114389181137085, 1.0263360738754272, 0.720385730266571, 0.8262020945549011, 0.9603818655014038, 0.6849712133407593, 0.8912324905395508, 1.0308833122253418, 0.8958356976509094, 0.8007423877716064, 0.809155285358429, 0.8432257771492004, 1.0421137809753418, 0.9008339643478394, 0.9137608408927917, 0.9869116544723511]
[2025-05-28 02:25:55,163]: Mean: 0.85922635
[2025-05-28 02:25:55,163]: Min: 0.61644977
[2025-05-28 02:25:55,163]: Max: 1.10569501
[2025-05-28 02:25:55,164]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,164]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11340412497520447, 0.11340412497520447, 0.0, 0.11340412497520447, -0.11340412497520447, 0.0, 0.0, 0.0, 0.11340412497520447, 0.11340412497520447, 0.11340412497520447, 0.0, 0.0, 0.0, -0.11340412497520447, 0.0, 0.0, 0.11340412497520447, 0.11340412497520447]
[2025-05-28 02:25:55,164]: Mean: 0.00051682
[2025-05-28 02:25:55,164]: Min: -0.34021237
[2025-05-28 02:25:55,165]: Max: 0.45361650
[2025-05-28 02:25:55,165]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,165]: Sample Values (25 elements): [0.7375550270080566, 0.560502827167511, 0.6952448487281799, 0.569843590259552, 0.527451753616333, 0.7165865898132324, 0.6981063485145569, 0.7479019165039062, 0.6249059438705444, 0.6784762144088745, 0.5198546051979065, 0.3292802572250366, 0.4815060794353485, 0.6760161519050598, 0.6936061382293701, 0.5625332593917847, 0.6223956942558289, 0.8161596655845642, 0.8413675427436829, 0.6564188599586487, 0.695729672908783, 0.6097182631492615, 0.735832691192627, 0.7635492086410522, 0.6755493879318237]
[2025-05-28 02:25:55,165]: Mean: 0.65745527
[2025-05-28 02:25:55,165]: Min: 0.32928026
[2025-05-28 02:25:55,165]: Max: 0.84136754
[2025-05-28 02:25:55,166]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,167]: Sample Values (25 elements): [0.0, 0.23387211561203003, 0.11693605780601501, -0.11693605780601501, -0.11693605780601501, 0.0, 0.0, -0.11693605780601501, 0.11693605780601501, 0.11693605780601501, 0.11693605780601501, 0.0, -0.11693605780601501, 0.23387211561203003, 0.11693605780601501, -0.11693605780601501, 0.0, 0.23387211561203003, -0.11693605780601501, -0.23387211561203003, -0.11693605780601501, -0.23387211561203003, 0.0, 0.0, 0.0]
[2025-05-28 02:25:55,167]: Mean: -0.00076130
[2025-05-28 02:25:55,167]: Min: -0.46774423
[2025-05-28 02:25:55,167]: Max: 0.35080817
[2025-05-28 02:25:55,167]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,167]: Sample Values (25 elements): [1.1849414110183716, 0.8634557723999023, 1.017977237701416, 1.0438498258590698, 1.1924022436141968, 0.9874029159545898, 0.8410245180130005, 1.1872915029525757, 0.8089337944984436, 0.6689110994338989, 0.8595011234283447, 0.8178206086158752, 1.012863039970398, 0.934752881526947, 1.1199678182601929, 0.8997020721435547, 0.8510603904724121, 0.9535173773765564, 0.9474779367446899, 1.0199027061462402, 1.1285815238952637, 1.0615979433059692, 0.8441683650016785, 0.8550753593444824, 0.9800628423690796]
[2025-05-28 02:25:55,167]: Mean: 0.97372025
[2025-05-28 02:25:55,168]: Min: 0.66891110
[2025-05-28 02:25:55,168]: Max: 1.23818147
[2025-05-28 02:25:55,169]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-28 02:25:55,169]: Sample Values (25 elements): [0.0, 0.0, -0.11747221648693085, 0.0, 0.0, 0.0, -0.11747221648693085, 0.0, 0.11747221648693085, 0.0, 0.0, -0.11747221648693085, -0.11747221648693085, -0.2349444329738617, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11747221648693085, 0.11747221648693085, -0.11747221648693085, 0.0, 0.0, 0.0]
[2025-05-28 02:25:55,169]: Mean: -0.00224339
[2025-05-28 02:25:55,169]: Min: -0.35241663
[2025-05-28 02:25:55,170]: Max: 0.46988887
[2025-05-28 02:25:55,170]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-28 02:25:55,170]: Sample Values (25 elements): [0.8575956225395203, 0.6526444554328918, 0.6931635737419128, 0.6915566921234131, 0.9363316297531128, 0.5908023118972778, 0.7786667346954346, 0.8015625476837158, 0.7710341215133667, 0.7877042889595032, 0.6194020509719849, 0.6301397085189819, 0.6293533444404602, 0.6140267252922058, 0.627839207649231, 0.6654895544052124, 0.7506932616233826, 0.6863690614700317, 0.5953887104988098, 0.5656394362449646, 0.7173781394958496, 0.7457585334777832, 0.8060193657875061, 0.528866708278656, 0.6853885054588318]
[2025-05-28 02:25:55,170]: Mean: 0.69510823
[2025-05-28 02:25:55,170]: Min: 0.52886671
[2025-05-28 02:25:55,170]: Max: 0.93633163
[2025-05-28 02:25:55,171]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-28 02:25:55,172]: Sample Values (25 elements): [-0.11753649264574051, -0.11753649264574051, 0.0, 0.0, 0.0, 0.0, 0.11753649264574051, 0.0, 0.0, -0.11753649264574051, 0.0, 0.0, 0.11753649264574051, -0.11753649264574051, 0.0, -0.11753649264574051, 0.0, -0.11753649264574051, 0.11753649264574051, -0.11753649264574051, 0.0, -0.11753649264574051, -0.11753649264574051, 0.0, 0.11753649264574051]
[2025-05-28 02:25:55,172]: Mean: -0.00265273
[2025-05-28 02:25:55,172]: Min: -0.47014597
[2025-05-28 02:25:55,172]: Max: 0.35260949
[2025-05-28 02:25:55,172]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,173]: Sample Values (25 elements): [0.9624865651130676, 0.5734809637069702, 0.9499053359031677, 1.0155253410339355, 0.8466586470603943, 0.501490592956543, 0.8444669246673584, 0.696139395236969, 0.7943884134292603, 0.9215104579925537, 0.8017493486404419, 0.8844654560089111, 0.8418026566505432, 0.6337157487869263, 1.046267032623291, 0.7903183102607727, 0.7215834259986877, 0.6641829013824463, 0.7388055324554443, 0.6468116044998169, 0.949694037437439, 0.8856381177902222, 0.8403341770172119, 0.8547196388244629, 0.9926234483718872]
[2025-05-28 02:25:55,173]: Mean: 0.78026688
[2025-05-28 02:25:55,173]: Min: 0.03086193
[2025-05-28 02:25:55,173]: Max: 1.04626703
[2025-05-28 02:25:55,174]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,175]: Sample Values (25 elements): [-0.22860854864120483, 0.0, 0.11430427432060242, -0.11430427432060242, 0.0, 0.0, 0.0, 0.0, -0.11430427432060242, 0.0, 0.0, 0.11430427432060242, -0.11430427432060242, -0.11430427432060242, 0.0, 0.0, 0.11430427432060242, 0.0, -0.11430427432060242, 0.11430427432060242, 0.0, 0.0, 0.0, 0.0, -0.11430427432060242]
[2025-05-28 02:25:55,175]: Mean: 0.00078138
[2025-05-28 02:25:55,175]: Min: -0.34291282
[2025-05-28 02:25:55,175]: Max: 0.45721710
[2025-05-28 02:25:55,175]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,175]: Sample Values (25 elements): [0.888995349407196, 0.7995696663856506, 0.7765844464302063, 0.7600758075714111, 0.5325523018836975, 0.7105078101158142, 0.7978779673576355, 0.7861396074295044, 0.7247591018676758, 0.718950092792511, 0.9695340394973755, 0.6795875430107117, 0.7829974293708801, 0.8956905007362366, 0.6893831491470337, 0.5480595827102661, 0.8487086892127991, 0.6883662343025208, 0.7975068688392639, 0.7804485559463501, 0.9070018529891968, 0.6935685276985168, 0.7908574342727661, 0.7159339189529419, 0.8925947546958923]
[2025-05-28 02:25:55,176]: Mean: 0.78231573
[2025-05-28 02:25:55,176]: Min: 0.53255230
[2025-05-28 02:25:55,176]: Max: 0.99538207
[2025-05-28 02:25:55,177]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-28 02:25:55,177]: Sample Values (25 elements): [-0.12955528497695923, 0.12955528497695923, 0.0, 0.0, 0.25911056995391846, 0.0, 0.0, 0.12955528497695923, 0.0, 0.0, 0.0, 0.0, -0.12955528497695923, 0.25911056995391846, 0.12955528497695923, -0.12955528497695923, -0.25911056995391846, 0.0, 0.12955528497695923, 0.0, 0.0, -0.12955528497695923, 0.0, -0.12955528497695923, 0.0]
[2025-05-28 02:25:55,177]: Mean: 0.00145497
[2025-05-28 02:25:55,177]: Min: -0.38866585
[2025-05-28 02:25:55,178]: Max: 0.38866585
[2025-05-28 02:25:55,178]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,178]: Sample Values (25 elements): [0.4765385389328003, 0.39690035581588745, 0.46569254994392395, 0.41462570428848267, 0.479436993598938, 0.3787315785884857, 0.42883747816085815, 0.7115005850791931, 0.43111175298690796, 0.4306192100048065, 0.6373619437217712, 0.5293716788291931, 0.5551925897598267, 0.7558866143226624, 0.486683189868927, 0.4413706958293915, 0.4907870292663574, 0.5340591669082642, 0.4255567491054535, 0.5430988669395447, 0.5235261917114258, 0.6796747446060181, 0.3630857467651367, 0.5076599717140198, 0.62054044008255]
[2025-05-28 02:25:55,178]: Mean: 0.48930529
[2025-05-28 02:25:55,178]: Min: 0.24191412
[2025-05-28 02:25:55,178]: Max: 0.75588661
[2025-05-28 02:25:55,179]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,180]: Sample Values (25 elements): [0.0, 0.0, -0.12534011900424957, 0.0, 0.0, 0.0, 0.0, -0.12534011900424957, 0.0, 0.0, 0.0, 0.0, -0.12534011900424957, 0.0, 0.0, 0.0, 0.12534011900424957, -0.12534011900424957, 0.12534011900424957, 0.0, -0.12534011900424957, 0.0, 0.12534011900424957, 0.0, -0.12534011900424957]
[2025-05-28 02:25:55,180]: Mean: -0.00062221
[2025-05-28 02:25:55,180]: Min: -0.50136048
[2025-05-28 02:25:55,181]: Max: 0.37602037
[2025-05-28 02:25:55,181]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,181]: Sample Values (25 elements): [1.0593513250350952, 0.6552159786224365, 0.8491916060447693, 0.7790395617485046, 0.931839644908905, 0.8494516611099243, 1.0417896509170532, 0.9749351143836975, 0.9808566570281982, 0.9611501097679138, 0.850631594657898, 0.9375057816505432, 0.18294458091259003, 0.7249884009361267, 1.0827621221542358, 1.0396740436553955, 1.0800104141235352, 0.7824307680130005, 0.9322637915611267, 0.8078113198280334, 0.8082084059715271, 0.8556857109069824, 0.8572257161140442, 0.9928857684135437, 1.0725829601287842]
[2025-05-28 02:25:55,181]: Mean: 0.92450035
[2025-05-28 02:25:55,181]: Min: 0.18294458
[2025-05-28 02:25:55,181]: Max: 1.26355469
[2025-05-28 02:25:55,182]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,183]: Sample Values (25 elements): [0.10522673279047012, 0.0, -0.10522673279047012, 0.10522673279047012, -0.10522673279047012, 0.0, 0.0, 0.0, 0.0, -0.10522673279047012, 0.0, -0.10522673279047012, 0.0, 0.10522673279047012, -0.10522673279047012, -0.10522673279047012, -0.10522673279047012, -0.21045346558094025, -0.10522673279047012, 0.0, 0.10522673279047012, 0.21045346558094025, 0.10522673279047012, 0.0, 0.0]
[2025-05-28 02:25:55,183]: Mean: -0.00003711
[2025-05-28 02:25:55,183]: Min: -0.42090693
[2025-05-28 02:25:55,183]: Max: 0.31568021
[2025-05-28 02:25:55,183]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,184]: Sample Values (25 elements): [0.8212926983833313, 0.7827681303024292, 0.7129367589950562, 0.9520220756530762, 0.7122600674629211, 0.76169753074646, 1.1493176221847534, 0.8803352117538452, 0.5816574096679688, 0.9595238566398621, 0.9569537043571472, 0.870025098323822, 0.7614462375640869, 0.8515357375144958, 0.942300021648407, 0.7436751127243042, 0.969732940196991, 0.8419330716133118, 0.8790997862815857, 1.0611913204193115, 0.9287850260734558, 0.48612725734710693, 0.9236570596694946, 0.8374102115631104, 0.9967639446258545]
[2025-05-28 02:25:55,184]: Mean: 0.84833229
[2025-05-28 02:25:55,184]: Min: 0.48612726
[2025-05-28 02:25:55,184]: Max: 1.14931762
[2025-05-28 02:25:55,185]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,186]: Sample Values (25 elements): [0.0, -0.10624626278877258, 0.0, 0.10624626278877258, -0.10624626278877258, 0.0, -0.10624626278877258, -0.10624626278877258, 0.0, 0.10624626278877258, 0.0, -0.10624626278877258, 0.0, 0.0, 0.10624626278877258, 0.10624626278877258, -0.10624626278877258, 0.0, 0.0, 0.10624626278877258, -0.10624626278877258, 0.0, 0.0, 0.0, -0.10624626278877258]
[2025-05-28 02:25:55,186]: Mean: -0.00101450
[2025-05-28 02:25:55,186]: Min: -0.31873879
[2025-05-28 02:25:55,186]: Max: 0.42498505
[2025-05-28 02:25:55,186]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,186]: Sample Values (25 elements): [1.1327742338180542, 1.2138915061950684, 1.0868273973464966, 0.8810564279556274, 0.7926161885261536, 5.080547712256057e-41, 1.1502816677093506, 1.144344449043274, 0.0022012919653207064, 0.8081487417221069, 0.8236158490180969, 1.0470889806747437, 0.7267525792121887, 0.6352288126945496, 5.993914051302973e-41, 0.8316540122032166, 1.1178882122039795, 0.5895190238952637, 0.776425838470459, 0.8358080983161926, 1.0093201398849487, 1.0910314321517944, 0.8837454915046692, 1.065826654434204, 1.002544641494751]
[2025-05-28 02:25:55,186]: Mean: 0.85025811
[2025-05-28 02:25:55,187]: Min: 0.00000000
[2025-05-28 02:25:55,187]: Max: 1.33133101
[2025-05-28 02:25:55,188]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-28 02:25:55,188]: Sample Values (25 elements): [0.0, 0.0, 0.0783577710390091, 0.0, -0.0783577710390091, 0.0, 0.0, -0.0783577710390091, 0.0, -0.0783577710390091, 0.0783577710390091, 0.0783577710390091, 0.0783577710390091, 0.0, 0.0, 0.0783577710390091, -0.0783577710390091, 0.0, 0.0, 0.0783577710390091, -0.0783577710390091, 0.0, 0.0, 0.0, 0.0]
[2025-05-28 02:25:55,189]: Mean: -0.00082260
[2025-05-28 02:25:55,189]: Min: -0.23507331
[2025-05-28 02:25:55,189]: Max: 0.31343108
[2025-05-28 02:25:55,189]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-28 02:25:55,190]: Sample Values (25 elements): [1.1113182306289673, 0.9165340662002563, 0.9587891697883606, 1.0070017576217651, 1.0005155801773071, 0.9557517170906067, 1.0123579502105713, 0.9278162717819214, 1.065739393234253, 0.8902961611747742, 1.0196563005447388, 0.9145594239234924, 1.0145611763000488, 1.05124032497406, 0.9783657193183899, 0.9154394865036011, 0.9328198432922363, 0.8857706785202026, 1.0707321166992188, 1.0104278326034546, 1.0870904922485352, 1.0048884153366089, 1.188826322555542, 0.8093259930610657, 1.2791123390197754]
[2025-05-28 02:25:55,190]: Mean: 0.99818385
[2025-05-28 02:25:55,190]: Min: 0.80932599
[2025-05-28 02:25:55,190]: Max: 1.27911234
[2025-05-28 02:25:55,190]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-28 02:25:55,191]: Sample Values (25 elements): [0.644254744052887, -0.4218088686466217, 0.23435530066490173, 0.08130688965320587, 0.19336001574993134, 0.40748897194862366, 0.638317346572876, -0.2617209851741791, 0.2630358338356018, -0.4138370752334595, -0.2356204241514206, -0.01589428074657917, 0.36018654704093933, 0.3097194731235504, -0.18278810381889343, -0.18749020993709564, 0.09316593408584595, -0.16930772364139557, -0.5754337906837463, 0.43946129083633423, -0.32419711351394653, 0.6749430298805237, -0.2676546275615692, 0.23565654456615448, 0.1459595113992691]
[2025-05-28 02:25:55,191]: Mean: -0.00658744
[2025-05-28 02:25:55,191]: Min: -1.00892770
[2025-05-28 02:25:55,191]: Max: 1.18766701
[2025-06-14 03:53:54,754]: 


QAT of ResNet20 with hardtanh down to 2 bits...
[2025-06-14 03:53:55,148]: [ResNet20_hardtanh_quantized_2_bits] after configure_qat:
[2025-06-14 03:53:55,325]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-14 03:55:20,245]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.0662 Train Acc: 0.6265 Eval Loss: 1.0626 Eval Acc: 0.6473 (LR: 0.00100000)
[2025-06-14 03:56:40,840]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.8718 Train Acc: 0.6940 Eval Loss: 1.0411 Eval Acc: 0.6489 (LR: 0.00100000)
[2025-06-14 03:57:57,378]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.8380 Train Acc: 0.7055 Eval Loss: 0.9533 Eval Acc: 0.6730 (LR: 0.00100000)
[2025-06-14 03:59:20,162]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.8245 Train Acc: 0.7120 Eval Loss: 0.8649 Eval Acc: 0.7011 (LR: 0.00100000)
[2025-06-14 04:00:40,183]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.8191 Train Acc: 0.7126 Eval Loss: 0.9384 Eval Acc: 0.6823 (LR: 0.00100000)
[2025-06-14 04:01:54,461]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.8141 Train Acc: 0.7141 Eval Loss: 1.0203 Eval Acc: 0.6592 (LR: 0.00100000)
[2025-06-14 04:03:05,572]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.8105 Train Acc: 0.7164 Eval Loss: 0.9706 Eval Acc: 0.6808 (LR: 0.00100000)
[2025-06-14 04:04:11,654]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.7989 Train Acc: 0.7203 Eval Loss: 0.8485 Eval Acc: 0.7079 (LR: 0.00100000)
[2025-06-14 04:05:19,255]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.7941 Train Acc: 0.7227 Eval Loss: 0.9924 Eval Acc: 0.6839 (LR: 0.00100000)
[2025-06-14 04:06:29,728]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.7900 Train Acc: 0.7226 Eval Loss: 0.7572 Eval Acc: 0.7401 (LR: 0.00100000)
[2025-06-14 04:07:37,325]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.7772 Train Acc: 0.7285 Eval Loss: 0.8811 Eval Acc: 0.6961 (LR: 0.00100000)
[2025-06-14 04:08:50,648]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.7750 Train Acc: 0.7285 Eval Loss: 0.8537 Eval Acc: 0.7032 (LR: 0.00100000)
[2025-06-14 04:10:05,401]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.7725 Train Acc: 0.7278 Eval Loss: 0.9448 Eval Acc: 0.6818 (LR: 0.00100000)
[2025-06-14 04:11:16,814]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.7633 Train Acc: 0.7325 Eval Loss: 0.8327 Eval Acc: 0.7194 (LR: 0.00100000)
[2025-06-14 04:12:29,963]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.7653 Train Acc: 0.7333 Eval Loss: 0.9237 Eval Acc: 0.6932 (LR: 0.00100000)
[2025-06-14 04:13:46,895]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.7649 Train Acc: 0.7304 Eval Loss: 0.9575 Eval Acc: 0.6860 (LR: 0.00010000)
[2025-06-14 04:15:04,700]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.6607 Train Acc: 0.7704 Eval Loss: 0.6266 Eval Acc: 0.7871 (LR: 0.00010000)
[2025-06-14 04:16:21,926]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.6454 Train Acc: 0.7756 Eval Loss: 0.6773 Eval Acc: 0.7706 (LR: 0.00010000)
[2025-06-14 04:17:38,258]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.6301 Train Acc: 0.7811 Eval Loss: 0.6636 Eval Acc: 0.7774 (LR: 0.00010000)
[2025-06-14 04:18:55,348]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.6299 Train Acc: 0.7786 Eval Loss: 0.6871 Eval Acc: 0.7660 (LR: 0.00010000)
[2025-06-14 04:20:11,839]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.6278 Train Acc: 0.7823 Eval Loss: 0.7418 Eval Acc: 0.7578 (LR: 0.00010000)
[2025-06-14 04:21:22,420]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.6289 Train Acc: 0.7817 Eval Loss: 0.6322 Eval Acc: 0.7837 (LR: 0.00010000)
[2025-06-14 04:22:32,664]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.6291 Train Acc: 0.7805 Eval Loss: 0.6568 Eval Acc: 0.7740 (LR: 0.00001000)
[2025-06-14 04:23:39,831]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.5992 Train Acc: 0.7923 Eval Loss: 0.6011 Eval Acc: 0.7900 (LR: 0.00001000)
[2025-06-14 04:24:45,851]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.5926 Train Acc: 0.7941 Eval Loss: 0.5909 Eval Acc: 0.7940 (LR: 0.00001000)
[2025-06-14 04:25:53,468]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.5940 Train Acc: 0.7920 Eval Loss: 0.5923 Eval Acc: 0.7963 (LR: 0.00001000)
[2025-06-14 04:27:00,589]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.5901 Train Acc: 0.7929 Eval Loss: 0.6079 Eval Acc: 0.7920 (LR: 0.00001000)
[2025-06-14 04:28:10,617]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.5857 Train Acc: 0.7941 Eval Loss: 0.5913 Eval Acc: 0.7968 (LR: 0.00001000)
[2025-06-14 04:29:23,562]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.5913 Train Acc: 0.7922 Eval Loss: 0.6031 Eval Acc: 0.7924 (LR: 0.00001000)
[2025-06-14 04:30:36,218]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.5930 Train Acc: 0.7940 Eval Loss: 0.6241 Eval Acc: 0.7887 (LR: 0.00001000)
[2025-06-14 04:31:50,908]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.5927 Train Acc: 0.7939 Eval Loss: 0.5912 Eval Acc: 0.7943 (LR: 0.00000100)
[2025-06-14 04:33:07,484]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.5821 Train Acc: 0.7980 Eval Loss: 0.5833 Eval Acc: 0.8008 (LR: 0.00000100)
[2025-06-14 04:34:25,467]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.5795 Train Acc: 0.7972 Eval Loss: 0.5889 Eval Acc: 0.7965 (LR: 0.00000100)
[2025-06-14 04:35:43,244]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.5792 Train Acc: 0.7983 Eval Loss: 0.5835 Eval Acc: 0.7994 (LR: 0.00000100)
[2025-06-14 04:36:57,564]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.5809 Train Acc: 0.7978 Eval Loss: 0.5790 Eval Acc: 0.8028 (LR: 0.00000100)
[2025-06-14 04:38:17,918]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.5802 Train Acc: 0.7984 Eval Loss: 0.5809 Eval Acc: 0.7981 (LR: 0.00000100)
[2025-06-14 04:39:33,325]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.5794 Train Acc: 0.7987 Eval Loss: 0.5767 Eval Acc: 0.8027 (LR: 0.00000100)
[2025-06-14 04:40:53,675]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.5827 Train Acc: 0.7961 Eval Loss: 0.5906 Eval Acc: 0.7937 (LR: 0.00000100)
[2025-06-14 04:42:03,291]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.5766 Train Acc: 0.7998 Eval Loss: 0.5993 Eval Acc: 0.7936 (LR: 0.00000100)
[2025-06-14 04:43:16,320]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.5785 Train Acc: 0.7992 Eval Loss: 0.5922 Eval Acc: 0.7974 (LR: 0.00000100)
[2025-06-14 04:44:30,106]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.5754 Train Acc: 0.8002 Eval Loss: 0.5770 Eval Acc: 0.8026 (LR: 0.00000100)
[2025-06-14 04:45:42,455]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.5835 Train Acc: 0.7985 Eval Loss: 0.5711 Eval Acc: 0.8031 (LR: 0.00000100)
[2025-06-14 04:46:54,400]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.5757 Train Acc: 0.8000 Eval Loss: 0.5781 Eval Acc: 0.8004 (LR: 0.00000100)
[2025-06-14 04:48:09,370]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.5776 Train Acc: 0.7992 Eval Loss: 0.5911 Eval Acc: 0.7959 (LR: 0.00000100)
[2025-06-14 04:49:26,025]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.5803 Train Acc: 0.7997 Eval Loss: 0.5986 Eval Acc: 0.7928 (LR: 0.00000100)
[2025-06-14 04:50:44,554]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.5820 Train Acc: 0.7962 Eval Loss: 0.5903 Eval Acc: 0.7986 (LR: 0.00000100)
[2025-06-14 04:52:05,218]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.5788 Train Acc: 0.7986 Eval Loss: 0.5728 Eval Acc: 0.8042 (LR: 0.00000100)
[2025-06-14 04:53:25,936]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.5816 Train Acc: 0.7967 Eval Loss: 0.5931 Eval Acc: 0.7977 (LR: 0.00000010)
[2025-06-14 04:54:50,305]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.5749 Train Acc: 0.7992 Eval Loss: 0.5816 Eval Acc: 0.8026 (LR: 0.00000010)
[2025-06-14 04:56:10,044]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.5777 Train Acc: 0.8004 Eval Loss: 0.5747 Eval Acc: 0.7990 (LR: 0.00000010)
[2025-06-14 04:57:27,758]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.5675 Train Acc: 0.8021 Eval Loss: 0.5874 Eval Acc: 0.7995 (LR: 0.00000010)
[2025-06-14 04:58:46,018]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.5756 Train Acc: 0.8005 Eval Loss: 0.5782 Eval Acc: 0.8035 (LR: 0.00000010)
[2025-06-14 05:00:00,867]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.5743 Train Acc: 0.8001 Eval Loss: 0.5829 Eval Acc: 0.7974 (LR: 0.00000010)
[2025-06-14 05:01:12,531]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.5728 Train Acc: 0.7987 Eval Loss: 0.5755 Eval Acc: 0.8026 (LR: 0.00000010)
[2025-06-14 05:02:24,833]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.5802 Train Acc: 0.7994 Eval Loss: 0.5786 Eval Acc: 0.8010 (LR: 0.00000010)
[2025-06-14 05:03:36,314]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.5766 Train Acc: 0.7985 Eval Loss: 0.5762 Eval Acc: 0.8023 (LR: 0.00000010)
[2025-06-14 05:04:46,941]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.5758 Train Acc: 0.7983 Eval Loss: 0.5817 Eval Acc: 0.8008 (LR: 0.00000010)
[2025-06-14 05:05:58,108]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.5756 Train Acc: 0.7991 Eval Loss: 0.5821 Eval Acc: 0.7998 (LR: 0.00000010)
[2025-06-14 05:07:10,554]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.5745 Train Acc: 0.7992 Eval Loss: 0.5803 Eval Acc: 0.8018 (LR: 0.00000010)
[2025-06-14 05:08:24,647]: [ResNet20_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.5759 Train Acc: 0.7993 Eval Loss: 0.5916 Eval Acc: 0.7960 (LR: 0.00000010)
[2025-06-14 05:08:24,647]: [ResNet20_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.8042
[2025-06-14 05:08:25,138]: 


Quantization of model down to 2 bits finished
[2025-06-14 05:08:25,138]: Model Architecture:
[2025-06-14 05:08:25,538]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4949], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7468885183334351, max_val=0.7376726865768433)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4426], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6484586000442505, max_val=0.6793054342269897)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3548], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5625348091125488, max_val=0.5017943382263184)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3546], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.508441686630249, max_val=0.5554141998291016)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5227], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7946845293045044, max_val=0.7734229564666748)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4156], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5554101467132568, max_val=0.6913797855377197)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3743], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5613628625869751, max_val=0.5614045858383179)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3394], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5038492679595947, max_val=0.5143837928771973)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4568], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6734845042228699, max_val=0.6969301700592041)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2814], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42145246267318726, max_val=0.42280954122543335)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3086], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46013742685317993, max_val=0.4657513499259949)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2941], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43456411361694336, max_val=0.4478462338447571)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2952], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4492412507534027, max_val=0.43621766567230225)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2817], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42286860942840576, max_val=0.4221113324165344)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3039], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4558134078979492, max_val=0.45581233501434326)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3556], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5334939956665039, max_val=0.5332163572311401)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2931], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4534584879875183, max_val=0.4257628917694092)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2598], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4070698022842407, max_val=0.37240004539489746)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2592], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38855934143066406, max_val=0.38889676332473755)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2083], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2927323579788208, max_val=0.33203136920928955)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=2, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-14 05:08:25,550]: 
Model Weights:
[2025-06-14 05:08:25,551]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-14 05:08:25,575]: Sample Values (25 elements): [-0.0868280902504921, -0.01151516754180193, 0.26437535881996155, 0.011203584261238575, -0.15837804973125458, 0.07688041776418686, 0.16721703112125397, 0.010770564898848534, -0.1567240208387375, -0.16382178664207458, -0.27330562472343445, 0.13460999727249146, 0.184380441904068, 0.07615406066179276, -0.1451188027858734, -0.007398751564323902, 0.06627582758665085, -0.14973613619804382, -0.09682565182447433, -0.06767010688781738, -0.15324723720550537, 0.3504316210746765, -0.17881324887275696, -0.037165310233831406, -0.07817159593105316]
[2025-06-14 05:08:25,633]: Mean: 0.00079805
[2025-06-14 05:08:25,648]: Min: -0.53605998
[2025-06-14 05:08:25,649]: Max: 0.44115266
[2025-06-14 05:08:25,649]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,649]: Sample Values (16 elements): [1.023728847503662, 1.110766053199768, 1.2515305280685425, 1.0907936096191406, 1.0808537006378174, 1.299539566040039, 1.1794922351837158, 1.1716669797897339, 1.076856255531311, 1.4366016387939453, 1.3666009902954102, 1.0245997905731201, 1.3033568859100342, 1.0634558200836182, 1.2739230394363403, 2.2178187370300293]
[2025-06-14 05:08:25,650]: Mean: 1.24822402
[2025-06-14 05:08:25,650]: Min: 1.02372885
[2025-06-14 05:08:25,650]: Max: 2.21781874
[2025-06-14 05:08:25,675]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 05:08:25,683]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4948537349700928, 0.4948537349700928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,688]: Mean: -0.00279214
[2025-06-14 05:08:25,698]: Min: -0.98970747
[2025-06-14 05:08:25,705]: Max: 0.49485373
[2025-06-14 05:08:25,705]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,710]: Sample Values (16 elements): [0.7141536474227905, 1.0027259588241577, 0.8880336880683899, 0.7862979769706726, 0.8527214527130127, 0.8676786422729492, 1.110858678817749, 1.0302109718322754, 0.9673991203308105, 0.9373953342437744, 0.8414098620414734, 0.6415853500366211, 0.834802508354187, 0.8220701217651367, 0.7888700366020203, 0.5925614833831787]
[2025-06-14 05:08:25,710]: Mean: 0.85492337
[2025-06-14 05:08:25,710]: Min: 0.59256148
[2025-06-14 05:08:25,710]: Max: 1.11085868
[2025-06-14 05:08:25,712]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 05:08:25,712]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.44258803129196167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44258803129196167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44258803129196167, 0.0, -0.44258803129196167, 0.0]
[2025-06-14 05:08:25,713]: Mean: 0.00537867
[2025-06-14 05:08:25,713]: Min: -0.44258803
[2025-06-14 05:08:25,713]: Max: 0.88517606
[2025-06-14 05:08:25,713]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,714]: Sample Values (16 elements): [0.6552256345748901, 0.7728078365325928, 1.1163058280944824, 0.7143163681030273, 0.5299275517463684, 0.6152218580245972, 0.49824121594429016, 0.8711109161376953, 0.6857548952102661, 0.5356698632240295, 0.7080726027488708, 0.8453676104545593, 0.6965333819389343, 0.8192397952079773, 0.4370291531085968, 0.45390480756759644]
[2025-06-14 05:08:25,714]: Mean: 0.68467057
[2025-06-14 05:08:25,715]: Min: 0.43702915
[2025-06-14 05:08:25,715]: Max: 1.11630583
[2025-06-14 05:08:25,717]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 05:08:25,717]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.35477638244628906, -0.35477638244628906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35477638244628906, -0.35477638244628906, 0.0, 0.0, 0.0, 0.0, 0.35477638244628906, 0.0, 0.0, 0.0, 0.0, -0.35477638244628906, 0.0]
[2025-06-14 05:08:25,717]: Mean: -0.00338762
[2025-06-14 05:08:25,718]: Min: -0.70955276
[2025-06-14 05:08:25,718]: Max: 0.35477638
[2025-06-14 05:08:25,718]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,718]: Sample Values (16 elements): [1.0601872205734253, 0.6648522615432739, 0.9454123377799988, 0.81315016746521, 0.7922411561012268, 1.061141014099121, 1.0760990381240845, 0.8599693775177002, 0.7203349471092224, 0.9354889392852783, 1.019463300704956, 1.0181233882904053, 0.758392333984375, 0.7014033198356628, 1.0015156269073486, 0.8605901002883911]
[2025-06-14 05:08:25,719]: Mean: 0.89302284
[2025-06-14 05:08:25,719]: Min: 0.66485226
[2025-06-14 05:08:25,720]: Max: 1.07609904
[2025-06-14 05:08:25,722]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 05:08:25,722]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.354618638753891, -0.354618638753891, 0.0, 0.0, 0.0, 0.0, -0.354618638753891, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.354618638753891, 0.0, 0.0, -0.354618638753891, 0.0, 0.0, -0.354618638753891, 0.354618638753891]
[2025-06-14 05:08:25,723]: Mean: -0.00446352
[2025-06-14 05:08:25,724]: Min: -0.35461864
[2025-06-14 05:08:25,724]: Max: 0.70923728
[2025-06-14 05:08:25,724]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,724]: Sample Values (16 elements): [0.6647890210151672, 0.5665342211723328, 0.6706914305686951, 0.5147724747657776, 0.5190882682800293, 0.8604453206062317, 1.1955726146697998, 0.531450092792511, 0.7694381475448608, 0.4052470922470093, 0.8835986852645874, 0.5617225766181946, 0.3588792383670807, 0.7335296273231506, 0.8192620277404785, 0.6179795861244202]
[2025-06-14 05:08:25,725]: Mean: 0.66706252
[2025-06-14 05:08:25,725]: Min: 0.35887924
[2025-06-14 05:08:25,725]: Max: 1.19557261
[2025-06-14 05:08:25,728]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 05:08:25,729]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.5227025151252747, -0.5227025151252747, 0.0, 0.0, 0.5227025151252747, 0.0, 0.0, 0.0, 0.0, 0.5227025151252747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5227025151252747, 0.5227025151252747, 0.0, 0.0, 0.0, -1.0454050302505493]
[2025-06-14 05:08:25,729]: Mean: -0.00476422
[2025-06-14 05:08:25,729]: Min: -1.04540503
[2025-06-14 05:08:25,730]: Max: 0.52270252
[2025-06-14 05:08:25,730]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,730]: Sample Values (16 elements): [0.8448569774627686, 0.8650465607643127, 1.1427620649337769, 0.8730652332305908, 0.7713615298271179, 0.9729657173156738, 0.7957523465156555, 0.9780883193016052, 0.6738845705986023, 0.7920522689819336, 1.0978171825408936, 0.8052639365196228, 0.8822391033172607, 0.5490831732749939, 0.8064063787460327, 0.7981708645820618]
[2025-06-14 05:08:25,731]: Mean: 0.85305101
[2025-06-14 05:08:25,731]: Min: 0.54908317
[2025-06-14 05:08:25,731]: Max: 1.14276206
[2025-06-14 05:08:25,733]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 05:08:25,734]: Sample Values (25 elements): [-0.4155966639518738, 0.0, 0.0, 0.4155966639518738, 0.0, 0.0, 0.0, -0.4155966639518738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4155966639518738, 0.0, 0.0, 0.0, 0.0, 0.4155966639518738, 0.0, 0.0, 0.0, -0.4155966639518738, 0.0, 0.0]
[2025-06-14 05:08:25,735]: Mean: -0.00162342
[2025-06-14 05:08:25,735]: Min: -0.41559666
[2025-06-14 05:08:25,736]: Max: 0.83119333
[2025-06-14 05:08:25,736]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-14 05:08:25,736]: Sample Values (16 elements): [0.743703305721283, 0.8974912166595459, 0.783356249332428, 0.7140354514122009, 0.4924573004245758, 0.35412269830703735, 0.712173581123352, 0.6162813901901245, 0.5932444930076599, 0.46183642745018005, 0.8747903108596802, 0.8407893180847168, 0.6481239199638367, 0.39552509784698486, 0.6084945797920227, 0.6115015149116516]
[2025-06-14 05:08:25,737]: Mean: 0.64674544
[2025-06-14 05:08:25,737]: Min: 0.35412270
[2025-06-14 05:08:25,737]: Max: 0.89749122
[2025-06-14 05:08:25,739]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-14 05:08:25,740]: Sample Values (25 elements): [0.0, 0.0, -0.37425583600997925, 0.0, 0.0, 0.0, 0.37425583600997925, -0.37425583600997925, 0.0, 0.0, 0.37425583600997925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.37425583600997925, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,740]: Mean: 0.00406093
[2025-06-14 05:08:25,741]: Min: -0.37425584
[2025-06-14 05:08:25,741]: Max: 0.74851167
[2025-06-14 05:08:25,741]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,742]: Sample Values (25 elements): [0.835552453994751, 0.818679928779602, 0.7656418085098267, 0.9938370585441589, 0.7964612245559692, 0.7549654841423035, 0.7638548016548157, 0.7494333982467651, 0.9114128947257996, 0.6476247310638428, 0.7096834778785706, 0.8534451723098755, 0.7460725903511047, 0.8502819538116455, 0.6939634680747986, 0.9634549021720886, 0.8358489871025085, 0.9049587249755859, 0.8114597797393799, 0.7985670566558838, 0.7430838942527771, 0.7522293329238892, 0.7815536260604858, 0.6571047306060791, 0.7564283013343811]
[2025-06-14 05:08:25,742]: Mean: 0.80632865
[2025-06-14 05:08:25,742]: Min: 0.59628195
[2025-06-14 05:08:25,742]: Max: 0.99383706
[2025-06-14 05:08:25,744]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 05:08:25,744]: Sample Values (25 elements): [0.0, 0.33941102027893066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,744]: Mean: 0.00128900
[2025-06-14 05:08:25,745]: Min: -0.33941102
[2025-06-14 05:08:25,745]: Max: 0.67882204
[2025-06-14 05:08:25,745]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,745]: Sample Values (25 elements): [0.9337868094444275, 0.7729042768478394, 1.0204133987426758, 0.9228571653366089, 0.6810354590415955, 0.8135755658149719, 0.888823390007019, 0.7734600901603699, 0.8262020349502563, 0.8619433045387268, 0.7312158942222595, 0.7097587585449219, 0.8811724781990051, 0.8595209121704102, 0.6813563704490662, 0.7059803009033203, 0.698096513748169, 0.6956320405006409, 0.7594549059867859, 0.8174620270729065, 0.8014227151870728, 0.6788743734359741, 0.7769289612770081, 0.6567721366882324, 0.7309260964393616]
[2025-06-14 05:08:25,746]: Mean: 0.77598000
[2025-06-14 05:08:25,746]: Min: 0.58035970
[2025-06-14 05:08:25,746]: Max: 1.02041340
[2025-06-14 05:08:25,749]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-14 05:08:25,750]: Sample Values (25 elements): [0.45680493116378784, 0.45680493116378784, 0.0, 0.0, 0.45680493116378784, 0.45680493116378784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.45680493116378784, 0.45680493116378784, 0.0]
[2025-06-14 05:08:25,750]: Mean: 0.01605955
[2025-06-14 05:08:25,750]: Min: -0.45680493
[2025-06-14 05:08:25,751]: Max: 0.91360986
[2025-06-14 05:08:25,751]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,751]: Sample Values (25 elements): [0.43658649921417236, 0.49287840723991394, 0.4056954085826874, 0.39359480142593384, 0.42523419857025146, 0.25677743554115295, 0.4666961133480072, 0.3151613771915436, 0.4192078709602356, 0.4276994466781616, 0.24057281017303467, 0.35132458806037903, 0.4199655055999756, 0.2293483018875122, 0.40515702962875366, 0.31263574957847595, 0.398576945066452, 0.2652706801891327, 0.22316063940525055, 0.5318840742111206, 0.574854850769043, 0.3988571763038635, 0.3833862841129303, 0.34718772768974304, 0.4224686324596405]
[2025-06-14 05:08:25,752]: Mean: 0.36883596
[2025-06-14 05:08:25,752]: Min: 0.11749669
[2025-06-14 05:08:25,753]: Max: 0.57485485
[2025-06-14 05:08:25,755]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 05:08:25,756]: Sample Values (25 elements): [0.0, -0.28142067790031433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28142067790031433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,756]: Mean: 0.00305361
[2025-06-14 05:08:25,757]: Min: -0.28142068
[2025-06-14 05:08:25,757]: Max: 0.56284136
[2025-06-14 05:08:25,757]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,757]: Sample Values (25 elements): [1.0498117208480835, 0.7691176533699036, 0.7214062213897705, 0.8095459342002869, 0.6759598851203918, 0.8425496220588684, 0.6434594392776489, 0.8363685011863708, 0.8267990350723267, 0.7311328053474426, 0.8767680525779724, 0.7030274271965027, 0.811049222946167, 0.8582617044448853, 0.948458731174469, 0.9564011693000793, 0.768580973148346, 0.7117146849632263, 1.0363632440567017, 0.8348422050476074, 0.9554997086524963, 0.9816489219665527, 0.9252180457115173, 0.7102826237678528, 0.8682378530502319]
[2025-06-14 05:08:25,758]: Mean: 0.84134543
[2025-06-14 05:08:25,758]: Min: 0.64345944
[2025-06-14 05:08:25,758]: Max: 1.04981172
[2025-06-14 05:08:25,759]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 05:08:25,760]: Sample Values (25 elements): [0.0, 0.0, -0.3086296021938324, 0.0, 0.0, 0.0, 0.0, -0.3086296021938324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3086296021938324, 0.0, 0.0, 0.0, 0.3086296021938324, 0.0, 0.0, 0.3086296021938324, 0.0]
[2025-06-14 05:08:25,760]: Mean: 0.00271256
[2025-06-14 05:08:25,761]: Min: -0.30862960
[2025-06-14 05:08:25,761]: Max: 0.61725920
[2025-06-14 05:08:25,761]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,762]: Sample Values (25 elements): [0.6529008746147156, 0.9510505795478821, 0.7815940976142883, 0.6765440702438354, 0.7433018088340759, 0.5488685369491577, 0.5414865612983704, 0.7574984431266785, 0.6381670236587524, 0.5956264734268188, 0.7721691727638245, 0.7549513578414917, 0.6852902173995972, 0.7538092136383057, 0.6750994324684143, 0.7461153864860535, 0.5863045454025269, 0.711397647857666, 0.8947746157646179, 0.6526278853416443, 0.7721806764602661, 0.63954758644104, 0.5534440875053406, 0.8559975028038025, 0.254312127828598]
[2025-06-14 05:08:25,762]: Mean: 0.69619936
[2025-06-14 05:08:25,762]: Min: 0.25431213
[2025-06-14 05:08:25,763]: Max: 0.95105058
[2025-06-14 05:08:25,764]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 05:08:25,764]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.29413679242134094, 0.0, 0.0, -0.29413679242134094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29413679242134094, 0.0, 0.0, 0.29413679242134094, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,765]: Mean: -0.00197878
[2025-06-14 05:08:25,765]: Min: -0.29413679
[2025-06-14 05:08:25,765]: Max: 0.58827358
[2025-06-14 05:08:25,765]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,766]: Sample Values (25 elements): [0.9728924036026001, 0.7275185585021973, 1.2402253150939941, 1.1384674310684204, 0.9243077039718628, 1.0605908632278442, 0.9316977262496948, 0.84088534116745, 0.7970213294029236, 0.8111931681632996, 0.8453801870346069, 0.8191510438919067, 1.145963430404663, 0.9301076531410217, 0.9782876968383789, 0.9883743524551392, 0.8600721955299377, 0.8329496383666992, 1.0356287956237793, 0.9835573434829712, 0.8574367165565491, 0.9628431797027588, 0.8919820189476013, 0.826531171798706, 0.9431531429290771]
[2025-06-14 05:08:25,766]: Mean: 0.94100255
[2025-06-14 05:08:25,766]: Min: 0.66677243
[2025-06-14 05:08:25,766]: Max: 1.24022532
[2025-06-14 05:08:25,768]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 05:08:25,769]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2951529622077942, 0.0, 0.0, 0.0, -0.2951529622077942, 0.0, 0.2951529622077942, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2951529622077942, 0.0, 0.2951529622077942, 0.0, 0.0, 0.0, -0.2951529622077942]
[2025-06-14 05:08:25,769]: Mean: -0.00384314
[2025-06-14 05:08:25,769]: Min: -0.59030592
[2025-06-14 05:08:25,769]: Max: 0.29515296
[2025-06-14 05:08:25,769]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-14 05:08:25,770]: Sample Values (25 elements): [0.9456573128700256, 0.6382811665534973, 0.7760231494903564, 0.6796079277992249, 0.8533015251159668, 0.5918254852294922, 0.7217796444892883, 0.8376355767250061, 0.786574125289917, 0.651450514793396, 0.665681779384613, 0.5957130193710327, 0.683367908000946, 0.904900074005127, 0.5624012351036072, 0.7982361316680908, 0.7327771186828613, 0.6829991340637207, 0.8327722549438477, 0.6416969895362854, 0.8657934665679932, 0.6888361573219299, 0.5981255769729614, 0.542899489402771, 0.7545824646949768]
[2025-06-14 05:08:25,770]: Mean: 0.70541203
[2025-06-14 05:08:25,770]: Min: 0.50604194
[2025-06-14 05:08:25,770]: Max: 0.94565731
[2025-06-14 05:08:25,772]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-14 05:08:25,772]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.2816599905490875, 0.0, -0.2816599905490875, 0.0, 0.2816599905490875, 0.0, 0.0, 0.2816599905490875, 0.0, -0.2816599905490875, 0.0, 0.0, 0.2816599905490875, 0.0, 0.0, 0.0, 0.0, 0.2816599905490875, 0.0, -0.2816599905490875, 0.0]
[2025-06-14 05:08:25,773]: Mean: -0.00215463
[2025-06-14 05:08:25,773]: Min: -0.56331998
[2025-06-14 05:08:25,773]: Max: 0.28165999
[2025-06-14 05:08:25,773]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,773]: Sample Values (25 elements): [0.8251391053199768, -5.016087982897115e-41, 0.7903017997741699, 0.6642799377441406, 0.9685570001602173, 0.8724868893623352, 0.8997163772583008, 0.9699243903160095, 0.9248055219650269, 0.6757973432540894, 0.8399428129196167, 0.7818006873130798, 0.8813377022743225, 0.8620889186859131, 0.7170672416687012, 0.6503422260284424, 0.7598015666007996, 0.993704617023468, 0.9323667287826538, 0.9139484763145447, 0.9543588161468506, 0.6812275052070618, 0.5655501484870911, 0.8313404321670532, 1.0671409368515015]
[2025-06-14 05:08:25,774]: Mean: 0.79719102
[2025-06-14 05:08:25,774]: Min: -0.00000000
[2025-06-14 05:08:25,774]: Max: 1.06714094
[2025-06-14 05:08:25,776]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 05:08:25,777]: Sample Values (25 elements): [0.0, 0.0, -0.30387526750564575, 0.0, 0.0, 0.0, 0.0, -0.30387526750564575, 0.0, 0.0, 0.0, -0.30387526750564575, 0.0, 0.0, 0.0, 0.30387526750564575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,777]: Mean: 0.00027202
[2025-06-14 05:08:25,778]: Min: -0.60775054
[2025-06-14 05:08:25,778]: Max: 0.30387527
[2025-06-14 05:08:25,778]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,778]: Sample Values (25 elements): [0.738052248954773, 0.767132580280304, 0.7368828058242798, 0.7250796556472778, 0.6718116998672485, 0.8974910378456116, 0.8397248983383179, 0.6725105047225952, 0.9432893395423889, 0.803577721118927, 0.6930550336837769, 0.7722604274749756, 0.8243170380592346, 0.7805882692337036, 1.075180172920227, 0.7281912565231323, 0.8545249104499817, 0.7344145178794861, 0.918416440486908, 0.7181100249290466, 0.7786543965339661, 0.5684372186660767, 0.7267439961433411, 0.6612228751182556, 0.8205295205116272]
[2025-06-14 05:08:25,778]: Mean: 0.77082574
[2025-06-14 05:08:25,779]: Min: 0.56843722
[2025-06-14 05:08:25,779]: Max: 1.07518017
[2025-06-14 05:08:25,783]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-14 05:08:25,785]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.35557013750076294, 0.0, 0.0, 0.0, 0.0, -0.35557013750076294, 0.0, 0.0, 0.0, 0.35557013750076294, 0.0, 0.35557013750076294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35557013750076294, -0.35557013750076294, 0.35557013750076294, 0.0]
[2025-06-14 05:08:25,787]: Mean: 0.00069447
[2025-06-14 05:08:25,789]: Min: -0.71114028
[2025-06-14 05:08:25,791]: Max: 0.35557014
[2025-06-14 05:08:25,792]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,796]: Sample Values (25 elements): [0.4378693401813507, 0.22905535995960236, 0.610373318195343, 0.55431067943573, 0.49141982197761536, 0.4695582091808319, 0.4695345461368561, 0.602746844291687, 0.30177125334739685, 0.6103589534759521, 0.501633882522583, 0.44274383783340454, 0.4033888578414917, 0.2988662123680115, 0.4387776255607605, 0.3075351119041443, 0.6249171495437622, 0.30229949951171875, 0.19590353965759277, 0.44382062554359436, 0.504342257976532, 0.4493124783039093, 0.3708772659301758, 0.3130171597003937, 0.4804496169090271]
[2025-06-14 05:08:25,799]: Mean: 0.42754591
[2025-06-14 05:08:25,802]: Min: 0.12570596
[2025-06-14 05:08:25,807]: Max: 0.63993281
[2025-06-14 05:08:25,833]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 05:08:25,834]: Sample Values (25 elements): [0.0, 0.0, 0.2930738031864166, 0.2930738031864166, 0.2930738031864166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2930738031864166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2930738031864166, 0.0]
[2025-06-14 05:08:25,834]: Mean: -0.00079501
[2025-06-14 05:08:25,834]: Min: -0.58614761
[2025-06-14 05:08:25,835]: Max: 0.29307380
[2025-06-14 05:08:25,835]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,835]: Sample Values (25 elements): [1.1420087814331055, 0.7272406816482544, 0.7185344696044922, 0.802574098110199, 1.0326826572418213, 0.9861966967582703, 0.8506768345832825, 0.8088820576667786, 0.9962435364723206, 0.8580555319786072, 0.7326541543006897, 0.013957030139863491, 0.9270759224891663, 1.1362899541854858, 0.8058208227157593, 0.9038814306259155, 0.7752618789672852, 0.6521975994110107, 1.0276556015014648, 1.003941297531128, 0.7955006957054138, 1.0033435821533203, 0.9705198407173157, 1.1418461799621582, 0.943377673625946]
[2025-06-14 05:08:25,836]: Mean: 0.87703866
[2025-06-14 05:08:25,836]: Min: 0.00000000
[2025-06-14 05:08:25,836]: Max: 1.17777324
[2025-06-14 05:08:25,837]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 05:08:25,840]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.2598232924938202, 0.2598232924938202, -0.2598232924938202, 0.0, 0.0, 0.0, 0.2598232924938202, 0.0, 0.0, 0.0, 0.2598232924938202, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2598232924938202, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,840]: Mean: 0.00075415
[2025-06-14 05:08:25,840]: Min: -0.51964658
[2025-06-14 05:08:25,840]: Max: 0.25982329
[2025-06-14 05:08:25,841]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,841]: Sample Values (25 elements): [0.8774110674858093, 0.8946287035942078, 0.7930731177330017, 0.9293408393859863, 0.8377625942230225, 0.8459003567695618, 0.740405261516571, 1.1186083555221558, 0.7853603363037109, 0.8540632724761963, 0.8667955994606018, 0.49728912115097046, 0.8713002800941467, 0.9681668281555176, 1.056843876838684, 0.8536065220832825, 0.6518045663833618, 0.8224246501922607, 0.8977831602096558, 0.712017297744751, 0.9107881784439087, 0.9208199977874756, 0.7242178916931152, 0.9178805947303772, 0.9011223912239075]
[2025-06-14 05:08:25,841]: Mean: 0.86052227
[2025-06-14 05:08:25,841]: Min: 0.38024178
[2025-06-14 05:08:25,842]: Max: 1.18726778
[2025-06-14 05:08:25,843]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 05:08:25,844]: Sample Values (25 elements): [0.0, 0.0, 0.25915202498435974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25915202498435974]
[2025-06-14 05:08:25,845]: Mean: -0.00127945
[2025-06-14 05:08:25,845]: Min: -0.25915202
[2025-06-14 05:08:25,845]: Max: 0.51830405
[2025-06-14 05:08:25,845]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,846]: Sample Values (25 elements): [0.7425389885902405, 0.8320014476776123, 0.7749935984611511, 1.0703068971633911, 1.3068228960037231, 0.8347991704940796, 0.7956835031509399, 0.7238859534263611, 0.9160106778144836, 0.9569909572601318, 1.0942389965057373, 0.899067759513855, 1.013637661933899, 1.163357138633728, 1.0429086685180664, 0.930575430393219, -5.416999473540445e-41, 1.014910340309143, 0.6739298701286316, 4.94406124183082e-41, 0.7591232657432556, 0.8659958243370056, 1.0841701030731201, 0.9523146748542786, 0.5900200605392456]
[2025-06-14 05:08:25,846]: Mean: 0.82002056
[2025-06-14 05:08:25,846]: Min: -0.00000000
[2025-06-14 05:08:25,846]: Max: 1.39403796
[2025-06-14 05:08:25,848]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 05:08:25,849]: Sample Values (25 elements): [-0.20825457572937012, 0.0, 0.0, -0.20825457572937012, 0.0, 0.0, 0.0, 0.20825457572937012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 05:08:25,849]: Mean: -0.00053103
[2025-06-14 05:08:25,849]: Min: -0.20825458
[2025-06-14 05:08:25,850]: Max: 0.41650915
[2025-06-14 05:08:25,850]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-14 05:08:25,851]: Sample Values (25 elements): [0.9812209606170654, 0.8720225691795349, 0.8310456871986389, 0.94205242395401, 0.993895411491394, 1.0474292039871216, 0.9487813711166382, 0.9299154877662659, 0.9841781854629517, 1.0376487970352173, 1.0105347633361816, 0.840880811214447, 0.9363568425178528, 0.9566482901573181, 0.86102294921875, 0.9824052453041077, 0.8930319547653198, 0.982406497001648, 0.8845740556716919, 0.9726747870445251, 1.006623387336731, 1.0631909370422363, 1.0104316473007202, 1.0352261066436768, 0.9544402360916138]
[2025-06-14 05:08:25,851]: Mean: 0.95764506
[2025-06-14 05:08:25,851]: Min: 0.71657348
[2025-06-14 05:08:25,851]: Max: 1.17842925
[2025-06-14 05:08:25,852]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-14 05:08:25,852]: Sample Values (25 elements): [0.1065187007188797, 0.10512714833021164, -0.4033235013484955, 0.21556442975997925, -0.46345511078834534, -0.23491869866847992, -0.025865301489830017, -0.2447369247674942, 0.12224147468805313, -0.19903385639190674, -0.3225955069065094, 0.05205156281590462, -0.3346249759197235, 0.010852556675672531, -0.3912717401981354, -0.3491954803466797, 0.6193011403083801, -0.11141227185726166, 0.04813302680850029, -0.5552132725715637, 0.3634687066078186, 0.13650639355182648, 0.032473403960466385, 0.5982195734977722, -0.41991889476776123]
[2025-06-14 05:08:25,852]: Mean: -0.00690672
[2025-06-14 05:08:25,853]: Min: -0.97420478
[2025-06-14 05:08:25,853]: Max: 1.02382255
