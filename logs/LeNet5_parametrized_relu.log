[2025-05-20 07:06:55,087]: Checkpoint of model at path [checkpoint/LeNet5_relu6.ckpt] will be used for QAT
[2025-05-20 07:06:55,087]: 


QAT of LeNet5 with parametrized_relu down to 2 bits...
[2025-05-20 07:06:55,217]: [LeNet5_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-05-20 07:06:55,345]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 07:07:23,365]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 1.6248 Train Acc: 0.4155 Eval Loss: 1.3697 Eval Acc: 0.5003 (LR: 0.010000)
[2025-05-20 07:07:49,762]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 1.5029 Train Acc: 0.4524 Eval Loss: 1.3451 Eval Acc: 0.5177 (LR: 0.010000)
[2025-05-20 07:08:16,405]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.4843 Train Acc: 0.4614 Eval Loss: 1.4043 Eval Acc: 0.4785 (LR: 0.010000)
[2025-05-20 07:08:43,127]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.4683 Train Acc: 0.4682 Eval Loss: 1.3284 Eval Acc: 0.5235 (LR: 0.010000)
[2025-05-20 07:09:10,006]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.4553 Train Acc: 0.4736 Eval Loss: 1.3102 Eval Acc: 0.5196 (LR: 0.010000)
[2025-05-20 07:09:36,422]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.4479 Train Acc: 0.4747 Eval Loss: 1.3009 Eval Acc: 0.5399 (LR: 0.010000)
[2025-05-20 07:10:04,205]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.4302 Train Acc: 0.4834 Eval Loss: 1.3388 Eval Acc: 0.5101 (LR: 0.010000)
[2025-05-20 07:10:31,195]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.4433 Train Acc: 0.4795 Eval Loss: 1.3483 Eval Acc: 0.5127 (LR: 0.010000)
[2025-05-20 07:10:57,493]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.4160 Train Acc: 0.4887 Eval Loss: 1.2975 Eval Acc: 0.5368 (LR: 0.010000)
[2025-05-20 07:11:23,830]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.4176 Train Acc: 0.4906 Eval Loss: 1.2568 Eval Acc: 0.5500 (LR: 0.010000)
[2025-05-20 07:11:51,515]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.4044 Train Acc: 0.4931 Eval Loss: 1.2463 Eval Acc: 0.5468 (LR: 0.010000)
[2025-05-20 07:12:19,211]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.4065 Train Acc: 0.4917 Eval Loss: 1.2505 Eval Acc: 0.5498 (LR: 0.010000)
[2025-05-20 07:12:47,052]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.3987 Train Acc: 0.4969 Eval Loss: 1.2650 Eval Acc: 0.5431 (LR: 0.010000)
[2025-05-20 07:13:13,368]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.3995 Train Acc: 0.4942 Eval Loss: 1.2580 Eval Acc: 0.5520 (LR: 0.010000)
[2025-05-20 07:13:39,882]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.3806 Train Acc: 0.5017 Eval Loss: 1.2478 Eval Acc: 0.5480 (LR: 0.001000)
[2025-05-20 07:14:05,980]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.2942 Train Acc: 0.5366 Eval Loss: 1.1688 Eval Acc: 0.5806 (LR: 0.001000)
[2025-05-20 07:14:31,863]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.2857 Train Acc: 0.5384 Eval Loss: 1.1548 Eval Acc: 0.5803 (LR: 0.001000)
[2025-05-20 07:14:58,162]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.2775 Train Acc: 0.5415 Eval Loss: 1.1507 Eval Acc: 0.5890 (LR: 0.001000)
[2025-05-20 07:15:25,864]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.2775 Train Acc: 0.5403 Eval Loss: 1.1425 Eval Acc: 0.5905 (LR: 0.001000)
[2025-05-20 07:15:52,106]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.2789 Train Acc: 0.5429 Eval Loss: 1.1685 Eval Acc: 0.5806 (LR: 0.001000)
[2025-05-20 07:16:19,217]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.2712 Train Acc: 0.5430 Eval Loss: 1.1588 Eval Acc: 0.5840 (LR: 0.001000)
[2025-05-20 07:16:45,540]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.2734 Train Acc: 0.5429 Eval Loss: 1.1649 Eval Acc: 0.5836 (LR: 0.001000)
[2025-05-20 07:17:12,697]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.2815 Train Acc: 0.5391 Eval Loss: 1.1460 Eval Acc: 0.5789 (LR: 0.001000)
[2025-05-20 07:17:39,996]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.2876 Train Acc: 0.5365 Eval Loss: 1.1394 Eval Acc: 0.5909 (LR: 0.001000)
[2025-05-20 07:18:07,627]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.2762 Train Acc: 0.5416 Eval Loss: 1.1484 Eval Acc: 0.5873 (LR: 0.001000)
[2025-05-20 07:18:35,998]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.2809 Train Acc: 0.5393 Eval Loss: 1.1386 Eval Acc: 0.5929 (LR: 0.001000)
[2025-05-20 07:19:03,142]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.2815 Train Acc: 0.5387 Eval Loss: 1.1841 Eval Acc: 0.5684 (LR: 0.001000)
[2025-05-20 07:19:31,340]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.2866 Train Acc: 0.5351 Eval Loss: 1.1773 Eval Acc: 0.5777 (LR: 0.001000)
[2025-05-20 07:19:58,756]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.2808 Train Acc: 0.5416 Eval Loss: 1.1593 Eval Acc: 0.5874 (LR: 0.001000)
[2025-05-20 07:20:26,548]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.2803 Train Acc: 0.5381 Eval Loss: 1.1267 Eval Acc: 0.5980 (LR: 0.000100)
[2025-05-20 07:20:54,047]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.2372 Train Acc: 0.5563 Eval Loss: 1.1263 Eval Acc: 0.5972 (LR: 0.000100)
[2025-05-20 07:21:21,769]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.2414 Train Acc: 0.5550 Eval Loss: 1.1226 Eval Acc: 0.5995 (LR: 0.000100)
[2025-05-20 07:21:49,154]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.2387 Train Acc: 0.5535 Eval Loss: 1.1301 Eval Acc: 0.5936 (LR: 0.000100)
[2025-05-20 07:22:16,414]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.2491 Train Acc: 0.5527 Eval Loss: 1.1340 Eval Acc: 0.5902 (LR: 0.000100)
[2025-05-20 07:22:44,371]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.2397 Train Acc: 0.5541 Eval Loss: 1.1417 Eval Acc: 0.5890 (LR: 0.000100)
[2025-05-20 07:23:12,381]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.2502 Train Acc: 0.5507 Eval Loss: 1.1328 Eval Acc: 0.5911 (LR: 0.000100)
[2025-05-20 07:23:40,737]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.2482 Train Acc: 0.5541 Eval Loss: 1.1361 Eval Acc: 0.5914 (LR: 0.000100)
[2025-05-20 07:24:08,687]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.2429 Train Acc: 0.5524 Eval Loss: 1.1271 Eval Acc: 0.5939 (LR: 0.000100)
[2025-05-20 07:24:36,051]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.2545 Train Acc: 0.5495 Eval Loss: 1.1329 Eval Acc: 0.5954 (LR: 0.000100)
[2025-05-20 07:25:03,059]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.2521 Train Acc: 0.5480 Eval Loss: 1.1112 Eval Acc: 0.6016 (LR: 0.000100)
[2025-05-20 07:25:31,299]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 1.2543 Train Acc: 0.5476 Eval Loss: 1.1345 Eval Acc: 0.6007 (LR: 0.000100)
[2025-05-20 07:25:58,658]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 1.2584 Train Acc: 0.5493 Eval Loss: 1.1368 Eval Acc: 0.5898 (LR: 0.000100)
[2025-05-20 07:26:26,138]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 1.2463 Train Acc: 0.5531 Eval Loss: 1.1411 Eval Acc: 0.5882 (LR: 0.000100)
[2025-05-20 07:26:53,885]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 1.2481 Train Acc: 0.5529 Eval Loss: 1.1469 Eval Acc: 0.5874 (LR: 0.000100)
[2025-05-20 07:27:21,212]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 1.2517 Train Acc: 0.5508 Eval Loss: 1.1476 Eval Acc: 0.5894 (LR: 0.000010)
[2025-05-20 07:27:48,354]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 1.2253 Train Acc: 0.5606 Eval Loss: 1.1224 Eval Acc: 0.5957 (LR: 0.000010)
[2025-05-20 07:28:16,852]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 1.2266 Train Acc: 0.5597 Eval Loss: 1.1149 Eval Acc: 0.5985 (LR: 0.000010)
[2025-05-20 07:28:44,490]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 1.2291 Train Acc: 0.5609 Eval Loss: 1.1167 Eval Acc: 0.6009 (LR: 0.000010)
[2025-05-20 07:29:11,734]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 1.2295 Train Acc: 0.5575 Eval Loss: 1.1432 Eval Acc: 0.5938 (LR: 0.000010)
[2025-05-20 07:29:38,612]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 1.2331 Train Acc: 0.5587 Eval Loss: 1.1214 Eval Acc: 0.5991 (LR: 0.000010)
[2025-05-20 07:30:05,326]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 1.2395 Train Acc: 0.5554 Eval Loss: 1.1232 Eval Acc: 0.5990 (LR: 0.000010)
[2025-05-20 07:30:31,857]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 1.2393 Train Acc: 0.5556 Eval Loss: 1.1532 Eval Acc: 0.5878 (LR: 0.000010)
[2025-05-20 07:30:58,525]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 1.2411 Train Acc: 0.5551 Eval Loss: 1.1334 Eval Acc: 0.5926 (LR: 0.000010)
[2025-05-20 07:31:25,085]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 1.2386 Train Acc: 0.5556 Eval Loss: 1.1558 Eval Acc: 0.5867 (LR: 0.000010)
[2025-05-20 07:31:51,724]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 1.2432 Train Acc: 0.5552 Eval Loss: 1.1137 Eval Acc: 0.5976 (LR: 0.000010)
[2025-05-20 07:32:18,232]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 1.2398 Train Acc: 0.5548 Eval Loss: 1.1410 Eval Acc: 0.5885 (LR: 0.000010)
[2025-05-20 07:32:44,738]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 1.2397 Train Acc: 0.5545 Eval Loss: 1.1099 Eval Acc: 0.6062 (LR: 0.000010)
[2025-05-20 07:33:11,217]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 1.2361 Train Acc: 0.5560 Eval Loss: 1.1362 Eval Acc: 0.5956 (LR: 0.000010)
[2025-05-20 07:33:38,193]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 1.2396 Train Acc: 0.5552 Eval Loss: 1.1322 Eval Acc: 0.5956 (LR: 0.000010)
[2025-05-20 07:34:05,603]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 1.2448 Train Acc: 0.5532 Eval Loss: 1.1395 Eval Acc: 0.5912 (LR: 0.000010)
[2025-05-20 07:34:05,604]: [LeNet5_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.6062
[2025-05-20 07:34:05,625]: 


Quantization of model down to 2 bits finished
[2025-05-20 07:34:05,625]: Model Architecture:
[2025-05-20 07:34:05,638]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4721], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7081258893013, max_val=0.7081519961357117)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1322], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2012], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2996032238006592, max_val=0.3041333854198456)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2573], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2875], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3509255051612854, max_val=0.511472225189209)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8502], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 07:34:05,638]: 
Model Weights:
[2025-05-20 07:34:05,639]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-20 07:34:05,639]: Sample Values (25 elements): [0.29042962193489075, 0.15979255735874176, -0.15401619672775269, -1.0435689687728882, 0.1594667285680771, 0.5461935997009277, 0.04847760125994682, 0.23973631858825684, -0.375244677066803, -0.2633323073387146, -0.6463786363601685, -0.029024813324213028, 0.044588226824998856, 0.08349261432886124, 0.026771778240799904, -0.4778474271297455, 0.44226765632629395, -0.22078247368335724, 0.4596201777458191, -0.15367190539836884, -0.3262261152267456, -0.10960521548986435, 0.10913322865962982, 0.22558428347110748, 0.05578145384788513]
[2025-05-20 07:34:05,648]: Mean: 0.00095298
[2025-05-20 07:34:05,648]: Min: -1.34105098
[2025-05-20 07:34:05,649]: Max: 1.21770442
[2025-05-20 07:34:05,650]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-20 07:34:05,651]: Sample Values (25 elements): [0.0, -0.4720926284790039, -0.4720926284790039, -0.4720926284790039, 0.0, 0.4720926284790039, 0.0, 0.0, 0.0, 0.0, 0.4720926284790039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4720926284790039, 0.0, 0.0, 0.0, -0.4720926284790039, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 07:34:05,651]: Mean: -0.02183428
[2025-05-20 07:34:05,651]: Min: -0.47209263
[2025-05-20 07:34:05,652]: Max: 0.94418526
[2025-05-20 07:34:05,653]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-20 07:34:05,654]: Sample Values (25 elements): [0.20124554634094238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20124554634094238, 0.0, 0.0, 0.20124554634094238, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 07:34:05,654]: Mean: -0.00276713
[2025-05-20 07:34:05,655]: Min: -0.20124555
[2025-05-20 07:34:05,655]: Max: 0.40249109
[2025-05-20 07:34:05,656]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-20 07:34:05,657]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.287465900182724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 07:34:05,657]: Mean: 0.00094111
[2025-05-20 07:34:05,657]: Min: -0.28746590
[2025-05-20 07:34:05,658]: Max: 0.57493180
[2025-05-20 07:34:05,658]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-20 07:34:05,658]: Sample Values (25 elements): [-0.03748337924480438, 0.0017234793631359935, -0.025347435846924782, -0.05165812745690346, 0.048151955008506775, -0.009570498019456863, 0.030214795842766762, -0.032595738768577576, 0.03850356116890907, -0.06722327321767807, 0.03796079382300377, -0.050547488033771515, 0.028455568477511406, 0.06175946816802025, -0.0220969058573246, -0.0623137541115284, -0.037988945841789246, 0.03319615498185158, -0.04748872295022011, -0.04412345960736275, -0.059483688324689865, 0.0036182303447276354, 0.10436371713876724, -0.10802305489778519, 0.028678543865680695]
[2025-05-20 07:34:05,658]: Mean: -0.00070435
[2025-05-20 07:34:05,659]: Min: -0.21173702
[2025-05-20 07:34:05,659]: Max: 0.15517637
[2025-05-20 07:34:05,659]: 


QAT of LeNet5 with parametrized_relu down to 3 bits...
[2025-05-20 07:34:05,679]: [LeNet5_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-05-20 07:34:05,685]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 07:34:31,827]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 1.3964 Train Acc: 0.5031 Eval Loss: 1.1366 Eval Acc: 0.5900 (LR: 0.010000)
[2025-05-20 07:34:58,503]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 1.3285 Train Acc: 0.5231 Eval Loss: 1.1537 Eval Acc: 0.5843 (LR: 0.010000)
[2025-05-20 07:35:25,322]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 1.2906 Train Acc: 0.5383 Eval Loss: 1.2017 Eval Acc: 0.5728 (LR: 0.010000)
[2025-05-20 07:35:53,157]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 1.2803 Train Acc: 0.5436 Eval Loss: 1.1587 Eval Acc: 0.5855 (LR: 0.010000)
[2025-05-20 07:36:20,287]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 1.2806 Train Acc: 0.5445 Eval Loss: 1.1678 Eval Acc: 0.5825 (LR: 0.010000)
[2025-05-20 07:36:46,592]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 1.2591 Train Acc: 0.5520 Eval Loss: 1.0997 Eval Acc: 0.6052 (LR: 0.010000)
[2025-05-20 07:37:13,101]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 1.2555 Train Acc: 0.5522 Eval Loss: 1.1310 Eval Acc: 0.5888 (LR: 0.010000)
[2025-05-20 07:37:39,352]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 1.2344 Train Acc: 0.5571 Eval Loss: 1.0987 Eval Acc: 0.6112 (LR: 0.010000)
[2025-05-20 07:38:05,439]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 1.2483 Train Acc: 0.5549 Eval Loss: 1.0837 Eval Acc: 0.6096 (LR: 0.010000)
[2025-05-20 07:38:32,794]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 1.2436 Train Acc: 0.5572 Eval Loss: 1.0867 Eval Acc: 0.6121 (LR: 0.010000)
[2025-05-20 07:38:59,683]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 1.2230 Train Acc: 0.5639 Eval Loss: 1.0997 Eval Acc: 0.6081 (LR: 0.010000)
[2025-05-20 07:39:26,245]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 1.2391 Train Acc: 0.5603 Eval Loss: 1.0964 Eval Acc: 0.6051 (LR: 0.010000)
[2025-05-20 07:39:52,823]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 1.2267 Train Acc: 0.5632 Eval Loss: 1.1432 Eval Acc: 0.6033 (LR: 0.010000)
[2025-05-20 07:40:19,204]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 1.2226 Train Acc: 0.5643 Eval Loss: 1.0691 Eval Acc: 0.6199 (LR: 0.010000)
[2025-05-20 07:40:45,832]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 1.2175 Train Acc: 0.5673 Eval Loss: 1.0856 Eval Acc: 0.6125 (LR: 0.001000)
[2025-05-20 07:41:12,218]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 1.1111 Train Acc: 0.6055 Eval Loss: 1.0028 Eval Acc: 0.6414 (LR: 0.001000)
[2025-05-20 07:41:38,773]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 1.0971 Train Acc: 0.6085 Eval Loss: 0.9793 Eval Acc: 0.6499 (LR: 0.001000)
[2025-05-20 07:42:06,143]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 1.0911 Train Acc: 0.6110 Eval Loss: 0.9727 Eval Acc: 0.6537 (LR: 0.001000)
[2025-05-20 07:42:34,316]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 1.0877 Train Acc: 0.6144 Eval Loss: 0.9759 Eval Acc: 0.6550 (LR: 0.001000)
[2025-05-20 07:43:01,505]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 1.0868 Train Acc: 0.6127 Eval Loss: 0.9884 Eval Acc: 0.6463 (LR: 0.001000)
[2025-05-20 07:43:27,513]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 1.0929 Train Acc: 0.6101 Eval Loss: 0.9872 Eval Acc: 0.6527 (LR: 0.001000)
[2025-05-20 07:43:53,770]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 1.0898 Train Acc: 0.6125 Eval Loss: 0.9875 Eval Acc: 0.6508 (LR: 0.001000)
[2025-05-20 07:44:19,908]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 1.0874 Train Acc: 0.6130 Eval Loss: 0.9932 Eval Acc: 0.6474 (LR: 0.001000)
[2025-05-20 07:44:45,952]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 1.0917 Train Acc: 0.6131 Eval Loss: 0.9943 Eval Acc: 0.6481 (LR: 0.001000)
[2025-05-20 07:45:12,084]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 1.0873 Train Acc: 0.6136 Eval Loss: 0.9883 Eval Acc: 0.6483 (LR: 0.001000)
[2025-05-20 07:45:38,196]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 1.0881 Train Acc: 0.6152 Eval Loss: 0.9942 Eval Acc: 0.6448 (LR: 0.001000)
[2025-05-20 07:46:04,512]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 1.0852 Train Acc: 0.6125 Eval Loss: 0.9645 Eval Acc: 0.6597 (LR: 0.001000)
[2025-05-20 07:46:30,630]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 1.0778 Train Acc: 0.6162 Eval Loss: 0.9612 Eval Acc: 0.6598 (LR: 0.001000)
[2025-05-20 07:46:56,903]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 1.0817 Train Acc: 0.6141 Eval Loss: 0.9743 Eval Acc: 0.6501 (LR: 0.001000)
[2025-05-20 07:47:23,146]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 1.0809 Train Acc: 0.6176 Eval Loss: 0.9825 Eval Acc: 0.6500 (LR: 0.000100)
[2025-05-20 07:47:49,183]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 1.0534 Train Acc: 0.6249 Eval Loss: 0.9436 Eval Acc: 0.6671 (LR: 0.000100)
[2025-05-20 07:48:15,289]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 1.0472 Train Acc: 0.6260 Eval Loss: 0.9676 Eval Acc: 0.6573 (LR: 0.000100)
[2025-05-20 07:48:41,552]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 1.0520 Train Acc: 0.6251 Eval Loss: 0.9589 Eval Acc: 0.6609 (LR: 0.000100)
[2025-05-20 07:49:07,928]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 1.0491 Train Acc: 0.6243 Eval Loss: 0.9515 Eval Acc: 0.6592 (LR: 0.000100)
[2025-05-20 07:49:33,954]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 1.0540 Train Acc: 0.6241 Eval Loss: 0.9536 Eval Acc: 0.6626 (LR: 0.000100)
[2025-05-20 07:49:59,890]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 1.0501 Train Acc: 0.6260 Eval Loss: 0.9452 Eval Acc: 0.6629 (LR: 0.000100)
[2025-05-20 07:50:25,484]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 1.0488 Train Acc: 0.6275 Eval Loss: 0.9554 Eval Acc: 0.6616 (LR: 0.000100)
[2025-05-20 07:50:51,532]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 1.0532 Train Acc: 0.6249 Eval Loss: 0.9523 Eval Acc: 0.6611 (LR: 0.000100)
[2025-05-20 07:51:17,577]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 1.0506 Train Acc: 0.6265 Eval Loss: 0.9666 Eval Acc: 0.6587 (LR: 0.000100)
[2025-05-20 07:51:43,656]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 1.0470 Train Acc: 0.6259 Eval Loss: 0.9552 Eval Acc: 0.6630 (LR: 0.000100)
[2025-05-20 07:52:09,581]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 1.0497 Train Acc: 0.6259 Eval Loss: 0.9440 Eval Acc: 0.6679 (LR: 0.000100)
[2025-05-20 07:52:35,634]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 1.0479 Train Acc: 0.6278 Eval Loss: 0.9621 Eval Acc: 0.6594 (LR: 0.000100)
[2025-05-20 07:53:01,772]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 1.0506 Train Acc: 0.6278 Eval Loss: 0.9573 Eval Acc: 0.6604 (LR: 0.000100)
[2025-05-20 07:53:27,844]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 1.0502 Train Acc: 0.6242 Eval Loss: 0.9442 Eval Acc: 0.6641 (LR: 0.000100)
[2025-05-20 07:53:54,211]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 1.0545 Train Acc: 0.6242 Eval Loss: 0.9552 Eval Acc: 0.6616 (LR: 0.000010)
[2025-05-20 07:54:20,404]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 1.0361 Train Acc: 0.6323 Eval Loss: 0.9382 Eval Acc: 0.6712 (LR: 0.000010)
[2025-05-20 07:54:46,247]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 1.0365 Train Acc: 0.6292 Eval Loss: 0.9371 Eval Acc: 0.6669 (LR: 0.000010)
[2025-05-20 07:55:12,200]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 1.0318 Train Acc: 0.6319 Eval Loss: 0.9483 Eval Acc: 0.6630 (LR: 0.000010)
[2025-05-20 07:55:38,043]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 1.0372 Train Acc: 0.6310 Eval Loss: 0.9445 Eval Acc: 0.6685 (LR: 0.000010)
[2025-05-20 07:56:03,735]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 1.0355 Train Acc: 0.6308 Eval Loss: 0.9438 Eval Acc: 0.6675 (LR: 0.000010)
[2025-05-20 07:56:29,592]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 1.0360 Train Acc: 0.6319 Eval Loss: 0.9474 Eval Acc: 0.6645 (LR: 0.000010)
[2025-05-20 07:56:55,739]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 1.0375 Train Acc: 0.6325 Eval Loss: 0.9431 Eval Acc: 0.6639 (LR: 0.000010)
[2025-05-20 07:57:21,778]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 1.0419 Train Acc: 0.6291 Eval Loss: 0.9573 Eval Acc: 0.6622 (LR: 0.000010)
[2025-05-20 07:57:47,526]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 1.0385 Train Acc: 0.6279 Eval Loss: 0.9398 Eval Acc: 0.6673 (LR: 0.000010)
[2025-05-20 07:58:13,797]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 1.0423 Train Acc: 0.6297 Eval Loss: 0.9462 Eval Acc: 0.6678 (LR: 0.000010)
[2025-05-20 07:58:40,116]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 1.0413 Train Acc: 0.6306 Eval Loss: 0.9563 Eval Acc: 0.6637 (LR: 0.000010)
[2025-05-20 07:59:06,135]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 1.0399 Train Acc: 0.6303 Eval Loss: 0.9342 Eval Acc: 0.6700 (LR: 0.000010)
[2025-05-20 07:59:31,925]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 1.0437 Train Acc: 0.6285 Eval Loss: 0.9444 Eval Acc: 0.6626 (LR: 0.000010)
[2025-05-20 07:59:57,750]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 1.0335 Train Acc: 0.6316 Eval Loss: 0.9420 Eval Acc: 0.6673 (LR: 0.000010)
[2025-05-20 08:00:23,456]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 1.0440 Train Acc: 0.6281 Eval Loss: 0.9521 Eval Acc: 0.6642 (LR: 0.000010)
[2025-05-20 08:00:23,457]: [LeNet5_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.6712
[2025-05-20 08:00:23,471]: 


Quantization of model down to 3 bits finished
[2025-05-20 08:00:23,471]: Model Architecture:
[2025-05-20 08:00:23,480]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1868], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6539568901062012, max_val=0.6537650227546692)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8678], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3248036801815033, max_val=0.3407031297683716)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9195], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1535], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43624815344810486, max_val=0.638314425945282)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8249], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 08:00:23,480]: 
Model Weights:
[2025-05-20 08:00:23,480]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-20 08:00:23,481]: Sample Values (25 elements): [-0.07568663358688354, 0.5375753045082092, -0.1931406706571579, -0.060172729194164276, -0.27152448892593384, 0.17144133150577545, -1.0517953634262085, 0.1218462735414505, -0.5925562977790833, -0.0635402724146843, -0.13865502178668976, -0.5856533646583557, 0.05735008046030998, -0.25944605469703674, -0.15800800919532776, -0.49771207571029663, -0.20275463163852692, -0.07067659497261047, 0.21088162064552307, 0.6768775582313538, -0.5261800289154053, -0.014929928816854954, -0.7104833722114563, 0.21497595310211182, -0.1267373263835907]
[2025-05-20 08:00:23,481]: Mean: 0.00086458
[2025-05-20 08:00:23,481]: Min: -1.31180561
[2025-05-20 08:00:23,481]: Max: 1.24743843
[2025-05-20 08:00:23,482]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-20 08:00:23,483]: Sample Values (25 elements): [-0.18681742250919342, 0.18681742250919342, -0.18681742250919342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18681742250919342, -0.18681742250919342, 0.5604522824287415, 0.0, -0.5604522824287415, 0.18681742250919342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18681742250919342, 0.0, 0.18681742250919342, -0.18681742250919342, 0.18681742250919342]
[2025-05-20 08:00:23,483]: Mean: -0.01206529
[2025-05-20 08:00:23,483]: Min: -0.74726969
[2025-05-20 08:00:23,483]: Max: 0.56045228
[2025-05-20 08:00:23,484]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-20 08:00:23,485]: Sample Values (25 elements): [0.0, -0.09507240355014801, 0.09507240355014801, -0.09507240355014801, 0.09507240355014801, 0.0, 0.0, -0.09507240355014801, -0.09507240355014801, -0.09507240355014801, 0.09507240355014801, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09507240355014801, -0.09507240355014801, 0.0, 0.0, 0.0, 0.0]
[2025-05-20 08:00:23,485]: Mean: -0.00182816
[2025-05-20 08:00:23,485]: Min: -0.28521723
[2025-05-20 08:00:23,485]: Max: 0.38028961
[2025-05-20 08:00:23,486]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-20 08:00:23,487]: Sample Values (25 elements): [0.15350890159606934, -0.15350890159606934, 0.0, 0.0, 0.0, 0.30701780319213867, 0.15350890159606934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15350890159606934, 0.0, -0.15350890159606934, 0.0, 0.0, 0.0, 0.0, -0.15350890159606934, -0.15350890159606934, 0.0, 0.0, 0.0, -0.15350890159606934]
[2025-05-20 08:00:23,487]: Mean: -0.00510173
[2025-05-20 08:00:23,487]: Min: -0.46052670
[2025-05-20 08:00:23,487]: Max: 0.61403561
[2025-05-20 08:00:23,487]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-20 08:00:23,488]: Sample Values (25 elements): [0.06363468617200851, -0.1418250948190689, -0.21795427799224854, 0.044646840542554855, 0.11398376524448395, -0.042097363620996475, 0.0015737841604277492, -0.005569166969507933, 0.024713940918445587, -0.061221711337566376, 0.03888968005776405, -0.009499826468527317, 0.0612272173166275, -0.015266701579093933, 0.13241629302501678, 0.14173126220703125, -0.07771002501249313, -0.02613385207951069, -0.0018667466938495636, 0.11879169940948486, 0.02049226686358452, -0.05229968950152397, 0.041117846965789795, -0.003982096444815397, -0.11081754416227341]
[2025-05-20 08:00:23,488]: Mean: -0.00070430
[2025-05-20 08:00:23,488]: Min: -0.23185444
[2025-05-20 08:00:23,488]: Max: 0.23862228
[2025-05-20 08:00:23,488]: 


QAT of LeNet5 with parametrized_relu down to 4 bits...
[2025-05-20 08:00:23,502]: [LeNet5_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-05-20 08:00:23,508]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 08:00:49,552]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 1.3165 Train Acc: 0.5299 Eval Loss: 1.1728 Eval Acc: 0.5747 (LR: 0.010000)
[2025-05-20 08:01:15,338]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 1.2514 Train Acc: 0.5537 Eval Loss: 1.0818 Eval Acc: 0.6118 (LR: 0.010000)
[2025-05-20 08:01:41,325]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 1.2244 Train Acc: 0.5639 Eval Loss: 1.0577 Eval Acc: 0.6266 (LR: 0.010000)
[2025-05-20 08:02:07,015]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 1.2113 Train Acc: 0.5710 Eval Loss: 1.0828 Eval Acc: 0.6190 (LR: 0.010000)
[2025-05-20 08:02:32,681]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 1.1971 Train Acc: 0.5698 Eval Loss: 1.0221 Eval Acc: 0.6385 (LR: 0.010000)
[2025-05-20 08:02:58,195]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 1.1838 Train Acc: 0.5793 Eval Loss: 1.0461 Eval Acc: 0.6267 (LR: 0.010000)
[2025-05-20 08:03:23,668]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 1.1839 Train Acc: 0.5764 Eval Loss: 1.0521 Eval Acc: 0.6273 (LR: 0.010000)
[2025-05-20 08:03:51,422]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 1.1750 Train Acc: 0.5827 Eval Loss: 1.0652 Eval Acc: 0.6174 (LR: 0.010000)
[2025-05-20 08:04:18,733]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 1.1679 Train Acc: 0.5856 Eval Loss: 1.0022 Eval Acc: 0.6461 (LR: 0.010000)
[2025-05-20 08:04:45,945]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 1.1471 Train Acc: 0.5907 Eval Loss: 1.0986 Eval Acc: 0.6181 (LR: 0.010000)
[2025-05-20 08:05:13,916]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 1.1420 Train Acc: 0.5933 Eval Loss: 1.0866 Eval Acc: 0.6137 (LR: 0.010000)
[2025-05-20 08:05:41,799]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 1.1510 Train Acc: 0.5886 Eval Loss: 1.0463 Eval Acc: 0.6269 (LR: 0.010000)
[2025-05-20 08:06:09,398]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 1.1457 Train Acc: 0.5913 Eval Loss: 1.0340 Eval Acc: 0.6271 (LR: 0.010000)
[2025-05-20 08:06:37,246]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 1.1415 Train Acc: 0.5931 Eval Loss: 1.0056 Eval Acc: 0.6456 (LR: 0.010000)
[2025-05-20 08:07:04,467]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 1.1344 Train Acc: 0.5956 Eval Loss: 1.0220 Eval Acc: 0.6347 (LR: 0.001000)
[2025-05-20 08:07:35,803]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 1.0359 Train Acc: 0.6332 Eval Loss: 0.9187 Eval Acc: 0.6745 (LR: 0.001000)
[2025-05-20 08:08:04,235]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 1.0195 Train Acc: 0.6375 Eval Loss: 0.9053 Eval Acc: 0.6783 (LR: 0.001000)
[2025-05-20 08:08:31,542]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 1.0153 Train Acc: 0.6380 Eval Loss: 0.9152 Eval Acc: 0.6764 (LR: 0.001000)
[2025-05-20 08:08:58,065]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 1.0087 Train Acc: 0.6426 Eval Loss: 0.9016 Eval Acc: 0.6826 (LR: 0.001000)
[2025-05-20 08:09:26,630]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 1.0072 Train Acc: 0.6409 Eval Loss: 0.9108 Eval Acc: 0.6761 (LR: 0.001000)
[2025-05-20 08:09:54,317]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.9980 Train Acc: 0.6435 Eval Loss: 0.8878 Eval Acc: 0.6834 (LR: 0.001000)
[2025-05-20 08:10:21,695]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 1.0050 Train Acc: 0.6411 Eval Loss: 0.9113 Eval Acc: 0.6760 (LR: 0.001000)
[2025-05-20 08:10:48,326]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.9977 Train Acc: 0.6465 Eval Loss: 0.9003 Eval Acc: 0.6772 (LR: 0.001000)
[2025-05-20 08:11:16,120]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.9977 Train Acc: 0.6454 Eval Loss: 0.8911 Eval Acc: 0.6834 (LR: 0.001000)
[2025-05-20 08:11:43,519]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.9985 Train Acc: 0.6440 Eval Loss: 0.9206 Eval Acc: 0.6753 (LR: 0.001000)
[2025-05-20 08:12:10,909]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.9940 Train Acc: 0.6477 Eval Loss: 0.9010 Eval Acc: 0.6819 (LR: 0.001000)
[2025-05-20 08:12:37,732]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.9935 Train Acc: 0.6468 Eval Loss: 0.8993 Eval Acc: 0.6814 (LR: 0.001000)
[2025-05-20 08:13:05,173]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.9871 Train Acc: 0.6484 Eval Loss: 0.8847 Eval Acc: 0.6886 (LR: 0.001000)
[2025-05-20 08:13:32,944]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.9917 Train Acc: 0.6473 Eval Loss: 0.8994 Eval Acc: 0.6803 (LR: 0.001000)
[2025-05-20 08:14:00,925]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.9929 Train Acc: 0.6453 Eval Loss: 0.8941 Eval Acc: 0.6826 (LR: 0.000100)
[2025-05-20 08:14:27,822]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.9678 Train Acc: 0.6580 Eval Loss: 0.8752 Eval Acc: 0.6900 (LR: 0.000100)
[2025-05-20 08:14:56,006]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.9614 Train Acc: 0.6577 Eval Loss: 0.8750 Eval Acc: 0.6913 (LR: 0.000100)
[2025-05-20 08:15:22,608]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.9613 Train Acc: 0.6594 Eval Loss: 0.8746 Eval Acc: 0.6902 (LR: 0.000100)
[2025-05-20 08:15:48,870]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.9632 Train Acc: 0.6574 Eval Loss: 0.8744 Eval Acc: 0.6901 (LR: 0.000100)
[2025-05-20 08:16:14,962]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.9630 Train Acc: 0.6565 Eval Loss: 0.8736 Eval Acc: 0.6932 (LR: 0.000100)
[2025-05-20 08:16:41,133]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.9656 Train Acc: 0.6565 Eval Loss: 0.8787 Eval Acc: 0.6887 (LR: 0.000100)
[2025-05-20 08:17:07,313]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.9617 Train Acc: 0.6586 Eval Loss: 0.8762 Eval Acc: 0.6916 (LR: 0.000100)
[2025-05-20 08:17:33,634]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.9628 Train Acc: 0.6571 Eval Loss: 0.8758 Eval Acc: 0.6924 (LR: 0.000100)
[2025-05-20 08:17:59,718]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.9656 Train Acc: 0.6589 Eval Loss: 0.8728 Eval Acc: 0.6907 (LR: 0.000100)
[2025-05-20 08:18:25,826]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.9597 Train Acc: 0.6594 Eval Loss: 0.8746 Eval Acc: 0.6890 (LR: 0.000100)
[2025-05-20 08:18:53,139]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.9577 Train Acc: 0.6599 Eval Loss: 0.8804 Eval Acc: 0.6887 (LR: 0.000100)
[2025-05-20 08:19:19,641]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.9620 Train Acc: 0.6577 Eval Loss: 0.8775 Eval Acc: 0.6890 (LR: 0.000100)
[2025-05-20 08:19:45,587]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.9578 Train Acc: 0.6591 Eval Loss: 0.8790 Eval Acc: 0.6907 (LR: 0.000100)
[2025-05-20 08:20:11,260]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.9656 Train Acc: 0.6572 Eval Loss: 0.8725 Eval Acc: 0.6904 (LR: 0.000100)
[2025-05-20 08:20:37,402]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.9635 Train Acc: 0.6575 Eval Loss: 0.8689 Eval Acc: 0.6901 (LR: 0.000010)
[2025-05-20 08:21:03,498]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.9559 Train Acc: 0.6604 Eval Loss: 0.8649 Eval Acc: 0.6921 (LR: 0.000010)
[2025-05-20 08:21:30,622]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.9510 Train Acc: 0.6639 Eval Loss: 0.8693 Eval Acc: 0.6949 (LR: 0.000010)
[2025-05-20 08:21:57,125]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.9511 Train Acc: 0.6623 Eval Loss: 0.8665 Eval Acc: 0.6944 (LR: 0.000010)
[2025-05-20 08:22:23,275]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.9550 Train Acc: 0.6627 Eval Loss: 0.8667 Eval Acc: 0.6921 (LR: 0.000010)
[2025-05-20 08:22:49,447]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.9530 Train Acc: 0.6610 Eval Loss: 0.8648 Eval Acc: 0.6945 (LR: 0.000010)
[2025-05-20 08:23:15,388]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.9542 Train Acc: 0.6629 Eval Loss: 0.8689 Eval Acc: 0.6930 (LR: 0.000010)
[2025-05-20 08:23:41,453]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.9514 Train Acc: 0.6610 Eval Loss: 0.8657 Eval Acc: 0.6930 (LR: 0.000010)
[2025-05-20 08:24:08,221]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.9516 Train Acc: 0.6599 Eval Loss: 0.8670 Eval Acc: 0.6937 (LR: 0.000010)
[2025-05-20 08:24:36,787]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.9556 Train Acc: 0.6581 Eval Loss: 0.8705 Eval Acc: 0.6925 (LR: 0.000010)
[2025-05-20 08:25:17,512]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.9609 Train Acc: 0.6579 Eval Loss: 0.8731 Eval Acc: 0.6911 (LR: 0.000010)
[2025-05-20 08:25:50,434]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.9542 Train Acc: 0.6610 Eval Loss: 0.8623 Eval Acc: 0.6948 (LR: 0.000010)
[2025-05-20 08:26:18,465]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.9589 Train Acc: 0.6604 Eval Loss: 0.8670 Eval Acc: 0.6915 (LR: 0.000010)
[2025-05-20 08:26:46,969]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.9589 Train Acc: 0.6574 Eval Loss: 0.8633 Eval Acc: 0.6948 (LR: 0.000010)
[2025-05-20 08:27:14,273]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.9571 Train Acc: 0.6608 Eval Loss: 0.8666 Eval Acc: 0.6958 (LR: 0.000010)
[2025-05-20 08:27:40,766]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.9609 Train Acc: 0.6549 Eval Loss: 0.8739 Eval Acc: 0.6912 (LR: 0.000010)
[2025-05-20 08:27:40,767]: [LeNet5_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.6958
[2025-05-20 08:27:40,781]: 


Quantization of model down to 4 bits finished
[2025-05-20 08:27:40,781]: Model Architecture:
[2025-05-20 08:27:40,792]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0850], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6212023496627808, max_val=0.6545447707176208)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4092], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3550466299057007, max_val=0.28676000237464905)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4217], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0612], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33657705783843994, max_val=0.5813120007514954)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3817], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-20 08:27:40,792]: 
Model Weights:
[2025-05-20 08:27:40,792]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-20 08:27:40,793]: Sample Values (25 elements): [-0.213178813457489, 0.2125009149312973, -0.6099887490272522, -0.08024958521127701, 0.030511438846588135, -0.3795336186885834, -0.024094341322779655, -0.7660062909126282, 0.10916711390018463, 0.01617787405848503, 0.0724262073636055, -0.09873675554990768, -0.13908834755420685, 0.2846261262893677, -0.25771433115005493, 0.8033139705657959, -0.31840410828590393, -0.035172298550605774, 0.01273361872881651, -0.5429187417030334, 0.37554019689559937, 0.10226798057556152, -0.12516075372695923, -0.028440400958061218, 0.7436060905456543]
[2025-05-20 08:27:40,793]: Mean: 0.00067438
[2025-05-20 08:27:40,793]: Min: -1.33520019
[2025-05-20 08:27:40,793]: Max: 1.26489377
[2025-05-20 08:27:40,794]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-20 08:27:40,795]: Sample Values (25 elements): [0.0850498229265213, -0.2551494836807251, 0.0850498229265213, -0.0850498229265213, 0.1700996458530426, -0.2551494836807251, 0.0, 0.0850498229265213, -0.1700996458530426, 0.3401992917060852, -0.3401992917060852, -0.1700996458530426, 0.0, -0.3401992917060852, 0.1700996458530426, 0.0, 0.0, -0.0850498229265213, 0.1700996458530426, -0.5953487753868103, 0.0850498229265213, 0.2551494836807251, 0.0, 0.1700996458530426, 0.1700996458530426]
[2025-05-20 08:27:40,795]: Mean: -0.00691030
[2025-05-20 08:27:40,795]: Min: -0.59534878
[2025-05-20 08:27:40,795]: Max: 0.68039858
[2025-05-20 08:27:40,796]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-20 08:27:40,797]: Sample Values (25 elements): [-0.08557421714067459, -0.042787108570337296, 0.0, 0.0, 0.08557421714067459, -0.042787108570337296, 0.0, -0.042787108570337296, 0.042787108570337296, -0.042787108570337296, -0.042787108570337296, 0.08557421714067459, 0.042787108570337296, 0.0, -0.042787108570337296, -0.08557421714067459, 0.08557421714067459, -0.08557421714067459, 0.0, 0.08557421714067459, -0.042787108570337296, 0.0, 0.042787108570337296, 0.042787108570337296, -0.08557421714067459]
[2025-05-20 08:27:40,797]: Mean: -0.00059635
[2025-05-20 08:27:40,797]: Min: -0.34229687
[2025-05-20 08:27:40,798]: Max: 0.29950976
[2025-05-20 08:27:40,799]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-20 08:27:40,799]: Sample Values (25 elements): [0.061192598193883896, 0.061192598193883896, -0.061192598193883896, -0.12238519638776779, 0.0, 0.12238519638776779, 0.0, -0.12238519638776779, 0.061192598193883896, 0.0, 0.061192598193883896, 0.1835777908563614, 0.12238519638776779, 0.0, 0.12238519638776779, 0.061192598193883896, -0.061192598193883896, 0.1835777908563614, 0.0, 0.0, 0.061192598193883896, -0.12238519638776779, -0.12238519638776779, -0.12238519638776779, -0.061192598193883896]
[2025-05-20 08:27:40,799]: Mean: -0.00417057
[2025-05-20 08:27:40,799]: Min: -0.36715558
[2025-05-20 08:27:40,800]: Max: 0.55073339
[2025-05-20 08:27:40,800]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-20 08:27:40,800]: Sample Values (25 elements): [-0.10678941011428833, 0.03552574664354324, 0.007748596835881472, -0.03631617873907089, 0.12237390875816345, 0.021512001752853394, 0.0011032522888854146, -0.0030284838285297155, 0.031757909804582596, -0.013241873122751713, 0.004131551366299391, 0.016057463362812996, 0.07801036536693573, 0.19953389465808868, -0.03174777328968048, 0.043819330632686615, -0.08858372271060944, 0.0918167382478714, 0.12709985673427582, 0.04955124855041504, -0.0767495408654213, 0.12850797176361084, -0.12113839387893677, 0.00015059452562127262, -0.14348427951335907]
[2025-05-20 08:27:40,800]: Mean: -0.00070434
[2025-05-20 08:27:40,800]: Min: -0.31578428
[2025-05-20 08:27:40,801]: Max: 0.26907250
