[2025-05-21 18:48:14,605]: Checkpoint of model at path [checkpoint/LeNet5_relu6.ckpt] will be used for QAT
[2025-05-21 18:48:14,605]: 


QAT of LeNet5 with parametrized_relu down to 2 bits...
[2025-05-21 18:48:14,633]: [LeNet5_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-05-21 18:48:14,639]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-21 18:48:39,352]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 1.6560 Train Acc: 0.4012 Eval Loss: 1.3455 Eval Acc: 0.4988 (LR: 0.010000)
[2025-05-21 18:49:03,676]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 1.5177 Train Acc: 0.4455 Eval Loss: 1.4035 Eval Acc: 0.4910 (LR: 0.010000)
[2025-05-21 18:49:27,284]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.4788 Train Acc: 0.4628 Eval Loss: 1.3369 Eval Acc: 0.5056 (LR: 0.010000)
[2025-05-21 18:49:50,675]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.4721 Train Acc: 0.4651 Eval Loss: 1.3638 Eval Acc: 0.5063 (LR: 0.010000)
[2025-05-21 18:50:15,653]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.4634 Train Acc: 0.4694 Eval Loss: 1.3505 Eval Acc: 0.5219 (LR: 0.010000)
[2025-05-21 18:50:41,524]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.4469 Train Acc: 0.4778 Eval Loss: 1.2622 Eval Acc: 0.5415 (LR: 0.010000)
[2025-05-21 18:51:09,376]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.4354 Train Acc: 0.4806 Eval Loss: 1.2852 Eval Acc: 0.5330 (LR: 0.010000)
[2025-05-21 18:51:35,926]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.4387 Train Acc: 0.4785 Eval Loss: 1.2651 Eval Acc: 0.5410 (LR: 0.010000)
[2025-05-21 18:52:02,471]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.4315 Train Acc: 0.4844 Eval Loss: 1.3053 Eval Acc: 0.5271 (LR: 0.010000)
[2025-05-21 18:52:28,952]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.4137 Train Acc: 0.4874 Eval Loss: 1.3027 Eval Acc: 0.5307 (LR: 0.010000)
[2025-05-21 18:52:55,385]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.4069 Train Acc: 0.4919 Eval Loss: 1.2819 Eval Acc: 0.5381 (LR: 0.010000)
[2025-05-21 18:53:21,910]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.4099 Train Acc: 0.4911 Eval Loss: 1.2839 Eval Acc: 0.5367 (LR: 0.010000)
[2025-05-21 18:53:48,349]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.4130 Train Acc: 0.4913 Eval Loss: 1.3208 Eval Acc: 0.5206 (LR: 0.010000)
[2025-05-21 18:54:14,815]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.4022 Train Acc: 0.4964 Eval Loss: 1.2718 Eval Acc: 0.5457 (LR: 0.010000)
[2025-05-21 18:54:41,427]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.4033 Train Acc: 0.4957 Eval Loss: 1.2910 Eval Acc: 0.5361 (LR: 0.001000)
[2025-05-21 18:55:07,886]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.3106 Train Acc: 0.5266 Eval Loss: 1.1915 Eval Acc: 0.5678 (LR: 0.001000)
[2025-05-21 18:55:34,426]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.2953 Train Acc: 0.5334 Eval Loss: 1.1983 Eval Acc: 0.5721 (LR: 0.001000)
[2025-05-21 18:56:00,538]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.2902 Train Acc: 0.5353 Eval Loss: 1.1663 Eval Acc: 0.5810 (LR: 0.001000)
[2025-05-21 18:56:26,861]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.2936 Train Acc: 0.5335 Eval Loss: 1.1624 Eval Acc: 0.5842 (LR: 0.001000)
[2025-05-21 18:56:53,773]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.2882 Train Acc: 0.5362 Eval Loss: 1.1983 Eval Acc: 0.5660 (LR: 0.001000)
[2025-05-21 18:57:20,112]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.2967 Train Acc: 0.5328 Eval Loss: 1.2436 Eval Acc: 0.5529 (LR: 0.001000)
[2025-05-21 18:57:46,535]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.2969 Train Acc: 0.5351 Eval Loss: 1.1780 Eval Acc: 0.5732 (LR: 0.001000)
[2025-05-21 18:58:13,158]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.2917 Train Acc: 0.5365 Eval Loss: 1.2285 Eval Acc: 0.5659 (LR: 0.001000)
[2025-05-21 18:58:39,800]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.2912 Train Acc: 0.5357 Eval Loss: 1.2378 Eval Acc: 0.5578 (LR: 0.001000)
[2025-05-21 18:59:06,392]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.2992 Train Acc: 0.5344 Eval Loss: 1.1704 Eval Acc: 0.5749 (LR: 0.001000)
[2025-05-21 18:59:33,018]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.2999 Train Acc: 0.5304 Eval Loss: 1.2292 Eval Acc: 0.5620 (LR: 0.001000)
[2025-05-21 19:00:00,411]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.2974 Train Acc: 0.5329 Eval Loss: 1.1723 Eval Acc: 0.5801 (LR: 0.001000)
[2025-05-21 19:00:26,990]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.2978 Train Acc: 0.5312 Eval Loss: 1.1860 Eval Acc: 0.5706 (LR: 0.001000)
[2025-05-21 19:00:53,201]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.2974 Train Acc: 0.5324 Eval Loss: 1.1956 Eval Acc: 0.5676 (LR: 0.001000)
[2025-05-21 19:01:19,275]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.2934 Train Acc: 0.5352 Eval Loss: 1.1839 Eval Acc: 0.5780 (LR: 0.000100)
[2025-05-21 19:01:45,198]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.2566 Train Acc: 0.5482 Eval Loss: 1.1588 Eval Acc: 0.5796 (LR: 0.000100)
[2025-05-21 19:02:11,481]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.2524 Train Acc: 0.5478 Eval Loss: 1.1755 Eval Acc: 0.5753 (LR: 0.000100)
[2025-05-21 19:02:37,559]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.2552 Train Acc: 0.5465 Eval Loss: 1.1465 Eval Acc: 0.5861 (LR: 0.000100)
[2025-05-21 19:03:03,717]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.2518 Train Acc: 0.5493 Eval Loss: 1.1596 Eval Acc: 0.5869 (LR: 0.000100)
[2025-05-21 19:03:29,895]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.2618 Train Acc: 0.5442 Eval Loss: 1.1948 Eval Acc: 0.5707 (LR: 0.000100)
[2025-05-21 19:03:55,960]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.2585 Train Acc: 0.5467 Eval Loss: 1.1582 Eval Acc: 0.5813 (LR: 0.000100)
[2025-05-21 19:04:23,057]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.2624 Train Acc: 0.5454 Eval Loss: 1.1541 Eval Acc: 0.5875 (LR: 0.000100)
[2025-05-21 19:04:49,578]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.2576 Train Acc: 0.5468 Eval Loss: 1.2191 Eval Acc: 0.5680 (LR: 0.000100)
[2025-05-21 19:05:15,868]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.2560 Train Acc: 0.5486 Eval Loss: 1.1771 Eval Acc: 0.5822 (LR: 0.000100)
[2025-05-21 19:05:42,358]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.2586 Train Acc: 0.5488 Eval Loss: 1.1661 Eval Acc: 0.5842 (LR: 0.000100)
[2025-05-21 19:06:08,713]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 1.2621 Train Acc: 0.5460 Eval Loss: 1.1612 Eval Acc: 0.5785 (LR: 0.000100)
[2025-05-21 19:06:34,848]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 1.2571 Train Acc: 0.5472 Eval Loss: 1.1508 Eval Acc: 0.5864 (LR: 0.000100)
[2025-05-21 19:07:00,960]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 1.2660 Train Acc: 0.5464 Eval Loss: 1.1462 Eval Acc: 0.5848 (LR: 0.000100)
[2025-05-21 19:07:26,939]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 1.2593 Train Acc: 0.5449 Eval Loss: 1.1629 Eval Acc: 0.5806 (LR: 0.000100)
[2025-05-21 19:07:52,907]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 1.2630 Train Acc: 0.5430 Eval Loss: 1.1546 Eval Acc: 0.5814 (LR: 0.000010)
[2025-05-21 19:08:18,821]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 1.2390 Train Acc: 0.5548 Eval Loss: 1.1535 Eval Acc: 0.5883 (LR: 0.000010)
[2025-05-21 19:08:44,805]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 1.2396 Train Acc: 0.5513 Eval Loss: 1.1449 Eval Acc: 0.5871 (LR: 0.000010)
[2025-05-21 19:09:10,505]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 1.2459 Train Acc: 0.5506 Eval Loss: 1.1372 Eval Acc: 0.5958 (LR: 0.000010)
[2025-05-21 19:09:37,765]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 1.2423 Train Acc: 0.5513 Eval Loss: 1.1621 Eval Acc: 0.5813 (LR: 0.000010)
[2025-05-21 19:10:02,854]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 1.2399 Train Acc: 0.5522 Eval Loss: 1.1501 Eval Acc: 0.5871 (LR: 0.000010)
[2025-05-21 19:10:28,740]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 1.2499 Train Acc: 0.5514 Eval Loss: 1.1405 Eval Acc: 0.5885 (LR: 0.000010)
[2025-05-21 19:10:53,296]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 1.2481 Train Acc: 0.5490 Eval Loss: 1.1379 Eval Acc: 0.5889 (LR: 0.000010)
[2025-05-21 19:11:17,963]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 1.2491 Train Acc: 0.5506 Eval Loss: 1.1628 Eval Acc: 0.5801 (LR: 0.000010)
[2025-05-21 19:11:44,276]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 1.2487 Train Acc: 0.5509 Eval Loss: 1.1394 Eval Acc: 0.5883 (LR: 0.000010)
[2025-05-21 19:12:08,135]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 1.2456 Train Acc: 0.5522 Eval Loss: 1.1435 Eval Acc: 0.5875 (LR: 0.000010)
[2025-05-21 19:12:32,023]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 1.2526 Train Acc: 0.5485 Eval Loss: 1.1433 Eval Acc: 0.5905 (LR: 0.000010)
[2025-05-21 19:12:58,046]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 1.2491 Train Acc: 0.5510 Eval Loss: 1.1681 Eval Acc: 0.5788 (LR: 0.000010)
[2025-05-21 19:13:22,773]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 1.2496 Train Acc: 0.5498 Eval Loss: 1.1531 Eval Acc: 0.5850 (LR: 0.000010)
[2025-05-21 19:13:46,532]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 1.2513 Train Acc: 0.5506 Eval Loss: 1.1485 Eval Acc: 0.5853 (LR: 0.000010)
[2025-05-21 19:14:10,302]: [LeNet5_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 1.2586 Train Acc: 0.5466 Eval Loss: 1.1443 Eval Acc: 0.5930 (LR: 0.000010)
[2025-05-21 19:14:10,302]: [LeNet5_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.5958
[2025-05-21 19:14:10,331]: 


Quantization of model down to 2 bits finished
[2025-05-21 19:14:10,331]: Model Architecture:
[2025-05-21 19:14:10,342]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5217], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7199785113334656, max_val=0.84515380859375)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1296], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2088], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31062638759613037, max_val=0.315798282623291)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2396], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2756], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4079075753688812, max_val=0.4189003109931946)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8763], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-21 19:14:10,342]: 
Model Weights:
[2025-05-21 19:14:10,342]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-21 19:14:10,342]: Sample Values (25 elements): [0.03529830649495125, 0.3999798595905304, -8.03660586825572e-05, -0.03893500566482544, -0.35241571068763733, -0.013203262351453304, 0.0705529972910881, 0.2974699139595032, 0.3121255040168762, -0.23178017139434814, 0.2689836025238037, 0.7665638327598572, 0.03689945861697197, -0.4959525763988495, 0.4148119390010834, -0.11839456856250763, -0.047370545566082, 0.5002229809761047, -0.5593968629837036, 0.6119852066040039, 0.06597759574651718, 0.23619163036346436, 0.3492225706577301, 0.12509037554264069, -0.0505591444671154]
[2025-05-21 19:14:10,342]: Mean: 0.00110480
[2025-05-21 19:14:10,342]: Min: -1.30477035
[2025-05-21 19:14:10,343]: Max: 1.23125243
[2025-05-21 19:14:10,344]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-21 19:14:10,344]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.5217108130455017, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5217108130455017, 0.5217108130455017, 0.0, -0.5217108130455017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 19:14:10,345]: Mean: -0.02956362
[2025-05-21 19:14:10,345]: Min: -0.52171081
[2025-05-21 19:14:10,345]: Max: 1.04342163
[2025-05-21 19:14:10,346]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-21 19:14:10,347]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20880822837352753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20880822837352753, -0.20880822837352753, -0.20880822837352753, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 19:14:10,347]: Mean: -0.00248830
[2025-05-21 19:14:10,347]: Min: -0.20880823
[2025-05-21 19:14:10,347]: Max: 0.41761646
[2025-05-21 19:14:10,348]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-21 19:14:10,349]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 19:14:10,349]: Mean: -0.00084759
[2025-05-21 19:14:10,349]: Min: -0.27560264
[2025-05-21 19:14:10,349]: Max: 0.55120528
[2025-05-21 19:14:10,349]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-21 19:14:10,350]: Sample Values (25 elements): [-0.006196696776896715, 0.05634439364075661, -0.029367193579673767, 0.04505912959575653, -0.024582557380199432, 0.0007928137201815844, 0.019104251638054848, 0.021592753008008003, 0.042116884142160416, -0.07513629645109177, 0.06088067591190338, 0.04443732649087906, 0.059894297271966934, -0.07058627903461456, -0.029093433171510696, -0.0580311082303524, -0.03684191033244133, 0.062440983951091766, 0.0858817920088768, 0.04066409170627594, -0.012382552959024906, 0.0011908654123544693, -0.02959061600267887, 0.03909756615757942, -0.04571409523487091]
[2025-05-21 19:14:10,350]: Mean: -0.00070429
[2025-05-21 19:14:10,350]: Min: -0.23713420
[2025-05-21 19:14:10,350]: Max: 0.18175025
[2025-05-21 19:14:10,350]: 


QAT of LeNet5 with parametrized_relu down to 3 bits...
[2025-05-21 19:14:10,365]: [LeNet5_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-05-21 19:14:10,371]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-21 19:14:34,174]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 1.3775 Train Acc: 0.5103 Eval Loss: 1.2106 Eval Acc: 0.5624 (LR: 0.010000)
[2025-05-21 19:14:58,020]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 1.3217 Train Acc: 0.5268 Eval Loss: 1.1997 Eval Acc: 0.5726 (LR: 0.010000)
[2025-05-21 19:15:21,750]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 1.3089 Train Acc: 0.5317 Eval Loss: 1.2186 Eval Acc: 0.5677 (LR: 0.010000)
[2025-05-21 19:15:45,411]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 1.2780 Train Acc: 0.5433 Eval Loss: 1.1269 Eval Acc: 0.5905 (LR: 0.010000)
[2025-05-21 19:16:09,175]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 1.2725 Train Acc: 0.5431 Eval Loss: 1.1088 Eval Acc: 0.6069 (LR: 0.010000)
[2025-05-21 19:16:32,932]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 1.2588 Train Acc: 0.5511 Eval Loss: 1.1483 Eval Acc: 0.5872 (LR: 0.010000)
[2025-05-21 19:16:56,691]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 1.2450 Train Acc: 0.5533 Eval Loss: 1.0911 Eval Acc: 0.6169 (LR: 0.010000)
[2025-05-21 19:17:20,458]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 1.2464 Train Acc: 0.5575 Eval Loss: 1.1084 Eval Acc: 0.6063 (LR: 0.010000)
[2025-05-21 19:17:44,168]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 1.2375 Train Acc: 0.5599 Eval Loss: 1.0866 Eval Acc: 0.6158 (LR: 0.010000)
[2025-05-21 19:18:07,856]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 1.2286 Train Acc: 0.5621 Eval Loss: 1.1201 Eval Acc: 0.6015 (LR: 0.010000)
[2025-05-21 19:18:31,574]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 1.2270 Train Acc: 0.5644 Eval Loss: 1.1100 Eval Acc: 0.6055 (LR: 0.010000)
[2025-05-21 19:18:55,271]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 1.2180 Train Acc: 0.5673 Eval Loss: 1.0557 Eval Acc: 0.6267 (LR: 0.010000)
[2025-05-21 19:19:19,227]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 1.2185 Train Acc: 0.5672 Eval Loss: 1.1339 Eval Acc: 0.5983 (LR: 0.010000)
[2025-05-21 19:19:44,209]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 1.2248 Train Acc: 0.5614 Eval Loss: 1.1638 Eval Acc: 0.5863 (LR: 0.010000)
[2025-05-21 19:20:09,556]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 1.2129 Train Acc: 0.5695 Eval Loss: 1.1088 Eval Acc: 0.6077 (LR: 0.001000)
[2025-05-21 19:20:36,477]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 1.1145 Train Acc: 0.6036 Eval Loss: 0.9836 Eval Acc: 0.6533 (LR: 0.001000)
[2025-05-21 19:21:02,850]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 1.0920 Train Acc: 0.6104 Eval Loss: 0.9794 Eval Acc: 0.6512 (LR: 0.001000)
[2025-05-21 19:21:28,857]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 1.0893 Train Acc: 0.6093 Eval Loss: 1.0094 Eval Acc: 0.6407 (LR: 0.001000)
[2025-05-21 19:21:54,958]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 1.0926 Train Acc: 0.6119 Eval Loss: 0.9690 Eval Acc: 0.6558 (LR: 0.001000)
[2025-05-21 19:22:21,321]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 1.0863 Train Acc: 0.6128 Eval Loss: 0.9832 Eval Acc: 0.6537 (LR: 0.001000)
[2025-05-21 19:22:47,981]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 1.0878 Train Acc: 0.6134 Eval Loss: 0.9726 Eval Acc: 0.6617 (LR: 0.001000)
[2025-05-21 19:23:15,917]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 1.0869 Train Acc: 0.6111 Eval Loss: 0.9676 Eval Acc: 0.6562 (LR: 0.001000)
[2025-05-21 19:23:44,187]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 1.0803 Train Acc: 0.6156 Eval Loss: 0.9749 Eval Acc: 0.6501 (LR: 0.001000)
[2025-05-21 19:24:11,719]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 1.0791 Train Acc: 0.6159 Eval Loss: 0.9680 Eval Acc: 0.6586 (LR: 0.001000)
[2025-05-21 19:24:38,724]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 1.0824 Train Acc: 0.6150 Eval Loss: 1.0059 Eval Acc: 0.6484 (LR: 0.001000)
[2025-05-21 19:25:05,520]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 1.0842 Train Acc: 0.6142 Eval Loss: 0.9808 Eval Acc: 0.6552 (LR: 0.001000)
[2025-05-21 19:25:32,087]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 1.0841 Train Acc: 0.6139 Eval Loss: 0.9687 Eval Acc: 0.6556 (LR: 0.001000)
[2025-05-21 19:25:58,713]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 1.0835 Train Acc: 0.6165 Eval Loss: 0.9457 Eval Acc: 0.6637 (LR: 0.001000)
[2025-05-21 19:26:25,837]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 1.0835 Train Acc: 0.6136 Eval Loss: 0.9653 Eval Acc: 0.6591 (LR: 0.001000)
[2025-05-21 19:26:52,758]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 1.0861 Train Acc: 0.6152 Eval Loss: 0.9678 Eval Acc: 0.6569 (LR: 0.000100)
[2025-05-21 19:27:19,500]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 1.0456 Train Acc: 0.6305 Eval Loss: 0.9384 Eval Acc: 0.6680 (LR: 0.000100)
[2025-05-21 19:27:46,218]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 1.0434 Train Acc: 0.6274 Eval Loss: 0.9488 Eval Acc: 0.6629 (LR: 0.000100)
[2025-05-21 19:28:13,619]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 1.0484 Train Acc: 0.6266 Eval Loss: 0.9401 Eval Acc: 0.6677 (LR: 0.000100)
[2025-05-21 19:28:40,305]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 1.0464 Train Acc: 0.6287 Eval Loss: 0.9401 Eval Acc: 0.6666 (LR: 0.000100)
[2025-05-21 19:29:07,068]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 1.0490 Train Acc: 0.6258 Eval Loss: 0.9401 Eval Acc: 0.6675 (LR: 0.000100)
[2025-05-21 19:29:33,802]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 1.0436 Train Acc: 0.6281 Eval Loss: 0.9399 Eval Acc: 0.6650 (LR: 0.000100)
[2025-05-21 19:30:00,673]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 1.0453 Train Acc: 0.6279 Eval Loss: 0.9391 Eval Acc: 0.6700 (LR: 0.000100)
[2025-05-21 19:30:27,517]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 1.0441 Train Acc: 0.6282 Eval Loss: 0.9535 Eval Acc: 0.6635 (LR: 0.000100)
[2025-05-21 19:30:54,081]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 1.0461 Train Acc: 0.6272 Eval Loss: 0.9443 Eval Acc: 0.6680 (LR: 0.000100)
[2025-05-21 19:31:20,648]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 1.0491 Train Acc: 0.6266 Eval Loss: 0.9426 Eval Acc: 0.6674 (LR: 0.000100)
[2025-05-21 19:31:47,001]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 1.0483 Train Acc: 0.6265 Eval Loss: 0.9390 Eval Acc: 0.6673 (LR: 0.000100)
[2025-05-21 19:32:13,716]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 1.0458 Train Acc: 0.6292 Eval Loss: 0.9346 Eval Acc: 0.6686 (LR: 0.000100)
[2025-05-21 19:32:40,389]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 1.0508 Train Acc: 0.6258 Eval Loss: 0.9416 Eval Acc: 0.6667 (LR: 0.000100)
[2025-05-21 19:33:07,040]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 1.0509 Train Acc: 0.6253 Eval Loss: 0.9520 Eval Acc: 0.6642 (LR: 0.000100)
[2025-05-21 19:33:33,635]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 1.0462 Train Acc: 0.6280 Eval Loss: 0.9540 Eval Acc: 0.6629 (LR: 0.000010)
[2025-05-21 19:34:00,122]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 1.0338 Train Acc: 0.6303 Eval Loss: 0.9353 Eval Acc: 0.6714 (LR: 0.000010)
[2025-05-21 19:34:26,772]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 1.0349 Train Acc: 0.6315 Eval Loss: 0.9465 Eval Acc: 0.6652 (LR: 0.000010)
[2025-05-21 19:34:53,395]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 1.0310 Train Acc: 0.6331 Eval Loss: 0.9328 Eval Acc: 0.6725 (LR: 0.000010)
[2025-05-21 19:35:19,819]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 1.0275 Train Acc: 0.6317 Eval Loss: 0.9335 Eval Acc: 0.6704 (LR: 0.000010)
[2025-05-21 19:35:46,278]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 1.0350 Train Acc: 0.6318 Eval Loss: 0.9337 Eval Acc: 0.6676 (LR: 0.000010)
[2025-05-21 19:36:12,692]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 1.0362 Train Acc: 0.6282 Eval Loss: 0.9310 Eval Acc: 0.6728 (LR: 0.000010)
[2025-05-21 19:36:39,222]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 1.0353 Train Acc: 0.6311 Eval Loss: 0.9344 Eval Acc: 0.6687 (LR: 0.000010)
[2025-05-21 19:37:05,728]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 1.0411 Train Acc: 0.6292 Eval Loss: 0.9393 Eval Acc: 0.6690 (LR: 0.000010)
[2025-05-21 19:37:32,486]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 1.0377 Train Acc: 0.6310 Eval Loss: 0.9323 Eval Acc: 0.6703 (LR: 0.000010)
[2025-05-21 19:37:59,003]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 1.0369 Train Acc: 0.6308 Eval Loss: 0.9290 Eval Acc: 0.6745 (LR: 0.000010)
[2025-05-21 19:38:25,410]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 1.0347 Train Acc: 0.6316 Eval Loss: 0.9397 Eval Acc: 0.6702 (LR: 0.000010)
[2025-05-21 19:38:51,917]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 1.0332 Train Acc: 0.6330 Eval Loss: 0.9345 Eval Acc: 0.6717 (LR: 0.000010)
[2025-05-21 19:39:18,116]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 1.0324 Train Acc: 0.6321 Eval Loss: 0.9417 Eval Acc: 0.6656 (LR: 0.000010)
[2025-05-21 19:39:44,382]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 1.0386 Train Acc: 0.6284 Eval Loss: 0.9248 Eval Acc: 0.6745 (LR: 0.000010)
[2025-05-21 19:40:10,782]: [LeNet5_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 1.0390 Train Acc: 0.6274 Eval Loss: 0.9345 Eval Acc: 0.6707 (LR: 0.000010)
[2025-05-21 19:40:10,782]: [LeNet5_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.6745
[2025-05-21 19:40:10,810]: 


Quantization of model down to 3 bits finished
[2025-05-21 19:40:10,810]: Model Architecture:
[2025-05-21 19:40:10,821]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1879], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6577157378196716, max_val=0.6577165126800537)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8691], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0971], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.34229421615600586, max_val=0.3375615179538727)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9461], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1352], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3799472749233246, max_val=0.5664165019989014)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8246], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-21 19:40:10,821]: 
Model Weights:
[2025-05-21 19:40:10,821]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-21 19:40:10,822]: Sample Values (25 elements): [-0.24375437200069427, -0.10809244215488434, 0.36582571268081665, -0.024815965443849564, 0.45179814100265503, -0.18525078892707825, -0.1472524106502533, -0.10160917043685913, -0.03641698509454727, -0.061726439744234085, -0.1508345901966095, -0.0604240782558918, -0.20836518704891205, -0.6435937881469727, -0.021754059940576553, 0.2888939678668976, 0.39908987283706665, 0.04554707184433937, -0.011262777261435986, 0.1530550867319107, -0.12333382666110992, -0.014899863861501217, 0.4029977321624756, 0.31519052386283875, -0.17012253403663635]
[2025-05-21 19:40:10,822]: Mean: 0.00109990
[2025-05-21 19:40:10,822]: Min: -1.35644078
[2025-05-21 19:40:10,823]: Max: 1.28651249
[2025-05-21 19:40:10,824]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-21 19:40:10,824]: Sample Values (25 elements): [0.0, 0.0, -0.18791888654232025, -0.18791888654232025, 0.0, 0.0, 0.18791888654232025, 0.0, 0.18791888654232025, 0.0, 0.0, -0.3758377730846405, 0.3758377730846405, 0.0, -0.18791888654232025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18791888654232025, 0.0, 0.0, 0.3758377730846405]
[2025-05-21 19:40:10,824]: Mean: -0.01158833
[2025-05-21 19:40:10,825]: Min: -0.56375664
[2025-05-21 19:40:10,825]: Max: 0.56375664
[2025-05-21 19:40:10,826]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-21 19:40:10,827]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.09712224453687668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09712224453687668, 0.0, 0.0, 0.0, 0.0, 0.09712224453687668, 0.0, 0.0, -0.09712224453687668, 0.0, -0.09712224453687668, 0.09712224453687668, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 19:40:10,827]: Mean: -0.00139816
[2025-05-21 19:40:10,827]: Min: -0.38848898
[2025-05-21 19:40:10,827]: Max: 0.29136673
[2025-05-21 19:40:10,829]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-21 19:40:10,830]: Sample Values (25 elements): [0.0, 0.0, -0.2703896462917328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1351948231458664, 0.0, 0.0, -0.1351948231458664, 0.0, 0.1351948231458664, 0.0, 0.0, -0.1351948231458664, 0.0, 0.0, 0.0, -0.1351948231458664, 0.0, -0.1351948231458664, 0.1351948231458664]
[2025-05-21 19:40:10,830]: Mean: -0.00579406
[2025-05-21 19:40:10,830]: Min: -0.40558445
[2025-05-21 19:40:10,830]: Max: 0.54077929
[2025-05-21 19:40:10,830]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-21 19:40:10,831]: Sample Values (25 elements): [-0.09786245226860046, 0.1684189885854721, -0.04178956151008606, 0.05964823067188263, -0.03532938286662102, 0.08548445254564285, 0.07316455245018005, 0.12186124920845032, 0.010800777934491634, 0.014533953741192818, 0.2017854005098343, 0.004941062536090612, 0.07587624341249466, -0.11744214594364166, -0.031781092286109924, 0.050916146486997604, -0.11821485310792923, 0.03260516747832298, 0.08400092273950577, -0.05225956067442894, -0.11165362596511841, 0.01330650970339775, -0.033744897693395615, -0.06733516603708267, -0.030167287215590477]
[2025-05-21 19:40:10,831]: Mean: -0.00070426
[2025-05-21 19:40:10,831]: Min: -0.26482144
[2025-05-21 19:40:10,831]: Max: 0.25556347
[2025-05-21 19:40:10,831]: 


QAT of LeNet5 with parametrized_relu down to 4 bits...
[2025-05-21 19:40:10,848]: [LeNet5_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-05-21 19:40:10,854]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-21 19:40:37,132]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 1.3289 Train Acc: 0.5251 Eval Loss: 1.1070 Eval Acc: 0.5982 (LR: 0.010000)
[2025-05-21 19:41:03,433]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 1.2607 Train Acc: 0.5513 Eval Loss: 1.0884 Eval Acc: 0.6068 (LR: 0.010000)
[2025-05-21 19:41:29,716]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 1.2279 Train Acc: 0.5603 Eval Loss: 1.1445 Eval Acc: 0.5924 (LR: 0.010000)
[2025-05-21 19:41:55,752]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 1.2216 Train Acc: 0.5636 Eval Loss: 1.0553 Eval Acc: 0.6294 (LR: 0.010000)
[2025-05-21 19:42:22,177]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 1.2033 Train Acc: 0.5736 Eval Loss: 1.0937 Eval Acc: 0.6064 (LR: 0.010000)
[2025-05-21 19:42:48,540]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 1.1911 Train Acc: 0.5737 Eval Loss: 1.0439 Eval Acc: 0.6309 (LR: 0.010000)
[2025-05-21 19:43:14,695]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 1.1792 Train Acc: 0.5787 Eval Loss: 1.1090 Eval Acc: 0.5955 (LR: 0.010000)
[2025-05-21 19:43:41,011]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 1.1682 Train Acc: 0.5850 Eval Loss: 1.0356 Eval Acc: 0.6326 (LR: 0.010000)
[2025-05-21 19:44:07,353]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 1.1618 Train Acc: 0.5871 Eval Loss: 1.0203 Eval Acc: 0.6290 (LR: 0.010000)
[2025-05-21 19:44:33,508]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 1.1542 Train Acc: 0.5876 Eval Loss: 1.0502 Eval Acc: 0.6262 (LR: 0.010000)
[2025-05-21 19:44:59,895]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 1.1373 Train Acc: 0.5958 Eval Loss: 1.0236 Eval Acc: 0.6381 (LR: 0.010000)
[2025-05-21 19:45:26,000]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 1.1413 Train Acc: 0.5911 Eval Loss: 1.0482 Eval Acc: 0.6249 (LR: 0.010000)
[2025-05-21 19:45:51,654]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 1.1377 Train Acc: 0.5954 Eval Loss: 0.9809 Eval Acc: 0.6513 (LR: 0.010000)
[2025-05-21 19:46:15,236]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 1.1398 Train Acc: 0.5946 Eval Loss: 1.0360 Eval Acc: 0.6317 (LR: 0.010000)
[2025-05-21 19:46:40,257]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 1.1268 Train Acc: 0.6005 Eval Loss: 1.0143 Eval Acc: 0.6422 (LR: 0.001000)
[2025-05-21 19:47:05,361]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 1.0322 Train Acc: 0.6329 Eval Loss: 0.9276 Eval Acc: 0.6708 (LR: 0.001000)
[2025-05-21 19:47:29,983]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 1.0143 Train Acc: 0.6371 Eval Loss: 0.9342 Eval Acc: 0.6724 (LR: 0.001000)
[2025-05-21 19:47:53,650]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 1.0085 Train Acc: 0.6414 Eval Loss: 0.9159 Eval Acc: 0.6766 (LR: 0.001000)
[2025-05-21 19:48:17,297]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 1.0071 Train Acc: 0.6427 Eval Loss: 0.9104 Eval Acc: 0.6771 (LR: 0.001000)
[2025-05-21 19:48:41,051]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 1.0007 Train Acc: 0.6461 Eval Loss: 0.9034 Eval Acc: 0.6766 (LR: 0.001000)
[2025-05-21 19:49:04,753]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 1.0023 Train Acc: 0.6446 Eval Loss: 0.9012 Eval Acc: 0.6784 (LR: 0.001000)
[2025-05-21 19:49:28,562]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 1.0000 Train Acc: 0.6436 Eval Loss: 0.9019 Eval Acc: 0.6799 (LR: 0.001000)
[2025-05-21 19:49:52,205]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.9978 Train Acc: 0.6461 Eval Loss: 0.9036 Eval Acc: 0.6769 (LR: 0.001000)
[2025-05-21 19:50:15,740]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.9942 Train Acc: 0.6470 Eval Loss: 0.9028 Eval Acc: 0.6797 (LR: 0.001000)
[2025-05-21 19:50:39,259]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.9969 Train Acc: 0.6462 Eval Loss: 0.8881 Eval Acc: 0.6803 (LR: 0.001000)
[2025-05-21 19:51:02,897]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.9949 Train Acc: 0.6465 Eval Loss: 0.8940 Eval Acc: 0.6829 (LR: 0.001000)
[2025-05-21 19:51:26,523]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.9940 Train Acc: 0.6473 Eval Loss: 0.8917 Eval Acc: 0.6835 (LR: 0.001000)
[2025-05-21 19:51:50,056]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.9892 Train Acc: 0.6483 Eval Loss: 0.9004 Eval Acc: 0.6801 (LR: 0.001000)
[2025-05-21 19:52:13,567]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.9854 Train Acc: 0.6495 Eval Loss: 0.9033 Eval Acc: 0.6824 (LR: 0.001000)
[2025-05-21 19:52:37,003]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.9841 Train Acc: 0.6508 Eval Loss: 0.8809 Eval Acc: 0.6862 (LR: 0.000100)
[2025-05-21 19:53:00,394]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.9647 Train Acc: 0.6591 Eval Loss: 0.8723 Eval Acc: 0.6917 (LR: 0.000100)
[2025-05-21 19:53:23,864]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.9649 Train Acc: 0.6567 Eval Loss: 0.8735 Eval Acc: 0.6868 (LR: 0.000100)
[2025-05-21 19:53:47,470]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.9558 Train Acc: 0.6589 Eval Loss: 0.8762 Eval Acc: 0.6856 (LR: 0.000100)
[2025-05-21 19:54:11,107]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.9659 Train Acc: 0.6564 Eval Loss: 0.8775 Eval Acc: 0.6846 (LR: 0.000100)
[2025-05-21 19:54:34,741]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.9575 Train Acc: 0.6590 Eval Loss: 0.8781 Eval Acc: 0.6838 (LR: 0.000100)
[2025-05-21 19:55:02,044]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.9664 Train Acc: 0.6561 Eval Loss: 0.8769 Eval Acc: 0.6864 (LR: 0.000100)
[2025-05-21 19:55:28,602]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.9637 Train Acc: 0.6556 Eval Loss: 0.8831 Eval Acc: 0.6895 (LR: 0.000100)
[2025-05-21 19:55:54,804]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.9623 Train Acc: 0.6581 Eval Loss: 0.8737 Eval Acc: 0.6859 (LR: 0.000100)
[2025-05-21 19:56:21,511]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.9561 Train Acc: 0.6621 Eval Loss: 0.8786 Eval Acc: 0.6835 (LR: 0.000100)
[2025-05-21 19:56:47,643]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.9579 Train Acc: 0.6612 Eval Loss: 0.8767 Eval Acc: 0.6865 (LR: 0.000100)
[2025-05-21 19:57:13,091]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.9592 Train Acc: 0.6591 Eval Loss: 0.8739 Eval Acc: 0.6868 (LR: 0.000100)
[2025-05-21 19:57:39,241]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.9623 Train Acc: 0.6584 Eval Loss: 0.8762 Eval Acc: 0.6858 (LR: 0.000100)
[2025-05-21 19:58:06,123]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.9581 Train Acc: 0.6585 Eval Loss: 0.8892 Eval Acc: 0.6826 (LR: 0.000100)
[2025-05-21 19:58:31,117]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.9592 Train Acc: 0.6577 Eval Loss: 0.8771 Eval Acc: 0.6857 (LR: 0.000100)
[2025-05-21 19:58:55,352]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.9612 Train Acc: 0.6599 Eval Loss: 0.8739 Eval Acc: 0.6889 (LR: 0.000010)
[2025-05-21 19:59:22,535]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.9599 Train Acc: 0.6597 Eval Loss: 0.8679 Eval Acc: 0.6893 (LR: 0.000010)
[2025-05-21 19:59:48,961]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.9534 Train Acc: 0.6621 Eval Loss: 0.8679 Eval Acc: 0.6891 (LR: 0.000010)
[2025-05-21 20:00:14,106]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.9543 Train Acc: 0.6595 Eval Loss: 0.8721 Eval Acc: 0.6885 (LR: 0.000010)
[2025-05-21 20:00:39,026]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.9561 Train Acc: 0.6596 Eval Loss: 0.8689 Eval Acc: 0.6885 (LR: 0.000010)
[2025-05-21 20:01:04,090]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.9569 Train Acc: 0.6597 Eval Loss: 0.8692 Eval Acc: 0.6906 (LR: 0.000010)
[2025-05-21 20:01:30,076]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.9614 Train Acc: 0.6581 Eval Loss: 0.8698 Eval Acc: 0.6885 (LR: 0.000010)
[2025-05-21 20:01:56,707]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.9553 Train Acc: 0.6602 Eval Loss: 0.8668 Eval Acc: 0.6883 (LR: 0.000010)
[2025-05-21 20:02:23,822]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.9532 Train Acc: 0.6604 Eval Loss: 0.8666 Eval Acc: 0.6913 (LR: 0.000010)
[2025-05-21 20:02:49,581]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.9593 Train Acc: 0.6597 Eval Loss: 0.8697 Eval Acc: 0.6886 (LR: 0.000010)
[2025-05-21 20:03:15,285]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.9607 Train Acc: 0.6580 Eval Loss: 0.8709 Eval Acc: 0.6898 (LR: 0.000010)
[2025-05-21 20:03:40,577]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.9624 Train Acc: 0.6606 Eval Loss: 0.8722 Eval Acc: 0.6884 (LR: 0.000010)
[2025-05-21 20:04:07,888]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.9561 Train Acc: 0.6622 Eval Loss: 0.8713 Eval Acc: 0.6882 (LR: 0.000010)
[2025-05-21 20:04:48,110]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.9522 Train Acc: 0.6616 Eval Loss: 0.8700 Eval Acc: 0.6884 (LR: 0.000010)
[2025-05-21 20:05:22,976]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.9529 Train Acc: 0.6611 Eval Loss: 0.8724 Eval Acc: 0.6914 (LR: 0.000010)
[2025-05-21 20:06:01,981]: [LeNet5_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.9555 Train Acc: 0.6606 Eval Loss: 0.8660 Eval Acc: 0.6901 (LR: 0.000010)
[2025-05-21 20:06:01,982]: [LeNet5_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.6917
[2025-05-21 20:06:02,004]: 


Quantization of model down to 4 bits finished
[2025-05-21 20:06:02,004]: Model Architecture:
[2025-05-21 20:06:02,022]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0879], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6507077217102051, max_val=0.6673673391342163)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0414], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3250153958797455, max_val=0.2956535220146179)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4245], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0606], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33467334508895874, max_val=0.5749278664588928)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU(
          (quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3822], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): FixedQParamsObserver()
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-21 20:06:02,022]: 
Model Weights:
[2025-05-21 20:06:02,022]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-21 20:06:02,022]: Sample Values (25 elements): [-0.678318202495575, -0.28345540165901184, 0.003417283296585083, -0.1425301730632782, -0.11221057176589966, -0.107618048787117, 0.2213210016489029, 0.26847073435783386, 0.2513129413127899, -0.17564541101455688, -0.7332512736320496, -0.6933925151824951, -0.36219078302383423, 0.020115507766604424, 0.19967129826545715, -0.5439493060112, -1.0979079008102417, 0.12690995633602142, 0.12741585075855255, -1.2921168804168701, -0.294592946767807, -0.06234021484851837, -0.07162202149629593, 0.2300628423690796, -0.259510338306427]
[2025-05-21 20:06:02,023]: Mean: 0.00075119
[2025-05-21 20:06:02,023]: Min: -1.32207763
[2025-05-21 20:06:02,023]: Max: 1.23469377
[2025-05-21 20:06:02,026]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-21 20:06:02,027]: Sample Values (25 elements): [0.0, 0.08787168562412262, 0.17574337124824524, 0.08787168562412262, -0.08787168562412262, -0.26361507177352905, -0.08787168562412262, -0.08787168562412262, 0.26361507177352905, 0.0, 0.0, 0.17574337124824524, 0.0, -0.4393584132194519, 0.17574337124824524, -0.17574337124824524, 0.0, 0.0, -0.08787168562412262, -0.26361507177352905, -0.17574337124824524, -0.08787168562412262, 0.0, -0.08787168562412262, 0.08787168562412262]
[2025-05-21 20:06:02,027]: Mean: -0.00706635
[2025-05-21 20:06:02,027]: Min: -0.61510181
[2025-05-21 20:06:02,027]: Max: 0.70297348
[2025-05-21 20:06:02,029]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-21 20:06:02,030]: Sample Values (25 elements): [-0.041377924382686615, 0.08275584876537323, 0.041377924382686615, 0.041377924382686615, -0.041377924382686615, -0.041377924382686615, 0.0, -0.041377924382686615, -0.08275584876537323, -0.08275584876537323, -0.08275584876537323, 0.08275584876537323, -0.08275584876537323, 0.0, -0.12413377314805984, -0.041377924382686615, 0.0, -0.08275584876537323, 0.0, -0.041377924382686615, -0.12413377314805984, -0.12413377314805984, -0.041377924382686615, -0.041377924382686615, 0.08275584876537323]
[2025-05-21 20:06:02,030]: Mean: -0.00058619
[2025-05-21 20:06:02,030]: Min: -0.33102340
[2025-05-21 20:06:02,031]: Max: 0.28964546
[2025-05-21 20:06:02,033]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-21 20:06:02,034]: Sample Values (25 elements): [0.12128020823001862, 0.0, 0.0, 0.0, 0.24256041646003723, 0.0, 0.0, 0.0, -0.06064010411500931, 0.06064010411500931, 0.12128020823001862, -0.06064010411500931, 0.06064010411500931, 0.06064010411500931, 0.06064010411500931, 0.0, -0.12128020823001862, 0.0, -0.06064010411500931, 0.06064010411500931, -0.06064010411500931, -0.12128020823001862, -0.12128020823001862, 0.0, 0.06064010411500931]
[2025-05-21 20:06:02,034]: Mean: -0.00439160
[2025-05-21 20:06:02,034]: Min: -0.36384064
[2025-05-21 20:06:02,035]: Max: 0.54576093
[2025-05-21 20:06:02,035]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-21 20:06:02,035]: Sample Values (25 elements): [-0.06441523879766464, 0.0886942520737648, 0.001091512618586421, 0.20614711940288544, 0.08781055361032486, -0.13191643357276917, 0.10825563222169876, -0.020537277683615685, -0.0305949579924345, -0.05087341368198395, -0.020400913432240486, 0.12984751164913177, -0.12535521388053894, -0.05224117264151573, -0.07675905525684357, 0.04502512887120247, 0.0876302644610405, 0.11413576453924179, 0.035771019756793976, -0.053988222032785416, -0.05293329805135727, -0.13533857464790344, 0.0364576131105423, -0.0880708247423172, -0.15076541900634766]
[2025-05-21 20:06:02,035]: Mean: -0.00070425
[2025-05-21 20:06:02,036]: Min: -0.28431970
[2025-05-21 20:06:02,036]: Max: 0.27021652
