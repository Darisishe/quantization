[2025-05-22 00:16:41,750]: Checkpoint of model at path [checkpoint/ResNet20_hardtanh.ckpt] will be used for QAT
[2025-05-22 00:16:41,750]: 


QAT of ResNet20 with parametrized_hardtanh down to 2 bits...
[2025-05-22 00:16:41,860]: [ResNet20_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-22 00:16:41,889]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 00:17:40,184]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.3856 Train Acc: 0.5048 Eval Loss: 1.3229 Eval Acc: 0.5358 (LR: 0.010000)
[2025-05-22 00:18:38,885]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 1.1847 Train Acc: 0.5764 Eval Loss: 1.3582 Eval Acc: 0.5367 (LR: 0.010000)
[2025-05-22 00:19:36,168]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 1.1207 Train Acc: 0.6030 Eval Loss: 1.4442 Eval Acc: 0.5147 (LR: 0.010000)
[2025-05-22 00:20:32,355]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 1.0895 Train Acc: 0.6161 Eval Loss: 1.0906 Eval Acc: 0.6124 (LR: 0.010000)
[2025-05-22 00:21:28,553]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 1.0561 Train Acc: 0.6253 Eval Loss: 1.1603 Eval Acc: 0.5980 (LR: 0.010000)
[2025-05-22 00:22:24,686]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 1.0439 Train Acc: 0.6294 Eval Loss: 1.3690 Eval Acc: 0.5564 (LR: 0.010000)
[2025-05-22 00:23:21,141]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 1.0285 Train Acc: 0.6346 Eval Loss: 1.3377 Eval Acc: 0.5431 (LR: 0.010000)
[2025-05-22 00:24:18,162]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 1.0104 Train Acc: 0.6406 Eval Loss: 1.0672 Eval Acc: 0.6300 (LR: 0.010000)
[2025-05-22 00:25:18,730]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 1.0006 Train Acc: 0.6471 Eval Loss: 1.3070 Eval Acc: 0.5608 (LR: 0.010000)
[2025-05-22 00:26:18,398]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.9998 Train Acc: 0.6478 Eval Loss: 1.0700 Eval Acc: 0.6268 (LR: 0.010000)
[2025-05-22 00:27:16,352]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.9865 Train Acc: 0.6560 Eval Loss: 1.2154 Eval Acc: 0.5924 (LR: 0.010000)
[2025-05-22 00:28:13,021]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.9800 Train Acc: 0.6540 Eval Loss: 1.1461 Eval Acc: 0.6196 (LR: 0.010000)
[2025-05-22 00:29:10,029]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.9678 Train Acc: 0.6572 Eval Loss: 1.1905 Eval Acc: 0.6085 (LR: 0.010000)
[2025-05-22 00:30:06,785]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.9624 Train Acc: 0.6607 Eval Loss: 1.0543 Eval Acc: 0.6326 (LR: 0.010000)
[2025-05-22 00:31:03,702]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.9580 Train Acc: 0.6621 Eval Loss: 1.0620 Eval Acc: 0.6377 (LR: 0.001000)
[2025-05-22 00:32:04,573]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.8656 Train Acc: 0.6956 Eval Loss: 0.8952 Eval Acc: 0.6897 (LR: 0.001000)
[2025-05-22 00:33:21,472]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.8463 Train Acc: 0.7025 Eval Loss: 0.9090 Eval Acc: 0.6846 (LR: 0.001000)
[2025-05-22 00:35:33,211]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.8484 Train Acc: 0.7010 Eval Loss: 0.8381 Eval Acc: 0.7093 (LR: 0.001000)
[2025-05-22 00:38:14,264]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.8398 Train Acc: 0.7037 Eval Loss: 0.8750 Eval Acc: 0.6996 (LR: 0.001000)
[2025-05-22 00:40:54,857]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.8388 Train Acc: 0.7023 Eval Loss: 0.8481 Eval Acc: 0.7037 (LR: 0.001000)
[2025-05-22 00:43:37,318]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.8412 Train Acc: 0.7044 Eval Loss: 0.8910 Eval Acc: 0.6852 (LR: 0.001000)
[2025-05-22 00:46:17,552]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.8408 Train Acc: 0.7035 Eval Loss: 0.8653 Eval Acc: 0.6999 (LR: 0.001000)
[2025-05-22 00:48:07,984]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.8360 Train Acc: 0.7056 Eval Loss: 0.8963 Eval Acc: 0.6873 (LR: 0.001000)
[2025-05-22 00:49:54,949]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.8366 Train Acc: 0.7050 Eval Loss: 0.9275 Eval Acc: 0.6821 (LR: 0.001000)
[2025-05-22 00:51:41,352]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.8394 Train Acc: 0.7038 Eval Loss: 0.8685 Eval Acc: 0.6990 (LR: 0.001000)
[2025-05-22 00:53:26,511]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.8359 Train Acc: 0.7061 Eval Loss: 0.8291 Eval Acc: 0.7114 (LR: 0.001000)
[2025-05-22 00:55:12,298]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.8399 Train Acc: 0.7040 Eval Loss: 0.8910 Eval Acc: 0.6893 (LR: 0.001000)
[2025-05-22 00:56:57,636]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.8415 Train Acc: 0.7041 Eval Loss: 0.8543 Eval Acc: 0.7064 (LR: 0.001000)
[2025-05-22 00:58:43,364]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.8288 Train Acc: 0.7078 Eval Loss: 0.8807 Eval Acc: 0.6966 (LR: 0.001000)
[2025-05-22 01:00:28,827]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.8387 Train Acc: 0.7026 Eval Loss: 0.9045 Eval Acc: 0.6901 (LR: 0.000100)
[2025-05-22 01:02:14,041]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.8006 Train Acc: 0.7192 Eval Loss: 0.8078 Eval Acc: 0.7195 (LR: 0.000100)
[2025-05-22 01:03:59,473]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.8047 Train Acc: 0.7149 Eval Loss: 0.8159 Eval Acc: 0.7167 (LR: 0.000100)
[2025-05-22 01:05:44,759]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.8016 Train Acc: 0.7189 Eval Loss: 0.8703 Eval Acc: 0.6941 (LR: 0.000100)
[2025-05-22 01:07:29,882]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.8060 Train Acc: 0.7157 Eval Loss: 0.8131 Eval Acc: 0.7171 (LR: 0.000100)
[2025-05-22 01:09:15,491]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.8032 Train Acc: 0.7168 Eval Loss: 0.8887 Eval Acc: 0.6959 (LR: 0.000100)
[2025-05-22 01:11:01,990]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.8027 Train Acc: 0.7161 Eval Loss: 0.9025 Eval Acc: 0.6855 (LR: 0.000100)
[2025-05-22 01:12:47,248]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.8047 Train Acc: 0.7181 Eval Loss: 0.8290 Eval Acc: 0.7145 (LR: 0.000100)
[2025-05-22 01:14:32,614]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.8028 Train Acc: 0.7186 Eval Loss: 0.9603 Eval Acc: 0.6710 (LR: 0.000100)
[2025-05-22 01:16:17,824]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.8066 Train Acc: 0.7140 Eval Loss: 0.8551 Eval Acc: 0.7100 (LR: 0.000100)
[2025-05-22 01:18:03,151]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.8049 Train Acc: 0.7160 Eval Loss: 0.8688 Eval Acc: 0.7038 (LR: 0.000100)
[2025-05-22 01:19:48,271]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.8035 Train Acc: 0.7162 Eval Loss: 0.8201 Eval Acc: 0.7179 (LR: 0.000100)
[2025-05-22 01:21:33,273]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.8049 Train Acc: 0.7166 Eval Loss: 0.8182 Eval Acc: 0.7167 (LR: 0.000100)
[2025-05-22 01:23:18,170]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.8099 Train Acc: 0.7144 Eval Loss: 0.9235 Eval Acc: 0.6899 (LR: 0.000100)
[2025-05-22 01:24:53,925]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.8056 Train Acc: 0.7150 Eval Loss: 0.8420 Eval Acc: 0.7141 (LR: 0.000100)
[2025-05-22 01:26:34,179]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.8114 Train Acc: 0.7154 Eval Loss: 0.8140 Eval Acc: 0.7133 (LR: 0.000010)
[2025-05-22 01:28:17,236]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.7992 Train Acc: 0.7189 Eval Loss: 0.8153 Eval Acc: 0.7215 (LR: 0.000010)
[2025-05-22 01:29:56,787]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.7958 Train Acc: 0.7200 Eval Loss: 0.9207 Eval Acc: 0.6806 (LR: 0.000010)
[2025-05-22 01:31:32,607]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.7930 Train Acc: 0.7206 Eval Loss: 0.8028 Eval Acc: 0.7199 (LR: 0.000010)
[2025-05-22 01:32:47,654]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.7920 Train Acc: 0.7215 Eval Loss: 0.8243 Eval Acc: 0.7142 (LR: 0.000010)
[2025-05-22 01:33:50,367]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.7916 Train Acc: 0.7213 Eval Loss: 0.7878 Eval Acc: 0.7250 (LR: 0.000010)
[2025-05-22 01:34:48,625]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.7924 Train Acc: 0.7197 Eval Loss: 0.7997 Eval Acc: 0.7193 (LR: 0.000010)
[2025-05-22 01:35:46,546]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.7920 Train Acc: 0.7190 Eval Loss: 0.7793 Eval Acc: 0.7329 (LR: 0.000010)
[2025-05-22 01:36:44,453]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.7974 Train Acc: 0.7185 Eval Loss: 0.8040 Eval Acc: 0.7238 (LR: 0.000010)
[2025-05-22 01:37:41,707]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.7949 Train Acc: 0.7207 Eval Loss: 0.7993 Eval Acc: 0.7178 (LR: 0.000010)
[2025-05-22 01:38:39,457]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.7923 Train Acc: 0.7205 Eval Loss: 0.8787 Eval Acc: 0.7025 (LR: 0.000010)
[2025-05-22 01:39:41,080]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.7937 Train Acc: 0.7209 Eval Loss: 0.7913 Eval Acc: 0.7253 (LR: 0.000010)
[2025-05-22 01:40:43,263]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.7938 Train Acc: 0.7200 Eval Loss: 0.8081 Eval Acc: 0.7168 (LR: 0.000010)
[2025-05-22 01:41:49,277]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.7954 Train Acc: 0.7190 Eval Loss: 0.8050 Eval Acc: 0.7232 (LR: 0.000010)
[2025-05-22 01:42:49,178]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.7941 Train Acc: 0.7226 Eval Loss: 0.8293 Eval Acc: 0.7137 (LR: 0.000010)
[2025-05-22 01:43:55,942]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.7941 Train Acc: 0.7185 Eval Loss: 0.8446 Eval Acc: 0.7060 (LR: 0.000010)
[2025-05-22 01:43:55,943]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.7329
[2025-05-22 01:43:56,008]: 


Quantization of model down to 2 bits finished
[2025-05-22 01:43:56,008]: Model Architecture:
[2025-05-22 01:43:56,076]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6667], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2732], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4067639708518982, max_val=0.41273581981658936)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8259], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5620], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8371989727020264, max_val=0.8489392995834351)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0753], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3170], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47549742460250854, max_val=0.4755101203918457)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2858], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2261], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3608006238937378, max_val=0.3175869584083557)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.4175], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2197], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3295867443084717, max_val=0.329584002494812)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9055], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1900], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27223584055900574, max_val=0.2977064251899719)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.9369], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1592], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25470608472824097, max_val=0.22293664515018463)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3519], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1191], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18235427141189575, max_val=0.17484453320503235)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2867], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42990946769714355, max_val=0.43020695447921753)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8310], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1444], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23257672786712646, max_val=0.20062120258808136)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8603], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1423], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21107086539268494, max_val=0.21569518744945526)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3138], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1276], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18836382031440735, max_val=0.1944475919008255)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7383], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1186], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18236947059631348, max_val=0.17330332100391388)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.9160], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1014], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15562297403812408, max_val=0.14867877960205078)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2370], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1059], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15810871124267578, max_val=0.1594562977552414)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2028], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3042599558830261, max_val=0.3042566180229187)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1073], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16260600090026855, max_val=0.1591656655073166)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9333], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1000], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14816997945308685, max_val=0.1517503410577774)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.7533], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0817], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11504769325256348, max_val=0.13013209402561188)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6144], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0746], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11572789400815964, max_val=0.10820918530225754)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9019], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 01:43:56,077]: 
Model Weights:
[2025-05-22 01:43:56,077]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-22 01:43:56,078]: Sample Values (25 elements): [-0.5099284052848816, -0.052278514951467514, -0.3364136517047882, -0.171589657664299, -0.2497866451740265, 0.18489030003547668, 0.562138557434082, -0.07133930921554565, -0.35458648204803467, 0.10479597747325897, 0.003521657781675458, 0.394570916891098, 0.07781390100717545, 0.21443811058998108, 0.2496541291475296, 0.3710940182209015, 0.08657283335924149, -0.4560319185256958, -0.26423394680023193, 0.220463827252388, 0.0527818389236927, -0.17838157713413239, -0.20157738029956818, -0.12860433757305145, 0.04369096830487251]
[2025-05-22 01:43:56,078]: Mean: -0.00334142
[2025-05-22 01:43:56,078]: Min: -0.66299170
[2025-05-22 01:43:56,078]: Max: 0.70793116
[2025-05-22 01:43:56,079]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,079]: Sample Values (16 elements): [1.3252999782562256, 0.9908111691474915, 1.2418066263198853, 1.3124871253967285, 1.6789324283599854, 0.9221682548522949, 1.2962894439697266, 1.3114556074142456, 1.314546823501587, 0.9864329695701599, 1.5898112058639526, 1.7641469240188599, 1.264512300491333, 1.2431426048278809, 0.8052500486373901, 1.2625551223754883]
[2025-05-22 01:43:56,079]: Mean: 1.26935303
[2025-05-22 01:43:56,079]: Min: 0.80525005
[2025-05-22 01:43:56,079]: Max: 1.76414692
[2025-05-22 01:43:56,081]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 01:43:56,081]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.27316659688949585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.27316659688949585, 0.27316659688949585, 0.0, 0.0, 0.0, 0.0, 0.27316659688949585, 0.0, 0.0, 0.27316659688949585, 0.0, 0.27316659688949585, 0.0]
[2025-05-22 01:43:56,081]: Mean: 0.00237124
[2025-05-22 01:43:56,081]: Min: -0.27316660
[2025-05-22 01:43:56,081]: Max: 0.54633319
[2025-05-22 01:43:56,081]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,082]: Sample Values (16 elements): [1.2228807210922241, 1.3138209581375122, 0.951007068157196, 0.8492373824119568, 1.3805269002914429, 1.2411344051361084, 1.0243452787399292, 1.4214116334915161, 1.3748869895935059, 1.1898280382156372, 0.9494342803955078, 1.2174850702285767, 1.2189291715621948, 0.8631472587585449, 1.0428638458251953, 1.0506248474121094]
[2025-05-22 01:43:56,082]: Mean: 1.14447272
[2025-05-22 01:43:56,082]: Min: 0.84923738
[2025-05-22 01:43:56,082]: Max: 1.42141163
[2025-05-22 01:43:56,083]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 01:43:56,084]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5620461106300354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,084]: Mean: -0.00268338
[2025-05-22 01:43:56,084]: Min: -0.56204611
[2025-05-22 01:43:56,085]: Max: 1.12409222
[2025-05-22 01:43:56,085]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,085]: Sample Values (16 elements): [0.6494062542915344, 0.6665395498275757, 1.524957299232483, 1.6254677772521973, 0.7232306599617004, 0.6343675255775452, 0.9263867735862732, 0.6070550680160522, 0.6900421380996704, 0.7018669247627258, 0.5265694856643677, 1.674301266670227, 0.7322632670402527, 0.847314715385437, 0.8430502414703369, 0.9337578415870667]
[2025-05-22 01:43:56,086]: Mean: 0.89416105
[2025-05-22 01:43:56,086]: Min: 0.52656949
[2025-05-22 01:43:56,086]: Max: 1.67430127
[2025-05-22 01:43:56,087]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 01:43:56,087]: Sample Values (25 elements): [0.0, -0.317002534866333, 0.0, 0.0, -0.317002534866333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.317002534866333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,087]: Mean: -0.00027518
[2025-05-22 01:43:56,088]: Min: -0.31700253
[2025-05-22 01:43:56,088]: Max: 0.63400507
[2025-05-22 01:43:56,088]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,088]: Sample Values (16 elements): [1.046539545059204, 0.9622160196304321, 1.227190613746643, 1.0868151187896729, 1.0353119373321533, 1.0535101890563965, 1.0091211795806885, 1.1615397930145264, 1.0431716442108154, 1.0342038869857788, 1.0801092386245728, 1.143705129623413, 1.0991649627685547, 1.120445966720581, 0.9895337820053101, 1.0583019256591797]
[2025-05-22 01:43:56,089]: Mean: 1.07193005
[2025-05-22 01:43:56,089]: Min: 0.96221602
[2025-05-22 01:43:56,089]: Max: 1.22719061
[2025-05-22 01:43:56,090]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 01:43:56,090]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.2261292189359665, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2261292189359665, 0.0, -0.2261292189359665, 0.0, -0.2261292189359665, 0.0, 0.0, 0.0, -0.2261292189359665, -0.2261292189359665, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,091]: Mean: 0.00176663
[2025-05-22 01:43:56,091]: Min: -0.45225844
[2025-05-22 01:43:56,091]: Max: 0.22612922
[2025-05-22 01:43:56,091]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,092]: Sample Values (16 elements): [1.1151090860366821, 1.7765177488327026, 1.418217658996582, 1.0812143087387085, 0.9453246593475342, 1.1968241930007935, 1.044515609741211, 1.3079488277435303, 0.9410097599029541, 1.1795227527618408, 1.1912034749984741, 1.0793284177780151, 0.9132712483406067, 1.3959834575653076, 1.0959933996200562, 1.1108566522598267]
[2025-05-22 01:43:56,092]: Mean: 1.17455256
[2025-05-22 01:43:56,092]: Min: 0.91327125
[2025-05-22 01:43:56,092]: Max: 1.77651775
[2025-05-22 01:43:56,093]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 01:43:56,094]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.21972358226776123, 0.21972358226776123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21972358226776123, 0.0, 0.0, -0.21972358226776123, 0.0, 0.0, 0.0, 0.21972358226776123]
[2025-05-22 01:43:56,094]: Mean: 0.00877368
[2025-05-22 01:43:56,094]: Min: -0.43944716
[2025-05-22 01:43:56,094]: Max: 0.21972358
[2025-05-22 01:43:56,094]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,094]: Sample Values (16 elements): [0.8894657492637634, 0.8152289986610413, 1.0288677215576172, 0.9577410817146301, 0.8514899015426636, 0.9281069040298462, 0.9311859607696533, 0.8953390717506409, 0.8167738318443298, 0.9454193711280823, 1.0989630222320557, 1.1283340454101562, 1.0127052068710327, 0.9621530771255493, 1.1196820735931396, 1.0099049806594849]
[2025-05-22 01:43:56,095]: Mean: 0.96196002
[2025-05-22 01:43:56,095]: Min: 0.81522900
[2025-05-22 01:43:56,095]: Max: 1.12833405
[2025-05-22 01:43:56,096]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 01:43:56,096]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.18998074531555176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18998074531555176, 0.0, 0.0, -0.18998074531555176, 0.18998074531555176, 0.0, 0.0, -0.18998074531555176, 0.18998074531555176, 0.0, 0.0, -0.18998074531555176, 0.0, 0.0]
[2025-05-22 01:43:56,096]: Mean: 0.00090703
[2025-05-22 01:43:56,096]: Min: -0.18998075
[2025-05-22 01:43:56,097]: Max: 0.37996149
[2025-05-22 01:43:56,097]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-22 01:43:56,097]: Sample Values (16 elements): [1.0573176145553589, 1.0229687690734863, 0.9421104788780212, 1.133469581604004, 0.9488165974617004, 1.0282025337219238, 1.0562559366226196, 0.9740143418312073, 0.9329626560211182, 0.8868725299835205, 1.084876298904419, 1.0864382982254028, 0.9989181756973267, 1.1290966272354126, 1.129992961883545, 1.234497308731079]
[2025-05-22 01:43:56,097]: Mean: 1.04042554
[2025-05-22 01:43:56,098]: Min: 0.88687253
[2025-05-22 01:43:56,098]: Max: 1.23449731
[2025-05-22 01:43:56,099]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-22 01:43:56,100]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.15921424329280853, -0.15921424329280853, 0.0, 0.0, -0.15921424329280853, 0.0, 0.0, 0.0, 0.0, -0.15921424329280853, 0.0, 0.0, -0.15921424329280853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15921424329280853, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,100]: Mean: -0.00041462
[2025-05-22 01:43:56,100]: Min: -0.31842849
[2025-05-22 01:43:56,100]: Max: 0.15921424
[2025-05-22 01:43:56,100]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,101]: Sample Values (25 elements): [0.9861228466033936, 0.9671571850776672, 1.0080491304397583, 1.0046902894973755, 0.9925013780593872, 1.0445489883422852, 1.023736834526062, 0.9874615669250488, 0.9897798895835876, 1.022022008895874, 0.9591029286384583, 0.9381130337715149, 1.0066384077072144, 1.0848991870880127, 1.0481891632080078, 0.9556293487548828, 0.9762728214263916, 0.9497189521789551, 0.8949746489524841, 1.0703959465026855, 0.9900458455085754, 0.9993104338645935, 0.9713097214698792, 1.181725025177002, 0.9942012429237366]
[2025-05-22 01:43:56,101]: Mean: 0.99188012
[2025-05-22 01:43:56,101]: Min: 0.87255937
[2025-05-22 01:43:56,101]: Max: 1.18172503
[2025-05-22 01:43:56,102]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 01:43:56,102]: Sample Values (25 elements): [0.0, 0.1190662682056427, -0.1190662682056427, 0.0, 0.0, 0.0, 0.0, -0.1190662682056427, -0.1190662682056427, 0.0, 0.0, 0.0, 0.0, -0.1190662682056427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1190662682056427, -0.1190662682056427, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,103]: Mean: -0.00202836
[2025-05-22 01:43:56,103]: Min: -0.23813254
[2025-05-22 01:43:56,103]: Max: 0.11906627
[2025-05-22 01:43:56,103]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,103]: Sample Values (25 elements): [1.0933538675308228, 1.0677999258041382, 1.1055529117584229, 0.9565395712852478, 1.0701851844787598, 1.0978177785873413, 0.9959356784820557, 0.936635434627533, 1.0183355808258057, 0.9931397438049316, 0.924622654914856, 1.0629820823669434, 0.9397225379943848, 1.040885329246521, 1.0451513528823853, 1.0291513204574585, 1.182073950767517, 1.01900315284729, 1.1697092056274414, 0.9176686406135559, 0.9973064661026001, 1.0069553852081299, 1.047237753868103, 1.0707670450210571, 0.8911405801773071]
[2025-05-22 01:43:56,103]: Mean: 1.02823389
[2025-05-22 01:43:56,103]: Min: 0.89114058
[2025-05-22 01:43:56,104]: Max: 1.18207395
[2025-05-22 01:43:56,105]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-22 01:43:56,106]: Sample Values (25 elements): [-0.28670549392700195, 0.0, 0.28670549392700195, 0.0, 0.0, 0.0, -0.28670549392700195, 0.28670549392700195, -0.28670549392700195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28670549392700195, 0.0, -0.28670549392700195, -0.28670549392700195, 0.0, 0.28670549392700195, 0.0]
[2025-05-22 01:43:56,106]: Mean: 0.00391980
[2025-05-22 01:43:56,106]: Min: -0.28670549
[2025-05-22 01:43:56,106]: Max: 0.57341099
[2025-05-22 01:43:56,106]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,106]: Sample Values (25 elements): [0.6739009022712708, 0.7203680276870728, 0.7750809788703918, 0.7704190611839294, 0.605525016784668, 0.6367747783660889, 0.7708196043968201, 0.6872028708457947, 0.7253409624099731, 0.7793108224868774, 0.8607565760612488, 0.8247453570365906, 0.7395469546318054, 0.7815588116645813, 0.7796158790588379, 0.7892581820487976, 0.7525408864021301, 0.7278626561164856, 0.7258580923080444, 0.8060029745101929, 0.8431755304336548, 0.6776309609413147, 0.7420783638954163, 0.7278173565864563, 0.6668401956558228]
[2025-05-22 01:43:56,107]: Mean: 0.74619341
[2025-05-22 01:43:56,107]: Min: 0.60552502
[2025-05-22 01:43:56,107]: Max: 0.86075658
[2025-05-22 01:43:56,108]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 01:43:56,108]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.14439933001995087, 0.14439933001995087, 0.0, 0.0, 0.14439933001995087, 0.0, 0.0, 0.0, 0.0, 0.14439933001995087, 0.0]
[2025-05-22 01:43:56,108]: Mean: -0.00026636
[2025-05-22 01:43:56,108]: Min: -0.28879866
[2025-05-22 01:43:56,109]: Max: 0.14439933
[2025-05-22 01:43:56,109]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,109]: Sample Values (25 elements): [0.8899011015892029, 0.9675387740135193, 1.0163694620132446, 0.9045084714889526, 0.8941918015480042, 1.0006272792816162, 1.0091753005981445, 0.9570485353469849, 0.9495314359664917, 0.9842089414596558, 0.9096425175666809, 0.9403621554374695, 0.9701504707336426, 0.8891211748123169, 0.9512413144111633, 0.9491953253746033, 0.9427114129066467, 0.942256510257721, 0.9061865210533142, 0.9423925280570984, 0.9409931302070618, 0.9238396286964417, 0.9588875770568848, 1.016374945640564, 0.9977655410766602]
[2025-05-22 01:43:56,109]: Mean: 0.94536620
[2025-05-22 01:43:56,109]: Min: 0.88868821
[2025-05-22 01:43:56,109]: Max: 1.01637495
[2025-05-22 01:43:56,110]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 01:43:56,111]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.14225536584854126, -0.14225536584854126, 0.14225536584854126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.14225536584854126, 0.0, 0.0, -0.14225536584854126, 0.0, 0.0, 0.0, 0.0, 0.0, -0.14225536584854126, -0.14225536584854126]
[2025-05-22 01:43:56,111]: Mean: 0.00118855
[2025-05-22 01:43:56,111]: Min: -0.14225537
[2025-05-22 01:43:56,112]: Max: 0.28451073
[2025-05-22 01:43:56,112]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,112]: Sample Values (25 elements): [1.0481690168380737, 1.0474625825881958, 1.1865123510360718, 1.075831413269043, 0.9824568629264832, 1.0658351182937622, 1.058900237083435, 0.9745587706565857, 1.097997784614563, 1.1083476543426514, 0.9558626413345337, 1.001179575920105, 1.0463851690292358, 1.033840298652649, 0.9810284376144409, 1.1383960247039795, 1.0984668731689453, 0.8649730086326599, 1.0757887363433838, 1.1281347274780273, 1.0104503631591797, 0.9815263152122498, 1.0283547639846802, 1.0983328819274902, 0.9599404335021973]
[2025-05-22 01:43:56,112]: Mean: 1.02985215
[2025-05-22 01:43:56,113]: Min: 0.86497301
[2025-05-22 01:43:56,113]: Max: 1.18651235
[2025-05-22 01:43:56,114]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 01:43:56,114]: Sample Values (25 elements): [0.12760381400585175, 0.0, 0.12760381400585175, 0.0, 0.0, -0.12760381400585175, 0.12760381400585175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,114]: Mean: 0.00066460
[2025-05-22 01:43:56,114]: Min: -0.12760381
[2025-05-22 01:43:56,115]: Max: 0.25520763
[2025-05-22 01:43:56,115]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,115]: Sample Values (25 elements): [0.9300190210342407, 0.9293687343597412, 0.9518178701400757, 0.9916781187057495, 0.9286515116691589, 0.9836084842681885, 0.9448646903038025, 0.9484172463417053, 1.0101319551467896, 1.0444759130477905, 0.9589963555335999, 0.9419905543327332, 0.9087234139442444, 0.9587757587432861, 0.9473827481269836, 0.932966947555542, 0.9232662916183472, 0.9692059755325317, 1.0208624601364136, 0.960052490234375, 0.9978145956993103, 0.9242355823516846, 0.9312729239463806, 0.9212450385093689, 1.012528896331787]
[2025-05-22 01:43:56,115]: Mean: 0.95467377
[2025-05-22 01:43:56,115]: Min: 0.90435332
[2025-05-22 01:43:56,115]: Max: 1.04447591
[2025-05-22 01:43:56,116]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 01:43:56,117]: Sample Values (25 elements): [0.0, 0.11855758726596832, 0.0, 0.0, 0.0, 0.0, 0.11855758726596832, 0.0, 0.11855758726596832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11855758726596832, 0.0, 0.0, -0.11855758726596832, 0.0, -0.11855758726596832, 0.0, 0.11855758726596832, 0.0]
[2025-05-22 01:43:56,117]: Mean: -0.00140221
[2025-05-22 01:43:56,117]: Min: -0.23711517
[2025-05-22 01:43:56,117]: Max: 0.11855759
[2025-05-22 01:43:56,117]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-22 01:43:56,117]: Sample Values (25 elements): [1.0266448259353638, 1.0535510778427124, 1.1883221864700317, 1.3363944292068481, 1.0865243673324585, 1.148339867591858, 1.179868221282959, 1.162777066230774, 1.0521399974822998, 1.1120847463607788, 1.083011269569397, 1.1161144971847534, 1.0462775230407715, 1.0274534225463867, 1.1334936618804932, 1.0120567083358765, 1.1348236799240112, 1.1358009576797485, 1.1262421607971191, 1.054870367050171, 1.0805028676986694, 1.1600890159606934, 1.0372133255004883, 1.0653369426727295, 1.0145950317382812]
[2025-05-22 01:43:56,118]: Mean: 1.09553325
[2025-05-22 01:43:56,118]: Min: 1.00335920
[2025-05-22 01:43:56,118]: Max: 1.33639443
[2025-05-22 01:43:56,120]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-22 01:43:56,120]: Sample Values (25 elements): [0.0, -0.10143393278121948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10143393278121948, 0.0, 0.0, 0.0, -0.10143393278121948, 0.10143393278121948, 0.0, -0.10143393278121948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10143393278121948, -0.10143393278121948, 0.0]
[2025-05-22 01:43:56,121]: Mean: -0.00046226
[2025-05-22 01:43:56,121]: Min: -0.20286787
[2025-05-22 01:43:56,121]: Max: 0.10143393
[2025-05-22 01:43:56,121]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,121]: Sample Values (25 elements): [0.9532015323638916, 0.9818171858787537, 0.9891097545623779, 0.9764979481697083, 1.0024007558822632, 0.9735124707221985, 0.9867842197418213, 1.001025915145874, 0.9210980534553528, 0.9260621070861816, 1.0448063611984253, 0.9122942686080933, 0.974631667137146, 0.9464011788368225, 1.0008429288864136, 0.8843846321105957, 0.9695606827735901, 1.0028669834136963, 0.9088588953018188, 0.9423772096633911, 0.9613710045814514, 0.9477341771125793, 0.9770705699920654, 0.9923765659332275, 0.9220608472824097]
[2025-05-22 01:43:56,122]: Mean: 0.97197682
[2025-05-22 01:43:56,122]: Min: 0.88438463
[2025-05-22 01:43:56,122]: Max: 1.07327163
[2025-05-22 01:43:56,123]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 01:43:56,123]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10585501044988632, -0.10585501044988632, 0.0, -0.10585501044988632, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10585501044988632, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,124]: Mean: 0.00012635
[2025-05-22 01:43:56,124]: Min: -0.10585501
[2025-05-22 01:43:56,124]: Max: 0.21171002
[2025-05-22 01:43:56,124]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,124]: Sample Values (25 elements): [0.9848315119743347, 1.0328141450881958, 1.0329581499099731, 1.0859689712524414, 0.9383763074874878, 1.1894053220748901, 1.0287832021713257, 1.0575125217437744, 0.994872510433197, 1.0119946002960205, 1.0498241186141968, 1.0298153162002563, 1.1099448204040527, 0.9686378836631775, 1.0505411624908447, 1.0146186351776123, 1.022945523262024, 0.9821532964706421, 1.0283769369125366, 0.9855238199234009, 1.091330885887146, 1.1048141717910767, 1.0940907001495361, 0.9260044693946838, 0.9916114807128906]
[2025-05-22 01:43:56,124]: Mean: 1.02685022
[2025-05-22 01:43:56,125]: Min: 0.90160173
[2025-05-22 01:43:56,125]: Max: 1.21089661
[2025-05-22 01:43:56,127]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-22 01:43:56,127]: Sample Values (25 elements): [-0.20283886790275574, 0.0, 0.0, -0.20283886790275574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20283886790275574, -0.20283886790275574, 0.0, 0.0, -0.20283886790275574, 0.0, 0.0, 0.0, 0.0, 0.20283886790275574, -0.20283886790275574]
[2025-05-22 01:43:56,127]: Mean: 0.00019808
[2025-05-22 01:43:56,127]: Min: -0.40567774
[2025-05-22 01:43:56,127]: Max: 0.20283887
[2025-05-22 01:43:56,127]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,128]: Sample Values (25 elements): [0.827255129814148, 0.790777862071991, 0.8455277681350708, 0.7922959327697754, 0.6956251263618469, 0.9454842209815979, 0.8366195559501648, 0.8318996429443359, 0.8849272727966309, 0.8001565933227539, 0.8888087272644043, 0.8196168541908264, 0.7228496074676514, 0.8780871629714966, 0.8522177934646606, 0.7986891269683838, 0.8624448776245117, 0.8408011198043823, 0.8950941562652588, 0.8547210097312927, 0.8781614303588867, 0.8644906282424927, 0.75557541847229, 0.8386862874031067, 0.8516632318496704]
[2025-05-22 01:43:56,128]: Mean: 0.83431113
[2025-05-22 01:43:56,128]: Min: 0.69562513
[2025-05-22 01:43:56,128]: Max: 0.95665544
[2025-05-22 01:43:56,129]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 01:43:56,130]: Sample Values (25 elements): [0.0, 0.0, 0.10725723206996918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10725723206996918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10725723206996918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,130]: Mean: 0.00018330
[2025-05-22 01:43:56,130]: Min: -0.21451446
[2025-05-22 01:43:56,130]: Max: 0.10725723
[2025-05-22 01:43:56,130]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,130]: Sample Values (25 elements): [0.9189959764480591, 0.9993934631347656, 0.9161099791526794, 1.092226266860962, 0.9639763236045837, 0.9377279877662659, 1.0543437004089355, 0.9687791466712952, 0.9330711364746094, 0.9807173013687134, 0.9491464495658875, 0.9766585826873779, 1.0034191608428955, 0.9903989434242249, 0.9342674016952515, 0.9781475067138672, 1.0670273303985596, 0.9425597190856934, 0.952812671661377, 0.967824399471283, 1.0276635885238647, 0.9037189483642578, 0.9154778122901917, 1.0144875049591064, 1.0233186483383179]
[2025-05-22 01:43:56,131]: Mean: 0.98049259
[2025-05-22 01:43:56,131]: Min: 0.90371895
[2025-05-22 01:43:56,131]: Max: 1.09222627
[2025-05-22 01:43:56,132]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 01:43:56,133]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.09997344017028809, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09997344017028809]
[2025-05-22 01:43:56,133]: Mean: 0.00002170
[2025-05-22 01:43:56,133]: Min: -0.09997344
[2025-05-22 01:43:56,134]: Max: 0.19994688
[2025-05-22 01:43:56,134]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,134]: Sample Values (25 elements): [1.0549485683441162, 1.0271962881088257, 1.0430774688720703, 0.992529034614563, 0.9866387844085693, 1.079718828201294, 1.0323504209518433, 0.9887192249298096, 1.0610662698745728, 1.0538763999938965, 1.0283433198928833, 1.0234887599945068, 1.1374285221099854, 1.035770297050476, 0.9584228992462158, 0.9769066572189331, 1.0540263652801514, 1.075684905052185, 1.0130091905593872, 1.0507733821868896, 1.0422707796096802, 0.9762778282165527, 1.0610564947128296, 1.0376070737838745, 1.062749981880188]
[2025-05-22 01:43:56,134]: Mean: 1.02869391
[2025-05-22 01:43:56,134]: Min: 0.93234563
[2025-05-22 01:43:56,134]: Max: 1.21461308
[2025-05-22 01:43:56,135]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 01:43:56,136]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.08172658085823059, 0.0, 0.0, -0.08172658085823059, 0.0, -0.08172658085823059, 0.0, 0.0, 0.0, 0.08172658085823059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08172658085823059]
[2025-05-22 01:43:56,136]: Mean: -0.00022835
[2025-05-22 01:43:56,136]: Min: -0.08172658
[2025-05-22 01:43:56,136]: Max: 0.16345316
[2025-05-22 01:43:56,137]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,137]: Sample Values (25 elements): [0.9712695479393005, 0.9489338397979736, 0.9362403154373169, 0.931675374507904, 0.9389760494232178, 0.9007642269134521, 0.922870934009552, 0.8821860551834106, 0.9411746859550476, 0.9728279709815979, 0.9487363696098328, 0.976419985294342, 0.9189297556877136, 0.9680899977684021, 0.9054555296897888, 0.9120534658432007, 0.9265121221542358, 0.9837009906768799, 0.9719821810722351, 0.9571003317832947, 0.9300658702850342, 0.914670467376709, 0.9287837743759155, 0.9393206834793091, 0.9211587309837341]
[2025-05-22 01:43:56,137]: Mean: 0.93652141
[2025-05-22 01:43:56,137]: Min: 0.88218606
[2025-05-22 01:43:56,137]: Max: 1.02030349
[2025-05-22 01:43:56,138]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 01:43:56,139]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.07464569061994553, 0.0, 0.07464569061994553, 0.07464569061994553, 0.0, -0.07464569061994553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-22 01:43:56,139]: Mean: -0.00033208
[2025-05-22 01:43:56,139]: Min: -0.14929138
[2025-05-22 01:43:56,140]: Max: 0.07464569
[2025-05-22 01:43:56,140]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-22 01:43:56,140]: Sample Values (25 elements): [1.0037769079208374, 0.9598626494407654, 0.9696442484855652, 0.9883289933204651, 1.0009980201721191, 1.0124863386154175, 0.9981233477592468, 1.0190818309783936, 1.0355030298233032, 0.9919536709785461, 1.0172711610794067, 1.0268877744674683, 1.0233585834503174, 0.9721593260765076, 0.9771719574928284, 1.0105942487716675, 1.002714991569519, 0.9620814919471741, 1.0234249830245972, 1.0135451555252075, 0.9247547388076782, 1.0162885189056396, 0.986565351486206, 0.978201687335968, 0.9864000678062439]
[2025-05-22 01:43:56,141]: Mean: 0.99089754
[2025-05-22 01:43:56,141]: Min: 0.92475474
[2025-05-22 01:43:56,141]: Max: 1.04295909
[2025-05-22 01:43:56,141]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-22 01:43:56,141]: Sample Values (25 elements): [-0.3078475892543793, -0.06477358192205429, 0.13813230395317078, 0.1623115986585617, -0.26633164286613464, -0.2968613803386688, 0.2614539861679077, -0.15218324959278107, 0.2668120265007019, 0.25608935952186584, -0.22121934592723846, 0.13718067109584808, 0.27785956859588623, 0.1842632293701172, -0.34385696053504944, -0.24738937616348267, 0.18756911158561707, -0.29666006565093994, 0.11069029569625854, 0.36184149980545044, 0.020289240404963493, -0.1345663070678711, 0.035183895379304886, 0.2929622232913971, -0.005305323749780655]
[2025-05-22 01:43:56,141]: Mean: 0.00289001
[2025-05-22 01:43:56,142]: Min: -0.54028529
[2025-05-22 01:43:56,142]: Max: 0.55124050
[2025-05-22 01:43:56,142]: 


QAT of ResNet20 with parametrized_hardtanh down to 3 bits...
[2025-05-22 01:43:56,263]: [ResNet20_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-22 01:43:56,297]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 01:45:04,358]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 1.0493 Train Acc: 0.6320 Eval Loss: 1.0455 Eval Acc: 0.6350 (LR: 0.010000)
[2025-05-22 01:46:04,794]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.9172 Train Acc: 0.6792 Eval Loss: 0.9163 Eval Acc: 0.6755 (LR: 0.010000)
[2025-05-22 01:47:04,743]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.8671 Train Acc: 0.6962 Eval Loss: 0.9098 Eval Acc: 0.6885 (LR: 0.010000)
[2025-05-22 01:48:05,033]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.8253 Train Acc: 0.7116 Eval Loss: 0.8575 Eval Acc: 0.7060 (LR: 0.010000)
[2025-05-22 01:49:05,575]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.7961 Train Acc: 0.7221 Eval Loss: 0.9478 Eval Acc: 0.6933 (LR: 0.010000)
[2025-05-22 01:50:05,819]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.7761 Train Acc: 0.7283 Eval Loss: 0.9440 Eval Acc: 0.6864 (LR: 0.010000)
[2025-05-22 01:51:06,202]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.7481 Train Acc: 0.7372 Eval Loss: 0.9859 Eval Acc: 0.6768 (LR: 0.010000)
[2025-05-22 01:52:06,297]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.7300 Train Acc: 0.7448 Eval Loss: 0.9275 Eval Acc: 0.6873 (LR: 0.010000)
[2025-05-22 01:53:06,571]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.7146 Train Acc: 0.7504 Eval Loss: 0.9283 Eval Acc: 0.7022 (LR: 0.010000)
[2025-05-22 01:54:06,894]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.7009 Train Acc: 0.7551 Eval Loss: 0.7433 Eval Acc: 0.7438 (LR: 0.010000)
[2025-05-22 01:55:07,211]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.6847 Train Acc: 0.7612 Eval Loss: 0.8471 Eval Acc: 0.7228 (LR: 0.010000)
[2025-05-22 01:56:07,769]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.6772 Train Acc: 0.7636 Eval Loss: 0.9327 Eval Acc: 0.6986 (LR: 0.010000)
[2025-05-22 01:57:08,038]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.6628 Train Acc: 0.7683 Eval Loss: 0.7150 Eval Acc: 0.7628 (LR: 0.010000)
[2025-05-22 01:58:08,172]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.6486 Train Acc: 0.7724 Eval Loss: 0.7664 Eval Acc: 0.7438 (LR: 0.010000)
[2025-05-22 01:59:08,600]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.6410 Train Acc: 0.7755 Eval Loss: 0.7975 Eval Acc: 0.7337 (LR: 0.001000)
[2025-05-22 02:00:08,979]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.5454 Train Acc: 0.8111 Eval Loss: 0.5711 Eval Acc: 0.8049 (LR: 0.001000)
[2025-05-22 02:01:09,440]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.5243 Train Acc: 0.8165 Eval Loss: 0.5633 Eval Acc: 0.8093 (LR: 0.001000)
[2025-05-22 02:02:09,654]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.5172 Train Acc: 0.8198 Eval Loss: 0.5619 Eval Acc: 0.8111 (LR: 0.001000)
[2025-05-22 02:03:10,034]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.5106 Train Acc: 0.8227 Eval Loss: 0.5495 Eval Acc: 0.8135 (LR: 0.001000)
[2025-05-22 02:04:11,074]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.5129 Train Acc: 0.8204 Eval Loss: 0.5546 Eval Acc: 0.8152 (LR: 0.001000)
[2025-05-22 02:05:10,974]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.5070 Train Acc: 0.8230 Eval Loss: 0.5327 Eval Acc: 0.8206 (LR: 0.001000)
[2025-05-22 02:06:11,478]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.5044 Train Acc: 0.8241 Eval Loss: 0.5630 Eval Acc: 0.8121 (LR: 0.001000)
[2025-05-22 02:07:11,476]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.5016 Train Acc: 0.8255 Eval Loss: 0.5409 Eval Acc: 0.8169 (LR: 0.001000)
[2025-05-22 02:08:11,708]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.4995 Train Acc: 0.8258 Eval Loss: 0.5637 Eval Acc: 0.8081 (LR: 0.001000)
[2025-05-22 02:09:11,989]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.5066 Train Acc: 0.8238 Eval Loss: 0.5489 Eval Acc: 0.8149 (LR: 0.001000)
[2025-05-22 02:10:11,959]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.4967 Train Acc: 0.8267 Eval Loss: 0.5647 Eval Acc: 0.8112 (LR: 0.001000)
[2025-05-22 02:11:11,646]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.4938 Train Acc: 0.8284 Eval Loss: 0.5492 Eval Acc: 0.8150 (LR: 0.001000)
[2025-05-22 02:12:11,653]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.4947 Train Acc: 0.8271 Eval Loss: 0.6304 Eval Acc: 0.7902 (LR: 0.001000)
[2025-05-22 02:13:11,715]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.4894 Train Acc: 0.8286 Eval Loss: 0.5685 Eval Acc: 0.8073 (LR: 0.001000)
[2025-05-22 02:14:11,930]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.4930 Train Acc: 0.8279 Eval Loss: 0.5529 Eval Acc: 0.8113 (LR: 0.000100)
[2025-05-22 02:15:11,964]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.4762 Train Acc: 0.8345 Eval Loss: 0.5157 Eval Acc: 0.8253 (LR: 0.000100)
[2025-05-22 02:16:11,987]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.4676 Train Acc: 0.8373 Eval Loss: 0.5179 Eval Acc: 0.8235 (LR: 0.000100)
[2025-05-22 02:17:13,071]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.4635 Train Acc: 0.8382 Eval Loss: 0.5202 Eval Acc: 0.8249 (LR: 0.000100)
[2025-05-22 02:18:13,797]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.4651 Train Acc: 0.8378 Eval Loss: 0.5194 Eval Acc: 0.8260 (LR: 0.000100)
[2025-05-22 02:19:14,194]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.4648 Train Acc: 0.8388 Eval Loss: 0.5152 Eval Acc: 0.8240 (LR: 0.000100)
[2025-05-22 02:20:14,556]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.4700 Train Acc: 0.8359 Eval Loss: 0.5220 Eval Acc: 0.8288 (LR: 0.000100)
[2025-05-22 02:21:15,130]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.4632 Train Acc: 0.8389 Eval Loss: 0.5192 Eval Acc: 0.8272 (LR: 0.000100)
[2025-05-22 02:22:15,508]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.4632 Train Acc: 0.8390 Eval Loss: 0.5250 Eval Acc: 0.8243 (LR: 0.000100)
[2025-05-22 02:23:15,961]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.4678 Train Acc: 0.8365 Eval Loss: 0.5271 Eval Acc: 0.8221 (LR: 0.000100)
[2025-05-22 02:24:16,902]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.4640 Train Acc: 0.8379 Eval Loss: 0.5151 Eval Acc: 0.8245 (LR: 0.000100)
[2025-05-22 02:25:16,869]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.4618 Train Acc: 0.8390 Eval Loss: 0.5246 Eval Acc: 0.8213 (LR: 0.000100)
[2025-05-22 02:26:16,973]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.4613 Train Acc: 0.8387 Eval Loss: 0.5283 Eval Acc: 0.8217 (LR: 0.000100)
[2025-05-22 02:27:16,894]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.4622 Train Acc: 0.8372 Eval Loss: 0.5139 Eval Acc: 0.8267 (LR: 0.000100)
[2025-05-22 02:28:17,333]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.4613 Train Acc: 0.8406 Eval Loss: 0.5341 Eval Acc: 0.8197 (LR: 0.000100)
[2025-05-22 02:29:17,429]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.4626 Train Acc: 0.8389 Eval Loss: 0.5363 Eval Acc: 0.8191 (LR: 0.000010)
[2025-05-22 02:30:17,351]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.4582 Train Acc: 0.8396 Eval Loss: 0.5130 Eval Acc: 0.8292 (LR: 0.000010)
[2025-05-22 02:31:15,249]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.4545 Train Acc: 0.8413 Eval Loss: 0.5189 Eval Acc: 0.8259 (LR: 0.000010)
[2025-05-22 02:32:12,930]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.4565 Train Acc: 0.8403 Eval Loss: 0.5201 Eval Acc: 0.8262 (LR: 0.000010)
[2025-05-22 02:33:08,186]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.4582 Train Acc: 0.8420 Eval Loss: 0.5127 Eval Acc: 0.8267 (LR: 0.000010)
[2025-05-22 02:34:03,353]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.4554 Train Acc: 0.8415 Eval Loss: 0.5217 Eval Acc: 0.8273 (LR: 0.000010)
[2025-05-22 02:34:58,781]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.4535 Train Acc: 0.8415 Eval Loss: 0.5152 Eval Acc: 0.8281 (LR: 0.000010)
[2025-05-22 02:35:59,124]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.4543 Train Acc: 0.8439 Eval Loss: 0.5130 Eval Acc: 0.8275 (LR: 0.000010)
[2025-05-22 02:36:59,056]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.4582 Train Acc: 0.8400 Eval Loss: 0.5084 Eval Acc: 0.8271 (LR: 0.000010)
[2025-05-22 02:37:59,296]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.4544 Train Acc: 0.8418 Eval Loss: 0.5116 Eval Acc: 0.8269 (LR: 0.000010)
[2025-05-22 02:38:59,446]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.4577 Train Acc: 0.8402 Eval Loss: 0.5152 Eval Acc: 0.8270 (LR: 0.000010)
[2025-05-22 02:40:00,083]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.4595 Train Acc: 0.8409 Eval Loss: 0.5120 Eval Acc: 0.8277 (LR: 0.000010)
[2025-05-22 02:41:00,592]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.4607 Train Acc: 0.8388 Eval Loss: 0.5084 Eval Acc: 0.8310 (LR: 0.000010)
[2025-05-22 02:42:00,922]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.4580 Train Acc: 0.8388 Eval Loss: 0.5124 Eval Acc: 0.8276 (LR: 0.000010)
[2025-05-22 02:43:01,080]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.4551 Train Acc: 0.8406 Eval Loss: 0.5211 Eval Acc: 0.8247 (LR: 0.000010)
[2025-05-22 02:44:01,821]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.4601 Train Acc: 0.8389 Eval Loss: 0.5081 Eval Acc: 0.8311 (LR: 0.000010)
[2025-05-22 02:44:01,860]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8311
[2025-05-22 02:44:01,931]: 


Quantization of model down to 3 bits finished
[2025-05-22 02:44:01,931]: Model Architecture:
[2025-05-22 02:44:01,990]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2857], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1109], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40062907338142395, max_val=0.37549787759780884)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3373], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2063], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7033056616783142, max_val=0.7406371831893921)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4438], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1592], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49417322874069214, max_val=0.6202703714370728)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2851], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1127], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4035855531692505, max_val=0.38517844676971436)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5163], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1145], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3612104654312134, max_val=0.44041839241981506)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2949], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0878], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30874118208885193, max_val=0.3059648275375366)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6094], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0717], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2504303753376007, max_val=0.25113505125045776)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3017], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0636], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22248180210590363, max_val=0.22258159518241882)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1231], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4308706521987915, max_val=0.43088021874427795)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4244], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0697], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2668711245059967, max_val=0.22128991782665253)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2610], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0650], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2251935750246048, max_val=0.22965842485427856)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4194], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0652], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23372164368629456, max_val=0.22293099761009216)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2554], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20260904729366302, max_val=0.20262230932712555)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4889], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0538], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1832166314125061, max_val=0.19361847639083862)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2649], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0500], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18904338777065277, max_val=0.16079257428646088)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0879], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3121001720428467, max_val=0.30296269059181213)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3747], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0471], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16693878173828125, max_val=0.1628042757511139)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2456], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0441], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1514284908771515, max_val=0.157331645488739)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3502], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12630239129066467, max_val=0.12966784834861755)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1588], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0308], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10672406852245331, max_val=0.10894129425287247)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6216], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 02:44:01,990]: 
Model Weights:
[2025-05-22 02:44:01,990]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-22 02:44:01,990]: Sample Values (25 elements): [-0.3630969524383545, -0.24380773305892944, 0.07101228088140488, -0.1513524204492569, -0.3731635808944702, 0.03413314372301102, 0.16498902440071106, -0.22005338966846466, -0.08927509933710098, 0.10809370130300522, -0.6005524396896362, -0.3033123314380646, 0.36559176445007324, 0.3227747678756714, -0.2418213188648224, 0.13333842158317566, -0.35597801208496094, -0.17474880814552307, 0.20468224585056305, 0.37831225991249084, -0.007454343605786562, 0.2458249181509018, 0.1750667691230774, 0.23556801676750183, -0.23788537085056305]
[2025-05-22 02:44:01,991]: Mean: -0.00335062
[2025-05-22 02:44:01,991]: Min: -0.71074575
[2025-05-22 02:44:01,991]: Max: 0.98850167
[2025-05-22 02:44:01,991]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-22 02:44:01,991]: Sample Values (16 elements): [0.43109965324401855, 0.5526523590087891, 0.8030444979667664, 0.5265733599662781, 0.8583707213401794, 1.0659312009811401, 0.7670873999595642, 0.7655968070030212, 0.829860270023346, 1.1913392543792725, 0.7701742649078369, 0.513031005859375, 0.878609299659729, 1.0461759567260742, 0.9634512066841125, 0.5540570616722107]
[2025-05-22 02:44:01,991]: Mean: 0.78231585
[2025-05-22 02:44:01,992]: Min: 0.43109965
[2025-05-22 02:44:01,992]: Max: 1.19133925
[2025-05-22 02:44:01,993]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 02:44:01,993]: Sample Values (25 elements): [-0.11087528616189957, 0.0, 0.11087528616189957, 0.22175057232379913, -0.11087528616189957, 0.11087528616189957, 0.0, 0.11087528616189957, 0.11087528616189957, 0.0, -0.11087528616189957, -0.11087528616189957, 0.11087528616189957, -0.11087528616189957, 0.0, -0.11087528616189957, 0.0, 0.0, 0.0, 0.11087528616189957, -0.22175057232379913, 0.22175057232379913, -0.11087528616189957, 0.11087528616189957, 0.0]
[2025-05-22 02:44:01,993]: Mean: 0.00596725
[2025-05-22 02:44:01,993]: Min: -0.44350114
[2025-05-22 02:44:01,994]: Max: 0.33262587
[2025-05-22 02:44:01,994]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-22 02:44:01,994]: Sample Values (16 elements): [1.131595253944397, 1.2494404315948486, 0.7709653377532959, 1.0191189050674438, 0.7607554793357849, 0.6045123934745789, 1.04993736743927, 0.8456268906593323, 1.1013710498809814, 1.1912376880645752, 0.9393357038497925, 0.6182758212089539, 0.8031534552574158, 0.8687953948974609, 0.9065718650817871, 1.2443054914474487]
[2025-05-22 02:44:01,994]: Mean: 0.94406241
[2025-05-22 02:44:01,994]: Min: 0.60451239
[2025-05-22 02:44:01,994]: Max: 1.24944043
[2025-05-22 02:44:01,995]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 02:44:01,996]: Sample Values (25 elements): [0.0, 0.0, -0.20627756416797638, 0.0, -0.20627756416797638, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20627756416797638, 0.0, 0.0, 0.0, 0.20627756416797638, -0.20627756416797638, 0.0, 0.20627756416797638, 0.0, 0.0, 0.0, 0.0, 0.20627756416797638, 0.0, -0.20627756416797638]
[2025-05-22 02:44:01,996]: Mean: -0.00707289
[2025-05-22 02:44:01,996]: Min: -0.61883271
[2025-05-22 02:44:01,996]: Max: 0.82511026
[2025-05-22 02:44:01,996]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-22 02:44:01,996]: Sample Values (16 elements): [0.9220651388168335, 1.0141459703445435, 0.5968241691589355, 1.2912659645080566, 0.7693748474121094, 0.6221686601638794, 0.8673588633537292, 0.7214414477348328, 1.3522531986236572, 0.6741713881492615, 0.5417142510414124, 1.2052066326141357, 0.9325099587440491, 0.6474390625953674, 0.8467585444450378, 0.8088238835334778]
[2025-05-22 02:44:01,997]: Mean: 0.86334515
[2025-05-22 02:44:01,997]: Min: 0.54171425
[2025-05-22 02:44:01,997]: Max: 1.35225320
[2025-05-22 02:44:01,998]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 02:44:01,998]: Sample Values (25 elements): [0.0, 0.15920624136924744, -0.15920624136924744, 0.0, 0.0, 0.15920624136924744, 0.0, 0.0, -0.15920624136924744, 0.0, -0.15920624136924744, 0.0, 0.15920624136924744, 0.15920624136924744, -0.15920624136924744, -0.15920624136924744, 0.0, 0.15920624136924744, -0.15920624136924744, 0.0, 0.15920624136924744, 0.0, -0.15920624136924744, 0.0, -0.15920624136924744]
[2025-05-22 02:44:01,999]: Mean: 0.00020730
[2025-05-22 02:44:01,999]: Min: -0.47761872
[2025-05-22 02:44:01,999]: Max: 0.63682497
[2025-05-22 02:44:01,999]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-22 02:44:01,999]: Sample Values (16 elements): [0.9480468034744263, 0.9893788695335388, 0.7952783703804016, 0.8784711956977844, 1.0025238990783691, 1.1482845544815063, 0.8911135196685791, 0.7244859933853149, 0.8085201978683472, 0.8742537498474121, 1.1089990139007568, 0.9825564026832581, 1.001114010810852, 0.7960546016693115, 0.8828103542327881, 0.8155397772789001]
[2025-05-22 02:44:01,999]: Mean: 0.91546446
[2025-05-22 02:44:02,000]: Min: 0.72448599
[2025-05-22 02:44:02,000]: Max: 1.14828455
[2025-05-22 02:44:02,001]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 02:44:02,001]: Sample Values (25 elements): [0.0, 0.11268056184053421, 0.11268056184053421, 0.11268056184053421, 0.0, 0.0, 0.11268056184053421, 0.11268056184053421, 0.11268056184053421, 0.0, 0.0, -0.11268056184053421, 0.11268056184053421, 0.0, 0.0, 0.22536112368106842, 0.0, 0.11268056184053421, 0.11268056184053421, 0.11268056184053421, 0.11268056184053421, 0.0, -0.11268056184053421, 0.0, 0.11268056184053421]
[2025-05-22 02:44:02,001]: Mean: 0.00234751
[2025-05-22 02:44:02,002]: Min: -0.45072225
[2025-05-22 02:44:02,003]: Max: 0.33804169
[2025-05-22 02:44:02,003]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-22 02:44:02,003]: Sample Values (16 elements): [1.1640938520431519, 0.652803897857666, 0.908545732498169, 0.7679232954978943, 0.9594711661338806, 0.8366090655326843, 0.8476104736328125, 0.745606541633606, 0.865725576877594, 0.7627966403961182, 1.3123761415481567, 0.8287877440452576, 0.7839937210083008, 0.8479043245315552, 1.0689997673034668, 0.8871934413909912]
[2025-05-22 02:44:02,004]: Mean: 0.89002752
[2025-05-22 02:44:02,004]: Min: 0.65280390
[2025-05-22 02:44:02,004]: Max: 1.31237614
[2025-05-22 02:44:02,005]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 02:44:02,006]: Sample Values (25 elements): [0.0, 0.1145184114575386, -0.1145184114575386, 0.1145184114575386, 0.0, 0.1145184114575386, 0.0, 0.0, 0.0, 0.1145184114575386, 0.0, -0.1145184114575386, 0.0, -0.1145184114575386, 0.0, -0.2290368229150772, -0.1145184114575386, -0.1145184114575386, 0.2290368229150772, 0.0, -0.1145184114575386, -0.1145184114575386, 0.1145184114575386, -0.2290368229150772, -0.1145184114575386]
[2025-05-22 02:44:02,006]: Mean: 0.00502012
[2025-05-22 02:44:02,006]: Min: -0.34355524
[2025-05-22 02:44:02,006]: Max: 0.45807365
[2025-05-22 02:44:02,006]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-22 02:44:02,006]: Sample Values (16 elements): [0.7674907445907593, 0.9017661213874817, 1.0724679231643677, 0.9372996687889099, 0.8642246723175049, 0.8880006074905396, 0.7394310832023621, 0.9850162863731384, 1.0586143732070923, 0.9802584052085876, 1.021857500076294, 0.8992210626602173, 0.7711740732192993, 1.0612322092056274, 0.7839664816856384, 0.8187006711959839]
[2025-05-22 02:44:02,007]: Mean: 0.90942013
[2025-05-22 02:44:02,007]: Min: 0.73943108
[2025-05-22 02:44:02,007]: Max: 1.07246792
[2025-05-22 02:44:02,008]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 02:44:02,008]: Sample Values (25 elements): [0.08781515061855316, -0.17563030123710632, -0.17563030123710632, 0.17563030123710632, 0.08781515061855316, -0.08781515061855316, 0.08781515061855316, 0.08781515061855316, 0.08781515061855316, -0.08781515061855316, 0.17563030123710632, -0.08781515061855316, 0.0, -0.17563030123710632, 0.08781515061855316, -0.17563030123710632, 0.0, -0.08781515061855316, -0.08781515061855316, 0.08781515061855316, -0.08781515061855316, 0.0, 0.0, 0.0, 0.08781515061855316]
[2025-05-22 02:44:02,008]: Mean: -0.00106720
[2025-05-22 02:44:02,009]: Min: -0.35126060
[2025-05-22 02:44:02,009]: Max: 0.26344544
[2025-05-22 02:44:02,009]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-22 02:44:02,009]: Sample Values (16 elements): [0.8115343451499939, 0.7418336272239685, 0.840064525604248, 0.8930526971817017, 0.8145613670349121, 0.8788167834281921, 0.9205171465873718, 0.8604775667190552, 0.8886399269104004, 0.8961472511291504, 0.8912880420684814, 0.9609108567237854, 1.1321567296981812, 1.1043025255203247, 0.914299488067627, 0.7899981141090393]
[2025-05-22 02:44:02,009]: Mean: 0.89616257
[2025-05-22 02:44:02,010]: Min: 0.74183363
[2025-05-22 02:44:02,010]: Max: 1.13215673
[2025-05-22 02:44:02,012]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-22 02:44:02,012]: Sample Values (25 elements): [0.14330443739891052, 0.0, 0.07165221869945526, 0.14330443739891052, -0.14330443739891052, 0.07165221869945526, 0.0, 0.07165221869945526, -0.14330443739891052, 0.0, 0.0, 0.07165221869945526, 0.07165221869945526, -0.07165221869945526, -0.07165221869945526, -0.07165221869945526, -0.07165221869945526, 0.07165221869945526, 0.07165221869945526, 0.0, 0.0, -0.07165221869945526, 0.0, 0.14330443739891052, 0.0]
[2025-05-22 02:44:02,013]: Mean: 0.00069973
[2025-05-22 02:44:02,013]: Min: -0.21495666
[2025-05-22 02:44:02,013]: Max: 0.28660887
[2025-05-22 02:44:02,013]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,013]: Sample Values (25 elements): [0.9660561680793762, 0.9769331216812134, 0.9107779860496521, 0.9528915286064148, 0.815635085105896, 1.0758953094482422, 0.9852601289749146, 0.9072879552841187, 0.9153038263320923, 0.9405477643013, 0.9035898447036743, 0.8335142731666565, 0.8005291819572449, 0.866241991519928, 0.9848067164421082, 0.9181177616119385, 0.9124841094017029, 1.020672082901001, 0.8184724450111389, 0.8770581483840942, 0.8968717455863953, 0.9062395095825195, 0.8835263252258301, 0.885112464427948, 0.9644277691841125]
[2025-05-22 02:44:02,013]: Mean: 0.91440701
[2025-05-22 02:44:02,014]: Min: 0.79972881
[2025-05-22 02:44:02,014]: Max: 1.07589531
[2025-05-22 02:44:02,015]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 02:44:02,015]: Sample Values (25 elements): [0.0, 0.06358049064874649, 0.0, 0.06358049064874649, 0.0, 0.0, 0.0, -0.06358049064874649, 0.06358049064874649, -0.12716098129749298, 0.0, -0.06358049064874649, 0.0, 0.12716098129749298, 0.0, -0.06358049064874649, -0.06358049064874649, 0.06358049064874649, 0.0, -0.12716098129749298, 0.06358049064874649, 0.0, 0.0, 0.06358049064874649, -0.12716098129749298]
[2025-05-22 02:44:02,015]: Mean: -0.00239393
[2025-05-22 02:44:02,016]: Min: -0.19074148
[2025-05-22 02:44:02,016]: Max: 0.25432196
[2025-05-22 02:44:02,016]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,016]: Sample Values (25 elements): [0.9584041833877563, 0.9525273442268372, 0.9487152695655823, 0.8782575130462646, 1.0264848470687866, 0.7920639514923096, 0.9783015251159668, 0.8567655682563782, 0.94697105884552, 0.900607705116272, 0.8985921144485474, 0.9712079167366028, 0.9473470449447632, 0.9302049875259399, 1.0090734958648682, 1.004486083984375, 1.0679343938827515, 0.8542209267616272, 0.9344316720962524, 0.8643606305122375, 0.9218343496322632, 0.9471055865287781, 1.0953956842422485, 0.8778802752494812, 0.9106428623199463]
[2025-05-22 02:44:02,017]: Mean: 0.93328691
[2025-05-22 02:44:02,017]: Min: 0.79206395
[2025-05-22 02:44:02,017]: Max: 1.09539568
[2025-05-22 02:44:02,018]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-22 02:44:02,019]: Sample Values (25 elements): [0.0, 0.24621453881263733, -0.12310726940631866, -0.12310726940631866, 0.0, -0.12310726940631866, 0.12310726940631866, 0.12310726940631866, 0.12310726940631866, 0.0, -0.24621453881263733, -0.12310726940631866, -0.12310726940631866, -0.12310726940631866, 0.24621453881263733, 0.12310726940631866, 0.24621453881263733, -0.12310726940631866, 0.12310726940631866, 0.0, -0.24621453881263733, 0.12310726940631866, -0.12310726940631866, -0.12310726940631866, -0.12310726940631866]
[2025-05-22 02:44:02,019]: Mean: 0.01250308
[2025-05-22 02:44:02,019]: Min: -0.36932182
[2025-05-22 02:44:02,019]: Max: 0.49242908
[2025-05-22 02:44:02,019]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,019]: Sample Values (25 elements): [0.9363388419151306, 0.8012763857841492, 0.6896025538444519, 0.7364360094070435, 0.846876323223114, 0.8621988892555237, 0.7590572237968445, 0.742152988910675, 0.7834811806678772, 0.716134786605835, 0.704118013381958, 0.817990243434906, 0.7650440335273743, 0.8625665307044983, 0.7659797668457031, 0.7494552135467529, 0.7223753333091736, 0.688493549823761, 0.7341541051864624, 0.8254525661468506, 0.8670414090156555, 0.7200531959533691, 0.7681372761726379, 0.7885753512382507, 0.723874568939209]
[2025-05-22 02:44:02,020]: Mean: 0.77023721
[2025-05-22 02:44:02,020]: Min: 0.64581901
[2025-05-22 02:44:02,020]: Max: 0.93633884
[2025-05-22 02:44:02,021]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 02:44:02,021]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.06973730772733688, 0.0, 0.06973730772733688, 0.0, 0.0, 0.06973730772733688, 0.0, 0.0, 0.0, -0.06973730772733688, -0.06973730772733688, 0.0, 0.20921191573143005, 0.0, 0.0, 0.0, -0.06973730772733688, 0.0, -0.06973730772733688, -0.06973730772733688, -0.06973730772733688]
[2025-05-22 02:44:02,022]: Mean: -0.00075670
[2025-05-22 02:44:02,022]: Min: -0.27894923
[2025-05-22 02:44:02,022]: Max: 0.20921192
[2025-05-22 02:44:02,022]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,022]: Sample Values (25 elements): [0.9019010066986084, 0.9159767031669617, 1.018905758857727, 0.9520589709281921, 0.8682067394256592, 0.9155862331390381, 0.9206237196922302, 0.949759304523468, 0.9455156326293945, 0.8995102047920227, 0.9304072260856628, 0.8587710857391357, 0.9503121376037598, 1.0068798065185547, 0.8267925381660461, 0.8635944128036499, 0.8680265545845032, 0.9899688363075256, 0.951807975769043, 0.9334054589271545, 0.9447851181030273, 0.8235560059547424, 0.9666514992713928, 0.9037833213806152, 0.8945119976997375]
[2025-05-22 02:44:02,023]: Mean: 0.91966021
[2025-05-22 02:44:02,023]: Min: 0.82355601
[2025-05-22 02:44:02,023]: Max: 1.01890576
[2025-05-22 02:44:02,025]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 02:44:02,025]: Sample Values (25 elements): [-0.06497886031866074, 0.0, 0.0, 0.1949365735054016, -0.06497886031866074, 0.0, -0.06497886031866074, -0.06497886031866074, 0.06497886031866074, 0.0, -0.06497886031866074, 0.0, 0.0, 0.0, 0.0, 0.06497886031866074, -0.06497886031866074, 0.0, 0.0, 0.1949365735054016, 0.0, 0.06497886031866074, -0.06497886031866074, 0.0, 0.06497886031866074]
[2025-05-22 02:44:02,025]: Mean: 0.00047239
[2025-05-22 02:44:02,025]: Min: -0.19493657
[2025-05-22 02:44:02,025]: Max: 0.25991544
[2025-05-22 02:44:02,026]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,026]: Sample Values (25 elements): [0.9755770564079285, 0.8806858658790588, 0.872190535068512, 1.0171799659729004, 0.7127731442451477, 0.8928444385528564, 0.8187870979309082, 0.9035440683364868, 0.7628318071365356, 0.9723276495933533, 0.8787415027618408, 0.8426781296730042, 0.9324933886528015, 0.9961827397346497, 0.9509950876235962, 0.8526304960250854, 0.9711906909942627, 0.8179647326469421, 0.9470632076263428, 0.7407880425453186, 0.9686439633369446, 1.0299561023712158, 0.8915984034538269, 0.9507198929786682, 0.7337462306022644]
[2025-05-22 02:44:02,026]: Mean: 0.89965963
[2025-05-22 02:44:02,026]: Min: 0.71277314
[2025-05-22 02:44:02,026]: Max: 1.03522491
[2025-05-22 02:44:02,027]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 02:44:02,028]: Sample Values (25 elements): [0.06523610651493073, 0.13047221302986145, -0.06523610651493073, 0.06523610651493073, 0.06523610651493073, -0.06523610651493073, 0.0, -0.13047221302986145, 0.0, 0.06523610651493073, 0.06523610651493073, 0.13047221302986145, -0.06523610651493073, 0.0, 0.06523610651493073, 0.06523610651493073, 0.13047221302986145, 0.0, 0.0, 0.0, 0.06523610651493073, 0.13047221302986145, -0.13047221302986145, 0.0, 0.0]
[2025-05-22 02:44:02,028]: Mean: -0.00004955
[2025-05-22 02:44:02,028]: Min: -0.26094443
[2025-05-22 02:44:02,028]: Max: 0.19570832
[2025-05-22 02:44:02,028]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,028]: Sample Values (25 elements): [0.9320098161697388, 0.9122759103775024, 0.954694926738739, 0.9156016707420349, 0.8595851063728333, 0.9933456182479858, 0.9649217128753662, 0.8452801704406738, 0.9290698170661926, 0.9103239178657532, 0.8846067190170288, 0.9121459126472473, 0.9423418641090393, 0.9942787885665894, 0.9242003560066223, 0.9212953448295593, 0.9710063338279724, 0.9516870379447937, 0.9264810681343079, 0.8857868909835815, 0.9519540071487427, 1.0378001928329468, 0.9202156662940979, 0.889664351940155, 0.8393096327781677]
[2025-05-22 02:44:02,029]: Mean: 0.92522967
[2025-05-22 02:44:02,029]: Min: 0.83930963
[2025-05-22 02:44:02,029]: Max: 1.03780019
[2025-05-22 02:44:02,030]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 02:44:02,031]: Sample Values (25 elements): [0.0, -0.11578037589788437, 0.057890187948942184, 0.057890187948942184, 0.17367056012153625, 0.057890187948942184, 0.057890187948942184, 0.0, 0.0, -0.057890187948942184, 0.057890187948942184, 0.057890187948942184, 0.0, -0.057890187948942184, 0.057890187948942184, -0.057890187948942184, 0.057890187948942184, 0.057890187948942184, 0.0, -0.057890187948942184, 0.0, 0.11578037589788437, -0.057890187948942184, 0.057890187948942184, 0.0]
[2025-05-22 02:44:02,031]: Mean: -0.00082287
[2025-05-22 02:44:02,031]: Min: -0.17367056
[2025-05-22 02:44:02,031]: Max: 0.23156075
[2025-05-22 02:44:02,031]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-22 02:44:02,032]: Sample Values (25 elements): [0.9756358861923218, 0.9138874411582947, 1.0159800052642822, 0.9388395547866821, 0.8844290971755981, 1.0614925622940063, 0.9166104197502136, 1.0311468839645386, 0.9090870022773743, 1.0978800058364868, 1.0917788743972778, 0.9282058477401733, 1.1314088106155396, 0.8772417306900024, 0.9149401783943176, 0.8470699787139893, 0.9772875905036926, 0.7525442838668823, 0.8992120027542114, 0.9634261131286621, 1.0182472467422485, 0.9764913320541382, 0.9470545053482056, 0.9702166318893433, 0.9706719517707825]
[2025-05-22 02:44:02,032]: Mean: 0.95888257
[2025-05-22 02:44:02,032]: Min: 0.75254428
[2025-05-22 02:44:02,032]: Max: 1.13140881
[2025-05-22 02:44:02,033]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-22 02:44:02,033]: Sample Values (25 elements): [0.0, 0.05383358895778656, 0.10766717791557312, 0.05383358895778656, 0.10766717791557312, 0.0, 0.0, 0.05383358895778656, 0.0, 0.0, 0.10766717791557312, 0.0, -0.05383358895778656, -0.05383358895778656, -0.10766717791557312, 0.05383358895778656, 0.05383358895778656, 0.05383358895778656, 0.0, 0.10766717791557312, -0.05383358895778656, 0.0, 0.0, -0.05383358895778656, -0.05383358895778656]
[2025-05-22 02:44:02,034]: Mean: 0.00020737
[2025-05-22 02:44:02,034]: Min: -0.16150077
[2025-05-22 02:44:02,034]: Max: 0.21533436
[2025-05-22 02:44:02,034]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,034]: Sample Values (25 elements): [0.9145050644874573, 0.9360394477844238, 0.920525312423706, 0.9062710404396057, 0.9271190762519836, 0.9316326975822449, 0.9248301386833191, 0.9698562026023865, 0.889122724533081, 0.9405391812324524, 0.9039953947067261, 0.9452590346336365, 0.9709134697914124, 0.8504719734191895, 0.8846661448478699, 0.8964065909385681, 0.9103376865386963, 0.9041789770126343, 0.9620703458786011, 0.8917232751846313, 0.9003521203994751, 0.9351352453231812, 0.8997445702552795, 0.9464964866638184, 0.892995297908783]
[2025-05-22 02:44:02,034]: Mean: 0.92038536
[2025-05-22 02:44:02,035]: Min: 0.85047197
[2025-05-22 02:44:02,035]: Max: 0.98713046
[2025-05-22 02:44:02,036]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 02:44:02,036]: Sample Values (25 elements): [0.049976564943790436, 0.09995312988758087, 0.049976564943790436, -0.049976564943790436, 0.0, 0.0, -0.049976564943790436, 0.049976564943790436, 0.0, -0.049976564943790436, 0.0, 0.0, -0.049976564943790436, 0.0, -0.049976564943790436, 0.0, -0.049976564943790436, 0.0, -0.049976564943790436, 0.0, 0.049976564943790436, 0.049976564943790436, -0.049976564943790436, 0.049976564943790436, -0.049976564943790436]
[2025-05-22 02:44:02,037]: Mean: 0.00041078
[2025-05-22 02:44:02,037]: Min: -0.19990626
[2025-05-22 02:44:02,037]: Max: 0.14992970
[2025-05-22 02:44:02,037]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,037]: Sample Values (25 elements): [0.88735032081604, 1.0764164924621582, 0.9949515461921692, 1.1045668125152588, 0.8978118896484375, 1.0138362646102905, 0.987441897392273, 0.8523995280265808, 0.9401282668113708, 0.9595340490341187, 1.0663483142852783, 0.9969798922538757, 0.9649387001991272, 0.9370717406272888, 0.9806531071662903, 0.9720560312271118, 0.9677983522415161, 0.9835710525512695, 0.9964309334754944, 0.9408313632011414, 0.9693477749824524, 1.0333606004714966, 0.979188084602356, 0.9949605464935303, 0.9664721488952637]
[2025-05-22 02:44:02,038]: Mean: 0.96581006
[2025-05-22 02:44:02,038]: Min: 0.84867197
[2025-05-22 02:44:02,038]: Max: 1.10456681
[2025-05-22 02:44:02,039]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-22 02:44:02,040]: Sample Values (25 elements): [0.08786612004041672, 0.08786612004041672, -0.08786612004041672, 0.0, 0.0, 0.0, -0.08786612004041672, 0.0, 0.08786612004041672, -0.17573224008083344, -0.17573224008083344, -0.08786612004041672, 0.0, 0.08786612004041672, 0.17573224008083344, -0.08786612004041672, -0.08786612004041672, 0.0, 0.17573224008083344, 0.0, -0.08786612004041672, -0.08786612004041672, 0.0, -0.17573224008083344, 0.0]
[2025-05-22 02:44:02,040]: Mean: 0.00107258
[2025-05-22 02:44:02,040]: Min: -0.35146448
[2025-05-22 02:44:02,040]: Max: 0.26359835
[2025-05-22 02:44:02,040]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,040]: Sample Values (25 elements): [0.829862117767334, 0.8624868392944336, 0.8233648538589478, 0.8111484050750732, 0.8636707067489624, 0.8947657346725464, 0.8689145445823669, 0.794461727142334, 0.9117867946624756, 0.9089487791061401, 0.8236303925514221, 0.7300006151199341, 0.8809376955032349, 0.874654233455658, 0.9288630485534668, 0.889285683631897, 0.800948977470398, 0.7819374203681946, 0.8497521281242371, 0.8522340655326843, 0.8903769254684448, 0.8849954009056091, 0.8020902872085571, 0.8499255180358887, 0.8287864327430725]
[2025-05-22 02:44:02,041]: Mean: 0.83745420
[2025-05-22 02:44:02,041]: Min: 0.70114547
[2025-05-22 02:44:02,041]: Max: 0.92886305
[2025-05-22 02:44:02,042]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 02:44:02,042]: Sample Values (25 elements): [0.0, 0.04710615053772926, 0.0, 0.0, 0.04710615053772926, -0.04710615053772926, 0.0, 0.04710615053772926, 0.0, -0.04710615053772926, 0.0, 0.0, -0.04710615053772926, 0.0, 0.04710615053772926, 0.0, 0.04710615053772926, -0.04710615053772926, 0.0, 0.0, -0.04710615053772926, -0.04710615053772926, 0.04710615053772926, 0.0, -0.04710615053772926]
[2025-05-22 02:44:02,043]: Mean: 0.00018145
[2025-05-22 02:44:02,043]: Min: -0.18842460
[2025-05-22 02:44:02,043]: Max: 0.14131846
[2025-05-22 02:44:02,043]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,043]: Sample Values (25 elements): [0.9266611337661743, 0.9063602089881897, 0.921859860420227, 0.9804210066795349, 0.9550023674964905, 0.9093336462974548, 0.9538756012916565, 0.958880603313446, 0.8955166339874268, 0.9598084092140198, 0.897707462310791, 0.8904165029525757, 0.9128856658935547, 0.9834155440330505, 0.9821296334266663, 0.9417543411254883, 0.9449634552001953, 0.9109672904014587, 0.9951263666152954, 0.946498692035675, 0.9748274087905884, 0.9180470705032349, 0.9705071449279785, 0.9749473929405212, 0.9159767031669617]
[2025-05-22 02:44:02,043]: Mean: 0.93877274
[2025-05-22 02:44:02,044]: Min: 0.89041650
[2025-05-22 02:44:02,044]: Max: 0.99512637
[2025-05-22 02:44:02,045]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 02:44:02,045]: Sample Values (25 elements): [0.0, 0.0, -0.044108591973781586, -0.044108591973781586, 0.0, -0.044108591973781586, 0.0, 0.08821718394756317, 0.044108591973781586, 0.0, -0.044108591973781586, 0.0, 0.0, -0.044108591973781586, 0.0, 0.044108591973781586, 0.044108591973781586, 0.044108591973781586, -0.044108591973781586, 0.044108591973781586, -0.08821718394756317, 0.0, 0.0, -0.044108591973781586, -0.044108591973781586]
[2025-05-22 02:44:02,046]: Mean: 0.00042237
[2025-05-22 02:44:02,046]: Min: -0.13232577
[2025-05-22 02:44:02,046]: Max: 0.17643437
[2025-05-22 02:44:02,046]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,046]: Sample Values (25 elements): [0.9354803562164307, 0.9488111734390259, 0.9936567544937134, 0.9654672145843506, 1.0158804655075073, 0.9304354190826416, 0.9436015486717224, 1.031585454940796, 0.9168605208396912, 0.9308516383171082, 0.9952303171157837, 1.021453619003296, 1.0511579513549805, 0.936311662197113, 0.9107242226600647, 0.9734299182891846, 0.9810799360275269, 1.0054630041122437, 1.081832766532898, 0.9854487776756287, 0.920976996421814, 0.9217318892478943, 0.9417065382003784, 0.9461663961410522, 0.9473766088485718]
[2025-05-22 02:44:02,046]: Mean: 0.96175677
[2025-05-22 02:44:02,046]: Min: 0.88778162
[2025-05-22 02:44:02,047]: Max: 1.08183277
[2025-05-22 02:44:02,048]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 02:44:02,048]: Sample Values (25 elements): [-0.03656717762351036, -0.03656717762351036, 0.0, 0.0, 0.0, -0.03656717762351036, -0.03656717762351036, -0.03656717762351036, 0.0, 0.0, 0.0, 0.03656717762351036, -0.07313435524702072, 0.0, 0.0, 0.0, -0.03656717762351036, 0.0, 0.07313435524702072, 0.0, -0.03656717762351036, 0.03656717762351036, 0.03656717762351036, -0.03656717762351036, 0.03656717762351036]
[2025-05-22 02:44:02,048]: Mean: -0.00005952
[2025-05-22 02:44:02,048]: Min: -0.10970153
[2025-05-22 02:44:02,049]: Max: 0.14626871
[2025-05-22 02:44:02,049]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,049]: Sample Values (25 elements): [0.8965726494789124, 0.9041770696640015, 0.9232911467552185, 0.9483850598335266, 0.959317147731781, 0.9094457626342773, 0.9076395630836487, 0.9276663661003113, 0.9143024682998657, 0.882830798625946, 0.9179162979125977, 0.9030760526657104, 0.9469045400619507, 0.9204849004745483, 0.9299495816230774, 0.9232374429702759, 0.9102897047996521, 0.9266034364700317, 0.9537917971611023, 0.9209786653518677, 0.9299232959747314, 0.9320439696311951, 0.9016919136047363, 0.9171733856201172, 0.9190878868103027]
[2025-05-22 02:44:02,049]: Mean: 0.92304945
[2025-05-22 02:44:02,049]: Min: 0.88283080
[2025-05-22 02:44:02,049]: Max: 1.00036311
[2025-05-22 02:44:02,051]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 02:44:02,052]: Sample Values (25 elements): [0.0, 0.03080933913588524, -0.03080933913588524, -0.03080933913588524, 0.0, -0.03080933913588524, 0.03080933913588524, 0.0, -0.03080933913588524, 0.03080933913588524, -0.03080933913588524, 0.03080933913588524, 0.06161867827177048, 0.0, -0.03080933913588524, 0.0, 0.03080933913588524, 0.0, 0.03080933913588524, 0.06161867827177048, 0.0, 0.0, 0.03080933913588524, -0.03080933913588524, 0.0]
[2025-05-22 02:44:02,052]: Mean: -0.00058670
[2025-05-22 02:44:02,052]: Min: -0.09242801
[2025-05-22 02:44:02,052]: Max: 0.12323736
[2025-05-22 02:44:02,053]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-22 02:44:02,053]: Sample Values (25 elements): [1.0154205560684204, 0.9763635993003845, 0.9881002902984619, 1.0122214555740356, 0.9559247493743896, 1.0300084352493286, 1.002697467803955, 0.9912697672843933, 1.018714189529419, 0.9578322172164917, 1.025526762008667, 1.026779294013977, 0.9756584763526917, 0.9947137236595154, 0.9633821249008179, 0.9744747281074524, 0.9659758806228638, 0.998279869556427, 0.9956656098365784, 0.9880616068840027, 1.0000265836715698, 1.0064038038253784, 0.997603714466095, 0.9999184608459473, 0.9803236126899719]
[2025-05-22 02:44:02,053]: Mean: 0.99469966
[2025-05-22 02:44:02,053]: Min: 0.95104963
[2025-05-22 02:44:02,053]: Max: 1.05698204
[2025-05-22 02:44:02,053]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-22 02:44:02,054]: Sample Values (25 elements): [-0.32209619879722595, 0.013797699473798275, -0.2789667248725891, -0.4381343722343445, 0.35606518387794495, -0.18470065295696259, 0.3816646337509155, 0.3675355017185211, -0.3797081410884857, -0.24500219523906708, -0.1373164802789688, 0.3476107120513916, -0.11381681263446808, -0.03259512037038803, -0.2916828393936157, -0.35007011890411377, 0.23103752732276917, 0.2615513205528259, -0.3576587438583374, -0.005407060030847788, -0.1381811946630478, -0.23958978056907654, 0.24178045988082886, 0.12872138619422913, -0.2720341682434082]
[2025-05-22 02:44:02,054]: Mean: 0.00289006
[2025-05-22 02:44:02,054]: Min: -0.51675677
[2025-05-22 02:44:02,054]: Max: 0.52906454
[2025-05-22 02:44:02,054]: 


QAT of ResNet20 with parametrized_hardtanh down to 4 bits...
[2025-05-22 02:44:02,161]: [ResNet20_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-22 02:44:02,194]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 02:45:02,584]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.9993 Train Acc: 0.6491 Eval Loss: 1.0182 Eval Acc: 0.6484 (LR: 0.010000)
[2025-05-22 02:46:03,000]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.8663 Train Acc: 0.6959 Eval Loss: 0.9483 Eval Acc: 0.6902 (LR: 0.010000)
[2025-05-22 02:47:03,193]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.8142 Train Acc: 0.7146 Eval Loss: 1.0682 Eval Acc: 0.6438 (LR: 0.010000)
[2025-05-22 02:48:03,925]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.7753 Train Acc: 0.7298 Eval Loss: 0.8502 Eval Acc: 0.7063 (LR: 0.010000)
[2025-05-22 02:49:04,799]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.7416 Train Acc: 0.7416 Eval Loss: 0.8901 Eval Acc: 0.7011 (LR: 0.010000)
[2025-05-22 02:50:05,636]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.7120 Train Acc: 0.7518 Eval Loss: 0.7313 Eval Acc: 0.7492 (LR: 0.010000)
[2025-05-22 02:51:02,418]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.6912 Train Acc: 0.7580 Eval Loss: 0.7868 Eval Acc: 0.7336 (LR: 0.010000)
[2025-05-22 02:51:59,594]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.6743 Train Acc: 0.7639 Eval Loss: 0.8641 Eval Acc: 0.7218 (LR: 0.010000)
[2025-05-22 02:53:01,406]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.6549 Train Acc: 0.7721 Eval Loss: 0.8213 Eval Acc: 0.7340 (LR: 0.010000)
[2025-05-22 02:54:03,049]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.6428 Train Acc: 0.7765 Eval Loss: 0.6915 Eval Acc: 0.7639 (LR: 0.010000)
[2025-05-22 02:55:05,092]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.6291 Train Acc: 0.7814 Eval Loss: 0.6619 Eval Acc: 0.7705 (LR: 0.010000)
[2025-05-22 02:56:07,981]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.6106 Train Acc: 0.7862 Eval Loss: 0.8157 Eval Acc: 0.7328 (LR: 0.010000)
[2025-05-22 02:57:10,246]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.6073 Train Acc: 0.7904 Eval Loss: 0.6926 Eval Acc: 0.7676 (LR: 0.010000)
[2025-05-22 02:58:11,974]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.5964 Train Acc: 0.7917 Eval Loss: 1.0998 Eval Acc: 0.6752 (LR: 0.010000)
[2025-05-22 02:59:15,555]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.5849 Train Acc: 0.7949 Eval Loss: 0.7695 Eval Acc: 0.7497 (LR: 0.001000)
[2025-05-22 03:00:18,025]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.4894 Train Acc: 0.8303 Eval Loss: 0.5290 Eval Acc: 0.8199 (LR: 0.001000)
[2025-05-22 03:01:16,687]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.4692 Train Acc: 0.8364 Eval Loss: 0.5243 Eval Acc: 0.8227 (LR: 0.001000)
[2025-05-22 03:02:13,404]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.4664 Train Acc: 0.8385 Eval Loss: 0.5214 Eval Acc: 0.8244 (LR: 0.001000)
[2025-05-22 03:03:10,011]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.4571 Train Acc: 0.8402 Eval Loss: 0.5301 Eval Acc: 0.8215 (LR: 0.001000)
[2025-05-22 03:04:11,028]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.4555 Train Acc: 0.8408 Eval Loss: 0.5310 Eval Acc: 0.8219 (LR: 0.001000)
[2025-05-22 03:05:12,511]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.4535 Train Acc: 0.8413 Eval Loss: 0.5128 Eval Acc: 0.8291 (LR: 0.001000)
[2025-05-22 03:06:14,094]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.4452 Train Acc: 0.8447 Eval Loss: 0.5218 Eval Acc: 0.8263 (LR: 0.001000)
[2025-05-22 03:07:14,500]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.4488 Train Acc: 0.8414 Eval Loss: 0.5228 Eval Acc: 0.8221 (LR: 0.001000)
[2025-05-22 03:08:14,900]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.4439 Train Acc: 0.8453 Eval Loss: 0.5070 Eval Acc: 0.8284 (LR: 0.001000)
[2025-05-22 03:09:15,241]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.4394 Train Acc: 0.8466 Eval Loss: 0.5364 Eval Acc: 0.8211 (LR: 0.001000)
[2025-05-22 03:10:15,672]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.4406 Train Acc: 0.8450 Eval Loss: 0.5304 Eval Acc: 0.8236 (LR: 0.001000)
[2025-05-22 03:11:16,227]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.4323 Train Acc: 0.8495 Eval Loss: 0.5151 Eval Acc: 0.8280 (LR: 0.001000)
[2025-05-22 03:12:16,567]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.4349 Train Acc: 0.8469 Eval Loss: 0.5100 Eval Acc: 0.8314 (LR: 0.001000)
[2025-05-22 03:13:15,209]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.4330 Train Acc: 0.8499 Eval Loss: 0.5086 Eval Acc: 0.8278 (LR: 0.001000)
[2025-05-22 03:14:13,367]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.4309 Train Acc: 0.8504 Eval Loss: 0.5127 Eval Acc: 0.8284 (LR: 0.000100)
[2025-05-22 03:15:10,319]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.4171 Train Acc: 0.8559 Eval Loss: 0.4944 Eval Acc: 0.8374 (LR: 0.000100)
[2025-05-22 03:16:09,537]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.4144 Train Acc: 0.8558 Eval Loss: 0.4938 Eval Acc: 0.8370 (LR: 0.000100)
[2025-05-22 03:17:05,683]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.4125 Train Acc: 0.8551 Eval Loss: 0.4911 Eval Acc: 0.8365 (LR: 0.000100)
[2025-05-22 03:18:04,866]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.4118 Train Acc: 0.8559 Eval Loss: 0.4871 Eval Acc: 0.8357 (LR: 0.000100)
[2025-05-22 03:19:02,054]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.4068 Train Acc: 0.8577 Eval Loss: 0.4947 Eval Acc: 0.8339 (LR: 0.000100)
[2025-05-22 03:20:00,170]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.4089 Train Acc: 0.8581 Eval Loss: 0.4981 Eval Acc: 0.8351 (LR: 0.000100)
[2025-05-22 03:20:56,710]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.4104 Train Acc: 0.8568 Eval Loss: 0.4955 Eval Acc: 0.8358 (LR: 0.000100)
[2025-05-22 03:21:55,857]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.4103 Train Acc: 0.8579 Eval Loss: 0.4928 Eval Acc: 0.8337 (LR: 0.000100)
[2025-05-22 03:22:53,592]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.4081 Train Acc: 0.8577 Eval Loss: 0.4981 Eval Acc: 0.8329 (LR: 0.000100)
[2025-05-22 03:23:50,805]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.4082 Train Acc: 0.8578 Eval Loss: 0.4973 Eval Acc: 0.8328 (LR: 0.000100)
[2025-05-22 03:24:48,449]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.4093 Train Acc: 0.8569 Eval Loss: 0.5007 Eval Acc: 0.8351 (LR: 0.000100)
[2025-05-22 03:25:45,080]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.4058 Train Acc: 0.8582 Eval Loss: 0.4931 Eval Acc: 0.8358 (LR: 0.000100)
[2025-05-22 03:26:42,089]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.4054 Train Acc: 0.8576 Eval Loss: 0.4949 Eval Acc: 0.8326 (LR: 0.000100)
[2025-05-22 03:27:39,388]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.4141 Train Acc: 0.8557 Eval Loss: 0.4967 Eval Acc: 0.8348 (LR: 0.000100)
[2025-05-22 03:28:36,938]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.4067 Train Acc: 0.8583 Eval Loss: 0.4979 Eval Acc: 0.8327 (LR: 0.000010)
[2025-05-22 03:29:34,940]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.4046 Train Acc: 0.8590 Eval Loss: 0.4934 Eval Acc: 0.8330 (LR: 0.000010)
[2025-05-22 03:30:34,914]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.4051 Train Acc: 0.8591 Eval Loss: 0.4934 Eval Acc: 0.8379 (LR: 0.000010)
[2025-05-22 03:31:32,359]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.4054 Train Acc: 0.8601 Eval Loss: 0.4918 Eval Acc: 0.8368 (LR: 0.000010)
[2025-05-22 03:32:33,659]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.4019 Train Acc: 0.8598 Eval Loss: 0.4948 Eval Acc: 0.8341 (LR: 0.000010)
[2025-05-22 03:33:32,401]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.4036 Train Acc: 0.8590 Eval Loss: 0.4931 Eval Acc: 0.8346 (LR: 0.000010)
[2025-05-22 03:34:30,001]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.4036 Train Acc: 0.8586 Eval Loss: 0.4926 Eval Acc: 0.8357 (LR: 0.000010)
[2025-05-22 03:35:27,718]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.4021 Train Acc: 0.8600 Eval Loss: 0.4912 Eval Acc: 0.8353 (LR: 0.000010)
[2025-05-22 03:36:24,698]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.4032 Train Acc: 0.8583 Eval Loss: 0.4930 Eval Acc: 0.8366 (LR: 0.000010)
[2025-05-22 03:37:21,561]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.4014 Train Acc: 0.8598 Eval Loss: 0.4902 Eval Acc: 0.8351 (LR: 0.000010)
[2025-05-22 03:38:18,160]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.4036 Train Acc: 0.8595 Eval Loss: 0.4938 Eval Acc: 0.8368 (LR: 0.000010)
[2025-05-22 03:39:16,133]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.4079 Train Acc: 0.8563 Eval Loss: 0.4907 Eval Acc: 0.8364 (LR: 0.000010)
[2025-05-22 03:40:12,003]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.4022 Train Acc: 0.8602 Eval Loss: 0.4922 Eval Acc: 0.8379 (LR: 0.000010)
[2025-05-22 03:41:07,577]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.4013 Train Acc: 0.8606 Eval Loss: 0.4950 Eval Acc: 0.8355 (LR: 0.000010)
[2025-05-22 03:42:03,211]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.4015 Train Acc: 0.8602 Eval Loss: 0.4908 Eval Acc: 0.8392 (LR: 0.000010)
[2025-05-22 03:42:59,300]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.4039 Train Acc: 0.8582 Eval Loss: 0.4967 Eval Acc: 0.8357 (LR: 0.000010)
[2025-05-22 03:42:59,301]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8392
[2025-05-22 03:42:59,367]: 


Quantization of model down to 4 bits finished
[2025-05-22 03:42:59,367]: Model Architecture:
[2025-05-22 03:42:59,421]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0513], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40754568576812744, max_val=0.3623822033405304)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1238], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0943], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6717429161071777, max_val=0.7420222163200378)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1779], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0683], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4439065158367157, max_val=0.5806230306625366)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1127], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0515], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41539090871810913, max_val=0.3568621873855591)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1924], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.335734486579895, max_val=0.41992995142936707)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1266], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30413204431533813, max_val=0.302394837141037)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2277], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0340], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2552522122859955, max_val=0.2542452812194824)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1144], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0289], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20433463156223297, max_val=0.2292640656232834)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0539], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4020772874355316, max_val=0.406722754240036)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1710], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0320], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2412806898355484, max_val=0.23798054456710815)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21592749655246735, max_val=0.2385135442018509)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1751], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2337649017572403, max_val=0.22095821797847748)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1046], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0307], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22778363525867462, max_val=0.23238293826580048)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2017], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0239], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17898239195346832, max_val=0.1795661598443985)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1060], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16651667654514313, max_val=0.1691409796476364)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0409], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3443484604358673, max_val=0.2694088816642761)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1494], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0228], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16579115390777588, max_val=0.17694318294525146)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0953], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0192], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13789178431034088, max_val=0.14975401759147644)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1338], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12059229612350464, max_val=0.12721587717533112)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0143], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10495556890964508, max_val=0.10935335606336594)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2669], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 03:42:59,421]: 
Model Weights:
[2025-05-22 03:42:59,421]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-22 03:42:59,421]: Sample Values (25 elements): [-0.07263369113206863, 0.13543006777763367, 0.10723799467086792, 0.009239119477570057, -0.07337818294763565, -0.32560479640960693, -0.10855109989643097, 0.634658694267273, -0.3250509798526764, 0.2888430058956146, 0.2682998478412628, 0.2887865900993347, -0.19189339876174927, -0.0985521599650383, -0.15717948973178864, -0.12348544597625732, 0.48715636134147644, 0.08158370107412338, 0.1285625398159027, 0.2243681699037552, 0.38102075457572937, 0.12461373955011368, -0.07681990414857864, -0.2674480974674225, -0.11264418065547943]
[2025-05-22 03:42:59,421]: Mean: -0.00550093
[2025-05-22 03:42:59,422]: Min: -0.68662101
[2025-05-22 03:42:59,422]: Max: 0.91610074
[2025-05-22 03:42:59,422]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,422]: Sample Values (16 elements): [0.40051934123039246, 0.7281681299209595, 0.7543554306030273, 0.928905725479126, 0.4943906366825104, 0.49968084692955017, 0.7713786363601685, 0.6696626543998718, 0.6931926608085632, 0.7854121923446655, 0.5161205530166626, 0.4780453145503998, 0.711000382900238, 0.9826368093490601, 0.7217021584510803, 1.1257647275924683]
[2025-05-22 03:42:59,422]: Mean: 0.70380855
[2025-05-22 03:42:59,422]: Min: 0.40051934
[2025-05-22 03:42:59,423]: Max: 1.12576473
[2025-05-22 03:42:59,424]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 03:42:59,424]: Sample Values (25 elements): [0.15398555994033813, 0.0, 0.20531406998634338, 0.051328517496585846, 0.051328517496585846, 0.10265703499317169, 0.0, -0.051328517496585846, 0.051328517496585846, 0.0, -0.15398555994033813, 0.051328517496585846, 0.051328517496585846, 0.15398555994033813, 0.051328517496585846, -0.051328517496585846, -0.20531406998634338, -0.051328517496585846, 0.051328517496585846, 0.15398555994033813, 0.15398555994033813, 0.30797111988067627, 0.10265703499317169, 0.0, -0.051328517496585846]
[2025-05-22 03:42:59,424]: Mean: 0.00570317
[2025-05-22 03:42:59,424]: Min: -0.41062814
[2025-05-22 03:42:59,424]: Max: 0.35929963
[2025-05-22 03:42:59,424]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,425]: Sample Values (16 elements): [0.9530707597732544, 0.8809401988983154, 0.7843474745750427, 1.2805492877960205, 1.152140736579895, 0.8674358129501343, 0.5413614511489868, 0.8287120461463928, 0.7511314153671265, 1.2219314575195312, 0.6614242792129517, 0.7980747818946838, 0.7803487777709961, 1.0512138605117798, 0.9043539762496948, 1.046229362487793]
[2025-05-22 03:42:59,425]: Mean: 0.90645409
[2025-05-22 03:42:59,425]: Min: 0.54136145
[2025-05-22 03:42:59,425]: Max: 1.28054929
[2025-05-22 03:42:59,426]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 03:42:59,426]: Sample Values (25 elements): [0.09425101429224014, -0.18850202858448029, 0.0, 0.0, -0.18850202858448029, -0.18850202858448029, -0.18850202858448029, -0.09425101429224014, 0.18850202858448029, 0.0, -0.18850202858448029, 0.09425101429224014, 0.0, 0.18850202858448029, 0.09425101429224014, 0.09425101429224014, 0.0, -0.09425101429224014, -0.18850202858448029, 0.09425101429224014, 0.09425101429224014, -0.09425101429224014, 0.0, 0.09425101429224014, 0.09425101429224014]
[2025-05-22 03:42:59,427]: Mean: -0.00732245
[2025-05-22 03:42:59,427]: Min: -0.65975708
[2025-05-22 03:42:59,427]: Max: 0.75400811
[2025-05-22 03:42:59,427]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,427]: Sample Values (16 elements): [0.7683041095733643, 0.842555582523346, 0.572335422039032, 0.920191764831543, 0.8429453372955322, 0.9538972973823547, 0.7970691323280334, 0.8082531690597534, 0.6728819012641907, 1.192213535308838, 0.7514020204544067, 0.5232114195823669, 1.2724792957305908, 0.6806918978691101, 0.7425740361213684, 1.117993950843811]
[2025-05-22 03:42:59,427]: Mean: 0.84118748
[2025-05-22 03:42:59,428]: Min: 0.52321142
[2025-05-22 03:42:59,428]: Max: 1.27247930
[2025-05-22 03:42:59,429]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 03:42:59,429]: Sample Values (25 elements): [0.0, 0.0683019757270813, 0.2049059271812439, -0.0683019757270813, 0.2049059271812439, 0.1366039514541626, -0.1366039514541626, -0.2049059271812439, -0.1366039514541626, 0.1366039514541626, -0.1366039514541626, -0.0683019757270813, -0.0683019757270813, 0.1366039514541626, 0.2049059271812439, 0.1366039514541626, 0.0, -0.1366039514541626, 0.0683019757270813, 0.1366039514541626, -0.0683019757270813, 0.0, -0.2732079029083252, 0.1366039514541626, 0.0]
[2025-05-22 03:42:59,429]: Mean: 0.00014822
[2025-05-22 03:42:59,429]: Min: -0.40981185
[2025-05-22 03:42:59,429]: Max: 0.61471778
[2025-05-22 03:42:59,430]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,430]: Sample Values (16 elements): [1.0489927530288696, 0.7396096587181091, 0.8919481635093689, 0.8165783286094666, 0.7074784636497498, 0.990609347820282, 0.8125593066215515, 0.8074291348457336, 1.1635828018188477, 0.997440755367279, 0.8542081117630005, 0.8702825903892517, 0.8159821629524231, 0.8970509171485901, 1.1014448404312134, 0.9277325868606567]
[2025-05-22 03:42:59,430]: Mean: 0.90268308
[2025-05-22 03:42:59,430]: Min: 0.70747846
[2025-05-22 03:42:59,430]: Max: 1.16358280
[2025-05-22 03:42:59,431]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 03:42:59,432]: Sample Values (25 elements): [0.05148353800177574, -0.20593415200710297, 0.20593415200710297, 0.15445061028003693, -0.05148353800177574, 0.05148353800177574, 0.0, 0.10296707600355148, 0.15445061028003693, 0.20593415200710297, 0.05148353800177574, -0.10296707600355148, -0.05148353800177574, -0.10296707600355148, 0.2574176788330078, 0.15445061028003693, 0.05148353800177574, 0.05148353800177574, 0.05148353800177574, -0.20593415200710297, 0.05148353800177574, -0.05148353800177574, -0.10296707600355148, 0.05148353800177574, 0.20593415200710297]
[2025-05-22 03:42:59,432]: Mean: 0.00297192
[2025-05-22 03:42:59,432]: Min: -0.41186830
[2025-05-22 03:42:59,432]: Max: 0.36038476
[2025-05-22 03:42:59,432]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,432]: Sample Values (16 elements): [0.9669165015220642, 0.6791000366210938, 0.8887654542922974, 0.7356457114219666, 0.8149732947349548, 0.8526298999786377, 0.7323967814445496, 0.8132378458976746, 0.739399790763855, 1.060928463935852, 0.8420518040657043, 0.9337049722671509, 0.8179309964179993, 0.7819504141807556, 1.2362806797027588, 0.5987588167190552]
[2025-05-22 03:42:59,433]: Mean: 0.84341693
[2025-05-22 03:42:59,433]: Min: 0.59875882
[2025-05-22 03:42:59,433]: Max: 1.23628068
[2025-05-22 03:42:59,434]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 03:42:59,434]: Sample Values (25 elements): [0.10075525939464569, 0.050377629697322845, -0.10075525939464569, -0.10075525939464569, 0.20151051878929138, 0.0, 0.10075525939464569, 0.10075525939464569, 0.0, -0.050377629697322845, -0.050377629697322845, 0.10075525939464569, -0.10075525939464569, 0.0, -0.10075525939464569, -0.10075525939464569, 0.15113288164138794, 0.050377629697322845, -0.050377629697322845, 0.15113288164138794, 0.20151051878929138, 0.10075525939464569, 0.0, -0.10075525939464569, 0.0]
[2025-05-22 03:42:59,434]: Mean: 0.00264570
[2025-05-22 03:42:59,435]: Min: -0.35264340
[2025-05-22 03:42:59,435]: Max: 0.40302104
[2025-05-22 03:42:59,435]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,435]: Sample Values (16 elements): [0.9288653135299683, 0.7828928828239441, 0.80772864818573, 0.8533725738525391, 0.9421600103378296, 0.781735360622406, 1.0321524143218994, 0.8191959857940674, 1.0379517078399658, 0.7220543026924133, 0.821258544921875, 0.976958155632019, 1.0108447074890137, 0.8122438788414001, 0.9005481004714966, 1.0378408432006836]
[2025-05-22 03:42:59,435]: Mean: 0.89173770
[2025-05-22 03:42:59,435]: Min: 0.72205430
[2025-05-22 03:42:59,436]: Max: 1.03795171
[2025-05-22 03:42:59,436]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 03:42:59,437]: Sample Values (25 elements): [0.040435124188661575, -0.12130537629127502, -0.1617404967546463, 0.0, -0.08087024837732315, 0.040435124188661575, -0.12130537629127502, 0.08087024837732315, 0.0, 0.1617404967546463, 0.0, -0.1617404967546463, 0.0, -0.08087024837732315, -0.12130537629127502, 0.0, 0.08087024837732315, 0.12130537629127502, 0.040435124188661575, 0.08087024837732315, 0.0, 0.12130537629127502, 0.08087024837732315, 0.040435124188661575, 0.12130537629127502]
[2025-05-22 03:42:59,437]: Mean: -0.00105300
[2025-05-22 03:42:59,438]: Min: -0.32348099
[2025-05-22 03:42:59,438]: Max: 0.28304586
[2025-05-22 03:42:59,438]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-22 03:42:59,438]: Sample Values (16 elements): [0.8820200562477112, 0.7177219986915588, 0.8652971386909485, 0.8004212975502014, 0.98249351978302, 0.8857092261314392, 0.7686423659324646, 0.9548060297966003, 0.8102161288261414, 0.8365223407745361, 0.9384899139404297, 0.6700553894042969, 0.8217670917510986, 0.8161426186561584, 0.8409479856491089, 1.0400954484939575]
[2025-05-22 03:42:59,438]: Mean: 0.85195929
[2025-05-22 03:42:59,438]: Min: 0.67005539
[2025-05-22 03:42:59,439]: Max: 1.04009545
[2025-05-22 03:42:59,440]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-22 03:42:59,440]: Sample Values (25 elements): [-0.13586601614952087, 0.03396650403738022, -0.06793300807476044, -0.03396650403738022, -0.03396650403738022, 0.0, 0.0, 0.06793300807476044, -0.03396650403738022, -0.10189951211214066, 0.13586601614952087, 0.03396650403738022, 0.0, -0.03396650403738022, -0.03396650403738022, 0.13586601614952087, 0.0, 0.10189951211214066, -0.10189951211214066, -0.10189951211214066, -0.06793300807476044, -0.06793300807476044, 0.03396650403738022, 0.03396650403738022, -0.03396650403738022]
[2025-05-22 03:42:59,440]: Mean: 0.00053810
[2025-05-22 03:42:59,440]: Min: -0.27173203
[2025-05-22 03:42:59,440]: Max: 0.23776552
[2025-05-22 03:42:59,440]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,441]: Sample Values (25 elements): [0.8314450979232788, 0.9407708644866943, 0.9001165628433228, 0.8506194949150085, 1.0217163562774658, 0.8982301950454712, 0.9243252873420715, 0.8644750118255615, 0.8733344674110413, 0.7950697541236877, 0.902930498123169, 0.8886739611625671, 0.8921383619308472, 0.9599541425704956, 0.9750803709030151, 0.8877779245376587, 0.9571680426597595, 0.9130938053131104, 0.9017510414123535, 0.8872259855270386, 0.9336382150650024, 0.9107176661491394, 0.9516547322273254, 0.8679620623588562, 0.8266826272010803]
[2025-05-22 03:42:59,441]: Mean: 0.90116048
[2025-05-22 03:42:59,441]: Min: 0.79506975
[2025-05-22 03:42:59,441]: Max: 1.02171636
[2025-05-22 03:42:59,442]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 03:42:59,442]: Sample Values (25 elements): [0.0, -0.028906578198075294, 0.028906578198075294, -0.028906578198075294, 0.0, 0.0, 0.0, 0.05781315639615059, 0.028906578198075294, -0.05781315639615059, -0.028906578198075294, -0.05781315639615059, -0.08671973645687103, 0.0, 0.0, 0.08671973645687103, 0.0, 0.05781315639615059, 0.08671973645687103, 0.0, 0.028906578198075294, 0.05781315639615059, 0.0, -0.028906578198075294, 0.05781315639615059]
[2025-05-22 03:42:59,443]: Mean: -0.00135500
[2025-05-22 03:42:59,443]: Min: -0.20234604
[2025-05-22 03:42:59,443]: Max: 0.23125263
[2025-05-22 03:42:59,443]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,443]: Sample Values (25 elements): [0.8893962502479553, 0.8650993704795837, 0.9966149926185608, 0.9927060604095459, 0.9729955196380615, 0.7914721965789795, 0.9087247848510742, 0.9353991150856018, 0.891352117061615, 0.8872191309928894, 0.8340398073196411, 0.839409589767456, 0.9477390646934509, 0.9094809293746948, 0.9228509664535522, 0.9775923490524292, 0.9460160732269287, 0.945603609085083, 0.8783875703811646, 0.9518343210220337, 0.8698572516441345, 0.8582742810249329, 0.8835218548774719, 0.9563795924186707, 0.8555154800415039]
[2025-05-22 03:42:59,443]: Mean: 0.91094613
[2025-05-22 03:42:59,444]: Min: 0.79147220
[2025-05-22 03:42:59,444]: Max: 1.00771356
[2025-05-22 03:42:59,445]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-22 03:42:59,445]: Sample Values (25 elements): [0.10783999413251877, 0.21567998826503754, 0.21567998826503754, 0.21567998826503754, 0.10783999413251877, -0.053919997066259384, -0.10783999413251877, -0.10783999413251877, -0.10783999413251877, 0.16175998747348785, -0.16175998747348785, 0.21567998826503754, -0.10783999413251877, 0.16175998747348785, 0.10783999413251877, -0.269599974155426, -0.269599974155426, 0.16175998747348785, 0.269599974155426, -0.16175998747348785, 0.0, -0.053919997066259384, -0.053919997066259384, 0.053919997066259384, -0.16175998747348785]
[2025-05-22 03:42:59,445]: Mean: 0.01021531
[2025-05-22 03:42:59,445]: Min: -0.37743998
[2025-05-22 03:42:59,446]: Max: 0.43135998
[2025-05-22 03:42:59,446]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,446]: Sample Values (25 elements): [0.8143986463546753, 0.838483452796936, 0.7968378067016602, 0.7190072536468506, 0.6865594983100891, 0.8171322345733643, 0.719157338142395, 0.9427810311317444, 0.7391155362129211, 0.7736049294471741, 0.7126158475875854, 0.7960940599441528, 0.7602360844612122, 0.850795567035675, 0.7870949506759644, 0.7684674859046936, 0.7357528209686279, 0.7018935084342957, 0.8759628534317017, 0.8308148384094238, 0.7569372653961182, 0.7756235003471375, 0.7005139589309692, 0.7795729041099548, 0.704953670501709]
[2025-05-22 03:42:59,446]: Mean: 0.76665968
[2025-05-22 03:42:59,446]: Min: 0.65351439
[2025-05-22 03:42:59,446]: Max: 0.94278103
[2025-05-22 03:42:59,447]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 03:42:59,448]: Sample Values (25 elements): [0.0, 0.031950753182172775, 0.06390150636434555, -0.06390150636434555, 0.031950753182172775, -0.06390150636434555, 0.031950753182172775, 0.06390150636434555, -0.031950753182172775, 0.06390150636434555, -0.031950753182172775, -0.06390150636434555, -0.031950753182172775, 0.09585225582122803, -0.06390150636434555, 0.031950753182172775, 0.031950753182172775, -0.09585225582122803, 0.09585225582122803, -0.031950753182172775, -0.031950753182172775, 0.09585225582122803, 0.031950753182172775, 0.09585225582122803, 0.1278030127286911]
[2025-05-22 03:42:59,448]: Mean: -0.00067257
[2025-05-22 03:42:59,448]: Min: -0.25560603
[2025-05-22 03:42:59,448]: Max: 0.22365527
[2025-05-22 03:42:59,448]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,448]: Sample Values (25 elements): [0.8835862278938293, 0.8719290494918823, 0.9645272493362427, 0.965438187122345, 0.9203207492828369, 0.9099074602127075, 0.9542969465255737, 0.931861400604248, 0.8884162902832031, 0.9682310819625854, 0.9661968946456909, 0.835857093334198, 0.9082083702087402, 0.8932762145996094, 0.9023141264915466, 0.8940771222114563, 0.9028200507164001, 0.9248126149177551, 1.0075019598007202, 0.9030459523200989, 0.9265820980072021, 0.8975479602813721, 0.8601689338684082, 0.9436602592468262, 0.8659554719924927]
[2025-05-22 03:42:59,449]: Mean: 0.91336626
[2025-05-22 03:42:59,449]: Min: 0.83585709
[2025-05-22 03:42:59,449]: Max: 1.00750196
[2025-05-22 03:42:59,450]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 03:42:59,450]: Sample Values (25 elements): [0.0, -0.030296070501208305, 0.0, 0.09088820964097977, -0.06059214100241661, -0.06059214100241661, 0.06059214100241661, -0.030296070501208305, 0.0, 0.030296070501208305, 0.030296070501208305, 0.030296070501208305, -0.030296070501208305, -0.030296070501208305, 0.0, -0.030296070501208305, -0.09088820964097977, 0.030296070501208305, 0.0, -0.030296070501208305, 0.09088820964097977, 0.030296070501208305, -0.06059214100241661, -0.06059214100241661, -0.06059214100241661]
[2025-05-22 03:42:59,450]: Mean: 0.00047995
[2025-05-22 03:42:59,451]: Min: -0.21207249
[2025-05-22 03:42:59,451]: Max: 0.24236856
[2025-05-22 03:42:59,451]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,451]: Sample Values (25 elements): [0.8329188227653503, 1.0087261199951172, 0.9615817070007324, 0.8876783847808838, 0.8279703259468079, 0.7565961480140686, 0.9054630398750305, 0.8849520683288574, 0.8648608922958374, 0.7519291639328003, 0.9710081815719604, 0.8061342835426331, 0.8438301086425781, 0.743675947189331, 0.9557995796203613, 0.9051496386528015, 0.9606282711029053, 0.982078492641449, 0.7950787544250488, 0.8495248556137085, 1.0435962677001953, 0.7656601667404175, 0.9286209344863892, 0.9309361577033997, 0.9767207503318787]
[2025-05-22 03:42:59,451]: Mean: 0.88881481
[2025-05-22 03:42:59,451]: Min: 0.74367595
[2025-05-22 03:42:59,451]: Max: 1.04359627
[2025-05-22 03:42:59,452]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 03:42:59,453]: Sample Values (25 elements): [0.0, -0.12125951796770096, 0.0, 0.0, 0.06062975898385048, 0.12125951796770096, 0.0, 0.09094464033842087, 0.09094464033842087, 0.0, 0.0, -0.06062975898385048, 0.0, 0.09094464033842087, 0.06062975898385048, 0.03031487949192524, 0.0, 0.0, -0.03031487949192524, -0.03031487949192524, -0.12125951796770096, -0.03031487949192524, 0.03031487949192524, -0.06062975898385048, 0.03031487949192524]
[2025-05-22 03:42:59,453]: Mean: 0.00000987
[2025-05-22 03:42:59,453]: Min: -0.24251904
[2025-05-22 03:42:59,453]: Max: 0.21220416
[2025-05-22 03:42:59,453]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,454]: Sample Values (25 elements): [0.898577094078064, 0.8414946794509888, 0.8660620450973511, 0.8773662447929382, 0.8829299807548523, 0.8878996968269348, 0.9546976089477539, 0.923952043056488, 0.8754199147224426, 0.9112483859062195, 0.8998990654945374, 0.9738677144050598, 0.9541763067245483, 0.9054856896400452, 0.9265906810760498, 0.8972644209861755, 1.0228711366653442, 0.877716064453125, 0.8013008236885071, 1.0076322555541992, 0.8759677410125732, 0.8928695917129517, 0.9131771922111511, 0.9142033457756042, 0.945538341999054]
[2025-05-22 03:42:59,454]: Mean: 0.91663700
[2025-05-22 03:42:59,454]: Min: 0.80130082
[2025-05-22 03:42:59,454]: Max: 1.02287114
[2025-05-22 03:42:59,455]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 03:42:59,455]: Sample Values (25 elements): [-0.06135553866624832, 0.0, -0.06135553866624832, 0.0, 0.06135553866624832, 0.12271107733249664, 0.03067776933312416, 0.0, 0.06135553866624832, -0.03067776933312416, -0.06135553866624832, 0.03067776933312416, 0.12271107733249664, 0.06135553866624832, -0.06135553866624832, -0.06135553866624832, 0.03067776933312416, -0.06135553866624832, 0.06135553866624832, 0.06135553866624832, -0.09203331172466278, 0.0, -0.12271107733249664, 0.06135553866624832, 0.0]
[2025-05-22 03:42:59,456]: Mean: -0.00081887
[2025-05-22 03:42:59,456]: Min: -0.21474439
[2025-05-22 03:42:59,456]: Max: 0.24542215
[2025-05-22 03:42:59,456]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-22 03:42:59,456]: Sample Values (25 elements): [0.8734807372093201, 0.8849968910217285, 0.932793915271759, 0.8807553052902222, 1.0245100259780884, 0.8971919417381287, 0.9098235964775085, 0.857901930809021, 0.9830988049507141, 1.0614696741104126, 1.0161223411560059, 0.9195224642753601, 0.8575826287269592, 0.9443684816360474, 0.8912732601165771, 0.9690683484077454, 0.9443233013153076, 0.8904879689216614, 0.9653688073158264, 0.9442802667617798, 0.9650015234947205, 0.8598992228507996, 1.0753402709960938, 0.9076219797134399, 0.9107083082199097]
[2025-05-22 03:42:59,456]: Mean: 0.92928052
[2025-05-22 03:42:59,457]: Min: 0.70147699
[2025-05-22 03:42:59,457]: Max: 1.12158787
[2025-05-22 03:42:59,458]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-22 03:42:59,458]: Sample Values (25 elements): [0.07170969247817993, 0.02390323206782341, -0.09561292827129364, 0.02390323206782341, -0.04780646413564682, 0.02390323206782341, 0.02390323206782341, -0.02390323206782341, 0.11951616406440735, 0.04780646413564682, 0.0, 0.04780646413564682, -0.04780646413564682, 0.0, 0.02390323206782341, -0.04780646413564682, 0.09561292827129364, 0.0, 0.07170969247817993, 0.11951616406440735, -0.02390323206782341, 0.0, 0.0, -0.07170969247817993, 0.0]
[2025-05-22 03:42:59,458]: Mean: 0.00018804
[2025-05-22 03:42:59,459]: Min: -0.16732262
[2025-05-22 03:42:59,459]: Max: 0.19122586
[2025-05-22 03:42:59,459]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,459]: Sample Values (25 elements): [0.9165017604827881, 0.8557703495025635, 0.9235393404960632, 0.9293652176856995, 0.8973371386528015, 0.8663008809089661, 0.926784336566925, 0.9004166126251221, 0.9117300510406494, 0.9336516857147217, 0.9254295229911804, 0.9311534762382507, 0.9121096134185791, 0.9334055185317993, 0.9458032846450806, 0.9418046474456787, 0.9245337247848511, 0.9069315791130066, 0.9200384020805359, 0.896992564201355, 0.8573653101921082, 0.8698742985725403, 0.9564880728721619, 0.9439844489097595, 0.9039903879165649]
[2025-05-22 03:42:59,459]: Mean: 0.91471553
[2025-05-22 03:42:59,459]: Min: 0.85577035
[2025-05-22 03:42:59,459]: Max: 0.96911514
[2025-05-22 03:42:59,460]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 03:42:59,461]: Sample Values (25 elements): [-0.022377178072929382, -0.022377178072929382, 0.022377178072929382, -0.08950871229171753, -0.06713153421878815, -0.022377178072929382, 0.044754356145858765, -0.11188589036464691, 0.022377178072929382, -0.022377178072929382, 0.022377178072929382, -0.022377178072929382, 0.0, -0.022377178072929382, 0.08950871229171753, 0.0, -0.022377178072929382, -0.022377178072929382, 0.022377178072929382, -0.022377178072929382, 0.06713153421878815, -0.022377178072929382, 0.06713153421878815, 0.0, 0.08950871229171753]
[2025-05-22 03:42:59,461]: Mean: 0.00043098
[2025-05-22 03:42:59,461]: Min: -0.15664025
[2025-05-22 03:42:59,462]: Max: 0.17901742
[2025-05-22 03:42:59,462]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,462]: Sample Values (25 elements): [0.9408190250396729, 0.9684281349182129, 0.9603677988052368, 0.9747391939163208, 0.9348641633987427, 1.026586890220642, 0.9396198391914368, 0.8890253901481628, 0.9360270500183105, 0.9470330476760864, 1.028450846672058, 0.9279052019119263, 0.9783445000648499, 0.96346515417099, 0.9937500357627869, 1.0103721618652344, 0.9818423390388489, 0.96234530210495, 0.8229794502258301, 1.0929691791534424, 0.971376359462738, 0.9784764051437378, 0.9877353310585022, 0.9507244825363159, 0.9499354362487793]
[2025-05-22 03:42:59,462]: Mean: 0.95830697
[2025-05-22 03:42:59,462]: Min: 0.82297945
[2025-05-22 03:42:59,462]: Max: 1.09296918
[2025-05-22 03:42:59,463]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-22 03:42:59,464]: Sample Values (25 elements): [-0.08183431625366211, 0.0, -0.08183431625366211, 0.16366863250732422, -0.12275147438049316, 0.12275147438049316, 0.0, 0.08183431625366211, 0.040917158126831055, -0.040917158126831055, 0.08183431625366211, 0.12275147438049316, 0.040917158126831055, -0.040917158126831055, 0.12275147438049316, 0.20458579063415527, -0.040917158126831055, 0.0, 0.0, -0.08183431625366211, -0.12275147438049316, 0.040917158126831055, 0.0, -0.12275147438049316, 0.12275147438049316]
[2025-05-22 03:42:59,464]: Mean: 0.00069927
[2025-05-22 03:42:59,464]: Min: -0.32733727
[2025-05-22 03:42:59,464]: Max: 0.28642011
[2025-05-22 03:42:59,464]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,464]: Sample Values (25 elements): [0.7931939363479614, 0.9126412272453308, 0.8589009642601013, 0.805450975894928, 0.8312356472015381, 0.8795603513717651, 0.8250269889831543, 0.8631547093391418, 0.815897524356842, 0.8148629069328308, 0.889878511428833, 0.8641952276229858, 0.8230364322662354, 0.8660140633583069, 0.9379711747169495, 0.7080758810043335, 0.8647210001945496, 0.7078646421432495, 0.8448577523231506, 0.8449252843856812, 0.8514420390129089, 0.8468369841575623, 0.7590149641036987, 0.8238255977630615, 0.8673141598701477]
[2025-05-22 03:42:59,465]: Mean: 0.83947521
[2025-05-22 03:42:59,465]: Min: 0.70786464
[2025-05-22 03:42:59,465]: Max: 0.93797117
[2025-05-22 03:42:59,466]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 03:42:59,466]: Sample Values (25 elements): [-0.045697905123233795, 0.045697905123233795, -0.045697905123233795, 0.045697905123233795, -0.045697905123233795, 0.09139581024646759, -0.06854686141014099, -0.045697905123233795, -0.022848952561616898, 0.022848952561616898, 0.0, 0.022848952561616898, 0.045697905123233795, 0.022848952561616898, -0.045697905123233795, 0.0, 0.09139581024646759, 0.0, 0.022848952561616898, 0.06854686141014099, 0.06854686141014099, -0.022848952561616898, 0.0, 0.0, -0.09139581024646759]
[2025-05-22 03:42:59,467]: Mean: 0.00045681
[2025-05-22 03:42:59,467]: Min: -0.15994267
[2025-05-22 03:42:59,467]: Max: 0.18279162
[2025-05-22 03:42:59,467]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,467]: Sample Values (25 elements): [0.9447916746139526, 0.9134858250617981, 0.9568877816200256, 0.9348211884498596, 0.9311612844467163, 0.9760205149650574, 0.9087067246437073, 0.9523218870162964, 0.9551641941070557, 0.9262275099754333, 0.9320076107978821, 0.9706096649169922, 0.930341899394989, 0.9687239527702332, 0.9452321529388428, 0.9573084712028503, 0.8906315565109253, 0.9412609934806824, 0.9742851257324219, 0.9184958934783936, 0.9844954609870911, 0.9242871999740601, 0.9235650897026062, 0.8665562272071838, 0.9080072045326233]
[2025-05-22 03:42:59,467]: Mean: 0.93320739
[2025-05-22 03:42:59,468]: Min: 0.86655623
[2025-05-22 03:42:59,468]: Max: 0.98449546
[2025-05-22 03:42:59,469]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 03:42:59,469]: Sample Values (25 elements): [0.11505833268165588, -0.03835277631878853, 0.019176388159394264, 0.03835277631878853, 0.0, -0.03835277631878853, -0.019176388159394264, -0.03835277631878853, 0.03835277631878853, -0.03835277631878853, -0.019176388159394264, 0.05752916634082794, -0.019176388159394264, -0.019176388159394264, 0.019176388159394264, 0.019176388159394264, 0.019176388159394264, 0.05752916634082794, -0.019176388159394264, 0.07670555263757706, 0.07670555263757706, 0.0, 0.05752916634082794, -0.07670555263757706, 0.0]
[2025-05-22 03:42:59,470]: Mean: 0.00019195
[2025-05-22 03:42:59,470]: Min: -0.13423471
[2025-05-22 03:42:59,470]: Max: 0.15341111
[2025-05-22 03:42:59,470]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,470]: Sample Values (25 elements): [0.9188736081123352, 0.910702109336853, 0.9334286451339722, 1.0292000770568848, 1.0274475812911987, 0.9169543385505676, 0.9660921096801758, 0.9577987194061279, 0.9449661374092102, 1.0171852111816406, 0.9494237303733826, 1.06377112865448, 0.9663963913917542, 0.9689816832542419, 1.0444116592407227, 0.9014440178871155, 1.0325959920883179, 0.9558457732200623, 0.986669659614563, 0.9657778739929199, 0.8786198496818542, 0.9244363307952881, 0.9622488021850586, 0.9741684794425964, 0.9503210186958313]
[2025-05-22 03:42:59,470]: Mean: 0.94729137
[2025-05-22 03:42:59,471]: Min: 0.86231399
[2025-05-22 03:42:59,471]: Max: 1.06377113
[2025-05-22 03:42:59,472]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 03:42:59,472]: Sample Values (25 elements): [0.0330410972237587, 0.0330410972237587, 0.0, -0.01652054861187935, -0.01652054861187935, -0.01652054861187935, 0.0, 0.0660821944475174, 0.0, 0.01652054861187935, -0.0330410972237587, 0.0, 0.0, 0.0330410972237587, 0.0660821944475174, 0.0330410972237587, 0.01652054861187935, 0.01652054861187935, -0.0330410972237587, -0.01652054861187935, 0.0660821944475174, 0.01652054861187935, 0.01652054861187935, -0.01652054861187935, -0.01652054861187935]
[2025-05-22 03:42:59,472]: Mean: -0.00011831
[2025-05-22 03:42:59,473]: Min: -0.11564384
[2025-05-22 03:42:59,473]: Max: 0.13216439
[2025-05-22 03:42:59,473]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,473]: Sample Values (25 elements): [0.9275915622711182, 0.915198028087616, 0.9127448797225952, 0.9291849136352539, 0.9238895773887634, 0.928438127040863, 0.8956478834152222, 0.9237129092216492, 0.9025210738182068, 0.9456034302711487, 0.8895789980888367, 0.9100403785705566, 0.9424315690994263, 0.9106808304786682, 0.9209770560264587, 0.8909730315208435, 0.9234218597412109, 0.9357017278671265, 0.9200253486633301, 0.9173455834388733, 0.9214349389076233, 0.9204407930374146, 0.9216368198394775, 0.888430655002594, 0.9499740600585938]
[2025-05-22 03:42:59,473]: Mean: 0.92184848
[2025-05-22 03:42:59,473]: Min: 0.88843066
[2025-05-22 03:42:59,474]: Max: 0.99955070
[2025-05-22 03:42:59,475]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 03:42:59,475]: Sample Values (25 elements): [0.014287264086306095, 0.02857452817261219, 0.0, 0.0714363232254982, 0.014287264086306095, 0.014287264086306095, 0.014287264086306095, -0.04286179319024086, 0.014287264086306095, -0.014287264086306095, 0.0, -0.02857452817261219, -0.04286179319024086, 0.05714905634522438, 0.02857452817261219, -0.02857452817261219, -0.02857452817261219, 0.04286179319024086, 0.0, -0.014287264086306095, 0.0, 0.04286179319024086, -0.02857452817261219, 0.014287264086306095, -0.04286179319024086]
[2025-05-22 03:42:59,475]: Mean: -0.00047283
[2025-05-22 03:42:59,476]: Min: -0.10001085
[2025-05-22 03:42:59,476]: Max: 0.11429811
[2025-05-22 03:42:59,476]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-22 03:42:59,476]: Sample Values (25 elements): [1.0170315504074097, 0.9630482196807861, 0.9817525148391724, 0.997637927532196, 1.0158851146697998, 0.9866669774055481, 1.0546789169311523, 0.978701651096344, 1.0303782224655151, 0.9457479119300842, 1.015767216682434, 1.0041112899780273, 1.0289525985717773, 1.016280174255371, 1.009379267692566, 1.0247607231140137, 1.0278465747833252, 0.9874194264411926, 1.0108782052993774, 0.951537549495697, 1.0572519302368164, 1.0010014772415161, 0.9961459636688232, 1.0077002048492432, 1.007061243057251]
[2025-05-22 03:42:59,476]: Mean: 0.99996310
[2025-05-22 03:42:59,476]: Min: 0.94574791
[2025-05-22 03:42:59,476]: Max: 1.07205284
[2025-05-22 03:42:59,477]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-22 03:42:59,477]: Sample Values (25 elements): [0.08846156299114227, -0.32854560017585754, -0.41336366534233093, -0.2658342719078064, 0.07294259965419769, 0.19090571999549866, -0.3058682680130005, 0.2980262339115143, 0.21392178535461426, 0.10944242775440216, -0.2533615529537201, 0.14688344299793243, 0.2940637171268463, 0.20096100866794586, 0.24181896448135376, 0.08719131350517273, -0.16627007722854614, -0.2756401300430298, 0.00808901246637106, -0.17293082177639008, -0.13647814095020294, 0.11239638179540634, 0.054656513035297394, 0.1398940235376358, -0.13450361788272858]
[2025-05-22 03:42:59,477]: Mean: 0.00289009
[2025-05-22 03:42:59,477]: Min: -0.51560032
[2025-05-22 03:42:59,477]: Max: 0.54910642
