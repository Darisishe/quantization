[2025-05-12 20:52:29,913]: 
Training ResNet20 with parametrized_hardtanh
[2025-05-12 20:53:24,451]: [ResNet20_parametrized_hardtanh] Epoch: 001 Train Loss: 2.0145 Train Acc: 0.2448 Eval Loss: 1.8714 Eval Acc: 0.3122 (LR: 0.001000)
[2025-05-12 20:54:04,574]: [ResNet20_parametrized_hardtanh] Epoch: 002 Train Loss: 1.7731 Train Acc: 0.3424 Eval Loss: 1.6203 Eval Acc: 0.4004 (LR: 0.001000)
[2025-05-12 20:54:44,293]: [ResNet20_parametrized_hardtanh] Epoch: 003 Train Loss: 1.6166 Train Acc: 0.4030 Eval Loss: 1.5623 Eval Acc: 0.4272 (LR: 0.001000)
[2025-05-12 20:55:23,871]: [ResNet20_parametrized_hardtanh] Epoch: 004 Train Loss: 1.5038 Train Acc: 0.4486 Eval Loss: 1.4734 Eval Acc: 0.4558 (LR: 0.001000)
[2025-05-12 20:56:03,489]: [ResNet20_parametrized_hardtanh] Epoch: 005 Train Loss: 1.4251 Train Acc: 0.4812 Eval Loss: 1.3608 Eval Acc: 0.5031 (LR: 0.001000)
[2025-05-12 20:56:41,681]: [ResNet20_parametrized_hardtanh] Epoch: 006 Train Loss: 1.3429 Train Acc: 0.5140 Eval Loss: 1.3532 Eval Acc: 0.5038 (LR: 0.001000)
[2025-05-12 20:57:19,625]: [ResNet20_parametrized_hardtanh] Epoch: 007 Train Loss: 1.2853 Train Acc: 0.5387 Eval Loss: 1.3023 Eval Acc: 0.5326 (LR: 0.001000)
[2025-05-12 20:57:58,621]: [ResNet20_parametrized_hardtanh] Epoch: 008 Train Loss: 1.2318 Train Acc: 0.5575 Eval Loss: 1.1750 Eval Acc: 0.5724 (LR: 0.001000)
[2025-05-12 20:58:37,364]: [ResNet20_parametrized_hardtanh] Epoch: 009 Train Loss: 1.1887 Train Acc: 0.5731 Eval Loss: 1.2013 Eval Acc: 0.5729 (LR: 0.001000)
[2025-05-12 20:59:17,293]: [ResNet20_parametrized_hardtanh] Epoch: 010 Train Loss: 1.1458 Train Acc: 0.5895 Eval Loss: 1.0865 Eval Acc: 0.6102 (LR: 0.001000)
[2025-05-12 20:59:58,300]: [ResNet20_parametrized_hardtanh] Epoch: 011 Train Loss: 1.1162 Train Acc: 0.6024 Eval Loss: 1.0556 Eval Acc: 0.6182 (LR: 0.001000)
[2025-05-12 21:00:38,409]: [ResNet20_parametrized_hardtanh] Epoch: 012 Train Loss: 1.0865 Train Acc: 0.6129 Eval Loss: 1.0992 Eval Acc: 0.6025 (LR: 0.001000)
[2025-05-12 21:01:16,768]: [ResNet20_parametrized_hardtanh] Epoch: 013 Train Loss: 1.0677 Train Acc: 0.6201 Eval Loss: 1.0812 Eval Acc: 0.6109 (LR: 0.001000)
[2025-05-12 21:01:56,469]: [ResNet20_parametrized_hardtanh] Epoch: 014 Train Loss: 1.0400 Train Acc: 0.6277 Eval Loss: 1.0383 Eval Acc: 0.6259 (LR: 0.001000)
[2025-05-12 21:02:36,222]: [ResNet20_parametrized_hardtanh] Epoch: 015 Train Loss: 1.0231 Train Acc: 0.6332 Eval Loss: 0.9813 Eval Acc: 0.6515 (LR: 0.001000)
[2025-05-12 21:03:15,800]: [ResNet20_parametrized_hardtanh] Epoch: 016 Train Loss: 1.0065 Train Acc: 0.6412 Eval Loss: 1.0028 Eval Acc: 0.6430 (LR: 0.001000)
[2025-05-12 21:03:55,862]: [ResNet20_parametrized_hardtanh] Epoch: 017 Train Loss: 0.9824 Train Acc: 0.6512 Eval Loss: 0.9737 Eval Acc: 0.6497 (LR: 0.001000)
[2025-05-12 21:04:36,042]: [ResNet20_parametrized_hardtanh] Epoch: 018 Train Loss: 0.9629 Train Acc: 0.6566 Eval Loss: 0.9442 Eval Acc: 0.6632 (LR: 0.001000)
[2025-05-12 21:05:15,206]: [ResNet20_parametrized_hardtanh] Epoch: 019 Train Loss: 0.9498 Train Acc: 0.6615 Eval Loss: 0.9942 Eval Acc: 0.6462 (LR: 0.001000)
[2025-05-12 21:05:56,264]: [ResNet20_parametrized_hardtanh] Epoch: 020 Train Loss: 0.9354 Train Acc: 0.6678 Eval Loss: 0.9405 Eval Acc: 0.6702 (LR: 0.001000)
[2025-05-12 21:06:34,207]: [ResNet20_parametrized_hardtanh] Epoch: 021 Train Loss: 0.9186 Train Acc: 0.6719 Eval Loss: 0.9153 Eval Acc: 0.6759 (LR: 0.001000)
[2025-05-12 21:07:11,761]: [ResNet20_parametrized_hardtanh] Epoch: 022 Train Loss: 0.9063 Train Acc: 0.6769 Eval Loss: 0.8941 Eval Acc: 0.6829 (LR: 0.001000)
[2025-05-12 21:07:51,790]: [ResNet20_parametrized_hardtanh] Epoch: 023 Train Loss: 0.8935 Train Acc: 0.6841 Eval Loss: 0.8860 Eval Acc: 0.6894 (LR: 0.001000)
[2025-05-12 21:08:31,740]: [ResNet20_parametrized_hardtanh] Epoch: 024 Train Loss: 0.8812 Train Acc: 0.6870 Eval Loss: 0.8989 Eval Acc: 0.6847 (LR: 0.001000)
[2025-05-12 21:09:11,399]: [ResNet20_parametrized_hardtanh] Epoch: 025 Train Loss: 0.8716 Train Acc: 0.6897 Eval Loss: 0.8566 Eval Acc: 0.6973 (LR: 0.001000)
[2025-05-12 21:09:50,797]: [ResNet20_parametrized_hardtanh] Epoch: 026 Train Loss: 0.8565 Train Acc: 0.6961 Eval Loss: 0.8807 Eval Acc: 0.6868 (LR: 0.001000)
[2025-05-12 21:10:30,749]: [ResNet20_parametrized_hardtanh] Epoch: 027 Train Loss: 0.8490 Train Acc: 0.6993 Eval Loss: 0.8455 Eval Acc: 0.7033 (LR: 0.001000)
[2025-05-12 21:11:09,831]: [ResNet20_parametrized_hardtanh] Epoch: 028 Train Loss: 0.8333 Train Acc: 0.7055 Eval Loss: 0.8603 Eval Acc: 0.7007 (LR: 0.001000)
[2025-05-12 21:11:49,600]: [ResNet20_parametrized_hardtanh] Epoch: 029 Train Loss: 0.8249 Train Acc: 0.7070 Eval Loss: 0.8309 Eval Acc: 0.7072 (LR: 0.001000)
[2025-05-12 21:12:29,501]: [ResNet20_parametrized_hardtanh] Epoch: 030 Train Loss: 0.8184 Train Acc: 0.7124 Eval Loss: 0.8020 Eval Acc: 0.7205 (LR: 0.001000)
[2025-05-12 21:13:09,237]: [ResNet20_parametrized_hardtanh] Epoch: 031 Train Loss: 0.8028 Train Acc: 0.7174 Eval Loss: 0.8260 Eval Acc: 0.7114 (LR: 0.001000)
[2025-05-12 21:13:46,978]: [ResNet20_parametrized_hardtanh] Epoch: 032 Train Loss: 0.7964 Train Acc: 0.7171 Eval Loss: 0.8240 Eval Acc: 0.7125 (LR: 0.001000)
[2025-05-12 21:14:24,697]: [ResNet20_parametrized_hardtanh] Epoch: 033 Train Loss: 0.7922 Train Acc: 0.7198 Eval Loss: 0.8404 Eval Acc: 0.7050 (LR: 0.001000)
[2025-05-12 21:15:02,241]: [ResNet20_parametrized_hardtanh] Epoch: 034 Train Loss: 0.7831 Train Acc: 0.7218 Eval Loss: 0.7999 Eval Acc: 0.7244 (LR: 0.001000)
[2025-05-12 21:15:40,024]: [ResNet20_parametrized_hardtanh] Epoch: 035 Train Loss: 0.7687 Train Acc: 0.7278 Eval Loss: 0.7592 Eval Acc: 0.7370 (LR: 0.001000)
[2025-05-12 21:16:17,541]: [ResNet20_parametrized_hardtanh] Epoch: 036 Train Loss: 0.7620 Train Acc: 0.7314 Eval Loss: 0.8177 Eval Acc: 0.7151 (LR: 0.001000)
[2025-05-12 21:16:55,453]: [ResNet20_parametrized_hardtanh] Epoch: 037 Train Loss: 0.7585 Train Acc: 0.7335 Eval Loss: 0.7855 Eval Acc: 0.7301 (LR: 0.001000)
[2025-05-12 21:17:33,025]: [ResNet20_parametrized_hardtanh] Epoch: 038 Train Loss: 0.7470 Train Acc: 0.7371 Eval Loss: 0.7693 Eval Acc: 0.7350 (LR: 0.001000)
[2025-05-12 21:18:10,390]: [ResNet20_parametrized_hardtanh] Epoch: 039 Train Loss: 0.7434 Train Acc: 0.7386 Eval Loss: 0.7471 Eval Acc: 0.7381 (LR: 0.001000)
[2025-05-12 21:18:48,159]: [ResNet20_parametrized_hardtanh] Epoch: 040 Train Loss: 0.7337 Train Acc: 0.7405 Eval Loss: 0.7802 Eval Acc: 0.7310 (LR: 0.001000)
[2025-05-12 21:19:25,670]: [ResNet20_parametrized_hardtanh] Epoch: 041 Train Loss: 0.7331 Train Acc: 0.7400 Eval Loss: 0.7530 Eval Acc: 0.7366 (LR: 0.001000)
[2025-05-12 21:20:03,607]: [ResNet20_parametrized_hardtanh] Epoch: 042 Train Loss: 0.7213 Train Acc: 0.7471 Eval Loss: 0.7659 Eval Acc: 0.7385 (LR: 0.001000)
[2025-05-12 21:20:41,615]: [ResNet20_parametrized_hardtanh] Epoch: 043 Train Loss: 0.7163 Train Acc: 0.7476 Eval Loss: 0.8003 Eval Acc: 0.7256 (LR: 0.001000)
[2025-05-12 21:21:19,293]: [ResNet20_parametrized_hardtanh] Epoch: 044 Train Loss: 0.7070 Train Acc: 0.7511 Eval Loss: 0.7379 Eval Acc: 0.7396 (LR: 0.001000)
[2025-05-12 21:21:56,848]: [ResNet20_parametrized_hardtanh] Epoch: 045 Train Loss: 0.7005 Train Acc: 0.7537 Eval Loss: 0.7369 Eval Acc: 0.7440 (LR: 0.001000)
[2025-05-12 21:22:34,560]: [ResNet20_parametrized_hardtanh] Epoch: 046 Train Loss: 0.6998 Train Acc: 0.7530 Eval Loss: 0.7197 Eval Acc: 0.7448 (LR: 0.001000)
[2025-05-12 21:23:12,547]: [ResNet20_parametrized_hardtanh] Epoch: 047 Train Loss: 0.6890 Train Acc: 0.7574 Eval Loss: 0.7479 Eval Acc: 0.7393 (LR: 0.001000)
[2025-05-12 21:23:50,833]: [ResNet20_parametrized_hardtanh] Epoch: 048 Train Loss: 0.6800 Train Acc: 0.7600 Eval Loss: 0.7291 Eval Acc: 0.7478 (LR: 0.001000)
[2025-05-12 21:24:28,765]: [ResNet20_parametrized_hardtanh] Epoch: 049 Train Loss: 0.6736 Train Acc: 0.7613 Eval Loss: 0.7151 Eval Acc: 0.7548 (LR: 0.001000)
[2025-05-12 21:25:07,295]: [ResNet20_parametrized_hardtanh] Epoch: 050 Train Loss: 0.6765 Train Acc: 0.7623 Eval Loss: 0.7346 Eval Acc: 0.7505 (LR: 0.001000)
[2025-05-12 21:25:45,477]: [ResNet20_parametrized_hardtanh] Epoch: 051 Train Loss: 0.6682 Train Acc: 0.7665 Eval Loss: 0.6956 Eval Acc: 0.7594 (LR: 0.001000)
[2025-05-12 21:26:23,876]: [ResNet20_parametrized_hardtanh] Epoch: 052 Train Loss: 0.6604 Train Acc: 0.7652 Eval Loss: 0.7176 Eval Acc: 0.7520 (LR: 0.001000)
[2025-05-12 21:27:04,481]: [ResNet20_parametrized_hardtanh] Epoch: 053 Train Loss: 0.6574 Train Acc: 0.7692 Eval Loss: 0.7296 Eval Acc: 0.7540 (LR: 0.001000)
[2025-05-12 21:27:43,193]: [ResNet20_parametrized_hardtanh] Epoch: 054 Train Loss: 0.6519 Train Acc: 0.7714 Eval Loss: 0.7113 Eval Acc: 0.7588 (LR: 0.001000)
[2025-05-12 21:28:21,414]: [ResNet20_parametrized_hardtanh] Epoch: 055 Train Loss: 0.6453 Train Acc: 0.7732 Eval Loss: 0.7217 Eval Acc: 0.7554 (LR: 0.001000)
[2025-05-12 21:28:59,441]: [ResNet20_parametrized_hardtanh] Epoch: 056 Train Loss: 0.6378 Train Acc: 0.7745 Eval Loss: 0.6976 Eval Acc: 0.7639 (LR: 0.001000)
[2025-05-12 21:29:37,915]: [ResNet20_parametrized_hardtanh] Epoch: 057 Train Loss: 0.6361 Train Acc: 0.7761 Eval Loss: 0.7067 Eval Acc: 0.7622 (LR: 0.001000)
[2025-05-12 21:30:17,289]: [ResNet20_parametrized_hardtanh] Epoch: 058 Train Loss: 0.6280 Train Acc: 0.7812 Eval Loss: 0.6861 Eval Acc: 0.7691 (LR: 0.001000)
[2025-05-12 21:31:00,387]: [ResNet20_parametrized_hardtanh] Epoch: 059 Train Loss: 0.6278 Train Acc: 0.7795 Eval Loss: 0.7124 Eval Acc: 0.7623 (LR: 0.001000)
[2025-05-12 21:31:41,285]: [ResNet20_parametrized_hardtanh] Epoch: 060 Train Loss: 0.6211 Train Acc: 0.7830 Eval Loss: 0.6769 Eval Acc: 0.7697 (LR: 0.001000)
[2025-05-12 21:32:22,043]: [ResNet20_parametrized_hardtanh] Epoch: 061 Train Loss: 0.6153 Train Acc: 0.7818 Eval Loss: 0.6928 Eval Acc: 0.7654 (LR: 0.001000)
[2025-05-12 21:33:02,754]: [ResNet20_parametrized_hardtanh] Epoch: 062 Train Loss: 0.6178 Train Acc: 0.7830 Eval Loss: 0.6686 Eval Acc: 0.7748 (LR: 0.001000)
[2025-05-12 21:33:42,468]: [ResNet20_parametrized_hardtanh] Epoch: 063 Train Loss: 0.6040 Train Acc: 0.7866 Eval Loss: 0.6869 Eval Acc: 0.7660 (LR: 0.001000)
[2025-05-12 21:34:20,907]: [ResNet20_parametrized_hardtanh] Epoch: 064 Train Loss: 0.6034 Train Acc: 0.7883 Eval Loss: 0.6474 Eval Acc: 0.7816 (LR: 0.001000)
[2025-05-12 21:35:00,493]: [ResNet20_parametrized_hardtanh] Epoch: 065 Train Loss: 0.5980 Train Acc: 0.7911 Eval Loss: 0.6736 Eval Acc: 0.7742 (LR: 0.001000)
[2025-05-12 21:35:41,280]: [ResNet20_parametrized_hardtanh] Epoch: 066 Train Loss: 0.5940 Train Acc: 0.7900 Eval Loss: 0.6214 Eval Acc: 0.7864 (LR: 0.001000)
[2025-05-12 21:36:21,450]: [ResNet20_parametrized_hardtanh] Epoch: 067 Train Loss: 0.5889 Train Acc: 0.7941 Eval Loss: 0.6474 Eval Acc: 0.7775 (LR: 0.001000)
[2025-05-12 21:37:00,735]: [ResNet20_parametrized_hardtanh] Epoch: 068 Train Loss: 0.5809 Train Acc: 0.7978 Eval Loss: 0.6365 Eval Acc: 0.7855 (LR: 0.001000)
[2025-05-12 21:37:43,100]: [ResNet20_parametrized_hardtanh] Epoch: 069 Train Loss: 0.5830 Train Acc: 0.7957 Eval Loss: 0.6411 Eval Acc: 0.7846 (LR: 0.001000)
[2025-05-12 21:38:22,424]: [ResNet20_parametrized_hardtanh] Epoch: 070 Train Loss: 0.5781 Train Acc: 0.7964 Eval Loss: 0.6338 Eval Acc: 0.7863 (LR: 0.000100)
[2025-05-12 21:39:02,359]: [ResNet20_parametrized_hardtanh] Epoch: 071 Train Loss: 0.5258 Train Acc: 0.8172 Eval Loss: 0.5819 Eval Acc: 0.7994 (LR: 0.000100)
[2025-05-12 21:39:44,019]: [ResNet20_parametrized_hardtanh] Epoch: 072 Train Loss: 0.5113 Train Acc: 0.8219 Eval Loss: 0.5794 Eval Acc: 0.8008 (LR: 0.000100)
[2025-05-12 21:40:25,279]: [ResNet20_parametrized_hardtanh] Epoch: 073 Train Loss: 0.5088 Train Acc: 0.8228 Eval Loss: 0.5789 Eval Acc: 0.8028 (LR: 0.000100)
[2025-05-12 21:41:04,081]: [ResNet20_parametrized_hardtanh] Epoch: 074 Train Loss: 0.5062 Train Acc: 0.8234 Eval Loss: 0.5805 Eval Acc: 0.8034 (LR: 0.000100)
[2025-05-12 21:41:43,434]: [ResNet20_parametrized_hardtanh] Epoch: 075 Train Loss: 0.5045 Train Acc: 0.8238 Eval Loss: 0.5829 Eval Acc: 0.8023 (LR: 0.000100)
[2025-05-12 21:42:22,699]: [ResNet20_parametrized_hardtanh] Epoch: 076 Train Loss: 0.5024 Train Acc: 0.8238 Eval Loss: 0.5815 Eval Acc: 0.8027 (LR: 0.000100)
[2025-05-12 21:43:00,661]: [ResNet20_parametrized_hardtanh] Epoch: 077 Train Loss: 0.5051 Train Acc: 0.8219 Eval Loss: 0.5782 Eval Acc: 0.8052 (LR: 0.000100)
[2025-05-12 21:43:39,534]: [ResNet20_parametrized_hardtanh] Epoch: 078 Train Loss: 0.5015 Train Acc: 0.8254 Eval Loss: 0.5785 Eval Acc: 0.8044 (LR: 0.000100)
[2025-05-12 21:44:17,946]: [ResNet20_parametrized_hardtanh] Epoch: 079 Train Loss: 0.4987 Train Acc: 0.8265 Eval Loss: 0.5804 Eval Acc: 0.8057 (LR: 0.000100)
[2025-05-12 21:44:56,065]: [ResNet20_parametrized_hardtanh] Epoch: 080 Train Loss: 0.4996 Train Acc: 0.8254 Eval Loss: 0.5808 Eval Acc: 0.8049 (LR: 0.000100)
[2025-05-12 21:45:33,697]: [ResNet20_parametrized_hardtanh] Epoch: 081 Train Loss: 0.5017 Train Acc: 0.8245 Eval Loss: 0.5750 Eval Acc: 0.8063 (LR: 0.000100)
[2025-05-12 21:46:15,306]: [ResNet20_parametrized_hardtanh] Epoch: 082 Train Loss: 0.4966 Train Acc: 0.8279 Eval Loss: 0.5728 Eval Acc: 0.8078 (LR: 0.000100)
[2025-05-12 21:46:53,453]: [ResNet20_parametrized_hardtanh] Epoch: 083 Train Loss: 0.4989 Train Acc: 0.8260 Eval Loss: 0.5833 Eval Acc: 0.8037 (LR: 0.000100)
[2025-05-12 21:47:31,816]: [ResNet20_parametrized_hardtanh] Epoch: 084 Train Loss: 0.4934 Train Acc: 0.8292 Eval Loss: 0.5858 Eval Acc: 0.8045 (LR: 0.000100)
[2025-05-12 21:48:10,048]: [ResNet20_parametrized_hardtanh] Epoch: 085 Train Loss: 0.4941 Train Acc: 0.8277 Eval Loss: 0.5847 Eval Acc: 0.8049 (LR: 0.000100)
[2025-05-12 21:48:48,703]: [ResNet20_parametrized_hardtanh] Epoch: 086 Train Loss: 0.4918 Train Acc: 0.8282 Eval Loss: 0.5826 Eval Acc: 0.8008 (LR: 0.000100)
[2025-05-12 21:49:27,942]: [ResNet20_parametrized_hardtanh] Epoch: 087 Train Loss: 0.4910 Train Acc: 0.8293 Eval Loss: 0.5771 Eval Acc: 0.8088 (LR: 0.000100)
[2025-05-12 21:50:06,981]: [ResNet20_parametrized_hardtanh] Epoch: 088 Train Loss: 0.4913 Train Acc: 0.8279 Eval Loss: 0.5773 Eval Acc: 0.8058 (LR: 0.000100)
[2025-05-12 21:50:45,105]: [ResNet20_parametrized_hardtanh] Epoch: 089 Train Loss: 0.4919 Train Acc: 0.8291 Eval Loss: 0.5749 Eval Acc: 0.8063 (LR: 0.000100)
[2025-05-12 21:51:23,482]: [ResNet20_parametrized_hardtanh] Epoch: 090 Train Loss: 0.4916 Train Acc: 0.8267 Eval Loss: 0.5749 Eval Acc: 0.8099 (LR: 0.000100)
[2025-05-12 21:52:02,618]: [ResNet20_parametrized_hardtanh] Epoch: 091 Train Loss: 0.4865 Train Acc: 0.8304 Eval Loss: 0.5763 Eval Acc: 0.8063 (LR: 0.000100)
[2025-05-12 21:52:42,233]: [ResNet20_parametrized_hardtanh] Epoch: 092 Train Loss: 0.4867 Train Acc: 0.8310 Eval Loss: 0.5750 Eval Acc: 0.8080 (LR: 0.000100)
[2025-05-12 21:53:21,382]: [ResNet20_parametrized_hardtanh] Epoch: 093 Train Loss: 0.4866 Train Acc: 0.8298 Eval Loss: 0.5795 Eval Acc: 0.8061 (LR: 0.000100)
[2025-05-12 21:54:00,870]: [ResNet20_parametrized_hardtanh] Epoch: 094 Train Loss: 0.4866 Train Acc: 0.8296 Eval Loss: 0.5774 Eval Acc: 0.8081 (LR: 0.000100)
[2025-05-12 21:54:40,603]: [ResNet20_parametrized_hardtanh] Epoch: 095 Train Loss: 0.4860 Train Acc: 0.8305 Eval Loss: 0.5758 Eval Acc: 0.8126 (LR: 0.000100)
[2025-05-12 21:55:21,532]: [ResNet20_parametrized_hardtanh] Epoch: 096 Train Loss: 0.4830 Train Acc: 0.8311 Eval Loss: 0.5715 Eval Acc: 0.8062 (LR: 0.000100)
[2025-05-12 21:56:01,564]: [ResNet20_parametrized_hardtanh] Epoch: 097 Train Loss: 0.4900 Train Acc: 0.8291 Eval Loss: 0.5750 Eval Acc: 0.8063 (LR: 0.000100)
[2025-05-12 21:56:52,092]: [ResNet20_parametrized_hardtanh] Epoch: 098 Train Loss: 0.4824 Train Acc: 0.8307 Eval Loss: 0.5704 Eval Acc: 0.8091 (LR: 0.000100)
[2025-05-12 21:58:10,422]: [ResNet20_parametrized_hardtanh] Epoch: 099 Train Loss: 0.4823 Train Acc: 0.8321 Eval Loss: 0.5761 Eval Acc: 0.8060 (LR: 0.000100)
[2025-05-12 21:59:42,024]: [ResNet20_parametrized_hardtanh] Epoch: 100 Train Loss: 0.4820 Train Acc: 0.8306 Eval Loss: 0.5710 Eval Acc: 0.8090 (LR: 0.000010)
[2025-05-12 22:00:24,188]: [ResNet20_parametrized_hardtanh] Epoch: 101 Train Loss: 0.4780 Train Acc: 0.8325 Eval Loss: 0.5708 Eval Acc: 0.8101 (LR: 0.000010)
[2025-05-12 22:01:46,338]: [ResNet20_parametrized_hardtanh] Epoch: 102 Train Loss: 0.4772 Train Acc: 0.8329 Eval Loss: 0.5715 Eval Acc: 0.8089 (LR: 0.000010)
[2025-05-12 22:03:10,749]: [ResNet20_parametrized_hardtanh] Epoch: 103 Train Loss: 0.4762 Train Acc: 0.8328 Eval Loss: 0.5741 Eval Acc: 0.8076 (LR: 0.000010)
[2025-05-12 22:04:34,716]: [ResNet20_parametrized_hardtanh] Epoch: 104 Train Loss: 0.4703 Train Acc: 0.8362 Eval Loss: 0.5674 Eval Acc: 0.8102 (LR: 0.000010)
[2025-05-12 22:06:01,766]: [ResNet20_parametrized_hardtanh] Epoch: 105 Train Loss: 0.4739 Train Acc: 0.8351 Eval Loss: 0.5693 Eval Acc: 0.8096 (LR: 0.000010)
[2025-05-12 22:06:52,086]: [ResNet20_parametrized_hardtanh] Epoch: 106 Train Loss: 0.4771 Train Acc: 0.8329 Eval Loss: 0.5711 Eval Acc: 0.8076 (LR: 0.000010)
[2025-05-12 22:07:49,495]: [ResNet20_parametrized_hardtanh] Epoch: 107 Train Loss: 0.4735 Train Acc: 0.8357 Eval Loss: 0.5668 Eval Acc: 0.8093 (LR: 0.000010)
[2025-05-12 22:08:56,660]: [ResNet20_parametrized_hardtanh] Epoch: 108 Train Loss: 0.4775 Train Acc: 0.8332 Eval Loss: 0.5696 Eval Acc: 0.8108 (LR: 0.000010)
[2025-05-12 22:10:29,126]: [ResNet20_parametrized_hardtanh] Epoch: 109 Train Loss: 0.4724 Train Acc: 0.8340 Eval Loss: 0.5707 Eval Acc: 0.8074 (LR: 0.000010)
[2025-05-12 22:12:34,094]: [ResNet20_parametrized_hardtanh] Epoch: 110 Train Loss: 0.4697 Train Acc: 0.8350 Eval Loss: 0.5693 Eval Acc: 0.8088 (LR: 0.000010)
[2025-05-12 22:14:25,274]: [ResNet20_parametrized_hardtanh] Epoch: 111 Train Loss: 0.4753 Train Acc: 0.8336 Eval Loss: 0.5708 Eval Acc: 0.8089 (LR: 0.000010)
[2025-05-12 22:16:25,696]: [ResNet20_parametrized_hardtanh] Epoch: 112 Train Loss: 0.4745 Train Acc: 0.8337 Eval Loss: 0.5703 Eval Acc: 0.8084 (LR: 0.000010)
[2025-05-12 22:18:37,760]: [ResNet20_parametrized_hardtanh] Epoch: 113 Train Loss: 0.4702 Train Acc: 0.8359 Eval Loss: 0.5721 Eval Acc: 0.8082 (LR: 0.000010)
[2025-05-12 22:20:48,457]: [ResNet20_parametrized_hardtanh] Epoch: 114 Train Loss: 0.4755 Train Acc: 0.8352 Eval Loss: 0.5696 Eval Acc: 0.8108 (LR: 0.000010)
[2025-05-12 22:22:13,859]: [ResNet20_parametrized_hardtanh] Epoch: 115 Train Loss: 0.4740 Train Acc: 0.8343 Eval Loss: 0.5677 Eval Acc: 0.8098 (LR: 0.000010)
[2025-05-12 22:24:21,317]: [ResNet20_parametrized_hardtanh] Epoch: 116 Train Loss: 0.4744 Train Acc: 0.8330 Eval Loss: 0.5675 Eval Acc: 0.8112 (LR: 0.000010)
[2025-05-12 22:26:22,364]: [ResNet20_parametrized_hardtanh] Epoch: 117 Train Loss: 0.4734 Train Acc: 0.8335 Eval Loss: 0.5678 Eval Acc: 0.8086 (LR: 0.000010)
[2025-05-12 22:28:05,955]: [ResNet20_parametrized_hardtanh] Epoch: 118 Train Loss: 0.4685 Train Acc: 0.8375 Eval Loss: 0.5668 Eval Acc: 0.8107 (LR: 0.000010)
[2025-05-12 22:29:49,524]: [ResNet20_parametrized_hardtanh] Epoch: 119 Train Loss: 0.4729 Train Acc: 0.8366 Eval Loss: 0.5687 Eval Acc: 0.8092 (LR: 0.000010)
[2025-05-12 22:31:18,512]: [ResNet20_parametrized_hardtanh] Epoch: 120 Train Loss: 0.4734 Train Acc: 0.8353 Eval Loss: 0.5681 Eval Acc: 0.8092 (LR: 0.000010)
[2025-05-12 22:31:18,514]: [ResNet20_parametrized_hardtanh] Best Eval Accuracy: 0.8126
[2025-05-12 22:31:18,634]: 
Training of full-precision model finished!
[2025-05-12 22:31:18,635]: Model Architecture:
[2025-05-12 22:31:18,642]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 22:31:18,644]: 
Model Weights:
[2025-05-12 22:31:18,644]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 22:31:18,691]: Sample Values (25 elements): [0.14993716776371002, 0.10587874799966812, -0.03960525244474411, -0.2354145497083664, 0.2063395082950592, -0.13415572047233582, 0.07314114272594452, -0.07937245815992355, -0.004468243569135666, 0.17317934334278107, -0.18489794433116913, -0.11002283543348312, -0.2463604062795639, -0.24997135996818542, 0.10623922944068909, -0.21637685596942902, 0.09318292140960693, 0.14163430035114288, 0.16535717248916626, -0.17379844188690186, 0.011835245415568352, -0.2771144509315491, -0.05962788686156273, 0.34292343258857727, 0.10872621834278107]
[2025-05-12 22:31:18,708]: Mean: 0.00580338
[2025-05-12 22:31:18,728]: Min: -0.49093211
[2025-05-12 22:31:18,734]: Max: 0.45355201
[2025-05-12 22:31:18,734]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,736]: Sample Values (16 elements): [0.8449642062187195, 0.999177098274231, 1.0698148012161255, 0.9931100010871887, 0.7848185896873474, 0.9991408586502075, 0.8570976853370667, 1.0630807876586914, 0.9047983884811401, 0.7242438793182373, 0.9334794282913208, 0.797723650932312, 1.1128778457641602, 0.7428175806999207, 0.9056758880615234, 0.761671781539917]
[2025-05-12 22:31:18,736]: Mean: 0.90590584
[2025-05-12 22:31:18,737]: Min: 0.72424388
[2025-05-12 22:31:18,738]: Max: 1.11287785
[2025-05-12 22:31:18,738]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 22:31:18,741]: Sample Values (25 elements): [-0.15362195670604706, 0.008409107103943825, 0.02966914512217045, 0.004974754527211189, -0.036832887679338455, 0.06660369783639908, 0.05654102563858032, -0.056450702250003815, -0.07221980392932892, -0.007510801777243614, 0.02813350036740303, 0.05124989151954651, 0.029628198593854904, -0.1213068887591362, -0.08890916407108307, 0.02073204517364502, 0.06526029109954834, -0.000906793458852917, 0.06744898110628128, 0.020734675228595734, 0.1083282008767128, -0.01946260780096054, 0.16588391363620758, 0.04979470372200012, -0.01816345937550068]
[2025-05-12 22:31:18,742]: Mean: 0.00160082
[2025-05-12 22:31:18,743]: Min: -0.27543163
[2025-05-12 22:31:18,744]: Max: 0.34046468
[2025-05-12 22:31:18,744]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,745]: Sample Values (16 elements): [0.9675499200820923, 1.0656850337982178, 0.8714597225189209, 0.8233813643455505, 0.8079792857170105, 0.8210365176200867, 0.9955154657363892, 0.9245284795761108, 0.9829064607620239, 0.8338279128074646, 0.9947732090950012, 0.8336325287818909, 0.9640817642211914, 1.1312168836593628, 1.0625522136688232, 0.8338336944580078]
[2025-05-12 22:31:18,746]: Mean: 0.93212253
[2025-05-12 22:31:18,746]: Min: 0.80797929
[2025-05-12 22:31:18,746]: Max: 1.13121688
[2025-05-12 22:31:18,747]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 22:31:18,747]: Sample Values (25 elements): [0.004262332338839769, 0.009192993864417076, -0.06168742850422859, 0.03428078070282936, -0.06772313266992569, -0.061571795493364334, -0.018875859677791595, -0.12550070881843567, -0.019033709540963173, -0.1447802037000656, -0.09773703664541245, -0.02126237377524376, 0.037884604185819626, -0.06653669476509094, -0.05516357719898224, -0.022231334820389748, -0.017176266759634018, 0.03214779496192932, 0.09583739191293716, -0.04864493012428284, 0.04804657772183418, -0.06366395205259323, -0.029221858829259872, 0.17615127563476562, -0.019745251163840294]
[2025-05-12 22:31:18,748]: Mean: -0.00101275
[2025-05-12 22:31:18,749]: Min: -0.28056559
[2025-05-12 22:31:18,749]: Max: 0.21372141
[2025-05-12 22:31:18,749]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,749]: Sample Values (16 elements): [0.8592733144760132, 0.8683924078941345, 1.0583751201629639, 0.8055822253227234, 0.9890655875205994, 0.8848341703414917, 0.7719627618789673, 0.8646254539489746, 0.7695501446723938, 0.9466413259506226, 0.9600188732147217, 0.8698744773864746, 0.8912811875343323, 0.8398168087005615, 0.955297589302063, 0.8074759244918823]
[2025-05-12 22:31:18,750]: Mean: 0.88387918
[2025-05-12 22:31:18,750]: Min: 0.76955014
[2025-05-12 22:31:18,750]: Max: 1.05837512
[2025-05-12 22:31:18,750]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 22:31:18,750]: Sample Values (25 elements): [0.03901002183556557, 0.08330132067203522, -0.0729067474603653, 0.04461296275258064, -0.007520411163568497, -0.04303014278411865, -0.03485134616494179, -0.07395651191473007, -0.005398779641836882, -0.02095278911292553, -0.12407849729061127, 0.04251952841877937, -0.0497780479490757, 0.056834254413843155, 0.05181228742003441, 0.018453288823366165, 0.018619371578097343, -0.03222833573818207, -0.02090633474290371, 0.0009626129758544266, 0.041116539388895035, 0.08481629937887192, 0.06841277331113815, 0.013436428271234035, -0.03332241252064705]
[2025-05-12 22:31:18,750]: Mean: 0.00196120
[2025-05-12 22:31:18,751]: Min: -0.23218365
[2025-05-12 22:31:18,751]: Max: 0.25390121
[2025-05-12 22:31:18,751]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,752]: Sample Values (16 elements): [0.9754980802536011, 0.811362624168396, 0.9839202165603638, 0.8633037805557251, 0.8926255702972412, 1.0125987529754639, 1.0764061212539673, 1.009320855140686, 1.003286361694336, 1.0034644603729248, 1.0178816318511963, 0.9672368764877319, 0.9551907181739807, 1.0267037153244019, 0.8546612858772278, 0.9425600171089172]
[2025-05-12 22:31:18,753]: Mean: 0.96225131
[2025-05-12 22:31:18,754]: Min: 0.81136262
[2025-05-12 22:31:18,755]: Max: 1.07640612
[2025-05-12 22:31:18,755]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 22:31:18,756]: Sample Values (25 elements): [0.20175908505916595, 0.0774272084236145, -0.023000385612249374, 0.006125856656581163, -0.06828076392412186, 0.048283159732818604, -0.06341076642274857, -0.07256250828504562, -0.01989787630736828, 0.001724923960864544, -0.0517800971865654, -0.048802342265844345, -0.038530394434928894, -0.03485768660902977, -0.022972801700234413, 0.05574554577469826, 0.06494338810443878, -0.030857445672154427, -0.13216166198253632, -0.09049784392118454, -0.05830160900950432, -0.015241701155900955, -0.025352735072374344, 4.353276744950563e-05, -0.008477495983242989]
[2025-05-12 22:31:18,757]: Mean: -0.00207761
[2025-05-12 22:31:18,758]: Min: -0.20914996
[2025-05-12 22:31:18,759]: Max: 0.24583083
[2025-05-12 22:31:18,759]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,760]: Sample Values (16 elements): [0.9130827784538269, 0.9548161029815674, 0.812678873538971, 0.9574089646339417, 1.0081123113632202, 0.9678794741630554, 0.9503033757209778, 0.9540833830833435, 0.946893572807312, 0.9636044502258301, 0.9980950355529785, 0.7907602787017822, 1.001698613166809, 0.9359283447265625, 0.891315758228302, 0.9001266360282898]
[2025-05-12 22:31:18,760]: Mean: 0.93417424
[2025-05-12 22:31:18,761]: Min: 0.79076028
[2025-05-12 22:31:18,762]: Max: 1.00811231
[2025-05-12 22:31:18,763]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 22:31:18,766]: Sample Values (25 elements): [0.057267893105745316, 0.08496707677841187, 2.8863166789960815e-06, 0.08875185251235962, -0.018446341156959534, 0.013536334037780762, 0.04710395261645317, 0.014318949542939663, -0.09564536809921265, -0.05976676940917969, -0.06206861510872841, 0.06829821318387985, -0.05443170666694641, 0.07906506210565567, -0.07566115260124207, -0.033726830035448074, -0.018432220444083214, 0.03709927573800087, 0.015783293172717094, -0.027915582060813904, 0.025566285476088524, 0.010671497322618961, -0.0376470722258091, -0.0698544979095459, 0.11277659982442856]
[2025-05-12 22:31:18,769]: Mean: 0.00096216
[2025-05-12 22:31:18,770]: Min: -0.19851162
[2025-05-12 22:31:18,771]: Max: 0.20705202
[2025-05-12 22:31:18,771]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,772]: Sample Values (16 elements): [0.9287643432617188, 0.9603696465492249, 0.9945805668830872, 1.0094362497329712, 0.9516215920448303, 1.0884546041488647, 1.0095640420913696, 0.9379429817199707, 0.9023962616920471, 0.9718529582023621, 0.9781944155693054, 0.9163007140159607, 0.8961912989616394, 0.9854603409767151, 0.9572914242744446, 0.9989683032035828]
[2025-05-12 22:31:18,773]: Mean: 0.96796185
[2025-05-12 22:31:18,773]: Min: 0.89619130
[2025-05-12 22:31:18,773]: Max: 1.08845460
[2025-05-12 22:31:18,773]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 22:31:18,774]: Sample Values (25 elements): [0.07913070172071457, 0.006621264386922121, 0.05738024786114693, -0.01008918322622776, 0.018910882994532585, -0.07380653917789459, 0.03362615033984184, -0.006329671014100313, 0.05085016414523125, 0.01598202809691429, 0.026999937370419502, 0.012517201714217663, -0.05790773406624794, 0.03482535481452942, 0.014239718206226826, -0.0006822907598689198, -0.029842881485819817, 0.09765687584877014, -0.02369944006204605, 0.044413160532712936, 0.06855516135692596, -0.15545965731143951, 0.010689391754567623, 0.05920867249369621, 0.007203007582575083]
[2025-05-12 22:31:18,774]: Mean: -0.00257257
[2025-05-12 22:31:18,775]: Min: -0.18726647
[2025-05-12 22:31:18,776]: Max: 0.17210649
[2025-05-12 22:31:18,776]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 22:31:18,778]: Sample Values (16 elements): [0.9391366243362427, 0.9084047079086304, 0.9815387725830078, 0.9842735528945923, 0.9942553043365479, 0.9890343546867371, 0.9321380853652954, 0.921097993850708, 0.9695586562156677, 0.9167410135269165, 0.905364990234375, 0.9431955218315125, 0.8892965912818909, 0.9993742108345032, 0.9636518955230713, 0.9358817934989929]
[2025-05-12 22:31:18,779]: Mean: 0.94830900
[2025-05-12 22:31:18,780]: Min: 0.88929659
[2025-05-12 22:31:18,781]: Max: 0.99937421
[2025-05-12 22:31:18,782]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 22:31:18,783]: Sample Values (25 elements): [0.025973912328481674, -0.042083002626895905, 0.025615723803639412, 0.031772226095199585, 0.0175664983689785, -0.02237267792224884, 0.07422751933336258, -0.07139269262552261, 0.04505156725645065, -0.018501948565244675, -0.024653354659676552, 0.03067280352115631, 0.019934041425585747, -0.06627547740936279, 0.04866467043757439, -0.05714959278702736, -0.043729428201913834, 0.08909370750188828, 0.013918161392211914, 0.0403972826898098, -0.029519498348236084, 0.005688173696398735, 0.062201227992773056, 0.004760557319968939, 0.022486330941319466]
[2025-05-12 22:31:18,784]: Mean: -0.00193317
[2025-05-12 22:31:18,784]: Min: -0.17283288
[2025-05-12 22:31:18,784]: Max: 0.15549849
[2025-05-12 22:31:18,784]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,785]: Sample Values (25 elements): [0.9587884545326233, 0.9605620503425598, 0.9682108759880066, 0.9649533629417419, 0.9824700355529785, 0.9515939354896545, 0.9613859057426453, 0.9615727066993713, 0.9853305220603943, 0.9736385941505432, 0.9751800894737244, 0.9758139252662659, 0.9615759253501892, 0.9779389500617981, 0.9457861185073853, 0.960080623626709, 0.9463346600532532, 0.9734436273574829, 0.9705976843833923, 0.9665493965148926, 0.9687529802322388, 0.9452807307243347, 0.9195654988288879, 0.9736297726631165, 0.9224995374679565]
[2025-05-12 22:31:18,785]: Mean: 0.95999146
[2025-05-12 22:31:18,786]: Min: 0.91956550
[2025-05-12 22:31:18,786]: Max: 0.98533052
[2025-05-12 22:31:18,786]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 22:31:18,787]: Sample Values (25 elements): [0.044884320348501205, 0.08637415617704391, 0.039362065494060516, -0.03972725197672844, 0.026778582483530045, -0.00019260836415924132, -0.09310799837112427, 0.0002325710520381108, 0.03336973115801811, 0.012382946908473969, -0.01771179772913456, 0.058411967009305954, -0.02382909506559372, 0.005045710131525993, 0.04707944393157959, 0.07624753564596176, 0.040303099900484085, -0.08180245757102966, -0.0009421128779649734, -0.05091993883252144, -0.02078055962920189, -0.02641524374485016, 0.07160119712352753, -0.015173910185694695, -0.06099186837673187]
[2025-05-12 22:31:18,787]: Mean: 0.00012633
[2025-05-12 22:31:18,787]: Min: -0.12017972
[2025-05-12 22:31:18,787]: Max: 0.11423466
[2025-05-12 22:31:18,787]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,788]: Sample Values (25 elements): [0.965050995349884, 0.9721454381942749, 0.9302850365638733, 0.9912567734718323, 1.0400593280792236, 0.9686850905418396, 1.0058895349502563, 0.9809208512306213, 0.9778895378112793, 0.9567418098449707, 0.9264675378799438, 0.9486306309700012, 0.935050368309021, 0.9776206016540527, 0.9818804860115051, 0.9606714844703674, 0.9713624715805054, 1.0246742963790894, 0.9617881178855896, 0.96197110414505, 0.9987105131149292, 0.9736868739128113, 0.9689837694168091, 0.9443319439888, 0.9735665321350098]
[2025-05-12 22:31:18,788]: Mean: 0.97214544
[2025-05-12 22:31:18,788]: Min: 0.92399400
[2025-05-12 22:31:18,788]: Max: 1.04005933
[2025-05-12 22:31:18,788]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 22:31:18,789]: Sample Values (25 elements): [-0.2536451816558838, 0.1436307281255722, 0.01725965179502964, 0.1629386693239212, 0.1860402673482895, 0.048500724136829376, 0.16687355935573578, -0.2092103213071823, 0.1822662204504013, -0.16593627631664276, 0.032856155186891556, -0.010225199162960052, 0.14084835350513458, 0.19709300994873047, -0.023385612294077873, -0.17566466331481934, 0.05336291715502739, -0.21193666756153107, 0.1451825499534607, -0.16284561157226562, 0.00815196055918932, 0.08480897545814514, -0.00689512025564909, 0.13089619576931, 0.15112560987472534]
[2025-05-12 22:31:18,789]: Mean: -0.00605696
[2025-05-12 22:31:18,789]: Min: -0.32176295
[2025-05-12 22:31:18,790]: Max: 0.30128515
[2025-05-12 22:31:18,790]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,791]: Sample Values (25 elements): [0.9011893272399902, 0.8956848382949829, 0.8506261110305786, 0.8702729344367981, 0.9022762775421143, 0.9414272308349609, 0.905013382434845, 0.9620683789253235, 0.871704638004303, 0.8966407179832458, 0.8981633186340332, 0.8996760249137878, 0.8704120516777039, 0.8900038599967957, 0.8949853777885437, 0.875182032585144, 0.9011863470077515, 0.9378581047058105, 0.8729246854782104, 0.865317702293396, 0.916554868221283, 0.8908209204673767, 0.8629251718521118, 0.8632578253746033, 0.9245301485061646]
[2025-05-12 22:31:18,793]: Mean: 0.89726943
[2025-05-12 22:31:18,794]: Min: 0.85062611
[2025-05-12 22:31:18,795]: Max: 0.96206838
[2025-05-12 22:31:18,795]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 22:31:18,798]: Sample Values (25 elements): [-0.017708824947476387, -0.047795120626688004, 0.04632950201630592, 0.007385489530861378, -0.01656084693968296, -0.0472615547478199, -0.06345086544752121, -0.04921445623040199, -0.010081715881824493, -0.0391668900847435, -0.005475815385580063, 0.06828062981367111, 0.04102731868624687, 0.07114558666944504, -0.011126902885735035, -0.023843446746468544, 0.025460153818130493, 0.0383279025554657, 0.04242406040430069, -0.028714537620544434, -0.022645404562354088, -0.05205218866467476, 0.00878493394702673, 0.053102582693099976, -0.04567616060376167]
[2025-05-12 22:31:18,799]: Mean: 0.00043399
[2025-05-12 22:31:18,800]: Min: -0.12249096
[2025-05-12 22:31:18,800]: Max: 0.12939350
[2025-05-12 22:31:18,800]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,801]: Sample Values (25 elements): [0.9566450119018555, 0.9672699570655823, 0.9758758544921875, 0.957675576210022, 0.9620034098625183, 0.9970224499702454, 0.9789420962333679, 1.0109065771102905, 1.0207765102386475, 0.9552245140075684, 0.9740121364593506, 0.959909975528717, 0.9978570342063904, 0.9746564030647278, 0.9773614406585693, 0.9534385800361633, 0.9758514761924744, 0.9902015328407288, 0.9747399687767029, 0.9654542803764343, 0.9548743367195129, 0.9918727278709412, 0.9744072556495667, 0.9705308675765991, 0.970770001411438]
[2025-05-12 22:31:18,802]: Mean: 0.97285473
[2025-05-12 22:31:18,803]: Min: 0.92993349
[2025-05-12 22:31:18,804]: Max: 1.02077651
[2025-05-12 22:31:18,804]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 22:31:18,807]: Sample Values (25 elements): [-0.06760600954294205, -0.006581916008144617, -0.001285570440813899, -0.032433971762657166, -0.05301646143198013, 0.006405405700206757, -0.005886557511985302, -0.023765558376908302, -0.013760840520262718, 0.021182138472795486, 0.10935254395008087, -0.03993287310004234, 0.04147926717996597, 0.04550088196992874, 0.021148929372429848, 0.017450528219342232, -0.05731872841715813, 0.014920884743332863, -0.001124880975112319, 0.008478907868266106, -0.08519575744867325, -0.007320653181523085, 0.03919471800327301, 0.0854988545179367, 0.008514068089425564]
[2025-05-12 22:31:18,808]: Mean: 0.00085945
[2025-05-12 22:31:18,808]: Min: -0.12387570
[2025-05-12 22:31:18,810]: Max: 0.13091438
[2025-05-12 22:31:18,810]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,811]: Sample Values (25 elements): [0.9786787629127502, 0.9585728645324707, 0.9823204874992371, 0.9872345328330994, 0.9182952642440796, 0.9918594360351562, 0.9600563645362854, 0.9450647234916687, 0.9953944683074951, 1.0061675310134888, 0.95005202293396, 0.9541016221046448, 0.9765458703041077, 1.0087275505065918, 0.9414522051811218, 1.0364210605621338, 0.987506091594696, 0.976932942867279, 0.9569054841995239, 0.9860261082649231, 0.9428080320358276, 0.9790680408477783, 0.9708608388900757, 1.0184545516967773, 0.9279574155807495]
[2025-05-12 22:31:18,811]: Mean: 0.97386700
[2025-05-12 22:31:18,811]: Min: 0.91829526
[2025-05-12 22:31:18,811]: Max: 1.03642106
[2025-05-12 22:31:18,811]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 22:31:18,812]: Sample Values (25 elements): [0.040273334830999374, -0.03140993043780327, 0.046151675283908844, -0.014140027575194836, 0.03674172982573509, 0.010018284432590008, -0.012698380276560783, -0.044168226420879364, -0.015722306445240974, -0.04885873571038246, -0.051135532557964325, 0.007876970805227757, -0.032128456979990005, 0.07022950798273087, 0.041226986795663834, -0.07002459466457367, 0.025858085602521896, 0.022637199610471725, -0.05231967195868492, -0.014352919533848763, -0.0019417894072830677, -0.03426886349916458, -0.03493395447731018, 0.046723440289497375, 0.015587843954563141]
[2025-05-12 22:31:18,812]: Mean: 0.00005752
[2025-05-12 22:31:18,812]: Min: -0.11210917
[2025-05-12 22:31:18,813]: Max: 0.10782708
[2025-05-12 22:31:18,813]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,813]: Sample Values (25 elements): [0.9610450267791748, 0.9470534324645996, 0.9774755835533142, 0.9627158045768738, 0.935260534286499, 0.9750933051109314, 0.9703951478004456, 0.9667390584945679, 1.0167344808578491, 0.9911813139915466, 0.9646301865577698, 0.9461212158203125, 0.9718529582023621, 0.9514371156692505, 0.9876634478569031, 0.974530816078186, 0.9743974208831787, 0.97967129945755, 0.965679407119751, 0.9600583910942078, 0.9655791521072388, 1.000899314880371, 0.9489805698394775, 0.9696109890937805, 0.9591838121414185]
[2025-05-12 22:31:18,813]: Mean: 0.97091496
[2025-05-12 22:31:18,813]: Min: 0.93526053
[2025-05-12 22:31:18,813]: Max: 1.01673448
[2025-05-12 22:31:18,814]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 22:31:18,814]: Sample Values (25 elements): [-0.0075128329917788506, 0.04091833531856537, 0.0485553964972496, -0.0037991637364029884, 0.06301239877939224, -0.00017333174764644355, 0.04244430735707283, -0.042178474366664886, -0.02294418029487133, 0.026671288534998894, 0.062851682305336, 0.06180550158023834, -0.04540383070707321, -0.06309505552053452, -0.002706096973270178, 0.02578531764447689, -0.04711957275867462, 0.024462180212140083, 0.018760941922664642, 0.06530790030956268, -0.05125507339835167, 0.015925033017992973, 0.017884427681565285, -0.02877771481871605, -0.03512224927544594]
[2025-05-12 22:31:18,814]: Mean: -0.00031427
[2025-05-12 22:31:18,814]: Min: -0.11252207
[2025-05-12 22:31:18,814]: Max: 0.11934998
[2025-05-12 22:31:18,814]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 22:31:18,815]: Sample Values (25 elements): [0.9936299920082092, 0.9688098430633545, 0.970936119556427, 0.9506238102912903, 1.0027011632919312, 1.004692554473877, 0.9505690336227417, 0.9609546065330505, 0.9733738899230957, 0.9934020042419434, 0.9868808388710022, 0.946871817111969, 0.9906023740768433, 1.0067312717437744, 0.9614368677139282, 0.992067813873291, 1.000501036643982, 0.9598683714866638, 0.9773356914520264, 1.014877200126648, 0.9666669964790344, 1.0322189331054688, 0.9821746349334717, 0.9430960416793823, 0.9921295642852783]
[2025-05-12 22:31:18,815]: Mean: 0.98178053
[2025-05-12 22:31:18,815]: Min: 0.94309604
[2025-05-12 22:31:18,815]: Max: 1.03221893
[2025-05-12 22:31:18,815]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 22:31:18,816]: Sample Values (25 elements): [0.0026110722683370113, 0.0016572913154959679, -0.026556840166449547, -0.09056789427995682, 0.02954207919538021, 0.040310926735401154, -0.017499832436442375, 0.034659888595342636, 0.04938553273677826, -0.030788477510213852, -0.0012787766754627228, 0.06572066992521286, -0.042849574238061905, -0.012701472267508507, 0.02189900167286396, -0.03460115194320679, 0.010683020576834679, -0.02674022689461708, 0.007859921082854271, -0.002104089129716158, -0.012031104415655136, 0.015971068292856216, -0.006685734260827303, -0.056588590145111084, 0.05010998249053955]
[2025-05-12 22:31:18,816]: Mean: 0.00011511
[2025-05-12 22:31:18,816]: Min: -0.10872660
[2025-05-12 22:31:18,817]: Max: 0.11199797
[2025-05-12 22:31:18,817]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,818]: Sample Values (25 elements): [0.970295786857605, 0.9679569602012634, 0.9512289762496948, 0.971207857131958, 0.9695687294006348, 0.9811874628067017, 0.9664666056632996, 0.9608884453773499, 0.9799684882164001, 0.9778613448143005, 0.9610723853111267, 0.9588693380355835, 0.9694236516952515, 0.9883158206939697, 0.9769814610481262, 0.9943432211875916, 0.9823608994483948, 0.9715729355812073, 0.9570087790489197, 0.9768148064613342, 0.9643175601959229, 0.9761884212493896, 0.9808657169342041, 0.9555560946464539, 0.9792419672012329]
[2025-05-12 22:31:18,818]: Mean: 0.97139144
[2025-05-12 22:31:18,819]: Min: 0.95122898
[2025-05-12 22:31:18,820]: Max: 1.03133035
[2025-05-12 22:31:18,821]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 22:31:18,823]: Sample Values (25 elements): [0.026759140193462372, 0.0362669937312603, 0.013203185051679611, 0.010694120079278946, 0.009633797220885754, 0.014016291126608849, 0.002409176668152213, -0.020433953031897545, 0.042770273983478546, 0.0030462341383099556, -0.019231144338846207, -0.030536295846104622, 0.01795358769595623, -0.0078043281100690365, 0.013993257656693459, 0.017327819019556046, 0.018879076465964317, 0.041192371398210526, -0.03278123214840889, -0.013623345643281937, -0.033206142485141754, -0.04637238755822182, 0.01321101188659668, -0.018131842836737633, -0.01661747321486473]
[2025-05-12 22:31:18,824]: Mean: 0.00033411
[2025-05-12 22:31:18,824]: Min: -0.08709712
[2025-05-12 22:31:18,825]: Max: 0.08795570
[2025-05-12 22:31:18,825]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,826]: Sample Values (25 elements): [0.9850483536720276, 0.9943747520446777, 0.9834266304969788, 0.9597156047821045, 0.9368845224380493, 0.9755434393882751, 0.9805851578712463, 0.9591569900512695, 0.9957272410392761, 0.9652786254882812, 0.9958697557449341, 0.966302216053009, 0.9857191443443298, 0.9873120188713074, 0.9869115352630615, 0.961803138256073, 0.9906880855560303, 0.9815893173217773, 0.9820181131362915, 0.9676231145858765, 0.9908339381217957, 0.9700425267219543, 0.9729471206665039, 0.9894025325775146, 0.9819374680519104]
[2025-05-12 22:31:18,827]: Mean: 0.98137248
[2025-05-12 22:31:18,828]: Min: 0.93688452
[2025-05-12 22:31:18,828]: Max: 1.01725876
[2025-05-12 22:31:18,828]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 22:31:18,831]: Sample Values (25 elements): [-0.06575082987546921, 0.13771694898605347, -0.11637769639492035, 0.03572317585349083, -0.05315509811043739, -0.06739559769630432, 0.19529737532138824, 0.0024848002940416336, -0.08437731862068176, -0.04628603532910347, -0.07680611312389374, -0.012541769072413445, 0.17397719621658325, -0.1458723247051239, -0.05298925191164017, -0.09025366604328156, 0.03436800092458725, 0.11249157041311264, 0.0462351031601429, 0.17739157378673553, -0.055056244134902954, 0.15579259395599365, 0.019660141319036484, -0.02020990289747715, 0.13926592469215393]
[2025-05-12 22:31:18,833]: Mean: -0.00023781
[2025-05-12 22:31:18,834]: Min: -0.22074609
[2025-05-12 22:31:18,835]: Max: 0.22495563
[2025-05-12 22:31:18,835]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,836]: Sample Values (25 elements): [0.9523703455924988, 0.9452598094940186, 0.910364031791687, 0.949280321598053, 0.9281198382377625, 0.9359238743782043, 0.9753568768501282, 0.9442336559295654, 0.9486972689628601, 0.9318968057632446, 0.9425592422485352, 0.9287601709365845, 0.9584987759590149, 0.9421340823173523, 0.9233483076095581, 0.9012708067893982, 0.9424079060554504, 0.9509468078613281, 0.9439489841461182, 0.941853940486908, 0.9426952004432678, 0.962391197681427, 0.9383057951927185, 0.939555287361145, 0.9374961256980896]
[2025-05-12 22:31:18,837]: Mean: 0.94118464
[2025-05-12 22:31:18,837]: Min: 0.90127081
[2025-05-12 22:31:18,838]: Max: 0.97953254
[2025-05-12 22:31:18,838]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 22:31:18,840]: Sample Values (25 elements): [0.011431200429797173, -0.030491983518004417, 0.029086116701364517, 0.0064598494209349155, -0.032845161855220795, 0.007952152751386166, -0.0411866158246994, 0.012492779642343521, -0.015726210549473763, 0.04705951362848282, -0.0006769433966837823, -0.018053380772471428, -0.006373535841703415, 0.005147934425622225, 0.041770968586206436, -0.030900539830327034, 0.036153364926576614, -0.02469007857143879, 0.011499745771288872, -0.05074601247906685, -0.018764156848192215, -0.043376341462135315, 0.0004317305574659258, 0.023376747965812683, -0.025400573387742043]
[2025-05-12 22:31:18,841]: Mean: 0.00001863
[2025-05-12 22:31:18,843]: Min: -0.09800242
[2025-05-12 22:31:18,844]: Max: 0.09548382
[2025-05-12 22:31:18,844]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,845]: Sample Values (25 elements): [0.9913877248764038, 0.9761907458305359, 0.9834648966789246, 0.9759649038314819, 0.9755550622940063, 0.9991592764854431, 0.9703847169876099, 0.9681891202926636, 0.9857493042945862, 0.9668096303939819, 0.9767877459526062, 0.9759061932563782, 0.9695075154304504, 0.9713292717933655, 0.9687872529029846, 0.9692772030830383, 0.9852097034454346, 0.9677793383598328, 0.9791695475578308, 0.9633066654205322, 0.9907752275466919, 0.9799774885177612, 0.9706177711486816, 0.9595852494239807, 0.9806739091873169]
[2025-05-12 22:31:18,846]: Mean: 0.97717196
[2025-05-12 22:31:18,847]: Min: 0.95958525
[2025-05-12 22:31:18,847]: Max: 1.01326585
[2025-05-12 22:31:18,847]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 22:31:18,848]: Sample Values (25 elements): [-0.04202909395098686, -0.021394431591033936, 0.06365959346294403, -0.0027806281577795744, 0.009835077449679375, 0.01941744051873684, -0.05698336660861969, -0.050277870148420334, 0.04439232498407364, 0.005948599427938461, -0.006628627423197031, 0.009520167484879494, 0.01869002915918827, 0.04296766594052315, 0.010151171125471592, -0.0008630764787085354, -0.02182086743414402, -0.016031553968787193, -0.03582124039530754, -0.023083772510290146, -0.022996805608272552, 0.02709162048995495, -0.005366003140807152, -0.030474884435534477, -0.02775641717016697]
[2025-05-12 22:31:18,849]: Mean: -0.00017700
[2025-05-12 22:31:18,849]: Min: -0.08577595
[2025-05-12 22:31:18,849]: Max: 0.08595664
[2025-05-12 22:31:18,849]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,850]: Sample Values (25 elements): [1.009803056716919, 1.0142107009887695, 0.9641691446304321, 0.9656519293785095, 1.0190297365188599, 0.9948314428329468, 1.0024781227111816, 1.0203337669372559, 0.9798423647880554, 0.9775859713554382, 0.9954625964164734, 0.9931541085243225, 0.9947537779808044, 1.0019737482070923, 0.9756056666374207, 0.9863012433052063, 0.9965149760246277, 0.97939532995224, 0.9737910628318787, 1.0198591947555542, 0.9948953986167908, 0.987885057926178, 0.983704686164856, 0.9902055859565735, 0.9967018365859985]
[2025-05-12 22:31:18,850]: Mean: 0.98889554
[2025-05-12 22:31:18,850]: Min: 0.95837027
[2025-05-12 22:31:18,850]: Max: 1.02973032
[2025-05-12 22:31:18,850]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 22:31:18,851]: Sample Values (25 elements): [-0.03826340287923813, -0.048069197684526443, 0.005188916809856892, 0.04084596410393715, 0.002103493083268404, -0.03695128858089447, 0.0010037943720817566, 0.07292251288890839, 0.03605160489678383, 0.0022991718724370003, 0.023199889808893204, -0.01979818008840084, 0.0062511689029634, -0.016178391873836517, 0.0033328626304864883, 0.024819564074277878, 0.014552824199199677, 0.026705680415034294, -0.03811929374933243, 0.005391099490225315, -0.015188171528279781, -0.021015925332903862, 0.04170791432261467, -0.04709333926439285, -0.019053252413868904]
[2025-05-12 22:31:18,851]: Mean: 0.00007270
[2025-05-12 22:31:18,851]: Min: -0.09016398
[2025-05-12 22:31:18,852]: Max: 0.08210816
[2025-05-12 22:31:18,852]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,852]: Sample Values (25 elements): [0.9653333425521851, 0.9722725749015808, 0.9880188703536987, 0.9838840365409851, 0.9713433384895325, 0.9822141528129578, 0.9789790511131287, 0.9714230895042419, 0.9814484715461731, 0.9949054718017578, 0.9763175249099731, 0.9660347104072571, 0.968437135219574, 0.9732280373573303, 0.9793249368667603, 0.9875707030296326, 0.9925931096076965, 0.988968014717102, 0.9786654114723206, 0.96989905834198, 0.9670418500900269, 0.9777694940567017, 0.9839270114898682, 0.9840355515480042, 0.978783130645752]
[2025-05-12 22:31:18,852]: Mean: 0.97936714
[2025-05-12 22:31:18,853]: Min: 0.96496636
[2025-05-12 22:31:18,853]: Max: 1.00273442
[2025-05-12 22:31:18,853]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 22:31:18,854]: Sample Values (25 elements): [-0.035476237535476685, 0.026388395577669144, -0.028908634558320045, 0.02053229697048664, -0.004622401669621468, 0.020802482962608337, 0.004516199696809053, -0.02660440281033516, -0.00983613170683384, -0.021826012060046196, -0.021335216239094734, 0.02822505310177803, 0.015190575271844864, -0.029731733724474907, 0.025140533223748207, -0.003220954444259405, 0.00489500118419528, -0.008058193139731884, 0.021536575630307198, -0.03200991079211235, -0.035944852977991104, -0.027082929387688637, 0.011162607930600643, 0.015790842473506927, -0.034400004893541336]
[2025-05-12 22:31:18,854]: Mean: 0.00004606
[2025-05-12 22:31:18,855]: Min: -0.07214292
[2025-05-12 22:31:18,856]: Max: 0.07342739
[2025-05-12 22:31:18,856]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 22:31:18,859]: Sample Values (25 elements): [1.0283722877502441, 1.018001675605774, 1.0311037302017212, 1.0602152347564697, 1.0338943004608154, 1.0349090099334717, 1.0257461071014404, 1.0344618558883667, 1.077282190322876, 1.0386825799942017, 1.0285000801086426, 1.0373051166534424, 1.0069389343261719, 1.058762788772583, 1.063431978225708, 1.0253170728683472, 1.0335421562194824, 1.0491366386413574, 1.0163061618804932, 1.049845576286316, 1.0350011587142944, 1.0485849380493164, 1.0319863557815552, 1.0257867574691772, 1.0170211791992188]
[2025-05-12 22:31:18,861]: Mean: 1.03698134
[2025-05-12 22:31:18,862]: Min: 1.00693893
[2025-05-12 22:31:18,863]: Max: 1.07728219
[2025-05-12 22:31:18,863]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 22:31:18,864]: Sample Values (25 elements): [0.176984965801239, -0.12890876829624176, 0.23709946870803833, -0.06095368042588234, -0.23326751589775085, 0.1462266743183136, 0.11196581274271011, -0.012959803454577923, 0.007274776231497526, 0.35626816749572754, 0.20146223902702332, 0.19032174348831177, 0.05664018914103508, -0.05679929256439209, 0.1076287254691124, 0.19461095333099365, 0.15190744400024414, -0.25379636883735657, 0.10897237807512283, 0.2803632915019989, 0.2729414403438568, -0.07795754820108414, -0.15640145540237427, -0.23489652574062347, 0.3160480558872223]
[2025-05-12 22:31:18,864]: Mean: 0.00519985
[2025-05-12 22:31:18,866]: Min: -0.35514390
[2025-05-12 22:31:18,866]: Max: 0.41591278
[2025-05-12 22:31:18,866]: 


QAT of ResNet20 with parametrized_hardtanh down to 4 bits...
[2025-05-12 22:31:19,339]: [ResNet20_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-05-12 22:31:19,601]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 22:34:15,951]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.6021 Train Acc: 0.7881 Eval Loss: 0.6952 Eval Acc: 0.7694 (LR: 0.001000)
[2025-05-12 22:37:03,158]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.6046 Train Acc: 0.7880 Eval Loss: 0.7024 Eval Acc: 0.7642 (LR: 0.001000)
[2025-05-12 22:39:12,210]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.6010 Train Acc: 0.7894 Eval Loss: 0.6657 Eval Acc: 0.7808 (LR: 0.001000)
[2025-05-12 22:41:26,757]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.6070 Train Acc: 0.7873 Eval Loss: 0.6907 Eval Acc: 0.7730 (LR: 0.001000)
[2025-05-12 22:44:14,963]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.5988 Train Acc: 0.7895 Eval Loss: 0.6763 Eval Acc: 0.7755 (LR: 0.001000)
[2025-05-12 22:46:15,607]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.5916 Train Acc: 0.7937 Eval Loss: 0.6959 Eval Acc: 0.7657 (LR: 0.001000)
[2025-05-12 22:48:39,610]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.5912 Train Acc: 0.7913 Eval Loss: 0.6899 Eval Acc: 0.7674 (LR: 0.001000)
[2025-05-12 22:51:00,305]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.5893 Train Acc: 0.7920 Eval Loss: 0.7186 Eval Acc: 0.7698 (LR: 0.001000)
[2025-05-12 22:53:56,995]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.5800 Train Acc: 0.7975 Eval Loss: 0.6947 Eval Acc: 0.7684 (LR: 0.001000)
[2025-05-12 22:56:57,007]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.5767 Train Acc: 0.7978 Eval Loss: 0.6957 Eval Acc: 0.7738 (LR: 0.001000)
[2025-05-12 22:59:54,545]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.5784 Train Acc: 0.7982 Eval Loss: 0.6526 Eval Acc: 0.7790 (LR: 0.001000)
[2025-05-12 23:02:56,445]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.5644 Train Acc: 0.8040 Eval Loss: 0.6502 Eval Acc: 0.7808 (LR: 0.001000)
[2025-05-12 23:05:56,109]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.5725 Train Acc: 0.7992 Eval Loss: 0.7149 Eval Acc: 0.7716 (LR: 0.001000)
[2025-05-12 23:08:22,039]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.5719 Train Acc: 0.8002 Eval Loss: 0.6657 Eval Acc: 0.7794 (LR: 0.001000)
[2025-05-12 23:11:19,115]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.5626 Train Acc: 0.8030 Eval Loss: 0.6374 Eval Acc: 0.7833 (LR: 0.001000)
[2025-05-12 23:14:04,307]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.5632 Train Acc: 0.8004 Eval Loss: 0.6428 Eval Acc: 0.7853 (LR: 0.001000)
[2025-05-12 23:17:05,448]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.5587 Train Acc: 0.8036 Eval Loss: 0.6400 Eval Acc: 0.7875 (LR: 0.001000)
[2025-05-12 23:20:04,600]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.5503 Train Acc: 0.8063 Eval Loss: 0.6381 Eval Acc: 0.7887 (LR: 0.001000)
[2025-05-12 23:23:21,697]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.5592 Train Acc: 0.8041 Eval Loss: 0.6581 Eval Acc: 0.7778 (LR: 0.001000)
[2025-05-12 23:25:35,187]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.5515 Train Acc: 0.8051 Eval Loss: 0.6570 Eval Acc: 0.7793 (LR: 0.001000)
[2025-05-12 23:28:37,656]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.5478 Train Acc: 0.8081 Eval Loss: 0.6501 Eval Acc: 0.7841 (LR: 0.001000)
[2025-05-12 23:31:44,587]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.5400 Train Acc: 0.8114 Eval Loss: 0.6527 Eval Acc: 0.7813 (LR: 0.001000)
[2025-05-12 23:33:42,310]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.5394 Train Acc: 0.8096 Eval Loss: 0.6669 Eval Acc: 0.7795 (LR: 0.001000)
[2025-05-12 23:36:21,357]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.5366 Train Acc: 0.8107 Eval Loss: 0.6477 Eval Acc: 0.7869 (LR: 0.001000)
[2025-05-12 23:38:47,177]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.5286 Train Acc: 0.8135 Eval Loss: 0.5960 Eval Acc: 0.7931 (LR: 0.001000)
[2025-05-12 23:41:36,807]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.5315 Train Acc: 0.8148 Eval Loss: 0.6327 Eval Acc: 0.7878 (LR: 0.001000)
[2025-05-12 23:44:41,680]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.5221 Train Acc: 0.8195 Eval Loss: 0.6131 Eval Acc: 0.7934 (LR: 0.001000)
[2025-05-12 23:46:59,431]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.5221 Train Acc: 0.8178 Eval Loss: 0.6160 Eval Acc: 0.7964 (LR: 0.001000)
[2025-05-12 23:48:09,423]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.5204 Train Acc: 0.8173 Eval Loss: 0.7157 Eval Acc: 0.7765 (LR: 0.001000)
[2025-05-12 23:49:48,213]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.5206 Train Acc: 0.8174 Eval Loss: 0.6184 Eval Acc: 0.7998 (LR: 0.000250)
[2025-05-12 23:52:39,824]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.4600 Train Acc: 0.8396 Eval Loss: 0.5429 Eval Acc: 0.8183 (LR: 0.000250)
[2025-05-12 23:56:02,470]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.4558 Train Acc: 0.8413 Eval Loss: 0.5497 Eval Acc: 0.8166 (LR: 0.000250)
[2025-05-12 23:59:30,967]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.4493 Train Acc: 0.8438 Eval Loss: 0.5647 Eval Acc: 0.8125 (LR: 0.000250)
[2025-05-13 00:02:45,598]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.4542 Train Acc: 0.8413 Eval Loss: 0.5453 Eval Acc: 0.8171 (LR: 0.000250)
[2025-05-13 00:06:03,230]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.4436 Train Acc: 0.8455 Eval Loss: 0.5415 Eval Acc: 0.8203 (LR: 0.000250)
[2025-05-13 00:09:13,022]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.4488 Train Acc: 0.8440 Eval Loss: 0.5388 Eval Acc: 0.8211 (LR: 0.000250)
[2025-05-13 00:11:58,094]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.4479 Train Acc: 0.8433 Eval Loss: 0.5544 Eval Acc: 0.8142 (LR: 0.000250)
[2025-05-13 00:14:46,731]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.4433 Train Acc: 0.8433 Eval Loss: 0.5690 Eval Acc: 0.8138 (LR: 0.000250)
[2025-05-13 00:17:56,779]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.4421 Train Acc: 0.8454 Eval Loss: 0.5867 Eval Acc: 0.8119 (LR: 0.000250)
[2025-05-13 00:20:57,737]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.4483 Train Acc: 0.8433 Eval Loss: 0.6088 Eval Acc: 0.8064 (LR: 0.000250)
[2025-05-13 00:23:35,190]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.4404 Train Acc: 0.8453 Eval Loss: 0.5555 Eval Acc: 0.8176 (LR: 0.000250)
[2025-05-13 00:26:33,787]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.4453 Train Acc: 0.8447 Eval Loss: 0.5725 Eval Acc: 0.8106 (LR: 0.000250)
[2025-05-13 00:29:01,892]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.4411 Train Acc: 0.8452 Eval Loss: 0.5574 Eval Acc: 0.8202 (LR: 0.000250)
[2025-05-13 00:32:08,983]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.4367 Train Acc: 0.8483 Eval Loss: 0.5311 Eval Acc: 0.8263 (LR: 0.000250)
[2025-05-13 00:34:12,712]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.4370 Train Acc: 0.8482 Eval Loss: 0.5775 Eval Acc: 0.8147 (LR: 0.000063)
[2025-05-13 00:36:30,390]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.4208 Train Acc: 0.8514 Eval Loss: 0.5260 Eval Acc: 0.8267 (LR: 0.000063)
[2025-05-13 00:38:48,924]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.4150 Train Acc: 0.8551 Eval Loss: 0.5243 Eval Acc: 0.8271 (LR: 0.000063)
[2025-05-13 00:41:51,296]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.4172 Train Acc: 0.8525 Eval Loss: 0.5301 Eval Acc: 0.8277 (LR: 0.000063)
[2025-05-13 00:44:54,124]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.4176 Train Acc: 0.8534 Eval Loss: 0.5455 Eval Acc: 0.8265 (LR: 0.000063)
[2025-05-13 00:47:53,554]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.4131 Train Acc: 0.8558 Eval Loss: 0.6992 Eval Acc: 0.7755 (LR: 0.000063)
[2025-05-13 00:50:44,977]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.4187 Train Acc: 0.8532 Eval Loss: 0.5500 Eval Acc: 0.8198 (LR: 0.000063)
[2025-05-13 00:53:37,608]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.4164 Train Acc: 0.8533 Eval Loss: 0.5313 Eval Acc: 0.8282 (LR: 0.000063)
[2025-05-13 00:56:16,597]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.4170 Train Acc: 0.8542 Eval Loss: 0.5343 Eval Acc: 0.8270 (LR: 0.000063)
[2025-05-13 00:59:08,933]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.4188 Train Acc: 0.8529 Eval Loss: 0.5374 Eval Acc: 0.8258 (LR: 0.000063)
[2025-05-13 01:01:26,235]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.4180 Train Acc: 0.8527 Eval Loss: 0.5362 Eval Acc: 0.8250 (LR: 0.000063)
[2025-05-13 01:04:02,760]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.4179 Train Acc: 0.8540 Eval Loss: 0.6016 Eval Acc: 0.8023 (LR: 0.000063)
[2025-05-13 01:06:37,529]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.4142 Train Acc: 0.8550 Eval Loss: 0.5607 Eval Acc: 0.8195 (LR: 0.000063)
[2025-05-13 01:09:08,026]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.4148 Train Acc: 0.8536 Eval Loss: 0.6032 Eval Acc: 0.8067 (LR: 0.000063)
[2025-05-13 01:10:26,401]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.4164 Train Acc: 0.8556 Eval Loss: 0.5512 Eval Acc: 0.8196 (LR: 0.000063)
[2025-05-13 01:11:45,634]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.4142 Train Acc: 0.8567 Eval Loss: 0.5939 Eval Acc: 0.8109 (LR: 0.000063)
[2025-05-13 01:11:45,635]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8282
[2025-05-13 01:11:45,707]: 


Quantization of model down to 4 bits finished
[2025-05-13 01:11:45,707]: Model Architecture:
[2025-05-13 01:11:45,799]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1650], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3014694452285767, max_val=1.174231767654419)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0506], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32872527837753296, max_val=0.4297131299972534)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1284], device='cuda:0'), zero_point=tensor([12], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.604435682296753, max_val=0.32089143991470337)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0396], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3369560241699219, max_val=0.25725895166397095)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1372], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0288190841674805, max_val=1.0288190841674805)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0399], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26858091354370117, max_val=0.32976776361465454)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1152], device='cuda:0'), zero_point=tensor([11], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3252860307693481, max_val=0.40334975719451904)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0384], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26121607422828674, max_val=0.3142268657684326)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1616], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7271149754524231, max_val=1.6968812942504883)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0332], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22519895434379578, max_val=0.2721706032752991)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1252], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8141036629676819, max_val=1.0646440982818604)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0271], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20987583696842194, max_val=0.19661195576190948)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2024], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.720552921295166, max_val=1.3159257173538208)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0244], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19529332220554352, max_val=0.17007483541965485)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1464], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8079637289047241, max_val=1.3875173330307007)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0186], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13573624193668365, max_val=0.14383737742900848)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.34129011631011963, max_val=0.32681581377983093)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1589], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1919904947280884, max_val=1.1919535398483276)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0204], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15915976464748383, max_val=0.14731483161449432)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1193], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7753120064735413, max_val=1.0138341188430786)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0199], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14588865637779236, max_val=0.15246553719043732)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1436], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.220353603363037, max_val=0.9331714510917664)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0168], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12459870427846909, max_val=0.12759628891944885)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1210], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6655454039573669, max_val=1.1495798826217651)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0169], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12093012779951096, max_val=0.13232457637786865)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2012], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1912696361541748, max_val=1.8264703750610352)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12830780446529388, max_val=0.12937982380390167)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1154], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4038983881473541, max_val=1.3270058631896973)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10546978563070297, max_val=0.10546444356441498)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0319], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23164500296115875, max_val=0.24675744771957397)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1384], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8994442224502563, max_val=1.1762343645095825)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0145], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10445016622543335, max_val=0.11332382261753082)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0973], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9248248338699341, max_val=0.5354233384132385)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0145], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1162518858909607, max_val=0.10142893344163895)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1267], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2036577463150024, max_val=0.6968592405319214)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0129], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09747602790594101, max_val=0.09552717208862305)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0812], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5280623435974121, max_val=0.6906107664108276)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0105], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07861506938934326, max_val=0.07815653085708618)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3250], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.413724184036255, max_val=2.4617178440093994)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-13 01:11:45,800]: 
Model Weights:
[2025-05-13 01:11:45,800]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-13 01:11:45,801]: Sample Values (25 elements): [-0.09581167995929718, -0.06391538679599762, -0.01915784552693367, -0.134750097990036, -0.26062920689582825, -0.06361336261034012, 0.3704417645931244, 0.048904113471508026, 0.21051880717277527, -0.04130146652460098, -0.0777658224105835, -0.12937791645526886, -0.1049141064286232, 0.12015581876039505, -0.0050214240327477455, 0.3509090542793274, 0.4186139702796936, -0.038245171308517456, -0.18814945220947266, -0.07799435406923294, 0.36671239137649536, 0.3519030511379242, 0.13815784454345703, 0.07540000230073929, -0.314081072807312]
[2025-05-13 01:11:45,801]: Mean: 0.00590760
[2025-05-13 01:11:45,802]: Min: -0.55778122
[2025-05-13 01:11:45,802]: Max: 0.50127441
[2025-05-13 01:11:45,802]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,802]: Sample Values (16 elements): [0.7088935971260071, 1.0028616189956665, 0.8630473017692566, 0.8343728184700012, 0.7493410110473633, 0.9581642150878906, 0.9455917477607727, 0.7332524657249451, 0.6519104838371277, 0.8557664155960083, 0.7198933959007263, 1.1501384973526, 0.9905135631561279, 0.8661397695541382, 1.1059340238571167, 1.1093366146087646]
[2025-05-13 01:11:45,802]: Mean: 0.89032233
[2025-05-13 01:11:45,802]: Min: 0.65191048
[2025-05-13 01:11:45,803]: Max: 1.15013850
[2025-05-13 01:11:45,804]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 01:11:45,804]: Sample Values (25 elements): [0.10112497210502625, 0.15168745815753937, 0.05056248605251312, -0.10112497210502625, 0.0, -0.10112497210502625, 0.05056248605251312, -0.05056248605251312, 0.0, 0.05056248605251312, 0.0, 0.0, -0.05056248605251312, 0.05056248605251312, 0.10112497210502625, 0.10112497210502625, -0.05056248605251312, 0.05056248605251312, 0.0, 0.0, 0.0, 0.05056248605251312, -0.05056248605251312, -0.05056248605251312, -0.05056248605251312]
[2025-05-13 01:11:45,804]: Mean: 0.00140451
[2025-05-13 01:11:45,804]: Min: -0.35393739
[2025-05-13 01:11:45,805]: Max: 0.40449989
[2025-05-13 01:11:45,805]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,805]: Sample Values (16 elements): [0.8316200971603394, 0.9442178010940552, 1.0995573997497559, 0.7648574709892273, 0.9850892424583435, 0.9856109023094177, 0.7717629671096802, 1.1510378122329712, 0.7799735069274902, 0.9855339527130127, 0.9861930012702942, 0.7970587611198425, 1.0981948375701904, 0.7478337287902832, 0.9006524085998535, 0.7386705875396729]
[2025-05-13 01:11:45,805]: Mean: 0.91049153
[2025-05-13 01:11:45,805]: Min: 0.73867059
[2025-05-13 01:11:45,806]: Max: 1.15103781
[2025-05-13 01:11:45,807]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 01:11:45,807]: Sample Values (25 elements): [-0.039614368230104446, -0.07922873646020889, -0.07922873646020889, -0.11884310841560364, 0.0, 0.039614368230104446, 0.0, -0.039614368230104446, -0.039614368230104446, -0.039614368230104446, 0.11884310841560364, 0.15845747292041779, 0.0, 0.0, 0.07922873646020889, 0.0, -0.07922873646020889, -0.07922873646020889, -0.07922873646020889, 0.039614368230104446, 0.039614368230104446, -0.07922873646020889, 0.07922873646020889, 0.0, -0.07922873646020889]
[2025-05-13 01:11:45,807]: Mean: -0.00137550
[2025-05-13 01:11:45,807]: Min: -0.35652933
[2025-05-13 01:11:45,808]: Max: 0.23768622
[2025-05-13 01:11:45,808]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,808]: Sample Values (16 elements): [0.7179915308952332, 0.8434551954269409, 0.9441356062889099, 0.7464898228645325, 1.0546772480010986, 0.9201319813728333, 0.6866670846939087, 0.7759435772895813, 0.8273056745529175, 0.9316896796226501, 0.946867048740387, 0.6883183717727661, 0.7652992010116577, 0.8314834237098694, 0.8544536828994751, 0.8298580646514893]
[2025-05-13 01:11:45,808]: Mean: 0.83529794
[2025-05-13 01:11:45,809]: Min: 0.68666708
[2025-05-13 01:11:45,809]: Max: 1.05467725
[2025-05-13 01:11:45,810]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 01:11:45,810]: Sample Values (25 elements): [0.0, -0.0797797366976738, 0.1595594733953476, 0.0797797366976738, -0.1196696013212204, -0.0797797366976738, -0.1196696013212204, 0.0797797366976738, -0.0797797366976738, 0.0797797366976738, 0.0398898683488369, -0.0797797366976738, -0.0797797366976738, -0.0797797366976738, -0.0398898683488369, 0.0797797366976738, 0.0, -0.0797797366976738, -0.0797797366976738, -0.0398898683488369, 0.0398898683488369, -0.0398898683488369, -0.0797797366976738, -0.1595594733953476, 0.0]
[2025-05-13 01:11:45,811]: Mean: 0.00228536
[2025-05-13 01:11:45,811]: Min: -0.27922907
[2025-05-13 01:11:45,811]: Max: 0.31911895
[2025-05-13 01:11:45,811]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,812]: Sample Values (16 elements): [1.0320158004760742, 0.7338951826095581, 1.0422004461288452, 0.8752610087394714, 0.8034970164299011, 0.8766974210739136, 0.7636436223983765, 1.002411127090454, 1.0270835161209106, 0.9549148678779602, 0.9356608986854553, 0.9508858919143677, 1.0257197618484497, 1.0419665575027466, 1.06759774684906, 0.9498658180236816]
[2025-05-13 01:11:45,812]: Mean: 0.94270730
[2025-05-13 01:11:45,812]: Min: 0.73389518
[2025-05-13 01:11:45,812]: Max: 1.06759775
[2025-05-13 01:11:45,814]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 01:11:45,816]: Sample Values (25 elements): [-0.07672552019357681, -0.038362760096788406, -0.11508828401565552, -0.15345104038715363, 0.07672552019357681, 0.0, 0.0, -0.11508828401565552, -0.038362760096788406, -0.038362760096788406, 0.11508828401565552, -0.038362760096788406, 0.0, 0.038362760096788406, 0.038362760096788406, 0.038362760096788406, 0.038362760096788406, 0.11508828401565552, 0.038362760096788406, 0.038362760096788406, 0.07672552019357681, -0.038362760096788406, 0.07672552019357681, 0.07672552019357681, -0.038362760096788406]
[2025-05-13 01:11:45,818]: Mean: -0.00164840
[2025-05-13 01:11:45,818]: Min: -0.26853931
[2025-05-13 01:11:45,819]: Max: 0.30690208
[2025-05-13 01:11:45,820]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,821]: Sample Values (16 elements): [0.7759807705879211, 0.8457947969436646, 0.8955977559089661, 0.8605121374130249, 0.9346559643745422, 0.9599847197532654, 0.741479754447937, 0.9229096174240112, 1.0135631561279297, 1.0186946392059326, 0.8790771961212158, 0.8739606738090515, 1.0392414331436157, 0.9318248629570007, 1.0189260244369507, 0.9117056727409363]
[2025-05-13 01:11:45,821]: Mean: 0.91399431
[2025-05-13 01:11:45,821]: Min: 0.74147975
[2025-05-13 01:11:45,821]: Max: 1.03924143
[2025-05-13 01:11:45,823]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 01:11:45,823]: Sample Values (25 elements): [0.09947411715984344, 0.033158037811517715, 0.0, -0.033158037811517715, -0.033158037811517715, -0.06631607562303543, 0.0, 0.06631607562303543, 0.06631607562303543, 0.0, -0.09947411715984344, -0.033158037811517715, 0.0, -0.06631607562303543, 0.06631607562303543, 0.033158037811517715, 0.033158037811517715, 0.0, -0.06631607562303543, 0.0, -0.033158037811517715, 0.13263215124607086, -0.06631607562303543, -0.033158037811517715, -0.033158037811517715]
[2025-05-13 01:11:45,823]: Mean: 0.00138159
[2025-05-13 01:11:45,823]: Min: -0.23210627
[2025-05-13 01:11:45,823]: Max: 0.26526430
[2025-05-13 01:11:45,823]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,824]: Sample Values (16 elements): [0.9064404964447021, 0.9937421083450317, 0.8520864844322205, 0.9836638569831848, 0.8565658926963806, 0.9003878235816956, 0.8940187692642212, 0.9902858138084412, 1.100562572479248, 0.9808509945869446, 0.937322199344635, 0.8908994793891907, 0.9522444009780884, 0.955589771270752, 0.9755806922912598, 0.9851951003074646]
[2025-05-13 01:11:45,824]: Mean: 0.94721478
[2025-05-13 01:11:45,824]: Min: 0.85208648
[2025-05-13 01:11:45,824]: Max: 1.10056257
[2025-05-13 01:11:45,825]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 01:11:45,825]: Sample Values (25 elements): [0.0812973901629448, 0.0, 0.0, 0.0812973901629448, 0.027099130675196648, -0.1354956477880478, -0.027099130675196648, -0.0812973901629448, -0.027099130675196648, -0.0812973901629448, -0.027099130675196648, 0.0, 0.054198261350393295, -0.027099130675196648, -0.027099130675196648, -0.054198261350393295, 0.0, -0.054198261350393295, -0.0812973901629448, 0.054198261350393295, -0.054198261350393295, 0.0, 0.0812973901629448, -0.1354956477880478, 0.027099130675196648]
[2025-05-13 01:11:45,826]: Mean: -0.00344620
[2025-05-13 01:11:45,826]: Min: -0.21679305
[2025-05-13 01:11:45,826]: Max: 0.18969391
[2025-05-13 01:11:45,826]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-13 01:11:45,826]: Sample Values (16 elements): [0.9634582996368408, 0.8890103101730347, 0.8930777311325073, 1.04409921169281, 0.9760769605636597, 0.91535484790802, 0.8499346971511841, 0.907992422580719, 0.9742674827575684, 0.8317337036132812, 0.993249237537384, 0.8756884932518005, 0.8952339887619019, 0.9976246356964111, 0.9346553087234497, 0.9125460982322693]
[2025-05-13 01:11:45,826]: Mean: 0.92837524
[2025-05-13 01:11:45,827]: Min: 0.83173370
[2025-05-13 01:11:45,827]: Max: 1.04409921
[2025-05-13 01:11:45,828]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-13 01:11:45,828]: Sample Values (25 elements): [0.0, 0.07307367026805878, 0.048715777695178986, 0.024357888847589493, 0.048715777695178986, 0.0, -0.024357888847589493, -0.024357888847589493, -0.024357888847589493, 0.024357888847589493, 0.0, 0.024357888847589493, 0.048715777695178986, -0.024357888847589493, 0.048715777695178986, -0.07307367026805878, 0.048715777695178986, -0.024357888847589493, -0.09743155539035797, 0.0, -0.024357888847589493, -0.048715777695178986, 0.024357888847589493, -0.048715777695178986, 0.0]
[2025-05-13 01:11:45,828]: Mean: -0.00252671
[2025-05-13 01:11:45,828]: Min: -0.19486311
[2025-05-13 01:11:45,829]: Max: 0.17050523
[2025-05-13 01:11:45,829]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,829]: Sample Values (25 elements): [0.9615128040313721, 0.9298028945922852, 0.9486917853355408, 0.9611360430717468, 0.9003782868385315, 0.933716893196106, 0.9207663536071777, 0.9511342644691467, 0.9616744518280029, 0.9339985251426697, 0.947135865688324, 0.9732249975204468, 0.9555492401123047, 0.9776909351348877, 0.9495344161987305, 0.9390595555305481, 0.9367412328720093, 0.9781577587127686, 0.9800569415092468, 0.9160513877868652, 0.9636678695678711, 0.9502403140068054, 0.975622296333313, 0.9421232342720032, 0.9020981788635254]
[2025-05-13 01:11:45,829]: Mean: 0.94628936
[2025-05-13 01:11:45,829]: Min: 0.89402550
[2025-05-13 01:11:45,829]: Max: 0.98005694
[2025-05-13 01:11:45,830]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 01:11:45,831]: Sample Values (25 elements): [0.03727640211582184, 0.03727640211582184, -0.11182920634746552, -0.07455280423164368, 0.0, 0.01863820105791092, 0.0, 0.05591460317373276, 0.01863820105791092, -0.03727640211582184, -0.01863820105791092, 0.05591460317373276, 0.01863820105791092, -0.01863820105791092, 0.01863820105791092, 0.0, 0.0, -0.0931910052895546, -0.03727640211582184, -0.05591460317373276, 0.07455280423164368, 0.01863820105791092, 0.03727640211582184, 0.0, 0.0]
[2025-05-13 01:11:45,831]: Mean: 0.00009303
[2025-05-13 01:11:45,831]: Min: -0.13046741
[2025-05-13 01:11:45,831]: Max: 0.14910561
[2025-05-13 01:11:45,831]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,832]: Sample Values (25 elements): [0.9134719371795654, 0.9590703845024109, 0.9555856585502625, 0.9277227520942688, 0.9840285778045654, 0.9395000338554382, 0.924022376537323, 0.9652529954910278, 0.9417547583580017, 1.0027748346328735, 0.9661067724227905, 1.0133408308029175, 0.9297887682914734, 1.0493361949920654, 0.9171566963195801, 0.9515160918235779, 0.9444496035575867, 0.9609528183937073, 0.9784297347068787, 0.9591389298439026, 0.9844600558280945, 0.9546035528182983, 0.9169648289680481, 0.9726372957229614, 1.0075064897537231]
[2025-05-13 01:11:45,832]: Mean: 0.95791954
[2025-05-13 01:11:45,832]: Min: 0.90055090
[2025-05-13 01:11:45,832]: Max: 1.04933619
[2025-05-13 01:11:45,833]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-13 01:11:45,833]: Sample Values (25 elements): [-0.2672423720359802, 0.1336211860179901, -0.1336211860179901, -0.2672423720359802, -0.1336211860179901, -0.17816157639026642, -0.1336211860179901, 0.08908078819513321, 0.08908078819513321, -0.044540394097566605, 0.1336211860179901, -0.1336211860179901, 0.044540394097566605, 0.1336211860179901, 0.22270196676254272, 0.0, -0.08908078819513321, 0.22270196676254272, 0.08908078819513321, 0.08908078819513321, 0.17816157639026642, -0.08908078819513321, -0.044540394097566605, 0.0, 0.08908078819513321]
[2025-05-13 01:11:45,834]: Mean: -0.00826433
[2025-05-13 01:11:45,834]: Min: -0.35632315
[2025-05-13 01:11:45,834]: Max: 0.31178275
[2025-05-13 01:11:45,834]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,834]: Sample Values (25 elements): [0.8729209303855896, 0.8419315218925476, 0.8525853753089905, 0.8371368050575256, 0.8921833634376526, 0.8367091417312622, 0.8212482333183289, 0.8492552638053894, 0.8071070313453674, 0.8363279104232788, 0.8936144113540649, 0.8577213883399963, 0.8037821650505066, 0.909673273563385, 0.7956616282463074, 0.8298452496528625, 0.8220536112785339, 0.8239160180091858, 0.8674318194389343, 0.8738855719566345, 0.8700456023216248, 0.8524514436721802, 0.8065745234489441, 0.8600361347198486, 0.8749145865440369]
[2025-05-13 01:11:45,834]: Mean: 0.84811640
[2025-05-13 01:11:45,835]: Min: 0.79566163
[2025-05-13 01:11:45,835]: Max: 0.93284059
[2025-05-13 01:11:45,837]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 01:11:45,837]: Sample Values (25 elements): [0.0, 0.04086335375905037, 0.020431676879525185, -0.020431676879525185, 0.04086335375905037, 0.04086335375905037, 0.08172670751810074, 0.0, -0.020431676879525185, -0.0612950325012207, -0.04086335375905037, -0.0612950325012207, 0.04086335375905037, -0.0612950325012207, -0.10215838253498077, 0.08172670751810074, 0.0, 0.0612950325012207, 0.08172670751810074, 0.04086335375905037, -0.0612950325012207, 0.020431676879525185, -0.04086335375905037, -0.04086335375905037, 0.04086335375905037]
[2025-05-13 01:11:45,838]: Mean: 0.00062519
[2025-05-13 01:11:45,838]: Min: -0.16345342
[2025-05-13 01:11:45,838]: Max: 0.14302173
[2025-05-13 01:11:45,838]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,839]: Sample Values (25 elements): [0.9803062081336975, 0.9260129332542419, 0.9652521014213562, 0.9514265656471252, 0.9986358880996704, 0.9461469650268555, 0.9230524301528931, 0.9465904235839844, 0.9752745628356934, 0.9675341248512268, 0.9426838159561157, 0.9609866738319397, 0.9561975598335266, 0.9830868244171143, 0.9601646661758423, 0.981819212436676, 0.9306914210319519, 0.9417771100997925, 0.9536879658699036, 0.9596890807151794, 0.9482395052909851, 0.9705077409744263, 0.9675996899604797, 0.9954056739807129, 1.0190801620483398]
[2025-05-13 01:11:45,839]: Mean: 0.95996070
[2025-05-13 01:11:45,839]: Min: 0.90683728
[2025-05-13 01:11:45,839]: Max: 1.01908016
[2025-05-13 01:11:45,841]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 01:11:45,843]: Sample Values (25 elements): [-0.019890300929546356, 0.0, -0.019890300929546356, -0.07956120371818542, -0.05967090278863907, 0.019890300929546356, 0.09945150464773178, -0.05967090278863907, -0.019890300929546356, -0.019890300929546356, 0.03978060185909271, -0.019890300929546356, -0.03978060185909271, -0.03978060185909271, 0.0, 0.03978060185909271, -0.019890300929546356, -0.019890300929546356, 0.03978060185909271, -0.05967090278863907, -0.05967090278863907, -0.019890300929546356, -0.03978060185909271, -0.03978060185909271, -0.09945150464773178]
[2025-05-13 01:11:45,844]: Mean: 0.00091078
[2025-05-13 01:11:45,845]: Min: -0.13923210
[2025-05-13 01:11:45,846]: Max: 0.15912241
[2025-05-13 01:11:45,846]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,847]: Sample Values (25 elements): [0.9257048964500427, 1.0080080032348633, 1.0514270067214966, 0.9409774541854858, 0.9803833365440369, 0.98738694190979, 0.973980724811554, 0.9400103688240051, 0.9133425951004028, 0.9175553321838379, 0.9698988795280457, 0.9735597372055054, 0.9208332300186157, 0.9671604633331299, 0.9850785136222839, 0.9839900732040405, 0.9630361199378967, 0.9672827124595642, 0.9782333970069885, 0.9877381920814514, 0.9265186786651611, 0.9742493629455566, 0.9309056997299194, 1.0044351816177368, 0.9215750694274902]
[2025-05-13 01:11:45,848]: Mean: 0.95741808
[2025-05-13 01:11:45,848]: Min: 0.91220433
[2025-05-13 01:11:45,848]: Max: 1.05142701
[2025-05-13 01:11:45,849]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 01:11:45,849]: Sample Values (25 elements): [-0.06725221127271652, -0.03362610563635826, 0.01681305281817913, 0.03362610563635826, 0.01681305281817913, 0.06725221127271652, 0.05043915659189224, 0.0840652659535408, -0.01681305281817913, 0.01681305281817913, 0.01681305281817913, 0.0, 0.0, 0.01681305281817913, -0.03362610563635826, -0.03362610563635826, 0.01681305281817913, -0.01681305281817913, 0.06725221127271652, 0.01681305281817913, 0.01681305281817913, -0.05043915659189224, -0.05043915659189224, -0.06725221127271652, -0.01681305281817913]
[2025-05-13 01:11:45,850]: Mean: -0.00003284
[2025-05-13 01:11:45,850]: Min: -0.11769137
[2025-05-13 01:11:45,850]: Max: 0.13450442
[2025-05-13 01:11:45,850]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,850]: Sample Values (25 elements): [0.9525018334388733, 0.9350120425224304, 0.9214527606964111, 0.9434595704078674, 0.9535372853279114, 0.9735580086708069, 0.978253960609436, 0.9617965817451477, 0.975473940372467, 0.9641549587249756, 0.9324682354927063, 0.942009687423706, 0.9311256408691406, 0.9537379741668701, 0.9643118977546692, 1.009764313697815, 0.978384256362915, 0.9662474393844604, 0.8928182125091553, 0.917079746723175, 0.9895450472831726, 0.9700546860694885, 0.9512715339660645, 0.9351043105125427, 0.9840465784072876]
[2025-05-13 01:11:45,850]: Mean: 0.95579511
[2025-05-13 01:11:45,851]: Min: 0.89281821
[2025-05-13 01:11:45,851]: Max: 1.00976431
[2025-05-13 01:11:45,852]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 01:11:45,852]: Sample Values (25 elements): [-0.03376718983054161, -0.06753437966108322, 0.03376718983054161, 0.016883594915270805, -0.05065078288316727, 0.06753437966108322, -0.03376718983054161, -0.05065078288316727, -0.03376718983054161, 0.05065078288316727, -0.016883594915270805, 0.03376718983054161, 0.03376718983054161, -0.016883594915270805, -0.016883594915270805, 0.016883594915270805, 0.016883594915270805, -0.03376718983054161, 0.0, 0.0, 0.08441797643899918, 0.05065078288316727, -0.05065078288316727, -0.08441797643899918, 0.05065078288316727]
[2025-05-13 01:11:45,852]: Mean: -0.00040853
[2025-05-13 01:11:45,852]: Min: -0.11818516
[2025-05-13 01:11:45,853]: Max: 0.13506876
[2025-05-13 01:11:45,853]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-13 01:11:45,853]: Sample Values (25 elements): [0.9673072099685669, 1.0000154972076416, 0.9911020994186401, 0.9627756476402283, 0.9877469539642334, 0.9771076440811157, 1.00629723072052, 0.9924496412277222, 0.9292543530464172, 0.9597476720809937, 1.0009053945541382, 0.9483014345169067, 1.023411750793457, 1.0070438385009766, 0.9213221073150635, 1.001815676689148, 0.9411531686782837, 0.9795354604721069, 0.9777263402938843, 0.9417047500610352, 1.0157216787338257, 0.9635867476463318, 1.0410782098770142, 1.0000698566436768, 0.9506458044052124]
[2025-05-13 01:11:45,853]: Mean: 0.98239768
[2025-05-13 01:11:45,853]: Min: 0.92132211
[2025-05-13 01:11:45,853]: Max: 1.04107821
[2025-05-13 01:11:45,855]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-13 01:11:45,855]: Sample Values (25 elements): [-0.034358397126197815, 0.06871679425239563, 0.034358397126197815, 0.034358397126197815, 0.0, 0.017179198563098907, -0.034358397126197815, 0.017179198563098907, 0.0, 0.017179198563098907, 0.05153759568929672, -0.034358397126197815, 0.05153759568929672, -0.06871679425239563, 0.034358397126197815, -0.017179198563098907, -0.06871679425239563, 0.05153759568929672, 0.034358397126197815, -0.06871679425239563, 0.034358397126197815, -0.034358397126197815, -0.034358397126197815, 0.08589599281549454, -0.017179198563098907]
[2025-05-13 01:11:45,855]: Mean: -0.00001771
[2025-05-13 01:11:45,856]: Min: -0.12025439
[2025-05-13 01:11:45,856]: Max: 0.13743359
[2025-05-13 01:11:45,856]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,856]: Sample Values (25 elements): [0.9463806748390198, 0.9501115679740906, 0.9594317674636841, 0.9607778191566467, 0.9559391140937805, 0.9489470720291138, 0.9585995078086853, 0.9683160781860352, 0.9764279127120972, 0.9396747946739197, 0.9421581625938416, 0.9454638957977295, 0.9424793720245361, 0.9640182852745056, 0.9410896301269531, 0.9438302516937256, 0.9460649490356445, 0.9516326189041138, 0.9577733874320984, 0.9563405513763428, 0.9519045948982239, 0.9401533603668213, 0.9750136137008667, 0.9588861465454102, 0.9814231991767883]
[2025-05-13 01:11:45,856]: Mean: 0.95617712
[2025-05-13 01:11:45,856]: Min: 0.91899401
[2025-05-13 01:11:45,857]: Max: 1.04635847
[2025-05-13 01:11:45,858]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 01:11:45,858]: Sample Values (25 elements): [-0.02812458947300911, 0.0, -0.014062294736504555, -0.02812458947300911, 0.042186886072158813, 0.014062294736504555, -0.02812458947300911, 0.014062294736504555, 0.014062294736504555, 0.0, -0.042186886072158813, 0.05624917894601822, 0.014062294736504555, -0.02812458947300911, 0.014062294736504555, 0.0, -0.02812458947300911, -0.014062294736504555, -0.02812458947300911, 0.014062294736504555, 0.02812458947300911, -0.02812458947300911, -0.02812458947300911, -0.02812458947300911, 0.02812458947300911]
[2025-05-13 01:11:45,858]: Mean: 0.00035781
[2025-05-13 01:11:45,859]: Min: -0.09843607
[2025-05-13 01:11:45,859]: Max: 0.09843607
[2025-05-13 01:11:45,859]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,859]: Sample Values (25 elements): [0.9801797866821289, 0.9291700124740601, 0.9624696969985962, 0.9642837643623352, 0.9909523129463196, 0.9610207676887512, 0.988979697227478, 0.9771137237548828, 0.9754207134246826, 0.9576130509376526, 0.9380723834037781, 0.9739962816238403, 0.9977180361747742, 0.9995386004447937, 0.9600697755813599, 0.9977237582206726, 0.944718599319458, 0.9454523324966431, 0.9818277359008789, 0.9701344966888428, 0.9943618178367615, 0.9745973944664001, 0.9882427453994751, 0.9128551483154297, 0.9928087592124939]
[2025-05-13 01:11:45,859]: Mean: 0.97218752
[2025-05-13 01:11:45,859]: Min: 0.91285515
[2025-05-13 01:11:45,860]: Max: 1.02397072
[2025-05-13 01:11:45,861]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-13 01:11:45,861]: Sample Values (25 elements): [-0.09568047523498535, 0.09568047523498535, -0.15946745872497559, -0.09568047523498535, 0.09568047523498535, 0.0, 0.12757396697998047, 0.12757396697998047, 0.0, -0.12757396697998047, 0.09568047523498535, 0.06378698348999023, 0.03189349174499512, 0.12757396697998047, -0.12757396697998047, -0.12757396697998047, -0.09568047523498535, 0.09568047523498535, 0.03189349174499512, -0.15946745872497559, 0.0, -0.03189349174499512, -0.06378698348999023, 0.15946745872497559, -0.03189349174499512]
[2025-05-13 01:11:45,861]: Mean: 0.00018688
[2025-05-13 01:11:45,861]: Min: -0.22325444
[2025-05-13 01:11:45,862]: Max: 0.25514793
[2025-05-13 01:11:45,862]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,862]: Sample Values (25 elements): [0.9268580675125122, 0.912043035030365, 0.9023151993751526, 0.9252296090126038, 0.9096096158027649, 0.9062459468841553, 0.9237277507781982, 0.9013239741325378, 0.9144478440284729, 0.93714439868927, 0.8989894986152649, 0.9109922647476196, 0.9276247024536133, 0.8975189328193665, 0.919022262096405, 0.9205469489097595, 0.919720470905304, 0.9108452200889587, 0.9255585074424744, 0.8809179663658142, 0.909737229347229, 0.8837471008300781, 0.9078205227851868, 0.8904270529747009, 0.90352463722229]
[2025-05-13 01:11:45,862]: Mean: 0.91126812
[2025-05-13 01:11:45,863]: Min: 0.85664189
[2025-05-13 01:11:45,863]: Max: 0.95476812
[2025-05-13 01:11:45,865]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 01:11:45,865]: Sample Values (25 elements): [0.02903647907078266, 0.01451823953539133, 0.02903647907078266, 0.02903647907078266, 0.0, 0.01451823953539133, -0.04355471953749657, 0.01451823953539133, -0.01451823953539133, 0.01451823953539133, 0.04355471953749657, 0.0, 0.0, -0.04355471953749657, 0.01451823953539133, -0.02903647907078266, -0.02903647907078266, 0.0, 0.04355471953749657, 0.04355471953749657, 0.01451823953539133, -0.02903647907078266, -0.01451823953539133, 0.0, 0.0]
[2025-05-13 01:11:45,866]: Mean: 0.00008625
[2025-05-13 01:11:45,866]: Min: -0.10162768
[2025-05-13 01:11:45,866]: Max: 0.11614592
[2025-05-13 01:11:45,866]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,866]: Sample Values (25 elements): [0.9634809494018555, 0.9492649435997009, 0.9462692737579346, 0.973808228969574, 0.9483625292778015, 0.9725397825241089, 0.9706116318702698, 0.9718550443649292, 0.9977988600730896, 0.9478970170021057, 0.9558357000350952, 0.969681441783905, 0.9585072994232178, 0.9519448280334473, 0.9618667364120483, 0.9604973196983337, 0.9683664441108704, 0.9562429785728455, 0.9501006603240967, 0.9541751742362976, 0.9684805274009705, 0.956028163433075, 0.9534023404121399, 0.9434956312179565, 0.9534650444984436]
[2025-05-13 01:11:45,866]: Mean: 0.96273077
[2025-05-13 01:11:45,867]: Min: 0.94004732
[2025-05-13 01:11:45,867]: Max: 0.99779886
[2025-05-13 01:11:45,870]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 01:11:45,872]: Sample Values (25 elements): [0.0, -0.029024196788668633, 0.029024196788668633, -0.043536294251680374, 0.058048393577337265, 0.043536294251680374, -0.043536294251680374, 0.014512098394334316, 0.014512098394334316, 0.0, 0.029024196788668633, -0.014512098394334316, 0.029024196788668633, 0.014512098394334316, 0.043536294251680374, -0.014512098394334316, 0.029024196788668633, 0.0, -0.014512098394334316, -0.043536294251680374, -0.029024196788668633, 0.0, 0.07256048917770386, -0.029024196788668633, -0.043536294251680374]
[2025-05-13 01:11:45,873]: Mean: -0.00021612
[2025-05-13 01:11:45,874]: Min: -0.11609679
[2025-05-13 01:11:45,874]: Max: 0.10158469
[2025-05-13 01:11:45,875]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,875]: Sample Values (25 elements): [0.9726176261901855, 0.9662680625915527, 0.9896699786186218, 0.9668331742286682, 1.0247646570205688, 0.9809207320213318, 0.9779430627822876, 1.0248711109161377, 0.9753753542900085, 0.9819713830947876, 0.9757755398750305, 0.9978689551353455, 1.0425114631652832, 0.9780697226524353, 1.0094292163848877, 0.9904240369796753, 0.9987033009529114, 0.9395225644111633, 0.9816723465919495, 0.9637471437454224, 0.9761194586753845, 0.9636107683181763, 0.9934686422348022, 0.9609917998313904, 0.9850132465362549]
[2025-05-13 01:11:45,875]: Mean: 0.98282045
[2025-05-13 01:11:45,875]: Min: 0.93952256
[2025-05-13 01:11:45,875]: Max: 1.04251146
[2025-05-13 01:11:45,876]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 01:11:45,877]: Sample Values (25 elements): [-0.012866903096437454, -0.012866903096437454, -0.05146761238574982, 0.012866903096437454, -0.012866903096437454, 0.0, -0.012866903096437454, 0.0, -0.012866903096437454, 0.012866903096437454, -0.012866903096437454, -0.012866903096437454, 0.02573380619287491, 0.012866903096437454, 0.02573380619287491, 0.012866903096437454, 0.012866903096437454, -0.05146761238574982, 0.0, -0.012866903096437454, -0.012866903096437454, -0.012866903096437454, 0.0, 0.05146761238574982, -0.03860070928931236]
[2025-05-13 01:11:45,877]: Mean: 0.00011414
[2025-05-13 01:11:45,877]: Min: -0.10293522
[2025-05-13 01:11:45,878]: Max: 0.09006833
[2025-05-13 01:11:45,878]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,878]: Sample Values (25 elements): [0.9713331460952759, 0.9671158790588379, 0.9721410274505615, 0.9738519787788391, 0.9710558652877808, 0.9661267995834351, 0.9514049291610718, 0.9768252968788147, 0.9792950749397278, 0.9719322323799133, 0.9717105627059937, 0.9967173337936401, 0.9563263654708862, 0.97210693359375, 0.9772160053253174, 0.961715579032898, 0.9574463963508606, 0.9606374502182007, 0.9653704762458801, 0.9850330948829651, 0.9530314207077026, 0.9553791284561157, 0.9605477452278137, 0.9699885249137878, 0.9570655822753906]
[2025-05-13 01:11:45,878]: Mean: 0.96769732
[2025-05-13 01:11:45,878]: Min: 0.95140493
[2025-05-13 01:11:45,878]: Max: 0.99671733
[2025-05-13 01:11:45,879]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 01:11:45,880]: Sample Values (25 elements): [-0.03135434538125992, -0.02090289629995823, -0.02090289629995823, 0.02090289629995823, -0.02090289629995823, 0.0, 0.02090289629995823, 0.0, 0.04180579259991646, 0.010451448149979115, -0.02090289629995823, 0.0, 0.02090289629995823, -0.052257239818573, -0.03135434538125992, 0.02090289629995823, -0.02090289629995823, 0.010451448149979115, -0.010451448149979115, 0.02090289629995823, 0.010451448149979115, 0.04180579259991646, -0.02090289629995823, 0.010451448149979115, 0.03135434538125992]
[2025-05-13 01:11:45,880]: Mean: 0.00008307
[2025-05-13 01:11:45,880]: Min: -0.08361159
[2025-05-13 01:11:45,880]: Max: 0.07316013
[2025-05-13 01:11:45,880]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-13 01:11:45,881]: Sample Values (25 elements): [1.0391861200332642, 1.019579529762268, 1.0622258186340332, 1.0273480415344238, 1.0297375917434692, 1.008156180381775, 1.0402318239212036, 1.0470162630081177, 1.0261300802230835, 1.0663765668869019, 1.0555411577224731, 1.0226209163665771, 1.0212560892105103, 1.0389366149902344, 1.041032314300537, 1.0407179594039917, 1.0217939615249634, 1.0216442346572876, 1.0388013124465942, 1.0554295778274536, 1.0672557353973389, 1.043911337852478, 1.0353748798370361, 1.042324423789978, 1.0544238090515137]
[2025-05-13 01:11:45,881]: Mean: 1.04503226
[2025-05-13 01:11:45,881]: Min: 1.00815618
[2025-05-13 01:11:45,881]: Max: 1.08735442
[2025-05-13 01:11:45,881]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-13 01:11:45,882]: Sample Values (25 elements): [-0.07153896987438202, -0.029980698600411415, -0.30850300192832947, -0.1753152757883072, 0.2734517455101013, 0.1633618026971817, -0.0039335330948233604, 0.17527395486831665, 0.22436486184597015, 0.3084535598754883, 0.13074874877929688, -0.32338395714759827, -0.017083538696169853, -0.08318961411714554, -0.11086279153823853, -0.10490512102842331, -0.09996860474348068, 0.013697809539735317, -0.021703967824578285, -0.2717377245426178, -0.22232869267463684, 0.0036319387145340443, -0.2315339744091034, -0.18373703956604004, 0.2940211892127991]
[2025-05-13 01:11:45,882]: Mean: 0.00512979
[2025-05-13 01:11:45,882]: Min: -0.38639504
[2025-05-13 01:11:45,882]: Max: 0.45491567
[2025-05-13 01:11:45,882]: 


[2025-05-13 02:12:28,392]: 


QAT of ResNet20 with parametrized_hardtanh down to 3 bits...
[2025-05-13 02:12:28,636]: [ResNet20_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-05-13 02:12:28,785]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-13 02:13:25,665]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.6982 Train Acc: 0.7557 Eval Loss: 0.8583 Eval Acc: 0.7125 (LR: 0.001000)
[2025-05-13 02:14:25,103]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.6771 Train Acc: 0.7605 Eval Loss: 0.7572 Eval Acc: 0.7467 (LR: 0.001000)
[2025-05-13 02:15:26,441]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.6809 Train Acc: 0.7606 Eval Loss: 0.7768 Eval Acc: 0.7384 (LR: 0.001000)
[2025-05-13 02:16:27,320]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.6756 Train Acc: 0.7599 Eval Loss: 0.8921 Eval Acc: 0.7069 (LR: 0.001000)
[2025-05-13 02:17:22,596]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.6691 Train Acc: 0.7652 Eval Loss: 1.1655 Eval Acc: 0.6491 (LR: 0.001000)
[2025-05-13 02:18:24,988]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.6771 Train Acc: 0.7620 Eval Loss: 0.7867 Eval Acc: 0.7368 (LR: 0.001000)
[2025-05-13 02:19:25,615]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.6675 Train Acc: 0.7659 Eval Loss: 0.9376 Eval Acc: 0.7009 (LR: 0.001000)
[2025-05-13 02:20:28,766]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.6790 Train Acc: 0.7596 Eval Loss: 1.0803 Eval Acc: 0.6445 (LR: 0.001000)
[2025-05-13 02:21:31,812]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.6714 Train Acc: 0.7644 Eval Loss: 0.7478 Eval Acc: 0.7515 (LR: 0.001000)
[2025-05-13 02:22:35,207]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.6736 Train Acc: 0.7643 Eval Loss: 0.8255 Eval Acc: 0.7114 (LR: 0.001000)
[2025-05-13 02:23:36,994]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.6606 Train Acc: 0.7693 Eval Loss: 0.9741 Eval Acc: 0.6872 (LR: 0.001000)
[2025-05-13 02:24:32,417]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.6569 Train Acc: 0.7703 Eval Loss: 0.8625 Eval Acc: 0.7140 (LR: 0.001000)
[2025-05-13 02:25:28,096]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.6556 Train Acc: 0.7701 Eval Loss: 0.7196 Eval Acc: 0.7530 (LR: 0.001000)
[2025-05-13 02:26:23,662]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.6546 Train Acc: 0.7703 Eval Loss: 0.8299 Eval Acc: 0.7206 (LR: 0.001000)
[2025-05-13 02:27:19,071]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.6451 Train Acc: 0.7756 Eval Loss: 0.7089 Eval Acc: 0.7597 (LR: 0.001000)
[2025-05-13 02:28:14,668]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.6473 Train Acc: 0.7740 Eval Loss: 0.6825 Eval Acc: 0.7715 (LR: 0.001000)
[2025-05-13 02:29:10,273]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.6439 Train Acc: 0.7725 Eval Loss: 0.8136 Eval Acc: 0.7297 (LR: 0.001000)
[2025-05-13 02:30:07,352]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.6413 Train Acc: 0.7750 Eval Loss: 0.7312 Eval Acc: 0.7524 (LR: 0.001000)
[2025-05-13 02:31:06,097]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.6378 Train Acc: 0.7762 Eval Loss: 0.8324 Eval Acc: 0.7269 (LR: 0.001000)
[2025-05-13 02:32:04,490]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.6303 Train Acc: 0.7786 Eval Loss: 0.8832 Eval Acc: 0.7214 (LR: 0.001000)
[2025-05-13 02:33:00,179]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.6339 Train Acc: 0.7778 Eval Loss: 0.6636 Eval Acc: 0.7797 (LR: 0.001000)
[2025-05-13 02:33:59,188]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.6281 Train Acc: 0.7793 Eval Loss: 0.7394 Eval Acc: 0.7523 (LR: 0.001000)
[2025-05-13 02:34:58,937]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.6253 Train Acc: 0.7815 Eval Loss: 0.6869 Eval Acc: 0.7669 (LR: 0.001000)
[2025-05-13 02:35:59,664]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.6234 Train Acc: 0.7832 Eval Loss: 0.7212 Eval Acc: 0.7540 (LR: 0.001000)
[2025-05-13 02:36:59,153]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.6183 Train Acc: 0.7829 Eval Loss: 0.6935 Eval Acc: 0.7656 (LR: 0.001000)
[2025-05-13 02:37:58,636]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.6172 Train Acc: 0.7824 Eval Loss: 0.8791 Eval Acc: 0.7078 (LR: 0.001000)
[2025-05-13 02:38:59,979]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.6143 Train Acc: 0.7855 Eval Loss: 0.9146 Eval Acc: 0.7063 (LR: 0.001000)
[2025-05-13 02:39:59,893]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.6160 Train Acc: 0.7838 Eval Loss: 0.7816 Eval Acc: 0.7440 (LR: 0.001000)
[2025-05-13 02:41:00,034]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.6153 Train Acc: 0.7840 Eval Loss: 0.8083 Eval Acc: 0.7332 (LR: 0.001000)
[2025-05-13 02:41:59,916]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.6010 Train Acc: 0.7904 Eval Loss: 0.9098 Eval Acc: 0.7086 (LR: 0.000250)
[2025-05-13 02:43:00,450]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.5441 Train Acc: 0.8112 Eval Loss: 0.7146 Eval Acc: 0.7639 (LR: 0.000250)
[2025-05-13 02:44:00,399]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.5326 Train Acc: 0.8129 Eval Loss: 0.8069 Eval Acc: 0.7395 (LR: 0.000250)
[2025-05-13 02:45:00,455]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.5318 Train Acc: 0.8131 Eval Loss: 0.6274 Eval Acc: 0.7855 (LR: 0.000250)
[2025-05-13 02:45:59,871]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.5286 Train Acc: 0.8151 Eval Loss: 0.6295 Eval Acc: 0.7924 (LR: 0.000250)
[2025-05-13 02:46:59,648]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.5277 Train Acc: 0.8165 Eval Loss: 0.8757 Eval Acc: 0.7318 (LR: 0.000250)
[2025-05-13 02:47:59,638]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.5308 Train Acc: 0.8149 Eval Loss: 0.6421 Eval Acc: 0.7860 (LR: 0.000250)
[2025-05-13 02:48:59,462]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.5311 Train Acc: 0.8132 Eval Loss: 0.6469 Eval Acc: 0.7864 (LR: 0.000250)
[2025-05-13 02:49:59,620]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.5297 Train Acc: 0.8158 Eval Loss: 0.6855 Eval Acc: 0.7729 (LR: 0.000250)
[2025-05-13 02:51:00,000]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.5266 Train Acc: 0.8155 Eval Loss: 0.6852 Eval Acc: 0.7721 (LR: 0.000250)
[2025-05-13 02:52:00,297]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.5241 Train Acc: 0.8168 Eval Loss: 0.6864 Eval Acc: 0.7800 (LR: 0.000250)
[2025-05-13 02:53:00,518]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.5231 Train Acc: 0.8164 Eval Loss: 0.6417 Eval Acc: 0.7877 (LR: 0.000250)
[2025-05-13 02:54:01,009]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.5286 Train Acc: 0.8142 Eval Loss: 0.6972 Eval Acc: 0.7729 (LR: 0.000250)
[2025-05-13 02:55:00,897]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.5289 Train Acc: 0.8159 Eval Loss: 0.6474 Eval Acc: 0.7852 (LR: 0.000250)
[2025-05-13 02:56:00,805]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.5248 Train Acc: 0.8186 Eval Loss: 0.7188 Eval Acc: 0.7726 (LR: 0.000250)
[2025-05-13 02:57:00,503]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.5221 Train Acc: 0.8173 Eval Loss: 0.6718 Eval Acc: 0.7749 (LR: 0.000063)
[2025-05-13 02:58:00,247]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.4992 Train Acc: 0.8244 Eval Loss: 0.6347 Eval Acc: 0.7896 (LR: 0.000063)
[2025-05-13 02:59:00,097]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.4971 Train Acc: 0.8290 Eval Loss: 0.6231 Eval Acc: 0.7959 (LR: 0.000063)
[2025-05-13 02:59:59,945]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.4925 Train Acc: 0.8278 Eval Loss: 0.6598 Eval Acc: 0.7887 (LR: 0.000063)
[2025-05-13 03:00:59,670]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.4940 Train Acc: 0.8280 Eval Loss: 0.7031 Eval Acc: 0.7704 (LR: 0.000063)
[2025-05-13 03:01:59,524]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.4951 Train Acc: 0.8249 Eval Loss: 0.6588 Eval Acc: 0.7835 (LR: 0.000063)
[2025-05-13 03:02:59,504]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.4953 Train Acc: 0.8259 Eval Loss: 0.6459 Eval Acc: 0.7895 (LR: 0.000063)
[2025-05-13 03:03:59,430]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.4941 Train Acc: 0.8277 Eval Loss: 0.7037 Eval Acc: 0.7648 (LR: 0.000063)
[2025-05-13 03:05:00,583]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.4927 Train Acc: 0.8265 Eval Loss: 0.6724 Eval Acc: 0.7757 (LR: 0.000063)
[2025-05-13 03:05:59,714]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.4934 Train Acc: 0.8276 Eval Loss: 0.6469 Eval Acc: 0.7856 (LR: 0.000063)
[2025-05-13 03:06:59,308]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.4946 Train Acc: 0.8280 Eval Loss: 0.6982 Eval Acc: 0.7698 (LR: 0.000063)
[2025-05-13 03:07:59,080]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.4962 Train Acc: 0.8260 Eval Loss: 0.7705 Eval Acc: 0.7618 (LR: 0.000063)
[2025-05-13 03:08:58,356]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.4978 Train Acc: 0.8245 Eval Loss: 0.6273 Eval Acc: 0.7891 (LR: 0.000063)
[2025-05-13 03:09:58,065]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.4923 Train Acc: 0.8263 Eval Loss: 0.7990 Eval Acc: 0.7419 (LR: 0.000063)
[2025-05-13 03:10:57,382]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.4872 Train Acc: 0.8306 Eval Loss: 0.9940 Eval Acc: 0.6953 (LR: 0.000063)
[2025-05-13 03:11:57,257]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.4929 Train Acc: 0.8266 Eval Loss: 0.7423 Eval Acc: 0.7616 (LR: 0.000063)
[2025-05-13 03:11:57,257]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.7959
[2025-05-13 03:11:57,334]: 


Quantization of model down to 3 bits finished
[2025-05-13 03:11:57,334]: Model Architecture:
[2025-05-13 03:11:57,394]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3648], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2766656875610352, max_val=1.2766659259796143)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1151], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3660327196121216, max_val=0.4396398961544037)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2767], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7987544536590576, max_val=0.138429656624794)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0944], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36680370569229126, max_val=0.294077068567276)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2952], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0330824851989746, max_val=1.0330629348754883)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0928], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3068108856678009, max_val=0.34260714054107666)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2554], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4044255018234253, max_val=0.38302621245384216)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0892], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28100189566612244, max_val=0.3432496190071106)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3832], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7232837080955505, max_val=1.9593322277069092)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0790], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25644487142562866, max_val=0.2965258061885834)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2714], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.949722170829773, max_val=0.9497373700141907)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2045036405324936, max_val=0.21128156781196594)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4754], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7454806566238403, max_val=1.5825895071029663)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0548], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20736169815063477, max_val=0.17647923529148102)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3190], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7975039482116699, max_val=1.435476303100586)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0422], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14046424627304077, max_val=0.15516695380210876)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0985], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3496474325656891, max_val=0.3398149013519287)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3325], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1636853218078613, max_val=1.1636043787002563)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0444], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15709726512432098, max_val=0.15392468869686127)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2602], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.910538375377655, max_val=0.9105369448661804)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0425], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1399751752614975, max_val=0.15781338512897491)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3068], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.380735993385315, max_val=0.7670740485191345)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0370], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1297406256198883, max_val=0.12908463180065155)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2682], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6704084873199463, max_val=1.2067310810089111)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0396], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13726180791854858, max_val=0.14006385207176208)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4148], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0370399951934814, max_val=1.866613745689392)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0363], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12716959416866302, max_val=0.12711994349956512)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2521], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3781912326812744, max_val=1.3867030143737793)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10826221108436584, max_val=0.10415662080049515)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0715], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24444347620010376, max_val=0.2563426196575165)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2991], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7478160262107849, max_val=1.3460376262664795)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0324], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10940506309270859, max_val=0.11772073805332184)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2156], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9701792597770691, max_val=0.5389977097511292)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0312], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11103963851928711, max_val=0.10729784518480301)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2712], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2204355001449585, max_val=0.6780208945274353)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0285], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10128504037857056, max_val=0.09827900677919388)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1743], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43570467829704285, max_val=0.7842068076133728)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0223], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07633312791585922, max_val=0.07974648475646973)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6873], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3918509483337402, max_val=2.419450044631958)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-13 03:11:57,394]: 
Model Weights:
[2025-05-13 03:11:57,394]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-13 03:11:57,395]: Sample Values (25 elements): [0.13370540738105774, -0.07891128957271576, -0.12095261365175247, 0.2244015634059906, -0.29507553577423096, -0.15370960533618927, 0.42247289419174194, -0.09229397028684616, -0.13025829195976257, -0.042639195919036865, 0.016018765047192574, 0.04241450875997543, 0.24217550456523895, -0.19888882339000702, -0.02179284580051899, -0.23572659492492676, -0.15553870797157288, 0.04883553832769394, -0.09241729974746704, 0.3073509633541107, 0.08101899176836014, -0.12074952572584152, 0.02869429625570774, 0.27453872561454773, 0.09195540845394135]
[2025-05-13 03:11:57,401]: Mean: 0.00560919
[2025-05-13 03:11:57,402]: Min: -0.55248141
[2025-05-13 03:11:57,402]: Max: 0.53382397
[2025-05-13 03:11:57,402]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,402]: Sample Values (16 elements): [0.8029036521911621, 1.214123010635376, 0.7448951005935669, 1.0381972789764404, 1.17548668384552, 1.0353069305419922, 0.6524134278297424, 0.6776055693626404, 0.8880615234375, 0.6941108703613281, 1.0387855768203735, 1.0789343118667603, 0.8972963094711304, 1.0123754739761353, 1.2137259244918823, 0.8999970555305481]
[2025-05-13 03:11:57,402]: Mean: 0.94151366
[2025-05-13 03:11:57,403]: Min: 0.65241343
[2025-05-13 03:11:57,403]: Max: 1.21412301
[2025-05-13 03:11:57,404]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 03:11:57,404]: Sample Values (25 elements): [0.11509616672992706, 0.0, 0.11509616672992706, 0.11509616672992706, 0.0, 0.0, -0.11509616672992706, 0.11509616672992706, -0.11509616672992706, 0.11509616672992706, 0.0, -0.23019233345985413, 0.0, 0.0, 0.0, 0.0, 0.11509616672992706, 0.0, 0.0, -0.11509616672992706, 0.0, -0.23019233345985413, 0.11509616672992706, -0.11509616672992706, 0.11509616672992706]
[2025-05-13 03:11:57,404]: Mean: 0.00134878
[2025-05-13 03:11:57,405]: Min: -0.34528852
[2025-05-13 03:11:57,405]: Max: 0.46038467
[2025-05-13 03:11:57,405]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,405]: Sample Values (16 elements): [0.7473980784416199, 0.8016557097434998, 0.8747737407684326, 0.7404280304908752, 0.9953952431678772, 1.0184251070022583, 0.9524255990982056, 0.7812877893447876, 0.9907349348068237, 0.8057264089584351, 0.7684134244918823, 0.9920554757118225, 1.155045986175537, 1.0929639339447021, 1.1924333572387695, 0.8144020438194275]
[2025-05-13 03:11:57,405]: Mean: 0.92022282
[2025-05-13 03:11:57,405]: Min: 0.74042803
[2025-05-13 03:11:57,406]: Max: 1.19243336
[2025-05-13 03:11:57,407]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 03:11:57,407]: Sample Values (25 elements): [0.0, 0.09441151469945908, -0.18882302939891815, -0.18882302939891815, 0.0, -0.18882302939891815, 0.0, 0.09441151469945908, 0.0, -0.18882302939891815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09441151469945908, 0.0, -0.09441151469945908, -0.09441151469945908, 0.09441151469945908, 0.0, -0.09441151469945908]
[2025-05-13 03:11:57,407]: Mean: -0.00180300
[2025-05-13 03:11:57,408]: Min: -0.37764606
[2025-05-13 03:11:57,408]: Max: 0.28323454
[2025-05-13 03:11:57,408]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,408]: Sample Values (16 elements): [0.8454769849777222, 0.9573929309844971, 0.650261640548706, 0.7952967882156372, 0.784927248954773, 0.6608837246894836, 0.9223393797874451, 0.882260799407959, 0.6395092606544495, 1.066446304321289, 0.6432422399520874, 0.8569223284721375, 0.8481934070587158, 0.9567795395851135, 0.8531193137168884, 0.7570159435272217]
[2025-05-13 03:11:57,408]: Mean: 0.82000422
[2025-05-13 03:11:57,409]: Min: 0.63950926
[2025-05-13 03:11:57,409]: Max: 1.06644630
[2025-05-13 03:11:57,410]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 03:11:57,410]: Sample Values (25 elements): [0.0, -0.09277412295341492, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09277412295341492, 0.0, -0.09277412295341492, 0.09277412295341492, -0.09277412295341492, 0.0, 0.0, 0.0, -0.09277412295341492, -0.09277412295341492, -0.09277412295341492, 0.0, 0.09277412295341492, 0.0, 0.0, -0.09277412295341492, 0.09277412295341492, 0.09277412295341492]
[2025-05-13 03:11:57,410]: Mean: 0.00273812
[2025-05-13 03:11:57,411]: Min: -0.27832237
[2025-05-13 03:11:57,411]: Max: 0.37109649
[2025-05-13 03:11:57,411]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,411]: Sample Values (16 elements): [1.0762795209884644, 1.07881498336792, 0.936208963394165, 1.1118718385696411, 1.0108355283737183, 1.0770584344863892, 0.8622788190841675, 0.7407153248786926, 0.9438881278038025, 1.0344409942626953, 0.9653671979904175, 0.942796528339386, 1.0419502258300781, 0.8819395899772644, 0.7867600321769714, 0.7427215576171875]
[2025-05-13 03:11:57,411]: Mean: 0.95212048
[2025-05-13 03:11:57,411]: Min: 0.74071532
[2025-05-13 03:11:57,411]: Max: 1.11187184
[2025-05-13 03:11:57,412]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 03:11:57,413]: Sample Values (25 elements): [-0.08917879313230515, 0.0, -0.26753637194633484, 0.0, 0.08917879313230515, 0.08917879313230515, -0.08917879313230515, 0.0, 0.0, -0.08917879313230515, 0.0, -0.08917879313230515, 0.1783575862646103, 0.08917879313230515, 0.3567151725292206, 0.0, -0.08917879313230515, 0.0, -0.08917879313230515, 0.1783575862646103, 0.08917879313230515, -0.08917879313230515, 0.08917879313230515, 0.08917879313230515, 0.0]
[2025-05-13 03:11:57,413]: Mean: -0.00212883
[2025-05-13 03:11:57,413]: Min: -0.26753637
[2025-05-13 03:11:57,413]: Max: 0.35671517
[2025-05-13 03:11:57,413]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,414]: Sample Values (16 elements): [0.954613208770752, 0.9677027463912964, 0.8699651956558228, 0.8840153813362122, 1.0674357414245605, 0.9007350206375122, 1.07068932056427, 0.777285635471344, 1.008684515953064, 0.8994869589805603, 0.7810075879096985, 0.8989095687866211, 1.0736380815505981, 0.951871395111084, 0.8984147310256958, 0.8341741561889648]
[2025-05-13 03:11:57,414]: Mean: 0.92741430
[2025-05-13 03:11:57,414]: Min: 0.77728564
[2025-05-13 03:11:57,414]: Max: 1.07363808
[2025-05-13 03:11:57,416]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 03:11:57,416]: Sample Values (25 elements): [0.0, 0.0, 0.07899592816829681, 0.0, 0.0, -0.07899592816829681, -0.07899592816829681, 0.07899592816829681, 0.0, -0.15799185633659363, 0.0, -0.07899592816829681, -0.07899592816829681, -0.07899592816829681, 0.07899592816829681, -0.07899592816829681, 0.07899592816829681, 0.07899592816829681, 0.0, 0.07899592816829681, 0.0, 0.15799185633659363, 0.0, -0.07899592816829681, 0.0]
[2025-05-13 03:11:57,416]: Mean: 0.00178289
[2025-05-13 03:11:57,416]: Min: -0.23698778
[2025-05-13 03:11:57,416]: Max: 0.31598371
[2025-05-13 03:11:57,416]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,417]: Sample Values (16 elements): [0.9175748229026794, 1.0009057521820068, 1.1014652252197266, 0.9062793254852295, 0.9176746010780334, 0.9947212338447571, 0.9324840903282166, 0.8967036008834839, 0.9801782965660095, 1.0192737579345703, 0.8490875363349915, 0.9310539364814758, 1.0166107416152954, 0.9494678974151611, 1.013368010520935, 0.988328754901886]
[2025-05-13 03:11:57,417]: Mean: 0.96344864
[2025-05-13 03:11:57,417]: Min: 0.84908754
[2025-05-13 03:11:57,417]: Max: 1.10146523
[2025-05-13 03:11:57,418]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 03:11:57,418]: Sample Values (25 elements): [0.0, 0.0, -0.11879587918519974, -0.05939793959259987, -0.05939793959259987, 0.0, 0.05939793959259987, 0.05939793959259987, -0.11879587918519974, -0.05939793959259987, 0.05939793959259987, -0.05939793959259987, -0.1781938225030899, -0.05939793959259987, 0.0, -0.1781938225030899, 0.0, 0.0, 0.11879587918519974, 0.0, -0.05939793959259987, 0.05939793959259987, -0.05939793959259987, 0.11879587918519974, -0.11879587918519974]
[2025-05-13 03:11:57,419]: Mean: -0.00304208
[2025-05-13 03:11:57,419]: Min: -0.17819382
[2025-05-13 03:11:57,419]: Max: 0.23759176
[2025-05-13 03:11:57,419]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-13 03:11:57,419]: Sample Values (16 elements): [0.8463266491889954, 1.078863501548767, 0.958803117275238, 0.931958019733429, 0.8846262097358704, 1.00540030002594, 0.9984036087989807, 0.9169800281524658, 0.9988032579421997, 0.9093177318572998, 0.881230890750885, 0.8585358262062073, 1.0057681798934937, 0.9353607296943665, 0.9051506519317627, 0.9018555879592896]
[2025-05-13 03:11:57,419]: Mean: 0.93858653
[2025-05-13 03:11:57,419]: Min: 0.84632665
[2025-05-13 03:11:57,420]: Max: 1.07886350
[2025-05-13 03:11:57,421]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-13 03:11:57,421]: Sample Values (25 elements): [0.0, -0.054834309965372086, -0.054834309965372086, -0.054834309965372086, 0.054834309965372086, -0.054834309965372086, -0.10966861993074417, 0.054834309965372086, 0.0, -0.054834309965372086, 0.054834309965372086, -0.10966861993074417, -0.054834309965372086, 0.054834309965372086, 0.054834309965372086, 0.0, -0.054834309965372086, 0.0, -0.10966861993074417, 0.10966861993074417, 0.054834309965372086, -0.10966861993074417, 0.0, 0.054834309965372086, 0.054834309965372086]
[2025-05-13 03:11:57,421]: Mean: -0.00234426
[2025-05-13 03:11:57,421]: Min: -0.21933724
[2025-05-13 03:11:57,421]: Max: 0.16450293
[2025-05-13 03:11:57,421]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,422]: Sample Values (25 elements): [0.950181782245636, 0.8888390064239502, 0.9479373693466187, 0.9739340543746948, 0.9361504912376404, 0.9374186992645264, 0.9260388612747192, 0.9706583023071289, 0.9386407732963562, 0.9650724530220032, 0.9284800887107849, 0.9792779684066772, 0.9753797650337219, 0.9102979898452759, 0.9658190011978149, 0.9682085514068604, 0.9086531400680542, 0.9519327282905579, 0.9838767051696777, 0.9066992402076721, 0.9539289474487305, 0.9645900726318359, 0.9002887606620789, 0.9387954473495483, 1.003214716911316]
[2025-05-13 03:11:57,422]: Mean: 0.94631803
[2025-05-13 03:11:57,422]: Min: 0.88883901
[2025-05-13 03:11:57,422]: Max: 1.00321472
[2025-05-13 03:11:57,423]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 03:11:57,424]: Sample Values (25 elements): [0.04223305359482765, -0.04223305359482765, -0.0844661071896553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04223305359482765, 0.04223305359482765, -0.04223305359482765, 0.04223305359482765, 0.04223305359482765, 0.0, -0.04223305359482765, -0.04223305359482765, 0.0, -0.04223305359482765, 0.0, 0.0844661071896553, 0.0, 0.0, 0.0, 0.04223305359482765, 0.0]
[2025-05-13 03:11:57,424]: Mean: -0.00005499
[2025-05-13 03:11:57,424]: Min: -0.12669916
[2025-05-13 03:11:57,424]: Max: 0.16893221
[2025-05-13 03:11:57,424]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,424]: Sample Values (25 elements): [1.023262858390808, 0.9150095582008362, 1.0026624202728271, 0.9142299890518188, 0.9350917339324951, 1.0376050472259521, 0.9463777542114258, 0.9672240614891052, 1.010205864906311, 0.9757159352302551, 0.9169833064079285, 0.9398711919784546, 0.9683776497840881, 0.9628098607063293, 0.9499725103378296, 0.9545184969902039, 0.9234499335289001, 0.9567492604255676, 0.8891815543174744, 0.950751006603241, 0.9531065821647644, 0.9742870330810547, 0.9634194374084473, 0.9799795746803284, 0.9287773370742798]
[2025-05-13 03:11:57,424]: Mean: 0.95707309
[2025-05-13 03:11:57,425]: Min: 0.88918155
[2025-05-13 03:11:57,425]: Max: 1.03760505
[2025-05-13 03:11:57,426]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-13 03:11:57,426]: Sample Values (25 elements): [0.09849456697702408, 0.0, -0.19698913395404816, 0.09849456697702408, -0.09849456697702408, 0.19698913395404816, -0.09849456697702408, -0.09849456697702408, -0.09849456697702408, 0.0, -0.19698913395404816, 0.09849456697702408, 0.0, 0.09849456697702408, 0.29548370838165283, -0.19698913395404816, 0.0, 0.0, -0.09849456697702408, 0.19698913395404816, 0.0, 0.0, -0.19698913395404816, -0.19698913395404816, 0.09849456697702408]
[2025-05-13 03:11:57,426]: Mean: -0.01000335
[2025-05-13 03:11:57,426]: Min: -0.39397827
[2025-05-13 03:11:57,427]: Max: 0.29548371
[2025-05-13 03:11:57,427]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,427]: Sample Values (25 elements): [0.810600221157074, 0.8384466767311096, 0.8568717837333679, 0.7733260989189148, 0.809497058391571, 0.8223657011985779, 0.8987499475479126, 0.7885396480560303, 0.8683288097381592, 0.8432849049568176, 0.8594103455543518, 0.8149195313453674, 0.8168598413467407, 0.8791614770889282, 0.826968789100647, 0.81081223487854, 0.8804213404655457, 0.8420253992080688, 0.8196207284927368, 0.9208514094352722, 0.8569804430007935, 0.8044842481613159, 0.8377571702003479, 0.8382182717323303, 0.8689802885055542]
[2025-05-13 03:11:57,427]: Mean: 0.84241098
[2025-05-13 03:11:57,427]: Min: 0.77332610
[2025-05-13 03:11:57,427]: Max: 0.93004060
[2025-05-13 03:11:57,428]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 03:11:57,429]: Sample Values (25 elements): [0.0, -0.08886349946260452, -0.04443174973130226, -0.04443174973130226, -0.04443174973130226, -0.04443174973130226, 0.0, 0.0, 0.0, 0.0, -0.08886349946260452, 0.0, 0.04443174973130226, -0.04443174973130226, -0.04443174973130226, 0.08886349946260452, 0.04443174973130226, 0.0, 0.04443174973130226, 0.04443174973130226, -0.08886349946260452, 0.04443174973130226, -0.04443174973130226, -0.04443174973130226, -0.04443174973130226]
[2025-05-13 03:11:57,429]: Mean: 0.00023624
[2025-05-13 03:11:57,429]: Min: -0.17772700
[2025-05-13 03:11:57,429]: Max: 0.13329525
[2025-05-13 03:11:57,429]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,430]: Sample Values (25 elements): [0.9509751796722412, 0.9381735920906067, 1.0025367736816406, 0.9588565826416016, 0.9807868003845215, 0.9559229016304016, 0.9760569334030151, 0.9499134421348572, 0.9472476243972778, 0.9641097187995911, 0.9593015313148499, 0.9792570471763611, 0.9842750430107117, 1.0300171375274658, 0.9487002491950989, 0.9248713850975037, 0.9350582361221313, 0.9735065698623657, 0.9842448830604553, 0.9815060496330261, 0.9636979699134827, 0.9073975086212158, 0.9700431823730469, 0.9343955516815186, 1.0206393003463745]
[2025-05-13 03:11:57,430]: Mean: 0.96529126
[2025-05-13 03:11:57,430]: Min: 0.90739751
[2025-05-13 03:11:57,430]: Max: 1.03001714
[2025-05-13 03:11:57,431]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 03:11:57,431]: Sample Values (25 elements): [-0.042541343718767166, 0.0, 0.0, 0.042541343718767166, 0.042541343718767166, 0.0, 0.042541343718767166, -0.042541343718767166, -0.042541343718767166, 0.042541343718767166, 0.0, -0.042541343718767166, -0.08508268743753433, 0.042541343718767166, 0.0, 0.0, 0.042541343718767166, 0.0, 0.0, 0.042541343718767166, 0.042541343718767166, -0.042541343718767166, 0.0, -0.042541343718767166, 0.042541343718767166]
[2025-05-13 03:11:57,431]: Mean: 0.00075241
[2025-05-13 03:11:57,432]: Min: -0.12762403
[2025-05-13 03:11:57,432]: Max: 0.17016537
[2025-05-13 03:11:57,432]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,432]: Sample Values (25 elements): [0.9091495275497437, 0.935335099697113, 0.918051540851593, 0.9783430695533752, 0.9284242987632751, 0.9849383234977722, 1.06570565700531, 0.9821916818618774, 0.9400134086608887, 0.983373761177063, 0.9844839572906494, 1.0234323740005493, 0.9632105231285095, 0.9890182018280029, 0.9603837132453918, 0.9235444068908691, 0.9598544239997864, 1.0106854438781738, 1.0173628330230713, 0.928469717502594, 0.9202958345413208, 0.9346316456794739, 0.9787635207176208, 0.9802545309066772, 0.9895558953285217]
[2025-05-13 03:11:57,432]: Mean: 0.96264553
[2025-05-13 03:11:57,432]: Min: 0.90480500
[2025-05-13 03:11:57,433]: Max: 1.06570566
[2025-05-13 03:11:57,433]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 03:11:57,434]: Sample Values (25 elements): [0.0369749441742897, -0.0739498883485794, 0.0369749441742897, 0.0, 0.0369749441742897, -0.0369749441742897, 0.0, 0.0, 0.0, -0.0369749441742897, 0.0369749441742897, -0.0369749441742897, 0.0, 0.0369749441742897, 0.0369749441742897, 0.0369749441742897, -0.0739498883485794, -0.0369749441742897, 0.0369749441742897, 0.0, -0.0369749441742897, 0.0739498883485794, 0.0, 0.0, -0.0369749441742897]
[2025-05-13 03:11:57,434]: Mean: 0.00000401
[2025-05-13 03:11:57,434]: Min: -0.14789978
[2025-05-13 03:11:57,434]: Max: 0.11092483
[2025-05-13 03:11:57,434]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,435]: Sample Values (25 elements): [0.959962010383606, 0.9685541391372681, 0.9631521701812744, 0.9351697564125061, 0.9258927702903748, 0.963236927986145, 0.9844638705253601, 0.8884405493736267, 0.9316232204437256, 1.018417477607727, 0.9472132921218872, 0.9742177128791809, 0.9473736882209778, 0.9733322262763977, 0.9468016028404236, 0.9507137537002563, 0.9751738905906677, 0.935455858707428, 0.9779724478721619, 0.9865889549255371, 0.9209179282188416, 0.9266170263290405, 0.9535897970199585, 0.9377942681312561, 0.9684262871742249]
[2025-05-13 03:11:57,435]: Mean: 0.95568138
[2025-05-13 03:11:57,435]: Min: 0.88844055
[2025-05-13 03:11:57,435]: Max: 1.01841748
[2025-05-13 03:11:57,436]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 03:11:57,437]: Sample Values (25 elements): [0.0792359709739685, -0.03961798548698425, -0.0792359709739685, 0.0, 0.03961798548698425, 0.0, 0.0, 0.0, 0.0, -0.03961798548698425, 0.0792359709739685, -0.03961798548698425, 0.0, -0.03961798548698425, -0.0792359709739685, 0.03961798548698425, -0.03961798548698425, 0.03961798548698425, 0.0, 0.03961798548698425, 0.03961798548698425, 0.0, -0.0792359709739685, 0.0, 0.0]
[2025-05-13 03:11:57,437]: Mean: -0.00040839
[2025-05-13 03:11:57,437]: Min: -0.11885396
[2025-05-13 03:11:57,437]: Max: 0.15847194
[2025-05-13 03:11:57,437]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-13 03:11:57,438]: Sample Values (25 elements): [0.955839216709137, 0.9264074563980103, 1.0365972518920898, 0.9800372123718262, 0.9437537789344788, 1.0280520915985107, 0.957987904548645, 0.9327356219291687, 0.9896642565727234, 0.9739891886711121, 0.9639157652854919, 1.007148265838623, 0.9780611991882324, 0.9865542054176331, 1.0135085582733154, 1.0062223672866821, 1.008419394493103, 0.9874379634857178, 0.9920873045921326, 0.9488956332206726, 0.9223077297210693, 0.9965950846672058, 0.9598956108093262, 0.9532098770141602, 1.0254788398742676]
[2025-05-13 03:11:57,438]: Mean: 0.98167050
[2025-05-13 03:11:57,438]: Min: 0.92230773
[2025-05-13 03:11:57,438]: Max: 1.03659725
[2025-05-13 03:11:57,439]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-13 03:11:57,439]: Sample Values (25 elements): [0.0, -0.03632714971899986, 0.03632714971899986, 0.03632714971899986, 0.03632714971899986, 0.03632714971899986, -0.03632714971899986, -0.03632714971899986, 0.0, 0.0, 0.0, 0.0, -0.03632714971899986, 0.03632714971899986, -0.03632714971899986, 0.03632714971899986, -0.03632714971899986, 0.0, -0.03632714971899986, 0.03632714971899986, -0.03632714971899986, 0.03632714971899986, -0.03632714971899986, 0.0, 0.0]
[2025-05-13 03:11:57,440]: Mean: 0.00011628
[2025-05-13 03:11:57,440]: Min: -0.14530860
[2025-05-13 03:11:57,440]: Max: 0.10898145
[2025-05-13 03:11:57,440]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,440]: Sample Values (25 elements): [0.9394316673278809, 0.9592793583869934, 0.9644290208816528, 0.9411339163780212, 0.9426743984222412, 0.9532919526100159, 0.9854946732521057, 0.9525741934776306, 0.9698885083198547, 0.9589330554008484, 0.9513900279998779, 0.9529895186424255, 0.9530471563339233, 0.9330573678016663, 0.9408524036407471, 0.9331243634223938, 0.9573294520378113, 0.9696232676506042, 0.9852323532104492, 0.9610735774040222, 0.9399567246437073, 0.9665406942367554, 0.9199121594429016, 0.9503795504570007, 0.9598552584648132]
[2025-05-13 03:11:57,440]: Mean: 0.95682889
[2025-05-13 03:11:57,441]: Min: 0.91991216
[2025-05-13 03:11:57,441]: Max: 1.07298648
[2025-05-13 03:11:57,442]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 03:11:57,443]: Sample Values (25 elements): [0.03034551814198494, -0.03034551814198494, -0.03034551814198494, 0.0, 0.06069103628396988, -0.03034551814198494, 0.0, 0.0, 0.03034551814198494, 0.09103655815124512, 0.03034551814198494, 0.06069103628396988, -0.03034551814198494, 0.03034551814198494, 0.0, 0.03034551814198494, -0.03034551814198494, -0.03034551814198494, 0.03034551814198494, 0.0, 0.0, -0.03034551814198494, 0.03034551814198494, 0.0, 0.03034551814198494]
[2025-05-13 03:11:57,443]: Mean: 0.00025107
[2025-05-13 03:11:57,443]: Min: -0.12138207
[2025-05-13 03:11:57,443]: Max: 0.09103656
[2025-05-13 03:11:57,443]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,444]: Sample Values (25 elements): [0.9972189664840698, 0.9719420075416565, 0.9424954652786255, 0.9909123778343201, 0.9705467224121094, 0.9376386404037476, 0.9415785670280457, 0.9826783537864685, 0.938507080078125, 0.9757181406021118, 0.9878694415092468, 0.9404479265213013, 0.9758257865905762, 0.9785681962966919, 0.9716799855232239, 0.9716077446937561, 0.9552083015441895, 0.9701166749000549, 0.9520905017852783, 0.9516294002532959, 0.9662320017814636, 0.9866266250610352, 0.9882381558418274, 0.9782534241676331, 0.9719878435134888]
[2025-05-13 03:11:57,444]: Mean: 0.97615623
[2025-05-13 03:11:57,444]: Min: 0.91355968
[2025-05-13 03:11:57,444]: Max: 1.03048050
[2025-05-13 03:11:57,445]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-13 03:11:57,445]: Sample Values (25 elements): [-0.07154082506895065, 0.07154082506895065, 0.1430816501379013, -0.07154082506895065, -0.07154082506895065, 0.07154082506895065, -0.1430816501379013, 0.0, 0.0, 0.21462246775627136, 0.07154082506895065, 0.07154082506895065, 0.07154082506895065, -0.07154082506895065, 0.0, 0.0, 0.1430816501379013, 0.0, -0.1430816501379013, 0.1430816501379013, 0.1430816501379013, 0.0, 0.07154082506895065, -0.1430816501379013, -0.1430816501379013]
[2025-05-13 03:11:57,445]: Mean: -0.00031439
[2025-05-13 03:11:57,446]: Min: -0.21462247
[2025-05-13 03:11:57,446]: Max: 0.28616330
[2025-05-13 03:11:57,446]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,446]: Sample Values (25 elements): [0.9273827075958252, 0.8946731090545654, 0.9291490316390991, 0.8910272717475891, 0.8969926834106445, 0.922246515750885, 0.9121901988983154, 0.918347179889679, 0.9001001715660095, 0.9308511018753052, 0.9415125846862793, 0.9113392233848572, 0.8895850777626038, 0.88775235414505, 0.9035108685493469, 0.9133409857749939, 0.9114027619361877, 0.9268507957458496, 0.9156097173690796, 0.8513723611831665, 0.9333748817443848, 0.9158195853233337, 0.9391410946846008, 0.9235706925392151, 0.8964625000953674]
[2025-05-13 03:11:57,446]: Mean: 0.90736729
[2025-05-13 03:11:57,446]: Min: 0.85137236
[2025-05-13 03:11:57,447]: Max: 0.95084745
[2025-05-13 03:11:57,448]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 03:11:57,448]: Sample Values (25 elements): [0.03244655206799507, -0.03244655206799507, 0.03244655206799507, 0.03244655206799507, 0.03244655206799507, 0.03244655206799507, 0.0, 0.06489310413599014, 0.09733965992927551, 0.06489310413599014, -0.03244655206799507, -0.03244655206799507, 0.06489310413599014, -0.03244655206799507, 0.03244655206799507, 0.03244655206799507, 0.03244655206799507, 0.0, 0.0, -0.03244655206799507, 0.03244655206799507, 0.03244655206799507, -0.03244655206799507, -0.03244655206799507, -0.03244655206799507]
[2025-05-13 03:11:57,448]: Mean: 0.00007041
[2025-05-13 03:11:57,448]: Min: -0.09733966
[2025-05-13 03:11:57,449]: Max: 0.12978621
[2025-05-13 03:11:57,449]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,449]: Sample Values (25 elements): [0.9957923889160156, 0.9748480916023254, 0.9585208296775818, 1.000484824180603, 0.9685332179069519, 0.9604119658470154, 0.9677074551582336, 0.9741109609603882, 0.9522644281387329, 0.9686956405639648, 0.9658629894256592, 0.9717376232147217, 0.9643715620040894, 0.9562752842903137, 0.9908991456031799, 0.9579219222068787, 0.9744133949279785, 0.9490048289299011, 0.9778012037277222, 0.9862673282623291, 0.9538290500640869, 0.993964672088623, 0.954201340675354, 0.9567334055900574, 0.9453980922698975]
[2025-05-13 03:11:57,449]: Mean: 0.96374106
[2025-05-13 03:11:57,450]: Min: 0.93578511
[2025-05-13 03:11:57,450]: Max: 1.00640333
[2025-05-13 03:11:57,451]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 03:11:57,451]: Sample Values (25 elements): [0.0, 0.0, -0.03119114227592945, 0.0, -0.03119114227592945, -0.03119114227592945, 0.0, -0.03119114227592945, 0.03119114227592945, 0.0, 0.03119114227592945, 0.0, 0.0, 0.0, 0.0935734286904335, -0.03119114227592945, -0.03119114227592945, 0.0, 0.03119114227592945, 0.0623822845518589, -0.0623822845518589, 0.03119114227592945, -0.03119114227592945, 0.0, -0.03119114227592945]
[2025-05-13 03:11:57,452]: Mean: -0.00017176
[2025-05-13 03:11:57,452]: Min: -0.12476457
[2025-05-13 03:11:57,452]: Max: 0.09357343
[2025-05-13 03:11:57,452]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,452]: Sample Values (25 elements): [0.9987056851387024, 0.9967382550239563, 0.9856610894203186, 0.9758670926094055, 0.979415774345398, 0.9943634867668152, 1.02782142162323, 0.973199725151062, 1.0319277048110962, 0.9596096277236938, 0.9814900159835815, 1.0258530378341675, 1.042096495628357, 0.9745847582817078, 1.0061275959014893, 0.9441272616386414, 0.9871186017990112, 0.9868611693382263, 0.9456373453140259, 0.9705148935317993, 0.9734014272689819, 0.9572762250900269, 0.9740422964096069, 0.9662966132164001, 1.0135564804077148]
[2025-05-13 03:11:57,452]: Mean: 0.98449159
[2025-05-13 03:11:57,453]: Min: 0.94412726
[2025-05-13 03:11:57,453]: Max: 1.04209650
[2025-05-13 03:11:57,454]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 03:11:57,454]: Sample Values (25 elements): [0.0, -0.02850920334458351, -0.02850920334458351, -0.02850920334458351, -0.02850920334458351, -0.02850920334458351, -0.02850920334458351, 0.0, 0.0, 0.0, 0.0, 0.02850920334458351, 0.0, -0.02850920334458351, 0.02850920334458351, 0.05701840668916702, 0.02850920334458351, -0.02850920334458351, 0.0, 0.0, 0.02850920334458351, 0.0, -0.02850920334458351, 0.02850920334458351, -0.02850920334458351]
[2025-05-13 03:11:57,454]: Mean: 0.00010518
[2025-05-13 03:11:57,455]: Min: -0.11403681
[2025-05-13 03:11:57,455]: Max: 0.08552761
[2025-05-13 03:11:57,455]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,455]: Sample Values (25 elements): [0.9564282894134521, 0.9689804315567017, 0.9552661180496216, 0.9611363410949707, 0.9814764261245728, 0.985217273235321, 0.9515860676765442, 0.9599889516830444, 0.9920387864112854, 0.9727720022201538, 0.9709544777870178, 0.9585885405540466, 0.9665974974632263, 0.9784240126609802, 0.9716507792472839, 0.9723324179649353, 0.9769429564476013, 0.9716300368309021, 0.9616100788116455, 0.9548109769821167, 0.9696826934814453, 0.964724600315094, 0.9614812731742859, 0.9622722864151001, 0.9716712236404419]
[2025-05-13 03:11:57,455]: Mean: 0.96773744
[2025-05-13 03:11:57,455]: Min: 0.95134747
[2025-05-13 03:11:57,456]: Max: 0.99509287
[2025-05-13 03:11:57,456]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 03:11:57,457]: Sample Values (25 elements): [-0.022297115996479988, -0.022297115996479988, 0.022297115996479988, -0.022297115996479988, -0.022297115996479988, 0.0, -0.022297115996479988, 0.0, 0.022297115996479988, 0.0, -0.022297115996479988, 0.022297115996479988, -0.022297115996479988, 0.022297115996479988, -0.044594231992959976, -0.044594231992959976, 0.0, -0.022297115996479988, -0.022297115996479988, 0.044594231992959976, 0.022297115996479988, 0.044594231992959976, 0.022297115996479988, -0.022297115996479988, 0.0]
[2025-05-13 03:11:57,457]: Mean: 0.00007077
[2025-05-13 03:11:57,457]: Min: -0.06689135
[2025-05-13 03:11:57,457]: Max: 0.08918846
[2025-05-13 03:11:57,457]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-13 03:11:57,458]: Sample Values (25 elements): [1.0463855266571045, 1.0138581991195679, 1.0615464448928833, 1.0250812768936157, 1.0513248443603516, 1.0044409036636353, 1.0378180742263794, 1.0557507276535034, 1.0322948694229126, 1.0600957870483398, 1.0247639417648315, 1.0535813570022583, 1.0266737937927246, 1.05330228805542, 1.0771843194961548, 1.056292176246643, 1.0187478065490723, 1.0420286655426025, 1.0258249044418335, 1.0355820655822754, 1.0338943004608154, 1.0323230028152466, 1.0347951650619507, 1.0354485511779785, 1.052327036857605]
[2025-05-13 03:11:57,458]: Mean: 1.03931689
[2025-05-13 03:11:57,458]: Min: 1.00444090
[2025-05-13 03:11:57,458]: Max: 1.07718432
[2025-05-13 03:11:57,458]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-13 03:11:57,459]: Sample Values (25 elements): [0.04061014577746391, -0.09230520576238632, -0.15901005268096924, 0.02191341668367386, 0.1144438087940216, 0.20939812064170837, -0.30607905983924866, -0.2237464338541031, -0.1916424185037613, 0.12933602929115295, -0.056861184537410736, 0.3085936903953552, -0.041097745299339294, 0.008604574017226696, 0.17686963081359863, -0.14066116511821747, 0.36805978417396545, 0.11006747931241989, 0.0020189560018479824, -0.01966300792992115, -0.25393950939178467, 0.017950529232621193, -0.05555683374404907, -0.2630893886089325, -0.1815226972103119]
[2025-05-13 03:11:57,459]: Mean: 0.00512980
[2025-05-13 03:11:57,459]: Min: -0.39212629
[2025-05-13 03:11:57,459]: Max: 0.43697667
[2025-05-13 03:11:57,459]: 


[2025-05-13 05:36:15,379]: 


QAT of ResNet20 with parametrized_hardtanh down to 2 bits...
[2025-05-13 05:36:15,619]: [ResNet20_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-05-13 05:36:15,776]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-13 05:37:18,454]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.1894 Train Acc: 0.5847 Eval Loss: 1.6107 Eval Acc: 0.5071 (LR: 0.001000)
[2025-05-13 05:38:16,458]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 1.0123 Train Acc: 0.6409 Eval Loss: 2.3261 Eval Acc: 0.3701 (LR: 0.001000)
[2025-05-13 05:39:17,065]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.9764 Train Acc: 0.6562 Eval Loss: 1.9621 Eval Acc: 0.4185 (LR: 0.001000)
[2025-05-13 05:40:17,165]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.9887 Train Acc: 0.6523 Eval Loss: 2.0175 Eval Acc: 0.4322 (LR: 0.001000)
[2025-05-13 05:41:12,306]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.9848 Train Acc: 0.6522 Eval Loss: 1.9016 Eval Acc: 0.4469 (LR: 0.001000)
[2025-05-13 05:42:07,275]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.9769 Train Acc: 0.6542 Eval Loss: 3.6710 Eval Acc: 0.2567 (LR: 0.001000)
[2025-05-13 05:43:02,066]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.9706 Train Acc: 0.6567 Eval Loss: 1.5008 Eval Acc: 0.5104 (LR: 0.001000)
[2025-05-13 05:44:01,024]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.9560 Train Acc: 0.6608 Eval Loss: 1.2035 Eval Acc: 0.5854 (LR: 0.001000)
[2025-05-13 05:45:02,046]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.9456 Train Acc: 0.6656 Eval Loss: 1.3475 Eval Acc: 0.5387 (LR: 0.001000)
[2025-05-13 05:45:59,070]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.9406 Train Acc: 0.6667 Eval Loss: 2.8830 Eval Acc: 0.2761 (LR: 0.001000)
[2025-05-13 05:46:56,583]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.9376 Train Acc: 0.6693 Eval Loss: 2.7243 Eval Acc: 0.2816 (LR: 0.001000)
[2025-05-13 05:47:57,269]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.9033 Train Acc: 0.6807 Eval Loss: 1.8296 Eval Acc: 0.4842 (LR: 0.001000)
[2025-05-13 05:48:56,800]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.8923 Train Acc: 0.6854 Eval Loss: 1.5960 Eval Acc: 0.5078 (LR: 0.001000)
[2025-05-13 05:49:56,798]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.8917 Train Acc: 0.6875 Eval Loss: 1.6601 Eval Acc: 0.4879 (LR: 0.001000)
[2025-05-13 05:50:56,667]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.8756 Train Acc: 0.6910 Eval Loss: 1.0698 Eval Acc: 0.6363 (LR: 0.001000)
[2025-05-13 05:51:56,555]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.8721 Train Acc: 0.6919 Eval Loss: 1.1400 Eval Acc: 0.6271 (LR: 0.001000)
[2025-05-13 05:52:54,843]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.8630 Train Acc: 0.6983 Eval Loss: 1.0658 Eval Acc: 0.6433 (LR: 0.001000)
[2025-05-13 05:53:49,856]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.8588 Train Acc: 0.6989 Eval Loss: 1.3996 Eval Acc: 0.5795 (LR: 0.001000)
[2025-05-13 05:54:45,067]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.8577 Train Acc: 0.6985 Eval Loss: 1.2978 Eval Acc: 0.5942 (LR: 0.001000)
[2025-05-13 05:55:40,396]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.8691 Train Acc: 0.6940 Eval Loss: 2.2864 Eval Acc: 0.4094 (LR: 0.001000)
[2025-05-13 05:56:35,354]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.8601 Train Acc: 0.6953 Eval Loss: 1.5783 Eval Acc: 0.5193 (LR: 0.001000)
[2025-05-13 05:57:34,816]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.8518 Train Acc: 0.7024 Eval Loss: 0.9519 Eval Acc: 0.6734 (LR: 0.001000)
[2025-05-13 05:58:29,796]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.8504 Train Acc: 0.7007 Eval Loss: 1.3433 Eval Acc: 0.5802 (LR: 0.001000)
[2025-05-13 05:59:29,413]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.8538 Train Acc: 0.7009 Eval Loss: 1.2757 Eval Acc: 0.5888 (LR: 0.001000)
[2025-05-13 06:00:27,424]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.8477 Train Acc: 0.7014 Eval Loss: 1.9508 Eval Acc: 0.4301 (LR: 0.001000)
[2025-05-13 06:01:26,058]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.8333 Train Acc: 0.7087 Eval Loss: 1.7126 Eval Acc: 0.4898 (LR: 0.001000)
[2025-05-13 06:02:26,320]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.8374 Train Acc: 0.7042 Eval Loss: 2.5144 Eval Acc: 0.4074 (LR: 0.001000)
[2025-05-13 06:03:25,484]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.8198 Train Acc: 0.7101 Eval Loss: 2.2798 Eval Acc: 0.4434 (LR: 0.001000)
[2025-05-13 06:04:24,520]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.8263 Train Acc: 0.7090 Eval Loss: 1.3952 Eval Acc: 0.5500 (LR: 0.001000)
[2025-05-13 06:05:24,030]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.8148 Train Acc: 0.7121 Eval Loss: 1.2882 Eval Acc: 0.5867 (LR: 0.000100)
[2025-05-13 06:06:23,529]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.7676 Train Acc: 0.7315 Eval Loss: 1.0106 Eval Acc: 0.6560 (LR: 0.000100)
[2025-05-13 06:07:25,575]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.7451 Train Acc: 0.7389 Eval Loss: 2.6460 Eval Acc: 0.3858 (LR: 0.000100)
[2025-05-13 06:08:24,425]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.7341 Train Acc: 0.7449 Eval Loss: 1.9954 Eval Acc: 0.4726 (LR: 0.000100)
[2025-05-13 06:09:20,929]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.7245 Train Acc: 0.7466 Eval Loss: 1.2776 Eval Acc: 0.5759 (LR: 0.000100)
[2025-05-13 06:10:17,707]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.7223 Train Acc: 0.7475 Eval Loss: 1.1130 Eval Acc: 0.6452 (LR: 0.000100)
[2025-05-13 06:11:15,692]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.7240 Train Acc: 0.7457 Eval Loss: 0.8398 Eval Acc: 0.7110 (LR: 0.000100)
[2025-05-13 06:12:16,058]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.7334 Train Acc: 0.7443 Eval Loss: 0.7824 Eval Acc: 0.7304 (LR: 0.000100)
[2025-05-13 06:13:19,676]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.7263 Train Acc: 0.7467 Eval Loss: 1.0781 Eval Acc: 0.6160 (LR: 0.000100)
[2025-05-13 06:14:18,817]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.7280 Train Acc: 0.7451 Eval Loss: 1.1227 Eval Acc: 0.6247 (LR: 0.000100)
[2025-05-13 06:15:22,039]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.7247 Train Acc: 0.7484 Eval Loss: 1.3119 Eval Acc: 0.5926 (LR: 0.000100)
[2025-05-13 06:16:19,144]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.7281 Train Acc: 0.7463 Eval Loss: 1.3055 Eval Acc: 0.5603 (LR: 0.000100)
[2025-05-13 06:17:14,487]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.7348 Train Acc: 0.7434 Eval Loss: 0.9977 Eval Acc: 0.6538 (LR: 0.000100)
[2025-05-13 06:18:09,742]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.7295 Train Acc: 0.7442 Eval Loss: 1.0686 Eval Acc: 0.6344 (LR: 0.000100)
[2025-05-13 06:19:07,774]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.7301 Train Acc: 0.7439 Eval Loss: 1.2509 Eval Acc: 0.5877 (LR: 0.000100)
[2025-05-13 06:20:03,892]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.7267 Train Acc: 0.7446 Eval Loss: 0.9685 Eval Acc: 0.6653 (LR: 0.000010)
[2025-05-13 06:20:59,263]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.7028 Train Acc: 0.7540 Eval Loss: 1.1498 Eval Acc: 0.6077 (LR: 0.000010)
[2025-05-13 06:21:54,644]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.6886 Train Acc: 0.7585 Eval Loss: 1.2375 Eval Acc: 0.6006 (LR: 0.000010)
[2025-05-13 06:22:52,682]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.6899 Train Acc: 0.7596 Eval Loss: 0.6949 Eval Acc: 0.7593 (LR: 0.000010)
[2025-05-13 06:23:51,257]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.6902 Train Acc: 0.7592 Eval Loss: 1.1561 Eval Acc: 0.6090 (LR: 0.000010)
[2025-05-13 06:24:50,300]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.6923 Train Acc: 0.7578 Eval Loss: 1.1455 Eval Acc: 0.6407 (LR: 0.000010)
[2025-05-13 06:25:49,780]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.6866 Train Acc: 0.7586 Eval Loss: 0.9647 Eval Acc: 0.6891 (LR: 0.000010)
[2025-05-13 06:26:46,701]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.6881 Train Acc: 0.7597 Eval Loss: 1.2067 Eval Acc: 0.6087 (LR: 0.000010)
[2025-05-13 06:27:42,011]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.6848 Train Acc: 0.7597 Eval Loss: 1.2019 Eval Acc: 0.5985 (LR: 0.000010)
[2025-05-13 06:28:38,630]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.6899 Train Acc: 0.7579 Eval Loss: 0.8143 Eval Acc: 0.7206 (LR: 0.000010)
[2025-05-13 06:29:37,007]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.6891 Train Acc: 0.7583 Eval Loss: 1.4675 Eval Acc: 0.5474 (LR: 0.000010)
[2025-05-13 06:30:35,608]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.6917 Train Acc: 0.7566 Eval Loss: 0.8640 Eval Acc: 0.7034 (LR: 0.000010)
[2025-05-13 06:31:31,264]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.6991 Train Acc: 0.7521 Eval Loss: 0.8010 Eval Acc: 0.7200 (LR: 0.000010)
[2025-05-13 06:32:30,635]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.6971 Train Acc: 0.7556 Eval Loss: 0.7248 Eval Acc: 0.7459 (LR: 0.000010)
[2025-05-13 06:33:28,659]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.6913 Train Acc: 0.7581 Eval Loss: 1.4794 Eval Acc: 0.5530 (LR: 0.000010)
[2025-05-13 06:34:24,757]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.6920 Train Acc: 0.7584 Eval Loss: 0.9530 Eval Acc: 0.6815 (LR: 0.000010)
[2025-05-13 06:34:24,758]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.7593
[2025-05-13 06:34:24,828]: 


Quantization of model down to 2 bits finished
[2025-05-13 06:34:24,829]: Model Architecture:
[2025-05-13 06:34:24,885]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0363], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5544697046279907, max_val=1.554449200630188)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3183], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43668365478515625, max_val=0.5183150768280029)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6674], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6683895587921143, max_val=0.33368468284606934)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2474], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3903810977935791, max_val=0.3519043028354645)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7203], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0804989337921143, max_val=1.0804942846298218)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2486], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3523969054222107, max_val=0.39351963996887207)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6204], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5510437488555908, max_val=0.3102119565010071)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2252], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32135698199272156, max_val=0.35419100522994995)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8366], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41834181547164917, max_val=2.09151291847229)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2040], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3022632300853729, max_val=0.3098068833351135)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6762], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0142261981964111, max_val=1.0142368078231812)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1668], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2501721978187561, max_val=0.25018036365509033)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0595], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5892982482910156, max_val=1.589287281036377)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1468], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22561568021774292, max_val=0.21466782689094543)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8115], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8405036926269531, max_val=1.5941128730773926)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1054], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1585778295993805, max_val=0.15763211250305176)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2444], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38219398260116577, max_val=0.35107019543647766)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7931], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.189682960510254, max_val=1.189677357673645)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1095], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.164302796125412, max_val=0.16429629921913147)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6344], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9516205787658691, max_val=0.951624870300293)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1070], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16062326729297638, max_val=0.16024905443191528)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7379], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.106783151626587, max_val=1.106777310371399)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1013], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16416364908218384, max_val=0.13962942361831665)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6005], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3194359838962555, max_val=1.4821429252624512)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1058], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1578257828950882, max_val=0.15944808721542358)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1842], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.734969973564148, max_val=1.8176264762878418)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0934], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14015279710292816, max_val=0.1401534080505371)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6113], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30566418170928955, max_val=1.528313159942627)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0730], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11034007370471954, max_val=0.1086295023560524)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1674], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24594339728355408, max_val=0.2562924027442932)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7360], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1040170192718506, max_val=1.104023814201355)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0838], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12569361925125122, max_val=0.1258513331413269)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5221], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7831653356552124, max_val=0.7831616401672363)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0703], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10600405931472778, max_val=0.10496443510055542)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7548], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6012108325958252, max_val=0.6630719304084778)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0707], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10514061897993088, max_val=0.10688351839780807)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4663], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6993995904922485, max_val=0.6993993520736694)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0544], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08139319717884064, max_val=0.08171981573104858)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5791], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3601267337799072, max_val=2.3770642280578613)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-13 06:34:24,885]: 
Model Weights:
[2025-05-13 06:34:24,885]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-13 06:34:24,891]: Sample Values (25 elements): [-0.05204716697335243, -0.27261558175086975, 0.18477198481559753, -0.14090421795845032, 0.13422571122646332, -0.1802213042974472, 0.42192861437797546, 0.0644759014248848, -0.1631791889667511, -0.05509188026189804, 0.04788580164313316, 0.009714334271848202, -0.11876857280731201, 0.4253684878349304, -0.30960264801979065, -0.029394879937171936, -0.1019618809223175, 0.05123734101653099, 0.35703182220458984, 0.36046549677848816, -0.024586644023656845, -0.03629276901483536, 0.33729520440101624, -0.03381097689270973, 0.04171910136938095]
[2025-05-13 06:34:24,902]: Mean: 0.00660820
[2025-05-13 06:34:24,903]: Min: -0.61136764
[2025-05-13 06:34:24,903]: Max: 0.59590220
[2025-05-13 06:34:24,903]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,904]: Sample Values (16 elements): [1.4782507419586182, 1.0450371503829956, 0.986126184463501, 0.8885141611099243, 1.1575307846069336, 0.9315060973167419, 1.1728549003601074, 1.0299513339996338, 1.4363009929656982, 1.3774139881134033, 0.617409348487854, 1.3391802310943604, 1.396744728088379, 1.639691948890686, 1.2889913320541382, 1.5539684295654297]
[2025-05-13 06:34:24,904]: Mean: 1.20871699
[2025-05-13 06:34:24,904]: Min: 0.61740935
[2025-05-13 06:34:24,904]: Max: 1.63969195
[2025-05-13 06:34:24,905]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 06:34:24,906]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3183329999446869, 0.0, 0.0, 0.0, 0.0, -0.3183329999446869, 0.0, 0.0, 0.0, -0.3183329999446869, 0.0, 0.3183329999446869, 0.0, 0.0, 0.0]
[2025-05-13 06:34:24,906]: Mean: 0.00193431
[2025-05-13 06:34:24,906]: Min: -0.31833300
[2025-05-13 06:34:24,906]: Max: 0.63666600
[2025-05-13 06:34:24,906]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,906]: Sample Values (16 elements): [0.8397453427314758, 0.8178920745849609, 1.2810194492340088, 0.7837890982627869, 0.9875587821006775, 0.9328999519348145, 1.1281050443649292, 0.8994821906089783, 0.9403420090675354, 1.0124744176864624, 1.2428406476974487, 0.9044947028160095, 1.0523039102554321, 0.9115495085716248, 1.053594946861267, 1.299349069595337]
[2025-05-13 06:34:24,906]: Mean: 1.00546503
[2025-05-13 06:34:24,907]: Min: 0.78378910
[2025-05-13 06:34:24,907]: Max: 1.29934907
[2025-05-13 06:34:24,908]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 06:34:24,908]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.24742847681045532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24742847681045532, 0.0, 0.0, 0.24742847681045532, 0.0, 0.0, 0.0, 0.24742847681045532, 0.0, -0.24742847681045532, 0.0, 0.0, 0.0]
[2025-05-13 06:34:24,908]: Mean: 0.00139608
[2025-05-13 06:34:24,908]: Min: -0.49485695
[2025-05-13 06:34:24,909]: Max: 0.24742848
[2025-05-13 06:34:24,909]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,909]: Sample Values (16 elements): [1.1688703298568726, 0.9273837208747864, 0.7324208617210388, 0.7825599312782288, 0.6525421738624573, 0.892647922039032, 0.876801073551178, 0.692047119140625, 0.8084474205970764, 0.816118061542511, 0.5526488423347473, 0.6588342189788818, 0.7359269261360168, 0.8824020624160767, 0.9510337710380554, 0.9911967515945435]
[2025-05-13 06:34:24,909]: Mean: 0.82011759
[2025-05-13 06:34:24,909]: Min: 0.55264884
[2025-05-13 06:34:24,909]: Max: 1.16887033
[2025-05-13 06:34:24,910]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 06:34:24,911]: Sample Values (25 elements): [0.24863886833190918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24863886833190918, 0.24863886833190918, 0.0, 0.0, 0.24863886833190918, 0.24863886833190918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-13 06:34:24,911]: Mean: 0.00097125
[2025-05-13 06:34:24,911]: Min: -0.24863887
[2025-05-13 06:34:24,911]: Max: 0.49727774
[2025-05-13 06:34:24,911]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,911]: Sample Values (16 elements): [1.0317984819412231, 0.9023765921592712, 1.0356818437576294, 0.8321393132209778, 1.0041227340698242, 0.9339802265167236, 0.932009220123291, 1.141454815864563, 0.894881010055542, 0.8159257173538208, 1.0079468488693237, 0.9924454689025879, 1.0605618953704834, 0.9623942375183105, 1.0724378824234009, 1.1247825622558594]
[2025-05-13 06:34:24,912]: Mean: 0.98405862
[2025-05-13 06:34:24,912]: Min: 0.81592572
[2025-05-13 06:34:24,912]: Max: 1.14145482
[2025-05-13 06:34:24,913]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 06:34:24,913]: Sample Values (25 elements): [0.0, 0.2251826524734497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2251826524734497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2251826524734497, 0.0, -0.2251826524734497, 0.0, 0.0]
[2025-05-13 06:34:24,913]: Mean: -0.00254112
[2025-05-13 06:34:24,913]: Min: -0.22518265
[2025-05-13 06:34:24,913]: Max: 0.45036530
[2025-05-13 06:34:24,914]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,914]: Sample Values (16 elements): [0.9354736804962158, 0.894564688205719, 1.1037694215774536, 0.9320852160453796, 1.2259875535964966, 0.8841755390167236, 1.165360689163208, 1.0292901992797852, 0.9416278600692749, 0.803309977054596, 0.9707563519477844, 0.9413110613822937, 1.2131106853485107, 1.0051167011260986, 1.0468556880950928, 0.9169257283210754]
[2025-05-13 06:34:24,914]: Mean: 1.00060749
[2025-05-13 06:34:24,914]: Min: 0.80330998
[2025-05-13 06:34:24,914]: Max: 1.22598755
[2025-05-13 06:34:24,915]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 06:34:24,916]: Sample Values (25 elements): [-0.2040233612060547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2040233612060547, 0.0, -0.2040233612060547, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2040233612060547, 0.0, 0.0, 0.0, 0.2040233612060547, -0.2040233612060547, 0.2040233612060547, 0.0, 0.0, 0.2040233612060547, 0.0]
[2025-05-13 06:34:24,916]: Mean: 0.00504745
[2025-05-13 06:34:24,916]: Min: -0.20402336
[2025-05-13 06:34:24,916]: Max: 0.40804672
[2025-05-13 06:34:24,916]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,917]: Sample Values (16 elements): [1.0175358057022095, 0.9385143518447876, 1.0173903703689575, 0.9765321016311646, 1.162137508392334, 0.98448246717453, 1.1493401527404785, 0.9752433896064758, 0.9101743698120117, 0.9896211624145508, 0.9330477714538574, 1.044359564781189, 0.9250423908233643, 1.0403361320495605, 0.9735311269760132, 1.0815856456756592]
[2025-05-13 06:34:24,917]: Mean: 1.00742960
[2025-05-13 06:34:24,917]: Min: 0.91017437
[2025-05-13 06:34:24,917]: Max: 1.16213751
[2025-05-13 06:34:24,918]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-13 06:34:24,918]: Sample Values (25 elements): [-0.16678421199321747, -0.16678421199321747, -0.16678421199321747, 0.0, -0.16678421199321747, 0.0, -0.16678421199321747, 0.0, 0.0, -0.16678421199321747, 0.0, 0.0, 0.16678421199321747, -0.16678421199321747, 0.0, -0.16678421199321747, 0.0, 0.0, 0.0, -0.16678421199321747, -0.16678421199321747, 0.0, 0.0, 0.16678421199321747, 0.16678421199321747]
[2025-05-13 06:34:24,918]: Mean: -0.00217167
[2025-05-13 06:34:24,919]: Min: -0.16678421
[2025-05-13 06:34:24,919]: Max: 0.33356842
[2025-05-13 06:34:24,919]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-13 06:34:24,919]: Sample Values (16 elements): [0.9387022852897644, 0.8801219463348389, 1.122298240661621, 1.0490460395812988, 0.9565067887306213, 0.9215712547302246, 1.0346736907958984, 0.8948339223861694, 1.0341943502426147, 0.9330308437347412, 1.0821212530136108, 1.0983798503875732, 1.012452483177185, 1.0184221267700195, 0.989323616027832, 0.9239351749420166]
[2025-05-13 06:34:24,919]: Mean: 0.99310088
[2025-05-13 06:34:24,919]: Min: 0.88012195
[2025-05-13 06:34:24,919]: Max: 1.12229824
[2025-05-13 06:34:24,920]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-13 06:34:24,921]: Sample Values (25 elements): [0.1467612087726593, -0.1467612087726593, 0.1467612087726593, 0.1467612087726593, 0.1467612087726593, 0.0, 0.0, -0.1467612087726593, 0.0, 0.0, 0.1467612087726593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1467612087726593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1467612087726593, 0.1467612087726593]
[2025-05-13 06:34:24,921]: Mean: -0.00261164
[2025-05-13 06:34:24,921]: Min: -0.29352242
[2025-05-13 06:34:24,921]: Max: 0.14676121
[2025-05-13 06:34:24,921]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,921]: Sample Values (25 elements): [0.9047306180000305, 0.9874737858772278, 0.9585592150688171, 1.003001093864441, 0.8956024050712585, 0.9588860273361206, 1.005827784538269, 0.952423095703125, 1.050199031829834, 0.9107275605201721, 0.9358305931091309, 0.9766114950180054, 0.9901754260063171, 0.9532773494720459, 0.9701586961746216, 0.976605236530304, 0.9983974099159241, 0.9443367123603821, 1.0092555284500122, 0.9955874085426331, 0.9361554980278015, 0.9111403822898865, 0.9679515361785889, 0.9581519961357117, 0.9426284432411194]
[2025-05-13 06:34:24,922]: Mean: 0.96467710
[2025-05-13 06:34:24,922]: Min: 0.89560241
[2025-05-13 06:34:24,922]: Max: 1.05019903
[2025-05-13 06:34:24,923]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 06:34:24,923]: Sample Values (25 elements): [0.0, 0.1054033488035202, 0.0, 0.0, 0.1054033488035202, 0.0, 0.0, -0.1054033488035202, 0.0, 0.0, 0.1054033488035202, -0.1054033488035202, 0.0, 0.0, 0.0, -0.1054033488035202, 0.0, 0.0, 0.0, 0.1054033488035202, 0.1054033488035202, 0.0, 0.0, 0.0, -0.1054033488035202]
[2025-05-13 06:34:24,923]: Mean: 0.00107508
[2025-05-13 06:34:24,923]: Min: -0.21080670
[2025-05-13 06:34:24,924]: Max: 0.10540335
[2025-05-13 06:34:24,924]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,924]: Sample Values (25 elements): [0.94568932056427, 1.0647449493408203, 1.0234625339508057, 1.0093963146209717, 1.0437935590744019, 0.9209070205688477, 0.9365849494934082, 0.943366289138794, 0.9770721793174744, 0.9759207963943481, 1.0075397491455078, 1.0122523307800293, 1.0043480396270752, 1.0827763080596924, 1.006788969039917, 0.9411349296569824, 1.0472710132598877, 0.9168614149093628, 1.0750296115875244, 0.9948706030845642, 1.0482122898101807, 0.9772077798843384, 0.9878483414649963, 1.0048980712890625, 0.9962701797485352]
[2025-05-13 06:34:24,924]: Mean: 0.99481404
[2025-05-13 06:34:24,924]: Min: 0.91686141
[2025-05-13 06:34:24,924]: Max: 1.08277631
[2025-05-13 06:34:24,925]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-13 06:34:24,925]: Sample Values (25 elements): [-0.24442145228385925, -0.24442145228385925, 0.0, 0.24442145228385925, 0.0, -0.24442145228385925, 0.0, 0.24442145228385925, 0.24442145228385925, -0.24442145228385925, -0.24442145228385925, 0.24442145228385925, 0.24442145228385925, 0.0, 0.24442145228385925, -0.24442145228385925, 0.0, 0.0, -0.24442145228385925, -0.24442145228385925, 0.0, 0.0, 0.0, -0.24442145228385925, -0.24442145228385925]
[2025-05-13 06:34:24,926]: Mean: -0.00811556
[2025-05-13 06:34:24,926]: Min: -0.48884290
[2025-05-13 06:34:24,926]: Max: 0.24442145
[2025-05-13 06:34:24,926]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,926]: Sample Values (25 elements): [0.8514761328697205, 0.8215434551239014, 0.801596462726593, 0.8312176465988159, 0.7455760836601257, 0.8614276051521301, 0.7593002319335938, 0.8672757148742676, 0.804261326789856, 0.8208878636360168, 0.8010202646255493, 0.8304767608642578, 0.8233747482299805, 0.8316399455070496, 0.9151334762573242, 0.7944634556770325, 0.8774396181106567, 0.8172329068183899, 0.7906158566474915, 0.7743475437164307, 0.766573965549469, 0.8269343376159668, 0.8838696479797363, 0.8446330428123474, 0.7461767792701721]
[2025-05-13 06:34:24,926]: Mean: 0.82021827
[2025-05-13 06:34:24,927]: Min: 0.74557608
[2025-05-13 06:34:24,927]: Max: 0.91513348
[2025-05-13 06:34:24,928]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 06:34:24,928]: Sample Values (25 elements): [0.0, -0.10953303426504135, 0.10953303426504135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10953303426504135, 0.0, 0.0, 0.0, 0.10953303426504135, -0.10953303426504135, 0.0, 0.0, 0.0, 0.10953303426504135, 0.10953303426504135, 0.10953303426504135, 0.0]
[2025-05-13 06:34:24,928]: Mean: 0.00062991
[2025-05-13 06:34:24,928]: Min: -0.21906607
[2025-05-13 06:34:24,929]: Max: 0.10953303
[2025-05-13 06:34:24,929]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,929]: Sample Values (25 elements): [0.965317964553833, 0.9583994746208191, 0.9934954047203064, 0.9414925575256348, 0.9779636859893799, 0.9288685917854309, 1.0355732440948486, 0.9852327108383179, 0.9675478339195251, 0.990205705165863, 0.9422826766967773, 1.0170929431915283, 0.9315979480743408, 1.0373226404190063, 1.06671941280365, 1.0574190616607666, 0.9660207033157349, 0.9829018712043762, 0.9377625584602356, 0.9845206141471863, 0.9643415808677673, 0.9741776585578918, 1.030666708946228, 0.9367126822471619, 0.9896379709243774]
[2025-05-13 06:34:24,929]: Mean: 0.98381484
[2025-05-13 06:34:24,929]: Min: 0.92886859
[2025-05-13 06:34:24,929]: Max: 1.08768547
[2025-05-13 06:34:24,930]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 06:34:24,931]: Sample Values (25 elements): [0.10695740580558777, 0.0, 0.0, 0.0, -0.10695740580558777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10695740580558777, 0.0, 0.0, 0.10695740580558777, 0.0, 0.0, -0.10695740580558777, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10695740580558777, 0.0]
[2025-05-13 06:34:24,931]: Mean: 0.00095166
[2025-05-13 06:34:24,931]: Min: -0.21391481
[2025-05-13 06:34:24,931]: Max: 0.10695741
[2025-05-13 06:34:24,931]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,931]: Sample Values (25 elements): [0.9628564119338989, 1.0199939012527466, 0.9626550078392029, 0.9482086300849915, 1.043697714805603, 0.9175724387168884, 1.0139209032058716, 0.9719552397727966, 1.0049152374267578, 0.9969154596328735, 0.9326763153076172, 0.9392384886741638, 0.9293015599250793, 1.0461182594299316, 0.9763356447219849, 1.0727638006210327, 0.9356528520584106, 0.9456995129585266, 1.0230578184127808, 0.9441245794296265, 0.9560257196426392, 1.0397310256958008, 0.9442850947380066, 0.9862684607505798, 0.945286214351654]
[2025-05-13 06:34:24,931]: Mean: 0.97528934
[2025-05-13 06:34:24,932]: Min: 0.89519566
[2025-05-13 06:34:24,932]: Max: 1.07276380
[2025-05-13 06:34:24,933]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 06:34:24,933]: Sample Values (25 elements): [0.0, -0.10126431286334991, 0.0, 0.0, -0.10126431286334991, 0.10126431286334991, 0.0, 0.0, 0.0, -0.10126431286334991, 0.10126431286334991, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10126431286334991, 0.0]
[2025-05-13 06:34:24,933]: Mean: 0.00019778
[2025-05-13 06:34:24,933]: Min: -0.20252863
[2025-05-13 06:34:24,933]: Max: 0.10126431
[2025-05-13 06:34:24,934]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,934]: Sample Values (25 elements): [0.9258749485015869, 1.0372554063796997, 0.9616449475288391, 0.8891205191612244, 0.9465449452400208, 0.9694432020187378, 0.9506324529647827, 0.9646977186203003, 0.9536015391349792, 0.9781783819198608, 0.9666731357574463, 1.0232402086257935, 0.9628885984420776, 1.0377697944641113, 0.9797529578208923, 0.9650419354438782, 0.9910230040550232, 0.9587499499320984, 1.0056960582733154, 0.9298537969589233, 0.9465060830116272, 0.9573407173156738, 0.9821354746818542, 0.9913603067398071, 0.9841453433036804]
[2025-05-13 06:34:24,934]: Mean: 0.96555328
[2025-05-13 06:34:24,934]: Min: 0.88912052
[2025-05-13 06:34:24,934]: Max: 1.03776979
[2025-05-13 06:34:24,935]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-13 06:34:24,936]: Sample Values (25 elements): [0.0, 0.0, 0.10575791448354721, 0.0, -0.10575791448354721, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10575791448354721, 0.10575791448354721, 0.0, 0.0, -0.10575791448354721, 0.0, 0.0, 0.0, 0.10575791448354721, -0.10575791448354721, -0.10575791448354721, 0.0, 0.0, 0.0, -0.10575791448354721]
[2025-05-13 06:34:24,936]: Mean: 0.00021803
[2025-05-13 06:34:24,936]: Min: -0.10575791
[2025-05-13 06:34:24,936]: Max: 0.21151583
[2025-05-13 06:34:24,936]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-13 06:34:24,936]: Sample Values (25 elements): [1.052894115447998, 1.0411845445632935, 1.0296305418014526, 0.9848006367683411, 1.0216494798660278, 1.0030745267868042, 1.0122606754302979, 0.9964740872383118, 1.0240631103515625, 1.0218278169631958, 1.0051789283752441, 1.023302435874939, 0.9948858022689819, 1.0480178594589233, 1.0190744400024414, 1.0009368658065796, 0.9770788550376892, 1.0200138092041016, 0.9904256463050842, 1.0223976373672485, 1.061589241027832, 1.0379663705825806, 1.032442331314087, 1.079790472984314, 1.0759435892105103]
[2025-05-13 06:34:24,936]: Mean: 1.02186179
[2025-05-13 06:34:24,937]: Min: 0.97061831
[2025-05-13 06:34:24,937]: Max: 1.07979047
[2025-05-13 06:34:24,938]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-13 06:34:24,938]: Sample Values (25 elements): [0.09343540668487549, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09343540668487549, 0.09343540668487549, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09343540668487549, 0.0, 0.0, -0.09343540668487549, 0.0]
[2025-05-13 06:34:24,938]: Mean: 0.00017235
[2025-05-13 06:34:24,938]: Min: -0.09343541
[2025-05-13 06:34:24,939]: Max: 0.09343541
[2025-05-13 06:34:24,939]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,939]: Sample Values (25 elements): [0.9866988658905029, 0.9390466213226318, 0.9712503552436829, 0.9744316935539246, 0.9445172548294067, 0.9549434781074524, 0.9983332753181458, 0.952308177947998, 0.9924739003181458, 0.955539345741272, 0.9777138233184814, 0.9591954946517944, 0.9665711522102356, 0.9378904104232788, 0.9763127565383911, 0.9629696011543274, 0.9592267274856567, 0.9430162906646729, 0.9547683596611023, 0.939706027507782, 0.9572340250015259, 0.9305570721626282, 0.9542270302772522, 0.9545763731002808, 0.9641467332839966]
[2025-05-13 06:34:24,939]: Mean: 0.96347213
[2025-05-13 06:34:24,939]: Min: 0.91977668
[2025-05-13 06:34:24,939]: Max: 1.07690978
[2025-05-13 06:34:24,940]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 06:34:24,941]: Sample Values (25 elements): [0.0, 0.07298984378576279, 0.0, 0.0, 0.0, -0.07298984378576279, 0.0, 0.0, 0.0, 0.07298984378576279, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07298984378576279]
[2025-05-13 06:34:24,941]: Mean: 0.00055835
[2025-05-13 06:34:24,941]: Min: -0.14597969
[2025-05-13 06:34:24,941]: Max: 0.07298984
[2025-05-13 06:34:24,941]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,942]: Sample Values (25 elements): [1.0142340660095215, 1.0500524044036865, 0.9793517589569092, 0.9925607442855835, 1.008288025856018, 1.0209684371948242, 0.9905306100845337, 1.0047986507415771, 0.960755467414856, 0.9965669512748718, 0.9896872043609619, 0.9659368991851807, 1.0205211639404297, 0.9872912168502808, 1.0023623704910278, 0.9717472791671753, 1.0037685632705688, 0.9867506623268127, 0.9971064925193787, 0.9733431339263916, 1.0044574737548828, 0.9747377038002014, 0.9867059588432312, 0.9892401695251465, 1.058021903038025]
[2025-05-13 06:34:24,942]: Mean: 0.99285603
[2025-05-13 06:34:24,942]: Min: 0.93431270
[2025-05-13 06:34:24,942]: Max: 1.05802190
[2025-05-13 06:34:24,943]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-13 06:34:24,943]: Sample Values (25 elements): [0.0, 0.0, 0.16741196811199188, 0.0, -0.16741196811199188, 0.16741196811199188, 0.16741196811199188, 0.0, -0.16741196811199188, -0.16741196811199188, -0.16741196811199188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16741196811199188, -0.16741196811199188, -0.16741196811199188, -0.16741196811199188, -0.16741196811199188, 0.0, 0.0, 0.0, -0.16741196811199188]
[2025-05-13 06:34:24,943]: Mean: -0.00261581
[2025-05-13 06:34:24,943]: Min: -0.16741197
[2025-05-13 06:34:24,944]: Max: 0.33482394
[2025-05-13 06:34:24,944]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,944]: Sample Values (25 elements): [0.9156243205070496, 0.903005063533783, 0.9289120435714722, 0.9430814981460571, 0.8789495825767517, 0.9193082451820374, 0.8695999383926392, 0.9153579473495483, 0.926760196685791, 0.8835321068763733, 0.915386974811554, 0.8953953981399536, 0.9093698263168335, 0.8807624578475952, 0.9154089093208313, 0.9542580842971802, 0.8829774856567383, 0.8631763458251953, 0.9039126038551331, 0.9176607728004456, 0.883018970489502, 0.8751612305641174, 0.9121401309967041, 0.9241570234298706, 0.9323461651802063]
[2025-05-13 06:34:24,944]: Mean: 0.90013748
[2025-05-13 06:34:24,944]: Min: 0.82218474
[2025-05-13 06:34:24,944]: Max: 0.95425808
[2025-05-13 06:34:24,945]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 06:34:24,946]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.08384829759597778, 0.0, 0.0, 0.0, 0.08384829759597778, 0.0, -0.08384829759597778, 0.0, -0.08384829759597778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08384829759597778, 0.0, 0.08384829759597778, -0.08384829759597778]
[2025-05-13 06:34:24,946]: Mean: 0.00006141
[2025-05-13 06:34:24,946]: Min: -0.08384830
[2025-05-13 06:34:24,946]: Max: 0.16769660
[2025-05-13 06:34:24,946]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,947]: Sample Values (25 elements): [0.9909082651138306, 0.97512286901474, 1.002880334854126, 0.9826680421829224, 0.9462912678718567, 0.9523836970329285, 0.968559980392456, 0.9507500529289246, 0.9948104619979858, 0.9935621023178101, 0.9802615642547607, 1.0102452039718628, 0.9674522280693054, 1.0305954217910767, 0.9888255000114441, 0.9684535264968872, 0.9778183102607727, 0.9674605131149292, 0.9809778928756714, 0.9632028937339783, 0.9955710768699646, 0.9566183686256409, 0.9694669246673584, 0.9981933832168579, 0.9739634394645691]
[2025-05-13 06:34:24,947]: Mean: 0.97739983
[2025-05-13 06:34:24,947]: Min: 0.94286180
[2025-05-13 06:34:24,947]: Max: 1.03059542
[2025-05-13 06:34:24,948]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 06:34:24,949]: Sample Values (25 elements): [0.0, -0.07032284140586853, 0.0, 0.07032284140586853, 0.0, 0.0, 0.07032284140586853, 0.0, 0.07032284140586853, -0.07032284140586853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07032284140586853, 0.0, 0.0, 0.0, -0.07032284140586853, 0.0, -0.07032284140586853, 0.0]
[2025-05-13 06:34:24,949]: Mean: -0.00023082
[2025-05-13 06:34:24,949]: Min: -0.14064568
[2025-05-13 06:34:24,949]: Max: 0.07032284
[2025-05-13 06:34:24,949]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,949]: Sample Values (25 elements): [1.0704658031463623, 1.0224416255950928, 0.9625692367553711, 0.9874036312103271, 1.0043277740478516, 0.9584459066390991, 0.9717637300491333, 0.9876791834831238, 0.9609288573265076, 1.0007433891296387, 1.0156018733978271, 0.9953033328056335, 0.9698414206504822, 1.0156261920928955, 1.0167592763900757, 0.9476785063743591, 0.9996718764305115, 1.0091899633407593, 1.0027713775634766, 0.9732555747032166, 0.9936283826828003, 0.9882648587226868, 1.0057848691940308, 0.9689083099365234, 0.9831637740135193]
[2025-05-13 06:34:24,949]: Mean: 0.99763137
[2025-05-13 06:34:24,950]: Min: 0.94767851
[2025-05-13 06:34:24,950]: Max: 1.07161534
[2025-05-13 06:34:24,951]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 06:34:24,951]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07067470252513885, 0.0, -0.07067470252513885, 0.0, 0.0, 0.07067470252513885, 0.07067470252513885, -0.07067470252513885, 0.07067470252513885, 0.0, 0.0, 0.0, 0.07067470252513885]
[2025-05-13 06:34:24,951]: Mean: 0.00011503
[2025-05-13 06:34:24,951]: Min: -0.07067470
[2025-05-13 06:34:24,952]: Max: 0.14134941
[2025-05-13 06:34:24,952]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,952]: Sample Values (25 elements): [0.9632717967033386, 0.9715454578399658, 1.0064388513565063, 0.9690356254577637, 0.9610185623168945, 0.9570093154907227, 0.9776289463043213, 0.9922471642494202, 0.9810752272605896, 0.9583905935287476, 0.9694819450378418, 0.9568607211112976, 0.9788361191749573, 0.9830489158630371, 0.9638797640800476, 0.9569740295410156, 0.98133385181427, 0.9846783876419067, 0.9812068939208984, 0.9774808287620544, 0.970824122428894, 0.9997159242630005, 0.9784830212593079, 0.9830212593078613, 0.9599901437759399]
[2025-05-13 06:34:24,952]: Mean: 0.97338861
[2025-05-13 06:34:24,952]: Min: 0.95380831
[2025-05-13 06:34:24,952]: Max: 1.00643885
[2025-05-13 06:34:24,953]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-13 06:34:24,954]: Sample Values (25 elements): [-0.05437099188566208, 0.0, 0.0, 0.0, 0.05437099188566208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05437099188566208, 0.0, 0.0, -0.05437099188566208, 0.0, 0.0, 0.0, 0.0, -0.05437099188566208, 0.0, 0.0, 0.0, 0.05437099188566208, 0.0]
[2025-05-13 06:34:24,954]: Mean: 0.00003540
[2025-05-13 06:34:24,954]: Min: -0.05437099
[2025-05-13 06:34:24,954]: Max: 0.10874198
[2025-05-13 06:34:24,954]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-13 06:34:24,955]: Sample Values (25 elements): [1.02293860912323, 1.0064215660095215, 1.0381596088409424, 1.0165677070617676, 1.0116997957229614, 1.0448007583618164, 1.0284641981124878, 1.045057773590088, 1.039979338645935, 1.047019600868225, 1.0486913919448853, 1.014793038368225, 1.0066214799880981, 1.024827003479004, 1.0280451774597168, 1.0423449277877808, 0.9999110698699951, 1.043308138847351, 1.0223712921142578, 1.016478419303894, 1.051116704940796, 1.031678318977356, 1.028242588043213, 1.0234912633895874, 1.0098564624786377]
[2025-05-13 06:34:24,955]: Mean: 1.03083873
[2025-05-13 06:34:24,955]: Min: 0.99991107
[2025-05-13 06:34:24,955]: Max: 1.07051909
[2025-05-13 06:34:24,955]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-13 06:34:24,955]: Sample Values (25 elements): [-0.3395962715148926, 0.16215918958187103, 0.27373257279396057, -0.27722352743148804, -0.28792664408683777, 0.03599780425429344, -0.05679274722933769, -0.19658438861370087, -0.11161667108535767, -0.22887320816516876, -0.03161724656820297, 0.051688794046640396, 0.09071829169988632, -0.232259601354599, -0.0750509575009346, -0.06709977984428406, -0.18478544056415558, -0.32958710193634033, -0.2296893149614334, 0.04635903984308243, -0.23476442694664001, -0.14400698244571686, 0.250652015209198, 0.10040197521448135, 0.19325846433639526]
[2025-05-13 06:34:24,956]: Mean: 0.00513596
[2025-05-13 06:34:24,956]: Min: -0.36964443
[2025-05-13 06:34:24,956]: Max: 0.40752020
