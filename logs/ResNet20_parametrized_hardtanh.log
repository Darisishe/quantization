[2025-06-13 18:42:46,724]: 
Training ResNet20 with parametrized_hardtanh
[2025-06-13 18:43:41,319]: [ResNet20_parametrized_hardtanh] Epoch: 001 Train Loss: 1.7934 Train Acc: 0.3349 Eval Loss: 1.5406 Eval Acc: 0.4280 (LR: 0.00100000)
[2025-06-13 18:44:27,480]: [ResNet20_parametrized_hardtanh] Epoch: 002 Train Loss: 1.4395 Train Acc: 0.4772 Eval Loss: 1.2802 Eval Acc: 0.5337 (LR: 0.00100000)
[2025-06-13 18:45:09,137]: [ResNet20_parametrized_hardtanh] Epoch: 003 Train Loss: 1.2688 Train Acc: 0.5440 Eval Loss: 1.2118 Eval Acc: 0.5615 (LR: 0.00100000)
[2025-06-13 18:45:49,989]: [ResNet20_parametrized_hardtanh] Epoch: 004 Train Loss: 1.1559 Train Acc: 0.5844 Eval Loss: 1.1839 Eval Acc: 0.5779 (LR: 0.00100000)
[2025-06-13 18:46:34,871]: [ResNet20_parametrized_hardtanh] Epoch: 005 Train Loss: 1.0854 Train Acc: 0.6129 Eval Loss: 1.1204 Eval Acc: 0.5922 (LR: 0.00100000)
[2025-06-13 18:47:24,766]: [ResNet20_parametrized_hardtanh] Epoch: 006 Train Loss: 1.0285 Train Acc: 0.6314 Eval Loss: 1.0671 Eval Acc: 0.6187 (LR: 0.00100000)
[2025-06-13 18:48:15,433]: [ResNet20_parametrized_hardtanh] Epoch: 007 Train Loss: 0.9723 Train Acc: 0.6520 Eval Loss: 1.0104 Eval Acc: 0.6445 (LR: 0.00100000)
[2025-06-13 18:49:01,226]: [ResNet20_parametrized_hardtanh] Epoch: 008 Train Loss: 0.9351 Train Acc: 0.6667 Eval Loss: 1.1048 Eval Acc: 0.6150 (LR: 0.00100000)
[2025-06-13 18:49:43,766]: [ResNet20_parametrized_hardtanh] Epoch: 009 Train Loss: 0.8960 Train Acc: 0.6814 Eval Loss: 0.8932 Eval Acc: 0.6899 (LR: 0.00100000)
[2025-06-13 18:50:24,559]: [ResNet20_parametrized_hardtanh] Epoch: 010 Train Loss: 0.8616 Train Acc: 0.6966 Eval Loss: 0.8914 Eval Acc: 0.6880 (LR: 0.00100000)
[2025-06-13 18:51:09,492]: [ResNet20_parametrized_hardtanh] Epoch: 011 Train Loss: 0.8263 Train Acc: 0.7077 Eval Loss: 0.8321 Eval Acc: 0.7128 (LR: 0.00100000)
[2025-06-13 18:51:59,180]: [ResNet20_parametrized_hardtanh] Epoch: 012 Train Loss: 0.7984 Train Acc: 0.7182 Eval Loss: 0.8675 Eval Acc: 0.7041 (LR: 0.00100000)
[2025-06-13 18:52:49,621]: [ResNet20_parametrized_hardtanh] Epoch: 013 Train Loss: 0.7712 Train Acc: 0.7294 Eval Loss: 0.8619 Eval Acc: 0.7051 (LR: 0.00100000)
[2025-06-13 18:53:34,268]: [ResNet20_parametrized_hardtanh] Epoch: 014 Train Loss: 0.7484 Train Acc: 0.7362 Eval Loss: 0.8273 Eval Acc: 0.7207 (LR: 0.00100000)
[2025-06-13 18:54:15,712]: [ResNet20_parametrized_hardtanh] Epoch: 015 Train Loss: 0.7287 Train Acc: 0.7456 Eval Loss: 0.7695 Eval Acc: 0.7410 (LR: 0.00100000)
[2025-06-13 18:54:56,866]: [ResNet20_parametrized_hardtanh] Epoch: 016 Train Loss: 0.7054 Train Acc: 0.7521 Eval Loss: 0.8271 Eval Acc: 0.7231 (LR: 0.00100000)
[2025-06-13 18:55:42,114]: [ResNet20_parametrized_hardtanh] Epoch: 017 Train Loss: 0.6881 Train Acc: 0.7594 Eval Loss: 0.7716 Eval Acc: 0.7427 (LR: 0.00100000)
[2025-06-13 18:56:32,676]: [ResNet20_parametrized_hardtanh] Epoch: 018 Train Loss: 0.6670 Train Acc: 0.7659 Eval Loss: 0.8863 Eval Acc: 0.7190 (LR: 0.00100000)
[2025-06-13 18:57:23,027]: [ResNet20_parametrized_hardtanh] Epoch: 019 Train Loss: 0.6487 Train Acc: 0.7732 Eval Loss: 0.6703 Eval Acc: 0.7708 (LR: 0.00100000)
[2025-06-13 18:58:08,313]: [ResNet20_parametrized_hardtanh] Epoch: 020 Train Loss: 0.6398 Train Acc: 0.7754 Eval Loss: 0.7438 Eval Acc: 0.7476 (LR: 0.00100000)
[2025-06-13 18:58:50,396]: [ResNet20_parametrized_hardtanh] Epoch: 021 Train Loss: 0.6207 Train Acc: 0.7817 Eval Loss: 0.7179 Eval Acc: 0.7610 (LR: 0.00100000)
[2025-06-13 18:59:33,101]: [ResNet20_parametrized_hardtanh] Epoch: 022 Train Loss: 0.6058 Train Acc: 0.7868 Eval Loss: 0.6586 Eval Acc: 0.7747 (LR: 0.00100000)
[2025-06-13 19:00:21,710]: [ResNet20_parametrized_hardtanh] Epoch: 023 Train Loss: 0.5910 Train Acc: 0.7935 Eval Loss: 0.6540 Eval Acc: 0.7799 (LR: 0.00100000)
[2025-06-13 19:01:18,044]: [ResNet20_parametrized_hardtanh] Epoch: 024 Train Loss: 0.5835 Train Acc: 0.7952 Eval Loss: 0.6629 Eval Acc: 0.7782 (LR: 0.00100000)
[2025-06-13 19:02:14,673]: [ResNet20_parametrized_hardtanh] Epoch: 025 Train Loss: 0.5737 Train Acc: 0.7993 Eval Loss: 0.7471 Eval Acc: 0.7566 (LR: 0.00100000)
[2025-06-13 19:03:04,465]: [ResNet20_parametrized_hardtanh] Epoch: 026 Train Loss: 0.5644 Train Acc: 0.8033 Eval Loss: 0.6475 Eval Acc: 0.7869 (LR: 0.00100000)
[2025-06-13 19:03:49,541]: [ResNet20_parametrized_hardtanh] Epoch: 027 Train Loss: 0.5504 Train Acc: 0.8078 Eval Loss: 0.6595 Eval Acc: 0.7890 (LR: 0.00100000)
[2025-06-13 19:04:39,426]: [ResNet20_parametrized_hardtanh] Epoch: 028 Train Loss: 0.5368 Train Acc: 0.8126 Eval Loss: 0.6138 Eval Acc: 0.7985 (LR: 0.00100000)
[2025-06-13 19:05:32,493]: [ResNet20_parametrized_hardtanh] Epoch: 029 Train Loss: 0.5347 Train Acc: 0.8120 Eval Loss: 0.6312 Eval Acc: 0.7855 (LR: 0.00100000)
[2025-06-13 19:06:28,991]: [ResNet20_parametrized_hardtanh] Epoch: 030 Train Loss: 0.5220 Train Acc: 0.8172 Eval Loss: 0.6131 Eval Acc: 0.8021 (LR: 0.00100000)
[2025-06-13 19:07:25,947]: [ResNet20_parametrized_hardtanh] Epoch: 031 Train Loss: 0.5160 Train Acc: 0.8206 Eval Loss: 0.6677 Eval Acc: 0.7803 (LR: 0.00100000)
[2025-06-13 19:08:14,618]: [ResNet20_parametrized_hardtanh] Epoch: 032 Train Loss: 0.5068 Train Acc: 0.8227 Eval Loss: 0.5896 Eval Acc: 0.8074 (LR: 0.00100000)
[2025-06-13 19:09:01,236]: [ResNet20_parametrized_hardtanh] Epoch: 033 Train Loss: 0.5026 Train Acc: 0.8261 Eval Loss: 0.6085 Eval Acc: 0.8045 (LR: 0.00100000)
[2025-06-13 19:09:49,319]: [ResNet20_parametrized_hardtanh] Epoch: 034 Train Loss: 0.4955 Train Acc: 0.8282 Eval Loss: 0.6590 Eval Acc: 0.7978 (LR: 0.00100000)
[2025-06-13 19:10:43,419]: [ResNet20_parametrized_hardtanh] Epoch: 035 Train Loss: 0.4893 Train Acc: 0.8275 Eval Loss: 0.6044 Eval Acc: 0.8015 (LR: 0.00100000)
[2025-06-13 19:11:39,931]: [ResNet20_parametrized_hardtanh] Epoch: 036 Train Loss: 0.4828 Train Acc: 0.8315 Eval Loss: 0.5653 Eval Acc: 0.8146 (LR: 0.00100000)
[2025-06-13 19:12:34,493]: [ResNet20_parametrized_hardtanh] Epoch: 037 Train Loss: 0.4776 Train Acc: 0.8328 Eval Loss: 0.5596 Eval Acc: 0.8169 (LR: 0.00100000)
[2025-06-13 19:13:22,892]: [ResNet20_parametrized_hardtanh] Epoch: 038 Train Loss: 0.4725 Train Acc: 0.8357 Eval Loss: 0.5913 Eval Acc: 0.8111 (LR: 0.00100000)
[2025-06-13 19:14:08,001]: [ResNet20_parametrized_hardtanh] Epoch: 039 Train Loss: 0.4690 Train Acc: 0.8350 Eval Loss: 0.6499 Eval Acc: 0.7889 (LR: 0.00100000)
[2025-06-13 19:14:58,157]: [ResNet20_parametrized_hardtanh] Epoch: 040 Train Loss: 0.4611 Train Acc: 0.8386 Eval Loss: 0.6203 Eval Acc: 0.8013 (LR: 0.00100000)
[2025-06-13 19:15:53,969]: [ResNet20_parametrized_hardtanh] Epoch: 041 Train Loss: 0.4538 Train Acc: 0.8410 Eval Loss: 0.5696 Eval Acc: 0.8143 (LR: 0.00100000)
[2025-06-13 19:16:53,115]: [ResNet20_parametrized_hardtanh] Epoch: 042 Train Loss: 0.4476 Train Acc: 0.8431 Eval Loss: 0.5795 Eval Acc: 0.8086 (LR: 0.00100000)
[2025-06-13 19:17:37,334]: [ResNet20_parametrized_hardtanh] Epoch: 043 Train Loss: 0.4469 Train Acc: 0.8433 Eval Loss: 0.5093 Eval Acc: 0.8309 (LR: 0.00100000)
[2025-06-13 19:18:19,221]: [ResNet20_parametrized_hardtanh] Epoch: 044 Train Loss: 0.4398 Train Acc: 0.8465 Eval Loss: 0.5353 Eval Acc: 0.8254 (LR: 0.00100000)
[2025-06-13 19:19:04,046]: [ResNet20_parametrized_hardtanh] Epoch: 045 Train Loss: 0.4409 Train Acc: 0.8461 Eval Loss: 0.5292 Eval Acc: 0.8304 (LR: 0.00100000)
[2025-06-13 19:19:52,014]: [ResNet20_parametrized_hardtanh] Epoch: 046 Train Loss: 0.4256 Train Acc: 0.8510 Eval Loss: 0.5841 Eval Acc: 0.8087 (LR: 0.00100000)
[2025-06-13 19:20:43,382]: [ResNet20_parametrized_hardtanh] Epoch: 047 Train Loss: 0.4250 Train Acc: 0.8510 Eval Loss: 0.5150 Eval Acc: 0.8322 (LR: 0.00100000)
[2025-06-13 19:21:47,296]: [ResNet20_parametrized_hardtanh] Epoch: 048 Train Loss: 0.4225 Train Acc: 0.8531 Eval Loss: 0.6049 Eval Acc: 0.8095 (LR: 0.00100000)
[2025-06-13 19:22:41,171]: [ResNet20_parametrized_hardtanh] Epoch: 049 Train Loss: 0.4212 Train Acc: 0.8532 Eval Loss: 0.4952 Eval Acc: 0.8375 (LR: 0.00100000)
[2025-06-13 19:23:38,687]: [ResNet20_parametrized_hardtanh] Epoch: 050 Train Loss: 0.4192 Train Acc: 0.8542 Eval Loss: 0.5761 Eval Acc: 0.8114 (LR: 0.00100000)
[2025-06-13 19:24:43,914]: [ResNet20_parametrized_hardtanh] Epoch: 051 Train Loss: 0.4083 Train Acc: 0.8578 Eval Loss: 0.5507 Eval Acc: 0.8258 (LR: 0.00100000)
[2025-06-13 19:25:53,091]: [ResNet20_parametrized_hardtanh] Epoch: 052 Train Loss: 0.4107 Train Acc: 0.8563 Eval Loss: 0.5782 Eval Acc: 0.8121 (LR: 0.00100000)
[2025-06-13 19:26:58,345]: [ResNet20_parametrized_hardtanh] Epoch: 053 Train Loss: 0.4068 Train Acc: 0.8589 Eval Loss: 0.5600 Eval Acc: 0.8285 (LR: 0.00100000)
[2025-06-13 19:27:54,570]: [ResNet20_parametrized_hardtanh] Epoch: 054 Train Loss: 0.4001 Train Acc: 0.8603 Eval Loss: 0.5508 Eval Acc: 0.8244 (LR: 0.00100000)
[2025-06-13 19:28:37,837]: [ResNet20_parametrized_hardtanh] Epoch: 055 Train Loss: 0.3947 Train Acc: 0.8610 Eval Loss: 0.5510 Eval Acc: 0.8209 (LR: 0.00010000)
[2025-06-13 19:29:24,815]: [ResNet20_parametrized_hardtanh] Epoch: 056 Train Loss: 0.3264 Train Acc: 0.8865 Eval Loss: 0.4167 Eval Acc: 0.8654 (LR: 0.00010000)
[2025-06-13 19:30:16,729]: [ResNet20_parametrized_hardtanh] Epoch: 057 Train Loss: 0.3095 Train Acc: 0.8926 Eval Loss: 0.4163 Eval Acc: 0.8661 (LR: 0.00010000)
[2025-06-13 19:31:09,755]: [ResNet20_parametrized_hardtanh] Epoch: 058 Train Loss: 0.3013 Train Acc: 0.8947 Eval Loss: 0.4221 Eval Acc: 0.8654 (LR: 0.00010000)
[2025-06-13 19:32:00,244]: [ResNet20_parametrized_hardtanh] Epoch: 059 Train Loss: 0.2969 Train Acc: 0.8957 Eval Loss: 0.4159 Eval Acc: 0.8684 (LR: 0.00010000)
[2025-06-13 19:32:49,066]: [ResNet20_parametrized_hardtanh] Epoch: 060 Train Loss: 0.2918 Train Acc: 0.8984 Eval Loss: 0.4202 Eval Acc: 0.8644 (LR: 0.00010000)
[2025-06-13 19:33:37,049]: [ResNet20_parametrized_hardtanh] Epoch: 061 Train Loss: 0.2876 Train Acc: 0.9000 Eval Loss: 0.4141 Eval Acc: 0.8677 (LR: 0.00010000)
[2025-06-13 19:34:31,574]: [ResNet20_parametrized_hardtanh] Epoch: 062 Train Loss: 0.2851 Train Acc: 0.8999 Eval Loss: 0.4143 Eval Acc: 0.8707 (LR: 0.00010000)
[2025-06-13 19:35:25,332]: [ResNet20_parametrized_hardtanh] Epoch: 063 Train Loss: 0.2820 Train Acc: 0.9016 Eval Loss: 0.4122 Eval Acc: 0.8713 (LR: 0.00010000)
[2025-06-13 19:36:20,676]: [ResNet20_parametrized_hardtanh] Epoch: 064 Train Loss: 0.2820 Train Acc: 0.9017 Eval Loss: 0.4180 Eval Acc: 0.8682 (LR: 0.00010000)
[2025-06-13 19:37:11,506]: [ResNet20_parametrized_hardtanh] Epoch: 065 Train Loss: 0.2766 Train Acc: 0.9020 Eval Loss: 0.4128 Eval Acc: 0.8695 (LR: 0.00010000)
[2025-06-13 19:37:56,128]: [ResNet20_parametrized_hardtanh] Epoch: 066 Train Loss: 0.2792 Train Acc: 0.9023 Eval Loss: 0.4144 Eval Acc: 0.8690 (LR: 0.00010000)
[2025-06-13 19:38:37,743]: [ResNet20_parametrized_hardtanh] Epoch: 067 Train Loss: 0.2750 Train Acc: 0.9033 Eval Loss: 0.4127 Eval Acc: 0.8687 (LR: 0.00010000)
[2025-06-13 19:39:24,009]: [ResNet20_parametrized_hardtanh] Epoch: 068 Train Loss: 0.2740 Train Acc: 0.9039 Eval Loss: 0.4277 Eval Acc: 0.8673 (LR: 0.00010000)
[2025-06-13 19:40:13,937]: [ResNet20_parametrized_hardtanh] Epoch: 069 Train Loss: 0.2677 Train Acc: 0.9069 Eval Loss: 0.4174 Eval Acc: 0.8702 (LR: 0.00001000)
[2025-06-13 19:41:09,120]: [ResNet20_parametrized_hardtanh] Epoch: 070 Train Loss: 0.2616 Train Acc: 0.9093 Eval Loss: 0.4124 Eval Acc: 0.8706 (LR: 0.00001000)
[2025-06-13 19:42:00,627]: [ResNet20_parametrized_hardtanh] Epoch: 071 Train Loss: 0.2621 Train Acc: 0.9078 Eval Loss: 0.4122 Eval Acc: 0.8706 (LR: 0.00001000)
[2025-06-13 19:42:45,262]: [ResNet20_parametrized_hardtanh] Epoch: 072 Train Loss: 0.2652 Train Acc: 0.9066 Eval Loss: 0.4116 Eval Acc: 0.8703 (LR: 0.00001000)
[2025-06-13 19:43:28,187]: [ResNet20_parametrized_hardtanh] Epoch: 073 Train Loss: 0.2583 Train Acc: 0.9096 Eval Loss: 0.4111 Eval Acc: 0.8712 (LR: 0.00001000)
[2025-06-13 19:44:18,019]: [ResNet20_parametrized_hardtanh] Epoch: 074 Train Loss: 0.2589 Train Acc: 0.9088 Eval Loss: 0.4104 Eval Acc: 0.8701 (LR: 0.00001000)
[2025-06-13 19:45:11,911]: [ResNet20_parametrized_hardtanh] Epoch: 075 Train Loss: 0.2598 Train Acc: 0.9085 Eval Loss: 0.4149 Eval Acc: 0.8700 (LR: 0.00001000)
[2025-06-13 19:46:06,665]: [ResNet20_parametrized_hardtanh] Epoch: 076 Train Loss: 0.2604 Train Acc: 0.9093 Eval Loss: 0.4119 Eval Acc: 0.8713 (LR: 0.00001000)
[2025-06-13 19:46:53,879]: [ResNet20_parametrized_hardtanh] Epoch: 077 Train Loss: 0.2599 Train Acc: 0.9086 Eval Loss: 0.4147 Eval Acc: 0.8698 (LR: 0.00001000)
[2025-06-13 19:47:37,561]: [ResNet20_parametrized_hardtanh] Epoch: 078 Train Loss: 0.2563 Train Acc: 0.9097 Eval Loss: 0.4113 Eval Acc: 0.8716 (LR: 0.00001000)
[2025-06-13 19:48:19,782]: [ResNet20_parametrized_hardtanh] Epoch: 079 Train Loss: 0.2618 Train Acc: 0.9082 Eval Loss: 0.4126 Eval Acc: 0.8699 (LR: 0.00001000)
[2025-06-13 19:49:08,213]: [ResNet20_parametrized_hardtanh] Epoch: 080 Train Loss: 0.2571 Train Acc: 0.9100 Eval Loss: 0.4126 Eval Acc: 0.8710 (LR: 0.00000100)
[2025-06-13 19:49:59,581]: [ResNet20_parametrized_hardtanh] Epoch: 081 Train Loss: 0.2561 Train Acc: 0.9119 Eval Loss: 0.4107 Eval Acc: 0.8711 (LR: 0.00000100)
[2025-06-13 19:50:51,580]: [ResNet20_parametrized_hardtanh] Epoch: 082 Train Loss: 0.2557 Train Acc: 0.9103 Eval Loss: 0.4104 Eval Acc: 0.8714 (LR: 0.00000100)
[2025-06-13 19:51:40,404]: [ResNet20_parametrized_hardtanh] Epoch: 083 Train Loss: 0.2597 Train Acc: 0.9095 Eval Loss: 0.4114 Eval Acc: 0.8707 (LR: 0.00000100)
[2025-06-13 19:52:22,608]: [ResNet20_parametrized_hardtanh] Epoch: 084 Train Loss: 0.2560 Train Acc: 0.9102 Eval Loss: 0.4128 Eval Acc: 0.8707 (LR: 0.00000100)
[2025-06-13 19:53:04,941]: [ResNet20_parametrized_hardtanh] Epoch: 085 Train Loss: 0.2581 Train Acc: 0.9081 Eval Loss: 0.4125 Eval Acc: 0.8710 (LR: 0.00000100)
[2025-06-13 19:53:52,084]: [ResNet20_parametrized_hardtanh] Epoch: 086 Train Loss: 0.2571 Train Acc: 0.9100 Eval Loss: 0.4110 Eval Acc: 0.8712 (LR: 0.00000010)
[2025-06-13 19:54:43,273]: [ResNet20_parametrized_hardtanh] Epoch: 087 Train Loss: 0.2588 Train Acc: 0.9085 Eval Loss: 0.4105 Eval Acc: 0.8720 (LR: 0.00000010)
[2025-06-13 19:55:34,379]: [ResNet20_parametrized_hardtanh] Epoch: 088 Train Loss: 0.2572 Train Acc: 0.9100 Eval Loss: 0.4122 Eval Acc: 0.8715 (LR: 0.00000010)
[2025-06-13 19:56:19,538]: [ResNet20_parametrized_hardtanh] Epoch: 089 Train Loss: 0.2564 Train Acc: 0.9100 Eval Loss: 0.4105 Eval Acc: 0.8715 (LR: 0.00000010)
[2025-06-13 19:57:02,644]: [ResNet20_parametrized_hardtanh] Epoch: 090 Train Loss: 0.2561 Train Acc: 0.9100 Eval Loss: 0.4114 Eval Acc: 0.8711 (LR: 0.00000010)
[2025-06-13 19:57:45,914]: [ResNet20_parametrized_hardtanh] Epoch: 091 Train Loss: 0.2552 Train Acc: 0.9103 Eval Loss: 0.4121 Eval Acc: 0.8716 (LR: 0.00000010)
[2025-06-13 19:58:33,016]: [ResNet20_parametrized_hardtanh] Epoch: 092 Train Loss: 0.2575 Train Acc: 0.9107 Eval Loss: 0.4114 Eval Acc: 0.8709 (LR: 0.00000010)
[2025-06-13 19:59:25,137]: [ResNet20_parametrized_hardtanh] Epoch: 093 Train Loss: 0.2561 Train Acc: 0.9111 Eval Loss: 0.4103 Eval Acc: 0.8709 (LR: 0.00000010)
[2025-06-13 20:00:18,468]: [ResNet20_parametrized_hardtanh] Epoch: 094 Train Loss: 0.2549 Train Acc: 0.9100 Eval Loss: 0.4112 Eval Acc: 0.8723 (LR: 0.00000010)
[2025-06-13 20:01:05,122]: [ResNet20_parametrized_hardtanh] Epoch: 095 Train Loss: 0.2551 Train Acc: 0.9102 Eval Loss: 0.4121 Eval Acc: 0.8713 (LR: 0.00000010)
[2025-06-13 20:01:47,108]: [ResNet20_parametrized_hardtanh] Epoch: 096 Train Loss: 0.2574 Train Acc: 0.9092 Eval Loss: 0.4124 Eval Acc: 0.8704 (LR: 0.00000010)
[2025-06-13 20:02:29,007]: [ResNet20_parametrized_hardtanh] Epoch: 097 Train Loss: 0.2555 Train Acc: 0.9109 Eval Loss: 0.4105 Eval Acc: 0.8719 (LR: 0.00000010)
[2025-06-13 20:03:15,191]: [ResNet20_parametrized_hardtanh] Epoch: 098 Train Loss: 0.2559 Train Acc: 0.9100 Eval Loss: 0.4102 Eval Acc: 0.8708 (LR: 0.00000010)
[2025-06-13 20:04:05,817]: [ResNet20_parametrized_hardtanh] Epoch: 099 Train Loss: 0.2570 Train Acc: 0.9097 Eval Loss: 0.4125 Eval Acc: 0.8711 (LR: 0.00000010)
[2025-06-13 20:04:56,994]: [ResNet20_parametrized_hardtanh] Epoch: 100 Train Loss: 0.2611 Train Acc: 0.9084 Eval Loss: 0.4114 Eval Acc: 0.8716 (LR: 0.00000010)
[2025-06-13 20:04:56,994]: [ResNet20_parametrized_hardtanh] Best Eval Accuracy: 0.8723
[2025-06-13 20:04:57,342]: 
Training of full-precision model finished!
[2025-06-13 20:04:57,343]: Model Architecture:
[2025-06-13 20:04:57,445]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 20:04:57,446]: 
Model Weights:
[2025-06-13 20:04:57,446]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-13 20:04:57,842]: Sample Values (25 elements): [-0.21032749116420746, -0.16832585632801056, -0.016765650361776352, 0.1874997615814209, -0.17421771585941315, 0.04504559934139252, -0.30979567766189575, 0.12012939155101776, -0.04768689349293709, -0.12645916640758514, -0.20087584853172302, 0.1416977196931839, -0.005127444863319397, 0.30762428045272827, 0.06064506247639656, -0.0011184248141944408, 0.10927533358335495, -0.2975149154663086, 0.19050908088684082, 0.08074687421321869, -0.2360198050737381, 0.03334629535675049, 0.17117798328399658, -0.20573043823242188, 0.209633469581604]
[2025-06-13 20:04:57,930]: Mean: 0.00156383
[2025-06-13 20:04:57,983]: Min: -0.45997185
[2025-06-13 20:04:58,051]: Max: 0.58972681
[2025-06-13 20:04:58,051]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,051]: Sample Values (16 elements): [0.9727416038513184, 1.0523263216018677, 0.643711268901825, 0.7742626070976257, 0.7477770447731018, 0.8727356195449829, 0.72024005651474, 0.8749696612358093, 0.5997281670570374, 0.7956380248069763, 0.8444374799728394, 0.7277807593345642, 0.8336390852928162, 0.8368315696716309, 0.9724543690681458, 0.80937659740448]
[2025-06-13 20:04:58,051]: Mean: 0.81741560
[2025-06-13 20:04:58,052]: Min: 0.59972817
[2025-06-13 20:04:58,052]: Max: 1.05232632
[2025-06-13 20:04:58,052]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:04:58,052]: Sample Values (25 elements): [0.2698744237422943, 0.019831566140055656, -0.2291174829006195, -0.03887505456805229, -0.2964177429676056, -0.2621396780014038, 0.013400854542851448, 0.0854194164276123, -0.009606994688510895, 0.05681236460804939, -0.2714160680770874, 0.1338057518005371, 0.12977531552314758, -0.00047883589286357164, 0.20672009885311127, 0.1691356897354126, 0.1504988968372345, 0.0459996722638607, 0.19968432188034058, -0.10750916600227356, -0.04456310719251633, -0.15525288879871368, -0.1056426540017128, 0.021355556324124336, -0.02286841906607151]
[2025-06-13 20:04:58,052]: Mean: -0.00460550
[2025-06-13 20:04:58,052]: Min: -0.69873524
[2025-06-13 20:04:58,053]: Max: 0.60628659
[2025-06-13 20:04:58,053]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,053]: Sample Values (16 elements): [0.756759762763977, 0.7018892765045166, 0.8397523164749146, 1.4330370426177979, 0.9059699773788452, 0.8317697048187256, 0.7606596946716309, 0.8931217789649963, 0.7768288254737854, 1.0213696956634521, 1.0732502937316895, 0.826846182346344, 1.2304251194000244, 0.7313969731330872, 0.6574612259864807, 0.8022949695587158]
[2025-06-13 20:04:58,053]: Mean: 0.89017707
[2025-06-13 20:04:58,053]: Min: 0.65746123
[2025-06-13 20:04:58,053]: Max: 1.43303704
[2025-06-13 20:04:58,053]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:04:58,054]: Sample Values (25 elements): [0.3742417097091675, -0.08615301549434662, 0.0056207445450127125, -0.08642525970935822, -0.11790532618761063, 0.34966790676116943, 0.11482170224189758, 0.1525890976190567, 0.034105245023965836, -0.14358165860176086, 0.04923936724662781, -0.056117892265319824, -0.054663218557834625, 0.18412992358207703, -0.013195975683629513, 0.06066879257559776, -0.0466177798807621, -0.21061857044696808, -0.15026763081550598, 0.033685822039842606, 0.17352013289928436, -0.04955795407295227, 0.028927216306328773, -0.18458867073059082, -0.11966909468173981]
[2025-06-13 20:04:58,054]: Mean: 0.00687450
[2025-06-13 20:04:58,054]: Min: -0.48161802
[2025-06-13 20:04:58,054]: Max: 0.53223097
[2025-06-13 20:04:58,054]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,054]: Sample Values (16 elements): [0.9029797911643982, 0.7740492224693298, 0.826654851436615, 0.8073068261146545, 0.48419228196144104, 1.1247323751449585, 1.0426265001296997, 0.6549862027168274, 0.6700217127799988, 0.8300742506980896, 0.8366490006446838, 0.5540676116943359, 0.7185433506965637, 0.850733757019043, 0.696125328540802, 0.5679022073745728]
[2025-06-13 20:04:58,055]: Mean: 0.77135289
[2025-06-13 20:04:58,055]: Min: 0.48419228
[2025-06-13 20:04:58,055]: Max: 1.12473238
[2025-06-13 20:04:58,055]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:04:58,055]: Sample Values (25 elements): [0.17115016281604767, -0.17809495329856873, -0.00291233672760427, -0.26972395181655884, -0.0640481486916542, -0.047560084611177444, -0.2347324937582016, 0.012309502810239792, -0.02370128221809864, 0.21257641911506653, -0.18161740899085999, 0.15489515662193298, 6.790715997340158e-05, 0.005154917016625404, 0.06409551203250885, -0.1299009472131729, -0.1462240070104599, 0.2642271816730499, -0.06402096897363663, -0.2780860364437103, -0.1480378657579422, 0.1549539417028427, 0.13789643347263336, 0.044812269508838654, -0.19024263322353363]
[2025-06-13 20:04:58,055]: Mean: 0.00241088
[2025-06-13 20:04:58,056]: Min: -0.48422527
[2025-06-13 20:04:58,056]: Max: 0.56795418
[2025-06-13 20:04:58,056]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,056]: Sample Values (16 elements): [0.7363662123680115, 0.9609647989273071, 0.7034069299697876, 0.5790272355079651, 0.9636501669883728, 0.8465288877487183, 0.7384750843048096, 0.7148287892341614, 0.6687480807304382, 1.1832165718078613, 0.6852056980133057, 1.0947192907333374, 0.7465512752532959, 0.860418975353241, 0.6858792304992676, 0.8746882081031799]
[2025-06-13 20:04:58,056]: Mean: 0.81516719
[2025-06-13 20:04:58,057]: Min: 0.57902724
[2025-06-13 20:04:58,057]: Max: 1.18321657
[2025-06-13 20:04:58,057]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:04:58,057]: Sample Values (25 elements): [0.026613399386405945, 0.05000963434576988, 0.10817552357912064, -0.20347873866558075, -0.41947633028030396, 0.13129834830760956, -0.13280561566352844, 0.0006187502876855433, 0.04302915185689926, -0.24856063723564148, 0.05688069388270378, 0.09612499922513962, 0.34046056866645813, 0.2441665381193161, -0.16415712237358093, -0.16949720680713654, -0.09854727983474731, 0.031729694455862045, -0.015751933678984642, 0.025221925228834152, 0.24234013259410858, 0.362139493227005, 0.0767349824309349, 0.11563470959663391, 0.09407274425029755]
[2025-06-13 20:04:58,057]: Mean: 0.00633841
[2025-06-13 20:04:58,057]: Min: -0.63969123
[2025-06-13 20:04:58,058]: Max: 0.53700602
[2025-06-13 20:04:58,058]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,058]: Sample Values (16 elements): [0.7872179746627808, 0.9425062537193298, 0.6852031946182251, 1.090861439704895, 0.8941742777824402, 0.7195418477058411, 0.704183042049408, 0.6697149276733398, 0.7851122617721558, 0.9495126008987427, 0.7358207106590271, 0.7086028456687927, 0.7932082414627075, 0.7980688214302063, 0.7360501289367676, 0.650562584400177]
[2025-06-13 20:04:58,058]: Mean: 0.79064631
[2025-06-13 20:04:58,075]: Min: 0.65056258
[2025-06-13 20:04:58,075]: Max: 1.09086144
[2025-06-13 20:04:58,076]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:04:58,076]: Sample Values (25 elements): [-0.16444720327854156, 0.18224693834781647, -0.028433186933398247, -0.1254442036151886, -0.006695413962006569, -0.1679711788892746, -0.1718667596578598, -0.1057623103260994, 0.1677701324224472, -0.02581273578107357, -0.0007496853941120207, 0.09955044835805893, -0.07788026332855225, -0.002008969895541668, 0.3519507646560669, 0.3514154553413391, 0.506263017654419, 0.19127778708934784, -0.04395829141139984, 0.09491807222366333, 0.15258288383483887, 0.01011763233691454, -0.10289223492145538, -0.12855204939842224, -0.004187536425888538]
[2025-06-13 20:04:58,076]: Mean: -0.00487389
[2025-06-13 20:04:58,076]: Min: -0.55928123
[2025-06-13 20:04:58,076]: Max: 0.56186384
[2025-06-13 20:04:58,076]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,077]: Sample Values (16 elements): [0.8920438289642334, 0.8962446451187134, 0.7864074110984802, 0.6356069445610046, 0.7129302024841309, 1.0529893636703491, 0.5906369686126709, 0.8262411952018738, 0.9078568816184998, 1.1446847915649414, 0.7019453644752502, 0.8429438471794128, 0.6931631565093994, 0.6314871311187744, 0.7357699871063232, 0.5403710007667542]
[2025-06-13 20:04:58,077]: Mean: 0.78695762
[2025-06-13 20:04:58,077]: Min: 0.54037100
[2025-06-13 20:04:58,077]: Max: 1.14468479
[2025-06-13 20:04:58,077]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:04:58,078]: Sample Values (25 elements): [-0.12987303733825684, -0.3492116928100586, 0.02348191663622856, -0.030281420797109604, -0.1759188324213028, 0.12160052359104156, -0.02229386195540428, 0.17794044315814972, -0.08246687799692154, -0.16984441876411438, -0.1524086594581604, 0.13486477732658386, -0.03870208188891411, -0.1610669642686844, -0.10087672621011734, 0.1044522374868393, -0.12522418797016144, -0.017761385068297386, -0.091488316655159, 0.033898208290338516, -0.013370768167078495, 0.023188795894384384, -0.10223808884620667, 0.07492297887802124, 0.2974553406238556]
[2025-06-13 20:04:58,078]: Mean: 0.00638750
[2025-06-13 20:04:58,078]: Min: -0.54571819
[2025-06-13 20:04:58,078]: Max: 0.54387265
[2025-06-13 20:04:58,078]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-13 20:04:58,078]: Sample Values (16 elements): [0.5397069454193115, 0.6608269810676575, 1.1071245670318604, 0.8516267538070679, 0.8217025399208069, 0.7049593329429626, 0.711467444896698, 0.92673259973526, 0.6005471348762512, 0.5984677076339722, 0.9270606637001038, 1.0176713466644287, 0.6021426320075989, 1.0901005268096924, 0.917992115020752, 0.8735081553459167]
[2025-06-13 20:04:58,079]: Mean: 0.80947733
[2025-06-13 20:04:58,079]: Min: 0.53970695
[2025-06-13 20:04:58,079]: Max: 1.10712457
[2025-06-13 20:04:58,079]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-13 20:04:58,079]: Sample Values (25 elements): [0.03758537396788597, 0.015405268408358097, 0.053727392107248306, 0.12150111049413681, 0.03148215636610985, -0.07360300421714783, 0.25467631220817566, -0.09419398009777069, -0.1599460244178772, -0.008098339661955833, 0.029806874692440033, -0.12483193725347519, -0.06430260092020035, 0.123655766248703, -0.05490924045443535, -0.4325021505355835, -0.1992533802986145, -0.039535559713840485, 0.09741047024726868, 0.031399063766002655, -0.19642187654972076, 0.03985553979873657, 0.0289533119648695, 0.08997451514005661, 0.0040982975624501705]
[2025-06-13 20:04:58,080]: Mean: 0.00258383
[2025-06-13 20:04:58,080]: Min: -0.58929598
[2025-06-13 20:04:58,080]: Max: 0.50005782
[2025-06-13 20:04:58,080]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,080]: Sample Values (25 elements): [0.8004061579704285, 0.7095600962638855, 0.8648650050163269, 0.669316828250885, 0.648400604724884, 0.679145097732544, 0.7844401001930237, 0.8102420568466187, 0.6740491986274719, 0.6088855266571045, 0.6373266577720642, 0.6038897037506104, 0.623152494430542, 0.6720319986343384, 0.6169873476028442, 0.7564926743507385, 0.7377879619598389, 0.6957786083221436, 0.6198585629463196, 0.9085789918899536, 0.6783478856086731, 0.6337021589279175, 0.7163369059562683, 0.7458311319351196, 0.5090710520744324]
[2025-06-13 20:04:58,080]: Mean: 0.70295209
[2025-06-13 20:04:58,081]: Min: 0.50907105
[2025-06-13 20:04:58,081]: Max: 0.90857899
[2025-06-13 20:04:58,081]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:04:58,081]: Sample Values (25 elements): [0.15181216597557068, 0.1308422088623047, 0.08731709420681, -0.04509495571255684, -0.021862579509615898, -0.039562687277793884, 0.09345932304859161, 0.013076270930469036, -0.282192587852478, -0.06570900976657867, 0.010944915004074574, -0.11397133022546768, -0.160960391163826, -0.1435064971446991, 0.06227119639515877, -0.013372899033129215, 0.055518072098493576, 0.07080689072608948, 0.0800449401140213, -0.013569929637014866, 0.04897219315171242, 0.1001909077167511, -0.22442527115345, -0.00036345585249364376, -0.10549949109554291]
[2025-06-13 20:04:58,081]: Mean: -0.00176734
[2025-06-13 20:04:58,081]: Min: -0.55789143
[2025-06-13 20:04:58,082]: Max: 0.46125370
[2025-06-13 20:04:58,082]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,082]: Sample Values (25 elements): [1.0241917371749878, 0.9883922338485718, 0.9348273277282715, 0.7524552941322327, 0.8166669607162476, 0.9022618532180786, 0.9098625779151917, 0.7794225215911865, 0.7301525473594666, 0.868867814540863, 0.8707457780838013, 0.8246358633041382, 0.9915853142738342, 0.744025707244873, 0.824520468711853, 0.8806488513946533, 0.9721815586090088, 0.7667283415794373, 0.9351164698600769, 0.9237121939659119, 0.8220428228378296, 0.9137251377105713, 0.8620855808258057, 0.6859098076820374, 0.9514347910881042]
[2025-06-13 20:04:58,082]: Mean: 0.87248611
[2025-06-13 20:04:58,082]: Min: 0.68590981
[2025-06-13 20:04:58,082]: Max: 1.02419174
[2025-06-13 20:04:58,082]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-13 20:04:58,083]: Sample Values (25 elements): [0.4426416754722595, 0.03704296797513962, -0.03134968504309654, 0.0030471610371023417, 0.2526906430721283, 0.22788073122501373, 0.1101788580417633, -0.015464866533875465, -0.04309651628136635, -0.0657164677977562, -0.07788936048746109, 0.2061309665441513, 0.1419767290353775, 0.13354448974132538, 0.011165972799062729, -0.16682957112789154, 0.017300689592957497, 0.024290023371577263, 0.0854291245341301, 0.19121070206165314, 0.19334948062896729, 0.26261642575263977, 0.03440714627504349, 0.05070040374994278, 0.08533012121915817]
[2025-06-13 20:04:58,083]: Mean: 0.00733417
[2025-06-13 20:04:58,083]: Min: -0.61661887
[2025-06-13 20:04:58,083]: Max: 0.58805662
[2025-06-13 20:04:58,083]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,083]: Sample Values (25 elements): [0.4364982545375824, 0.534442126750946, 0.7122611403465271, 0.5891027450561523, 0.7217111587524414, 0.6689057946205139, 0.7940605878829956, 0.5492699146270752, 0.4879944324493408, 0.7957903742790222, 0.7493054866790771, 0.7353696227073669, 0.5688990950584412, 0.7968906164169312, 0.5289329290390015, 0.8509909510612488, 0.5361399054527283, 0.5445197820663452, 0.5074114203453064, 0.5147693753242493, 0.5901352763175964, 0.7645893096923828, 0.6781752109527588, 0.5735986232757568, 0.4799118638038635]
[2025-06-13 20:04:58,084]: Mean: 0.63245857
[2025-06-13 20:04:58,084]: Min: 0.43649825
[2025-06-13 20:04:58,084]: Max: 0.85099095
[2025-06-13 20:04:58,084]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:04:58,085]: Sample Values (25 elements): [-0.1735733449459076, -0.0845063179731369, 0.07503288239240646, 0.048079267144203186, 0.004131547175347805, -0.18636304140090942, 0.055716075003147125, 0.037619441747665405, -0.12415817379951477, 0.10013208538293839, -0.10598734766244888, -0.1031164899468422, -0.08519387990236282, -0.11141510307788849, -0.03209788724780083, -0.007086713798344135, -0.028980731964111328, -0.08805406838655472, 0.03398662805557251, -0.1602020114660263, 0.24489468336105347, -0.048880401998758316, -0.01924476958811283, -0.18045391142368317, -0.13075275719165802]
[2025-06-13 20:04:58,085]: Mean: -0.00136406
[2025-06-13 20:04:58,085]: Min: -0.43626308
[2025-06-13 20:04:58,085]: Max: 0.52669758
[2025-06-13 20:04:58,085]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,085]: Sample Values (25 elements): [0.5691293478012085, 0.5875029563903809, 0.6831769943237305, 0.7388532161712646, 0.6295768618583679, 0.49495357275009155, 0.6076832413673401, 0.7102422118186951, 0.8279601335525513, 0.7133083343505859, 0.6397354602813721, 0.7235749363899231, 0.6466025710105896, 0.5245214700698853, 0.6007593274116516, 0.5089467763900757, 0.782216489315033, 0.515434205532074, 0.7238978147506714, 0.6445915102958679, 0.7878484129905701, 0.6596287488937378, 0.698367178440094, 0.8181908130645752, 0.6388540267944336]
[2025-06-13 20:04:58,085]: Mean: 0.65234077
[2025-06-13 20:04:58,086]: Min: 0.49495357
[2025-06-13 20:04:58,086]: Max: 0.82796013
[2025-06-13 20:04:58,086]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:04:58,086]: Sample Values (25 elements): [0.17376846075057983, -0.13246233761310577, -0.10963955521583557, -0.24011960625648499, -0.021513227373361588, 0.025859670713543892, -0.06122489646077156, -0.17571893334388733, -0.01694427616894245, 0.03635376691818237, 0.07562366873025894, -0.05694324150681496, -0.08821775764226913, -0.13326041400432587, -0.07793769240379333, 0.17799830436706543, 0.16899420320987701, 0.01803704909980297, 0.08783060312271118, -0.05732984468340874, 0.20317061245441437, -0.14310985803604126, -0.022392474114894867, 0.026171144098043442, 0.059140149503946304]
[2025-06-13 20:04:58,086]: Mean: 0.00178989
[2025-06-13 20:04:58,087]: Min: -0.57674110
[2025-06-13 20:04:58,087]: Max: 0.45279145
[2025-06-13 20:04:58,087]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,087]: Sample Values (25 elements): [0.9318712949752808, 0.7444376945495605, 0.6628899574279785, 0.879414439201355, 0.8071462512016296, 0.8296393156051636, 0.7046459913253784, 0.9260891675949097, 0.8205722570419312, 0.8029747009277344, 0.7084074020385742, 0.6129384636878967, 0.5874000787734985, 0.6982706785202026, 0.6558521389961243, 0.7030460834503174, 0.8344178199768066, 0.6797516345977783, 0.8105515241622925, 0.8707354664802551, 0.8786886930465698, 0.7152513265609741, 0.7749168872833252, 0.8558256030082703, 0.7429200410842896]
[2025-06-13 20:04:58,087]: Mean: 0.78015119
[2025-06-13 20:04:58,087]: Min: 0.54371929
[2025-06-13 20:04:58,087]: Max: 1.05615139
[2025-06-13 20:04:58,087]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:04:58,088]: Sample Values (25 elements): [-0.17703592777252197, 0.023692304268479347, 0.006559271365404129, -0.13415467739105225, -0.12069534510374069, -0.15049704909324646, -0.021117882803082466, 0.014138485305011272, -0.09393064677715302, 0.06441449373960495, -0.059403080493211746, 0.10905633866786957, -0.23552368581295013, -0.22313177585601807, 0.08608248084783554, -0.13948701322078705, -0.01424119621515274, 0.03309255838394165, 0.0795508548617363, -0.05661512166261673, 0.16531075537204742, 0.23807327449321747, 0.08365637063980103, 0.03596978634595871, 0.05340273678302765]
[2025-06-13 20:04:58,088]: Mean: -0.00018713
[2025-06-13 20:04:58,088]: Min: -0.41938651
[2025-06-13 20:04:58,088]: Max: 0.53567690
[2025-06-13 20:04:58,088]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,089]: Sample Values (25 elements): [0.7669046521186829, 0.6475461721420288, 0.7206791043281555, 0.7523819208145142, 0.6204143166542053, 0.6369055509567261, 0.6290963888168335, 0.660608172416687, 0.5533130764961243, 0.6303322315216064, 0.6238324642181396, 0.8000566363334656, 0.6891966462135315, 0.651108980178833, 0.6352942585945129, 0.6783857941627502, 0.6784458756446838, 0.7763574719429016, 0.686062753200531, 0.6111174821853638, 0.6464295983314514, 0.6736604571342468, 0.6485347747802734, 0.664301335811615, 0.7472572326660156]
[2025-06-13 20:04:58,089]: Mean: 0.67140627
[2025-06-13 20:04:58,089]: Min: 0.55331308
[2025-06-13 20:04:58,089]: Max: 0.80005664
[2025-06-13 20:04:58,089]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:04:58,089]: Sample Values (25 elements): [-0.010459487326443195, 0.07940495014190674, -0.012273691594600677, 0.05566895008087158, -0.04890737310051918, -0.03576696664094925, 0.06993920356035233, 0.09212365001440048, 0.022333770990371704, -0.013834275305271149, 0.2289138287305832, 0.018092188984155655, -0.08186602592468262, -0.0641828179359436, 0.1508905440568924, 0.1423833668231964, -0.016823258250951767, 0.1074395552277565, -0.08767355978488922, -0.02881358005106449, -0.05058083310723305, 0.06448698788881302, 0.2622634172439575, 0.01833110675215721, -0.10484372824430466]
[2025-06-13 20:04:58,090]: Mean: 0.00461619
[2025-06-13 20:04:58,090]: Min: -0.53183192
[2025-06-13 20:04:58,090]: Max: 0.48995182
[2025-06-13 20:04:58,090]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-13 20:04:58,090]: Sample Values (25 elements): [0.7277282476425171, 0.8817575573921204, 0.8397498726844788, 0.9495323896408081, 1.0101583003997803, 0.7258301377296448, 0.6301715970039368, 0.6932277679443359, 1.0199562311172485, 0.6390008330345154, 0.9454755783081055, 0.8769220113754272, 0.934932291507721, 1.0237541198730469, 0.6417108178138733, 0.647955596446991, 0.7565389275550842, 0.8433758616447449, 0.7777017951011658, 1.0268510580062866, 0.7986955642700195, 0.999630868434906, 0.9549645185470581, 0.8657295107841492, 0.9529411196708679]
[2025-06-13 20:04:58,090]: Mean: 0.83705640
[2025-06-13 20:04:58,090]: Min: 0.62895244
[2025-06-13 20:04:58,091]: Max: 1.02685106
[2025-06-13 20:04:58,091]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-13 20:04:58,091]: Sample Values (25 elements): [-0.0749465674161911, -0.15586377680301666, 0.01957114413380623, -0.05003415420651436, 0.005683728028088808, -0.1746642142534256, -0.007871057838201523, 0.022977706044912338, 0.02021031826734543, -0.0900569036602974, -0.019384168088436127, 0.09815052896738052, -0.06423591077327728, 0.23784789443016052, -0.09350060671567917, 0.19449058175086975, 0.11105675995349884, -0.05532827228307724, -0.02077043242752552, -0.03682314604520798, 0.01516579370945692, -0.2582123875617981, 0.044316306710243225, -0.05587097629904747, -0.12658460438251495]
[2025-06-13 20:04:58,091]: Mean: 0.00040462
[2025-06-13 20:04:58,091]: Min: -0.44359741
[2025-06-13 20:04:58,091]: Max: 0.53405648
[2025-06-13 20:04:58,092]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,092]: Sample Values (25 elements): [0.5680947303771973, 0.532325267791748, 0.48870518803596497, 0.6090549230575562, 0.5766762495040894, 0.426461786031723, 0.5355706214904785, 0.4988931715488434, 0.5479856729507446, 0.5629847049713135, 0.5138781070709229, 0.5731150507926941, 0.5197508931159973, 0.4980223476886749, 0.5253823399543762, 0.5118861794471741, 0.5644570589065552, 0.6295967102050781, 0.553054690361023, 0.4974272549152374, 0.6416096091270447, 0.49992790818214417, 0.4735279679298401, 0.4755156338214874, 0.5477340221405029]
[2025-06-13 20:04:58,092]: Mean: 0.55039763
[2025-06-13 20:04:58,092]: Min: 0.42646179
[2025-06-13 20:04:58,092]: Max: 0.72789830
[2025-06-13 20:04:58,092]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:04:58,093]: Sample Values (25 elements): [0.15094421803951263, 0.07666722685098648, 0.11351668834686279, 0.011821506544947624, -0.09034007787704468, -0.03994949907064438, -0.0584365651011467, 0.008919989690184593, 0.1828741431236267, 0.076900415122509, -0.07303974032402039, 0.120005764067173, 0.051047518849372864, -0.032562512904405594, -0.12624691426753998, 0.011473093181848526, -0.14300565421581268, -0.15301106870174408, -0.16979333758354187, 0.05027447268366814, -0.047596223652362823, 0.04297056049108505, -0.0741453692317009, 0.035480450838804245, 0.08028094470500946]
[2025-06-13 20:04:58,093]: Mean: 0.00040928
[2025-06-13 20:04:58,093]: Min: -0.50086755
[2025-06-13 20:04:58,093]: Max: 0.45715672
[2025-06-13 20:04:58,093]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,094]: Sample Values (25 elements): [1.0332651138305664, 0.8753172159194946, 0.9152916073799133, 0.9257513284683228, 0.8442654013633728, 0.7502899765968323, 0.802223801612854, 0.9435372948646545, 0.9548082947731018, 0.8485787510871887, 1.0405939817428589, 0.8910384178161621, 0.7774809002876282, 0.9944977164268494, 0.8627796769142151, 0.9609023928642273, 0.9077150821685791, 1.0256712436676025, 0.9080056548118591, 0.8300841450691223, 0.835096538066864, 0.9631091952323914, 1.0457202196121216, 0.9830507040023804, 0.8235880732536316]
[2025-06-13 20:04:58,094]: Mean: 0.89795613
[2025-06-13 20:04:58,094]: Min: 0.70622092
[2025-06-13 20:04:58,094]: Max: 1.08590007
[2025-06-13 20:04:58,094]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-13 20:04:58,094]: Sample Values (25 elements): [-0.07017320394515991, 0.1731363832950592, -0.15841680765151978, -0.3684006631374359, 0.1870371550321579, -0.29589107632637024, 0.020240794867277145, 0.030051345005631447, 0.050837885588407516, -0.027424359694123268, -0.10085394233465195, 0.10285545140504837, -0.13502845168113708, 0.043832696974277496, -0.15148305892944336, 0.06430738419294357, 0.18727993965148926, 0.31617751717567444, 0.1092577874660492, 0.027787622064352036, 0.2274869978427887, -0.10346060246229172, 0.12338711321353912, -0.08148259669542313, 0.13929720222949982]
[2025-06-13 20:04:58,095]: Mean: 0.00093755
[2025-06-13 20:04:58,095]: Min: -0.46978530
[2025-06-13 20:04:58,095]: Max: 0.51005471
[2025-06-13 20:04:58,095]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,095]: Sample Values (25 elements): [0.7976378798484802, 0.558873176574707, 0.7669202089309692, 0.5583611130714417, 0.6784282326698303, 0.6186899542808533, 0.7687457203865051, 0.7094348073005676, 0.7568796873092651, 0.6984667778015137, 0.889146625995636, 0.7394373416900635, 0.5163612365722656, 0.748345673084259, 0.70626300573349, 0.9095025658607483, 0.8903484344482422, 0.7995743155479431, 0.6382746696472168, 0.43755441904067993, 0.6056466698646545, 0.5997068285942078, 0.7392135262489319, 0.5635738968849182, 0.6320689916610718]
[2025-06-13 20:04:58,095]: Mean: 0.66471672
[2025-06-13 20:04:58,096]: Min: 0.40826184
[2025-06-13 20:04:58,096]: Max: 0.90950257
[2025-06-13 20:04:58,096]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:04:58,096]: Sample Values (25 elements): [0.024663086980581284, 0.09370990097522736, -0.10063572227954865, 0.092770054936409, 0.010662219487130642, 0.04685652628540993, -0.0708983764052391, -0.07154273241758347, 0.19403964281082153, 0.10071717202663422, 0.21890254318714142, 0.20951047539710999, -0.11753365397453308, -0.010753878392279148, 0.27027884125709534, -0.06341251730918884, 0.23116816580295563, -0.10724308341741562, 0.008051176555454731, 0.0012443972518667579, -0.0375540666282177, 0.2485765814781189, 0.09558922052383423, -0.09543607383966446, -0.05513862892985344]
[2025-06-13 20:04:58,097]: Mean: -0.00026182
[2025-06-13 20:04:58,097]: Min: -0.48555741
[2025-06-13 20:04:58,097]: Max: 0.49383178
[2025-06-13 20:04:58,097]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,097]: Sample Values (25 elements): [0.6885653138160706, 0.5969823002815247, 0.5789620876312256, 0.6082702875137329, 0.6524962186813354, 0.5248433351516724, 0.6837716698646545, 0.5918300747871399, 0.5726571083068848, 0.6015729904174805, 0.5242542028427124, 0.6258625984191895, 0.6148624420166016, 0.6593993902206421, 0.626556932926178, 0.6130512952804565, 0.5014103651046753, 0.6624209880828857, 0.5991662740707397, 0.6596039533615112, 0.6773334741592407, 0.5763680934906006, 0.6259939074516296, 0.6347097754478455, 0.5874179601669312]
[2025-06-13 20:04:58,097]: Mean: 0.61624783
[2025-06-13 20:04:58,097]: Min: 0.50141037
[2025-06-13 20:04:58,098]: Max: 0.77225661
[2025-06-13 20:04:58,098]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:04:58,098]: Sample Values (25 elements): [-0.02019793726503849, 0.07793455570936203, -0.1576395183801651, -0.12614285945892334, 0.04813404008746147, 0.058383479714393616, 0.048957765102386475, -0.11742459982633591, -0.1941957175731659, 0.11127186566591263, 0.09660592675209045, -0.12579640746116638, -0.0539846234023571, -0.013095256872475147, -0.0018338713562116027, 0.03282548859715462, 0.01189111266285181, -0.015296824276447296, -0.00994886551052332, -0.04991185665130615, -0.009070837870240211, -0.10280616581439972, -0.06230529025197029, 0.08787305653095245, -0.07415345311164856]
[2025-06-13 20:04:58,098]: Mean: 0.00152548
[2025-06-13 20:04:58,098]: Min: -0.40517080
[2025-06-13 20:04:58,099]: Max: 0.45393118
[2025-06-13 20:04:58,099]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,099]: Sample Values (25 elements): [0.791897177696228, 0.8237758874893188, 0.775144636631012, 0.6227480173110962, 0.7333053350448608, 0.7973788976669312, 0.8441676497459412, 0.8775997757911682, 0.8100478053092957, 0.728711724281311, 0.741738498210907, 0.896356463432312, 0.9269484877586365, 0.8186600804328918, 0.7957144379615784, 0.8582534193992615, 0.9026941061019897, 0.8383840918540955, 0.755445659160614, 0.6874306201934814, 0.7653684020042419, 0.7416852116584778, 0.8382931351661682, 0.8701339364051819, 0.9532739520072937]
[2025-06-13 20:04:58,099]: Mean: 0.82178897
[2025-06-13 20:04:58,099]: Min: 0.57318199
[2025-06-13 20:04:58,099]: Max: 1.00904655
[2025-06-13 20:04:58,099]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:04:58,100]: Sample Values (25 elements): [-0.05233066901564598, 0.14631736278533936, 0.007457581348717213, -0.18279452621936798, 0.00183458486571908, 0.05187004432082176, -0.1049356609582901, -0.08617580682039261, 0.2539084851741791, -0.11664866656064987, -0.27002644538879395, 0.12753428518772125, -0.06811477243900299, -0.17041759192943573, 0.07015219330787659, -0.04384452849626541, 0.1110495999455452, 0.05982130765914917, -0.007458757143467665, -0.060673508793115616, -0.06780222058296204, 0.1575782150030136, -0.0027614750433713198, -0.25658881664276123, -0.24839000403881073]
[2025-06-13 20:04:58,100]: Mean: -0.00070212
[2025-06-13 20:04:58,100]: Min: -0.39752051
[2025-06-13 20:04:58,100]: Max: 0.45463890
[2025-06-13 20:04:58,100]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,101]: Sample Values (25 elements): [0.5407952666282654, 0.6246604323387146, 0.6743358373641968, 0.7158827185630798, 0.3879571855068207, 0.5705479979515076, 0.5399293303489685, 0.6387541890144348, 0.6210282444953918, 0.5495699644088745, 0.5737077593803406, 0.557542085647583, 0.4682917594909668, 0.5361336469650269, 0.5376559495925903, 0.5282500982284546, 0.7073361873626709, 0.559540331363678, 0.6524117588996887, 0.5785569548606873, 0.6764693260192871, 0.6301599740982056, 0.6966487169265747, 0.6291665434837341, 0.5042582154273987]
[2025-06-13 20:04:58,101]: Mean: 0.58378357
[2025-06-13 20:04:58,101]: Min: 0.38795719
[2025-06-13 20:04:58,101]: Max: 0.71588272
[2025-06-13 20:04:58,101]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:04:58,102]: Sample Values (25 elements): [-0.026909388601779938, 0.05420931056141853, 0.0181021299213171, -0.0962054431438446, 0.07773224264383316, -0.021551504731178284, -0.049250513315200806, -0.09720613062381744, 0.039894070476293564, 0.007139062974601984, -0.0037404473405331373, -0.12345859408378601, 0.009378474205732346, -0.0052732559852302074, -0.041419681161642075, -0.08780195564031601, 0.017799055203795433, 0.025830285623669624, -0.09764629602432251, 0.10705298185348511, -0.0380050428211689, 0.017008036375045776, 0.06646184623241425, 0.07517927139997482, 0.011361852288246155]
[2025-06-13 20:04:58,102]: Mean: -0.00075383
[2025-06-13 20:04:58,102]: Min: -0.28933427
[2025-06-13 20:04:58,102]: Max: 0.30813077
[2025-06-13 20:04:58,102]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-13 20:04:58,102]: Sample Values (25 elements): [0.9905869960784912, 1.0406523942947388, 1.319438099861145, 1.0847936868667603, 0.9750878810882568, 1.0632191896438599, 0.8101524710655212, 0.9626652598381042, 0.8478385210037231, 1.053282618522644, 1.1060545444488525, 0.8940396904945374, 1.377914309501648, 1.0693364143371582, 1.2687716484069824, 1.1341071128845215, 1.1614360809326172, 0.9795015454292297, 1.0972896814346313, 1.169355869293213, 1.1828961372375488, 1.271532654762268, 1.2576661109924316, 1.2105779647827148, 0.9933845400810242]
[2025-06-13 20:04:58,103]: Mean: 1.09367752
[2025-06-13 20:04:58,103]: Min: 0.81015247
[2025-06-13 20:04:58,103]: Max: 1.57195413
[2025-06-13 20:04:58,103]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-13 20:04:58,112]: Sample Values (25 elements): [0.2788802683353424, -0.1392292082309723, 0.22520150244235992, -0.23934201896190643, 0.4426637589931488, -0.2274981290102005, 0.008897707797586918, -0.217797189950943, -0.17565952241420746, -0.1605641394853592, 0.3163308799266815, 0.5603376626968384, -0.18486271798610687, -0.15047775208950043, 0.33137354254722595, -0.3925422132015228, 0.037733204662799835, 0.6057168841362, 0.2615312933921814, -0.3752182722091675, 0.19811049103736877, 0.33187136054039, 0.07902666926383972, -0.06525697559118271, 0.20465485751628876]
[2025-06-13 20:04:58,112]: Mean: 0.00226420
[2025-06-13 20:04:58,113]: Min: -0.60939276
[2025-06-13 20:04:58,113]: Max: 0.73007059
[2025-06-13 20:04:58,113]: Checkpoint of model at path [checkpoint/ResNet20_parametrized_hardtanh.ckpt] will be used for QAT
[2025-06-13 20:04:58,113]: 


QAT of ResNet20 with parametrized_hardtanh down to 4 bits...
[2025-06-13 20:04:59,057]: [ResNet20_parametrized_hardtanh_quantized_4_bits] after configure_qat:
[2025-06-13 20:04:59,577]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 20:06:36,939]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 001 Train Loss: 0.3984 Train Acc: 0.8602 Eval Loss: 0.5662 Eval Acc: 0.8203 (LR: 0.00100000)
[2025-06-13 20:08:19,323]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 002 Train Loss: 0.4008 Train Acc: 0.8592 Eval Loss: 0.5920 Eval Acc: 0.8135 (LR: 0.00100000)
[2025-06-13 20:09:55,291]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 003 Train Loss: 0.4023 Train Acc: 0.8588 Eval Loss: 0.6233 Eval Acc: 0.8144 (LR: 0.00100000)
[2025-06-13 20:11:26,587]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 004 Train Loss: 0.4024 Train Acc: 0.8595 Eval Loss: 0.5332 Eval Acc: 0.8278 (LR: 0.00100000)
[2025-06-13 20:13:03,181]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 005 Train Loss: 0.4032 Train Acc: 0.8592 Eval Loss: 0.5230 Eval Acc: 0.8324 (LR: 0.00100000)
[2025-06-13 20:14:45,308]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 006 Train Loss: 0.4026 Train Acc: 0.8592 Eval Loss: 0.6569 Eval Acc: 0.7981 (LR: 0.00100000)
[2025-06-13 20:16:27,460]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 007 Train Loss: 0.4057 Train Acc: 0.8578 Eval Loss: 0.5674 Eval Acc: 0.8236 (LR: 0.00100000)
[2025-06-13 20:17:48,852]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 008 Train Loss: 0.4005 Train Acc: 0.8600 Eval Loss: 0.5510 Eval Acc: 0.8252 (LR: 0.00100000)
[2025-06-13 20:19:06,434]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 009 Train Loss: 0.3982 Train Acc: 0.8599 Eval Loss: 0.5176 Eval Acc: 0.8331 (LR: 0.00100000)
[2025-06-13 20:20:28,578]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 010 Train Loss: 0.4009 Train Acc: 0.8596 Eval Loss: 0.5722 Eval Acc: 0.8116 (LR: 0.00100000)
[2025-06-13 20:21:49,553]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 011 Train Loss: 0.3895 Train Acc: 0.8634 Eval Loss: 0.7438 Eval Acc: 0.7839 (LR: 0.00100000)
[2025-06-13 20:23:10,492]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 012 Train Loss: 0.3943 Train Acc: 0.8624 Eval Loss: 0.5967 Eval Acc: 0.8097 (LR: 0.00100000)
[2025-06-13 20:24:30,348]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 013 Train Loss: 0.3846 Train Acc: 0.8649 Eval Loss: 0.5937 Eval Acc: 0.8119 (LR: 0.00100000)
[2025-06-13 20:25:49,493]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 014 Train Loss: 0.3869 Train Acc: 0.8646 Eval Loss: 0.5224 Eval Acc: 0.8362 (LR: 0.00100000)
[2025-06-13 20:27:06,055]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 015 Train Loss: 0.3891 Train Acc: 0.8647 Eval Loss: 0.5329 Eval Acc: 0.8314 (LR: 0.00010000)
[2025-06-13 20:28:22,179]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 016 Train Loss: 0.3105 Train Acc: 0.8916 Eval Loss: 0.4017 Eval Acc: 0.8710 (LR: 0.00010000)
[2025-06-13 20:29:37,577]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 017 Train Loss: 0.2875 Train Acc: 0.9002 Eval Loss: 0.4080 Eval Acc: 0.8712 (LR: 0.00010000)
[2025-06-13 20:30:55,598]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 018 Train Loss: 0.2866 Train Acc: 0.9001 Eval Loss: 0.4091 Eval Acc: 0.8723 (LR: 0.00010000)
[2025-06-13 20:32:12,181]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 019 Train Loss: 0.2801 Train Acc: 0.9027 Eval Loss: 0.4077 Eval Acc: 0.8720 (LR: 0.00010000)
[2025-06-13 20:33:29,443]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 020 Train Loss: 0.2742 Train Acc: 0.9050 Eval Loss: 0.4072 Eval Acc: 0.8730 (LR: 0.00010000)
[2025-06-13 20:34:44,522]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 021 Train Loss: 0.2744 Train Acc: 0.9037 Eval Loss: 0.4054 Eval Acc: 0.8727 (LR: 0.00010000)
[2025-06-13 20:35:59,621]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 022 Train Loss: 0.2691 Train Acc: 0.9063 Eval Loss: 0.4043 Eval Acc: 0.8737 (LR: 0.00001000)
[2025-06-13 20:37:13,686]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 023 Train Loss: 0.2596 Train Acc: 0.9083 Eval Loss: 0.3941 Eval Acc: 0.8782 (LR: 0.00001000)
[2025-06-13 20:38:26,841]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 024 Train Loss: 0.2565 Train Acc: 0.9104 Eval Loss: 0.3953 Eval Acc: 0.8761 (LR: 0.00001000)
[2025-06-13 20:39:39,537]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 025 Train Loss: 0.2527 Train Acc: 0.9110 Eval Loss: 0.3973 Eval Acc: 0.8769 (LR: 0.00001000)
[2025-06-13 20:40:56,420]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 026 Train Loss: 0.2488 Train Acc: 0.9131 Eval Loss: 0.3962 Eval Acc: 0.8764 (LR: 0.00001000)
[2025-06-13 20:42:09,375]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 027 Train Loss: 0.2532 Train Acc: 0.9108 Eval Loss: 0.3918 Eval Acc: 0.8758 (LR: 0.00001000)
[2025-06-13 20:43:22,696]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 028 Train Loss: 0.2542 Train Acc: 0.9104 Eval Loss: 0.3966 Eval Acc: 0.8765 (LR: 0.00001000)
[2025-06-13 20:44:37,764]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 029 Train Loss: 0.2518 Train Acc: 0.9144 Eval Loss: 0.3915 Eval Acc: 0.8785 (LR: 0.00001000)
[2025-06-13 20:45:53,543]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 030 Train Loss: 0.2522 Train Acc: 0.9129 Eval Loss: 0.3981 Eval Acc: 0.8763 (LR: 0.00001000)
[2025-06-13 20:47:07,549]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 031 Train Loss: 0.2566 Train Acc: 0.9108 Eval Loss: 0.3973 Eval Acc: 0.8795 (LR: 0.00001000)
[2025-06-13 20:48:20,598]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 032 Train Loss: 0.2507 Train Acc: 0.9136 Eval Loss: 0.3970 Eval Acc: 0.8769 (LR: 0.00001000)
[2025-06-13 20:49:36,087]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 033 Train Loss: 0.2530 Train Acc: 0.9102 Eval Loss: 0.3945 Eval Acc: 0.8770 (LR: 0.00001000)
[2025-06-13 20:50:49,897]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 034 Train Loss: 0.2532 Train Acc: 0.9113 Eval Loss: 0.3948 Eval Acc: 0.8769 (LR: 0.00001000)
[2025-06-13 20:52:04,212]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 035 Train Loss: 0.2537 Train Acc: 0.9112 Eval Loss: 0.3974 Eval Acc: 0.8760 (LR: 0.00000100)
[2025-06-13 20:53:21,184]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 036 Train Loss: 0.2498 Train Acc: 0.9116 Eval Loss: 0.3936 Eval Acc: 0.8783 (LR: 0.00000100)
[2025-06-13 20:54:36,125]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 037 Train Loss: 0.2460 Train Acc: 0.9125 Eval Loss: 0.3927 Eval Acc: 0.8780 (LR: 0.00000100)
[2025-06-13 20:55:50,083]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 038 Train Loss: 0.2484 Train Acc: 0.9136 Eval Loss: 0.3931 Eval Acc: 0.8766 (LR: 0.00000100)
[2025-06-13 20:57:02,096]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 039 Train Loss: 0.2494 Train Acc: 0.9125 Eval Loss: 0.3964 Eval Acc: 0.8773 (LR: 0.00000100)
[2025-06-13 20:58:14,787]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 040 Train Loss: 0.2475 Train Acc: 0.9140 Eval Loss: 0.3940 Eval Acc: 0.8763 (LR: 0.00000100)
[2025-06-13 20:59:27,237]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 041 Train Loss: 0.2504 Train Acc: 0.9117 Eval Loss: 0.3943 Eval Acc: 0.8763 (LR: 0.00000010)
[2025-06-13 21:00:48,085]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 042 Train Loss: 0.2498 Train Acc: 0.9128 Eval Loss: 0.3967 Eval Acc: 0.8748 (LR: 0.00000010)
[2025-06-13 21:02:08,601]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 043 Train Loss: 0.2471 Train Acc: 0.9135 Eval Loss: 0.3924 Eval Acc: 0.8765 (LR: 0.00000010)
[2025-06-13 21:03:27,667]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 044 Train Loss: 0.2487 Train Acc: 0.9137 Eval Loss: 0.3932 Eval Acc: 0.8780 (LR: 0.00000010)
[2025-06-13 21:04:41,909]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 045 Train Loss: 0.2482 Train Acc: 0.9136 Eval Loss: 0.3932 Eval Acc: 0.8790 (LR: 0.00000010)
[2025-06-13 21:05:56,023]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 046 Train Loss: 0.2507 Train Acc: 0.9118 Eval Loss: 0.3975 Eval Acc: 0.8770 (LR: 0.00000010)
[2025-06-13 21:07:10,090]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 047 Train Loss: 0.2476 Train Acc: 0.9130 Eval Loss: 0.3933 Eval Acc: 0.8770 (LR: 0.00000010)
[2025-06-13 21:08:24,528]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 048 Train Loss: 0.2483 Train Acc: 0.9139 Eval Loss: 0.3928 Eval Acc: 0.8784 (LR: 0.00000010)
[2025-06-13 21:09:38,464]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 049 Train Loss: 0.2487 Train Acc: 0.9125 Eval Loss: 0.3938 Eval Acc: 0.8782 (LR: 0.00000010)
[2025-06-13 21:10:52,351]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 050 Train Loss: 0.2494 Train Acc: 0.9134 Eval Loss: 0.3949 Eval Acc: 0.8784 (LR: 0.00000010)
[2025-06-13 21:12:06,770]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 051 Train Loss: 0.2464 Train Acc: 0.9142 Eval Loss: 0.3919 Eval Acc: 0.8763 (LR: 0.00000010)
[2025-06-13 21:13:23,056]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 052 Train Loss: 0.2452 Train Acc: 0.9145 Eval Loss: 0.3956 Eval Acc: 0.8764 (LR: 0.00000010)
[2025-06-13 21:14:37,212]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 053 Train Loss: 0.2465 Train Acc: 0.9144 Eval Loss: 0.3956 Eval Acc: 0.8773 (LR: 0.00000010)
[2025-06-13 21:15:51,837]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 054 Train Loss: 0.2465 Train Acc: 0.9135 Eval Loss: 0.3924 Eval Acc: 0.8775 (LR: 0.00000010)
[2025-06-13 21:17:10,581]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 055 Train Loss: 0.2445 Train Acc: 0.9142 Eval Loss: 0.3943 Eval Acc: 0.8759 (LR: 0.00000010)
[2025-06-13 21:18:33,487]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 056 Train Loss: 0.2466 Train Acc: 0.9139 Eval Loss: 0.3925 Eval Acc: 0.8789 (LR: 0.00000010)
[2025-06-13 21:19:55,069]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 057 Train Loss: 0.2492 Train Acc: 0.9122 Eval Loss: 0.3938 Eval Acc: 0.8770 (LR: 0.00000010)
[2025-06-13 21:21:35,083]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 058 Train Loss: 0.2474 Train Acc: 0.9118 Eval Loss: 0.3912 Eval Acc: 0.8784 (LR: 0.00000010)
[2025-06-13 21:23:15,643]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 059 Train Loss: 0.2476 Train Acc: 0.9129 Eval Loss: 0.3962 Eval Acc: 0.8776 (LR: 0.00000010)
[2025-06-13 21:24:58,162]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Epoch: 060 Train Loss: 0.2503 Train Acc: 0.9120 Eval Loss: 0.3953 Eval Acc: 0.8785 (LR: 0.00000010)
[2025-06-13 21:24:58,162]: [ResNet20_parametrized_hardtanh_quantized_4_bits] Best Eval Accuracy: 0.8795
[2025-06-13 21:24:58,346]: 


Quantization of model down to 4 bits finished
[2025-06-13 21:24:58,346]: Model Architecture:
[2025-06-13 21:24:58,672]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1768], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.325760841369629, max_val=1.325760841369629)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0969], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7623481750488281, max_val=0.6905899047851562)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1250], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0802], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5516631603240967, max_val=0.6509264707565308)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1648], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0824], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5721147060394287, max_val=0.6632254123687744)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0996], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0949], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7707442045211792, max_val=0.6522603034973145)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2078], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0851], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6410627365112305, max_val=0.6348289251327515)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0876], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0817], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6138670444488525, max_val=0.6123030185699463)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2296], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0814], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6707735657691956, max_val=0.5504086017608643)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1216], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0742], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6112323999404907, max_val=0.5018144845962524)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0943], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6774107217788696, max_val=0.7365642786026001)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1920], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0715], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4772021174430847, max_val=0.5951591730117798)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0678], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0796], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6673974990844727, max_val=0.5261983871459961)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2108], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0757], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4848902225494385, max_val=0.6511000394821167)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0652], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0669], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5355385541915894, max_val=0.4673258662223816)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2236], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0721], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48608797788619995, max_val=0.5960626602172852)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0623], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0696], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5437113046646118, max_val=0.5001063346862793)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0704], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5402462482452393, max_val=0.5159194469451904)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1438], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0716], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5391498804092407, max_val=0.5354107618331909)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0650], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4855656027793884, max_val=0.4894843101501465)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1095], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0637], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45085543394088745, max_val=0.5048383474349976)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0418], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27647510170936584, max_val=0.3031168580055237)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-7, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4257], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 21:24:58,692]: 
Model Weights:
[2025-06-13 21:24:58,693]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-13 21:24:58,701]: Sample Values (25 elements): [0.04489290714263916, -0.015264678746461868, 0.03849809244275093, 0.0011155818356201053, -0.012169108726084232, -0.28474709391593933, -0.05157327651977539, 0.6576785445213318, -0.007754508871585131, -0.21856077015399933, 0.19311533868312836, -0.1961367279291153, -0.04546555504202843, 0.037431079894304276, -0.07703243941068649, 0.3560483455657959, -0.048037320375442505, 0.2655746638774872, -0.21961110830307007, 0.24130909144878387, -0.1678350269794464, 0.07615012675523758, -0.2730301022529602, -0.49033328890800476, 0.311533123254776]
[2025-06-13 21:24:58,707]: Mean: 0.00042829
[2025-06-13 21:24:58,714]: Min: -0.51380020
[2025-06-13 21:24:58,714]: Max: 0.65767854
[2025-06-13 21:24:58,714]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,715]: Sample Values (16 elements): [0.6403127908706665, 0.9374719262123108, 0.7260574102401733, 1.0586175918579102, 0.8391624689102173, 0.7621551752090454, 0.8860624432563782, 0.816821813583374, 0.853789210319519, 0.8442996144294739, 0.8315862417221069, 0.855404794216156, 0.8442299365997314, 0.7318601012229919, 0.7364062666893005, 1.0348132848739624]
[2025-06-13 21:24:58,715]: Mean: 0.83744073
[2025-06-13 21:24:58,715]: Min: 0.64031279
[2025-06-13 21:24:58,715]: Max: 1.05861759
[2025-06-13 21:24:58,718]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:24:58,718]: Sample Values (25 elements): [0.1937250941991806, 0.0, 0.0, -0.0968625470995903, 0.0, 0.0968625470995903, 0.0, -0.0968625470995903, -0.1937250941991806, -0.2905876338481903, 0.0, 0.1937250941991806, 0.0968625470995903, 0.1937250941991806, 0.0968625470995903, -0.0968625470995903, 0.1937250941991806, 0.0968625470995903, -0.1937250941991806, 0.0968625470995903, 0.1937250941991806, -0.1937250941991806, 0.1937250941991806, -0.1937250941991806, 0.0]
[2025-06-13 21:24:58,718]: Mean: -0.00403594
[2025-06-13 21:24:58,719]: Min: -0.77490038
[2025-06-13 21:24:58,719]: Max: 0.67803782
[2025-06-13 21:24:58,719]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,719]: Sample Values (16 elements): [0.8910160660743713, 0.7840425968170166, 0.7471392750740051, 0.7602571249008179, 0.7508610486984253, 0.6645849943161011, 0.8776348233222961, 0.7667155861854553, 1.2381926774978638, 1.4433262348175049, 0.7792295813560486, 1.0609267950057983, 1.0102813243865967, 0.8263579607009888, 0.7187933921813965, 0.6136724352836609]
[2025-06-13 21:24:58,721]: Mean: 0.87081444
[2025-06-13 21:24:58,721]: Min: 0.61367244
[2025-06-13 21:24:58,721]: Max: 1.44332623
[2025-06-13 21:24:58,723]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:24:58,723]: Sample Values (25 elements): [-0.16034528613090515, 0.16034528613090515, -0.16034528613090515, -0.24051792919635773, 0.0, 0.3206905722618103, -0.08017264306545258, 0.0, -0.08017264306545258, -0.16034528613090515, -0.24051792919635773, 0.08017264306545258, 0.24051792919635773, 0.0, -0.16034528613090515, -0.16034528613090515, -0.08017264306545258, -0.16034528613090515, 0.0, 0.08017264306545258, -0.3206905722618103, 0.08017264306545258, 0.4008632302284241, 0.08017264306545258, 0.08017264306545258]
[2025-06-13 21:24:58,724]: Mean: 0.00591552
[2025-06-13 21:24:58,724]: Min: -0.56120849
[2025-06-13 21:24:58,724]: Max: 0.64138114
[2025-06-13 21:24:58,724]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,726]: Sample Values (16 elements): [0.7502903342247009, 0.8115505576133728, 1.0026956796646118, 1.098200798034668, 0.8634281754493713, 0.5144498944282532, 0.6765061616897583, 0.5023304224014282, 0.7059307098388672, 0.5437410473823547, 0.7157130837440491, 0.768988311290741, 0.8096954822540283, 0.7477387189865112, 0.8649585247039795, 0.5907437801361084]
[2025-06-13 21:24:58,726]: Mean: 0.74793512
[2025-06-13 21:24:58,726]: Min: 0.50233042
[2025-06-13 21:24:58,727]: Max: 1.09820080
[2025-06-13 21:24:58,728]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:24:58,729]: Sample Values (25 elements): [0.16471202671527863, 0.16471202671527863, 0.0, 0.0, 0.0, -0.16471202671527863, -0.08235601335763931, 0.16471202671527863, -0.24706804752349854, 0.0, -0.16471202671527863, 0.08235601335763931, -0.32942405343055725, -0.24706804752349854, -0.16471202671527863, 0.08235601335763931, 0.24706804752349854, -0.16471202671527863, -0.08235601335763931, -0.16471202671527863, 0.0, 0.08235601335763931, -0.24706804752349854, 0.0, -0.08235601335763931]
[2025-06-13 21:24:58,729]: Mean: 0.00353874
[2025-06-13 21:24:58,730]: Min: -0.57649207
[2025-06-13 21:24:58,730]: Max: 0.65884811
[2025-06-13 21:24:58,731]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,731]: Sample Values (16 elements): [0.6622670292854309, 0.6047092080116272, 0.9956899881362915, 0.7003352046012878, 0.6345992684364319, 0.8663506507873535, 0.7101441025733948, 1.127854347229004, 0.8388577103614807, 0.6368458867073059, 0.750948429107666, 0.5807435512542725, 0.9937242865562439, 0.6871739029884338, 0.6872385740280151, 1.1624819040298462]
[2025-06-13 21:24:58,731]: Mean: 0.78999776
[2025-06-13 21:24:58,732]: Min: 0.58074355
[2025-06-13 21:24:58,732]: Max: 1.16248190
[2025-06-13 21:24:58,733]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:24:58,734]: Sample Values (25 elements): [0.09486696869134903, -0.09486696869134903, -0.18973393738269806, -0.09486696869134903, -0.09486696869134903, 0.09486696869134903, -0.09486696869134903, 0.0, 0.09486696869134903, 0.0, 0.0, -0.2846009135246277, 0.09486696869134903, 0.0, 0.18973393738269806, 0.0, 0.09486696869134903, 0.2846009135246277, 0.09486696869134903, -0.2846009135246277, 0.09486696869134903, 0.2846009135246277, -0.18973393738269806, 0.0, 0.0]
[2025-06-13 21:24:58,734]: Mean: 0.00695856
[2025-06-13 21:24:58,735]: Min: -0.75893575
[2025-06-13 21:24:58,736]: Max: 0.66406876
[2025-06-13 21:24:58,736]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,736]: Sample Values (16 elements): [0.7359649538993835, 0.9012438654899597, 0.7925499677658081, 0.6655778884887695, 0.7363393902778625, 0.7961001992225647, 0.698991060256958, 1.143993616104126, 0.8415537476539612, 0.6689491868019104, 0.6410122513771057, 0.7608281373977661, 0.9381231069564819, 0.7799456715583801, 0.6464188694953918, 0.6813246607780457]
[2025-06-13 21:24:58,737]: Mean: 0.77680725
[2025-06-13 21:24:58,737]: Min: 0.64101225
[2025-06-13 21:24:58,737]: Max: 1.14399362
[2025-06-13 21:24:58,740]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:24:58,741]: Sample Values (25 elements): [0.08505944907665253, -0.08505944907665253, 0.0, 0.0, -0.2551783323287964, -0.08505944907665253, -0.08505944907665253, 0.08505944907665253, -0.08505944907665253, -0.3402377963066101, -0.17011889815330505, -0.17011889815330505, -0.17011889815330505, 0.0, 0.08505944907665253, 0.2551783323287964, 0.0, 0.0, -0.17011889815330505, 0.0, 0.2551783323287964, 0.08505944907665253, 0.08505944907665253, -0.08505944907665253, 0.17011889815330505]
[2025-06-13 21:24:58,741]: Mean: -0.00682986
[2025-06-13 21:24:58,741]: Min: -0.68047559
[2025-06-13 21:24:58,742]: Max: 0.59541613
[2025-06-13 21:24:58,742]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,742]: Sample Values (16 elements): [1.0948667526245117, 0.5735395550727844, 0.8491485118865967, 0.7567664980888367, 0.623600423336029, 0.7368924617767334, 0.750554621219635, 0.4964652359485626, 0.636722207069397, 0.8619447946548462, 0.7260199785232544, 0.9963486194610596, 0.8496822118759155, 0.6232085824012756, 0.8550494909286499, 0.600225567817688]
[2025-06-13 21:24:58,742]: Mean: 0.75193965
[2025-06-13 21:24:58,743]: Min: 0.49646524
[2025-06-13 21:24:58,743]: Max: 1.09486675
[2025-06-13 21:24:58,746]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:24:58,747]: Sample Values (25 elements): [0.24523404240608215, 0.16348935663700104, 0.08174467831850052, 0.0, 0.08174467831850052, 0.16348935663700104, 0.3269787132740021, 0.16348935663700104, -0.08174467831850052, 0.0, 0.16348935663700104, -0.08174467831850052, -0.16348935663700104, 0.0, 0.16348935663700104, -0.08174467831850052, 0.408723384141922, 0.16348935663700104, -0.08174467831850052, -0.08174467831850052, -0.08174467831850052, -0.16348935663700104, -0.16348935663700104, -0.08174467831850052, -0.16348935663700104]
[2025-06-13 21:24:58,747]: Mean: 0.00560576
[2025-06-13 21:24:58,747]: Min: -0.65395743
[2025-06-13 21:24:58,748]: Max: 0.57221276
[2025-06-13 21:24:58,748]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-13 21:24:58,748]: Sample Values (16 elements): [0.8109318614006042, 0.6718412637710571, 1.1191376447677612, 0.5085732340812683, 0.696556568145752, 0.5639389753341675, 0.8092254996299744, 0.8812330365180969, 0.5968560576438904, 1.1483997106552124, 0.9087849855422974, 0.9052895903587341, 0.5769885182380676, 0.6368076801300049, 0.9447462558746338, 1.0562759637832642]
[2025-06-13 21:24:58,749]: Mean: 0.80222416
[2025-06-13 21:24:58,749]: Min: 0.50857323
[2025-06-13 21:24:58,749]: Max: 1.14839971
[2025-06-13 21:24:58,751]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-13 21:24:58,751]: Sample Values (25 elements): [-0.08141214400529861, 0.0, 0.08141214400529861, 0.16282428801059723, -0.24423643946647644, 0.16282428801059723, -0.08141214400529861, -0.08141214400529861, -0.08141214400529861, 0.0, 0.16282428801059723, 0.24423643946647644, 0.0, -0.08141214400529861, 0.24423643946647644, 0.16282428801059723, 0.08141214400529861, -0.08141214400529861, 0.24423643946647644, -0.16282428801059723, -0.08141214400529861, -0.08141214400529861, -0.08141214400529861, -0.08141214400529861, 0.0]
[2025-06-13 21:24:58,752]: Mean: 0.00413421
[2025-06-13 21:24:58,752]: Min: -0.65129715
[2025-06-13 21:24:58,752]: Max: 0.56988502
[2025-06-13 21:24:58,753]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,753]: Sample Values (25 elements): [0.709191620349884, 0.82889723777771, 0.6411195993423462, 0.6689065098762512, 0.5982523560523987, 0.7343674302101135, 0.6548171043395996, 0.6185534000396729, 0.6633267402648926, 0.7226907014846802, 0.6887907981872559, 0.6060376167297363, 0.5856407284736633, 0.7573494911193848, 0.513476550579071, 0.6401767134666443, 0.6922871470451355, 0.7763649821281433, 0.8381093144416809, 0.8172746300697327, 0.6084862947463989, 0.6181029677391052, 0.6397188305854797, 0.8551532626152039, 0.6401340961456299]
[2025-06-13 21:24:58,753]: Mean: 0.67547625
[2025-06-13 21:24:58,754]: Min: 0.49145797
[2025-06-13 21:24:58,754]: Max: 0.85515326
[2025-06-13 21:24:58,756]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:24:58,756]: Sample Values (25 elements): [0.22260937094688416, 0.1484062522649765, -0.07420312613248825, 0.0, -0.1484062522649765, 0.0, 0.1484062522649765, -0.07420312613248825, -0.22260937094688416, -0.07420312613248825, 0.0, 0.0, 0.0, 0.07420312613248825, 0.0, -0.1484062522649765, 0.07420312613248825, 0.1484062522649765, 0.22260937094688416, -0.22260937094688416, 0.22260937094688416, 0.07420312613248825, -0.07420312613248825, -0.22260937094688416, -0.07420312613248825]
[2025-06-13 21:24:58,757]: Mean: -0.00200484
[2025-06-13 21:24:58,757]: Min: -0.59362501
[2025-06-13 21:24:58,757]: Max: 0.51942188
[2025-06-13 21:24:58,757]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,758]: Sample Values (25 elements): [0.7015120387077332, 0.8887715339660645, 0.7465444207191467, 0.8448838591575623, 0.9136916995048523, 0.7839946150779724, 0.7721797823905945, 0.8915536403656006, 0.7653717398643494, 0.9868987202644348, 0.7147095203399658, 0.8907228708267212, 0.9933876991271973, 0.7673565745353699, 0.993135392665863, 0.9201816320419312, 0.8759343028068542, 1.0379997491836548, 0.8692175149917603, 0.868344247341156, 0.9135247468948364, 0.8215768337249756, 0.9432587623596191, 0.8172647356987, 0.7937111854553223]
[2025-06-13 21:24:58,758]: Mean: 0.86573792
[2025-06-13 21:24:58,758]: Min: 0.70151204
[2025-06-13 21:24:58,759]: Max: 1.03799975
[2025-06-13 21:24:58,760]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-13 21:24:58,761]: Sample Values (25 elements): [-0.2827950119972229, 0.2827950119972229, -0.2827950119972229, 0.09426500648260117, 0.18853001296520233, 0.09426500648260117, 0.09426500648260117, -0.09426500648260117, -0.18853001296520233, 0.2827950119972229, 0.2827950119972229, 0.09426500648260117, -0.37706002593040466, 0.0, 0.37706002593040466, 0.18853001296520233, -0.4713250398635864, -0.18853001296520233, 0.18853001296520233, 0.09426500648260117, -0.37706002593040466, 0.18853001296520233, 0.0, 0.09426500648260117, 0.09426500648260117]
[2025-06-13 21:24:58,761]: Mean: 0.00754856
[2025-06-13 21:24:58,762]: Min: -0.65985507
[2025-06-13 21:24:58,762]: Max: 0.75412005
[2025-06-13 21:24:58,762]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,762]: Sample Values (25 elements): [0.44549620151519775, 0.7209446430206299, 0.4781913161277771, 0.48587766289711, 0.6254334449768066, 0.7621428370475769, 0.6191223859786987, 0.7332899570465088, 0.47747236490249634, 0.6987267136573792, 0.6598396897315979, 0.5238415002822876, 0.5056675672531128, 0.5257227420806885, 0.6843955516815186, 0.5527201294898987, 0.771265983581543, 0.5489310622215271, 0.6270556449890137, 0.7457168102264404, 0.7569389939308167, 0.485299676656723, 0.37392017245292664, 0.6455336213111877, 0.44697490334510803]
[2025-06-13 21:24:58,763]: Mean: 0.58393192
[2025-06-13 21:24:58,763]: Min: 0.37392017
[2025-06-13 21:24:58,763]: Max: 0.77470559
[2025-06-13 21:24:58,765]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:24:58,766]: Sample Values (25 elements): [-0.07149074971675873, 0.07149074971675873, 0.21447224915027618, 0.0, 0.0, -0.07149074971675873, 0.07149074971675873, 0.21447224915027618, 0.14298149943351746, -0.14298149943351746, 0.14298149943351746, 0.07149074971675873, -0.14298149943351746, 0.14298149943351746, 0.21447224915027618, -0.07149074971675873, -0.21447224915027618, 0.14298149943351746, 0.07149074971675873, 0.0, 0.14298149943351746, 0.0, 0.07149074971675873, 0.14298149943351746, 0.0]
[2025-06-13 21:24:58,766]: Mean: -0.00058955
[2025-06-13 21:24:58,766]: Min: -0.50043523
[2025-06-13 21:24:58,767]: Max: 0.57192600
[2025-06-13 21:24:58,767]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,768]: Sample Values (25 elements): [0.6621915102005005, 0.5846527814865112, 0.7262732982635498, 0.7845059633255005, 0.5430220365524292, 0.5049253702163696, 0.5111205577850342, 0.6387029886245728, 0.5112579464912415, 0.5463995933532715, 0.5689412951469421, 0.688454806804657, 0.7165027856826782, 0.6751667857170105, 0.48739558458328247, 0.7019975185394287, 0.6104775071144104, 0.5892008543014526, 0.4964008331298828, 0.6228460073471069, 0.7643776535987854, 0.5680508017539978, 0.6286748051643372, 0.5500244498252869, 0.6792383790016174]
[2025-06-13 21:24:58,768]: Mean: 0.61945993
[2025-06-13 21:24:58,768]: Min: 0.48739558
[2025-06-13 21:24:58,769]: Max: 0.78450596
[2025-06-13 21:24:58,772]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:24:58,773]: Sample Values (25 elements): [0.0, 0.07957306504249573, 0.07957306504249573, 0.15914613008499146, -0.07957306504249573, 0.0, 0.15914613008499146, 0.07957306504249573, 0.07957306504249573, 0.15914613008499146, 0.07957306504249573, 0.0, -0.07957306504249573, -0.07957306504249573, 0.15914613008499146, 0.23871919512748718, 0.23871919512748718, -0.15914613008499146, 0.07957306504249573, 0.0, -0.07957306504249573, 0.0, -0.07957306504249573, 0.23871919512748718, -0.23871919512748718]
[2025-06-13 21:24:58,774]: Mean: 0.00177002
[2025-06-13 21:24:58,774]: Min: -0.63658452
[2025-06-13 21:24:58,774]: Max: 0.55701149
[2025-06-13 21:24:58,774]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,776]: Sample Values (25 elements): [0.541030764579773, 0.6482988595962524, 0.7713119387626648, 0.973582923412323, 0.7183088064193726, 1.051299810409546, 0.6734204292297363, 0.6924843788146973, 0.8526312708854675, 0.7730675339698792, 0.5375509858131409, 0.7476234436035156, 0.8394260406494141, 0.7686101794242859, 0.932498574256897, 0.8884571194648743, 0.7270462512969971, 0.7652407288551331, 0.8162258267402649, 0.7926468849182129, 0.731228768825531, 0.7294982075691223, 0.9255577921867371, 0.7839862108230591, 0.7635482549667358]
[2025-06-13 21:24:58,776]: Mean: 0.76660657
[2025-06-13 21:24:58,776]: Min: 0.53755099
[2025-06-13 21:24:58,777]: Max: 1.05129981
[2025-06-13 21:24:58,779]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:24:58,779]: Sample Values (25 elements): [0.0, 0.0, 0.07573268562555313, 0.0, 0.0, 0.0, 0.0, 0.3029307425022125, -0.22719806432724, -0.3029307425022125, 0.07573268562555313, 0.07573268562555313, -0.07573268562555313, 0.68159419298172, -0.07573268562555313, -0.15146537125110626, 0.15146537125110626, 0.07573268562555313, 0.0, -0.3029307425022125, 0.15146537125110626, 0.0, -0.07573268562555313, -0.15146537125110626, 0.15146537125110626]
[2025-06-13 21:24:58,780]: Mean: -0.00123263
[2025-06-13 21:24:58,780]: Min: -0.45439613
[2025-06-13 21:24:58,781]: Max: 0.68159419
[2025-06-13 21:24:58,781]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,781]: Sample Values (25 elements): [0.6204516291618347, 0.5334272384643555, 0.5493067502975464, 0.7441922426223755, 0.6312479972839355, 0.6163812875747681, 0.6997931003570557, 0.6106173992156982, 0.6758009195327759, 0.6932596564292908, 0.6537771821022034, 0.6068207025527954, 0.5818748474121094, 0.6522603034973145, 0.775069534778595, 0.5600417852401733, 0.6650916934013367, 0.5867307782173157, 0.6136430501937866, 0.6003337502479553, 0.5870137810707092, 0.5943916440010071, 0.5687244534492493, 0.602336049079895, 0.5859572887420654]
[2025-06-13 21:24:58,781]: Mean: 0.62565005
[2025-06-13 21:24:58,782]: Min: 0.48721558
[2025-06-13 21:24:58,782]: Max: 0.77506953
[2025-06-13 21:24:58,784]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:24:58,784]: Sample Values (25 elements): [0.13371525704860687, -0.26743051409721375, 0.0, 0.0, 0.0, 0.06685762852430344, 0.06685762852430344, 0.06685762852430344, -0.2005728781223297, -0.13371525704860687, -0.13371525704860687, 0.06685762852430344, -0.13371525704860687, -0.13371525704860687, -0.06685762852430344, -0.13371525704860687, 0.2005728781223297, 0.0, -0.2005728781223297, -0.06685762852430344, -0.2005728781223297, 0.0, -0.06685762852430344, -0.06685762852430344, 0.0]
[2025-06-13 21:24:58,785]: Mean: 0.00572381
[2025-06-13 21:24:58,785]: Min: -0.53486103
[2025-06-13 21:24:58,785]: Max: 0.46800339
[2025-06-13 21:24:58,785]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-13 21:24:58,786]: Sample Values (25 elements): [0.7886480093002319, 0.7140201330184937, 0.7564753890037537, 0.7993370294570923, 0.7283411622047424, 0.8652909994125366, 0.7445254921913147, 0.9516950249671936, 0.9029655456542969, 0.843104362487793, 0.937597393989563, 0.736332893371582, 0.5555695295333862, 0.958271324634552, 1.0364563465118408, 0.7840986847877502, 1.0569759607315063, 0.9356153011322021, 0.8758131861686707, 1.031006932258606, 0.9475565552711487, 0.5945985913276672, 0.6411039233207703, 0.7549841403961182, 0.8016615509986877]
[2025-06-13 21:24:58,786]: Mean: 0.82459903
[2025-06-13 21:24:58,786]: Min: 0.55556953
[2025-06-13 21:24:58,787]: Max: 1.05697596
[2025-06-13 21:24:58,788]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-13 21:24:58,789]: Sample Values (25 elements): [0.14428676664829254, 0.0, 0.14428676664829254, -0.07214338332414627, -0.07214338332414627, 0.07214338332414627, -0.07214338332414627, -0.07214338332414627, -0.07214338332414627, -0.14428676664829254, 0.2885735332965851, 0.07214338332414627, 0.14428676664829254, -0.14428676664829254, -0.2164301574230194, 0.2885735332965851, -0.2164301574230194, -0.07214338332414627, 0.0, 0.07214338332414627, 0.07214338332414627, -0.2164301574230194, 0.07214338332414627, -0.07214338332414627, 0.0]
[2025-06-13 21:24:58,789]: Mean: 0.00096677
[2025-06-13 21:24:58,790]: Min: -0.50500369
[2025-06-13 21:24:58,790]: Max: 0.57714707
[2025-06-13 21:24:58,790]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,790]: Sample Values (25 elements): [0.5356987714767456, 0.463509738445282, 0.5100000500679016, 0.4929923117160797, 0.5180267095565796, 0.5692520141601562, 0.5258133411407471, 0.4780753552913666, 0.4987724721431732, 0.4984589219093323, 0.650393009185791, 0.5364396572113037, 0.5275022983551025, 0.5241851806640625, 0.3923931121826172, 0.5359699726104736, 0.5700197219848633, 0.4874163568019867, 0.45519721508026123, 0.6135784387588501, 0.46179547905921936, 0.49359330534935, 0.6216623783111572, 0.472844660282135, 0.4913724660873413]
[2025-06-13 21:24:58,791]: Mean: 0.51905495
[2025-06-13 21:24:58,791]: Min: 0.38472086
[2025-06-13 21:24:58,791]: Max: 0.65039301
[2025-06-13 21:24:58,793]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:24:58,794]: Sample Values (25 elements): [-0.06958784908056259, -0.13917569816112518, 0.0, -0.06958784908056259, 0.0, 0.20876353979110718, -0.13917569816112518, 0.20876353979110718, 0.13917569816112518, 0.20876353979110718, 0.06958784908056259, -0.20876353979110718, 0.34793925285339355, 0.13917569816112518, -0.13917569816112518, 0.06958784908056259, 0.13917569816112518, 0.13917569816112518, 0.0, -0.06958784908056259, -0.13917569816112518, 0.06958784908056259, -0.06958784908056259, 0.20876353979110718, 0.0]
[2025-06-13 21:24:58,794]: Mean: -0.00007740
[2025-06-13 21:24:58,794]: Min: -0.55670279
[2025-06-13 21:24:58,795]: Max: 0.48711494
[2025-06-13 21:24:58,795]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,795]: Sample Values (25 elements): [0.8262214064598083, 0.9856334328651428, 0.8817141652107239, 1.0729318857192993, 0.8248864412307739, 0.9044711589813232, 0.9746572375297546, 0.8383866548538208, 0.9665831327438354, 0.6771958470344543, 0.873710036277771, 0.9984942674636841, 0.7951168417930603, 0.9064050316810608, 0.7602942585945129, 0.803267240524292, 0.858587384223938, 0.9801172018051147, 0.873776376247406, 0.9289541840553284, 0.934569776058197, 0.8855471611022949, 0.8485437631607056, 0.7637218832969666, 0.7991737723350525]
[2025-06-13 21:24:58,796]: Mean: 0.87081087
[2025-06-13 21:24:58,796]: Min: 0.67719585
[2025-06-13 21:24:58,796]: Max: 1.07293189
[2025-06-13 21:24:58,798]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-13 21:24:58,798]: Sample Values (25 elements): [-0.14082209765911102, 0.0, -0.28164419531822205, 0.14082209765911102, 0.07041104882955551, -0.28164419531822205, -0.14082209765911102, 0.0, 0.21123313903808594, -0.07041104882955551, 0.28164419531822205, 0.0, -0.07041104882955551, 0.07041104882955551, -0.14082209765911102, -0.07041104882955551, 0.21123313903808594, 0.0, -0.28164419531822205, 0.21123313903808594, 0.07041104882955551, 0.4224662780761719, 0.0, -0.07041104882955551, 0.0]
[2025-06-13 21:24:58,799]: Mean: 0.00051571
[2025-06-13 21:24:58,799]: Min: -0.56328839
[2025-06-13 21:24:58,799]: Max: 0.49287733
[2025-06-13 21:24:58,799]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,800]: Sample Values (25 elements): [0.817775547504425, 0.49107131361961365, 0.6241793632507324, 0.7363951206207275, 0.5062154531478882, 0.6041092872619629, 0.519270122051239, 0.6817150712013245, 0.401004433631897, 0.5833581686019897, 0.5107051134109497, 0.639404833316803, 0.5589183568954468, 0.602318525314331, 0.697185218334198, 0.6029635071754456, 0.5578626990318298, 0.47005459666252136, 0.4291665852069855, 0.3445740044116974, 0.8080095052719116, 0.5940355062484741, 0.5958481431007385, 0.5627544522285461, 0.5507693886756897]
[2025-06-13 21:24:58,801]: Mean: 0.60338199
[2025-06-13 21:24:58,809]: Min: 0.34457400
[2025-06-13 21:24:58,813]: Max: 0.82940418
[2025-06-13 21:24:58,848]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:24:58,863]: Sample Values (25 elements): [0.0, 0.0, -0.07163737714290619, -0.07163737714290619, 0.0, 0.07163737714290619, -0.07163737714290619, 0.07163737714290619, 0.0, 0.07163737714290619, -0.07163737714290619, 0.0, -0.14327475428581238, 0.21491213142871857, 0.0, 0.14327475428581238, 0.0, -0.14327475428581238, -0.14327475428581238, -0.07163737714290619, -0.14327475428581238, -0.14327475428581238, 0.14327475428581238, 0.0, 0.14327475428581238]
[2025-06-13 21:24:58,864]: Mean: -0.00023708
[2025-06-13 21:24:58,865]: Min: -0.57309902
[2025-06-13 21:24:58,865]: Max: 0.50146163
[2025-06-13 21:24:58,865]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,865]: Sample Values (25 elements): [0.5896527767181396, 0.5950248837471008, 0.5769751071929932, 0.6697213649749756, 0.4576214849948883, 0.6208329200744629, 0.5824339389801025, 0.550607442855835, 0.7026680707931519, 0.5502451062202454, 0.49538034200668335, 0.6384191513061523, 0.6631001234054565, 0.5546448826789856, 0.6121701598167419, 0.5801391005516052, 0.558137059211731, 0.5580422282218933, 0.55752032995224, 0.6032009720802307, 0.6510436534881592, 0.5999256372451782, 0.5092390775680542, 0.5501387715339661, 0.63704514503479]
[2025-06-13 21:24:58,866]: Mean: 0.58772504
[2025-06-13 21:24:58,866]: Min: 0.45762148
[2025-06-13 21:24:58,867]: Max: 0.70266807
[2025-06-13 21:24:58,869]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:24:58,869]: Sample Values (25 elements): [0.06500332802534103, -0.06500332802534103, -0.06500332802534103, 0.0, -0.06500332802534103, 0.0, 0.1950099766254425, 0.390019953250885, -0.13000665605068207, 0.0, 0.06500332802534103, 0.0, 0.0, -0.06500332802534103, 0.06500332802534103, 0.06500332802534103, 0.13000665605068207, -0.06500332802534103, 0.06500332802534103, 0.0, 0.13000665605068207, 0.0, 0.0, -0.06500332802534103, -0.06500332802534103]
[2025-06-13 21:24:58,870]: Mean: 0.00160816
[2025-06-13 21:24:58,870]: Min: -0.45502329
[2025-06-13 21:24:58,871]: Max: 0.52002662
[2025-06-13 21:24:58,871]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,871]: Sample Values (25 elements): [0.8089507818222046, 0.788773238658905, 0.8237247467041016, 0.8014593720436096, 0.8229773640632629, 0.7548819780349731, 0.821370542049408, 0.804376482963562, 0.9820820093154907, 0.7060492038726807, 0.9798351526260376, 0.9666908979415894, 0.9918726682662964, 0.8075048327445984, 0.8240156173706055, 0.7274475693702698, 0.8830982446670532, 0.7376278042793274, 0.7955389618873596, 0.7610251307487488, 0.6967573165893555, 0.7415850162506104, 0.9052746295928955, 1.0287790298461914, 0.7689220905303955]
[2025-06-13 21:24:58,872]: Mean: 0.81041187
[2025-06-13 21:24:58,872]: Min: 0.59802312
[2025-06-13 21:24:58,872]: Max: 1.02877903
[2025-06-13 21:24:58,875]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:24:58,877]: Sample Values (25 elements): [-0.06371292471885681, 0.0, -0.06371292471885681, 0.0, -0.19113877415657043, 0.19113877415657043, -0.19113877415657043, 0.06371292471885681, 0.12742584943771362, 0.0, 0.0, -0.06371292471885681, 0.0, -0.06371292471885681, -0.25485169887542725, -0.06371292471885681, 0.12742584943771362, 0.0, 0.06371292471885681, 0.12742584943771362, -0.12742584943771362, -0.06371292471885681, 0.12742584943771362, 0.0, -0.19113877415657043]
[2025-06-13 21:24:58,877]: Mean: -0.00127550
[2025-06-13 21:24:58,877]: Min: -0.44599047
[2025-06-13 21:24:58,878]: Max: 0.50970340
[2025-06-13 21:24:58,878]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,878]: Sample Values (25 elements): [0.6022251844406128, 0.5487879514694214, 0.554094135761261, 0.44513779878616333, 0.4526442289352417, 0.5723770260810852, 0.6228660345077515, 0.5974267721176147, 0.5406038761138916, 0.4540197253227234, 0.6191186904907227, 0.5751209259033203, 0.4384405016899109, 0.6576929092407227, 0.5615994334220886, 0.5138922929763794, 0.4990059435367584, 0.5265287756919861, 0.5112935900688171, 0.6161170601844788, 0.5872880220413208, 0.5987510085105896, 0.6812482476234436, 0.5256331562995911, 0.5573049187660217]
[2025-06-13 21:24:58,878]: Mean: 0.55996060
[2025-06-13 21:24:58,879]: Min: 0.36784133
[2025-06-13 21:24:58,879]: Max: 0.68124825
[2025-06-13 21:24:58,881]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:24:58,881]: Sample Values (25 elements): [0.03863946348428726, 0.07727892696857452, 0.03863946348428726, 0.11591839045286179, 0.0, 0.0, -0.07727892696857452, 0.0, -0.11591839045286179, -0.07727892696857452, 0.11591839045286179, 0.03863946348428726, 0.1931973099708557, -0.07727892696857452, -0.07727892696857452, -0.11591839045286179, -0.03863946348428726, 0.0, 0.0, -0.03863946348428726, -0.1931973099708557, -0.11591839045286179, 0.03863946348428726, -0.07727892696857452, -0.03863946348428726]
[2025-06-13 21:24:58,881]: Mean: -0.00063204
[2025-06-13 21:24:58,883]: Min: -0.27047625
[2025-06-13 21:24:58,884]: Max: 0.30911571
[2025-06-13 21:24:58,884]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-13 21:24:58,884]: Sample Values (25 elements): [0.8903298377990723, 0.9195012450218201, 0.9155805706977844, 1.0518858432769775, 0.858291506767273, 0.9723051190376282, 1.1948704719543457, 0.7309390902519226, 1.0760300159454346, 1.1355654001235962, 0.9729926586151123, 1.0880165100097656, 0.9917922616004944, 0.9332219362258911, 0.9831272959709167, 1.101314902305603, 0.8243129849433899, 1.1234225034713745, 1.0876215696334839, 1.0888651609420776, 1.1492491960525513, 1.3030006885528564, 1.1763396263122559, 1.2111061811447144, 1.0188125371932983]
[2025-06-13 21:24:58,884]: Mean: 1.05679297
[2025-06-13 21:24:58,885]: Min: 0.73093909
[2025-06-13 21:24:58,885]: Max: 1.64740193
[2025-06-13 21:24:58,885]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-13 21:24:58,886]: Sample Values (25 elements): [-0.21650870144367218, 0.08427131921052933, 0.10995228588581085, -0.14915528893470764, -0.0621170848608017, 0.10560380667448044, 0.4220472276210785, 0.2966383993625641, -0.3954293429851532, -0.049570608884096146, -0.22749543190002441, 0.3923488259315491, -0.11810822039842606, 0.3049880862236023, 0.3878077566623688, -0.09322753548622131, 0.271584153175354, -0.21205781400203705, -0.2442002147436142, -0.25493863224983215, 0.20236633718013763, -0.19323253631591797, -0.03844132274389267, -0.27715662121772766, -0.12979696691036224]
[2025-06-13 21:24:58,886]: Mean: 0.00202774
[2025-06-13 21:24:58,887]: Min: -0.62983781
[2025-06-13 21:24:58,887]: Max: 0.77909195
[2025-06-13 21:24:58,887]: 


QAT of ResNet20 with parametrized_hardtanh down to 3 bits...
[2025-06-13 21:24:59,256]: [ResNet20_parametrized_hardtanh_quantized_3_bits] after configure_qat:
[2025-06-13 21:24:59,377]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 21:26:34,085]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 001 Train Loss: 0.4744 Train Acc: 0.8325 Eval Loss: 0.6784 Eval Acc: 0.7893 (LR: 0.00100000)
[2025-06-13 21:28:07,844]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 002 Train Loss: 0.4721 Train Acc: 0.8358 Eval Loss: 0.6400 Eval Acc: 0.7901 (LR: 0.00100000)
[2025-06-13 21:29:50,718]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 003 Train Loss: 0.4838 Train Acc: 0.8307 Eval Loss: 0.6900 Eval Acc: 0.7812 (LR: 0.00100000)
[2025-06-13 21:31:30,618]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 004 Train Loss: 0.4798 Train Acc: 0.8321 Eval Loss: 0.6111 Eval Acc: 0.8020 (LR: 0.00100000)
[2025-06-13 21:33:07,442]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 005 Train Loss: 0.4734 Train Acc: 0.8327 Eval Loss: 0.8176 Eval Acc: 0.7557 (LR: 0.00100000)
[2025-06-13 21:34:40,317]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 006 Train Loss: 0.4816 Train Acc: 0.8311 Eval Loss: 0.7894 Eval Acc: 0.7679 (LR: 0.00100000)
[2025-06-13 21:36:10,985]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 007 Train Loss: 0.4774 Train Acc: 0.8333 Eval Loss: 0.6244 Eval Acc: 0.7985 (LR: 0.00100000)
[2025-06-13 21:37:32,591]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 008 Train Loss: 0.4785 Train Acc: 0.8334 Eval Loss: 0.8247 Eval Acc: 0.7565 (LR: 0.00100000)
[2025-06-13 21:38:54,305]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 009 Train Loss: 0.4719 Train Acc: 0.8358 Eval Loss: 0.7960 Eval Acc: 0.7553 (LR: 0.00100000)
[2025-06-13 21:40:16,270]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 010 Train Loss: 0.4730 Train Acc: 0.8349 Eval Loss: 0.5968 Eval Acc: 0.8051 (LR: 0.00100000)
[2025-06-13 21:41:33,942]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 011 Train Loss: 0.4721 Train Acc: 0.8361 Eval Loss: 0.8484 Eval Acc: 0.7443 (LR: 0.00100000)
[2025-06-13 21:42:51,935]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 012 Train Loss: 0.4689 Train Acc: 0.8363 Eval Loss: 0.8202 Eval Acc: 0.7607 (LR: 0.00100000)
[2025-06-13 21:44:09,794]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 013 Train Loss: 0.4697 Train Acc: 0.8367 Eval Loss: 0.6211 Eval Acc: 0.8048 (LR: 0.00100000)
[2025-06-13 21:45:39,905]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 014 Train Loss: 0.4673 Train Acc: 0.8353 Eval Loss: 0.7416 Eval Acc: 0.7724 (LR: 0.00100000)
[2025-06-13 21:47:10,435]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 015 Train Loss: 0.4679 Train Acc: 0.8372 Eval Loss: 0.6994 Eval Acc: 0.7869 (LR: 0.00100000)
[2025-06-13 21:48:32,239]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 016 Train Loss: 0.4666 Train Acc: 0.8372 Eval Loss: 0.5450 Eval Acc: 0.8196 (LR: 0.00100000)
[2025-06-13 21:49:55,981]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 017 Train Loss: 0.4639 Train Acc: 0.8376 Eval Loss: 0.7699 Eval Acc: 0.7575 (LR: 0.00100000)
[2025-06-13 21:51:20,574]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 018 Train Loss: 0.4646 Train Acc: 0.8385 Eval Loss: 0.7938 Eval Acc: 0.7504 (LR: 0.00100000)
[2025-06-13 21:52:41,706]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 019 Train Loss: 0.4631 Train Acc: 0.8384 Eval Loss: 0.9064 Eval Acc: 0.7374 (LR: 0.00100000)
[2025-06-13 21:54:03,892]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 020 Train Loss: 0.4580 Train Acc: 0.8410 Eval Loss: 0.5656 Eval Acc: 0.8201 (LR: 0.00100000)
[2025-06-13 21:55:44,427]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 021 Train Loss: 0.4575 Train Acc: 0.8388 Eval Loss: 0.5355 Eval Acc: 0.8249 (LR: 0.00100000)
[2025-06-13 21:57:22,309]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 022 Train Loss: 0.4585 Train Acc: 0.8395 Eval Loss: 0.7451 Eval Acc: 0.7565 (LR: 0.00100000)
[2025-06-13 21:58:58,858]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 023 Train Loss: 0.4524 Train Acc: 0.8424 Eval Loss: 0.6850 Eval Acc: 0.7825 (LR: 0.00100000)
[2025-06-13 22:00:31,113]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 024 Train Loss: 0.4493 Train Acc: 0.8430 Eval Loss: 0.6432 Eval Acc: 0.7901 (LR: 0.00100000)
[2025-06-13 22:02:13,494]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 025 Train Loss: 0.4518 Train Acc: 0.8419 Eval Loss: 0.5539 Eval Acc: 0.8157 (LR: 0.00100000)
[2025-06-13 22:03:54,853]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 026 Train Loss: 0.4503 Train Acc: 0.8419 Eval Loss: 0.7616 Eval Acc: 0.7631 (LR: 0.00100000)
[2025-06-13 22:05:37,406]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 027 Train Loss: 0.4481 Train Acc: 0.8451 Eval Loss: 0.7980 Eval Acc: 0.7678 (LR: 0.00010000)
[2025-06-13 22:07:02,693]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 028 Train Loss: 0.3624 Train Acc: 0.8724 Eval Loss: 0.4154 Eval Acc: 0.8659 (LR: 0.00010000)
[2025-06-13 22:08:26,440]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 029 Train Loss: 0.3427 Train Acc: 0.8807 Eval Loss: 0.4095 Eval Acc: 0.8638 (LR: 0.00010000)
[2025-06-13 22:09:49,938]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 030 Train Loss: 0.3340 Train Acc: 0.8832 Eval Loss: 0.4216 Eval Acc: 0.8629 (LR: 0.00010000)
[2025-06-13 22:11:16,280]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 031 Train Loss: 0.3324 Train Acc: 0.8827 Eval Loss: 0.4189 Eval Acc: 0.8652 (LR: 0.00010000)
[2025-06-13 22:12:41,873]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 032 Train Loss: 0.3256 Train Acc: 0.8864 Eval Loss: 0.4334 Eval Acc: 0.8605 (LR: 0.00010000)
[2025-06-13 22:14:10,336]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 033 Train Loss: 0.3260 Train Acc: 0.8848 Eval Loss: 0.4224 Eval Acc: 0.8601 (LR: 0.00010000)
[2025-06-13 22:15:56,308]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 034 Train Loss: 0.3195 Train Acc: 0.8884 Eval Loss: 0.4208 Eval Acc: 0.8678 (LR: 0.00010000)
[2025-06-13 22:17:26,274]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 035 Train Loss: 0.3206 Train Acc: 0.8879 Eval Loss: 0.4268 Eval Acc: 0.8602 (LR: 0.00001000)
[2025-06-13 22:18:53,548]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 036 Train Loss: 0.3040 Train Acc: 0.8928 Eval Loss: 0.3988 Eval Acc: 0.8721 (LR: 0.00001000)
[2025-06-13 22:20:21,633]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 037 Train Loss: 0.2992 Train Acc: 0.8955 Eval Loss: 0.4017 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-06-13 22:21:51,029]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 038 Train Loss: 0.2963 Train Acc: 0.8957 Eval Loss: 0.3987 Eval Acc: 0.8722 (LR: 0.00001000)
[2025-06-13 22:23:19,540]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 039 Train Loss: 0.2962 Train Acc: 0.8966 Eval Loss: 0.4023 Eval Acc: 0.8713 (LR: 0.00001000)
[2025-06-13 22:24:49,532]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 040 Train Loss: 0.2975 Train Acc: 0.8967 Eval Loss: 0.3983 Eval Acc: 0.8743 (LR: 0.00001000)
[2025-06-13 22:26:17,256]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 041 Train Loss: 0.2982 Train Acc: 0.8958 Eval Loss: 0.3984 Eval Acc: 0.8695 (LR: 0.00001000)
[2025-06-13 22:27:45,074]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 042 Train Loss: 0.2966 Train Acc: 0.8968 Eval Loss: 0.3930 Eval Acc: 0.8727 (LR: 0.00001000)
[2025-06-13 22:29:13,481]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 043 Train Loss: 0.2960 Train Acc: 0.8965 Eval Loss: 0.4069 Eval Acc: 0.8698 (LR: 0.00001000)
[2025-06-13 22:30:41,716]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 044 Train Loss: 0.2949 Train Acc: 0.8965 Eval Loss: 0.3989 Eval Acc: 0.8716 (LR: 0.00001000)
[2025-06-13 22:32:06,341]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 045 Train Loss: 0.2934 Train Acc: 0.8967 Eval Loss: 0.4017 Eval Acc: 0.8674 (LR: 0.00001000)
[2025-06-13 22:33:30,063]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 046 Train Loss: 0.2949 Train Acc: 0.8960 Eval Loss: 0.4058 Eval Acc: 0.8706 (LR: 0.00001000)
[2025-06-13 22:34:58,675]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 047 Train Loss: 0.2946 Train Acc: 0.8979 Eval Loss: 0.3980 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-06-13 22:36:28,283]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 048 Train Loss: 0.2938 Train Acc: 0.8973 Eval Loss: 0.3915 Eval Acc: 0.8762 (LR: 0.00001000)
[2025-06-13 22:37:56,599]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 049 Train Loss: 0.2937 Train Acc: 0.8978 Eval Loss: 0.3981 Eval Acc: 0.8705 (LR: 0.00001000)
[2025-06-13 22:39:24,944]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 050 Train Loss: 0.2982 Train Acc: 0.8947 Eval Loss: 0.4004 Eval Acc: 0.8723 (LR: 0.00001000)
[2025-06-13 22:40:52,916]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 051 Train Loss: 0.2952 Train Acc: 0.8968 Eval Loss: 0.3958 Eval Acc: 0.8730 (LR: 0.00001000)
[2025-06-13 22:42:22,619]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 052 Train Loss: 0.2942 Train Acc: 0.8973 Eval Loss: 0.4075 Eval Acc: 0.8680 (LR: 0.00001000)
[2025-06-13 22:43:50,704]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 053 Train Loss: 0.2944 Train Acc: 0.8980 Eval Loss: 0.3926 Eval Acc: 0.8736 (LR: 0.00001000)
[2025-06-13 22:45:18,395]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 054 Train Loss: 0.2932 Train Acc: 0.8956 Eval Loss: 0.3966 Eval Acc: 0.8727 (LR: 0.00000100)
[2025-06-13 22:46:46,151]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 055 Train Loss: 0.2884 Train Acc: 0.8987 Eval Loss: 0.3968 Eval Acc: 0.8746 (LR: 0.00000100)
[2025-06-13 22:48:12,427]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 056 Train Loss: 0.2895 Train Acc: 0.8988 Eval Loss: 0.3953 Eval Acc: 0.8724 (LR: 0.00000100)
[2025-06-13 22:49:41,872]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 057 Train Loss: 0.2879 Train Acc: 0.8992 Eval Loss: 0.3936 Eval Acc: 0.8705 (LR: 0.00000100)
[2025-06-13 22:51:07,682]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 058 Train Loss: 0.2893 Train Acc: 0.8984 Eval Loss: 0.3890 Eval Acc: 0.8743 (LR: 0.00000100)
[2025-06-13 22:52:38,926]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 059 Train Loss: 0.2887 Train Acc: 0.8990 Eval Loss: 0.3857 Eval Acc: 0.8736 (LR: 0.00000100)
[2025-06-13 22:54:19,302]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Epoch: 060 Train Loss: 0.2845 Train Acc: 0.9005 Eval Loss: 0.3905 Eval Acc: 0.8744 (LR: 0.00000100)
[2025-06-13 22:54:19,302]: [ResNet20_parametrized_hardtanh_quantized_3_bits] Best Eval Accuracy: 0.8762
[2025-06-13 22:54:19,488]: 


Quantization of model down to 3 bits finished
[2025-06-13 22:54:19,488]: Model Architecture:
[2025-06-13 22:54:19,722]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3996], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3985493183135986, max_val=1.3985493183135986)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2593], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7566237449645996, max_val=1.0582423210144043)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3129], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2028], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6512909531593323, max_val=0.7682855129241943)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4701], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2164], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7479473352432251, max_val=0.7667498588562012)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2341], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2490], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9085655212402344, max_val=0.8345857262611389)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5804], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2251], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7927002906799316, max_val=0.7828021049499512)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2199], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2067], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7991235852241516, max_val=0.6477856636047363)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6612], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2091], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7914878726005554, max_val=0.6720784306526184)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3249], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1852], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6714057922363281, max_val=0.6247872710227966)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2186], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7495150566101074, max_val=0.780457615852356)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1662], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.566227912902832, max_val=0.5969444513320923)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1672], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1849], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6792968511581421, max_val=0.615272581577301)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5744], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1709], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5078827738761902, max_val=0.6883570551872253)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1513], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1618], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5709325075149536, max_val=0.5614925622940063)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6282], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1738], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5318297147750854, max_val=0.6844363808631897)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1655], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1739], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6094434857368469, max_val=0.6078610420227051)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1822], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6410660743713379, max_val=0.6342502236366272)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3569], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1580], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5490018129348755, max_val=0.5572464466094971)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1428], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1557], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5021838545799255, max_val=0.5880630612373352)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2862], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1436], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5029189586639404, max_val=0.5020102262496948)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0925], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3205985128879547, max_val=0.3267784118652344)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-3, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0089], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 22:54:19,797]: 
Model Weights:
[2025-06-13 22:54:19,797]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-13 22:54:19,798]: Sample Values (25 elements): [-0.33605989813804626, 0.07129613310098648, 0.033742427825927734, 0.10658485442399979, -0.274531751871109, 0.31483587622642517, 0.3330076038837433, -0.18704959750175476, 0.17244967818260193, 0.03604063019156456, 0.15377920866012573, -0.034364987164735794, -0.37801072001457214, 0.04172252491116524, 0.09491380304098129, -0.04992128536105156, -0.10347723215818405, -0.4766608476638794, -0.2847180664539337, -0.1403430551290512, 0.09804040938615799, 0.01805827207863331, 0.30225878953933716, 0.18777255713939667, 0.057595252990722656]
[2025-06-13 22:54:19,798]: Mean: 0.00021079
[2025-06-13 22:54:19,799]: Min: -0.55701280
[2025-06-13 22:54:19,799]: Max: 0.67656517
[2025-06-13 22:54:19,799]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,800]: Sample Values (16 elements): [1.0865761041641235, 1.2094019651412964, 1.180161952972412, 0.8057379722595215, 1.2393018007278442, 0.995927095413208, 1.1127300262451172, 1.487572193145752, 1.0293612480163574, 0.8877930641174316, 0.7985506057739258, 0.846381425857544, 0.9464279413223267, 1.0351403951644897, 0.9368290305137634, 1.2929654121398926]
[2025-06-13 22:54:19,800]: Mean: 1.05567861
[2025-06-13 22:54:19,800]: Min: 0.79855061
[2025-06-13 22:54:19,800]: Max: 1.48757219
[2025-06-13 22:54:19,801]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 22:54:19,802]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5185331702232361, 0.25926658511161804, 0.25926658511161804, 0.0, 0.0, 0.0, 0.0, -0.25926658511161804, 0.25926658511161804, 0.0, 0.5185331702232361, 0.0, 0.0, 0.25926658511161804, 0.0, 0.0, 0.25926658511161804, -0.25926658511161804]
[2025-06-13 22:54:19,802]: Mean: -0.00157540
[2025-06-13 22:54:19,802]: Min: -0.77779973
[2025-06-13 22:54:19,802]: Max: 1.03706634
[2025-06-13 22:54:19,802]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,803]: Sample Values (16 elements): [0.8228526711463928, 0.820906937122345, 1.1799242496490479, 0.8537461161613464, 0.9695983529090881, 0.8237443566322327, 1.0355702638626099, 0.7756116390228271, 0.9605433940887451, 1.1085231304168701, 0.8253538012504578, 0.8423634171485901, 0.7220233082771301, 0.8400607109069824, 1.3951207399368286, 0.7118337154388428]
[2025-06-13 22:54:19,803]: Mean: 0.91798604
[2025-06-13 22:54:19,803]: Min: 0.71183372
[2025-06-13 22:54:19,803]: Max: 1.39512074
[2025-06-13 22:54:19,804]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 22:54:19,804]: Sample Values (25 elements): [0.0, -0.40559330582618713, 0.20279665291309357, -0.40559330582618713, -0.20279665291309357, 0.6083899736404419, 0.20279665291309357, 0.0, 0.0, 0.20279665291309357, 0.0, 0.0, 0.0, -0.20279665291309357, -0.20279665291309357, -0.20279665291309357, 0.0, -0.20279665291309357, -0.20279665291309357, -0.20279665291309357, 0.20279665291309357, 0.0, -0.20279665291309357, 0.0, -0.20279665291309357]
[2025-06-13 22:54:19,805]: Mean: 0.00809778
[2025-06-13 22:54:19,805]: Min: -0.60838997
[2025-06-13 22:54:19,805]: Max: 0.81118661
[2025-06-13 22:54:19,805]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,806]: Sample Values (16 elements): [0.9819793701171875, 0.6850481629371643, 0.7599117755889893, 0.884011447429657, 0.7159078121185303, 0.9446735978126526, 0.7558435201644897, 0.8289604187011719, 0.5521745681762695, 0.5317071676254272, 0.8011674880981445, 1.0499839782714844, 1.1506012678146362, 0.5179755687713623, 0.9517529010772705, 0.569754958152771]
[2025-06-13 22:54:19,806]: Mean: 0.79259086
[2025-06-13 22:54:19,806]: Min: 0.51797557
[2025-06-13 22:54:19,806]: Max: 1.15060127
[2025-06-13 22:54:19,808]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 22:54:19,808]: Sample Values (25 elements): [0.21638530492782593, -0.21638530492782593, 0.0, 0.0, 0.6491559147834778, 0.0, 0.21638530492782593, 0.0, 0.0, 0.0, 0.0, 0.21638530492782593, -0.21638530492782593, 0.21638530492782593, 0.21638530492782593, -0.21638530492782593, 0.43277060985565186, 0.21638530492782593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21638530492782593, -0.21638530492782593]
[2025-06-13 22:54:19,808]: Mean: 0.00385061
[2025-06-13 22:54:19,809]: Min: -0.64915591
[2025-06-13 22:54:19,809]: Max: 0.86554122
[2025-06-13 22:54:19,809]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,809]: Sample Values (16 elements): [0.6223390102386475, 0.6877414584159851, 1.2180384397506714, 0.740807294845581, 1.035664439201355, 0.674544632434845, 0.6365269422531128, 0.6476931571960449, 0.634638249874115, 1.0457264184951782, 0.914890706539154, 0.5734771490097046, 0.7713202834129333, 0.8709946870803833, 0.7379671335220337, 0.8721122145652771]
[2025-06-13 22:54:19,809]: Mean: 0.79278016
[2025-06-13 22:54:19,809]: Min: 0.57347715
[2025-06-13 22:54:19,809]: Max: 1.21803844
[2025-06-13 22:54:19,810]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 22:54:19,811]: Sample Values (25 elements): [0.0, 0.24902164936065674, 0.24902164936065674, 0.24902164936065674, 0.24902164936065674, 0.0, 0.0, -0.24902164936065674, 0.0, 0.24902164936065674, 0.0, -0.24902164936065674, 0.0, 0.24902164936065674, 0.24902164936065674, 0.0, 0.24902164936065674, 0.24902164936065674, 0.0, 0.0, 0.0, 0.24902164936065674, -0.24902164936065674, 0.24902164936065674, -0.24902164936065674]
[2025-06-13 22:54:19,811]: Mean: 0.00940316
[2025-06-13 22:54:19,811]: Min: -0.99608660
[2025-06-13 22:54:19,811]: Max: 0.74706495
[2025-06-13 22:54:19,811]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,811]: Sample Values (16 elements): [0.9055446982383728, 0.6560375690460205, 0.9291688799858093, 0.7604455351829529, 0.786817193031311, 0.7139222621917725, 0.7574630975723267, 0.8411792516708374, 0.7936888933181763, 1.3280956745147705, 0.7499096393585205, 0.8015314340591431, 0.930084228515625, 0.8592461347579956, 0.8858351111412048, 0.8711278438568115]
[2025-06-13 22:54:19,812]: Mean: 0.84813106
[2025-06-13 22:54:19,812]: Min: 0.65603757
[2025-06-13 22:54:19,812]: Max: 1.32809567
[2025-06-13 22:54:19,814]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 22:54:19,814]: Sample Values (25 elements): [-0.2250717580318451, 0.0, 0.0, 0.0, 0.0, 0.2250717580318451, 0.2250717580318451, -0.2250717580318451, -0.2250717580318451, 0.2250717580318451, 0.0, 0.0, 0.0, -0.6752152442932129, 0.0, 0.0, 0.2250717580318451, 0.0, 0.0, 0.0, 0.2250717580318451, 0.2250717580318451, 0.0, 0.0, -0.4501435160636902]
[2025-06-13 22:54:19,814]: Mean: -0.00507974
[2025-06-13 22:54:19,814]: Min: -0.90028703
[2025-06-13 22:54:19,814]: Max: 0.67521524
[2025-06-13 22:54:19,815]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,815]: Sample Values (16 elements): [1.0245099067687988, 0.8339217901229858, 0.8765456080436707, 0.8145300149917603, 0.4894777536392212, 0.7568055391311646, 0.759409487247467, 0.5040167570114136, 0.8760064840316772, 0.6276637315750122, 0.6087300777435303, 0.5939586162567139, 0.9756836295127869, 0.5390267968177795, 0.6094703078269958, 0.9787759780883789]
[2025-06-13 22:54:19,815]: Mean: 0.74178326
[2025-06-13 22:54:19,815]: Min: 0.48947775
[2025-06-13 22:54:19,815]: Max: 1.02450991
[2025-06-13 22:54:19,817]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 22:54:19,817]: Sample Values (25 elements): [0.20670132339000702, 0.41340264678001404, 0.0, -0.41340264678001404, 0.0, -0.20670132339000702, 0.20670132339000702, -0.20670132339000702, 0.41340264678001404, 0.0, -0.20670132339000702, 0.0, -0.20670132339000702, 0.0, 0.20670132339000702, 0.0, 0.0, 0.20670132339000702, 0.41340264678001404, -0.20670132339000702, 0.0, 0.20670132339000702, -0.41340264678001404, 0.0, 0.0]
[2025-06-13 22:54:19,818]: Mean: 0.00941998
[2025-06-13 22:54:19,818]: Min: -0.82680529
[2025-06-13 22:54:19,818]: Max: 0.62010396
[2025-06-13 22:54:19,818]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-13 22:54:19,818]: Sample Values (16 elements): [1.2163699865341187, 0.6494721174240112, 1.043569564819336, 1.130662441253662, 0.6559709906578064, 0.7823042273521423, 0.6137959361076355, 0.8565025329589844, 0.924986720085144, 1.2800573110580444, 0.5805495977401733, 0.9321762323379517, 0.5940428376197815, 1.359161615371704, 0.4961061477661133, 0.8489183187484741]
[2025-06-13 22:54:19,819]: Mean: 0.87279040
[2025-06-13 22:54:19,819]: Min: 0.49610615
[2025-06-13 22:54:19,819]: Max: 1.35916162
[2025-06-13 22:54:19,821]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-13 22:54:19,821]: Sample Values (25 elements): [0.0, 0.0, 0.20908090472221375, 0.0, -0.4181618094444275, 0.20908090472221375, 0.0, 0.0, 0.0, 0.4181618094444275, 0.20908090472221375, 0.20908090472221375, 0.0, 0.0, 0.20908090472221375, 0.20908090472221375, 0.0, 0.0, 0.0, 0.0, 0.20908090472221375, 0.20908090472221375, 0.20908090472221375, 0.0, 0.20908090472221375]
[2025-06-13 22:54:19,821]: Mean: 0.00331226
[2025-06-13 22:54:19,821]: Min: -0.83632362
[2025-06-13 22:54:19,821]: Max: 0.62724268
[2025-06-13 22:54:19,821]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,822]: Sample Values (25 elements): [0.6204089522361755, 0.6856602430343628, 0.6182259321212769, 0.854751467704773, 0.8239294290542603, 0.7506687641143799, 0.7682908177375793, 0.5960173010826111, 0.6190147399902344, 0.8120889067649841, 0.7131425738334656, 0.7331551313400269, 0.5496283769607544, 0.6657058000564575, 0.8207164406776428, 0.7687738537788391, 0.7175961136817932, 0.8536934852600098, 0.9741465449333191, 0.7958465218544006, 0.7771039605140686, 0.6383718252182007, 0.7354847192764282, 0.7176012396812439, 0.7979564070701599]
[2025-06-13 22:54:19,822]: Mean: 0.72731817
[2025-06-13 22:54:19,822]: Min: 0.54962838
[2025-06-13 22:54:19,822]: Max: 0.97414654
[2025-06-13 22:54:19,823]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 22:54:19,823]: Sample Values (25 elements): [0.18517044186592102, 0.0, 0.0, 0.0, 0.18517044186592102, 0.0, -0.18517044186592102, 0.18517044186592102, 0.18517044186592102, -0.18517044186592102, 0.0, 0.0, 0.0, 0.0, 0.18517044186592102, 0.0, 0.0, -0.18517044186592102, -0.18517044186592102, 0.18517044186592102, 0.18517044186592102, 0.0, 0.18517044186592102, 0.18517044186592102, 0.18517044186592102]
[2025-06-13 22:54:19,824]: Mean: -0.00233070
[2025-06-13 22:54:19,824]: Min: -0.74068177
[2025-06-13 22:54:19,824]: Max: 0.55551136
[2025-06-13 22:54:19,824]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,824]: Sample Values (25 elements): [0.7265751361846924, 0.9264261722564697, 0.9710259437561035, 1.017126441001892, 1.0309149026870728, 0.8446019887924194, 1.0718326568603516, 1.1356968879699707, 0.8221002221107483, 0.8974060416221619, 0.9191285371780396, 0.9690696597099304, 0.7499266266822815, 0.7789639830589294, 1.0228298902511597, 0.903503954410553, 0.8510416150093079, 0.7113901376724243, 0.88155198097229, 0.9579779505729675, 1.0114941596984863, 0.9212681651115417, 0.8337887525558472, 0.8363252282142639, 0.9052738547325134]
[2025-06-13 22:54:19,824]: Mean: 0.91637301
[2025-06-13 22:54:19,825]: Min: 0.71139014
[2025-06-13 22:54:19,825]: Max: 1.13569689
[2025-06-13 22:54:19,826]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-13 22:54:19,826]: Sample Values (25 elements): [0.0, 0.21856753528118134, 0.0, -0.43713507056236267, 0.0, -0.21856753528118134, 0.0, -0.21856753528118134, 0.21856753528118134, 0.21856753528118134, -0.43713507056236267, -0.21856753528118134, 0.21856753528118134, 0.0, 0.0, -0.21856753528118134, 0.21856753528118134, -0.21856753528118134, 0.21856753528118134, -0.21856753528118134, 0.6557025909423828, -0.43713507056236267, 0.0, 0.0, -0.21856753528118134]
[2025-06-13 22:54:19,826]: Mean: 0.01067224
[2025-06-13 22:54:19,826]: Min: -0.65570259
[2025-06-13 22:54:19,827]: Max: 0.87427014
[2025-06-13 22:54:19,827]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,827]: Sample Values (25 elements): [0.5674750208854675, 0.7074284553527832, 0.654396653175354, 0.6311599016189575, 0.46392083168029785, 0.5537709593772888, 0.665669858455658, 0.649945080280304, 0.5039349794387817, 0.5336732268333435, 0.48300811648368835, 0.6881852746009827, 0.6562144756317139, 0.634030282497406, 0.6157591938972473, 0.3933159112930298, 0.3658904731273651, 0.6776975393295288, 0.3746909201145172, 0.49638429284095764, 0.43172532320022583, 0.4109550416469574, 0.6653007864952087, 0.3515227138996124, 0.657727837562561]
[2025-06-13 22:54:19,827]: Mean: 0.54965997
[2025-06-13 22:54:19,827]: Min: 0.35152271
[2025-06-13 22:54:19,827]: Max: 0.75071269
[2025-06-13 22:54:19,829]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 22:54:19,830]: Sample Values (25 elements): [0.1661674976348877, 0.3323349952697754, 0.0, 0.0, 0.3323349952697754, 0.1661674976348877, -0.1661674976348877, 0.3323349952697754, -0.1661674976348877, -0.1661674976348877, -0.1661674976348877, 0.0, 0.1661674976348877, 0.0, 0.0, 0.1661674976348877, 0.1661674976348877, 0.3323349952697754, 0.4985024929046631, -0.1661674976348877, -0.1661674976348877, 0.0, -0.1661674976348877, -0.1661674976348877, 0.1661674976348877]
[2025-06-13 22:54:19,830]: Mean: -0.00146046
[2025-06-13 22:54:19,830]: Min: -0.49850249
[2025-06-13 22:54:19,830]: Max: 0.66466999
[2025-06-13 22:54:19,830]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,831]: Sample Values (25 elements): [0.4396466612815857, 0.68912672996521, 0.625334620475769, 0.5190324187278748, 0.7499793767929077, 0.7448222041130066, 0.5740755796432495, 0.6330446004867554, 0.668185293674469, 0.7206319570541382, 0.5373154282569885, 0.5677560567855835, 0.46910566091537476, 0.6085794568061829, 0.5329159498214722, 0.5420265197753906, 0.576698899269104, 0.5472158193588257, 0.694576621055603, 0.5450282096862793, 0.5575599670410156, 0.6596760749816895, 0.6337212324142456, 0.5609021186828613, 0.6007676124572754]
[2025-06-13 22:54:19,831]: Mean: 0.59637332
[2025-06-13 22:54:19,831]: Min: 0.43964666
[2025-06-13 22:54:19,832]: Max: 0.74997938
[2025-06-13 22:54:19,833]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 22:54:19,834]: Sample Values (25 elements): [0.0, 0.1849384903907776, -0.3698769807815552, 0.1849384903907776, 0.1849384903907776, -0.1849384903907776, -0.1849384903907776, 0.0, 0.0, 0.0, 0.1849384903907776, 0.0, -0.1849384903907776, 0.0, 0.1849384903907776, -0.1849384903907776, 0.0, -0.1849384903907776, 0.0, 0.1849384903907776, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 22:54:19,834]: Mean: 0.00383282
[2025-06-13 22:54:19,834]: Min: -0.73975396
[2025-06-13 22:54:19,834]: Max: 0.55481547
[2025-06-13 22:54:19,835]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,835]: Sample Values (25 elements): [0.7536020874977112, 0.8447701334953308, 0.9355871677398682, 0.999440610408783, 0.8203118443489075, 0.9244836568832397, 0.8443763852119446, 0.7873063087463379, 0.726722240447998, 0.731780469417572, 0.7486980557441711, 0.8173078298568726, 0.5137071013450623, 0.8651077747344971, 0.7740859389305115, 0.9071005582809448, 0.8218404650688171, 0.7505785822868347, 0.9652882814407349, 0.7270569205284119, 0.7088738679885864, 0.6616789102554321, 0.7243552803993225, 0.8735825419425964, 1.0820618867874146]
[2025-06-13 22:54:19,835]: Mean: 0.80958247
[2025-06-13 22:54:19,835]: Min: 0.51370710
[2025-06-13 22:54:19,836]: Max: 1.08206189
[2025-06-13 22:54:19,837]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 22:54:19,837]: Sample Values (25 elements): [-0.1708914339542389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1708914339542389, 0.0, 0.3417828679084778, 0.0, 0.0, -0.1708914339542389, 0.1708914339542389, 0.0, 0.0, -0.1708914339542389, 0.1708914339542389, 0.0, -0.3417828679084778, 0.0, 0.0, 0.0, -0.1708914339542389, 0.1708914339542389]
[2025-06-13 22:54:19,837]: Mean: 0.00025960
[2025-06-13 22:54:19,837]: Min: -0.51267433
[2025-06-13 22:54:19,838]: Max: 0.68356574
[2025-06-13 22:54:19,838]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,838]: Sample Values (25 elements): [0.7377952933311462, 0.598169207572937, 0.6406906843185425, 0.6825574040412903, 0.6157961487770081, 0.5937921404838562, 0.7387326955795288, 0.6177156567573547, 0.5014528036117554, 0.6015405058860779, 0.6851058006286621, 0.5819467902183533, 0.5644161105155945, 0.6048120260238647, 0.5980623364448547, 0.7095062136650085, 0.6142281889915466, 0.6241621971130371, 0.5496535301208496, 0.6449121832847595, 0.584644615650177, 0.49904167652130127, 0.541701078414917, 0.6816526055335999, 0.6258829236030579]
[2025-06-13 22:54:19,838]: Mean: 0.61339337
[2025-06-13 22:54:19,838]: Min: 0.49904168
[2025-06-13 22:54:19,838]: Max: 0.73873270
[2025-06-13 22:54:19,839]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 22:54:19,840]: Sample Values (25 elements): [-0.16177499294281006, 0.0, 0.0, 0.0, 0.0, 0.3235499858856201, 0.0, 0.0, -0.16177499294281006, 0.16177499294281006, -0.16177499294281006, 0.16177499294281006, 0.0, 0.0, 0.0, 0.0, 0.16177499294281006, -0.16177499294281006, -0.16177499294281006, -0.16177499294281006, 0.16177499294281006, 0.3235499858856201, 0.16177499294281006, 0.0, -0.16177499294281006]
[2025-06-13 22:54:19,840]: Mean: 0.00740767
[2025-06-13 22:54:19,840]: Min: -0.64709997
[2025-06-13 22:54:19,840]: Max: 0.48532498
[2025-06-13 22:54:19,841]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-13 22:54:19,841]: Sample Values (25 elements): [0.8653916716575623, 0.807181715965271, 1.0479809045791626, 0.8261041641235352, 1.1235932111740112, 0.6339706778526306, 1.0160887241363525, 1.0137505531311035, 1.0854313373565674, 0.8078592419624329, 0.6421265006065369, 0.7070134878158569, 0.8367785811424255, 0.8879091143608093, 1.014485239982605, 0.9756704568862915, 0.7779572606086731, 0.5920742154121399, 0.8531063795089722, 0.6912429332733154, 0.9972901940345764, 0.9430816769599915, 0.866829514503479, 1.172224521636963, 0.7898409962654114]
[2025-06-13 22:54:19,841]: Mean: 0.87278461
[2025-06-13 22:54:19,841]: Min: 0.59207422
[2025-06-13 22:54:19,841]: Max: 1.17222452
[2025-06-13 22:54:19,843]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-13 22:54:19,843]: Sample Values (25 elements): [0.1737523227930069, 0.1737523227930069, 0.1737523227930069, -0.1737523227930069, 0.3475046455860138, 0.0, -0.1737523227930069, 0.1737523227930069, 0.0, 0.3475046455860138, 0.0, 0.0, 0.0, 0.1737523227930069, -0.5212569832801819, 0.0, 0.1737523227930069, -0.1737523227930069, -0.1737523227930069, -0.1737523227930069, 0.0, 0.1737523227930069, -0.1737523227930069, 0.0, 0.1737523227930069]
[2025-06-13 22:54:19,843]: Mean: 0.00143285
[2025-06-13 22:54:19,844]: Min: -0.52125698
[2025-06-13 22:54:19,844]: Max: 0.69500929
[2025-06-13 22:54:19,844]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,844]: Sample Values (25 elements): [0.5235660672187805, 0.5137367844581604, 0.4973931610584259, 0.49104470014572144, 0.4926542639732361, 0.45320925116539, 0.4842652380466461, 0.6364178657531738, 0.589569091796875, 0.4531402289867401, 0.5618879795074463, 0.5169517397880554, 0.4129889905452728, 0.5251689553260803, 0.618676483631134, 0.6494346261024475, 0.47985875606536865, 0.5686682462692261, 0.5024466514587402, 0.48721960186958313, 0.53281569480896, 0.5314251184463501, 0.5043834447860718, 0.6763760447502136, 0.5630846619606018]
[2025-06-13 22:54:19,844]: Mean: 0.52619386
[2025-06-13 22:54:19,844]: Min: 0.40402648
[2025-06-13 22:54:19,845]: Max: 0.67637604
[2025-06-13 22:54:19,846]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 22:54:19,847]: Sample Values (25 elements): [0.0, -0.17390063405036926, 0.0, 0.0, -0.17390063405036926, 0.0, 0.0, 0.17390063405036926, -0.17390063405036926, 0.0, 0.17390063405036926, 0.17390063405036926, 0.17390063405036926, -0.3478012681007385, 0.17390063405036926, -0.17390063405036926, 0.0, 0.0, 0.0, 0.0, -0.17390063405036926, -0.17390063405036926, 0.0, -0.17390063405036926, 0.17390063405036926]
[2025-06-13 22:54:19,847]: Mean: -0.00069817
[2025-06-13 22:54:19,847]: Min: -0.69560254
[2025-06-13 22:54:19,847]: Max: 0.52170193
[2025-06-13 22:54:19,847]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,847]: Sample Values (25 elements): [1.0014210939407349, 0.8421115875244141, 0.9119051098823547, 0.7950109839439392, 0.811621367931366, 1.0016438961029053, 0.8448034524917603, 0.7795343995094299, 0.8694698214530945, 0.9127606749534607, 0.7111027836799622, 0.978996753692627, 0.8494517803192139, 0.7928014993667603, 0.8215072154998779, 0.7959893345832825, 0.8792876601219177, 0.8064078688621521, 0.8942805528640747, 0.9953740239143372, 0.7608764171600342, 0.7696195244789124, 0.7389915585517883, 0.9358769655227661, 0.8613150715827942]
[2025-06-13 22:54:19,848]: Mean: 0.87529993
[2025-06-13 22:54:19,848]: Min: 0.65567762
[2025-06-13 22:54:19,848]: Max: 1.10321927
[2025-06-13 22:54:19,849]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-13 22:54:19,850]: Sample Values (25 elements): [-0.18218806385993958, 0.18218806385993958, -0.36437612771987915, 0.18218806385993958, -0.18218806385993958, 0.0, 0.18218806385993958, 0.0, 0.0, 0.0, -0.18218806385993958, 0.18218806385993958, 0.18218806385993958, -0.18218806385993958, 0.0, -0.18218806385993958, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18218806385993958, -0.18218806385993958, -0.18218806385993958, 0.18218806385993958]
[2025-06-13 22:54:19,850]: Mean: 0.00382524
[2025-06-13 22:54:19,850]: Min: -0.72875226
[2025-06-13 22:54:19,850]: Max: 0.54656422
[2025-06-13 22:54:19,851]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,851]: Sample Values (25 elements): [0.531183123588562, 0.6339213252067566, 0.5354457497596741, 0.6098714470863342, 0.4904225766658783, 0.5355762839317322, 0.7186345458030701, 0.4885275065898895, 0.5269975662231445, 0.7812313437461853, 0.796922504901886, 0.41569700837135315, 0.505155622959137, 0.5692592263221741, 0.5115985870361328, 0.511833131313324, 0.5985739231109619, 0.49564626812934875, 0.4928862750530243, 0.6340940594673157, 0.596558153629303, 0.6835640668869019, 0.749077320098877, 0.5638979077339172, 0.46475717425346375]
[2025-06-13 22:54:19,851]: Mean: 0.54150194
[2025-06-13 22:54:19,851]: Min: 0.25397819
[2025-06-13 22:54:19,851]: Max: 0.79692250
[2025-06-13 22:54:19,853]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 22:54:19,853]: Sample Values (25 elements): [0.15803545713424683, -0.15803545713424683, 0.31607091426849365, 0.0, 0.0, -0.15803545713424683, -0.15803545713424683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15803545713424683, 0.0, -0.15803545713424683, -0.15803545713424683, 0.0, 0.0, 0.15803545713424683, 0.0, 0.0, -0.15803545713424683, 0.0]
[2025-06-13 22:54:19,853]: Mean: -0.00051444
[2025-06-13 22:54:19,853]: Min: -0.47410637
[2025-06-13 22:54:19,854]: Max: 0.63214183
[2025-06-13 22:54:19,854]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,854]: Sample Values (25 elements): [0.5327507853507996, 0.6149658560752869, 0.6972030997276306, 0.4525018036365509, 0.5910444259643555, 0.5753064155578613, 0.5693671107292175, 0.5331621170043945, 0.5479186773300171, 0.6353092789649963, 0.7250441312789917, 0.5463417172431946, 0.5688620209693909, 0.4192151129245758, 0.624356746673584, 0.5010129809379578, 0.6233537793159485, 0.49540987610816956, 0.48157745599746704, 0.6244940757751465, 0.6735196709632874, 0.5969255566596985, 0.5992963910102844, 0.6308123469352722, 0.7754644155502319]
[2025-06-13 22:54:19,854]: Mean: 0.58344269
[2025-06-13 22:54:19,854]: Min: 0.41921511
[2025-06-13 22:54:19,855]: Max: 0.77546442
[2025-06-13 22:54:19,856]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 22:54:19,856]: Sample Values (25 elements): [-0.15574955940246582, -0.15574955940246582, 0.15574955940246582, -0.15574955940246582, -0.15574955940246582, 0.0, 0.15574955940246582, -0.15574955940246582, 0.15574955940246582, 0.15574955940246582, 0.0, 0.0, 0.0, -0.15574955940246582, 0.0, 0.0, 0.15574955940246582, 0.15574955940246582, 0.0, -0.15574955940246582, 0.0, -0.31149911880493164, -0.15574955940246582, 0.15574955940246582, 0.0]
[2025-06-13 22:54:19,857]: Mean: 0.00065065
[2025-06-13 22:54:19,857]: Min: -0.46724868
[2025-06-13 22:54:19,857]: Max: 0.62299824
[2025-06-13 22:54:19,857]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,857]: Sample Values (25 elements): [0.8462246656417847, 0.7857382893562317, 0.8492258191108704, 0.841331958770752, 0.9020016193389893, 0.7853310108184814, 0.6638482809066772, 0.922893226146698, 0.8369125723838806, 0.738536536693573, 0.9366813898086548, 0.6665900349617004, 0.8259075880050659, 0.702045202255249, 0.771003782749176, 0.8800795674324036, 1.0042322874069214, 0.7849678993225098, 1.1560319662094116, 0.9812507629394531, 0.7261654734611511, 0.9145343899726868, 1.0037904977798462, 0.8900870680809021, 0.8775565028190613]
[2025-06-13 22:54:19,858]: Mean: 0.82509351
[2025-06-13 22:54:19,858]: Min: 0.62943435
[2025-06-13 22:54:19,859]: Max: 1.15603197
[2025-06-13 22:54:19,860]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 22:54:19,861]: Sample Values (25 elements): [-0.14356130361557007, 0.28712260723114014, 0.0, -0.14356130361557007, 0.0, 0.28712260723114014, -0.14356130361557007, 0.14356130361557007, 0.14356130361557007, 0.0, 0.0, -0.14356130361557007, -0.28712260723114014, 0.28712260723114014, 0.14356130361557007, -0.14356130361557007, 0.14356130361557007, 0.14356130361557007, 0.0, 0.0, 0.14356130361557007, 0.0, -0.14356130361557007, -0.14356130361557007, 0.0]
[2025-06-13 22:54:19,862]: Mean: -0.00134744
[2025-06-13 22:54:19,862]: Min: -0.57424521
[2025-06-13 22:54:19,862]: Max: 0.43068391
[2025-06-13 22:54:19,862]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,863]: Sample Values (25 elements): [0.6212679147720337, 0.5237210392951965, 0.5825279355049133, 0.486474871635437, 0.5285040140151978, 0.48260462284088135, 0.6722519397735596, 0.44073763489723206, 0.5122376084327698, 0.6171303391456604, 0.5702032446861267, 0.43691161274909973, 0.5532649159431458, 0.5321512222290039, 0.6063673496246338, 0.5840284824371338, 0.494052916765213, 0.6468099355697632, 0.5520457029342651, 0.5418475866317749, 0.46341821551322937, 0.45482611656188965, 0.6984423995018005, 0.6363758444786072, 0.5230461359024048]
[2025-06-13 22:54:19,863]: Mean: 0.54781079
[2025-06-13 22:54:19,863]: Min: 0.35492423
[2025-06-13 22:54:19,863]: Max: 0.69844240
[2025-06-13 22:54:19,865]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 22:54:19,865]: Sample Values (25 elements): [0.09248241037130356, 0.0, 0.0, 0.09248241037130356, 0.0, -0.09248241037130356, 0.18496482074260712, 0.0, 0.0, 0.18496482074260712, 0.0, 0.0, 0.09248241037130356, -0.09248241037130356, -0.09248241037130356, 0.09248241037130356, 0.0, -0.09248241037130356, 0.09248241037130356, 0.0, 0.18496482074260712, -0.09248241037130356, -0.09248241037130356, -0.09248241037130356, 0.0]
[2025-06-13 22:54:19,866]: Mean: -0.00081032
[2025-06-13 22:54:19,866]: Min: -0.27744722
[2025-06-13 22:54:19,866]: Max: 0.36992964
[2025-06-13 22:54:19,866]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-13 22:54:19,866]: Sample Values (25 elements): [1.093894362449646, 0.8851590156555176, 1.4601402282714844, 1.0141642093658447, 1.0707464218139648, 0.8771196603775024, 0.9478821158409119, 1.115001916885376, 1.2281800508499146, 0.8025474548339844, 0.8757049441337585, 1.1405160427093506, 1.2288734912872314, 0.9715014100074768, 0.9249516129493713, 0.8479750752449036, 1.0251127481460571, 1.0300836563110352, 1.0475049018859863, 1.1341150999069214, 1.0438289642333984, 0.868212103843689, 0.9314095973968506, 1.2764973640441895, 0.9706767797470093]
[2025-06-13 22:54:19,866]: Mean: 1.02838540
[2025-06-13 22:54:19,867]: Min: 0.64276969
[2025-06-13 22:54:19,867]: Max: 1.73028862
[2025-06-13 22:54:19,867]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-13 22:54:19,867]: Sample Values (25 elements): [0.44242268800735474, 0.23222938179969788, 0.22810140252113342, 0.30950936675071716, 0.4121580421924591, -0.12203782051801682, 0.2197667360305786, 0.15126709640026093, -0.11979774385690689, -0.11590733379125595, -0.3183581829071045, 0.34242501854896545, 0.05665400251746178, 0.028257258236408234, -0.14060164988040924, -0.28872647881507874, 0.3066367208957672, -0.29843249917030334, -0.15946577489376068, -0.33875465393066406, -0.2866748869419098, 0.7757726311683655, -0.13382570445537567, -0.02597023919224739, 0.25416815280914307]
[2025-06-13 22:54:19,867]: Mean: 0.00193806
[2025-06-13 22:54:19,868]: Min: -0.62357312
[2025-06-13 22:54:19,868]: Max: 0.77577263
[2025-06-14 06:21:13,536]: 


QAT of ResNet20 with parametrized_hardtanh down to 2 bits...
[2025-06-14 06:21:13,791]: [ResNet20_parametrized_hardtanh_quantized_2_bits] after configure_qat:
[2025-06-14 06:21:13,942]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-14 06:22:14,566]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 001 Train Loss: 1.1318 Train Acc: 0.6064 Eval Loss: 1.0858 Eval Acc: 0.6321 (LR: 0.00100000)
[2025-06-14 06:23:13,613]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 002 Train Loss: 0.9065 Train Acc: 0.6797 Eval Loss: 0.9790 Eval Acc: 0.6666 (LR: 0.00100000)
[2025-06-14 06:24:14,848]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 003 Train Loss: 0.8629 Train Acc: 0.6966 Eval Loss: 1.1285 Eval Acc: 0.6269 (LR: 0.00100000)
[2025-06-14 06:25:17,407]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 004 Train Loss: 0.8475 Train Acc: 0.7018 Eval Loss: 0.9711 Eval Acc: 0.6801 (LR: 0.00100000)
[2025-06-14 06:26:16,557]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 005 Train Loss: 0.8444 Train Acc: 0.7025 Eval Loss: 0.9940 Eval Acc: 0.6650 (LR: 0.00100000)
[2025-06-14 06:27:12,104]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 006 Train Loss: 0.8325 Train Acc: 0.7081 Eval Loss: 0.8594 Eval Acc: 0.7009 (LR: 0.00100000)
[2025-06-14 06:28:07,503]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 007 Train Loss: 0.8289 Train Acc: 0.7096 Eval Loss: 1.0571 Eval Acc: 0.6597 (LR: 0.00100000)
[2025-06-14 06:29:02,921]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 008 Train Loss: 0.8213 Train Acc: 0.7112 Eval Loss: 1.0895 Eval Acc: 0.6479 (LR: 0.00100000)
[2025-06-14 06:29:58,358]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 009 Train Loss: 0.8182 Train Acc: 0.7130 Eval Loss: 1.2752 Eval Acc: 0.5938 (LR: 0.00100000)
[2025-06-14 06:30:53,402]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 010 Train Loss: 0.8123 Train Acc: 0.7183 Eval Loss: 0.9092 Eval Acc: 0.6974 (LR: 0.00100000)
[2025-06-14 06:31:51,597]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 011 Train Loss: 0.8160 Train Acc: 0.7149 Eval Loss: 0.8979 Eval Acc: 0.6973 (LR: 0.00100000)
[2025-06-14 06:32:49,763]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 012 Train Loss: 0.8076 Train Acc: 0.7178 Eval Loss: 0.9323 Eval Acc: 0.6846 (LR: 0.00100000)
[2025-06-14 06:33:45,444]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 013 Train Loss: 0.8112 Train Acc: 0.7155 Eval Loss: 0.9011 Eval Acc: 0.6896 (LR: 0.00100000)
[2025-06-14 06:34:44,852]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 014 Train Loss: 0.7988 Train Acc: 0.7216 Eval Loss: 0.9037 Eval Acc: 0.6947 (LR: 0.00100000)
[2025-06-14 06:35:41,421]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 015 Train Loss: 0.8029 Train Acc: 0.7181 Eval Loss: 0.9401 Eval Acc: 0.6807 (LR: 0.00010000)
[2025-06-14 06:36:39,422]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 016 Train Loss: 0.7046 Train Acc: 0.7540 Eval Loss: 0.6902 Eval Acc: 0.7605 (LR: 0.00010000)
[2025-06-14 06:37:35,067]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 017 Train Loss: 0.6806 Train Acc: 0.7610 Eval Loss: 0.6814 Eval Acc: 0.7610 (LR: 0.00010000)
[2025-06-14 06:38:35,426]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 018 Train Loss: 0.6800 Train Acc: 0.7619 Eval Loss: 0.6984 Eval Acc: 0.7595 (LR: 0.00010000)
[2025-06-14 06:39:33,512]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 019 Train Loss: 0.6809 Train Acc: 0.7612 Eval Loss: 0.6813 Eval Acc: 0.7645 (LR: 0.00010000)
[2025-06-14 06:40:31,798]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 020 Train Loss: 0.6757 Train Acc: 0.7634 Eval Loss: 0.7108 Eval Acc: 0.7604 (LR: 0.00010000)
[2025-06-14 06:41:27,034]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 021 Train Loss: 0.6836 Train Acc: 0.7612 Eval Loss: 0.6977 Eval Acc: 0.7623 (LR: 0.00010000)
[2025-06-14 06:42:22,764]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 022 Train Loss: 0.6797 Train Acc: 0.7628 Eval Loss: 0.7436 Eval Acc: 0.7446 (LR: 0.00010000)
[2025-06-14 06:43:18,664]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 023 Train Loss: 0.6856 Train Acc: 0.7587 Eval Loss: 0.6879 Eval Acc: 0.7608 (LR: 0.00010000)
[2025-06-14 06:44:14,766]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 024 Train Loss: 0.6842 Train Acc: 0.7614 Eval Loss: 0.6809 Eval Acc: 0.7661 (LR: 0.00010000)
[2025-06-14 06:45:11,950]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 025 Train Loss: 0.6856 Train Acc: 0.7594 Eval Loss: 0.7606 Eval Acc: 0.7450 (LR: 0.00010000)
[2025-06-14 06:46:09,627]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 026 Train Loss: 0.6861 Train Acc: 0.7608 Eval Loss: 0.6912 Eval Acc: 0.7633 (LR: 0.00010000)
[2025-06-14 06:47:08,537]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 027 Train Loss: 0.6843 Train Acc: 0.7618 Eval Loss: 0.6939 Eval Acc: 0.7571 (LR: 0.00001000)
[2025-06-14 06:48:08,133]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 028 Train Loss: 0.6449 Train Acc: 0.7740 Eval Loss: 0.6272 Eval Acc: 0.7825 (LR: 0.00001000)
[2025-06-14 06:49:04,708]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 029 Train Loss: 0.6449 Train Acc: 0.7736 Eval Loss: 0.6326 Eval Acc: 0.7781 (LR: 0.00001000)
[2025-06-14 06:50:00,424]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 030 Train Loss: 0.6392 Train Acc: 0.7747 Eval Loss: 0.6393 Eval Acc: 0.7742 (LR: 0.00001000)
[2025-06-14 06:50:55,617]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 031 Train Loss: 0.6392 Train Acc: 0.7751 Eval Loss: 0.6483 Eval Acc: 0.7764 (LR: 0.00001000)
[2025-06-14 06:51:50,890]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 032 Train Loss: 0.6351 Train Acc: 0.7774 Eval Loss: 0.6457 Eval Acc: 0.7826 (LR: 0.00001000)
[2025-06-14 06:52:45,984]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 033 Train Loss: 0.6400 Train Acc: 0.7758 Eval Loss: 0.6980 Eval Acc: 0.7621 (LR: 0.00001000)
[2025-06-14 06:53:43,043]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 034 Train Loss: 0.6463 Train Acc: 0.7740 Eval Loss: 0.6515 Eval Acc: 0.7790 (LR: 0.00001000)
[2025-06-14 06:54:38,848]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 035 Train Loss: 0.6429 Train Acc: 0.7759 Eval Loss: 0.6389 Eval Acc: 0.7797 (LR: 0.00000100)
[2025-06-14 06:55:35,952]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 036 Train Loss: 0.6288 Train Acc: 0.7804 Eval Loss: 0.5955 Eval Acc: 0.7937 (LR: 0.00000100)
[2025-06-14 06:56:31,883]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 037 Train Loss: 0.6327 Train Acc: 0.7795 Eval Loss: 0.6312 Eval Acc: 0.7790 (LR: 0.00000100)
[2025-06-14 06:57:27,205]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 038 Train Loss: 0.6229 Train Acc: 0.7818 Eval Loss: 0.6109 Eval Acc: 0.7871 (LR: 0.00000100)
[2025-06-14 06:58:22,571]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 039 Train Loss: 0.6248 Train Acc: 0.7827 Eval Loss: 0.6114 Eval Acc: 0.7856 (LR: 0.00000100)
[2025-06-14 06:59:18,212]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 040 Train Loss: 0.6312 Train Acc: 0.7785 Eval Loss: 0.6080 Eval Acc: 0.7890 (LR: 0.00000100)
[2025-06-14 07:00:13,563]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 041 Train Loss: 0.6326 Train Acc: 0.7792 Eval Loss: 0.7117 Eval Acc: 0.7581 (LR: 0.00000100)
[2025-06-14 07:01:09,121]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 042 Train Loss: 0.6290 Train Acc: 0.7810 Eval Loss: 0.6361 Eval Acc: 0.7843 (LR: 0.00000100)
[2025-06-14 07:02:05,666]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 043 Train Loss: 0.6315 Train Acc: 0.7796 Eval Loss: 0.5967 Eval Acc: 0.7955 (LR: 0.00000100)
[2025-06-14 07:03:01,195]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 044 Train Loss: 0.6280 Train Acc: 0.7812 Eval Loss: 0.6236 Eval Acc: 0.7885 (LR: 0.00000100)
[2025-06-14 07:03:56,561]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 045 Train Loss: 0.6276 Train Acc: 0.7818 Eval Loss: 0.6381 Eval Acc: 0.7848 (LR: 0.00000100)
[2025-06-14 07:04:59,932]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 046 Train Loss: 0.6324 Train Acc: 0.7793 Eval Loss: 0.6022 Eval Acc: 0.7880 (LR: 0.00000100)
[2025-06-14 07:06:01,523]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 047 Train Loss: 0.6340 Train Acc: 0.7784 Eval Loss: 0.6647 Eval Acc: 0.7711 (LR: 0.00000010)
[2025-06-14 07:06:59,053]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 048 Train Loss: 0.6230 Train Acc: 0.7825 Eval Loss: 0.5997 Eval Acc: 0.7914 (LR: 0.00000010)
[2025-06-14 07:07:56,513]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 049 Train Loss: 0.6210 Train Acc: 0.7832 Eval Loss: 0.6403 Eval Acc: 0.7814 (LR: 0.00000010)
[2025-06-14 07:08:56,272]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 050 Train Loss: 0.6159 Train Acc: 0.7844 Eval Loss: 0.6024 Eval Acc: 0.7912 (LR: 0.00000010)
[2025-06-14 07:09:56,055]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 051 Train Loss: 0.6191 Train Acc: 0.7832 Eval Loss: 0.6169 Eval Acc: 0.7867 (LR: 0.00000010)
[2025-06-14 07:10:56,028]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 052 Train Loss: 0.6215 Train Acc: 0.7846 Eval Loss: 0.6110 Eval Acc: 0.7903 (LR: 0.00000010)
[2025-06-14 07:11:53,186]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 053 Train Loss: 0.6248 Train Acc: 0.7797 Eval Loss: 0.6080 Eval Acc: 0.7915 (LR: 0.00000010)
[2025-06-14 07:12:48,102]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 054 Train Loss: 0.6262 Train Acc: 0.7815 Eval Loss: 0.6258 Eval Acc: 0.7839 (LR: 0.00000010)
[2025-06-14 07:13:42,918]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 055 Train Loss: 0.6254 Train Acc: 0.7816 Eval Loss: 0.6114 Eval Acc: 0.7924 (LR: 0.00000010)
[2025-06-14 07:14:38,136]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 056 Train Loss: 0.6286 Train Acc: 0.7795 Eval Loss: 0.6092 Eval Acc: 0.7915 (LR: 0.00000010)
[2025-06-14 07:15:32,910]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 057 Train Loss: 0.6216 Train Acc: 0.7831 Eval Loss: 0.6130 Eval Acc: 0.7884 (LR: 0.00000010)
[2025-06-14 07:16:27,859]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 058 Train Loss: 0.6233 Train Acc: 0.7823 Eval Loss: 0.6462 Eval Acc: 0.7758 (LR: 0.00000010)
[2025-06-14 07:17:23,024]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 059 Train Loss: 0.6272 Train Acc: 0.7809 Eval Loss: 0.5986 Eval Acc: 0.7898 (LR: 0.00000010)
[2025-06-14 07:18:19,925]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Epoch: 060 Train Loss: 0.6269 Train Acc: 0.7811 Eval Loss: 0.6228 Eval Acc: 0.7859 (LR: 0.00000010)
[2025-06-14 07:18:19,926]: [ResNet20_parametrized_hardtanh_quantized_2_bits] Best Eval Accuracy: 0.7955
[2025-06-14 07:18:19,983]: 


Quantization of model down to 2 bits finished
[2025-06-14 07:18:19,984]: Model Architecture:
[2025-06-14 07:18:20,042]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9149], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3722765445709229, max_val=1.3722765445709229)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParameterizedHardtanh()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4420], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6570159196853638, max_val=0.6688559055328369)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6856], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4497], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6745127439498901, max_val=0.6745177507400513)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3160], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5764], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8281365633010864, max_val=0.9010845422744751)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9521], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5308], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8040300607681274, max_val=0.7882691621780396)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8973], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5238], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7856136560440063, max_val=0.7856398820877075)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9917], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5166], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7772985696792603, max_val=0.772587776184082)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.4960], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4724], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7086890935897827, max_val=0.708625316619873)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1562], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3836], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5799723863601685, max_val=0.5707082748413086)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5389], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8080810308456421, max_val=0.8085047006607056)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2396], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3769], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5653542280197144, max_val=0.5653563737869263)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6939], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3860], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5790565013885498, max_val=0.5790584087371826)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7813], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3759], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5000227689743042, max_val=0.6276136636734009)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5857], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3622], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5433014631271362, max_val=0.54329514503479)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.5083], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3795], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5695648193359375, max_val=0.5689810514450073)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3832], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.555496335029602, max_val=0.5939769744873047)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4409], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6613900661468506, max_val=0.6613932847976685)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5706], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3892], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5673781633377075, max_val=0.6003050804138184)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6731], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3445], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5056248903274536, max_val=0.5279116630554199)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5162], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3470], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.52049720287323, max_val=0.5206419229507446)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2366], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3347422480583191, max_val=0.37507420778274536)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParameterizedHardtanh(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-1, quant_max=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.0730], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-14 07:18:20,043]: 
Model Weights:
[2025-06-14 07:18:20,043]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-14 07:18:20,045]: Sample Values (25 elements): [0.036801017820835114, -0.07970062643289566, -0.15780296921730042, -0.19839122891426086, -0.08406243473291397, 0.09507589787244797, -0.047616343945264816, -0.17489497363567352, -0.2645401060581207, -0.1020902544260025, 0.23909752070903778, 0.4163413345813751, 0.11038734018802643, -0.27896350622177124, -0.3660500645637512, 0.0027173508424311876, -0.31517618894577026, 0.13816893100738525, 0.14081722497940063, -0.02318131923675537, 0.07838110625743866, 0.1548861265182495, -0.026619218289852142, -0.1679529994726181, 0.09812687337398529]
[2025-06-14 07:18:20,051]: Mean: -0.00165540
[2025-06-14 07:18:20,052]: Min: -0.52896053
[2025-06-14 07:18:20,053]: Max: 0.60791731
[2025-06-14 07:18:20,053]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,053]: Sample Values (16 elements): [1.532155990600586, 1.295990228652954, 1.6122381687164307, 1.2358347177505493, 1.7042683362960815, 1.3133659362792969, 2.0875134468078613, 1.1491825580596924, 1.7604001760482788, 1.2442671060562134, 1.5508018732070923, 1.6256988048553467, 2.093385934829712, 1.9623690843582153, 1.674862265586853, 1.064149260520935]
[2025-06-14 07:18:20,053]: Mean: 1.55665529
[2025-06-14 07:18:20,053]: Min: 1.06414926
[2025-06-14 07:18:20,053]: Max: 2.09338593
[2025-06-14 07:18:20,055]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 07:18:20,055]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4419572949409485, 0.4419572949409485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4419572949409485, -0.4419572949409485]
[2025-06-14 07:18:20,055]: Mean: -0.00000000
[2025-06-14 07:18:20,055]: Min: -0.44195729
[2025-06-14 07:18:20,055]: Max: 0.88391459
[2025-06-14 07:18:20,055]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,056]: Sample Values (16 elements): [1.3890196084976196, 1.387004017829895, 1.789461612701416, 1.097359299659729, 1.5392132997512817, 1.5479710102081299, 0.9894230961799622, 1.2326503992080688, 0.92557692527771, 1.3233357667922974, 1.1162598133087158, 1.0591251850128174, 1.3198972940444946, 1.1544755697250366, 0.9687390327453613, 1.335618495941162]
[2025-06-14 07:18:20,056]: Mean: 1.26094556
[2025-06-14 07:18:20,056]: Min: 0.92557693
[2025-06-14 07:18:20,056]: Max: 1.78946161
[2025-06-14 07:18:20,057]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 07:18:20,057]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44967684149742126, 0.0, 0.0, -0.44967684149742126, 0.44967684149742126, 0.0, 0.0, 0.44967684149742126, 0.0, 0.0, 0.0, 0.0, 0.44967684149742126, 0.0, 0.0, 0.44967684149742126, 0.0, 0.0]
[2025-06-14 07:18:20,057]: Mean: 0.00585517
[2025-06-14 07:18:20,058]: Min: -0.44967684
[2025-06-14 07:18:20,058]: Max: 0.89935368
[2025-06-14 07:18:20,058]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,058]: Sample Values (16 elements): [1.071184515953064, 1.112876296043396, 0.9121383428573608, 1.0988277196884155, 0.8967145085334778, 1.0072402954101562, 0.7942861914634705, 1.3186641931533813, 0.8816419839859009, 0.9679718613624573, 1.077462911605835, 1.2287322282791138, 1.0943354368209839, 0.8590168356895447, 1.494850516319275, 1.080869436264038]
[2025-06-14 07:18:20,058]: Mean: 1.05605078
[2025-06-14 07:18:20,058]: Min: 0.79428619
[2025-06-14 07:18:20,058]: Max: 1.49485052
[2025-06-14 07:18:20,059]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 07:18:20,060]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.5764070749282837, 0.0, 0.0, 0.0, 0.5764070749282837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5764070749282837, 0.5764070749282837, 0.0]
[2025-06-14 07:18:20,060]: Mean: -0.00275194
[2025-06-14 07:18:20,060]: Min: -0.57640707
[2025-06-14 07:18:20,060]: Max: 1.15281415
[2025-06-14 07:18:20,060]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,060]: Sample Values (16 elements): [0.6177216172218323, 1.0108394622802734, 1.1407235860824585, 1.0310237407684326, 0.8043816089630127, 0.678813099861145, 0.816704273223877, 0.8563177585601807, 0.7336844801902771, 1.0587186813354492, 0.9942343831062317, 0.7824448943138123, 0.6759243607521057, 0.9876334071159363, 1.215330958366394, 0.8897298574447632]
[2025-06-14 07:18:20,061]: Mean: 0.89338917
[2025-06-14 07:18:20,061]: Min: 0.61772162
[2025-06-14 07:18:20,061]: Max: 1.21533096
[2025-06-14 07:18:20,062]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 07:18:20,062]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.5307664275169373, 0.0, 0.0, 0.0, 0.0, 0.5307664275169373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5307664275169373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,062]: Mean: 0.01543461
[2025-06-14 07:18:20,062]: Min: -1.06153286
[2025-06-14 07:18:20,063]: Max: 0.53076643
[2025-06-14 07:18:20,063]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,063]: Sample Values (16 elements): [1.0762535333633423, 1.099351406097412, 0.985491931438446, 1.8810291290283203, 1.0452172756195068, 1.3176766633987427, 1.2263354063034058, 1.042669415473938, 1.0794084072113037, 1.1204452514648438, 1.0828187465667725, 1.1576319932937622, 0.9555904269218445, 0.9798086285591125, 1.1429890394210815, 1.0082454681396484]
[2025-06-14 07:18:20,063]: Mean: 1.13756013
[2025-06-14 07:18:20,063]: Min: 0.95559043
[2025-06-14 07:18:20,063]: Max: 1.88102913
[2025-06-14 07:18:20,064]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 07:18:20,065]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.5237511992454529, 0.0, 0.0, -0.5237511992454529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5237511992454529]
[2025-06-14 07:18:20,065]: Mean: -0.00522842
[2025-06-14 07:18:20,065]: Min: -0.52375120
[2025-06-14 07:18:20,065]: Max: 1.04750240
[2025-06-14 07:18:20,065]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,065]: Sample Values (16 elements): [0.7414736747741699, 0.932311475276947, 1.048987865447998, 0.845100462436676, 0.6970074772834778, 0.6357497572898865, 0.7736007571220398, 1.059604525566101, 0.7447616457939148, 1.0206612348556519, 0.6421723961830139, 0.9080069065093994, 0.8323507905006409, 0.4737115800380707, 0.8387475609779358, 1.1146436929702759]
[2025-06-14 07:18:20,066]: Mean: 0.83180571
[2025-06-14 07:18:20,066]: Min: 0.47371158
[2025-06-14 07:18:20,066]: Max: 1.11464369
[2025-06-14 07:18:20,067]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 07:18:20,067]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5166288018226624, 0.0]
[2025-06-14 07:18:20,067]: Mean: 0.00044846
[2025-06-14 07:18:20,067]: Min: -1.03325760
[2025-06-14 07:18:20,068]: Max: 0.51662880
[2025-06-14 07:18:20,068]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-14 07:18:20,068]: Sample Values (16 elements): [1.1152206659317017, 1.7224842309951782, 0.6844257712364197, 1.4772658348083496, 1.6028926372528076, 1.192841649055481, 0.9063382148742676, 0.7790146470069885, 0.9425290822982788, 1.3079659938812256, 1.1504719257354736, 1.6644957065582275, 0.8484341502189636, 1.1654731035232544, 1.7045471668243408, 0.918086588382721]
[2025-06-14 07:18:20,068]: Mean: 1.19890547
[2025-06-14 07:18:20,068]: Min: 0.68442577
[2025-06-14 07:18:20,068]: Max: 1.72248423
[2025-06-14 07:18:20,069]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-14 07:18:20,070]: Sample Values (25 elements): [0.0, -0.47243815660476685, 0.0, 0.0, -0.47243815660476685, -0.47243815660476685, -0.47243815660476685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47243815660476685, 0.0, 0.0, 0.0, 0.47243815660476685, 0.0, 0.0, 0.0, 0.0, 0.0, -0.47243815660476685, 0.0, 0.0]
[2025-06-14 07:18:20,070]: Mean: 0.00707427
[2025-06-14 07:18:20,070]: Min: -0.94487631
[2025-06-14 07:18:20,070]: Max: 0.47243816
[2025-06-14 07:18:20,070]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,070]: Sample Values (25 elements): [1.2052302360534668, 1.1456931829452515, 1.253734827041626, 1.225496530532837, 1.1834537982940674, 0.9230355620384216, 1.2695797681808472, 1.0520594120025635, 1.1536294221878052, 1.0338002443313599, 0.8549104332923889, 1.110201358795166, 1.0720902681350708, 1.1034198999404907, 1.4164903163909912, 0.7952743172645569, 1.090446949005127, 0.965542733669281, 1.2232139110565186, 0.9905180931091309, 1.0059999227523804, 1.3066505193710327, 1.253208875656128, 1.2573317289352417, 1.142940878868103]
[2025-06-14 07:18:20,070]: Mean: 1.13109374
[2025-06-14 07:18:20,071]: Min: 0.79527432
[2025-06-14 07:18:20,071]: Max: 1.41649032
[2025-06-14 07:18:20,072]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 07:18:20,072]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.3835602402687073, 0.0, 0.3835602402687073, 0.0, 0.0, 0.0, 0.3835602402687073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3835602402687073, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,072]: Mean: -0.00690875
[2025-06-14 07:18:20,073]: Min: -0.76712048
[2025-06-14 07:18:20,073]: Max: 0.38356024
[2025-06-14 07:18:20,073]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,073]: Sample Values (25 elements): [1.0933648347854614, 1.0227586030960083, 1.1593928337097168, 0.9589630961418152, 1.2673863172531128, 1.2292559146881104, 0.9277846813201904, 1.1708831787109375, 1.336613655090332, 1.0438960790634155, 1.4322456121444702, 1.1391358375549316, 0.9980443120002747, 1.0679688453674316, 1.1558972597122192, 0.9662002921104431, 0.8700679540634155, 1.1941202878952026, 1.205940842628479, 1.25733482837677, 1.1603671312332153, 1.0393027067184448, 1.0771478414535522, 1.1237285137176514, 1.3999583721160889]
[2025-06-14 07:18:20,073]: Mean: 1.13213575
[2025-06-14 07:18:20,074]: Min: 0.87006795
[2025-06-14 07:18:20,074]: Max: 1.43224561
[2025-06-14 07:18:20,075]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-14 07:18:20,075]: Sample Values (25 elements): [-0.5388619303703308, 0.0, 0.5388619303703308, -0.5388619303703308, 0.0, 0.5388619303703308, -0.5388619303703308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5388619303703308, 0.0, 0.0, 0.0, -0.5388619303703308]
[2025-06-14 07:18:20,075]: Mean: 0.00315739
[2025-06-14 07:18:20,075]: Min: -0.53886193
[2025-06-14 07:18:20,075]: Max: 1.07772386
[2025-06-14 07:18:20,075]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,076]: Sample Values (25 elements): [0.7669051289558411, 0.4911090135574341, 0.5087041854858398, 0.45088082551956177, 0.8018881678581238, 0.5171640515327454, 0.5998281240463257, 0.564143180847168, 0.5500518679618835, 0.789949357509613, 0.7440464496612549, 0.7638120055198669, 0.56928950548172, 0.4895160496234894, 0.5450080633163452, 0.6643280982971191, 0.7776264548301697, 0.6697196364402771, 0.49343693256378174, 0.4632061719894409, 0.675542414188385, 0.5826842784881592, 0.694241464138031, 0.652219295501709, 0.4859999120235443]
[2025-06-14 07:18:20,076]: Mean: 0.60202587
[2025-06-14 07:18:20,076]: Min: 0.45088083
[2025-06-14 07:18:20,076]: Max: 0.80188817
[2025-06-14 07:18:20,077]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 07:18:20,077]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.3769035339355469, 0.0, 0.3769035339355469, 0.0, 0.0, -0.3769035339355469, -0.3769035339355469, 0.0, 0.0, 0.0, 0.3769035339355469, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3769035339355469, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,078]: Mean: -0.00363980
[2025-06-14 07:18:20,078]: Min: -0.37690353
[2025-06-14 07:18:20,078]: Max: 0.37690353
[2025-06-14 07:18:20,078]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,078]: Sample Values (25 elements): [0.8555911779403687, 0.5563239455223083, 0.6924281120300293, 0.639122724533081, 0.6427146792411804, 0.6209352612495422, 0.665132462978363, 0.4698469638824463, 0.7152358889579773, 0.5955680012702942, 0.667871356010437, 0.5754952430725098, 0.7315780520439148, 0.6895297169685364, 0.7434802651405334, 0.6574751734733582, 0.6477432250976562, 0.6379278302192688, 0.6174716353416443, 0.576208770275116, 0.6132528185844421, 0.8354344964027405, 0.6049745082855225, 0.5341671705245972, 0.6849733591079712]
[2025-06-14 07:18:20,078]: Mean: 0.65061450
[2025-06-14 07:18:20,079]: Min: 0.46984696
[2025-06-14 07:18:20,079]: Max: 0.85559118
[2025-06-14 07:18:20,080]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 07:18:20,080]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38603830337524414, -0.38603830337524414, 0.0, -0.38603830337524414, 0.0, 0.0, 0.0, 0.0, 0.0, -0.38603830337524414, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,080]: Mean: 0.00360235
[2025-06-14 07:18:20,080]: Min: -0.38603830
[2025-06-14 07:18:20,080]: Max: 0.38603830
[2025-06-14 07:18:20,080]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,081]: Sample Values (25 elements): [0.9985666275024414, 1.1637749671936035, 1.4521428346633911, 1.146458387374878, 0.9760008454322815, 1.0909132957458496, 1.153562068939209, 1.070053219795227, 1.1081968545913696, 1.0822374820709229, 1.1289210319519043, 0.9571940898895264, 1.3165669441223145, 0.9541473984718323, 1.2491555213928223, 1.0928311347961426, 0.8257145881652832, 1.1630529165267944, 1.0251009464263916, 1.1767021417617798, 1.044937014579773, 1.1017762422561646, 0.7301619052886963, 1.299913763999939, 0.9099941253662109]
[2025-06-14 07:18:20,081]: Mean: 1.09993160
[2025-06-14 07:18:20,081]: Min: 0.73016191
[2025-06-14 07:18:20,081]: Max: 1.45214283
[2025-06-14 07:18:20,082]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 07:18:20,082]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.37587881088256836, 0.0, 0.0, 0.0, 0.37587881088256836, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,083]: Mean: -0.00032628
[2025-06-14 07:18:20,083]: Min: -0.37587881
[2025-06-14 07:18:20,083]: Max: 0.75175762
[2025-06-14 07:18:20,083]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,083]: Sample Values (25 elements): [0.7615305781364441, 0.6278979778289795, 0.841011106967926, 0.6422861218452454, 0.596964418888092, 0.7113184332847595, 0.6305338144302368, 0.7768540382385254, 0.7894351482391357, 0.588189423084259, 0.573047935962677, 0.7984253764152527, 0.5444362163543701, 0.700774610042572, 0.7502570748329163, 0.6340590119361877, 0.6001310348510742, 0.6411540508270264, 0.4977013170719147, 0.6277143955230713, 0.6825475692749023, 0.6113100051879883, 0.5360695719718933, 0.6254435181617737, 0.6848494410514832]
[2025-06-14 07:18:20,083]: Mean: 0.65424293
[2025-06-14 07:18:20,083]: Min: 0.49770132
[2025-06-14 07:18:20,084]: Max: 0.84613270
[2025-06-14 07:18:20,085]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 07:18:20,085]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3621988892555237, 0.3621988892555237, 0.0, -0.3621988892555237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,085]: Mean: 0.00581656
[2025-06-14 07:18:20,085]: Min: -0.72439778
[2025-06-14 07:18:20,085]: Max: 0.36219889
[2025-06-14 07:18:20,085]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-14 07:18:20,086]: Sample Values (25 elements): [1.0832732915878296, 1.0470513105392456, 1.1321312189102173, 1.3851577043533325, 1.0477665662765503, 1.0393532514572144, 1.0333102941513062, 1.3220945596694946, 1.1904765367507935, 1.340349555015564, 0.973980188369751, 1.5426841974258423, 1.3208444118499756, 1.0312103033065796, 1.3566395044326782, 1.3796055316925049, 1.1373605728149414, 1.2327800989151, 1.0823864936828613, 1.0280160903930664, 1.369505763053894, 1.1404197216033936, 1.3524000644683838, 1.0733227729797363, 0.9811739325523376]
[2025-06-14 07:18:20,086]: Mean: 1.18382978
[2025-06-14 07:18:20,086]: Min: 0.96350074
[2025-06-14 07:18:20,086]: Max: 1.54268420
[2025-06-14 07:18:20,087]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-14 07:18:20,087]: Sample Values (25 elements): [0.37951529026031494, 0.0, 0.0, 0.37951529026031494, 0.0, 0.0, -0.37951529026031494, 0.0, 0.37951529026031494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.37951529026031494, 0.0, 0.0]
[2025-06-14 07:18:20,088]: Mean: -0.00053534
[2025-06-14 07:18:20,088]: Min: -0.75903058
[2025-06-14 07:18:20,088]: Max: 0.37951529
[2025-06-14 07:18:20,088]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,088]: Sample Values (25 elements): [0.7911280393600464, 0.8809694647789001, 0.7488481998443604, 0.7638264894485474, 0.7790879607200623, 0.8985041379928589, 0.6945075988769531, 0.8581649661064148, 0.5966871976852417, 0.8218553066253662, 0.6138491630554199, 0.843828558921814, 0.6515775918960571, 0.7259308099746704, 0.8087297677993774, 0.6185846328735352, 0.5967774987220764, 0.6596775650978088, 0.7362112402915955, 0.8773878812789917, 0.8370140194892883, 0.7982627153396606, 0.7331929802894592, 0.6483423709869385, 0.6604136824607849]
[2025-06-14 07:18:20,088]: Mean: 0.73977715
[2025-06-14 07:18:20,089]: Min: 0.59668720
[2025-06-14 07:18:20,089]: Max: 0.91074085
[2025-06-14 07:18:20,090]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 07:18:20,090]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.38315778970718384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38315778970718384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38315778970718384, 0.0, 0.0]
[2025-06-14 07:18:20,090]: Mean: -0.00087308
[2025-06-14 07:18:20,091]: Min: -0.38315779
[2025-06-14 07:18:20,091]: Max: 0.76631558
[2025-06-14 07:18:20,091]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,091]: Sample Values (25 elements): [1.0156822204589844, 1.044589638710022, 0.9464308619499207, 0.9171819686889648, 0.9701455235481262, 0.8945169448852539, 1.0851213932037354, 1.0764552354812622, 0.8014511466026306, 1.0018975734710693, 0.989719808101654, 0.9591736197471619, 0.9112827181816101, 0.9196453094482422, 0.9757391810417175, 1.1320918798446655, 0.881920337677002, 0.9488595128059387, 1.1163766384124756, 0.9165772795677185, 0.9357465505599976, 0.970003068447113, 0.9850188493728638, 0.9845066666603088, 0.9862396121025085]
[2025-06-14 07:18:20,091]: Mean: 0.99076384
[2025-06-14 07:18:20,091]: Min: 0.74542773
[2025-06-14 07:18:20,092]: Max: 1.27836728
[2025-06-14 07:18:20,093]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-14 07:18:20,093]: Sample Values (25 elements): [0.44092780351638794, 0.0, 0.0, 0.0, 0.0, 0.44092780351638794, 0.0, 0.0, 0.0, 0.0, -0.44092780351638794, 0.44092780351638794, 0.44092780351638794, 0.44092780351638794, 0.0, 0.0, -0.44092780351638794, 0.0, 0.0, 0.0, -0.44092780351638794, -0.44092780351638794, -0.44092780351638794, 0.0, -0.44092780351638794]
[2025-06-14 07:18:20,093]: Mean: 0.00452123
[2025-06-14 07:18:20,093]: Min: -0.44092780
[2025-06-14 07:18:20,093]: Max: 0.44092780
[2025-06-14 07:18:20,093]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,094]: Sample Values (25 elements): [0.5613691806793213, 0.33107247948646545, 0.6955501437187195, 0.6316683888435364, 0.5956572890281677, 0.49562177062034607, 0.33710214495658875, 0.5801920294761658, 0.429357647895813, 0.5375823378562927, 0.6532462239265442, 0.5453300476074219, 0.5632664561271667, 0.5789531469345093, 0.669791042804718, 0.4430447816848755, 0.7354569435119629, 0.5568762421607971, 0.5833407640457153, 0.7981281280517578, 0.42032885551452637, 0.6518502235412598, 0.6386244297027588, 0.5320906639099121, 0.4597777724266052]
[2025-06-14 07:18:20,094]: Mean: 0.54543471
[2025-06-14 07:18:20,094]: Min: 0.25746447
[2025-06-14 07:18:20,094]: Max: 0.79812813
[2025-06-14 07:18:20,095]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 07:18:20,096]: Sample Values (25 elements): [0.0, 0.3892277479171753, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3892277479171753, -0.3892277479171753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 07:18:20,096]: Mean: -0.00002112
[2025-06-14 07:18:20,096]: Min: -0.38922775
[2025-06-14 07:18:20,096]: Max: 0.77845550
[2025-06-14 07:18:20,096]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,096]: Sample Values (25 elements): [0.6932225227355957, 0.6203322410583496, 0.6411848664283752, 0.6208270192146301, 0.6914321184158325, 0.616458535194397, 0.5816863179206848, 0.6814056038856506, 0.7065348625183105, 0.7213226556777954, 0.7287777066230774, 0.7093504667282104, 0.7219602465629578, 0.7731702923774719, 0.6436271667480469, 0.6740084886550903, 0.571424663066864, 0.6108258962631226, 0.5506300926208496, 0.8464186191558838, 0.5780816674232483, 0.919274628162384, 0.6515602469444275, 0.6370769143104553, 0.6578519344329834]
[2025-06-14 07:18:20,096]: Mean: 0.68821245
[2025-06-14 07:18:20,097]: Min: 0.45974544
[2025-06-14 07:18:20,097]: Max: 0.93525714
[2025-06-14 07:18:20,098]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 07:18:20,098]: Sample Values (25 elements): [0.0, -0.3445121943950653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3445121943950653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3445121943950653, 0.0]
[2025-06-14 07:18:20,098]: Mean: 0.00136444
[2025-06-14 07:18:20,099]: Min: -0.34451219
[2025-06-14 07:18:20,099]: Max: 0.68902439
[2025-06-14 07:18:20,099]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,099]: Sample Values (25 elements): [1.2358282804489136, 0.9456634521484375, 0.7461723685264587, 1.0552295446395874, 0.9661643505096436, 1.0208567380905151, 0.9530242085456848, 0.7918452620506287, 0.9445880651473999, 0.8666886687278748, 0.9277568459510803, 1.2111499309539795, 1.0334699153900146, 0.920760452747345, 1.0597970485687256, 0.8475136756896973, 1.0442317724227905, 1.081987738609314, 0.9875437021255493, 0.8861300945281982, 1.1607747077941895, 1.0540131330490112, 1.122420310974121, 0.812001645565033, 1.2469959259033203]
[2025-06-14 07:18:20,099]: Mean: 0.98774654
[2025-06-14 07:18:20,099]: Min: 0.74617237
[2025-06-14 07:18:20,099]: Max: 1.36587012
[2025-06-14 07:18:20,100]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 07:18:20,101]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.3470463752746582, 0.0, 0.0, 0.3470463752746582, 0.0, 0.0, 0.0, 0.3470463752746582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3470463752746582, 0.0, 0.0, 0.0, -0.3470463752746582, 0.0, 0.0]
[2025-06-14 07:18:20,101]: Mean: -0.00202406
[2025-06-14 07:18:20,101]: Min: -0.34704638
[2025-06-14 07:18:20,101]: Max: 0.69409275
[2025-06-14 07:18:20,101]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,102]: Sample Values (25 elements): [0.666517436504364, 0.5624608993530273, 0.5421577095985413, 0.663611650466919, 0.43727943301200867, 0.6755965948104858, 0.4624868333339691, 0.5145827531814575, 0.5297090411186218, 0.7000443935394287, 0.7944384217262268, 0.7153260707855225, 0.6984871625900269, 0.727322518825531, 0.4991399049758911, 0.869880199432373, 0.45588696002960205, 0.45186951756477356, 0.7300854325294495, 0.6359931826591492, 0.6865949034690857, 0.6247866749763489, 0.6156936883926392, 0.6951140761375427, 0.5638750195503235]
[2025-06-14 07:18:20,102]: Mean: 0.60890579
[2025-06-14 07:18:20,102]: Min: 0.42542842
[2025-06-14 07:18:20,102]: Max: 0.89931244
[2025-06-14 07:18:20,103]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 07:18:20,104]: Sample Values (25 elements): [-0.23660549521446228, 0.0, 0.0, 0.0, 0.0, 0.23660549521446228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23660549521446228, 0.23660549521446228, 0.0, 0.0, 0.23660549521446228, 0.0, 0.0]
[2025-06-14 07:18:20,104]: Mean: -0.00099484
[2025-06-14 07:18:20,104]: Min: -0.23660550
[2025-06-14 07:18:20,104]: Max: 0.47321099
[2025-06-14 07:18:20,104]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-14 07:18:20,105]: Sample Values (25 elements): [0.9654507040977478, 1.1102533340454102, 1.334397792816162, 1.1211435794830322, 1.0522301197052002, 0.8783162236213684, 0.8931642770767212, 1.1470004320144653, 0.721737802028656, 0.9999058842658997, 0.9326252341270447, 0.6561456322669983, 0.9982791543006897, 1.085143804550171, 0.9835888743400574, 1.3673731088638306, 1.0015993118286133, 0.8378022313117981, 0.9496326446533203, 0.982701301574707, 0.8080527186393738, 1.1325170993804932, 0.9659985303878784, 0.9293422698974609, 1.0405725240707397]
[2025-06-14 07:18:20,105]: Mean: 1.03297114
[2025-06-14 07:18:20,105]: Min: 0.65614563
[2025-06-14 07:18:20,105]: Max: 1.63381445
[2025-06-14 07:18:20,105]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-14 07:18:20,105]: Sample Values (25 elements): [-0.1146334633231163, 0.030927591025829315, 0.025300126522779465, -0.19561667740345, -0.15605765581130981, 0.03552037850022316, -0.2929711937904358, -0.08060310781002045, -0.2651854455471039, -0.173221617937088, 0.18392327427864075, -0.20144397020339966, -0.018143275752663612, -0.02348756603896618, 0.2694089114665985, -0.02024238370358944, 0.06517138332128525, 0.33023226261138916, -0.10217446088790894, -0.18081022799015045, 0.3018200993537903, 0.18727970123291016, -0.08558673411607742, 0.2855243384838104, -0.02379215694963932]
[2025-06-14 07:18:20,106]: Mean: 0.00308350
[2025-06-14 07:18:20,106]: Min: -0.47735384
[2025-06-14 07:18:20,106]: Max: 0.65890038
