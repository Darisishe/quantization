[2025-05-26 13:43:42,190]: 
Training ResNet18 with relu6
[2025-05-26 13:45:04,278]: [ResNet18_relu6] Epoch: 001 Train Loss: 1.4851 Train Acc: 0.4513 Eval Loss: 1.2744 Eval Acc: 0.5307 (LR: 0.00100000)
[2025-05-26 13:46:26,185]: [ResNet18_relu6] Epoch: 002 Train Loss: 1.0535 Train Acc: 0.6232 Eval Loss: 1.0410 Eval Acc: 0.6386 (LR: 0.00100000)
[2025-05-26 13:47:48,250]: [ResNet18_relu6] Epoch: 003 Train Loss: 0.8700 Train Acc: 0.6936 Eval Loss: 0.8653 Eval Acc: 0.7011 (LR: 0.00100000)
[2025-05-26 13:49:10,371]: [ResNet18_relu6] Epoch: 004 Train Loss: 0.7363 Train Acc: 0.7445 Eval Loss: 0.6954 Eval Acc: 0.7635 (LR: 0.00100000)
[2025-05-26 13:50:32,190]: [ResNet18_relu6] Epoch: 005 Train Loss: 0.6528 Train Acc: 0.7736 Eval Loss: 0.6638 Eval Acc: 0.7757 (LR: 0.00100000)
[2025-05-26 13:51:54,043]: [ResNet18_relu6] Epoch: 006 Train Loss: 0.5966 Train Acc: 0.7942 Eval Loss: 0.6883 Eval Acc: 0.7719 (LR: 0.00100000)
[2025-05-26 13:53:15,273]: [ResNet18_relu6] Epoch: 007 Train Loss: 0.5401 Train Acc: 0.8131 Eval Loss: 0.5428 Eval Acc: 0.8197 (LR: 0.00100000)
[2025-05-26 13:54:37,253]: [ResNet18_relu6] Epoch: 008 Train Loss: 0.5038 Train Acc: 0.8256 Eval Loss: 0.5691 Eval Acc: 0.8143 (LR: 0.00100000)
[2025-05-26 13:55:59,333]: [ResNet18_relu6] Epoch: 009 Train Loss: 0.4705 Train Acc: 0.8396 Eval Loss: 0.5173 Eval Acc: 0.8288 (LR: 0.00100000)
[2025-05-26 13:57:21,229]: [ResNet18_relu6] Epoch: 010 Train Loss: 0.4467 Train Acc: 0.8470 Eval Loss: 0.5054 Eval Acc: 0.8393 (LR: 0.00100000)
[2025-05-26 13:58:43,088]: [ResNet18_relu6] Epoch: 011 Train Loss: 0.4214 Train Acc: 0.8551 Eval Loss: 0.4779 Eval Acc: 0.8383 (LR: 0.00100000)
[2025-05-26 14:00:04,888]: [ResNet18_relu6] Epoch: 012 Train Loss: 0.3973 Train Acc: 0.8635 Eval Loss: 0.4436 Eval Acc: 0.8522 (LR: 0.00100000)
[2025-05-26 14:01:27,025]: [ResNet18_relu6] Epoch: 013 Train Loss: 0.3808 Train Acc: 0.8689 Eval Loss: 0.3972 Eval Acc: 0.8702 (LR: 0.00100000)
[2025-05-26 14:02:51,405]: [ResNet18_relu6] Epoch: 014 Train Loss: 0.3667 Train Acc: 0.8739 Eval Loss: 0.4586 Eval Acc: 0.8572 (LR: 0.00100000)
[2025-05-26 14:04:13,346]: [ResNet18_relu6] Epoch: 015 Train Loss: 0.3470 Train Acc: 0.8804 Eval Loss: 0.3893 Eval Acc: 0.8713 (LR: 0.00100000)
[2025-05-26 14:05:35,334]: [ResNet18_relu6] Epoch: 016 Train Loss: 0.3393 Train Acc: 0.8831 Eval Loss: 0.4218 Eval Acc: 0.8625 (LR: 0.00100000)
[2025-05-26 14:06:57,082]: [ResNet18_relu6] Epoch: 017 Train Loss: 0.3237 Train Acc: 0.8884 Eval Loss: 0.4221 Eval Acc: 0.8641 (LR: 0.00100000)
[2025-05-26 14:08:19,026]: [ResNet18_relu6] Epoch: 018 Train Loss: 0.3126 Train Acc: 0.8911 Eval Loss: 0.3428 Eval Acc: 0.8898 (LR: 0.00100000)
[2025-05-26 14:09:41,020]: [ResNet18_relu6] Epoch: 019 Train Loss: 0.3013 Train Acc: 0.8957 Eval Loss: 0.3831 Eval Acc: 0.8777 (LR: 0.00100000)
[2025-05-26 14:11:03,009]: [ResNet18_relu6] Epoch: 020 Train Loss: 0.2931 Train Acc: 0.8981 Eval Loss: 0.4211 Eval Acc: 0.8690 (LR: 0.00100000)
[2025-05-26 14:12:25,116]: [ResNet18_relu6] Epoch: 021 Train Loss: 0.2851 Train Acc: 0.9021 Eval Loss: 0.4351 Eval Acc: 0.8614 (LR: 0.00100000)
[2025-05-26 14:13:47,458]: [ResNet18_relu6] Epoch: 022 Train Loss: 0.2726 Train Acc: 0.9045 Eval Loss: 0.3643 Eval Acc: 0.8818 (LR: 0.00100000)
[2025-05-26 14:15:09,659]: [ResNet18_relu6] Epoch: 023 Train Loss: 0.2670 Train Acc: 0.9087 Eval Loss: 0.3919 Eval Acc: 0.8796 (LR: 0.00100000)
[2025-05-26 14:16:31,685]: [ResNet18_relu6] Epoch: 024 Train Loss: 0.2622 Train Acc: 0.9102 Eval Loss: 0.3647 Eval Acc: 0.8822 (LR: 0.00010000)
[2025-05-26 14:17:53,389]: [ResNet18_relu6] Epoch: 025 Train Loss: 0.1760 Train Acc: 0.9401 Eval Loss: 0.2418 Eval Acc: 0.9215 (LR: 0.00010000)
[2025-05-26 14:19:15,369]: [ResNet18_relu6] Epoch: 026 Train Loss: 0.1423 Train Acc: 0.9516 Eval Loss: 0.2370 Eval Acc: 0.9246 (LR: 0.00010000)
[2025-05-26 14:20:37,251]: [ResNet18_relu6] Epoch: 027 Train Loss: 0.1284 Train Acc: 0.9564 Eval Loss: 0.2405 Eval Acc: 0.9264 (LR: 0.00010000)
[2025-05-26 14:21:59,260]: [ResNet18_relu6] Epoch: 028 Train Loss: 0.1184 Train Acc: 0.9600 Eval Loss: 0.2349 Eval Acc: 0.9282 (LR: 0.00010000)
[2025-05-26 14:23:21,186]: [ResNet18_relu6] Epoch: 029 Train Loss: 0.1130 Train Acc: 0.9618 Eval Loss: 0.2455 Eval Acc: 0.9252 (LR: 0.00010000)
[2025-05-26 14:24:42,446]: [ResNet18_relu6] Epoch: 030 Train Loss: 0.1049 Train Acc: 0.9646 Eval Loss: 0.2402 Eval Acc: 0.9271 (LR: 0.00010000)
[2025-05-26 14:26:04,039]: [ResNet18_relu6] Epoch: 031 Train Loss: 0.0980 Train Acc: 0.9656 Eval Loss: 0.2505 Eval Acc: 0.9278 (LR: 0.00010000)
[2025-05-26 14:27:26,084]: [ResNet18_relu6] Epoch: 032 Train Loss: 0.0965 Train Acc: 0.9672 Eval Loss: 0.2492 Eval Acc: 0.9276 (LR: 0.00010000)
[2025-05-26 14:28:48,601]: [ResNet18_relu6] Epoch: 033 Train Loss: 0.0895 Train Acc: 0.9685 Eval Loss: 0.2517 Eval Acc: 0.9280 (LR: 0.00010000)
[2025-05-26 14:30:10,276]: [ResNet18_relu6] Epoch: 034 Train Loss: 0.0840 Train Acc: 0.9708 Eval Loss: 0.2580 Eval Acc: 0.9282 (LR: 0.00001000)
[2025-05-26 14:31:32,040]: [ResNet18_relu6] Epoch: 035 Train Loss: 0.0743 Train Acc: 0.9746 Eval Loss: 0.2498 Eval Acc: 0.9304 (LR: 0.00001000)
[2025-05-26 14:32:53,678]: [ResNet18_relu6] Epoch: 036 Train Loss: 0.0711 Train Acc: 0.9755 Eval Loss: 0.2523 Eval Acc: 0.9309 (LR: 0.00001000)
[2025-05-26 14:34:15,786]: [ResNet18_relu6] Epoch: 037 Train Loss: 0.0693 Train Acc: 0.9761 Eval Loss: 0.2522 Eval Acc: 0.9307 (LR: 0.00001000)
[2025-05-26 14:35:37,558]: [ResNet18_relu6] Epoch: 038 Train Loss: 0.0683 Train Acc: 0.9762 Eval Loss: 0.2530 Eval Acc: 0.9313 (LR: 0.00001000)
[2025-05-26 14:36:59,370]: [ResNet18_relu6] Epoch: 039 Train Loss: 0.0681 Train Acc: 0.9765 Eval Loss: 0.2536 Eval Acc: 0.9325 (LR: 0.00001000)
[2025-05-26 14:38:21,815]: [ResNet18_relu6] Epoch: 040 Train Loss: 0.0660 Train Acc: 0.9768 Eval Loss: 0.2533 Eval Acc: 0.9322 (LR: 0.00000100)
[2025-05-26 14:39:44,138]: [ResNet18_relu6] Epoch: 041 Train Loss: 0.0639 Train Acc: 0.9789 Eval Loss: 0.2532 Eval Acc: 0.9317 (LR: 0.00000100)
[2025-05-26 14:41:05,972]: [ResNet18_relu6] Epoch: 042 Train Loss: 0.0628 Train Acc: 0.9787 Eval Loss: 0.2533 Eval Acc: 0.9317 (LR: 0.00000100)
[2025-05-26 14:42:27,804]: [ResNet18_relu6] Epoch: 043 Train Loss: 0.0640 Train Acc: 0.9780 Eval Loss: 0.2543 Eval Acc: 0.9321 (LR: 0.00000100)
[2025-05-26 14:43:49,609]: [ResNet18_relu6] Epoch: 044 Train Loss: 0.0662 Train Acc: 0.9772 Eval Loss: 0.2533 Eval Acc: 0.9319 (LR: 0.00000100)
[2025-05-26 14:45:11,521]: [ResNet18_relu6] Epoch: 045 Train Loss: 0.0653 Train Acc: 0.9772 Eval Loss: 0.2518 Eval Acc: 0.9323 (LR: 0.00000100)
[2025-05-26 14:46:33,470]: [ResNet18_relu6] Epoch: 046 Train Loss: 0.0644 Train Acc: 0.9784 Eval Loss: 0.2538 Eval Acc: 0.9327 (LR: 0.00000010)
[2025-05-26 14:47:55,088]: [ResNet18_relu6] Epoch: 047 Train Loss: 0.0637 Train Acc: 0.9790 Eval Loss: 0.2533 Eval Acc: 0.9323 (LR: 0.00000010)
[2025-05-26 14:49:16,908]: [ResNet18_relu6] Epoch: 048 Train Loss: 0.0650 Train Acc: 0.9779 Eval Loss: 0.2538 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-05-26 14:50:38,345]: [ResNet18_relu6] Epoch: 049 Train Loss: 0.0651 Train Acc: 0.9775 Eval Loss: 0.2541 Eval Acc: 0.9328 (LR: 0.00000010)
[2025-05-26 14:52:00,496]: [ResNet18_relu6] Epoch: 050 Train Loss: 0.0644 Train Acc: 0.9784 Eval Loss: 0.2524 Eval Acc: 0.9328 (LR: 0.00000010)
[2025-05-26 14:53:22,504]: [ResNet18_relu6] Epoch: 051 Train Loss: 0.0642 Train Acc: 0.9785 Eval Loss: 0.2538 Eval Acc: 0.9316 (LR: 0.00000010)
[2025-05-26 14:54:44,067]: [ResNet18_relu6] Epoch: 052 Train Loss: 0.0650 Train Acc: 0.9777 Eval Loss: 0.2538 Eval Acc: 0.9325 (LR: 0.00000010)
[2025-05-26 14:56:06,001]: [ResNet18_relu6] Epoch: 053 Train Loss: 0.0639 Train Acc: 0.9787 Eval Loss: 0.2536 Eval Acc: 0.9316 (LR: 0.00000010)
[2025-05-26 14:57:27,654]: [ResNet18_relu6] Epoch: 054 Train Loss: 0.0644 Train Acc: 0.9773 Eval Loss: 0.2552 Eval Acc: 0.9321 (LR: 0.00000010)
[2025-05-26 14:58:49,485]: [ResNet18_relu6] Epoch: 055 Train Loss: 0.0613 Train Acc: 0.9788 Eval Loss: 0.2550 Eval Acc: 0.9320 (LR: 0.00000010)
[2025-05-26 15:00:11,174]: [ResNet18_relu6] Epoch: 056 Train Loss: 0.0653 Train Acc: 0.9770 Eval Loss: 0.2545 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-05-26 15:01:32,725]: [ResNet18_relu6] Epoch: 057 Train Loss: 0.0645 Train Acc: 0.9775 Eval Loss: 0.2541 Eval Acc: 0.9326 (LR: 0.00000010)
[2025-05-26 15:02:54,335]: [ResNet18_relu6] Epoch: 058 Train Loss: 0.0621 Train Acc: 0.9790 Eval Loss: 0.2564 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-05-26 15:04:16,686]: [ResNet18_relu6] Epoch: 059 Train Loss: 0.0613 Train Acc: 0.9790 Eval Loss: 0.2519 Eval Acc: 0.9328 (LR: 0.00000010)
[2025-05-26 15:05:39,817]: [ResNet18_relu6] Epoch: 060 Train Loss: 0.0652 Train Acc: 0.9774 Eval Loss: 0.2536 Eval Acc: 0.9315 (LR: 0.00000010)
[2025-05-26 15:07:01,861]: [ResNet18_relu6] Epoch: 061 Train Loss: 0.0629 Train Acc: 0.9782 Eval Loss: 0.2544 Eval Acc: 0.9330 (LR: 0.00000010)
[2025-05-26 15:08:23,783]: [ResNet18_relu6] Epoch: 062 Train Loss: 0.0646 Train Acc: 0.9776 Eval Loss: 0.2564 Eval Acc: 0.9313 (LR: 0.00000010)
[2025-05-26 15:09:45,878]: [ResNet18_relu6] Epoch: 063 Train Loss: 0.0653 Train Acc: 0.9776 Eval Loss: 0.2516 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-05-26 15:11:07,811]: [ResNet18_relu6] Epoch: 064 Train Loss: 0.0651 Train Acc: 0.9777 Eval Loss: 0.2520 Eval Acc: 0.9329 (LR: 0.00000010)
[2025-05-26 15:12:29,889]: [ResNet18_relu6] Epoch: 065 Train Loss: 0.0622 Train Acc: 0.9785 Eval Loss: 0.2539 Eval Acc: 0.9329 (LR: 0.00000010)
[2025-05-26 15:13:51,503]: [ResNet18_relu6] Epoch: 066 Train Loss: 0.0637 Train Acc: 0.9779 Eval Loss: 0.2574 Eval Acc: 0.9315 (LR: 0.00000010)
[2025-05-26 15:15:13,264]: [ResNet18_relu6] Epoch: 067 Train Loss: 0.0633 Train Acc: 0.9781 Eval Loss: 0.2519 Eval Acc: 0.9321 (LR: 0.00000010)
[2025-05-26 15:16:35,191]: [ResNet18_relu6] Epoch: 068 Train Loss: 0.0647 Train Acc: 0.9779 Eval Loss: 0.2521 Eval Acc: 0.9335 (LR: 0.00000010)
[2025-05-26 15:17:57,226]: [ResNet18_relu6] Epoch: 069 Train Loss: 0.0628 Train Acc: 0.9780 Eval Loss: 0.2538 Eval Acc: 0.9327 (LR: 0.00000010)
[2025-05-26 15:19:19,482]: [ResNet18_relu6] Epoch: 070 Train Loss: 0.0646 Train Acc: 0.9779 Eval Loss: 0.2537 Eval Acc: 0.9315 (LR: 0.00000010)
[2025-05-26 15:20:41,076]: [ResNet18_relu6] Epoch: 071 Train Loss: 0.0647 Train Acc: 0.9778 Eval Loss: 0.2537 Eval Acc: 0.9329 (LR: 0.00000010)
[2025-05-26 15:22:02,808]: [ResNet18_relu6] Epoch: 072 Train Loss: 0.0635 Train Acc: 0.9783 Eval Loss: 0.2521 Eval Acc: 0.9318 (LR: 0.00000010)
[2025-05-26 15:23:24,975]: [ResNet18_relu6] Epoch: 073 Train Loss: 0.0630 Train Acc: 0.9784 Eval Loss: 0.2545 Eval Acc: 0.9318 (LR: 0.00000010)
[2025-05-26 15:24:46,688]: [ResNet18_relu6] Epoch: 074 Train Loss: 0.0651 Train Acc: 0.9776 Eval Loss: 0.2544 Eval Acc: 0.9324 (LR: 0.00000010)
[2025-05-26 15:26:11,019]: [ResNet18_relu6] Epoch: 075 Train Loss: 0.0650 Train Acc: 0.9781 Eval Loss: 0.2546 Eval Acc: 0.9317 (LR: 0.00000010)
[2025-05-26 15:27:53,611]: [ResNet18_relu6] Epoch: 076 Train Loss: 0.0635 Train Acc: 0.9786 Eval Loss: 0.2536 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-05-26 15:29:37,673]: [ResNet18_relu6] Epoch: 077 Train Loss: 0.0641 Train Acc: 0.9782 Eval Loss: 0.2538 Eval Acc: 0.9315 (LR: 0.00000010)
[2025-05-26 15:31:21,599]: [ResNet18_relu6] Epoch: 078 Train Loss: 0.0636 Train Acc: 0.9788 Eval Loss: 0.2536 Eval Acc: 0.9315 (LR: 0.00000010)
[2025-05-26 15:33:05,760]: [ResNet18_relu6] Epoch: 079 Train Loss: 0.0642 Train Acc: 0.9780 Eval Loss: 0.2537 Eval Acc: 0.9320 (LR: 0.00000010)
[2025-05-26 15:34:52,126]: [ResNet18_relu6] Epoch: 080 Train Loss: 0.0649 Train Acc: 0.9774 Eval Loss: 0.2555 Eval Acc: 0.9320 (LR: 0.00000010)
[2025-05-26 15:36:39,835]: [ResNet18_relu6] Epoch: 081 Train Loss: 0.0651 Train Acc: 0.9773 Eval Loss: 0.2511 Eval Acc: 0.9323 (LR: 0.00000010)
[2025-05-26 15:38:26,610]: [ResNet18_relu6] Epoch: 082 Train Loss: 0.0655 Train Acc: 0.9774 Eval Loss: 0.2550 Eval Acc: 0.9318 (LR: 0.00000010)
[2025-05-26 15:40:12,829]: [ResNet18_relu6] Epoch: 083 Train Loss: 0.0657 Train Acc: 0.9770 Eval Loss: 0.2525 Eval Acc: 0.9322 (LR: 0.00000010)
[2025-05-26 15:41:58,259]: [ResNet18_relu6] Epoch: 084 Train Loss: 0.0647 Train Acc: 0.9780 Eval Loss: 0.2533 Eval Acc: 0.9325 (LR: 0.00000010)
[2025-05-26 15:43:41,802]: [ResNet18_relu6] Epoch: 085 Train Loss: 0.0627 Train Acc: 0.9787 Eval Loss: 0.2508 Eval Acc: 0.9333 (LR: 0.00000010)
[2025-05-26 15:45:24,738]: [ResNet18_relu6] Epoch: 086 Train Loss: 0.0639 Train Acc: 0.9784 Eval Loss: 0.2520 Eval Acc: 0.9332 (LR: 0.00000010)
[2025-05-26 15:47:06,600]: [ResNet18_relu6] Epoch: 087 Train Loss: 0.0651 Train Acc: 0.9771 Eval Loss: 0.2521 Eval Acc: 0.9329 (LR: 0.00000010)
[2025-05-26 15:48:48,180]: [ResNet18_relu6] Epoch: 088 Train Loss: 0.0625 Train Acc: 0.9785 Eval Loss: 0.2533 Eval Acc: 0.9321 (LR: 0.00000010)
[2025-05-26 15:48:48,185]: Early stopping was triggered!
[2025-05-26 15:48:48,185]: [ResNet18_relu6] Best Eval Accuracy: 0.9335
[2025-05-26 15:48:48,249]: 
Training of full-precision model finished!
[2025-05-26 15:48:48,249]: Model Architecture:
[2025-05-26 15:48:48,250]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-26 15:48:48,250]: 
Model Weights:
[2025-05-26 15:48:48,250]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-26 15:48:48,252]: Sample Values (25 elements): [0.16778616607189178, 0.232217937707901, 0.008982615545392036, 0.06649547070264816, 0.030399270355701447, 0.022366462275385857, 0.16866154968738556, -0.15244241058826447, 0.10085222125053406, -0.21899884939193726, 0.16056674718856812, 0.13032351434230804, -0.19839096069335938, -0.23548616468906403, -0.20985648036003113, 0.08737872540950775, -0.012055299244821072, 0.023121751844882965, -0.10277191549539566, 0.07017554342746735, -0.21456867456436157, 0.08367902785539627, 0.0750664472579956, 0.17119525372982025, 0.12948182225227356]
[2025-05-26 15:48:48,256]: Mean: -0.00210525
[2025-05-26 15:48:48,258]: Min: -0.34035161
[2025-05-26 15:48:48,259]: Max: 0.29923475
[2025-05-26 15:48:48,259]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-26 15:48:48,262]: Sample Values (25 elements): [0.6414973139762878, 0.5694661140441895, 0.7094448804855347, 0.8496028780937195, 1.226518988609314, 0.6162450313568115, 0.7417699098587036, 1.010140299797058, 0.6030169725418091, 0.6670079231262207, 0.6717849969863892, 0.7518215775489807, 1.044515609741211, 0.716789722442627, 0.676017165184021, 1.1779842376708984, 0.9479161500930786, 0.5970520377159119, 0.7719720005989075, 1.0329564809799194, 0.7366198897361755, 0.9312918782234192, 0.6606447696685791, 0.7214253544807434, 0.8664388656616211]
[2025-05-26 15:48:48,265]: Mean: 0.75581896
[2025-05-26 15:48:48,270]: Min: 0.47719511
[2025-05-26 15:48:48,281]: Max: 1.24143529
[2025-05-26 15:48:48,281]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 15:48:48,294]: Sample Values (25 elements): [0.08129660040140152, -0.024718595668673515, -0.051627736538648605, -0.1348561942577362, -0.06314431130886078, -0.029942186549305916, -0.055533282458782196, 0.01939627341926098, 0.031852979212999344, 0.040774133056402206, 0.008594326674938202, 0.057953111827373505, 0.0032083371188491583, -0.012688533402979374, 0.0643404871225357, 0.04739769548177719, -0.11586205661296844, -0.04536804184317589, -0.025693058967590332, -0.027149366214871407, 0.039664506912231445, -0.006466226186603308, -0.015164881944656372, 0.06908325850963593, -0.01822081208229065]
[2025-05-26 15:48:48,294]: Mean: -0.00768820
[2025-05-26 15:48:48,294]: Min: -0.37743592
[2025-05-26 15:48:48,294]: Max: 0.30585283
[2025-05-26 15:48:48,294]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 15:48:48,295]: Sample Values (25 elements): [0.8614835739135742, 0.7676077485084534, 0.7172077894210815, 0.5875610709190369, 0.6514787673950195, 0.7284923195838928, 0.5255123972892761, 0.7536908388137817, 0.8531484603881836, 0.6683609485626221, 0.7409366369247437, 0.7113212943077087, 0.5263664722442627, 0.7246520519256592, 1.0109730958938599, 0.7455334067344666, 0.7784197926521301, 0.7308158874511719, 0.8502480387687683, 0.9991298913955688, 0.920461118221283, 0.5960401296615601, 0.7061779499053955, 0.8778526186943054, 0.691626250743866]
[2025-05-26 15:48:48,295]: Mean: 0.73809814
[2025-05-26 15:48:48,295]: Min: 0.52551240
[2025-05-26 15:48:48,297]: Max: 1.03350961
[2025-05-26 15:48:48,297]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 15:48:48,303]: Sample Values (25 elements): [-0.04133814573287964, 0.03488939628005028, -0.04514140635728836, -0.05602667108178139, -0.07853633165359497, -0.0011556867975741625, 0.0311720073223114, 0.06387106329202652, -0.043215349316596985, -0.047367215156555176, -0.043001409620046616, -0.06773285567760468, -0.06496532261371613, 0.08685757964849472, -0.004027960821986198, -0.01755986362695694, -0.014654796570539474, -0.014764304272830486, 0.10985369235277176, -0.0463847853243351, 0.014075054787099361, 0.04046572744846344, 0.0014869937440380454, -0.027340946719050407, 0.0007616612128913403]
[2025-05-26 15:48:48,304]: Mean: -0.00325830
[2025-05-26 15:48:48,305]: Min: -0.30617964
[2025-05-26 15:48:48,307]: Max: 0.36234686
[2025-05-26 15:48:48,307]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 15:48:48,315]: Sample Values (25 elements): [0.7732227444648743, 1.2308692932128906, 0.8937610983848572, 0.7856165766716003, 1.2518519163131714, 0.8233577013015747, 0.51290363073349, 0.7465190291404724, 0.9633327126502991, 0.7457442879676819, 0.6542978882789612, 1.211960792541504, 0.7624371647834778, 0.587614119052887, 0.928370475769043, 0.9810743927955627, 0.7511005997657776, 0.7981048822402954, 0.7412733435630798, 0.7996377348899841, 0.8131310939788818, 0.6160752177238464, 0.5061696767807007, 0.6217630505561829, 0.8869746923446655]
[2025-05-26 15:48:48,325]: Mean: 0.83755374
[2025-05-26 15:48:48,338]: Min: 0.50616968
[2025-05-26 15:48:48,338]: Max: 1.48307073
[2025-05-26 15:48:48,338]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 15:48:48,339]: Sample Values (25 elements): [0.03753504902124405, 0.040390562266111374, 0.037723563611507416, -0.04970869794487953, 0.02240646816790104, 0.037850651890039444, 0.004693017806857824, 0.03290325403213501, -0.04221509024500847, -0.0026884502731263638, -0.006260639522224665, -0.09938596934080124, 0.055781569331884384, -0.010455181822180748, -0.04189644753932953, 0.04680795595049858, 0.023448796942830086, -0.0003722624678630382, -0.09227585047483444, -0.06139010936021805, 0.006311912555247545, 0.014917144551873207, -0.21550092101097107, -0.013587640598416328, 0.026915671303868294]
[2025-05-26 15:48:48,339]: Mean: -0.00496294
[2025-05-26 15:48:48,339]: Min: -0.37955225
[2025-05-26 15:48:48,340]: Max: 0.29563177
[2025-05-26 15:48:48,340]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 15:48:48,345]: Sample Values (25 elements): [0.6354960203170776, 0.7546529173851013, 0.8172479867935181, 0.6618984341621399, 0.765630304813385, 0.5007722973823547, 0.5610107779502869, 0.9182040691375732, 0.7509353160858154, 0.7108898162841797, 0.9771685004234314, 0.7608187794685364, 0.6300769448280334, 0.5722904205322266, 0.9311954379081726, 0.9600425362586975, 1.0983532667160034, 0.5749816298484802, 0.6058403253555298, 0.7298076748847961, 0.6826416254043579, 0.8729056715965271, 0.50975501537323, 0.631608784198761, 0.47077837586402893]
[2025-05-26 15:48:48,348]: Mean: 0.68945718
[2025-05-26 15:48:48,349]: Min: 0.42705911
[2025-05-26 15:48:48,350]: Max: 1.10493731
[2025-05-26 15:48:48,350]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 15:48:48,355]: Sample Values (25 elements): [-0.024502843618392944, -0.02305012010037899, -0.029276546090841293, 0.01592593640089035, -0.006331021897494793, -0.057688843458890915, 0.039512455463409424, -0.03261515498161316, 0.0601188950240612, -0.03922693058848381, -0.05783781781792641, 0.024906937032938004, -0.1079116091132164, 0.0833468958735466, -0.0017821327783167362, -0.05507442355155945, -0.041833002120256424, -0.011789421550929546, 0.01925554685294628, -0.06569201499223709, -0.0025323934387415648, 0.015335156582295895, 0.1317853480577469, 0.05275733023881912, -0.06110109016299248]
[2025-05-26 15:48:48,359]: Mean: -0.00322696
[2025-05-26 15:48:48,370]: Min: -0.37147474
[2025-05-26 15:48:48,383]: Max: 0.35575244
[2025-05-26 15:48:48,383]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 15:48:48,383]: Sample Values (25 elements): [0.7915802001953125, 0.8682015538215637, 0.8793171644210815, 1.145231008529663, 0.8565509915351868, 0.7836501598358154, 0.6361914873123169, 0.8302850127220154, 0.4916200041770935, 0.6547435522079468, 0.4891817569732666, 0.950319230556488, 0.8429597020149231, 0.8225240111351013, 0.7637484669685364, 0.49815553426742554, 0.6756256818771362, 0.6479385495185852, 0.7523987889289856, 0.6600486040115356, 0.7264969944953918, 0.7721714377403259, 1.0376110076904297, 0.9770692586898804, 0.5531163215637207]
[2025-05-26 15:48:48,383]: Mean: 0.75802279
[2025-05-26 15:48:48,383]: Min: 0.42944854
[2025-05-26 15:48:48,384]: Max: 1.14523101
[2025-05-26 15:48:48,384]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-26 15:48:48,390]: Sample Values (25 elements): [-0.007894719950854778, 0.020740030333399773, -0.06293507665395737, -0.0689988061785698, 0.01481666136533022, 0.060172948986291885, 0.12181819975376129, -0.05457448214292526, -0.07068581879138947, 0.04191422089934349, -0.07700667530298233, -0.054990269243717194, 0.058738838881254196, -0.036283716559410095, 0.015601575374603271, -0.009187743067741394, 0.03690006583929062, -0.040149442851543427, -0.003957468084990978, 0.05473833158612251, -0.02061811275780201, -0.053147438913583755, -0.01643223501741886, -0.02091662771999836, -0.06790103018283844]
[2025-05-26 15:48:48,392]: Mean: -0.00289627
[2025-05-26 15:48:48,393]: Min: -0.31632897
[2025-05-26 15:48:48,394]: Max: 0.27648923
[2025-05-26 15:48:48,394]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-26 15:48:48,399]: Sample Values (25 elements): [0.5701456069946289, 0.7115190029144287, 0.4456217885017395, 0.5620556473731995, 0.7436287999153137, 0.6978083252906799, 0.5324890613555908, 0.7812309861183167, 0.6531046628952026, 0.6543641090393066, 0.5423752665519714, 0.6642928123474121, 0.45062685012817383, 0.6780040264129639, 0.513996422290802, 0.7047585844993591, 0.7405774593353271, 0.6951406002044678, 0.6095725297927856, 0.8263047337532043, 0.7581714391708374, 0.7717058062553406, 0.5740063786506653, 0.6183397769927979, 0.6823656558990479]
[2025-05-26 15:48:48,404]: Mean: 0.60689372
[2025-05-26 15:48:48,415]: Min: 0.20627765
[2025-05-26 15:48:48,428]: Max: 0.89816022
[2025-05-26 15:48:48,428]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 15:48:48,429]: Sample Values (25 elements): [-0.020367182791233063, 0.008937692269682884, 0.04509953036904335, 0.017227686941623688, -0.00723723229020834, -0.01790655218064785, 0.13769438862800598, -0.08058705180883408, 0.08296015113592148, 0.05649145692586899, -0.02521875686943531, -0.021633416414260864, 0.027208130806684494, -0.08363361656665802, -0.03999197855591774, -0.0006805032026022673, -0.013682173565030098, 0.03418983519077301, -0.02116347663104534, -0.03338174894452095, -0.047393135726451874, -0.008458641357719898, -0.055983901023864746, 0.03377344459295273, -0.030222732573747635]
[2025-05-26 15:48:48,431]: Mean: -0.00360586
[2025-05-26 15:48:48,435]: Min: -0.27619371
[2025-05-26 15:48:48,437]: Max: 0.27466959
[2025-05-26 15:48:48,437]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-26 15:48:48,439]: Sample Values (25 elements): [0.7562179565429688, 0.6801937222480774, 0.48728320002555847, 0.5178931355476379, 0.7482048273086548, 0.827373206615448, 0.7945059537887573, 0.5938131213188171, 0.9234083294868469, 1.036482810974121, 0.611821711063385, 0.7084549069404602, 0.8376590609550476, 0.5092898607254028, 0.7606820464134216, 0.427622526884079, 0.7722935676574707, 0.8811705708503723, 0.878165066242218, 0.7120987176895142, 0.6688755750656128, 0.6453627347946167, 0.8603487610816956, 0.6471875309944153, 0.6514973640441895]
[2025-05-26 15:48:48,441]: Mean: 0.70426977
[2025-05-26 15:48:48,444]: Min: 0.35850856
[2025-05-26 15:48:48,449]: Max: 1.03648281
[2025-05-26 15:48:48,449]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-26 15:48:48,472]: Sample Values (25 elements): [-0.10208536684513092, 0.06522820144891739, -0.06579209119081497, -0.12479027360677719, -0.16121935844421387, -0.037674680352211, -0.08499842882156372, 0.02420645020902157, 0.045618150383234024, -0.14442507922649384, 0.15198656916618347, 0.14132140576839447, -0.02869633585214615, 0.0009845650056377053, 0.20811337232589722, 0.07216185331344604, -0.09628979861736298, 0.06372421979904175, 0.071462482213974, 0.06218985095620155, -0.08267296105623245, -0.050661973655223846, -0.011966286227107048, 0.06454109400510788, 0.021451471373438835]
[2025-05-26 15:48:48,472]: Mean: -0.00531883
[2025-05-26 15:48:48,473]: Min: -0.40090862
[2025-05-26 15:48:48,473]: Max: 0.39598411
[2025-05-26 15:48:48,473]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-26 15:48:48,473]: Sample Values (25 elements): [0.8507604598999023, 0.6265451312065125, 0.7478029131889343, 0.5754582285881042, 0.6739625334739685, 0.697213351726532, 0.559319019317627, 0.5889550447463989, 0.61524897813797, 0.6948514580726624, 0.8659034967422485, 0.46707937121391296, 0.7534809708595276, 0.5925658345222473, 0.6328970789909363, 0.7348447442054749, 0.6044832468032837, 0.5164186358451843, 0.4564441740512848, 0.7320415377616882, 0.7236675024032593, 0.7053722143173218, 0.40766093134880066, 0.6839765906333923, 0.6668334603309631]
[2025-05-26 15:48:48,473]: Mean: 0.61459315
[2025-05-26 15:48:48,473]: Min: 0.30873573
[2025-05-26 15:48:48,474]: Max: 0.88323200
[2025-05-26 15:48:48,474]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 15:48:48,480]: Sample Values (25 elements): [-0.049322567880153656, -0.023247798904776573, -0.03183924779295921, -0.014247496612370014, 0.017185857519507408, 0.08418483287096024, 0.017299890518188477, 0.059087157249450684, -0.03167387470602989, 0.011149166151881218, 0.10981407761573792, -0.020867755636572838, 0.0036499781999737024, 0.06034136936068535, -0.010440681129693985, -0.020320117473602295, -0.030503520742058754, -0.01911347359418869, -0.1072579026222229, 0.04008989781141281, -0.03873249143362045, 0.050138119608163834, -0.0344083271920681, -0.06830686330795288, -0.03286760300397873]
[2025-05-26 15:48:48,482]: Mean: -0.00367168
[2025-05-26 15:48:48,483]: Min: -0.26073715
[2025-05-26 15:48:48,485]: Max: 0.31019825
[2025-05-26 15:48:48,485]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-26 15:48:48,491]: Sample Values (25 elements): [0.5675302147865295, 0.49393072724342346, 0.532725989818573, 0.49179717898368835, 0.9075993299484253, 0.6339610815048218, 0.5541128516197205, 0.6116666793823242, 0.5005772113800049, 0.4810955226421356, 0.6181122660636902, 0.4129067659378052, 0.4807977080345154, 0.5348293781280518, 0.47099927067756653, 0.6345528364181519, 0.636320173740387, 0.5721920728683472, 0.5334960222244263, 0.5812322497367859, 0.523076593875885, 0.3831004500389099, 0.47695669531822205, 0.3905666470527649, 0.9398511052131653]
[2025-05-26 15:48:48,496]: Mean: 0.57561135
[2025-05-26 15:48:48,513]: Min: 0.10331958
[2025-05-26 15:48:48,517]: Max: 0.96976483
[2025-05-26 15:48:48,517]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-26 15:48:48,518]: Sample Values (25 elements): [0.03936747461557388, 0.0061513385735452175, -0.026714326813817024, 0.038296639919281006, -0.0783311277627945, -0.037941206246614456, -0.025037774816155434, 0.002955749863758683, 0.03292360529303551, 0.01775231771171093, 0.015297974459826946, 0.008152165450155735, -0.0014819478383287787, 0.03458268195390701, 0.041575703769922256, 0.05314811319112778, -0.018481917679309845, 0.01255126390606165, 0.01276665460318327, 0.012834911234676838, 0.03635649383068085, -0.011237266473472118, 0.023787692189216614, -0.01672309823334217, -0.04372170567512512]
[2025-05-26 15:48:48,520]: Mean: -0.00267442
[2025-05-26 15:48:48,524]: Min: -0.22956228
[2025-05-26 15:48:48,526]: Max: 0.24226925
[2025-05-26 15:48:48,526]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-26 15:48:48,528]: Sample Values (25 elements): [0.6017926335334778, 0.5279872417449951, 0.6559967398643494, 0.4633042514324188, 0.5269870758056641, 0.6876450181007385, 0.5605311393737793, 0.5150831937789917, 0.6736968159675598, 0.5960221290588379, 0.4315092861652374, 0.5249199867248535, 0.6040858626365662, 0.7149184346199036, 0.6026113629341125, 0.6542811393737793, 0.6636391282081604, 0.5940144658088684, 0.7001509070396423, 0.5952186584472656, 0.6703731417655945, 0.4026019871234894, 0.5241478085517883, 0.5783022046089172, 0.5709320306777954]
[2025-05-26 15:48:48,530]: Mean: 0.58300823
[2025-05-26 15:48:48,533]: Min: 0.26903045
[2025-05-26 15:48:48,538]: Max: 0.86921459
[2025-05-26 15:48:48,538]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-26 15:48:48,561]: Sample Values (25 elements): [0.016210978850722313, 0.05274626612663269, 0.04549145698547363, -0.0705266147851944, 0.08772767335176468, -0.003341113915666938, -0.01748550683259964, 0.00046412390656769276, 0.03680453449487686, -0.020606692880392075, 0.01805989444255829, -0.011794033460319042, -0.011990626342594624, 0.0009127003722824156, 0.055296991020441055, -0.026575040072202682, -0.003838688600808382, -0.02250085026025772, 0.005486725829541683, -0.018167952075600624, -0.024688158184289932, -0.10099462419748306, 0.07836028188467026, 0.01704770140349865, -0.009500243701040745]
[2025-05-26 15:48:48,561]: Mean: -0.00160346
[2025-05-26 15:48:48,562]: Min: -0.29747847
[2025-05-26 15:48:48,562]: Max: 0.28909254
[2025-05-26 15:48:48,562]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-26 15:48:48,565]: Sample Values (25 elements): [0.5586453080177307, 0.5693528652191162, 0.5676690936088562, 0.3568580150604248, 0.18057571351528168, 0.41530752182006836, 0.9026529788970947, 0.5076281428337097, 0.46534276008605957, 0.5064751505851746, 0.1837880164384842, 0.7756655812263489, 0.11096760630607605, 0.4295501410961151, 0.7581045627593994, 0.2588207721710205, 0.5978874564170837, 0.7565614581108093, 0.46590444445610046, 0.3815487325191498, 0.47677290439605713, 0.612506091594696, 0.43751025199890137, 0.6134357452392578, 0.48694851994514465]
[2025-05-26 15:48:48,568]: Mean: 0.44014168
[2025-05-26 15:48:48,571]: Min: 0.00015990
[2025-05-26 15:48:48,572]: Max: 0.90265298
[2025-05-26 15:48:48,572]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 15:48:48,593]: Sample Values (25 elements): [-0.09384264796972275, 0.0339939258992672, 0.033459439873695374, 0.016835445538163185, -0.018560102209448814, -0.04874678701162338, 0.0015822917921468616, -0.03409777954220772, 0.009652581997215748, -0.05543426424264908, -0.012333080172538757, -0.009498486295342445, 0.002380229299888015, -0.006200153846293688, -0.005122307222336531, -0.03962785378098488, -0.027057917788624763, -0.0013093273155391216, 0.035132989287376404, 0.008560238406062126, 0.01803477853536606, 1.871472704806365e-05, 0.00903810653835535, 0.0007455626619048417, -0.035360563546419144]
[2025-05-26 15:48:48,605]: Mean: -0.00133753
[2025-05-26 15:48:48,606]: Min: -0.29242405
[2025-05-26 15:48:48,606]: Max: 0.27241370
[2025-05-26 15:48:48,606]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-26 15:48:48,606]: Sample Values (25 elements): [0.5933018326759338, 0.6940782070159912, 0.3503081202507019, 0.20477785170078278, 0.48747822642326355, 0.6270297765731812, 0.5543312430381775, 0.5456146001815796, 0.4655497074127197, 0.5355096459388733, 0.41938266158103943, 0.7009605169296265, 0.39396604895591736, 0.6512072086334229, 0.5849665999412537, 0.7142987847328186, 0.6303081512451172, 0.5617071390151978, 0.5806716680526733, 0.5622766613960266, 0.6531887650489807, 0.03827887028455734, 0.6386570334434509, 0.5614470839500427, 0.74480140209198]
[2025-05-26 15:48:48,607]: Mean: 0.49630752
[2025-05-26 15:48:48,607]: Min: 0.00000001
[2025-05-26 15:48:48,607]: Max: 0.93399525
[2025-05-26 15:48:48,607]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-26 15:48:48,613]: Sample Values (25 elements): [0.05660003796219826, 0.04272160679101944, -0.06732548028230667, -0.06558157503604889, 0.05326447635889053, 0.014724256470799446, 0.016366107389330864, -0.1269187182188034, 0.007470302749425173, 0.003974427469074726, -0.07679108530282974, 0.03992737829685211, -0.014458101242780685, -0.03792625665664673, 0.12572850286960602, -0.033569950610399246, -0.01770935021340847, -5.3581657994072884e-05, -0.04994305595755577, -0.09862978011369705, -0.03196680545806885, 0.01107350829988718, -0.028113078325986862, -0.0019910435657948256, 0.036849603056907654]
[2025-05-26 15:48:48,615]: Mean: -0.00493383
[2025-05-26 15:48:48,616]: Min: -0.28298634
[2025-05-26 15:48:48,617]: Max: 0.27253243
[2025-05-26 15:48:48,617]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-26 15:48:48,622]: Sample Values (25 elements): [0.40324947237968445, 0.1906629055738449, 0.02017628774046898, 0.27097806334495544, 0.3017524480819702, 0.20476925373077393, 0.36530807614326477, 0.2123231291770935, 0.3047911822795868, 0.5549060702323914, 0.32697150111198425, 0.5261619091033936, 0.3841327428817749, 0.4605734944343567, 0.2731142044067383, 0.5197569727897644, 0.3683834671974182, 0.40983256697654724, 0.39695972204208374, 0.3823314905166626, 0.32950371503829956, 0.3549641966819763, 0.4378289580345154, 0.4179048240184784, 0.1953929215669632]
[2025-05-26 15:48:48,627]: Mean: 0.36886334
[2025-05-26 15:48:48,637]: Min: 0.00067262
[2025-05-26 15:48:48,650]: Max: 0.66059053
[2025-05-26 15:48:48,650]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 15:48:48,660]: Sample Values (25 elements): [-2.6714362406301466e-28, 0.06222947686910629, 0.03700026124715805, -0.007487630937248468, -0.00046994336298666894, -0.0033580909948796034, -1.7722969479053862e-28, -0.03376995399594307, -0.037802230566740036, 9.512964275201805e-32, 0.0016464445507153869, -0.015050478279590607, -0.021570706740021706, -1.771004099282436e-05, -0.07719080150127411, 0.12086521089076996, -0.0033903808798640966, -0.0347197949886322, -9.950452658813447e-05, 9.676139657172438e-39, -0.0029710945673286915, -0.056864239275455475, 0.0067235385067760944, -2.153987975786878e-28, -0.004468477331101894]
[2025-05-26 15:48:48,661]: Mean: -0.00159810
[2025-05-26 15:48:48,662]: Min: -0.27365088
[2025-05-26 15:48:48,664]: Max: 0.30478683
[2025-05-26 15:48:48,664]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-26 15:48:48,674]: Sample Values (25 elements): [0.2261682152748108, 0.4629376232624054, 0.2311994880437851, 0.3061608076095581, 0.43652570247650146, 0.7043948173522949, 0.7390391230583191, 0.39202234148979187, 0.4153019189834595, 0.3737713098526001, 0.3068945109844208, 0.4308024048805237, 0.529299795627594, 0.6856914162635803, 0.042257994413375854, 0.10483699291944504, 0.34399908781051636, 0.4404836595058441, 0.5594472289085388, 0.30261868238449097, 0.40806692838668823, 0.0005783216911368072, 0.43669456243515015, 0.004748217295855284, 0.03135446831583977]
[2025-05-26 15:48:48,690]: Mean: 0.29407170
[2025-05-26 15:48:48,695]: Min: -0.00011837
[2025-05-26 15:48:48,695]: Max: 0.79643559
[2025-05-26 15:48:48,695]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-26 15:48:48,705]: Sample Values (25 elements): [0.0002066916204057634, -0.00046648786519654095, 0.000114507507532835, 1.0956095452534e-26, -2.441127602387332e-19, 0.04599820449948311, 1.5927770953995022e-31, 0.000665477302391082, 4.905385404215455e-41, -0.005751208867877722, 0.003390589728951454, 0.036361340433359146, 0.0009292819304391742, -0.017100751399993896, -0.025255493819713593, 1.6370647675946984e-28, -1.767404092789015e-30, -0.00025060633197426796, -0.0005946837482042611, -0.002227162476629019, 4.908047871297672e-41, 1.6059328816670988e-39, 0.0056223939172923565, -0.004451708868145943, -0.0021732111927121878]
[2025-05-26 15:48:48,706]: Mean: -0.00114856
[2025-05-26 15:48:48,707]: Min: -0.21320769
[2025-05-26 15:48:48,709]: Max: 0.23106256
[2025-05-26 15:48:48,709]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-26 15:48:48,719]: Sample Values (25 elements): [0.44999393820762634, 0.07916924357414246, 0.4143765866756439, 0.3290489614009857, 0.44202879071235657, 0.17745473980903625, 0.5596742033958435, 0.05417298153042793, 0.33612489700317383, 0.168857604265213, 0.5783902406692505, 0.37510326504707336, 0.3594714105129242, 0.48613062500953674, 0.49764519929885864, 0.44781798124313354, 0.11379227787256241, 0.0107734901830554, 0.43672066926956177, 0.6885712146759033, 0.5485495328903198, 0.41665583848953247, 0.4780379831790924, 0.12355493754148483, 0.37214091420173645]
[2025-05-26 15:48:48,735]: Mean: 0.32897156
[2025-05-26 15:48:48,739]: Min: -0.00535681
[2025-05-26 15:48:48,739]: Max: 0.84628040
[2025-05-26 15:48:48,740]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-26 15:48:48,760]: Sample Values (25 elements): [-4.913933324847836e-41, -0.01083381101489067, -4.907066962372644e-41, -4.922481245480217e-41, -0.01024895254522562, 0.036294952034950256, -5.426107913558557e-41, 4.912672156229944e-41, 4.910289948840591e-41, 0.00013393472181633115, -4.905805793754752e-41, 4.92093981716946e-41, 2.0887271299546508e-23, -0.0026626884937286377, -6.83154576108791e-05, 0.014502745121717453, -4.936914619662763e-41, -4.935933710737736e-41, -4.918417479933675e-41, 0.009707523509860039, 4.922200985787352e-41, 4.947564487991632e-41, 4.948825656609524e-41, -0.005369050428271294, -5.064028515398713e-35]
[2025-05-26 15:48:48,771]: Mean: -0.00015183
[2025-05-26 15:48:48,783]: Min: -0.16872747
[2025-05-26 15:48:48,783]: Max: 0.22004095
[2025-05-26 15:48:48,784]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-26 15:48:48,785]: Sample Values (25 elements): [5.193944747982317e-34, 3.135534202534509e-33, 0.4319479465484619, 0.02470214106142521, 5.6172582319109765e-14, 4.088014302072729e-11, 0.0012365737929940224, 0.20075856149196625, 8.889547643775586e-06, 1.769460826539298e-16, 8.42146352653117e-09, 1.3066604273305134e-24, 1.5604370226518505e-11, 4.202290515564755e-09, 0.5022574067115784, 1.5878372323641088e-06, 9.819944435287452e-20, 4.113838343042713e-20, 1.3482110097702779e-17, 1.2205779285068274e-06, 1.216046984246419e-13, 4.873700105445206e-21, 1.7153467313536612e-20, 3.4654406080858458e-12, 1.164747742699035e-10]
[2025-05-26 15:48:48,785]: Mean: 0.09218143
[2025-05-26 15:48:48,787]: Min: -0.00000006
[2025-05-26 15:48:48,791]: Max: 0.94458759
[2025-05-26 15:48:48,791]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 15:48:48,846]: Sample Values (25 elements): [-0.011929296888411045, 0.0015780391404405236, 4.961717602481312e-41, 4.911551117458484e-41, -4.904965014676157e-41, 8.921403787098825e-05, 4.0612201966222843e-35, 4.927806179644652e-41, -0.004660388920456171, -4.952188772923904e-41, -4.956112408624013e-41, -5.614582557010245e-41, -4.915054363619296e-41, -0.0011663563782349229, -4.929908127341139e-41, 2.827217742667821e-40, 4.92150033655519e-41, -4.943360592598657e-41, 8.432364847976714e-05, 4.943080332905792e-41, 4.909589299608429e-41, 0.010708876885473728, 4.651291037087379e-25, -4.907767611604807e-41, -4.394178318313516e-19]
[2025-05-26 15:48:48,851]: Mean: -0.00000952
[2025-05-26 15:48:48,862]: Min: -0.12554958
[2025-05-26 15:48:48,872]: Max: 0.15093885
[2025-05-26 15:48:48,872]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-26 15:48:48,872]: Sample Values (25 elements): [0.27666670083999634, 0.39109474420547485, 0.5080479979515076, 0.6934068202972412, 0.45088067650794983, 0.32095062732696533, 0.575530469417572, 0.7222307920455933, 0.5537409782409668, 0.2956865727901459, 0.24163594841957092, 0.074478879570961, 0.4699978530406952, 0.44376513361930847, 0.42322948575019836, 0.4175320267677307, 0.37028953433036804, 0.3823483884334564, 0.3169565200805664, 0.22933177649974823, 0.2598148286342621, 0.2351134568452835, 0.6136413216590881, 0.5086275935173035, 0.6167945265769958]
[2025-05-26 15:48:48,872]: Mean: 0.38139889
[2025-05-26 15:48:48,873]: Min: -0.02285928
[2025-05-26 15:48:48,873]: Max: 0.83898664
[2025-05-26 15:48:48,873]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-26 15:48:48,880]: Sample Values (25 elements): [-0.007222156971693039, -0.0010680188424885273, -0.015428123064339161, -0.020535297691822052, 0.06680987030267715, -0.007699165027588606, 0.028025291860103607, 0.050987906754016876, 0.000985330785624683, 0.025857824832201004, 0.009029779583215714, 0.00435650022700429, 0.001359128742478788, 0.017834609374403954, -0.0010489483829587698, 0.007078771945089102, -0.024303428828716278, -0.008945230394601822, 0.003280108328908682, 0.04611757770180702, 0.016836192458868027, -0.011117440648376942, -0.02107291668653488, -0.013939429074525833, 0.0026328484527766705]
[2025-05-26 15:48:48,882]: Mean: -0.00027845
[2025-05-26 15:48:48,883]: Min: -0.16400212
[2025-05-26 15:48:48,884]: Max: 0.24430513
[2025-05-26 15:48:48,884]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-26 15:48:48,890]: Sample Values (25 elements): [0.20798656344413757, 0.4656301736831665, 0.6485329866409302, 0.5982222557067871, 0.4383317530155182, 0.5666714310646057, 0.6076191067695618, 0.4816032648086548, 0.5996772646903992, 0.6194612383842468, 0.5302808284759521, 0.1873900443315506, 0.5391941666603088, 0.4599916636943817, 0.5532190799713135, 0.5067827105522156, 0.4937618374824524, 0.584084689617157, 0.5008764266967773, 0.37798118591308594, 0.34970909357070923, 0.34647780656814575, 0.7066072225570679, 0.6745967864990234, 0.5538676381111145]
[2025-05-26 15:48:48,894]: Mean: 0.44127911
[2025-05-26 15:48:48,908]: Min: 0.00000000
[2025-05-26 15:48:48,916]: Max: 0.77336264
[2025-05-26 15:48:48,916]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 15:48:48,968]: Sample Values (25 elements): [4.92430293348384e-41, 4.948265137223794e-41, 4.99997305055738e-41, 4.928506828876814e-41, -4.905385404215455e-41, 4.954150590773958e-41, -6.074348583155217e-41, 4.915334623312161e-41, -4.94294020305936e-41, -4.951768383384606e-41, -4.932150204884059e-41, 4.970545782806559e-41, -4.939156697205683e-41, -4.953449941541796e-41, 4.970966172345856e-41, -4.917016181469351e-41, 4.940978385209305e-41, 4.964239939717097e-41, -4.946163189527307e-41, -4.914914233772863e-41, 4.957793966781203e-41, 4.945042150755847e-41, 4.952048643077471e-41, -5.965047302937881e-41, -4.970265523113694e-41]
[2025-05-26 15:48:48,970]: Mean: 0.00000819
[2025-05-26 15:48:48,971]: Min: -0.04029313
[2025-05-26 15:48:48,972]: Max: 0.05685307
[2025-05-26 15:48:48,973]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-26 15:48:48,979]: Sample Values (25 elements): [5.317786636466921e-35, 1.3148711821761843e-14, 1.6031628480048612e-15, 1.3633180445691035e-26, 0.05468711629509926, -4.908188001144104e-41, -4.91869773962654e-41, 1.2280422208803164e-29, 2.694702708093752e-16, 4.936073840584168e-41, 4.3493249504298394e-21, 7.019070460136239e-28, 0.40318700671195984, 4.908188001144104e-41, 3.2627886574229016e-23, 1.7827144992957518e-14, 4.913092545769241e-41, 4.518848910079631e-28, 1.5471898957385306e-23, 4.908468260836969e-41, 1.3694773104670364e-16, -4.906366313140482e-41, -4.905805793754752e-41, 6.734243163108999e-10, 1.0408379878197117e-23]
[2025-05-26 15:48:48,984]: Mean: 0.00466432
[2025-05-26 15:48:49,001]: Min: -0.05897705
[2025-05-26 15:48:49,005]: Max: 0.49106058
[2025-05-26 15:48:49,005]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-26 15:48:49,056]: Sample Values (25 elements): [-4.908468260836969e-41, 5.217734831913456e-41, -1.7702218467874842e-31, -4.913653065154971e-41, 4.905805793754752e-41, -4.962838641252772e-41, 4.911691247304916e-41, -4.4757472950534657e-41, -4.947284228298767e-41, -4.081141647499597e-41, -4.909449169761997e-41, 4.928366699030382e-41, -4.92374241409811e-41, -4.932290334730491e-41, 4.904824884829725e-41, -4.938035658434223e-41, -4.846110479174515e-41, -4.927666049798219e-41, 4.941398774748602e-41, 4.904824884829725e-41, -4.932150204884059e-41, -4.910009689147727e-41, -4.923462154405245e-41, -4.964380069563529e-41, -4.925704231948164e-41]
[2025-05-26 15:48:49,058]: Mean: -0.00000592
[2025-05-26 15:48:49,059]: Min: -0.06082385
[2025-05-26 15:48:49,061]: Max: 0.04587806
[2025-05-26 15:48:49,061]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-26 15:48:49,069]: Sample Values (25 elements): [0.02572639286518097, 0.042351774871349335, 0.05368930101394653, 0.022041868418455124, 7.97120010247454e-05, 0.005163752008229494, 0.16252245008945465, 0.11483220010995865, 0.18951578438282013, 0.11963160336017609, 0.01343963760882616, 0.3098353147506714, 0.04359688609838486, 0.25369226932525635, 0.13619278371334076, 0.08451566100120544, 0.016505077481269836, 0.029330240562558174, 0.0026143016293644905, 0.11795410513877869, -6.024079672595217e-12, 0.06212881579995155, 0.1497022807598114, 7.367697199021935e-19, 0.052948616445064545]
[2025-05-26 15:48:49,080]: Mean: 0.12911120
[2025-05-26 15:48:49,093]: Min: -0.01004845
[2025-05-26 15:48:49,093]: Max: 0.37342960
[2025-05-26 15:48:49,093]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-26 15:48:49,093]: Sample Values (25 elements): [0.09252337366342545, 0.07706686109304428, -0.04509381204843521, -0.09521342813968658, -0.10751907527446747, 0.09057168662548065, -0.050317008048295975, -0.016692377626895905, -0.09530314058065414, 0.10486558079719543, -0.2771724462509155, 0.11354247480630875, -0.19235418736934662, -0.06733261048793793, -0.022468650713562965, 0.062842458486557, 0.13203339278697968, 0.0906485766172409, -0.1550590991973877, 0.10364039987325668, -0.009574858471751213, -0.026451868936419487, 0.050638847053050995, 0.027650687843561172, 0.09740936756134033]
[2025-05-26 15:48:49,093]: Mean: -0.02095569
[2025-05-26 15:48:49,094]: Min: -0.37029228
[2025-05-26 15:48:49,094]: Max: 0.25228548
[2025-05-26 15:48:49,094]: Checkpoint of model at path [checkpoint/ResNet18_relu6.ckpt] will be used for QAT
[2025-05-27 14:19:27,794]: Checkpoint of model at path [checkpoint/ResNet18_relu6.ckpt] will be used for QAT
[2025-05-27 14:19:27,804]: 


QAT of ResNet18 with relu6 down to 4 bits...
[2025-05-27 14:19:28,030]: [ResNet18_relu6_quantized_4_bits] after configure_qat:
[2025-05-27 14:19:28,077]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 14:21:07,441]: [ResNet18_relu6_quantized_4_bits] Epoch: 001 Train Loss: 0.2399 Train Acc: 0.9166 Eval Loss: 0.3900 Eval Acc: 0.8804 (LR: 0.00100000)
[2025-05-27 14:22:47,177]: [ResNet18_relu6_quantized_4_bits] Epoch: 002 Train Loss: 0.2492 Train Acc: 0.9127 Eval Loss: 0.3783 Eval Acc: 0.8838 (LR: 0.00100000)
[2025-05-27 14:24:26,716]: [ResNet18_relu6_quantized_4_bits] Epoch: 003 Train Loss: 0.2511 Train Acc: 0.9134 Eval Loss: 0.4594 Eval Acc: 0.8652 (LR: 0.00100000)
[2025-05-27 14:26:05,921]: [ResNet18_relu6_quantized_4_bits] Epoch: 004 Train Loss: 0.2489 Train Acc: 0.9126 Eval Loss: 0.3741 Eval Acc: 0.8832 (LR: 0.00100000)
[2025-05-27 14:27:45,390]: [ResNet18_relu6_quantized_4_bits] Epoch: 005 Train Loss: 0.2514 Train Acc: 0.9119 Eval Loss: 0.4042 Eval Acc: 0.8725 (LR: 0.00100000)
[2025-05-27 14:29:24,719]: [ResNet18_relu6_quantized_4_bits] Epoch: 006 Train Loss: 0.2461 Train Acc: 0.9140 Eval Loss: 0.3310 Eval Acc: 0.8943 (LR: 0.00100000)
[2025-05-27 14:31:04,293]: [ResNet18_relu6_quantized_4_bits] Epoch: 007 Train Loss: 0.2483 Train Acc: 0.9129 Eval Loss: 0.3948 Eval Acc: 0.8778 (LR: 0.00100000)
[2025-05-27 14:32:43,415]: [ResNet18_relu6_quantized_4_bits] Epoch: 008 Train Loss: 0.2462 Train Acc: 0.9144 Eval Loss: 0.3473 Eval Acc: 0.8903 (LR: 0.00100000)
[2025-05-27 14:34:22,985]: [ResNet18_relu6_quantized_4_bits] Epoch: 009 Train Loss: 0.2399 Train Acc: 0.9160 Eval Loss: 0.3695 Eval Acc: 0.8913 (LR: 0.00100000)
[2025-05-27 14:36:02,498]: [ResNet18_relu6_quantized_4_bits] Epoch: 010 Train Loss: 0.2443 Train Acc: 0.9154 Eval Loss: 0.3631 Eval Acc: 0.8826 (LR: 0.00100000)
[2025-05-27 14:37:41,824]: [ResNet18_relu6_quantized_4_bits] Epoch: 011 Train Loss: 0.2414 Train Acc: 0.9149 Eval Loss: 0.3728 Eval Acc: 0.8848 (LR: 0.00100000)
[2025-05-27 14:39:21,170]: [ResNet18_relu6_quantized_4_bits] Epoch: 012 Train Loss: 0.2376 Train Acc: 0.9173 Eval Loss: 0.3602 Eval Acc: 0.8921 (LR: 0.00010000)
[2025-05-27 14:41:00,519]: [ResNet18_relu6_quantized_4_bits] Epoch: 013 Train Loss: 0.1557 Train Acc: 0.9462 Eval Loss: 0.2485 Eval Acc: 0.9195 (LR: 0.00010000)
[2025-05-27 14:42:39,898]: [ResNet18_relu6_quantized_4_bits] Epoch: 014 Train Loss: 0.1273 Train Acc: 0.9556 Eval Loss: 0.2421 Eval Acc: 0.9239 (LR: 0.00010000)
[2025-05-27 14:44:18,878]: [ResNet18_relu6_quantized_4_bits] Epoch: 015 Train Loss: 0.1146 Train Acc: 0.9608 Eval Loss: 0.2482 Eval Acc: 0.9239 (LR: 0.00010000)
[2025-05-27 14:45:57,770]: [ResNet18_relu6_quantized_4_bits] Epoch: 016 Train Loss: 0.1069 Train Acc: 0.9632 Eval Loss: 0.2519 Eval Acc: 0.9238 (LR: 0.00010000)
[2025-05-27 14:47:36,720]: [ResNet18_relu6_quantized_4_bits] Epoch: 017 Train Loss: 0.1015 Train Acc: 0.9651 Eval Loss: 0.2454 Eval Acc: 0.9261 (LR: 0.00010000)
[2025-05-27 14:49:15,969]: [ResNet18_relu6_quantized_4_bits] Epoch: 018 Train Loss: 0.0981 Train Acc: 0.9669 Eval Loss: 0.2494 Eval Acc: 0.9277 (LR: 0.00010000)
[2025-05-27 14:50:55,155]: [ResNet18_relu6_quantized_4_bits] Epoch: 019 Train Loss: 0.0910 Train Acc: 0.9681 Eval Loss: 0.2514 Eval Acc: 0.9255 (LR: 0.00010000)
[2025-05-27 14:52:34,985]: [ResNet18_relu6_quantized_4_bits] Epoch: 020 Train Loss: 0.0888 Train Acc: 0.9690 Eval Loss: 0.2476 Eval Acc: 0.9284 (LR: 0.00001000)
[2025-05-27 14:54:14,413]: [ResNet18_relu6_quantized_4_bits] Epoch: 021 Train Loss: 0.0798 Train Acc: 0.9725 Eval Loss: 0.2436 Eval Acc: 0.9307 (LR: 0.00001000)
[2025-05-27 14:55:53,642]: [ResNet18_relu6_quantized_4_bits] Epoch: 022 Train Loss: 0.0782 Train Acc: 0.9734 Eval Loss: 0.2454 Eval Acc: 0.9288 (LR: 0.00001000)
[2025-05-27 14:57:32,765]: [ResNet18_relu6_quantized_4_bits] Epoch: 023 Train Loss: 0.0724 Train Acc: 0.9747 Eval Loss: 0.2446 Eval Acc: 0.9299 (LR: 0.00001000)
[2025-05-27 14:59:11,875]: [ResNet18_relu6_quantized_4_bits] Epoch: 024 Train Loss: 0.0716 Train Acc: 0.9756 Eval Loss: 0.2457 Eval Acc: 0.9302 (LR: 0.00001000)
[2025-05-27 15:00:51,252]: [ResNet18_relu6_quantized_4_bits] Epoch: 025 Train Loss: 0.0732 Train Acc: 0.9747 Eval Loss: 0.2454 Eval Acc: 0.9302 (LR: 0.00001000)
[2025-05-27 15:02:30,763]: [ResNet18_relu6_quantized_4_bits] Epoch: 026 Train Loss: 0.0700 Train Acc: 0.9761 Eval Loss: 0.2448 Eval Acc: 0.9299 (LR: 0.00000100)
[2025-05-27 15:04:10,339]: [ResNet18_relu6_quantized_4_bits] Epoch: 027 Train Loss: 0.0698 Train Acc: 0.9760 Eval Loss: 0.2492 Eval Acc: 0.9307 (LR: 0.00000100)
[2025-05-27 15:05:49,420]: [ResNet18_relu6_quantized_4_bits] Epoch: 028 Train Loss: 0.0694 Train Acc: 0.9754 Eval Loss: 0.2486 Eval Acc: 0.9310 (LR: 0.00000100)
[2025-05-27 15:07:28,552]: [ResNet18_relu6_quantized_4_bits] Epoch: 029 Train Loss: 0.0668 Train Acc: 0.9772 Eval Loss: 0.2421 Eval Acc: 0.9310 (LR: 0.00000100)
[2025-05-27 15:09:07,892]: [ResNet18_relu6_quantized_4_bits] Epoch: 030 Train Loss: 0.0713 Train Acc: 0.9763 Eval Loss: 0.2483 Eval Acc: 0.9294 (LR: 0.00000100)
[2025-05-27 15:10:47,231]: [ResNet18_relu6_quantized_4_bits] Epoch: 031 Train Loss: 0.0685 Train Acc: 0.9772 Eval Loss: 0.2466 Eval Acc: 0.9296 (LR: 0.00000100)
[2025-05-27 15:12:26,749]: [ResNet18_relu6_quantized_4_bits] Epoch: 032 Train Loss: 0.0689 Train Acc: 0.9768 Eval Loss: 0.2480 Eval Acc: 0.9305 (LR: 0.00000010)
[2025-05-27 15:14:06,421]: [ResNet18_relu6_quantized_4_bits] Epoch: 033 Train Loss: 0.0692 Train Acc: 0.9766 Eval Loss: 0.2425 Eval Acc: 0.9309 (LR: 0.00000010)
[2025-05-27 15:15:45,949]: [ResNet18_relu6_quantized_4_bits] Epoch: 034 Train Loss: 0.0685 Train Acc: 0.9764 Eval Loss: 0.2459 Eval Acc: 0.9313 (LR: 0.00000010)
[2025-05-27 15:17:25,493]: [ResNet18_relu6_quantized_4_bits] Epoch: 035 Train Loss: 0.0703 Train Acc: 0.9755 Eval Loss: 0.2490 Eval Acc: 0.9291 (LR: 0.00000010)
[2025-05-27 15:19:04,868]: [ResNet18_relu6_quantized_4_bits] Epoch: 036 Train Loss: 0.0713 Train Acc: 0.9758 Eval Loss: 0.2446 Eval Acc: 0.9300 (LR: 0.00000010)
[2025-05-27 15:20:44,410]: [ResNet18_relu6_quantized_4_bits] Epoch: 037 Train Loss: 0.0705 Train Acc: 0.9760 Eval Loss: 0.2491 Eval Acc: 0.9300 (LR: 0.00000010)
[2025-05-27 15:22:23,944]: [ResNet18_relu6_quantized_4_bits] Epoch: 038 Train Loss: 0.0688 Train Acc: 0.9769 Eval Loss: 0.2456 Eval Acc: 0.9306 (LR: 0.00000010)
[2025-05-27 15:24:03,240]: [ResNet18_relu6_quantized_4_bits] Epoch: 039 Train Loss: 0.0673 Train Acc: 0.9767 Eval Loss: 0.2445 Eval Acc: 0.9288 (LR: 0.00000010)
[2025-05-27 15:25:43,309]: [ResNet18_relu6_quantized_4_bits] Epoch: 040 Train Loss: 0.0712 Train Acc: 0.9756 Eval Loss: 0.2470 Eval Acc: 0.9299 (LR: 0.00000010)
[2025-05-27 15:27:23,015]: [ResNet18_relu6_quantized_4_bits] Epoch: 041 Train Loss: 0.0696 Train Acc: 0.9761 Eval Loss: 0.2507 Eval Acc: 0.9303 (LR: 0.00000010)
[2025-05-27 15:29:02,518]: [ResNet18_relu6_quantized_4_bits] Epoch: 042 Train Loss: 0.0676 Train Acc: 0.9771 Eval Loss: 0.2441 Eval Acc: 0.9312 (LR: 0.00000010)
[2025-05-27 15:30:41,851]: [ResNet18_relu6_quantized_4_bits] Epoch: 043 Train Loss: 0.0707 Train Acc: 0.9756 Eval Loss: 0.2452 Eval Acc: 0.9302 (LR: 0.00000010)
[2025-05-27 15:32:21,104]: [ResNet18_relu6_quantized_4_bits] Epoch: 044 Train Loss: 0.0688 Train Acc: 0.9766 Eval Loss: 0.2464 Eval Acc: 0.9283 (LR: 0.00000010)
[2025-05-27 15:34:00,141]: [ResNet18_relu6_quantized_4_bits] Epoch: 045 Train Loss: 0.0691 Train Acc: 0.9765 Eval Loss: 0.2493 Eval Acc: 0.9307 (LR: 0.00000010)
[2025-05-27 15:35:39,320]: [ResNet18_relu6_quantized_4_bits] Epoch: 046 Train Loss: 0.0682 Train Acc: 0.9772 Eval Loss: 0.2447 Eval Acc: 0.9307 (LR: 0.00000010)
[2025-05-27 15:37:18,426]: [ResNet18_relu6_quantized_4_bits] Epoch: 047 Train Loss: 0.0691 Train Acc: 0.9763 Eval Loss: 0.2527 Eval Acc: 0.9292 (LR: 0.00000010)
[2025-05-27 15:39:05,059]: [ResNet18_relu6_quantized_4_bits] Epoch: 048 Train Loss: 0.0671 Train Acc: 0.9771 Eval Loss: 0.2470 Eval Acc: 0.9286 (LR: 0.00000010)
[2025-05-27 15:40:51,788]: [ResNet18_relu6_quantized_4_bits] Epoch: 049 Train Loss: 0.0687 Train Acc: 0.9765 Eval Loss: 0.2461 Eval Acc: 0.9311 (LR: 0.00000010)
[2025-05-27 15:42:40,088]: [ResNet18_relu6_quantized_4_bits] Epoch: 050 Train Loss: 0.0683 Train Acc: 0.9765 Eval Loss: 0.2468 Eval Acc: 0.9299 (LR: 0.00000010)
[2025-05-27 15:44:20,614]: [ResNet18_relu6_quantized_4_bits] Epoch: 051 Train Loss: 0.0665 Train Acc: 0.9764 Eval Loss: 0.2434 Eval Acc: 0.9307 (LR: 0.00000010)
[2025-05-27 15:46:11,535]: [ResNet18_relu6_quantized_4_bits] Epoch: 052 Train Loss: 0.0676 Train Acc: 0.9772 Eval Loss: 0.2481 Eval Acc: 0.9300 (LR: 0.00000010)
[2025-05-27 15:47:56,606]: [ResNet18_relu6_quantized_4_bits] Epoch: 053 Train Loss: 0.0690 Train Acc: 0.9766 Eval Loss: 0.2424 Eval Acc: 0.9315 (LR: 0.00000010)
[2025-05-27 15:49:42,686]: [ResNet18_relu6_quantized_4_bits] Epoch: 054 Train Loss: 0.0679 Train Acc: 0.9771 Eval Loss: 0.2469 Eval Acc: 0.9292 (LR: 0.00000010)
[2025-05-27 15:51:28,420]: [ResNet18_relu6_quantized_4_bits] Epoch: 055 Train Loss: 0.0682 Train Acc: 0.9767 Eval Loss: 0.2446 Eval Acc: 0.9314 (LR: 0.00000010)
[2025-05-27 15:53:17,650]: [ResNet18_relu6_quantized_4_bits] Epoch: 056 Train Loss: 0.0711 Train Acc: 0.9753 Eval Loss: 0.2439 Eval Acc: 0.9301 (LR: 0.00000010)
[2025-05-27 15:55:00,062]: [ResNet18_relu6_quantized_4_bits] Epoch: 057 Train Loss: 0.0679 Train Acc: 0.9764 Eval Loss: 0.2516 Eval Acc: 0.9306 (LR: 0.00000010)
[2025-05-27 15:56:41,808]: [ResNet18_relu6_quantized_4_bits] Epoch: 058 Train Loss: 0.0697 Train Acc: 0.9763 Eval Loss: 0.2482 Eval Acc: 0.9301 (LR: 0.00000010)
[2025-05-27 15:58:23,637]: [ResNet18_relu6_quantized_4_bits] Epoch: 059 Train Loss: 0.0684 Train Acc: 0.9760 Eval Loss: 0.2481 Eval Acc: 0.9309 (LR: 0.00000010)
[2025-05-27 16:00:08,188]: [ResNet18_relu6_quantized_4_bits] Epoch: 060 Train Loss: 0.0666 Train Acc: 0.9781 Eval Loss: 0.2437 Eval Acc: 0.9305 (LR: 0.00000010)
[2025-05-27 16:00:08,188]: [ResNet18_relu6_quantized_4_bits] Best Eval Accuracy: 0.9315
[2025-05-27 16:00:08,280]: 


Quantization of model down to 4 bits finished
[2025-05-27 16:00:08,280]: Model Architecture:
[2025-05-27 16:00:08,328]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0496], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3769569396972656, max_val=0.36712759733200073)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0513], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3299974799156189, max_val=0.4401113986968994)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0533], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4510604739189148, max_val=0.3491346836090088)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3246], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.868614673614502)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39793771505355835, max_val=0.38488274812698364)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0423], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32984107732772827, max_val=0.30536985397338867)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.7081780433654785)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0396], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2912319004535675, max_val=0.3032710552215576)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4539458751678467, max_val=0.4758211374282837)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0408], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28472787141799927, max_val=0.32666048407554626)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3309], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.9628376960754395)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0342], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24057555198669434, max_val=0.2723844051361084)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0412], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2837524116039276, max_val=0.3345143795013428)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3545], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.317681789398193)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0400], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2986796200275421, max_val=0.30185389518737793)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0394], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28142350912094116, max_val=0.30977118015289307)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3989], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.9830803871154785)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0409], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2719898819923401, max_val=0.34129005670547485)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3240], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.860633373260498)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0330], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24361681938171387, max_val=0.2508821487426758)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0295], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1918405294418335, max_val=0.2502754330635071)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3993], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.9890875816345215)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0215], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14524638652801514, max_val=0.17663943767547607)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22991350293159485, max_val=0.24080805480480194)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.029291797429323196, max_val=0.06848882883787155)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1451], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.1759238243103027)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0078], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06863243877887726, max_val=0.048525620251894)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 16:00:08,328]: 
Model Weights:
[2025-05-27 16:00:08,329]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-27 16:00:08,329]: Sample Values (25 elements): [-0.03255826234817505, -0.08969928324222565, 0.03434039279818535, -0.1839524507522583, -0.09352473169565201, 0.08320699632167816, -0.001503806677646935, -0.09700921922922134, -0.15095549821853638, 0.14683188498020172, -0.07857419550418854, 0.012342799454927444, 0.010824455879628658, -0.008515847846865654, 0.10324304550886154, -0.10209368914365768, 0.05570711940526962, 0.11754792183637619, -0.12256145477294922, 0.2117050588130951, 0.030787155032157898, -0.13512161374092102, -0.12282897531986237, -0.05936378613114357, -0.02217518724501133]
[2025-05-27 16:00:08,329]: Mean: -0.00219156
[2025-05-27 16:00:08,329]: Min: -0.35825095
[2025-05-27 16:00:08,329]: Max: 0.32093379
[2025-05-27 16:00:08,329]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-27 16:00:08,330]: Sample Values (25 elements): [1.4214030504226685, 0.8966297507286072, 0.6031435132026672, 0.6791064143180847, 0.7749847769737244, 0.5069637894630432, 0.7980828881263733, 0.5438103079795837, 0.6917546987533569, 0.46835413575172424, 1.056951880455017, 0.7559453248977661, 0.5554122924804688, 0.9045979976654053, 0.7579302787780762, 0.6410071849822998, 0.6537508368492126, 0.5227439999580383, 0.714042603969574, 0.691258430480957, 0.6911108493804932, 0.5322085022926331, 0.6727290153503418, 0.7249605655670166, 0.6193675398826599]
[2025-05-27 16:00:08,330]: Mean: 0.73378658
[2025-05-27 16:00:08,330]: Min: 0.40271032
[2025-05-27 16:00:08,330]: Max: 1.42140305
[2025-05-27 16:00:08,331]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 16:00:08,332]: Sample Values (25 elements): [-0.04960563778877258, 0.09921127557754517, 0.0, -0.04960563778877258, -0.09921127557754517, -0.04960563778877258, -0.04960563778877258, 0.0, 0.0, 0.04960563778877258, 0.0, -0.04960563778877258, -0.09921127557754517, 0.0, 0.04960563778877258, -0.04960563778877258, 0.0, 0.0, 0.0, -0.04960563778877258, 0.09921127557754517, -0.09921127557754517, -0.04960563778877258, 0.0, -0.04960563778877258]
[2025-05-27 16:00:08,332]: Mean: -0.00780067
[2025-05-27 16:00:08,332]: Min: -0.39684510
[2025-05-27 16:00:08,332]: Max: 0.34723946
[2025-05-27 16:00:08,332]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 16:00:08,332]: Sample Values (25 elements): [0.5097334980964661, 0.4583280682563782, 0.7345066666603088, 0.7498106956481934, 0.7029933929443359, 1.0846024751663208, 0.699366569519043, 0.8383784294128418, 0.7763729691505432, 0.6340049505233765, 0.8436264991760254, 0.5200231671333313, 0.7373060584068298, 0.5391278266906738, 0.6729163527488708, 0.6405516266822815, 0.5238189101219177, 0.4532091021537781, 0.5235325694084167, 0.8427906632423401, 0.7203270196914673, 0.5469295978546143, 0.6322329640388489, 1.0246577262878418, 0.7662076950073242]
[2025-05-27 16:00:08,333]: Mean: 0.69425690
[2025-05-27 16:00:08,333]: Min: 0.37955633
[2025-05-27 16:00:08,333]: Max: 1.08665586
[2025-05-27 16:00:08,334]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 16:00:08,334]: Sample Values (25 elements): [0.0, 0.05134059488773346, 0.05134059488773346, 0.0, 0.0, 0.0, 0.0, -0.05134059488773346, 0.0, 0.0, 0.05134059488773346, 0.0, 0.0, -0.05134059488773346, 0.0, 0.05134059488773346, -0.05134059488773346, 0.10268118977546692, 0.05134059488773346, 0.0, 0.05134059488773346, 0.0, -0.05134059488773346, -0.05134059488773346, 0.0]
[2025-05-27 16:00:08,335]: Mean: -0.00278958
[2025-05-27 16:00:08,335]: Min: -0.30804357
[2025-05-27 16:00:08,335]: Max: 0.46206534
[2025-05-27 16:00:08,335]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 16:00:08,335]: Sample Values (25 elements): [0.713074803352356, 0.8878162503242493, 0.6475203633308411, 0.7751133441925049, 0.7253232598304749, 0.7490118741989136, 0.7658584713935852, 0.5757174491882324, 0.4703809916973114, 0.8764969706535339, 1.0038641691207886, 1.0582865476608276, 1.0952019691467285, 0.6228722333908081, 1.127709984779358, 0.6426889896392822, 0.7979981899261475, 0.7024463415145874, 0.4797915816307068, 0.7304893732070923, 0.7230883836746216, 0.6152371764183044, 0.8931565284729004, 0.6023514866828918, 0.963127613067627]
[2025-05-27 16:00:08,335]: Mean: 0.77371424
[2025-05-27 16:00:08,336]: Min: 0.37999007
[2025-05-27 16:00:08,336]: Max: 1.56047487
[2025-05-27 16:00:08,337]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 16:00:08,337]: Sample Values (25 elements): [-0.05334634333848953, 0.0, 0.0, 0.0, -0.10669268667697906, -0.10669268667697906, 0.05334634333848953, 0.0, 0.05334634333848953, 0.0, 0.0, 0.0, -0.1600390374660492, 0.10669268667697906, 0.0, 0.0, -0.05334634333848953, 0.0, 0.10669268667697906, 0.0, 0.0, 0.0, -0.1600390374660492, 0.05334634333848953, 0.0]
[2025-05-27 16:00:08,337]: Mean: -0.00425740
[2025-05-27 16:00:08,337]: Min: -0.42677075
[2025-05-27 16:00:08,338]: Max: 0.37342441
[2025-05-27 16:00:08,338]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 16:00:08,338]: Sample Values (25 elements): [0.6808892488479614, 0.9833711981773376, 1.0443013906478882, 0.47289037704467773, 0.9126890897750854, 0.7018048167228699, 1.1266536712646484, 0.4282177984714508, 0.42565247416496277, 0.5558421611785889, 0.8610000610351562, 0.6729586124420166, 0.6466787457466125, 0.5852106213569641, 0.6455632448196411, 0.6834834218025208, 0.322384774684906, 0.5452807545661926, 0.6971364617347717, 0.7074854373931885, 0.9413210153579712, 0.6193835735321045, 0.4773956537246704, 0.6822860836982727, 0.6010087728500366]
[2025-05-27 16:00:08,338]: Mean: 0.63207221
[2025-05-27 16:00:08,338]: Min: 0.29795995
[2025-05-27 16:00:08,338]: Max: 1.12665367
[2025-05-27 16:00:08,339]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 16:00:08,340]: Sample Values (25 elements): [0.05218803510069847, -0.05218803510069847, -0.05218803510069847, -0.05218803510069847, 0.05218803510069847, 0.05218803510069847, 0.0, -0.05218803510069847, 0.0, 0.0, 0.05218803510069847, -0.05218803510069847, 0.05218803510069847, 0.0, 0.0, 0.0, 0.0, -0.15656410157680511, -0.05218803510069847, 0.10437607020139694, -0.10437607020139694, -0.05218803510069847, -0.05218803510069847, 0.0, 0.0]
[2025-05-27 16:00:08,340]: Mean: -0.00279174
[2025-05-27 16:00:08,340]: Min: -0.41750428
[2025-05-27 16:00:08,340]: Max: 0.36531624
[2025-05-27 16:00:08,340]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 16:00:08,341]: Sample Values (25 elements): [0.5667971968650818, 0.5676329731941223, 0.7162306904792786, 0.9874795079231262, 0.4245636761188507, 0.872905969619751, 0.7441560626029968, 0.8151304125785828, 0.5403671860694885, 0.6075537204742432, 0.7815456986427307, 0.6086277961730957, 0.2578314244747162, 0.6560828685760498, 0.6920337677001953, 0.8014698028564453, 0.5377386212348938, 0.8977100253105164, 0.7096383571624756, 0.8234268426895142, 0.8105558753013611, 0.9533520936965942, 0.6319177150726318, 1.1227988004684448, 0.7601488828659058]
[2025-05-27 16:00:08,341]: Mean: 0.69916481
[2025-05-27 16:00:08,341]: Min: 0.25783142
[2025-05-27 16:00:08,341]: Max: 1.12279880
[2025-05-27 16:00:08,343]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-27 16:00:08,344]: Sample Values (25 elements): [0.0, -0.04234739765524864, 0.0, 0.0, 0.0, 0.0, -0.04234739765524864, 0.0, 0.16938959062099457, 0.04234739765524864, 0.0, -0.04234739765524864, -0.08469479531049728, 0.04234739765524864, 0.0, 0.08469479531049728, 0.0, -0.08469479531049728, 0.08469479531049728, 0.0, 0.0, 0.04234739765524864, -0.04234739765524864, 0.0, 0.0]
[2025-05-27 16:00:08,344]: Mean: -0.00233483
[2025-05-27 16:00:08,344]: Min: -0.33877918
[2025-05-27 16:00:08,344]: Max: 0.29643178
[2025-05-27 16:00:08,344]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-27 16:00:08,345]: Sample Values (25 elements): [0.4835987389087677, 0.5202796459197998, 0.38544875383377075, 0.6308688521385193, 0.6199495196342468, 0.3103400766849518, 0.5742289423942566, 0.6389183402061462, 0.15738174319267273, 0.32597073912620544, 0.5911021828651428, 0.295366495847702, 0.7183182239532471, 0.32588255405426025, 0.016731608659029007, 0.6707594394683838, 0.4360542297363281, 0.284957617521286, 0.6472588181495667, 0.669465184211731, 0.6331096291542053, 0.41573894023895264, 0.4296664297580719, 0.7306402325630188, 0.37122663855552673]
[2025-05-27 16:00:08,345]: Mean: 0.53263986
[2025-05-27 16:00:08,345]: Min: 0.01379518
[2025-05-27 16:00:08,345]: Max: 0.85217917
[2025-05-27 16:00:08,346]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 16:00:08,348]: Sample Values (25 elements): [0.039633531123399734, 0.0, 0.0, 0.07926706224679947, 0.1189005970954895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039633531123399734, 0.0, 0.0, 0.0, 0.039633531123399734, 0.07926706224679947, 0.039633531123399734, 0.0, 0.0, 0.0, -0.039633531123399734, 0.0, 0.039633531123399734, -0.039633531123399734, -0.039633531123399734]
[2025-05-27 16:00:08,348]: Mean: -0.00304019
[2025-05-27 16:00:08,348]: Min: -0.27743471
[2025-05-27 16:00:08,348]: Max: 0.31706825
[2025-05-27 16:00:08,348]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-27 16:00:08,348]: Sample Values (25 elements): [0.8731199502944946, 0.6112077832221985, 0.7227013111114502, 0.4007599353790283, 0.38607797026634216, 0.5127447247505188, 0.79579758644104, 0.6435723900794983, 0.4217563569545746, 0.3363458514213562, 0.763709545135498, 0.8214341402053833, 0.597640872001648, 0.5446237921714783, 0.697960615158081, 1.0262607336044312, 0.777128279209137, 0.6773157715797424, 0.7787668108940125, 0.5081735253334045, 0.719230592250824, 0.7468518018722534, 0.7298696637153625, 0.5001590251922607, 0.6467857956886292]
[2025-05-27 16:00:08,349]: Mean: 0.64968520
[2025-05-27 16:00:08,349]: Min: 0.22809334
[2025-05-27 16:00:08,349]: Max: 1.02626073
[2025-05-27 16:00:08,351]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-27 16:00:08,351]: Sample Values (25 elements): [0.06198447197675705, 0.0, 0.06198447197675705, 0.1239689439535141, -0.06198447197675705, -0.1239689439535141, 0.1239689439535141, 0.0, 0.06198447197675705, -0.06198447197675705, 0.0, 0.0, 0.06198447197675705, -0.06198447197675705, 0.06198447197675705, 0.18595340847969055, -0.18595340847969055, -0.18595340847969055, 0.1239689439535141, 0.18595340847969055, 0.0, 0.0, 0.06198447197675705, 0.0, -0.18595340847969055]
[2025-05-27 16:00:08,351]: Mean: -0.00522843
[2025-05-27 16:00:08,351]: Min: -0.43389130
[2025-05-27 16:00:08,352]: Max: 0.49587578
[2025-05-27 16:00:08,352]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-27 16:00:08,352]: Sample Values (25 elements): [0.6629301309585571, 0.41314926743507385, 0.31641867756843567, 0.5102712512016296, 0.53003990650177, 0.665937066078186, 0.5532470345497131, 0.40970930457115173, 0.6436911821365356, 0.6628087759017944, 0.4687002897262573, 0.47171857953071594, 0.625236988067627, 0.8334174156188965, 0.5346803665161133, 0.6622889041900635, 0.6577007174491882, 0.6792238354682922, 0.5828813910484314, 0.3433364927768707, 0.4604787230491638, 0.6640002727508545, 0.6071880459785461, 0.4156706631183624, 0.4117516279220581]
[2025-05-27 16:00:08,352]: Mean: 0.52536196
[2025-05-27 16:00:08,352]: Min: 0.13390642
[2025-05-27 16:00:08,352]: Max: 0.84463149
[2025-05-27 16:00:08,353]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 16:00:08,355]: Sample Values (25 elements): [0.04075922444462776, -0.04075922444462776, -0.08151844888925552, 0.08151844888925552, 0.0, 0.0, -0.04075922444462776, 0.04075922444462776, -0.04075922444462776, 0.08151844888925552, -0.04075922444462776, -0.08151844888925552, 0.0, 0.08151844888925552, -0.04075922444462776, 0.04075922444462776, 0.0, -0.04075922444462776, -0.08151844888925552, -0.08151844888925552, 0.0, 0.0, 0.0, -0.04075922444462776, -0.04075922444462776]
[2025-05-27 16:00:08,355]: Mean: -0.00292752
[2025-05-27 16:00:08,356]: Min: -0.28531456
[2025-05-27 16:00:08,356]: Max: 0.32607380
[2025-05-27 16:00:08,356]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-27 16:00:08,356]: Sample Values (25 elements): [0.516991913318634, 0.41778984665870667, 0.5573036670684814, 0.6072442531585693, 0.5032255053520203, 0.7343268990516663, 0.5146666765213013, 0.6805069446563721, 0.446496844291687, 7.393813916678482e-08, 0.4310753345489502, 0.2623220682144165, 0.7321880459785461, 0.5707477927207947, 0.46749237179756165, 0.20084117352962494, 0.4289500415325165, 0.2635630667209625, 0.45360198616981506, 0.17310257256031036, 0.5242616534233093, 0.6496544480323792, 0.5032014846801758, 0.4207315146923065, 0.5249328017234802]
[2025-05-27 16:00:08,356]: Mean: 0.50268781
[2025-05-27 16:00:08,356]: Min: 0.00000000
[2025-05-27 16:00:08,357]: Max: 0.86901343
[2025-05-27 16:00:08,358]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 16:00:08,359]: Sample Values (25 elements): [0.0, 0.034197330474853516, 0.0, 0.06839466094970703, 0.0, 0.0, 0.0, 0.0, 0.034197330474853516, 0.0, -0.06839466094970703, 0.0, 0.034197330474853516, -0.034197330474853516, 0.06839466094970703, 0.0, -0.034197330474853516, -0.034197330474853516, 0.034197330474853516, -0.034197330474853516, -0.034197330474853516, 0.0, 0.034197330474853516, 0.034197330474853516, 0.0]
[2025-05-27 16:00:08,359]: Mean: -0.00306384
[2025-05-27 16:00:08,359]: Min: -0.23938131
[2025-05-27 16:00:08,359]: Max: 0.27357864
[2025-05-27 16:00:08,360]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-27 16:00:08,360]: Sample Values (25 elements): [0.5905199646949768, 0.5209405422210693, 0.3970417380332947, 0.3894166648387909, 0.6210924983024597, 0.39104533195495605, 0.6353815793991089, 0.29981106519699097, 0.5173724293708801, 0.6522350907325745, 0.6106031537055969, 0.5603639483451843, 0.6353245973587036, 0.4125001132488251, 0.7145830988883972, 0.4573630690574646, 0.31857845187187195, 0.529271125793457, 0.5307217240333557, 0.6429963111877441, 0.6721612811088562, 0.6704382300376892, 0.5142824053764343, 0.47586530447006226, 0.4799114763736725]
[2025-05-27 16:00:08,360]: Mean: 0.51280296
[2025-05-27 16:00:08,360]: Min: 0.16676515
[2025-05-27 16:00:08,360]: Max: 0.83044034
[2025-05-27 16:00:08,361]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-27 16:00:08,364]: Sample Values (25 elements): [0.0, -0.08243557065725327, 0.0, 0.0, -0.08243557065725327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04121778532862663, 0.0, 0.0, 0.04121778532862663, -0.04121778532862663, 0.0, 0.0, 0.08243557065725327, 0.0, 0.0, 0.04121778532862663, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,365]: Mean: -0.00128274
[2025-05-27 16:00:08,365]: Min: -0.28852451
[2025-05-27 16:00:08,365]: Max: 0.32974228
[2025-05-27 16:00:08,365]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-27 16:00:08,365]: Sample Values (25 elements): [4.972507600656613e-41, 0.675662636756897, 0.22039860486984253, 0.23684801161289215, 0.41225358843803406, 0.8003135323524475, 0.48784929513931274, 0.5953157544136047, 5.075783297477352e-41, 0.5197574496269226, 0.4790765345096588, -4.996750064089433e-41, 0.4596792459487915, 0.3868257701396942, 0.6698027849197388, 5.85882887934206e-41, 0.5926218628883362, 0.26864829659461975, 0.024126309901475906, 0.003221070859581232, 0.5199098587036133, 0.3886875808238983, 0.5547978281974792, 0.4758544862270355, 0.5976146459579468]
[2025-05-27 16:00:08,366]: Mean: 0.37256384
[2025-05-27 16:00:08,366]: Min: -0.00000000
[2025-05-27 16:00:08,366]: Max: 0.85270911
[2025-05-27 16:00:08,367]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 16:00:08,373]: Sample Values (25 elements): [0.0, 0.0, 0.12010670453310013, 0.0, 0.0, 0.0, 0.0, -0.04003556817770004, 0.04003556817770004, 0.04003556817770004, 0.0, 0.0, 0.04003556817770004, -0.04003556817770004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04003556817770004]
[2025-05-27 16:00:08,373]: Mean: -0.00100506
[2025-05-27 16:00:08,374]: Min: -0.28024897
[2025-05-27 16:00:08,374]: Max: 0.32028455
[2025-05-27 16:00:08,374]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-27 16:00:08,374]: Sample Values (25 elements): [0.4596617519855499, 0.5918038487434387, 0.6422529816627502, 3.7923357787406076e-09, 0.32375437021255493, 0.6319592595100403, 0.7289677262306213, 0.6417436003684998, 0.6968965530395508, 0.5800132751464844, 0.7226163744926453, -5.337545850613228e-41, 0.4692017734050751, 0.0025188650470227003, 0.5902917981147766, 0.41758355498313904, 0.21692973375320435, 0.4127255976200104, 0.5337142944335938, 0.6169450283050537, 3.897492661053548e-06, 0.1250709593296051, 0.2099170833826065, 0.41644012928009033, 0.5576349496841431]
[2025-05-27 16:00:08,374]: Mean: 0.42296559
[2025-05-27 16:00:08,374]: Min: -0.00000095
[2025-05-27 16:00:08,375]: Max: 0.87896299
[2025-05-27 16:00:08,376]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-27 16:00:08,376]: Sample Values (25 elements): [0.039412982761859894, -0.039412982761859894, 0.0, 0.0, -0.07882596552371979, 0.07882596552371979, -0.039412982761859894, 0.0, 0.0, 0.11823894828557968, 0.0, 0.07882596552371979, -0.039412982761859894, 0.0, 0.0, 0.0, 0.039412982761859894, 0.0, 0.0, 0.15765193104743958, 0.039412982761859894, -0.07882596552371979, 0.0, -0.039412982761859894, 0.039412982761859894]
[2025-05-27 16:00:08,377]: Mean: -0.00468847
[2025-05-27 16:00:08,377]: Min: -0.27589089
[2025-05-27 16:00:08,378]: Max: 0.31530386
[2025-05-27 16:00:08,378]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-27 16:00:08,378]: Sample Values (25 elements): [0.2286486029624939, 0.41556715965270996, 0.3764350712299347, 0.29139432311058044, 0.28553158044815063, 0.333588182926178, 5.332962246029638e-06, 0.31514182686805725, 0.34528231620788574, 0.23987236618995667, 0.28097718954086304, 0.44420573115348816, 0.2607952952384949, 0.2891785502433777, 0.0027771866880357265, 0.20474399626255035, 0.30202046036720276, 5.399283797835608e-10, 0.35672444105148315, 0.38592708110809326, 0.37086814641952515, 0.4720504581928253, 0.28115811944007874, 0.427632600069046, 0.4293208122253418]
[2025-05-27 16:00:08,378]: Mean: 0.28457338
[2025-05-27 16:00:08,378]: Min: -0.00000000
[2025-05-27 16:00:08,379]: Max: 0.60411173
[2025-05-27 16:00:08,380]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 16:00:08,386]: Sample Values (25 elements): [-0.040885332971811295, 0.0, -0.040885332971811295, 0.0, 0.040885332971811295, 0.0, 0.0, 0.0, 0.040885332971811295, 0.0, 0.0, -0.040885332971811295, 0.0, 0.0, 0.0, 0.0, 0.0, -0.12265600264072418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,387]: Mean: -0.00122422
[2025-05-27 16:00:08,387]: Min: -0.28619733
[2025-05-27 16:00:08,387]: Max: 0.32708266
[2025-05-27 16:00:08,387]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-27 16:00:08,387]: Sample Values (25 elements): [5.218575610992051e-41, -5.782738372729223e-41, -4.904965014676157e-41, 0.5522143840789795, 0.194840207695961, -4.918837869472973e-41, 0.4691847860813141, -6.024462357825254e-41, -5.868637968592334e-41, 0.25369417667388916, 0.08306441456079483, 0.650709331035614, 5.144166662536403e-41, 0.2640495002269745, 0.4124484956264496, 5.202460678652316e-41, 0.4368322491645813, -5.858688749495628e-41, 0.5984952449798584, 0.4869295656681061, 0.40451523661613464, 0.34132885932922363, -5.395699736882708e-41, 0.09530194103717804, 0.7422367334365845]
[2025-05-27 16:00:08,387]: Mean: 0.23540023
[2025-05-27 16:00:08,388]: Min: -0.00000000
[2025-05-27 16:00:08,388]: Max: 0.74223673
[2025-05-27 16:00:08,389]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 16:00:08,395]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.032966598868370056, 0.0, 0.0, 0.0, 0.032966598868370056, 0.0, 0.032966598868370056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032966598868370056, 0.0, 0.0, -0.06593319773674011, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,395]: Mean: -0.00090981
[2025-05-27 16:00:08,396]: Min: -0.23076619
[2025-05-27 16:00:08,396]: Max: 0.26373279
[2025-05-27 16:00:08,396]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-27 16:00:08,396]: Sample Values (25 elements): [-6.059074429894077e-41, 2.807491910061799e-07, 0.05639069154858589, 0.28996843099594116, 0.04930859059095383, 0.015590360388159752, 0.3450646996498108, -2.6987250748788938e-05, 0.5676813125610352, -1.251314824912697e-05, 0.3998764753341675, 0.3938775658607483, 0.006981066893786192, 0.07718754559755325, 0.26125478744506836, 0.42172250151634216, 0.5782925486564636, 1.1441115610466568e-08, -4.907207092219077e-41, 1.7244775052382977e-11, 0.390919953584671, 4.354179054644192e-06, 0.2843083143234253, 0.37156569957733154, 0.42905983328819275]
[2025-05-27 16:00:08,397]: Mean: 0.25590038
[2025-05-27 16:00:08,397]: Min: -0.01934710
[2025-05-27 16:00:08,397]: Max: 0.88352835
[2025-05-27 16:00:08,398]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-27 16:00:08,411]: Sample Values (25 elements): [-0.05894879624247551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,412]: Mean: -0.00007218
[2025-05-27 16:00:08,412]: Min: -0.20632079
[2025-05-27 16:00:08,412]: Max: 0.23579518
[2025-05-27 16:00:08,412]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-27 16:00:08,413]: Sample Values (25 elements): [5.566377889837471e-41, -5.950894188448201e-41, 4.968303705263639e-41, -5.78259824288279e-41, -4.907627481758374e-41, 5.302793648697973e-41, 0.007758182939141989, 6.253574656742361e-41, 5.124968873575153e-41, 5.26874209601488e-41, 6.027405084600336e-41, -4.904684754983292e-41, 5.408451552908064e-41, -4.987501494224889e-41, -5.317647412419816e-41, 0.43261075019836426, 5.515791015275345e-41, 4.98301733913905e-41, 5.034164733086905e-41, 0.04071876034140587, 4.932850854116221e-41, 5.247162099664278e-41, 4.939156697205683e-41, -4.991004740385701e-41, 5.230626777785245e-41]
[2025-05-27 16:00:08,413]: Mean: 0.05638156
[2025-05-27 16:00:08,413]: Min: -0.00000000
[2025-05-27 16:00:08,413]: Max: 0.99225229
[2025-05-27 16:00:08,415]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 16:00:08,468]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,468]: Mean: 0.00003994
[2025-05-27 16:00:08,469]: Min: -0.15021339
[2025-05-27 16:00:08,469]: Max: 0.17167245
[2025-05-27 16:00:08,469]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-27 16:00:08,471]: Sample Values (25 elements): [0.5297251343727112, 0.0074696955271065235, 0.4443568289279938, 0.1300920993089676, 5.5099673772929236e-05, 0.3783484399318695, 0.3174649477005005, 0.45300528407096863, 0.5128570199012756, 0.3153408467769623, -4.954010460927526e-41, 0.16350750625133514, 0.029274215921759605, 0.327691912651062, 0.32599812746047974, -5.614582557010245e-41, 0.3187078833580017, 0.235941082239151, 1.9518642344756865e-13, 0.1255209892988205, 0.3050902485847473, 0.4556988477706909, 0.5157169103622437, 0.31620219349861145, 0.4262111485004425]
[2025-05-27 16:00:08,471]: Mean: 0.28571743
[2025-05-27 16:00:08,471]: Min: -0.00010240
[2025-05-27 16:00:08,471]: Max: 0.83564031
[2025-05-27 16:00:08,472]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-27 16:00:08,473]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0941443145275116, 0.0, 0.03138143941760063, 0.0, 0.03138143941760063, -0.03138143941760063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,474]: Mean: 0.00035027
[2025-05-27 16:00:08,475]: Min: -0.21967007
[2025-05-27 16:00:08,475]: Max: 0.25105152
[2025-05-27 16:00:08,475]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-27 16:00:08,475]: Sample Values (25 elements): [0.5236557722091675, 0.006308636628091335, 0.19429028034210205, 0.4925895035266876, 0.4372468590736389, 0.4165984094142914, 0.2031642198562622, 6.256657513363876e-41, 0.3501664102077484, 4.90566566390832e-41, 0.5862608551979065, 1.0004289833887015e-05, 0.3892050087451935, 0.3456529378890991, 0.15174460411071777, 4.2468027797492107e-20, 0.4100305140018463, -5.609958272077973e-41, 0.4666617512702942, 0.12023717164993286, 0.4516860842704773, 0.37180718779563904, 0.5184652209281921, 0.49463140964508057, 0.04394612833857536]
[2025-05-27 16:00:08,476]: Mean: 0.31903887
[2025-05-27 16:00:08,476]: Min: -0.00000000
[2025-05-27 16:00:08,476]: Max: 0.70988601
[2025-05-27 16:00:08,477]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 16:00:08,518]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,519]: Mean: 0.00000393
[2025-05-27 16:00:08,519]: Min: -0.02607483
[2025-05-27 16:00:08,519]: Max: 0.07170580
[2025-05-27 16:00:08,519]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-27 16:00:08,520]: Sample Values (25 elements): [5.297328584687106e-41, 5.874383292296066e-41, 5.485663098292361e-41, 5.791146163515172e-41, 5.025056293068794e-41, -5.224881454081513e-41, 4.909449169761997e-41, -6.008347425485518e-41, -5.886014069549962e-41, -5.321150658580628e-41, -5.301952869619378e-41, 5.394158308571951e-41, 6.214618559434131e-41, 4.952188772923904e-41, 5.790025124743712e-41, 5.088535113502708e-41, -4.904965014676157e-41, 5.143325883457809e-41, -5.330118968752307e-41, -4.90566566390832e-41, 5.638124371210901e-41, -5.193492368480637e-41, -4.906506442986914e-41, -5.711972800280819e-41, -5.676379819286969e-41]
[2025-05-27 16:00:08,520]: Mean: 0.00152300
[2025-05-27 16:00:08,520]: Min: -0.00000156
[2025-05-27 16:00:08,520]: Max: 0.37657875
[2025-05-27 16:00:08,521]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 16:00:08,565]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 16:00:08,566]: Mean: -0.00000562
[2025-05-27 16:00:08,566]: Min: -0.07029483
[2025-05-27 16:00:08,567]: Max: 0.04686322
[2025-05-27 16:00:08,567]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-27 16:00:08,567]: Sample Values (25 elements): [0.042500972747802734, 0.1724918633699417, 6.156324543318219e-41, 0.01925421692430973, 0.10360048711299896, 0.07471601665019989, -0.004687714856117964, 0.061180371791124344, 0.00863280426710844, 0.08077818155288696, 0.0016508540138602257, -0.0228214543312788, 0.016979441046714783, -1.6324475105022884e-09, 0.02543495036661625, 0.10530657321214676, 4.905525534061887e-41, 0.05806761980056763, 0.06692512333393097, 0.18978816270828247, 0.03926411643624306, 4.2355280394090755e-10, 0.11227138340473175, -0.009250457398593426, -0.018738143146038055]
[2025-05-27 16:00:08,567]: Mean: 0.02959180
[2025-05-27 16:00:08,567]: Min: -0.02368442
[2025-05-27 16:00:08,568]: Max: 0.20963785
[2025-05-27 16:00:08,568]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-27 16:00:08,568]: Sample Values (25 elements): [-0.17631472647190094, -0.002043033018708229, -0.10497825592756271, -0.223561629652977, 0.014208145439624786, -0.0002518550318200141, 0.04535238444805145, 0.05707723647356033, 0.09832889586687088, -0.06483767181634903, -0.10019651055335999, 0.06488662213087082, 0.05349555239081383, -0.13802692294120789, 0.18015240132808685, 2.5085917501263544e-13, -0.03769103065133095, 0.05469808354973793, -0.051179174333810806, -0.04706737771630287, -0.19181275367736816, -0.0548258051276207, -0.12226628512144089, -0.04169868305325508, -0.005936145316809416]
[2025-05-27 16:00:08,568]: Mean: -0.02264254
[2025-05-27 16:00:08,568]: Min: -0.44784826
[2025-05-27 16:00:08,568]: Max: 0.30535132
[2025-05-27 16:00:08,569]: 


QAT of ResNet18 with relu6 down to 3 bits...
[2025-05-27 16:00:08,782]: [ResNet18_relu6_quantized_3_bits] after configure_qat:
[2025-05-27 16:00:08,827]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 16:01:54,396]: [ResNet18_relu6_quantized_3_bits] Epoch: 001 Train Loss: 0.3039 Train Acc: 0.8952 Eval Loss: 0.4253 Eval Acc: 0.8654 (LR: 0.00100000)
[2025-05-27 16:03:45,894]: [ResNet18_relu6_quantized_3_bits] Epoch: 002 Train Loss: 0.3077 Train Acc: 0.8935 Eval Loss: 0.3603 Eval Acc: 0.8791 (LR: 0.00100000)
[2025-05-27 16:05:41,698]: [ResNet18_relu6_quantized_3_bits] Epoch: 003 Train Loss: 0.3142 Train Acc: 0.8902 Eval Loss: 0.5469 Eval Acc: 0.8338 (LR: 0.00100000)
[2025-05-27 16:07:36,576]: [ResNet18_relu6_quantized_3_bits] Epoch: 004 Train Loss: 0.3184 Train Acc: 0.8887 Eval Loss: 0.4399 Eval Acc: 0.8636 (LR: 0.00100000)
[2025-05-27 16:09:24,768]: [ResNet18_relu6_quantized_3_bits] Epoch: 005 Train Loss: 0.3131 Train Acc: 0.8918 Eval Loss: 0.4511 Eval Acc: 0.8622 (LR: 0.00100000)
[2025-05-27 16:11:16,769]: [ResNet18_relu6_quantized_3_bits] Epoch: 006 Train Loss: 0.3186 Train Acc: 0.8895 Eval Loss: 0.4054 Eval Acc: 0.8688 (LR: 0.00100000)
[2025-05-27 16:13:09,342]: [ResNet18_relu6_quantized_3_bits] Epoch: 007 Train Loss: 0.3207 Train Acc: 0.8886 Eval Loss: 0.4272 Eval Acc: 0.8603 (LR: 0.00100000)
[2025-05-27 16:14:57,547]: [ResNet18_relu6_quantized_3_bits] Epoch: 008 Train Loss: 0.3162 Train Acc: 0.8904 Eval Loss: 0.4277 Eval Acc: 0.8629 (LR: 0.00010000)
[2025-05-27 16:16:47,430]: [ResNet18_relu6_quantized_3_bits] Epoch: 009 Train Loss: 0.2253 Train Acc: 0.9231 Eval Loss: 0.2746 Eval Acc: 0.9093 (LR: 0.00010000)
[2025-05-27 16:18:31,448]: [ResNet18_relu6_quantized_3_bits] Epoch: 010 Train Loss: 0.1937 Train Acc: 0.9337 Eval Loss: 0.2686 Eval Acc: 0.9132 (LR: 0.00010000)
[2025-05-27 16:20:19,826]: [ResNet18_relu6_quantized_3_bits] Epoch: 011 Train Loss: 0.1837 Train Acc: 0.9369 Eval Loss: 0.2664 Eval Acc: 0.9163 (LR: 0.00010000)
[2025-05-27 16:22:06,896]: [ResNet18_relu6_quantized_3_bits] Epoch: 012 Train Loss: 0.1756 Train Acc: 0.9390 Eval Loss: 0.2732 Eval Acc: 0.9155 (LR: 0.00010000)
[2025-05-27 16:23:49,483]: [ResNet18_relu6_quantized_3_bits] Epoch: 013 Train Loss: 0.1705 Train Acc: 0.9409 Eval Loss: 0.2711 Eval Acc: 0.9130 (LR: 0.00010000)
[2025-05-27 16:25:34,895]: [ResNet18_relu6_quantized_3_bits] Epoch: 014 Train Loss: 0.1623 Train Acc: 0.9440 Eval Loss: 0.2693 Eval Acc: 0.9158 (LR: 0.00010000)
[2025-05-27 16:27:25,079]: [ResNet18_relu6_quantized_3_bits] Epoch: 015 Train Loss: 0.1601 Train Acc: 0.9440 Eval Loss: 0.2756 Eval Acc: 0.9125 (LR: 0.00010000)
[2025-05-27 16:29:10,924]: [ResNet18_relu6_quantized_3_bits] Epoch: 016 Train Loss: 0.1515 Train Acc: 0.9478 Eval Loss: 0.2816 Eval Acc: 0.9130 (LR: 0.00010000)
[2025-05-27 16:30:56,880]: [ResNet18_relu6_quantized_3_bits] Epoch: 017 Train Loss: 0.1488 Train Acc: 0.9478 Eval Loss: 0.2854 Eval Acc: 0.9130 (LR: 0.00001000)
[2025-05-27 16:32:44,496]: [ResNet18_relu6_quantized_3_bits] Epoch: 018 Train Loss: 0.1349 Train Acc: 0.9536 Eval Loss: 0.2691 Eval Acc: 0.9190 (LR: 0.00001000)
[2025-05-27 16:34:27,338]: [ResNet18_relu6_quantized_3_bits] Epoch: 019 Train Loss: 0.1340 Train Acc: 0.9529 Eval Loss: 0.2632 Eval Acc: 0.9179 (LR: 0.00001000)
[2025-05-27 16:36:13,322]: [ResNet18_relu6_quantized_3_bits] Epoch: 020 Train Loss: 0.1285 Train Acc: 0.9561 Eval Loss: 0.2643 Eval Acc: 0.9183 (LR: 0.00001000)
[2025-05-27 16:38:01,618]: [ResNet18_relu6_quantized_3_bits] Epoch: 021 Train Loss: 0.1261 Train Acc: 0.9555 Eval Loss: 0.2551 Eval Acc: 0.9203 (LR: 0.00001000)
[2025-05-27 16:40:09,722]: [ResNet18_relu6_quantized_3_bits] Epoch: 022 Train Loss: 0.1251 Train Acc: 0.9568 Eval Loss: 0.2663 Eval Acc: 0.9168 (LR: 0.00001000)
[2025-05-27 16:42:27,528]: [ResNet18_relu6_quantized_3_bits] Epoch: 023 Train Loss: 0.1231 Train Acc: 0.9577 Eval Loss: 0.2586 Eval Acc: 0.9210 (LR: 0.00001000)
[2025-05-27 16:44:28,377]: [ResNet18_relu6_quantized_3_bits] Epoch: 024 Train Loss: 0.1240 Train Acc: 0.9572 Eval Loss: 0.2610 Eval Acc: 0.9184 (LR: 0.00001000)
[2025-05-27 16:46:13,106]: [ResNet18_relu6_quantized_3_bits] Epoch: 025 Train Loss: 0.1249 Train Acc: 0.9571 Eval Loss: 0.2588 Eval Acc: 0.9197 (LR: 0.00001000)
[2025-05-27 16:48:00,199]: [ResNet18_relu6_quantized_3_bits] Epoch: 026 Train Loss: 0.1224 Train Acc: 0.9578 Eval Loss: 0.2637 Eval Acc: 0.9213 (LR: 0.00001000)
[2025-05-27 16:49:48,425]: [ResNet18_relu6_quantized_3_bits] Epoch: 027 Train Loss: 0.1214 Train Acc: 0.9583 Eval Loss: 0.2575 Eval Acc: 0.9223 (LR: 0.00000100)
[2025-05-27 16:51:36,289]: [ResNet18_relu6_quantized_3_bits] Epoch: 028 Train Loss: 0.1173 Train Acc: 0.9606 Eval Loss: 0.2594 Eval Acc: 0.9198 (LR: 0.00000100)
[2025-05-27 16:53:26,003]: [ResNet18_relu6_quantized_3_bits] Epoch: 029 Train Loss: 0.1220 Train Acc: 0.9573 Eval Loss: 0.2652 Eval Acc: 0.9204 (LR: 0.00000100)
[2025-05-27 16:55:13,590]: [ResNet18_relu6_quantized_3_bits] Epoch: 030 Train Loss: 0.1209 Train Acc: 0.9575 Eval Loss: 0.2638 Eval Acc: 0.9204 (LR: 0.00000100)
[2025-05-27 16:56:58,738]: [ResNet18_relu6_quantized_3_bits] Epoch: 031 Train Loss: 0.1188 Train Acc: 0.9588 Eval Loss: 0.2610 Eval Acc: 0.9203 (LR: 0.00000100)
[2025-05-27 16:58:47,099]: [ResNet18_relu6_quantized_3_bits] Epoch: 032 Train Loss: 0.1197 Train Acc: 0.9585 Eval Loss: 0.2672 Eval Acc: 0.9183 (LR: 0.00000100)
[2025-05-27 17:00:27,825]: [ResNet18_relu6_quantized_3_bits] Epoch: 033 Train Loss: 0.1221 Train Acc: 0.9580 Eval Loss: 0.2605 Eval Acc: 0.9209 (LR: 0.00000010)
[2025-05-27 17:02:16,341]: [ResNet18_relu6_quantized_3_bits] Epoch: 034 Train Loss: 0.1199 Train Acc: 0.9592 Eval Loss: 0.2626 Eval Acc: 0.9202 (LR: 0.00000010)
[2025-05-27 17:04:12,173]: [ResNet18_relu6_quantized_3_bits] Epoch: 035 Train Loss: 0.1188 Train Acc: 0.9584 Eval Loss: 0.2608 Eval Acc: 0.9189 (LR: 0.00000010)
[2025-05-27 17:05:59,709]: [ResNet18_relu6_quantized_3_bits] Epoch: 036 Train Loss: 0.1177 Train Acc: 0.9594 Eval Loss: 0.2626 Eval Acc: 0.9207 (LR: 0.00000010)
[2025-05-27 17:07:44,522]: [ResNet18_relu6_quantized_3_bits] Epoch: 037 Train Loss: 0.1178 Train Acc: 0.9593 Eval Loss: 0.2657 Eval Acc: 0.9193 (LR: 0.00000010)
[2025-05-27 17:09:38,464]: [ResNet18_relu6_quantized_3_bits] Epoch: 038 Train Loss: 0.1197 Train Acc: 0.9586 Eval Loss: 0.2571 Eval Acc: 0.9203 (LR: 0.00000010)
[2025-05-27 17:11:26,553]: [ResNet18_relu6_quantized_3_bits] Epoch: 039 Train Loss: 0.1204 Train Acc: 0.9580 Eval Loss: 0.2580 Eval Acc: 0.9192 (LR: 0.00000010)
[2025-05-27 17:13:12,017]: [ResNet18_relu6_quantized_3_bits] Epoch: 040 Train Loss: 0.1202 Train Acc: 0.9579 Eval Loss: 0.2626 Eval Acc: 0.9204 (LR: 0.00000010)
[2025-05-27 17:15:06,926]: [ResNet18_relu6_quantized_3_bits] Epoch: 041 Train Loss: 0.1202 Train Acc: 0.9584 Eval Loss: 0.2670 Eval Acc: 0.9187 (LR: 0.00000010)
[2025-05-27 17:17:06,172]: [ResNet18_relu6_quantized_3_bits] Epoch: 042 Train Loss: 0.1190 Train Acc: 0.9593 Eval Loss: 0.2597 Eval Acc: 0.9213 (LR: 0.00000010)
[2025-05-27 17:18:56,489]: [ResNet18_relu6_quantized_3_bits] Epoch: 043 Train Loss: 0.1211 Train Acc: 0.9572 Eval Loss: 0.2631 Eval Acc: 0.9193 (LR: 0.00000010)
[2025-05-27 17:20:42,238]: [ResNet18_relu6_quantized_3_bits] Epoch: 044 Train Loss: 0.1174 Train Acc: 0.9600 Eval Loss: 0.2651 Eval Acc: 0.9198 (LR: 0.00000010)
[2025-05-27 17:22:31,423]: [ResNet18_relu6_quantized_3_bits] Epoch: 045 Train Loss: 0.1174 Train Acc: 0.9604 Eval Loss: 0.2597 Eval Acc: 0.9214 (LR: 0.00000010)
[2025-05-27 17:24:14,785]: [ResNet18_relu6_quantized_3_bits] Epoch: 046 Train Loss: 0.1183 Train Acc: 0.9595 Eval Loss: 0.2603 Eval Acc: 0.9203 (LR: 0.00000010)
[2025-05-27 17:25:59,289]: [ResNet18_relu6_quantized_3_bits] Epoch: 047 Train Loss: 0.1185 Train Acc: 0.9585 Eval Loss: 0.2633 Eval Acc: 0.9209 (LR: 0.00000010)
[2025-05-27 17:27:42,835]: [ResNet18_relu6_quantized_3_bits] Epoch: 048 Train Loss: 0.1183 Train Acc: 0.9582 Eval Loss: 0.2567 Eval Acc: 0.9214 (LR: 0.00000010)
[2025-05-27 17:29:26,173]: [ResNet18_relu6_quantized_3_bits] Epoch: 049 Train Loss: 0.1198 Train Acc: 0.9579 Eval Loss: 0.2610 Eval Acc: 0.9189 (LR: 0.00000010)
[2025-05-27 17:31:13,917]: [ResNet18_relu6_quantized_3_bits] Epoch: 050 Train Loss: 0.1192 Train Acc: 0.9587 Eval Loss: 0.2626 Eval Acc: 0.9183 (LR: 0.00000010)
[2025-05-27 17:33:01,907]: [ResNet18_relu6_quantized_3_bits] Epoch: 051 Train Loss: 0.1184 Train Acc: 0.9594 Eval Loss: 0.2572 Eval Acc: 0.9194 (LR: 0.00000010)
[2025-05-27 17:34:46,940]: [ResNet18_relu6_quantized_3_bits] Epoch: 052 Train Loss: 0.1159 Train Acc: 0.9604 Eval Loss: 0.2533 Eval Acc: 0.9220 (LR: 0.00000010)
[2025-05-27 17:36:32,480]: [ResNet18_relu6_quantized_3_bits] Epoch: 053 Train Loss: 0.1201 Train Acc: 0.9582 Eval Loss: 0.2618 Eval Acc: 0.9196 (LR: 0.00000010)
[2025-05-27 17:38:15,750]: [ResNet18_relu6_quantized_3_bits] Epoch: 054 Train Loss: 0.1205 Train Acc: 0.9579 Eval Loss: 0.2596 Eval Acc: 0.9204 (LR: 0.00000010)
[2025-05-27 17:40:14,039]: [ResNet18_relu6_quantized_3_bits] Epoch: 055 Train Loss: 0.1170 Train Acc: 0.9595 Eval Loss: 0.2621 Eval Acc: 0.9196 (LR: 0.00000010)
[2025-05-27 17:42:07,123]: [ResNet18_relu6_quantized_3_bits] Epoch: 056 Train Loss: 0.1171 Train Acc: 0.9598 Eval Loss: 0.2632 Eval Acc: 0.9196 (LR: 0.00000010)
[2025-05-27 17:43:50,183]: [ResNet18_relu6_quantized_3_bits] Epoch: 057 Train Loss: 0.1168 Train Acc: 0.9597 Eval Loss: 0.2571 Eval Acc: 0.9203 (LR: 0.00000010)
[2025-05-27 17:45:31,565]: [ResNet18_relu6_quantized_3_bits] Epoch: 058 Train Loss: 0.1174 Train Acc: 0.9595 Eval Loss: 0.2647 Eval Acc: 0.9196 (LR: 0.00000010)
[2025-05-27 17:47:18,330]: [ResNet18_relu6_quantized_3_bits] Epoch: 059 Train Loss: 0.1169 Train Acc: 0.9599 Eval Loss: 0.2612 Eval Acc: 0.9214 (LR: 0.00000010)
[2025-05-27 17:49:04,905]: [ResNet18_relu6_quantized_3_bits] Epoch: 060 Train Loss: 0.1201 Train Acc: 0.9571 Eval Loss: 0.2619 Eval Acc: 0.9188 (LR: 0.00000010)
[2025-05-27 17:49:04,906]: [ResNet18_relu6_quantized_3_bits] Best Eval Accuracy: 0.9223
[2025-05-27 17:49:05,003]: 


Quantization of model down to 3 bits finished
[2025-05-27 17:49:05,003]: Model Architecture:
[2025-05-27 17:49:05,058]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0939], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33595138788223267, max_val=0.32117730379104614)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0980], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3089427351951599, max_val=0.37677162885665894)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1108], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.44399014115333557, max_val=0.33194291591644287)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8366], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.856497287750244)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1013], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32876044511795044, max_val=0.380487859249115)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0803], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27460211515426636, max_val=0.2873728573322296)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7251], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.0756001472473145)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0776], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2609555721282959, max_val=0.28227442502975464)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1380], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45522087812423706, max_val=0.5110063552856445)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0837], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2675233483314514, max_val=0.3185708522796631)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7937], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.555564880371094)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0679], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2272660732269287, max_val=0.24793782830238342)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0796], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2576743960380554, max_val=0.2992161512374878)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8194], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7357611656188965)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0783], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25759661197662354, max_val=0.29084551334381104)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0790], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26301896572113037, max_val=0.290194571018219)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0810], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2502124309539795, max_val=0.31674689054489136)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7985], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.589432716369629)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0652], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22422654926776886, max_val=0.2318539321422577)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0652], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19209259748458862, max_val=0.26422321796417236)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0429], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1320919692516327, max_val=0.16803328692913055)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0626], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16526219248771667, max_val=0.2728089690208435)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.045811355113983154, max_val=0.07474873214960098)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4151], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.90582013130188)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0207], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07557936757802963, max_val=0.06902262568473816)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 17:49:05,058]: 
Model Weights:
[2025-05-27 17:49:05,058]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-27 17:49:05,059]: Sample Values (25 elements): [-0.17063608765602112, 0.1651380956172943, 0.2048325389623642, 0.03886963054537773, 0.03066527098417282, 0.10797705501317978, -0.05001102387905121, -0.13363634049892426, 0.19387705624103546, -0.08677051216363907, -0.04124189913272858, -0.08071354031562805, -0.025944558903574944, -0.01752897910773754, -0.05472664162516594, -0.11662919074296951, -0.0039380923844873905, -0.1596565544605255, 0.012605411931872368, -0.015298578888177872, -0.2099004089832306, -0.2463982254266739, -0.24876977503299713, -0.10593249648809433, 0.10480520129203796]
[2025-05-27 17:49:05,059]: Mean: -0.00186380
[2025-05-27 17:49:05,059]: Min: -0.34273311
[2025-05-27 17:49:05,059]: Max: 0.31400433
[2025-05-27 17:49:05,060]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-27 17:49:05,060]: Sample Values (25 elements): [0.754741907119751, 0.5615066885948181, 0.631873607635498, 0.5837451815605164, 0.49199047684669495, 1.2744951248168945, 1.158279538154602, 0.7582496404647827, 0.6973824501037598, 0.8457390666007996, 0.8659592270851135, 0.7952885031700134, 0.8999260663986206, 0.6763700842857361, 0.6228464841842651, 0.4606114625930786, 1.0620863437652588, 0.6999692320823669, 0.7997149229049683, 0.7155658006668091, 0.9175727367401123, 0.6146839261054993, 0.9259119629859924, 1.0720505714416504, 0.5676751136779785]
[2025-05-27 17:49:05,060]: Mean: 0.82714528
[2025-05-27 17:49:05,060]: Min: 0.46061146
[2025-05-27 17:49:05,060]: Max: 1.61947680
[2025-05-27 17:49:05,061]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 17:49:05,062]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.09387553483247757, 0.0, -0.09387553483247757, -0.09387553483247757, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09387553483247757, 0.09387553483247757, 0.0, -0.09387553483247757, 0.09387553483247757, 0.0, -0.09387553483247757, 0.0, 0.0, 0.09387553483247757, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,062]: Mean: -0.00692149
[2025-05-27 17:49:05,062]: Min: -0.37550214
[2025-05-27 17:49:05,062]: Max: 0.28162661
[2025-05-27 17:49:05,062]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 17:49:05,063]: Sample Values (25 elements): [0.7874197959899902, 0.6532052159309387, 0.6801568269729614, 0.7233064770698547, 0.7498287558555603, 0.7950036525726318, 0.5644538998603821, 1.1771128177642822, 0.635408878326416, 1.0078092813491821, 0.6846613883972168, 1.1322929859161377, 0.7836808562278748, 0.5980484485626221, 0.6372424960136414, 0.7510757446289062, 0.7442644238471985, 0.747681736946106, 0.5677657127380371, 0.8496753573417664, 0.7945612072944641, 0.9406713843345642, 0.6941591501235962, 0.7060601115226746, 0.5794976949691772]
[2025-05-27 17:49:05,063]: Mean: 0.76369464
[2025-05-27 17:49:05,063]: Min: 0.51129180
[2025-05-27 17:49:05,063]: Max: 1.17711282
[2025-05-27 17:49:05,064]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 17:49:05,065]: Sample Values (25 elements): [-0.09795919805765152, 0.0, 0.0, 0.09795919805765152, 0.0, 0.09795919805765152, 0.0, 0.0, -0.09795919805765152, -0.09795919805765152, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09795919805765152, -0.09795919805765152, 0.0, 0.09795919805765152, 0.0, 0.09795919805765152, 0.0, 0.09795919805765152, 0.0, 0.0]
[2025-05-27 17:49:05,065]: Mean: -0.00210725
[2025-05-27 17:49:05,065]: Min: -0.29387760
[2025-05-27 17:49:05,065]: Max: 0.39183679
[2025-05-27 17:49:05,065]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 17:49:05,066]: Sample Values (25 elements): [0.9595305919647217, 1.508913516998291, 0.8921385407447815, 0.726356565952301, 0.6911978125572205, 0.7136530876159668, 0.4956219792366028, 1.039525032043457, 0.9277894496917725, 1.1912442445755005, 0.7585070133209229, 0.6957583427429199, 0.7100241780281067, 0.8687964677810669, 1.241828203201294, 0.7307750582695007, 0.8075615763664246, 0.856829047203064, 0.8681715130805969, 0.9270989298820496, 0.9393047094345093, 0.5638542175292969, 1.1521587371826172, 0.9680003523826599, 0.504620373249054]
[2025-05-27 17:49:05,066]: Mean: 0.81924939
[2025-05-27 17:49:05,066]: Min: 0.44286093
[2025-05-27 17:49:05,066]: Max: 1.50891352
[2025-05-27 17:49:05,067]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 17:49:05,068]: Sample Values (25 elements): [0.0, 0.1108475774526596, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1108475774526596, 0.1108475774526596, 0.0, -0.2216951549053192, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1108475774526596, 0.0, -0.1108475774526596, 0.0, 0.0, 0.0, 0.0, 0.1108475774526596, -0.1108475774526596]
[2025-05-27 17:49:05,068]: Mean: -0.00236646
[2025-05-27 17:49:05,068]: Min: -0.44339031
[2025-05-27 17:49:05,068]: Max: 0.33254272
[2025-05-27 17:49:05,068]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 17:49:05,068]: Sample Values (25 elements): [0.9736003875732422, 0.5983184576034546, 0.7749466896057129, 0.6429054141044617, 0.5001525282859802, 0.4139922857284546, 0.5489475131034851, 0.809497058391571, 1.1842931509017944, 0.7335014343261719, 0.9862984418869019, 0.6769051551818848, 0.41961514949798584, 0.441683292388916, 0.70333331823349, 0.6889384984970093, 0.697800874710083, 0.6401941776275635, 0.6252050399780273, 0.44494420289993286, 0.4614362120628357, 1.1590192317962646, 1.1703249216079712, 0.9497491717338562, 0.4876863956451416]
[2025-05-27 17:49:05,068]: Mean: 0.68960816
[2025-05-27 17:49:05,069]: Min: 0.26524234
[2025-05-27 17:49:05,069]: Max: 1.18429315
[2025-05-27 17:49:05,070]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 17:49:05,070]: Sample Values (25 elements): [-0.10132119059562683, -0.10132119059562683, 0.0, 0.0, 0.0, 0.10132119059562683, 0.0, 0.0, 0.0, 0.0, 0.10132119059562683, 0.0, 0.10132119059562683, 0.0, 0.0, -0.10132119059562683, 0.0, -0.10132119059562683, 0.0, 0.0, 0.0, 0.0, 0.10132119059562683, 0.0, -0.10132119059562683]
[2025-05-27 17:49:05,070]: Mean: -0.00226477
[2025-05-27 17:49:05,071]: Min: -0.30396357
[2025-05-27 17:49:05,071]: Max: 0.40528476
[2025-05-27 17:49:05,071]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 17:49:05,071]: Sample Values (25 elements): [0.7776830196380615, 0.6763738393783569, 0.9153616428375244, 0.8456394076347351, 0.8467671275138855, 0.7703157663345337, 0.5411176681518555, 0.9881381392478943, 1.0127590894699097, 0.590538501739502, 0.6272361874580383, 0.8293107748031616, 0.6121861934661865, 0.8134267330169678, 0.8864133358001709, 0.6955010890960693, 0.6624346971511841, 0.6071628928184509, 0.4399063289165497, 0.4495501220226288, 1.091224193572998, 0.8691635727882385, 0.8944627046585083, 0.6280282139778137, 0.6987965106964111]
[2025-05-27 17:49:05,071]: Mean: 0.74652255
[2025-05-27 17:49:05,071]: Min: 0.32810706
[2025-05-27 17:49:05,072]: Max: 1.15279424
[2025-05-27 17:49:05,072]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-27 17:49:05,073]: Sample Values (25 elements): [0.08028213679790497, 0.0, 0.0, 0.0, -0.08028213679790497, 0.0, 0.0, 0.0, 0.08028213679790497, -0.08028213679790497, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08028213679790497, 0.0, -0.08028213679790497, -0.08028213679790497, 0.0, 0.0, 0.0, 0.0, 0.08028213679790497, -0.08028213679790497]
[2025-05-27 17:49:05,074]: Mean: -0.00173897
[2025-05-27 17:49:05,074]: Min: -0.24084641
[2025-05-27 17:49:05,074]: Max: 0.32112855
[2025-05-27 17:49:05,074]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-27 17:49:05,074]: Sample Values (25 elements): [0.5156946182250977, 0.7825762033462524, 5.061357569502434e-06, 0.4099437892436981, 0.4671713709831238, 0.6576676368713379, 0.5365430116653442, 0.5479763150215149, 0.8865805864334106, 0.6934369206428528, 0.011836332269012928, 0.500243067741394, 0.7080878019332886, 0.6066737174987793, 0.5079890489578247, 0.8121736645698547, 0.6623241901397705, 0.6890404224395752, 0.7451404929161072, 0.497275173664093, 0.6104286909103394, 0.4343116283416748, 0.22683118283748627, 0.8339296579360962, 0.5543916821479797]
[2025-05-27 17:49:05,074]: Mean: 0.58811349
[2025-05-27 17:49:05,075]: Min: 0.00000506
[2025-05-27 17:49:05,075]: Max: 0.91845387
[2025-05-27 17:49:05,076]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 17:49:05,077]: Sample Values (25 elements): [-0.07760428637266159, 0.0, 0.0, 0.0, -0.07760428637266159, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07760428637266159, 0.0, 0.0, 0.0, 0.0, 0.07760428637266159, 0.0, 0.0, 0.0, 0.0, -0.07760428637266159, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,078]: Mean: -0.00254092
[2025-05-27 17:49:05,078]: Min: -0.23281285
[2025-05-27 17:49:05,078]: Max: 0.31041715
[2025-05-27 17:49:05,078]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-27 17:49:05,078]: Sample Values (25 elements): [0.7113337516784668, 0.8514078259468079, 0.7030478119850159, 0.7232099771499634, 0.8926319479942322, 0.560886561870575, 0.5642059445381165, 0.6576614379882812, 0.6721683144569397, 0.7028259634971619, 0.37164297699928284, 0.610231339931488, 0.7703970074653625, 0.7642688155174255, 0.7835748195648193, 0.47911491990089417, 0.6826509833335876, 0.9192652702331543, 0.7756772041320801, 0.5646544098854065, 0.7903377413749695, 0.8753728270530701, 0.7433437705039978, 0.9375265836715698, 0.512169361114502]
[2025-05-27 17:49:05,078]: Mean: 0.69461423
[2025-05-27 17:49:05,079]: Min: 0.30446842
[2025-05-27 17:49:05,079]: Max: 1.08558953
[2025-05-27 17:49:05,080]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-27 17:49:05,080]: Sample Values (25 elements): [-0.138032466173172, 0.0, -0.138032466173172, 0.0, -0.138032466173172, 0.0, 0.0, 0.0, 0.0, -0.138032466173172, 0.138032466173172, 0.0, 0.138032466173172, 0.0, -0.138032466173172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.138032466173172, 0.0, -0.138032466173172]
[2025-05-27 17:49:05,081]: Mean: -0.00475161
[2025-05-27 17:49:05,081]: Min: -0.41409740
[2025-05-27 17:49:05,081]: Max: 0.55212986
[2025-05-27 17:49:05,081]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-27 17:49:05,081]: Sample Values (25 elements): [0.5109323859214783, 0.8002869486808777, 0.4111100435256958, 0.6323513984680176, 0.6261032223701477, 0.8481482863426208, 0.45273396372795105, 0.7145776152610779, 0.3799495995044708, 0.6551271677017212, 0.5474024415016174, 0.5373910665512085, 0.3381178677082062, 0.555569052696228, 0.6314454674720764, 0.40228432416915894, 0.6534388065338135, 0.6684435606002808, 0.574079692363739, 0.6873719692230225, 0.5990157723426819, 0.4887157380580902, 0.38442373275756836, 0.5044321417808533, 0.569993793964386]
[2025-05-27 17:49:05,081]: Mean: 0.54061007
[2025-05-27 17:49:05,082]: Min: 0.18638098
[2025-05-27 17:49:05,082]: Max: 0.85542893
[2025-05-27 17:49:05,083]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 17:49:05,084]: Sample Values (25 elements): [0.0, 0.08372773975133896, 0.0, 0.08372773975133896, 0.0, 0.0, -0.08372773975133896, 0.0, -0.08372773975133896, 0.08372773975133896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08372773975133896, 0.0, 0.0, 0.08372773975133896, 0.0, 0.0, -0.08372773975133896, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,084]: Mean: -0.00207991
[2025-05-27 17:49:05,085]: Min: -0.25118321
[2025-05-27 17:49:05,085]: Max: 0.33491096
[2025-05-27 17:49:05,085]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-27 17:49:05,085]: Sample Values (25 elements): [0.531173050403595, 0.33453240990638733, 0.47500357031822205, 0.6489299535751343, 0.5790669322013855, 0.33841419219970703, 0.6494565606117249, 0.6391116380691528, 0.761201024055481, 0.6650115847587585, 0.582763671875, 0.6059094667434692, 0.3447433412075043, 0.7270832061767578, 0.28765231370925903, 0.7551780343055725, 0.5752369165420532, 0.7027612924575806, 0.8738847374916077, 0.26120200753211975, 0.7349768280982971, 0.6686574220657349, 0.6864493489265442, 0.5904210209846497, 0.7671747207641602]
[2025-05-27 17:49:05,085]: Mean: 0.55011868
[2025-05-27 17:49:05,085]: Min: 0.00000000
[2025-05-27 17:49:05,085]: Max: 0.94048494
[2025-05-27 17:49:05,086]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 17:49:05,088]: Sample Values (25 elements): [0.0, -0.06788627058267593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06788627058267593, 0.0, -0.06788627058267593, 0.0, 0.06788627058267593, 0.0, 0.06788627058267593, -0.06788627058267593, -0.06788627058267593, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,088]: Mean: -0.00234013
[2025-05-27 17:49:05,088]: Min: -0.20365882
[2025-05-27 17:49:05,088]: Max: 0.27154508
[2025-05-27 17:49:05,088]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-27 17:49:05,089]: Sample Values (25 elements): [0.6535186767578125, 0.48945870995521545, 0.7669805884361267, 0.648073136806488, 0.5144288539886475, 0.6956627368927002, 0.5689818859100342, 0.5484133958816528, 0.5485122203826904, 0.5739654302597046, 0.640381932258606, 0.49688053131103516, 0.513622522354126, 0.7242077589035034, 0.38107728958129883, 0.7208435535430908, 0.7446906566619873, 0.6144481301307678, 0.715559184551239, 0.36813291907310486, 0.4098537862300873, 0.6644830703735352, 0.5059413313865662, 0.49728941917419434, 0.45167475938796997]
[2025-05-27 17:49:05,089]: Mean: 0.56425542
[2025-05-27 17:49:05,089]: Min: 0.24533476
[2025-05-27 17:49:05,089]: Max: 0.88385075
[2025-05-27 17:49:05,090]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-27 17:49:05,093]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07955579459667206, 0.0, 0.0, 0.0, 0.0, -0.07955579459667206, 0.0, -0.07955579459667206, 0.07955579459667206, 0.0, 0.07955579459667206, 0.07955579459667206, 0.0, 0.0]
[2025-05-27 17:49:05,093]: Mean: -0.00043351
[2025-05-27 17:49:05,093]: Min: -0.23866738
[2025-05-27 17:49:05,093]: Max: 0.31822318
[2025-05-27 17:49:05,093]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-27 17:49:05,094]: Sample Values (25 elements): [0.6300524473190308, 0.7606680989265442, 0.5104434490203857, 0.5542281270027161, 0.6054280996322632, 0.7590658068656921, 0.6770700216293335, 0.7132224440574646, 4.916876051622918e-41, 0.2433391958475113, 0.014191970229148865, 0.15550103783607483, 0.6939408779144287, 0.6868305206298828, 0.44955381751060486, 0.48597100377082825, 0.2596467435359955, 0.4335273206233978, 0.2689882516860962, 0.5933162569999695, 0.7089958786964417, 0.43487438559532166, 0.5344001054763794, 0.3450165092945099, 0.475960373878479]
[2025-05-27 17:49:05,094]: Mean: 0.42864299
[2025-05-27 17:49:05,094]: Min: -0.00000000
[2025-05-27 17:49:05,094]: Max: 0.91285461
[2025-05-27 17:49:05,095]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 17:49:05,101]: Sample Values (25 elements): [0.0, 0.0, 0.07834887504577637, 0.0, 0.0, 0.0, 0.15669775009155273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07834887504577637, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,102]: Mean: -0.00053572
[2025-05-27 17:49:05,102]: Min: -0.23504663
[2025-05-27 17:49:05,102]: Max: 0.31339550
[2025-05-27 17:49:05,102]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-27 17:49:05,102]: Sample Values (25 elements): [0.5104647874832153, 0.5466843247413635, 0.7253471612930298, 0.7903521656990051, 0.3965405523777008, 0.5925276279449463, 0.2717444896697998, 0.018752330914139748, 0.49221664667129517, 0.4375883936882019, 0.6791710257530212, 0.2158784419298172, 0.2518261671066284, 0.47609227895736694, 0.6047069430351257, 0.6074526309967041, 0.7185880541801453, 0.605211615562439, 0.5719642639160156, 0.7156857848167419, 0.5618204474449158, 0.46290841698646545, 0.6222133040428162, 0.4793563485145569, 0.8216683864593506]
[2025-05-27 17:49:05,102]: Mean: 0.46401709
[2025-05-27 17:49:05,103]: Min: -0.00000000
[2025-05-27 17:49:05,103]: Max: 0.92920840
[2025-05-27 17:49:05,104]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-27 17:49:05,104]: Sample Values (25 elements): [0.0, 0.0, -0.07903051376342773, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07903051376342773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07903051376342773, 0.0, 0.0, 0.07903051376342773, -0.07903051376342773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07903051376342773]
[2025-05-27 17:49:05,104]: Mean: -0.00438469
[2025-05-27 17:49:05,104]: Min: -0.23709154
[2025-05-27 17:49:05,105]: Max: 0.31612206
[2025-05-27 17:49:05,105]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-27 17:49:05,105]: Sample Values (25 elements): [0.39399558305740356, 0.3739537298679352, 0.411722868680954, 0.3262738585472107, 0.41093385219573975, 0.2863900661468506, 0.4548405110836029, 0.6407217979431152, 5.567639058455363e-41, 0.2864786386489868, 0.47927895188331604, 0.2555030584335327, 0.24637888371944427, 0.3049280047416687, 0.4438757002353668, 0.25069087743759155, 0.39815303683280945, 0.012326010502874851, 0.40616941452026367, 0.40987876057624817, 0.3603392243385315, 0.44627830386161804, 0.46835893392562866, 0.29130467772483826, 0.2799695134162903]
[2025-05-27 17:49:05,105]: Mean: 0.31216925
[2025-05-27 17:49:05,105]: Min: -0.00000000
[2025-05-27 17:49:05,105]: Max: 0.64072180
[2025-05-27 17:49:05,107]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 17:49:05,112]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08099418878555298, 0.08099418878555298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08099418878555298, -0.08099418878555298, 0.0, 0.0, 0.08099418878555298]
[2025-05-27 17:49:05,112]: Mean: -0.00062467
[2025-05-27 17:49:05,113]: Min: -0.24298257
[2025-05-27 17:49:05,113]: Max: 0.32397676
[2025-05-27 17:49:05,113]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-27 17:49:05,113]: Sample Values (25 elements): [0.4517788589000702, 0.38527482748031616, 0.25277554988861084, 0.42189010977745056, 5.542976205483246e-41, -5.050559925119506e-41, 0.43057215213775635, 0.4261796474456787, 0.6506854295730591, 0.6272487640380859, 0.7287744879722595, 4.930889036266166e-41, 4.920239167937298e-41, -6.024462357825254e-41, 0.3725375533103943, 0.5701751708984375, -5.029960837693931e-41, 5.202460678652316e-41, 0.3425823152065277, -4.910289948840591e-41, 0.21792912483215332, -4.906086053447617e-41, 0.20746691524982452, -4.973208249888776e-41, 5.878306927996175e-41]
[2025-05-27 17:49:05,113]: Mean: 0.26314309
[2025-05-27 17:49:05,114]: Min: -0.00000000
[2025-05-27 17:49:05,114]: Max: 0.78699893
[2025-05-27 17:49:05,115]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 17:49:05,120]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06515435874462128, 0.0, 0.0]
[2025-05-27 17:49:05,120]: Mean: -0.00062280
[2025-05-27 17:49:05,120]: Min: -0.19546308
[2025-05-27 17:49:05,120]: Max: 0.26061743
[2025-05-27 17:49:05,120]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-27 17:49:05,121]: Sample Values (25 elements): [0.5157862305641174, 0.6050410270690918, 0.42386847734451294, 0.39271870255470276, 0.34229204058647156, 0.6175811886787415, 0.5545742511749268, 0.4235764145851135, -1.0054824997496326e-05, 0.36804094910621643, 0.6545472741127014, 0.6232318878173828, 0.3804774284362793, 0.4756050109863281, 4.916315532237188e-41, 0.12010830640792847, 5.413776487072498e-41, 0.23315387964248657, 0.5969918370246887, 0.1591622680425644, 0.44369104504585266, -6.078412348701759e-41, 0.37284478545188904, 0.11229779571294785, 0.1839611977338791]
[2025-05-27 17:49:05,121]: Mean: 0.29228565
[2025-05-27 17:49:05,121]: Min: -0.02769608
[2025-05-27 17:49:05,121]: Max: 0.89768577
[2025-05-27 17:49:05,122]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-27 17:49:05,133]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,133]: Mean: -0.00001426
[2025-05-27 17:49:05,133]: Min: -0.19556393
[2025-05-27 17:49:05,134]: Max: 0.26075190
[2025-05-27 17:49:05,134]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-27 17:49:05,134]: Sample Values (25 elements): [4.912952415922809e-41, 5.782458113036358e-41, -5.628735671499925e-41, 5.413636357226066e-41, 5.573244252312662e-41, 0.8680577278137207, 0.21193253993988037, 5.730890329549204e-41, 5.146829129618621e-41, -5.674698261129779e-41, 0.0005683065974153578, -5.164485490269113e-41, 5.830662780209131e-41, 0.31325531005859375, 5.336004422302471e-41, -4.905245274369022e-41, -5.410693630450984e-41, 4.931869945191194e-41, 5.461700894552407e-41, -5.20946717097394e-41, -5.51270815865383e-41, 5.522377118057672e-41, 5.125669522807316e-41, -4.991004740385701e-41, 4.94462176121655e-41]
[2025-05-27 17:49:05,134]: Mean: 0.06600168
[2025-05-27 17:49:05,134]: Min: -0.00000000
[2025-05-27 17:49:05,134]: Max: 1.01055419
[2025-05-27 17:49:05,136]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 17:49:05,174]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,174]: Mean: 0.00008610
[2025-05-27 17:49:05,175]: Min: -0.12862511
[2025-05-27 17:49:05,175]: Max: 0.17150015
[2025-05-27 17:49:05,175]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-27 17:49:05,175]: Sample Values (25 elements): [5.412235058761741e-41, 0.41195303201675415, 0.37426015734672546, 0.24363242089748383, 0.4018281400203705, 5.93558020511864e-08, 0.5214642882347107, 0.4486863315105438, 0.3635713756084442, 0.5835824608802795, 0.10008977353572845, 0.15233509242534637, 4.905525534061887e-41, 0.09936065226793289, 0.3506505489349365, 0.4294752776622772, 0.12829376757144928, 2.3445590087434187e-13, 5.157058608408192e-41, 0.2482789307832718, 0.5321148037910461, 0.49388086795806885, 0.14326724410057068, 0.6546303629875183, 0.26267385482788086]
[2025-05-27 17:49:05,175]: Mean: 0.30355442
[2025-05-27 17:49:05,176]: Min: -0.01204783
[2025-05-27 17:49:05,176]: Max: 0.81622458
[2025-05-27 17:49:05,177]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-27 17:49:05,178]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06258159875869751, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06258159875869751, -0.06258159875869751]
[2025-05-27 17:49:05,178]: Mean: 0.00147392
[2025-05-27 17:49:05,178]: Min: -0.18774480
[2025-05-27 17:49:05,179]: Max: 0.25032640
[2025-05-27 17:49:05,179]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-27 17:49:05,179]: Sample Values (25 elements): [0.35737618803977966, 0.5198308825492859, 2.108546505041886e-05, 0.29852718114852905, 4.926124621487462e-41, 0.41858619451522827, 0.40494996309280396, 5.600429442520564e-41, 0.2953614592552185, 0.24910569190979004, 0.4069896638393402, 0.5950964093208313, -5.080267452563192e-41, 0.6177437901496887, 0.5060250759124756, 0.42192021012306213, 0.5248121023178101, 0.4887874722480774, 0.46563097834587097, 5.584875029566558e-41, 8.571420949010644e-06, 0.16663041710853577, 0.5916174650192261, 0.23411774635314941, 9.479590029615181e-11]
[2025-05-27 17:49:05,179]: Mean: 0.34898478
[2025-05-27 17:49:05,179]: Min: -0.00000000
[2025-05-27 17:49:05,179]: Max: 0.73064125
[2025-05-27 17:49:05,180]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 17:49:05,213]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,213]: Mean: 0.00000748
[2025-05-27 17:49:05,213]: Min: -0.05166860
[2025-05-27 17:49:05,214]: Max: 0.06889147
[2025-05-27 17:49:05,214]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-27 17:49:05,215]: Sample Values (25 elements): [5.171211722897872e-41, 5.581792172945044e-41, -5.647232811229013e-41, 4.917997090394378e-41, 5.172472891515765e-41, -5.127631340657371e-41, -5.379865064235838e-41, -4.904684754983292e-41, -4.90566566390832e-41, 5.479637514895765e-41, 5.676660078979834e-41, 6.074068323462352e-41, -5.524338935907726e-41, 6.276415821710856e-41, -5.789044215818684e-41, -5.120204458796449e-41, 5.481178943206522e-41, -5.537090751933082e-41, 6.262823226606905e-41, 5.713374098745144e-41, 5.193772628173502e-41, 0.4710138440132141, -5.101987578760226e-41, -4.926264751333894e-41, -5.905071728664779e-41]
[2025-05-27 17:49:05,215]: Mean: 0.00205362
[2025-05-27 17:49:05,216]: Min: -0.04530855
[2025-05-27 17:49:05,216]: Max: 0.47101384
[2025-05-27 17:49:05,217]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 17:49:05,251]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 17:49:05,251]: Mean: -0.00000780
[2025-05-27 17:49:05,251]: Min: -0.08262970
[2025-05-27 17:49:05,252]: Max: 0.06197228
[2025-05-27 17:49:05,252]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-27 17:49:05,252]: Sample Values (25 elements): [0.06842683255672455, 0.03741142898797989, 0.04146920144557953, -0.0025188445579260588, 0.16955305635929108, 2.7464956531275675e-08, -6.206350898494615e-41, 0.03762926161289215, 0.07804951071739197, 0.010784349404275417, 0.027442973107099533, 4.99997305055738e-41, 0.1752777248620987, 0.10148509591817856, 0.0037765041925013065, 0.02912881225347519, 0.20507405698299408, -4.136663847020827e-05, 0.10641515254974365, 0.033758629113435745, 0.05495952442288399, 0.10232307016849518, 5.884472641239204e-41, 0.07625583559274673, 0.04536458104848862]
[2025-05-27 17:49:05,252]: Mean: 0.05121492
[2025-05-27 17:49:05,252]: Min: -0.04790470
[2025-05-27 17:49:05,252]: Max: 0.25570089
[2025-05-27 17:49:05,253]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-27 17:49:05,253]: Sample Values (25 elements): [0.06278032064437866, -4.910850468226321e-41, 0.0978020429611206, 0.07003413140773773, 0.07696013897657394, 0.049841880798339844, 0.07746091485023499, 0.1196444183588028, -0.09225361794233322, 5.612900998853055e-41, -0.09894458949565887, 0.057725727558135986, -0.17398013174533844, -0.0001544939586892724, -0.06131565198302269, -0.08576202392578125, -0.10504412651062012, 0.036615122109651566, -0.14786487817764282, -0.14596670866012573, -0.07238398492336273, 0.10059454292058945, 0.11793026328086853, -0.24021664261817932, 0.001169967814348638]
[2025-05-27 17:49:05,253]: Mean: -0.02361778
[2025-05-27 17:49:05,253]: Min: -0.46068877
[2025-05-27 17:49:05,253]: Max: 0.28793526
[2025-05-27 17:49:05,253]: 


QAT of ResNet18 with relu6 down to 2 bits...
[2025-05-27 17:49:05,466]: [ResNet18_relu6_quantized_2_bits] after configure_qat:
[2025-05-27 17:49:05,512]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 17:50:53,984]: [ResNet18_relu6_quantized_2_bits] Epoch: 001 Train Loss: 1.3959 Train Acc: 0.5036 Eval Loss: 0.8204 Eval Acc: 0.7242 (LR: 0.00100000)
[2025-05-27 17:52:42,941]: [ResNet18_relu6_quantized_2_bits] Epoch: 002 Train Loss: 0.7345 Train Acc: 0.7452 Eval Loss: 0.7717 Eval Acc: 0.7454 (LR: 0.00100000)
[2025-05-27 17:54:44,057]: [ResNet18_relu6_quantized_2_bits] Epoch: 003 Train Loss: 0.6312 Train Acc: 0.7809 Eval Loss: 0.8072 Eval Acc: 0.7360 (LR: 0.00100000)
[2025-05-27 17:56:35,541]: [ResNet18_relu6_quantized_2_bits] Epoch: 004 Train Loss: 0.6011 Train Acc: 0.7912 Eval Loss: 0.6342 Eval Acc: 0.7882 (LR: 0.00100000)
[2025-05-27 17:58:27,697]: [ResNet18_relu6_quantized_2_bits] Epoch: 005 Train Loss: 0.5749 Train Acc: 0.8016 Eval Loss: 0.7093 Eval Acc: 0.7712 (LR: 0.00100000)
[2025-05-27 18:00:14,752]: [ResNet18_relu6_quantized_2_bits] Epoch: 006 Train Loss: 0.5756 Train Acc: 0.8019 Eval Loss: 0.6821 Eval Acc: 0.7867 (LR: 0.00100000)
[2025-05-27 18:01:59,532]: [ResNet18_relu6_quantized_2_bits] Epoch: 007 Train Loss: 0.5658 Train Acc: 0.8055 Eval Loss: 0.7716 Eval Acc: 0.7423 (LR: 0.00100000)
[2025-05-27 18:03:44,378]: [ResNet18_relu6_quantized_2_bits] Epoch: 008 Train Loss: 0.5610 Train Acc: 0.8053 Eval Loss: 0.6178 Eval Acc: 0.7919 (LR: 0.00100000)
[2025-05-27 18:05:27,750]: [ResNet18_relu6_quantized_2_bits] Epoch: 009 Train Loss: 0.5568 Train Acc: 0.8063 Eval Loss: 0.6059 Eval Acc: 0.8005 (LR: 0.00100000)
[2025-05-27 18:07:11,656]: [ResNet18_relu6_quantized_2_bits] Epoch: 010 Train Loss: 0.5529 Train Acc: 0.8083 Eval Loss: 0.7950 Eval Acc: 0.7411 (LR: 0.00100000)
[2025-05-27 18:09:01,979]: [ResNet18_relu6_quantized_2_bits] Epoch: 011 Train Loss: 0.5445 Train Acc: 0.8127 Eval Loss: 0.7818 Eval Acc: 0.7491 (LR: 0.00100000)
[2025-05-27 18:11:01,302]: [ResNet18_relu6_quantized_2_bits] Epoch: 012 Train Loss: 0.5527 Train Acc: 0.8098 Eval Loss: 0.6459 Eval Acc: 0.7886 (LR: 0.00100000)
[2025-05-27 18:12:56,396]: [ResNet18_relu6_quantized_2_bits] Epoch: 013 Train Loss: 0.5411 Train Acc: 0.8123 Eval Loss: 0.7222 Eval Acc: 0.7671 (LR: 0.00100000)
[2025-05-27 18:14:42,235]: [ResNet18_relu6_quantized_2_bits] Epoch: 014 Train Loss: 0.5421 Train Acc: 0.8124 Eval Loss: 0.6164 Eval Acc: 0.7892 (LR: 0.00100000)
[2025-05-27 18:16:31,170]: [ResNet18_relu6_quantized_2_bits] Epoch: 015 Train Loss: 0.5485 Train Acc: 0.8102 Eval Loss: 0.6909 Eval Acc: 0.7747 (LR: 0.00010000)
[2025-05-27 18:18:16,225]: [ResNet18_relu6_quantized_2_bits] Epoch: 016 Train Loss: 0.4492 Train Acc: 0.8463 Eval Loss: 0.4269 Eval Acc: 0.8556 (LR: 0.00010000)
[2025-05-27 18:20:00,482]: [ResNet18_relu6_quantized_2_bits] Epoch: 017 Train Loss: 0.4273 Train Acc: 0.8508 Eval Loss: 0.4338 Eval Acc: 0.8535 (LR: 0.00010000)
[2025-05-27 18:21:46,208]: [ResNet18_relu6_quantized_2_bits] Epoch: 018 Train Loss: 0.4166 Train Acc: 0.8565 Eval Loss: 0.4633 Eval Acc: 0.8491 (LR: 0.00010000)
[2025-05-27 18:23:31,589]: [ResNet18_relu6_quantized_2_bits] Epoch: 019 Train Loss: 0.4086 Train Acc: 0.8588 Eval Loss: 0.4396 Eval Acc: 0.8508 (LR: 0.00010000)
[2025-05-27 18:25:16,975]: [ResNet18_relu6_quantized_2_bits] Epoch: 020 Train Loss: 0.4029 Train Acc: 0.8595 Eval Loss: 0.4127 Eval Acc: 0.8639 (LR: 0.00010000)
[2025-05-27 18:27:02,377]: [ResNet18_relu6_quantized_2_bits] Epoch: 021 Train Loss: 0.4046 Train Acc: 0.8602 Eval Loss: 0.4243 Eval Acc: 0.8556 (LR: 0.00010000)
[2025-05-27 18:28:47,183]: [ResNet18_relu6_quantized_2_bits] Epoch: 022 Train Loss: 0.4022 Train Acc: 0.8604 Eval Loss: 0.5047 Eval Acc: 0.8360 (LR: 0.00010000)
[2025-05-27 18:30:32,217]: [ResNet18_relu6_quantized_2_bits] Epoch: 023 Train Loss: 0.4070 Train Acc: 0.8583 Eval Loss: 0.4570 Eval Acc: 0.8504 (LR: 0.00010000)
[2025-05-27 18:32:17,312]: [ResNet18_relu6_quantized_2_bits] Epoch: 024 Train Loss: 0.4039 Train Acc: 0.8603 Eval Loss: 0.4357 Eval Acc: 0.8524 (LR: 0.00010000)
[2025-05-27 18:34:01,572]: [ResNet18_relu6_quantized_2_bits] Epoch: 025 Train Loss: 0.3991 Train Acc: 0.8613 Eval Loss: 0.4252 Eval Acc: 0.8567 (LR: 0.00010000)
[2025-05-27 18:35:45,006]: [ResNet18_relu6_quantized_2_bits] Epoch: 026 Train Loss: 0.4075 Train Acc: 0.8584 Eval Loss: 0.5448 Eval Acc: 0.8218 (LR: 0.00001000)
[2025-05-27 18:37:28,596]: [ResNet18_relu6_quantized_2_bits] Epoch: 027 Train Loss: 0.3759 Train Acc: 0.8705 Eval Loss: 0.3931 Eval Acc: 0.8673 (LR: 0.00001000)
[2025-05-27 18:39:14,936]: [ResNet18_relu6_quantized_2_bits] Epoch: 028 Train Loss: 0.3654 Train Acc: 0.8729 Eval Loss: 0.3839 Eval Acc: 0.8716 (LR: 0.00001000)
[2025-05-27 18:41:06,130]: [ResNet18_relu6_quantized_2_bits] Epoch: 029 Train Loss: 0.3668 Train Acc: 0.8730 Eval Loss: 0.3798 Eval Acc: 0.8712 (LR: 0.00001000)
[2025-05-27 18:42:48,026]: [ResNet18_relu6_quantized_2_bits] Epoch: 030 Train Loss: 0.3611 Train Acc: 0.8742 Eval Loss: 0.4043 Eval Acc: 0.8654 (LR: 0.00001000)
[2025-05-27 18:44:30,773]: [ResNet18_relu6_quantized_2_bits] Epoch: 031 Train Loss: 0.3633 Train Acc: 0.8730 Eval Loss: 0.3989 Eval Acc: 0.8633 (LR: 0.00001000)
[2025-05-27 18:46:13,571]: [ResNet18_relu6_quantized_2_bits] Epoch: 032 Train Loss: 0.3580 Train Acc: 0.8754 Eval Loss: 0.3994 Eval Acc: 0.8661 (LR: 0.00001000)
[2025-05-27 18:47:57,678]: [ResNet18_relu6_quantized_2_bits] Epoch: 033 Train Loss: 0.3595 Train Acc: 0.8740 Eval Loss: 0.3943 Eval Acc: 0.8669 (LR: 0.00001000)
[2025-05-27 18:49:40,811]: [ResNet18_relu6_quantized_2_bits] Epoch: 034 Train Loss: 0.3558 Train Acc: 0.8765 Eval Loss: 0.3833 Eval Acc: 0.8712 (LR: 0.00001000)
[2025-05-27 18:51:24,074]: [ResNet18_relu6_quantized_2_bits] Epoch: 035 Train Loss: 0.3591 Train Acc: 0.8759 Eval Loss: 0.4041 Eval Acc: 0.8641 (LR: 0.00000100)
[2025-05-27 18:53:07,255]: [ResNet18_relu6_quantized_2_bits] Epoch: 036 Train Loss: 0.3509 Train Acc: 0.8784 Eval Loss: 0.3827 Eval Acc: 0.8739 (LR: 0.00000100)
[2025-05-27 18:54:56,216]: [ResNet18_relu6_quantized_2_bits] Epoch: 037 Train Loss: 0.3473 Train Acc: 0.8787 Eval Loss: 0.3721 Eval Acc: 0.8741 (LR: 0.00000100)
[2025-05-27 18:56:50,120]: [ResNet18_relu6_quantized_2_bits] Epoch: 038 Train Loss: 0.3464 Train Acc: 0.8790 Eval Loss: 0.3807 Eval Acc: 0.8742 (LR: 0.00000100)
[2025-05-27 18:58:40,897]: [ResNet18_relu6_quantized_2_bits] Epoch: 039 Train Loss: 0.3456 Train Acc: 0.8808 Eval Loss: 0.3884 Eval Acc: 0.8725 (LR: 0.00000100)
[2025-05-27 19:00:30,370]: [ResNet18_relu6_quantized_2_bits] Epoch: 040 Train Loss: 0.3508 Train Acc: 0.8769 Eval Loss: 0.3766 Eval Acc: 0.8738 (LR: 0.00000100)
[2025-05-27 19:02:15,935]: [ResNet18_relu6_quantized_2_bits] Epoch: 041 Train Loss: 0.3497 Train Acc: 0.8800 Eval Loss: 0.3842 Eval Acc: 0.8721 (LR: 0.00000100)
[2025-05-27 19:04:02,900]: [ResNet18_relu6_quantized_2_bits] Epoch: 042 Train Loss: 0.3476 Train Acc: 0.8811 Eval Loss: 0.3705 Eval Acc: 0.8761 (LR: 0.00000100)
[2025-05-27 19:05:48,607]: [ResNet18_relu6_quantized_2_bits] Epoch: 043 Train Loss: 0.3501 Train Acc: 0.8784 Eval Loss: 0.3703 Eval Acc: 0.8755 (LR: 0.00000100)
[2025-05-27 19:07:38,816]: [ResNet18_relu6_quantized_2_bits] Epoch: 044 Train Loss: 0.3500 Train Acc: 0.8788 Eval Loss: 0.3685 Eval Acc: 0.8724 (LR: 0.00000100)
[2025-05-27 19:09:24,588]: [ResNet18_relu6_quantized_2_bits] Epoch: 045 Train Loss: 0.3492 Train Acc: 0.8790 Eval Loss: 0.3825 Eval Acc: 0.8725 (LR: 0.00000100)
[2025-05-27 19:11:30,612]: [ResNet18_relu6_quantized_2_bits] Epoch: 046 Train Loss: 0.3524 Train Acc: 0.8770 Eval Loss: 0.3827 Eval Acc: 0.8733 (LR: 0.00000100)
[2025-05-27 19:13:31,334]: [ResNet18_relu6_quantized_2_bits] Epoch: 047 Train Loss: 0.3479 Train Acc: 0.8796 Eval Loss: 0.3648 Eval Acc: 0.8768 (LR: 0.00000100)
[2025-05-27 19:15:18,193]: [ResNet18_relu6_quantized_2_bits] Epoch: 048 Train Loss: 0.3470 Train Acc: 0.8786 Eval Loss: 0.3795 Eval Acc: 0.8722 (LR: 0.00000100)
[2025-05-27 19:17:01,496]: [ResNet18_relu6_quantized_2_bits] Epoch: 049 Train Loss: 0.3475 Train Acc: 0.8798 Eval Loss: 0.3780 Eval Acc: 0.8731 (LR: 0.00000100)
[2025-05-27 19:18:43,185]: [ResNet18_relu6_quantized_2_bits] Epoch: 050 Train Loss: 0.3509 Train Acc: 0.8787 Eval Loss: 0.3861 Eval Acc: 0.8699 (LR: 0.00000100)
[2025-05-27 19:20:24,689]: [ResNet18_relu6_quantized_2_bits] Epoch: 051 Train Loss: 0.3470 Train Acc: 0.8778 Eval Loss: 0.3743 Eval Acc: 0.8721 (LR: 0.00000100)
[2025-05-27 19:22:11,397]: [ResNet18_relu6_quantized_2_bits] Epoch: 052 Train Loss: 0.3522 Train Acc: 0.8778 Eval Loss: 0.3778 Eval Acc: 0.8756 (LR: 0.00000100)
[2025-05-27 19:24:12,503]: [ResNet18_relu6_quantized_2_bits] Epoch: 053 Train Loss: 0.3523 Train Acc: 0.8780 Eval Loss: 0.3712 Eval Acc: 0.8741 (LR: 0.00000010)
[2025-05-27 19:26:08,436]: [ResNet18_relu6_quantized_2_bits] Epoch: 054 Train Loss: 0.3423 Train Acc: 0.8811 Eval Loss: 0.3686 Eval Acc: 0.8802 (LR: 0.00000010)
[2025-05-27 19:28:00,093]: [ResNet18_relu6_quantized_2_bits] Epoch: 055 Train Loss: 0.3444 Train Acc: 0.8803 Eval Loss: 0.3634 Eval Acc: 0.8787 (LR: 0.00000010)
[2025-05-27 19:30:00,420]: [ResNet18_relu6_quantized_2_bits] Epoch: 056 Train Loss: 0.3466 Train Acc: 0.8803 Eval Loss: 0.3582 Eval Acc: 0.8782 (LR: 0.00000010)
[2025-05-27 19:31:49,736]: [ResNet18_relu6_quantized_2_bits] Epoch: 057 Train Loss: 0.3456 Train Acc: 0.8798 Eval Loss: 0.3739 Eval Acc: 0.8736 (LR: 0.00000010)
[2025-05-27 19:33:53,444]: [ResNet18_relu6_quantized_2_bits] Epoch: 058 Train Loss: 0.3437 Train Acc: 0.8809 Eval Loss: 0.3754 Eval Acc: 0.8760 (LR: 0.00000010)
[2025-05-27 19:35:56,524]: [ResNet18_relu6_quantized_2_bits] Epoch: 059 Train Loss: 0.3408 Train Acc: 0.8814 Eval Loss: 0.3750 Eval Acc: 0.8729 (LR: 0.00000010)
[2025-05-27 19:37:51,208]: [ResNet18_relu6_quantized_2_bits] Epoch: 060 Train Loss: 0.3429 Train Acc: 0.8795 Eval Loss: 0.3776 Eval Acc: 0.8752 (LR: 0.00000010)
[2025-05-27 19:37:51,209]: [ResNet18_relu6_quantized_2_bits] Best Eval Accuracy: 0.8802
[2025-05-27 19:37:51,711]: 


Quantization of model down to 2 bits finished
[2025-05-27 19:37:51,711]: Model Architecture:
[2025-05-27 19:37:52,147]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2145], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32173478603363037, max_val=0.32173240184783936)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2661], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3571590185165405, max_val=0.44115695357322693)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3181], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4795190095901489, max_val=0.4747191369533539)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2457], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2915663719177246, max_val=0.4456746578216553)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2211], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33157891035079956, max_val=0.33165085315704346)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2087], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31349754333496094, max_val=0.3126443028450012)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4055], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4958069920539856, max_val=0.7207028865814209)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2379], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.342906653881073, max_val=0.3706800937652588)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9994], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.998264789581299)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1869], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2700992822647095, max_val=0.29071080684661865)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1921], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2624497413635254, max_val=0.3138555884361267)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2053], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28935766220092773, max_val=0.3266795873641968)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2238], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31043535470962524, max_val=0.36086493730545044)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2206], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2841607928276062, max_val=0.3776237368583679)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99997615814209)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1674], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.251140296459198, max_val=0.25110822916030884)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1588], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19611546397209167, max_val=0.28028345108032227)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1459], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21191543340682983, max_val=0.22564071416854858)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1973], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2841363549232483, max_val=0.30764704942703247)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1067], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14234963059425354, max_val=0.1776266247034073)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7987], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.396134853363037)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0788], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12719154357910156, max_val=0.10933423787355423)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-27 19:37:52,147]: 
Model Weights:
[2025-05-27 19:37:52,148]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-27 19:37:52,170]: Sample Values (25 elements): [-0.105659618973732, 0.033944085240364075, -0.13528943061828613, -0.06949906051158905, -0.23145991563796997, -0.012659642845392227, 0.2142854481935501, -0.11824510991573334, -0.0006623672670684755, 0.19232356548309326, -0.08363261818885803, -0.04015754908323288, 0.03007541410624981, -0.16316662728786469, 0.3045872449874878, -0.15156351029872894, 0.11405058950185776, -0.09212425351142883, 0.17931601405143738, 0.2492467164993286, -0.09369687736034393, 0.034862957894802094, -0.10433890670537949, -0.02730918489396572, 0.05676749721169472]
[2025-05-27 19:37:52,171]: Mean: -0.00166154
[2025-05-27 19:37:52,171]: Min: -0.36059448
[2025-05-27 19:37:52,171]: Max: 0.34379348
[2025-05-27 19:37:52,171]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-27 19:37:52,171]: Sample Values (25 elements): [1.044037103652954, 1.1690256595611572, 0.759034276008606, 0.7869800329208374, 0.7009878754615784, 2.2122304439544678, 1.2977173328399658, 1.6936064958572388, 2.0904877185821533, 2.2769734859466553, 0.49286121129989624, 0.5026941895484924, 1.5228434801101685, 0.03860748931765556, 1.1782186031341553, 0.10490784794092178, 0.550767183303833, 1.3258981704711914, 1.6304175853729248, 0.8014459013938904, 1.411481499671936, 1.8165420293807983, 0.7735066413879395, 1.0207570791244507, 1.0691856145858765]
[2025-05-27 19:37:52,172]: Mean: 0.97487414
[2025-05-27 19:37:52,172]: Min: 0.03860749
[2025-05-27 19:37:52,172]: Max: 2.27697349
[2025-05-27 19:37:52,173]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 19:37:52,173]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,174]: Mean: -0.00136150
[2025-05-27 19:37:52,174]: Min: -0.21448907
[2025-05-27 19:37:52,174]: Max: 0.21448907
[2025-05-27 19:37:52,174]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 19:37:52,174]: Sample Values (25 elements): [0.7007599472999573, 1.3674384291562092e-08, 0.9009671211242676, 8.538429227436355e-15, 0.8080082535743713, 1.1435179710388184, 0.00163180916570127, 0.7102457284927368, 0.8319370150566101, 0.8027580380439758, 0.7848036885261536, 1.0728323459625244, 0.7215086817741394, 0.8500039577484131, 0.8442535996437073, 0.9632944464683533, 1.1495661735534668, 0.6567234992980957, 0.913304328918457, 1.3803349929730757e-06, 1.0368118286132812, 1.389596700668335, 3.029559416300174e-25, 0.20637241005897522, 0.021946823224425316]
[2025-05-27 19:37:52,175]: Mean: 0.69571728
[2025-05-27 19:37:52,175]: Min: 0.00000000
[2025-05-27 19:37:52,175]: Max: 1.38959670
[2025-05-27 19:37:52,176]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 19:37:52,176]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,177]: Mean: 0.00066411
[2025-05-27 19:37:52,177]: Min: -0.26610532
[2025-05-27 19:37:52,177]: Max: 0.53221065
[2025-05-27 19:37:52,177]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 19:37:52,177]: Sample Values (25 elements): [0.6959490180015564, 1.3426584005355835, 0.8576049208641052, 0.8718485236167908, 3.919939899788005e-06, 0.9610856175422668, 1.1746834516525269, 0.04148350656032562, 0.5799815058708191, 0.5468306541442871, 0.9837049841880798, 0.6057050228118896, 0.8848690390586853, 1.1045894622802734, 0.8168503046035767, 0.9536600708961487, 0.9054206013679504, 0.999764084815979, 1.3014284372329712, 0.845661461353302, 1.2829869985580444, 0.134246364235878, 1.0252794027328491, 0.42406418919563293, 0.7561848163604736]
[2025-05-27 19:37:52,177]: Mean: 0.82445478
[2025-05-27 19:37:52,178]: Min: 0.00000392
[2025-05-27 19:37:52,178]: Max: 1.50779295
[2025-05-27 19:37:52,179]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 19:37:52,179]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,179]: Mean: 0.00018983
[2025-05-27 19:37:52,179]: Min: -0.63615882
[2025-05-27 19:37:52,180]: Max: 0.31807941
[2025-05-27 19:37:52,180]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 19:37:52,180]: Sample Values (25 elements): [0.721003532409668, 0.9182075262069702, 0.05112021043896675, 0.7195147275924683, 1.1512154340744019, 0.20060785114765167, 0.5523092746734619, 0.21277374029159546, 0.5209393501281738, 1.1978693008422852, 0.8444089889526367, 1.4274317026138306, 2.624318995003705e-06, 1.421356201171875, 0.9197365641593933, 0.4789138436317444, 0.8093563318252563, 0.09234095364809036, 0.49935293197631836, 0.7746689319610596, 0.4175795316696167, 0.13511697947978973, 9.686968430977076e-19, 0.24553662538528442, 1.0478500127792358]
[2025-05-27 19:37:52,180]: Mean: 0.62131548
[2025-05-27 19:37:52,180]: Min: -0.00000000
[2025-05-27 19:37:52,180]: Max: 1.42743170
[2025-05-27 19:37:52,181]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 19:37:52,182]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,182]: Mean: 0.00005333
[2025-05-27 19:37:52,182]: Min: -0.24574701
[2025-05-27 19:37:52,182]: Max: 0.49149403
[2025-05-27 19:37:52,182]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 19:37:52,183]: Sample Values (25 elements): [0.9939521551132202, 0.7518908977508545, 0.6258128881454468, 0.871534526348114, 0.718421220779419, 0.7397368550300598, 1.0646040439605713, 0.5644888877868652, 1.1160283088684082, 0.4288955330848694, 0.8110283017158508, 0.5342874526977539, 0.7268778681755066, 0.6863248348236084, 0.8062673807144165, 1.020796537399292, 0.7464653253555298, 0.577196478843689, 1.0274507999420166, 1.0002444982528687, 0.4843796491622925, 0.8591258525848389, 0.4843512773513794, 1.0963530540466309, 0.619193971157074]
[2025-05-27 19:37:52,183]: Mean: 0.75971383
[2025-05-27 19:37:52,183]: Min: 0.00000000
[2025-05-27 19:37:52,183]: Max: 1.28609872
[2025-05-27 19:37:52,184]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-27 19:37:52,185]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,185]: Mean: 0.00088757
[2025-05-27 19:37:52,185]: Min: -0.22107659
[2025-05-27 19:37:52,185]: Max: 0.44215319
[2025-05-27 19:37:52,185]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-27 19:37:52,186]: Sample Values (25 elements): [0.7783723473548889, 0.6274011731147766, 0.874957799911499, 0.8713731169700623, 0.5514147877693176, 0.8494333624839783, 0.1774856299161911, 0.8432855010032654, 0.16304142773151398, 0.7726176977157593, 0.7184592485427856, 4.945322410448712e-41, 0.6572242975234985, 1.052897334098816, 0.1210213452577591, 0.6811596155166626, 0.9558615684509277, 0.8722020983695984, 3.1069618491540965e-17, 6.063482604901083e-09, 0.9667971730232239, 1.0189745426177979, 0.8812490105628967, 0.8584926128387451, 2.8113286099583877e-14]
[2025-05-27 19:37:52,186]: Mean: 0.62994659
[2025-05-27 19:37:52,186]: Min: -0.00000000
[2025-05-27 19:37:52,186]: Max: 1.12683320
[2025-05-27 19:37:52,187]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 19:37:52,189]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20871397852897644, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,189]: Mean: 0.00008493
[2025-05-27 19:37:52,189]: Min: -0.41742796
[2025-05-27 19:37:52,189]: Max: 0.20871398
[2025-05-27 19:37:52,189]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-27 19:37:52,189]: Sample Values (25 elements): [0.8454951643943787, 0.6581237316131592, 1.0075451135635376, 0.8962757587432861, 0.9960643649101257, 0.9489270448684692, 0.8887929320335388, 0.9037511348724365, 0.8679242134094238, 0.7057009935379028, -4.907207092219077e-41, 0.6158491969108582, 0.9294242262840271, 0.7045296430587769, 0.7302577495574951, 0.8473026752471924, 0.8960833549499512, 0.8948344588279724, 0.9395554661750793, 0.779193639755249, 0.7634510397911072, 1.250083088874817, 0.9551040530204773, 0.4807974100112915, 0.7944381833076477]
[2025-05-27 19:37:52,190]: Mean: 0.75492966
[2025-05-27 19:37:52,190]: Min: -0.00000000
[2025-05-27 19:37:52,190]: Max: 1.25008309
[2025-05-27 19:37:52,191]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-27 19:37:52,192]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4055032730102539, 0.0, 0.0, 0.0, 0.4055032730102539, 0.4055032730102539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,192]: Mean: 0.00039600
[2025-05-27 19:37:52,192]: Min: -0.40550327
[2025-05-27 19:37:52,192]: Max: 0.81100655
[2025-05-27 19:37:52,192]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-27 19:37:52,193]: Sample Values (25 elements): [0.39267846941947937, 0.208086758852005, 0.6088063716888428, 0.43637314438819885, 0.5370543599128723, 0.4251749515533447, 0.24712970852851868, 0.43381768465042114, 0.5436548590660095, 0.4922519326210022, 0.28883612155914307, 0.7277926802635193, 0.6796640157699585, 0.5512216091156006, 0.564243733882904, 0.5125634670257568, 0.33290088176727295, 0.45615917444229126, 0.5069548487663269, 0.5402570962905884, -4.949386175995254e-41, 0.3400859832763672, 0.3923921287059784, 0.3709917962551117, 0.46313512325286865]
[2025-05-27 19:37:52,193]: Mean: 0.44798374
[2025-05-27 19:37:52,193]: Min: -0.00000000
[2025-05-27 19:37:52,193]: Max: 0.99648398
[2025-05-27 19:37:52,194]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 19:37:52,196]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,196]: Mean: 0.00048393
[2025-05-27 19:37:52,196]: Min: -0.23786226
[2025-05-27 19:37:52,196]: Max: 0.47572452
[2025-05-27 19:37:52,196]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-27 19:37:52,217]: Sample Values (25 elements): [0.4167085886001587, 0.08648450672626495, 0.7215897440910339, 0.0002454690693411976, 0.40277865529060364, 0.876173198223114, 0.5028880834579468, 1.0517370700836182, 0.9314839243888855, 0.8900506496429443, 0.5912466645240784, 0.14214076101779938, 0.5821127891540527, 0.6290457248687744, 0.36119624972343445, 0.8318323493003845, 0.6533085703849792, 0.9270917177200317, 0.7844844460487366, 0.7157137989997864, 0.7856073379516602, 0.3195953369140625, 0.8645684123039246, 0.8490437269210815, 0.9523486495018005]
[2025-05-27 19:37:52,217]: Mean: 0.54570866
[2025-05-27 19:37:52,217]: Min: -0.00000000
[2025-05-27 19:37:52,218]: Max: 1.13343334
[2025-05-27 19:37:52,219]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-27 19:37:52,221]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.18693670630455017, 0.0, 0.0, -0.18693670630455017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18693670630455017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,221]: Mean: -0.00047921
[2025-05-27 19:37:52,221]: Min: -0.18693671
[2025-05-27 19:37:52,221]: Max: 0.37387341
[2025-05-27 19:37:52,222]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-27 19:37:52,222]: Sample Values (25 elements): [0.0065727392211556435, 0.6007813811302185, 0.6963782906532288, 0.37484675645828247, 0.7006683945655823, 0.6632465720176697, 0.6647503972053528, 0.4793908894062042, 0.7008289694786072, 0.6694876551628113, 0.6232231855392456, 0.7906360626220703, 0.7632716298103333, 0.6311697363853455, 0.7097097635269165, 0.5332839488983154, 0.7521821856498718, 0.6562198996543884, 0.8665624260902405, 0.23287315666675568, 0.5871458053588867, 0.40644609928131104, 0.4955216646194458, 0.5417170524597168, 0.2897273004055023]
[2025-05-27 19:37:52,222]: Mean: 0.63072586
[2025-05-27 19:37:52,222]: Min: 0.00000010
[2025-05-27 19:37:52,222]: Max: 1.06876576
[2025-05-27 19:37:52,224]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-27 19:37:52,227]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.19210177659988403, 0.0, 0.0, 0.0, 0.0, -0.19210177659988403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,228]: Mean: 0.00066116
[2025-05-27 19:37:52,228]: Min: -0.19210178
[2025-05-27 19:37:52,228]: Max: 0.38420355
[2025-05-27 19:37:52,228]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-27 19:37:52,229]: Sample Values (25 elements): [0.27068740129470825, 4.961297212942015e-41, 0.86955726146698, 1.0646462440490723, 0.6832663416862488, 0.8251411318778992, 4.314523539505899e-05, -4.968163575417206e-41, 0.5208701491355896, 0.6929254531860352, 0.4041317403316498, 0.9048748016357422, 4.946443449220172e-41, 4.917016181469351e-41, 2.020580495809554e-06, -4.940978385209305e-41, 0.7326536774635315, 4.937054749509196e-41, -4.922481245480217e-41, -4.947003968605902e-41, 0.37600305676460266, 4.907347222065509e-41, 0.6742576360702515, 0.5779165029525757, 0.784928560256958]
[2025-05-27 19:37:52,229]: Mean: 0.43554682
[2025-05-27 19:37:52,230]: Min: -0.00000000
[2025-05-27 19:37:52,230]: Max: 1.15210319
[2025-05-27 19:37:52,231]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 19:37:52,242]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053457498550415, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,242]: Mean: 0.00030463
[2025-05-27 19:37:52,243]: Min: -0.20534575
[2025-05-27 19:37:52,243]: Max: 0.41069150
[2025-05-27 19:37:52,243]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-27 19:37:52,243]: Sample Values (25 elements): [0.1923563927412033, 0.7363241314888, 0.6554535627365112, 4.949386175995254e-41, 4.910149818994159e-41, 4.954991369852553e-41, 0.7016164064407349, 2.6808573238668032e-05, 0.49676793813705444, 0.6680969595909119, 0.11796458810567856, 0.0012868628837168217, 0.05311125889420509, 0.7799869775772095, 0.7913140654563904, 7.690424408401952e-10, 0.6517575979232788, 0.49388930201530457, 0.6471459269523621, 0.723964273929596, 0.5494117736816406, 1.010197639465332, 5.167848606583493e-41, 0.7613772749900818, 0.7648825645446777]
[2025-05-27 19:37:52,243]: Mean: 0.46860597
[2025-05-27 19:37:52,244]: Min: -0.00000000
[2025-05-27 19:37:52,244]: Max: 1.10693026
[2025-05-27 19:37:52,245]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-27 19:37:52,246]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.2237667739391327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2237667739391327, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,246]: Mean: 0.00030047
[2025-05-27 19:37:52,246]: Min: -0.22376677
[2025-05-27 19:37:52,246]: Max: 0.44753355
[2025-05-27 19:37:52,246]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-27 19:37:52,247]: Sample Values (25 elements): [0.5005636811256409, 0.31890869140625, 3.617064127056126e-14, 0.5186185836791992, 0.3083891272544861, 0.3873993158340454, -4.915614883005026e-41, -5.182562240458903e-41, 0.6589539051055908, 0.3999958336353302, 4.949526305841686e-41, 0.3401936888694763, 0.4842208921909332, 5.064993299302051e-41, 0.4176899492740631, 0.0015081563033163548, 0.6287212371826172, 8.38975902297534e-05, 0.2737452983856201, 0.5284681916236877, 0.40014421939849854, -4.965360978488557e-41, 3.2431214425131596e-14, 0.5002109408378601, 0.4859137237071991]
[2025-05-27 19:37:52,247]: Mean: 0.28024244
[2025-05-27 19:37:52,247]: Min: -0.00000000
[2025-05-27 19:37:52,247]: Max: 0.81725389
[2025-05-27 19:37:52,249]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 19:37:52,256]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,256]: Mean: 0.00028200
[2025-05-27 19:37:52,256]: Min: -0.22059485
[2025-05-27 19:37:52,257]: Max: 0.44118971
[2025-05-27 19:37:52,257]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-27 19:37:52,257]: Sample Values (25 elements): [4.933411373501951e-41, 4.962278121867042e-41, 5.509003291966259e-16, 4.934112022734113e-41, -5.289761572979752e-41, 0.5594514012336731, -6.271090887546421e-41, -4.978953573592508e-41, 4.952048643077471e-41, 0.9144771695137024, -6.071826245919432e-41, 0.3477749824523926, 0.8642846941947937, 4.975029937892398e-41, 0.48727306723594666, 0.4099515378475189, 0.5432792901992798, 0.37056446075439453, -4.929908127341139e-41, -4.958354486166933e-41, -4.984839027142672e-41, -4.906086053447617e-41, 0.21838441491127014, -4.973208249888776e-41, -5.952295486912525e-41]
[2025-05-27 19:37:52,257]: Mean: 0.25721711
[2025-05-27 19:37:52,258]: Min: -0.00000000
[2025-05-27 19:37:52,258]: Max: 1.00895023
[2025-05-27 19:37:52,260]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-27 19:37:52,269]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,270]: Mean: -0.00013993
[2025-05-27 19:37:52,270]: Min: -0.33483237
[2025-05-27 19:37:52,270]: Max: 0.16741619
[2025-05-27 19:37:52,270]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-27 19:37:52,271]: Sample Values (25 elements): [0.3829527199268341, 0.3389964997768402, 0.05931791290640831, 6.085699100716248e-41, 0.5220629572868347, 0.48478439450263977, 4.966482017260017e-41, -4.98245681975332e-41, 0.6072545051574707, 0.6817917823791504, 0.7070404291152954, 0.4238263964653015, 0.35217392444610596, 5.129733288353858e-41, 0.1227884590625763, 0.37237975001335144, 0.1489330679178238, 0.6709479093551636, 0.48404327034950256, 0.9080277681350708, 4.943640852291522e-41, 0.7468506097793579, 4.95877487570623e-41, 0.5199307799339294, -4.907627481758374e-41]
[2025-05-27 19:37:52,271]: Mean: 0.30360296
[2025-05-27 19:37:52,271]: Min: -0.00000006
[2025-05-27 19:37:52,271]: Max: 0.97801036
[2025-05-27 19:37:52,273]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-27 19:37:52,292]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,292]: Mean: 0.00007027
[2025-05-27 19:37:52,292]: Min: -0.15879965
[2025-05-27 19:37:52,292]: Max: 0.31759930
[2025-05-27 19:37:52,292]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-27 19:37:52,293]: Sample Values (25 elements): [5.036266680783393e-41, 5.545638672565464e-41, -4.904824884829725e-41, -5.660405016793666e-41, -5.528823090993566e-41, 4.917856960547946e-41, -5.926931984708246e-41, 0.4767242670059204, -5.494351148771175e-41, 6.035812875386285e-41, 5.259493526150336e-41, 5.352679874027936e-41, 4.956953187702608e-41, 5.498274784471285e-41, 4.948545396916659e-41, 5.551243866422763e-41, 6.111623122306257e-41, 5.414757395997526e-41, 0.7901419997215271, 4.917016181469351e-41, -5.674698261129779e-41, 5.175976137676577e-41, 6.271371147239286e-41, 5.542976205483246e-41, 6.063138195440619e-41]
[2025-05-27 19:37:52,293]: Mean: 0.08639953
[2025-05-27 19:37:52,293]: Min: -0.00000000
[2025-05-27 19:37:52,293]: Max: 1.14729404
[2025-05-27 19:37:52,294]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 19:37:52,349]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,350]: Mean: 0.00007202
[2025-05-27 19:37:52,350]: Min: -0.14585206
[2025-05-27 19:37:52,350]: Max: 0.29170412
[2025-05-27 19:37:52,351]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-27 19:37:52,351]: Sample Values (25 elements): [0.13144996762275696, 0.3858296573162079, 0.23653613030910492, 0.3670170307159424, 4.987081104685591e-41, 0.6284435987472534, 0.36057618260383606, 0.2922270894050598, 0.3969135284423828, -6.056972482197589e-41, 0.4396616220474243, 0.17702697217464447, -5.004457205643219e-41, 0.36920031905174255, 0.000332321273162961, 0.309455931186676, -4.954150590773958e-41, 0.3476593792438507, 0.0020577902905642986, 0.0029524117708206177, 0.24489033222198486, 0.47429129481315613, 5.084891737495464e-41, -4.940838255362873e-41, 0.6331143379211426]
[2025-05-27 19:37:52,351]: Mean: 0.27820539
[2025-05-27 19:37:52,351]: Min: -0.00000000
[2025-05-27 19:37:52,352]: Max: 0.89184397
[2025-05-27 19:37:52,353]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-27 19:37:52,354]: Sample Values (25 elements): [0.0, 0.19726113975048065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,355]: Mean: 0.00165548
[2025-05-27 19:37:52,355]: Min: -0.19726114
[2025-05-27 19:37:52,355]: Max: 0.39452228
[2025-05-27 19:37:52,355]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-27 19:37:52,355]: Sample Values (25 elements): [0.4040161967277527, 0.4564398229122162, 1.2497267562139314e-05, 0.5815279483795166, 5.034164733086905e-41, 0.02667710743844509, 4.964660329256394e-41, 0.25773170590400696, 0.17105945944786072, 4.737565541290678e-05, 4.936634359969898e-41, 0.5464691519737244, 1.451250142281424e-07, 0.5544565916061401, 0.4277080297470093, 0.5803772211074829, 0.3441797196865082, 0.566814124584198, 0.5492221713066101, 1.369205068783833e-16, 0.3726588189601898, 4.936774489816331e-41, -5.638124371210901e-41, 0.4116526246070862, 0.6363092064857483]
[2025-05-27 19:37:52,355]: Mean: 0.30465376
[2025-05-27 19:37:52,356]: Min: -0.00000000
[2025-05-27 19:37:52,356]: Max: 0.80793941
[2025-05-27 19:37:52,357]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 19:37:52,411]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,412]: Mean: 0.00001777
[2025-05-27 19:37:52,412]: Min: -0.10665875
[2025-05-27 19:37:52,412]: Max: 0.21331750
[2025-05-27 19:37:52,412]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-27 19:37:52,414]: Sample Values (25 elements): [4.941398774748602e-41, -4.907627481758374e-41, 6.052067937572452e-41, 4.926965400566057e-41, -6.080794556091111e-41, -5.711972800280819e-41, 6.052908716651047e-41, -6.120731562324368e-41, 5.401865450125737e-41, -4.90510514452259e-41, 5.581091523712881e-41, 5.40312661874363e-41, -4.908748520529834e-41, 5.067655766384268e-41, -5.905071728664779e-41, -6.121852601095828e-41, 4.919818778398e-41, 5.892460042485856e-41, 5.876625369838985e-41, -6.016615086425035e-41, -5.076343816863082e-41, 4.932010075037626e-41, 5.05252174296956e-41, -4.993667207467918e-41, -4.921220076862325e-41]
[2025-05-27 19:37:52,414]: Mean: 0.00233410
[2025-05-27 19:37:52,414]: Min: -0.01357476
[2025-05-27 19:37:52,415]: Max: 0.54841810
[2025-05-27 19:37:52,416]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-27 19:37:52,517]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 19:37:52,518]: Mean: -0.00000612
[2025-05-27 19:37:52,518]: Min: -0.15768386
[2025-05-27 19:37:52,518]: Max: 0.07884193
[2025-05-27 19:37:52,519]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-27 19:37:52,519]: Sample Values (25 elements): [0.03367368504405022, 5.198677172798639e-41, 0.19920605421066284, 0.14443562924861908, 4.914914233772863e-41, -5.932537178565546e-41, 0.24603448808193207, 0.236498162150383, 0.10086171329021454, 0.10627394914627075, 5.034164733086905e-41, 0.07572493702173233, 6.773770292056724e-05, 0.28570082783699036, 0.047927819192409515, 0.04222481697797775, 5.135198352364725e-41, 0.1262407898902893, -6.144133246678593e-41, 0.14842650294303894, 0.08488383144140244, 0.2696332633495331, 0.16814365983009338, 0.0005808323039673269, 0.01735035516321659]
[2025-05-27 19:37:52,519]: Mean: 0.06457869
[2025-05-27 19:37:52,519]: Min: -0.02080005
[2025-05-27 19:37:52,520]: Max: 0.43805775
[2025-05-27 19:37:52,520]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-27 19:37:52,550]: Sample Values (25 elements): [6.22638946653446e-41, 0.051794636994600296, 0.12173817306756973, 0.010067638009786606, -0.022335795685648918, -5.165886788733438e-41, -0.0052873967215418816, 0.13575229048728943, 4.909449169761997e-41, -0.2150215208530426, 0.030039334669709206, -0.07210223376750946, -0.14235654473304749, -0.02218685671687126, -0.2759378254413605, 5.615563465935272e-41, 0.16328835487365723, -0.10254183411598206, -4.918837869472973e-41, -0.011654251255095005, -0.09724164009094238, -5.375801298689296e-41, -0.023300055414438248, -0.2033095359802246, -0.13510185480117798]
[2025-05-27 19:37:52,550]: Mean: -0.02110828
[2025-05-27 19:37:52,551]: Min: -0.54063392
[2025-05-27 19:37:52,551]: Max: 0.36228171
