[2025-05-13 21:42:22,034]: 
Training ResNet18 with relu6
[2025-05-13 21:43:44,071]: [ResNet18_relu6] Epoch: 001 Train Loss: 1.6104 Train Acc: 0.4036 Eval Loss: 1.3066 Eval Acc: 0.5198 (LR: 0.001000)
[2025-05-13 21:45:07,034]: [ResNet18_relu6] Epoch: 002 Train Loss: 1.2094 Train Acc: 0.5630 Eval Loss: 1.1075 Eval Acc: 0.6032 (LR: 0.001000)
[2025-05-13 21:46:30,277]: [ResNet18_relu6] Epoch: 003 Train Loss: 1.0192 Train Acc: 0.6350 Eval Loss: 1.0239 Eval Acc: 0.6340 (LR: 0.001000)
[2025-05-13 21:47:49,900]: [ResNet18_relu6] Epoch: 004 Train Loss: 0.8968 Train Acc: 0.6820 Eval Loss: 0.8329 Eval Acc: 0.7036 (LR: 0.001000)
[2025-05-13 21:49:14,962]: [ResNet18_relu6] Epoch: 005 Train Loss: 0.7926 Train Acc: 0.7201 Eval Loss: 0.7607 Eval Acc: 0.7338 (LR: 0.001000)
[2025-05-13 21:50:37,986]: [ResNet18_relu6] Epoch: 006 Train Loss: 0.7207 Train Acc: 0.7483 Eval Loss: 0.8417 Eval Acc: 0.7286 (LR: 0.001000)
[2025-05-13 21:52:00,063]: [ResNet18_relu6] Epoch: 007 Train Loss: 0.6631 Train Acc: 0.7674 Eval Loss: 0.7205 Eval Acc: 0.7615 (LR: 0.001000)
[2025-05-13 21:53:23,200]: [ResNet18_relu6] Epoch: 008 Train Loss: 0.6131 Train Acc: 0.7859 Eval Loss: 0.6499 Eval Acc: 0.7784 (LR: 0.001000)
[2025-05-13 21:54:49,026]: [ResNet18_relu6] Epoch: 009 Train Loss: 0.5777 Train Acc: 0.7978 Eval Loss: 0.5944 Eval Acc: 0.8016 (LR: 0.001000)
[2025-05-13 21:56:16,367]: [ResNet18_relu6] Epoch: 010 Train Loss: 0.5406 Train Acc: 0.8112 Eval Loss: 0.5432 Eval Acc: 0.8180 (LR: 0.001000)
[2025-05-13 21:57:36,519]: [ResNet18_relu6] Epoch: 011 Train Loss: 0.5130 Train Acc: 0.8203 Eval Loss: 0.5791 Eval Acc: 0.8066 (LR: 0.001000)
[2025-05-13 21:58:55,363]: [ResNet18_relu6] Epoch: 012 Train Loss: 0.4889 Train Acc: 0.8289 Eval Loss: 0.5262 Eval Acc: 0.8232 (LR: 0.001000)
[2025-05-13 22:00:14,085]: [ResNet18_relu6] Epoch: 013 Train Loss: 0.4617 Train Acc: 0.8373 Eval Loss: 0.5041 Eval Acc: 0.8335 (LR: 0.001000)
[2025-05-13 22:01:33,232]: [ResNet18_relu6] Epoch: 014 Train Loss: 0.4389 Train Acc: 0.8456 Eval Loss: 0.5303 Eval Acc: 0.8287 (LR: 0.001000)
[2025-05-13 22:02:52,076]: [ResNet18_relu6] Epoch: 015 Train Loss: 0.4251 Train Acc: 0.8515 Eval Loss: 0.5077 Eval Acc: 0.8360 (LR: 0.001000)
[2025-05-13 22:04:10,796]: [ResNet18_relu6] Epoch: 016 Train Loss: 0.4052 Train Acc: 0.8592 Eval Loss: 0.4975 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-13 22:05:29,496]: [ResNet18_relu6] Epoch: 017 Train Loss: 0.3904 Train Acc: 0.8648 Eval Loss: 0.4857 Eval Acc: 0.8425 (LR: 0.001000)
[2025-05-13 22:06:49,324]: [ResNet18_relu6] Epoch: 018 Train Loss: 0.3731 Train Acc: 0.8692 Eval Loss: 0.4755 Eval Acc: 0.8498 (LR: 0.001000)
[2025-05-13 22:08:10,589]: [ResNet18_relu6] Epoch: 019 Train Loss: 0.3589 Train Acc: 0.8755 Eval Loss: 0.4509 Eval Acc: 0.8538 (LR: 0.001000)
[2025-05-13 22:09:32,680]: [ResNet18_relu6] Epoch: 020 Train Loss: 0.3465 Train Acc: 0.8791 Eval Loss: 0.4480 Eval Acc: 0.8586 (LR: 0.001000)
[2025-05-13 22:10:53,957]: [ResNet18_relu6] Epoch: 021 Train Loss: 0.3369 Train Acc: 0.8823 Eval Loss: 0.4329 Eval Acc: 0.8607 (LR: 0.001000)
[2025-05-13 22:12:16,077]: [ResNet18_relu6] Epoch: 022 Train Loss: 0.3199 Train Acc: 0.8872 Eval Loss: 0.4265 Eval Acc: 0.8601 (LR: 0.001000)
[2025-05-13 22:13:45,491]: [ResNet18_relu6] Epoch: 023 Train Loss: 0.3121 Train Acc: 0.8891 Eval Loss: 0.4238 Eval Acc: 0.8643 (LR: 0.001000)
[2025-05-13 22:15:14,872]: [ResNet18_relu6] Epoch: 024 Train Loss: 0.2938 Train Acc: 0.8964 Eval Loss: 0.4651 Eval Acc: 0.8543 (LR: 0.001000)
[2025-05-13 22:16:40,879]: [ResNet18_relu6] Epoch: 025 Train Loss: 0.2895 Train Acc: 0.8975 Eval Loss: 0.4665 Eval Acc: 0.8556 (LR: 0.001000)
[2025-05-13 22:18:00,380]: [ResNet18_relu6] Epoch: 026 Train Loss: 0.2824 Train Acc: 0.9016 Eval Loss: 0.4003 Eval Acc: 0.8736 (LR: 0.001000)
[2025-05-13 22:19:19,691]: [ResNet18_relu6] Epoch: 027 Train Loss: 0.2702 Train Acc: 0.9057 Eval Loss: 0.3981 Eval Acc: 0.8744 (LR: 0.001000)
[2025-05-13 22:20:43,996]: [ResNet18_relu6] Epoch: 028 Train Loss: 0.2594 Train Acc: 0.9090 Eval Loss: 0.4154 Eval Acc: 0.8699 (LR: 0.001000)
[2025-05-13 22:22:17,943]: [ResNet18_relu6] Epoch: 029 Train Loss: 0.2535 Train Acc: 0.9113 Eval Loss: 0.4777 Eval Acc: 0.8531 (LR: 0.001000)
[2025-05-13 22:23:40,263]: [ResNet18_relu6] Epoch: 030 Train Loss: 0.2456 Train Acc: 0.9132 Eval Loss: 0.3912 Eval Acc: 0.8776 (LR: 0.001000)
[2025-05-13 22:25:02,926]: [ResNet18_relu6] Epoch: 031 Train Loss: 0.2345 Train Acc: 0.9173 Eval Loss: 0.3833 Eval Acc: 0.8807 (LR: 0.001000)
[2025-05-13 22:26:29,992]: [ResNet18_relu6] Epoch: 032 Train Loss: 0.2270 Train Acc: 0.9201 Eval Loss: 0.3953 Eval Acc: 0.8742 (LR: 0.001000)
[2025-05-13 22:27:52,254]: [ResNet18_relu6] Epoch: 033 Train Loss: 0.2234 Train Acc: 0.9216 Eval Loss: 0.4289 Eval Acc: 0.8647 (LR: 0.001000)
[2025-05-13 22:29:12,867]: [ResNet18_relu6] Epoch: 034 Train Loss: 0.2143 Train Acc: 0.9245 Eval Loss: 0.4115 Eval Acc: 0.8725 (LR: 0.001000)
[2025-05-13 22:30:32,972]: [ResNet18_relu6] Epoch: 035 Train Loss: 0.2093 Train Acc: 0.9262 Eval Loss: 0.3952 Eval Acc: 0.8798 (LR: 0.001000)
[2025-05-13 22:31:53,021]: [ResNet18_relu6] Epoch: 036 Train Loss: 0.2028 Train Acc: 0.9280 Eval Loss: 0.4214 Eval Acc: 0.8736 (LR: 0.001000)
[2025-05-13 22:33:13,123]: [ResNet18_relu6] Epoch: 037 Train Loss: 0.1943 Train Acc: 0.9313 Eval Loss: 0.3926 Eval Acc: 0.8800 (LR: 0.001000)
[2025-05-13 22:34:32,984]: [ResNet18_relu6] Epoch: 038 Train Loss: 0.1923 Train Acc: 0.9332 Eval Loss: 0.3883 Eval Acc: 0.8838 (LR: 0.001000)
[2025-05-13 22:35:53,604]: [ResNet18_relu6] Epoch: 039 Train Loss: 0.1814 Train Acc: 0.9350 Eval Loss: 0.3793 Eval Acc: 0.8891 (LR: 0.001000)
[2025-05-13 22:37:15,601]: [ResNet18_relu6] Epoch: 040 Train Loss: 0.1779 Train Acc: 0.9374 Eval Loss: 0.4585 Eval Acc: 0.8676 (LR: 0.001000)
[2025-05-13 22:38:35,714]: [ResNet18_relu6] Epoch: 041 Train Loss: 0.1755 Train Acc: 0.9382 Eval Loss: 0.4072 Eval Acc: 0.8816 (LR: 0.001000)
[2025-05-13 22:39:55,764]: [ResNet18_relu6] Epoch: 042 Train Loss: 0.1667 Train Acc: 0.9411 Eval Loss: 0.4270 Eval Acc: 0.8764 (LR: 0.001000)
[2025-05-13 22:41:15,839]: [ResNet18_relu6] Epoch: 043 Train Loss: 0.1641 Train Acc: 0.9415 Eval Loss: 0.4207 Eval Acc: 0.8806 (LR: 0.001000)
[2025-05-13 22:42:35,454]: [ResNet18_relu6] Epoch: 044 Train Loss: 0.1605 Train Acc: 0.9431 Eval Loss: 0.3927 Eval Acc: 0.8863 (LR: 0.001000)
[2025-05-13 22:43:56,214]: [ResNet18_relu6] Epoch: 045 Train Loss: 0.1563 Train Acc: 0.9435 Eval Loss: 0.4123 Eval Acc: 0.8834 (LR: 0.001000)
[2025-05-13 22:45:17,532]: [ResNet18_relu6] Epoch: 046 Train Loss: 0.1529 Train Acc: 0.9457 Eval Loss: 0.4089 Eval Acc: 0.8791 (LR: 0.001000)
[2025-05-13 22:46:38,794]: [ResNet18_relu6] Epoch: 047 Train Loss: 0.1414 Train Acc: 0.9506 Eval Loss: 0.4039 Eval Acc: 0.8867 (LR: 0.001000)
[2025-05-13 22:48:00,425]: [ResNet18_relu6] Epoch: 048 Train Loss: 0.1378 Train Acc: 0.9509 Eval Loss: 0.4226 Eval Acc: 0.8834 (LR: 0.001000)
[2025-05-13 22:49:22,542]: [ResNet18_relu6] Epoch: 049 Train Loss: 0.1429 Train Acc: 0.9489 Eval Loss: 0.4211 Eval Acc: 0.8821 (LR: 0.001000)
[2025-05-13 22:50:43,787]: [ResNet18_relu6] Epoch: 050 Train Loss: 0.1386 Train Acc: 0.9513 Eval Loss: 0.4274 Eval Acc: 0.8817 (LR: 0.001000)
[2025-05-13 22:52:05,020]: [ResNet18_relu6] Epoch: 051 Train Loss: 0.1289 Train Acc: 0.9539 Eval Loss: 0.4323 Eval Acc: 0.8829 (LR: 0.001000)
[2025-05-13 22:53:24,996]: [ResNet18_relu6] Epoch: 052 Train Loss: 0.1248 Train Acc: 0.9547 Eval Loss: 0.4221 Eval Acc: 0.8868 (LR: 0.001000)
[2025-05-13 22:54:58,863]: [ResNet18_relu6] Epoch: 053 Train Loss: 0.1204 Train Acc: 0.9579 Eval Loss: 0.4388 Eval Acc: 0.8835 (LR: 0.001000)
[2025-05-13 22:58:28,781]: [ResNet18_relu6] Epoch: 054 Train Loss: 0.1191 Train Acc: 0.9583 Eval Loss: 0.4045 Eval Acc: 0.8937 (LR: 0.001000)
[2025-05-13 23:01:52,314]: [ResNet18_relu6] Epoch: 055 Train Loss: 0.1129 Train Acc: 0.9600 Eval Loss: 0.4153 Eval Acc: 0.8882 (LR: 0.001000)
[2025-05-13 23:04:50,547]: [ResNet18_relu6] Epoch: 056 Train Loss: 0.1112 Train Acc: 0.9607 Eval Loss: 0.3956 Eval Acc: 0.8962 (LR: 0.001000)
[2025-05-13 23:08:15,777]: [ResNet18_relu6] Epoch: 057 Train Loss: 0.1111 Train Acc: 0.9607 Eval Loss: 0.4479 Eval Acc: 0.8823 (LR: 0.001000)
[2025-05-13 23:12:21,699]: [ResNet18_relu6] Epoch: 058 Train Loss: 0.1047 Train Acc: 0.9630 Eval Loss: 0.3861 Eval Acc: 0.8956 (LR: 0.001000)
[2025-05-13 23:17:54,212]: [ResNet18_relu6] Epoch: 059 Train Loss: 0.1058 Train Acc: 0.9620 Eval Loss: 0.3891 Eval Acc: 0.8945 (LR: 0.001000)
[2025-05-13 23:23:10,766]: [ResNet18_relu6] Epoch: 060 Train Loss: 0.1012 Train Acc: 0.9640 Eval Loss: 0.3852 Eval Acc: 0.8960 (LR: 0.001000)
[2025-05-13 23:28:29,553]: [ResNet18_relu6] Epoch: 061 Train Loss: 0.0980 Train Acc: 0.9652 Eval Loss: 0.4454 Eval Acc: 0.8855 (LR: 0.001000)
[2025-05-13 23:33:48,580]: [ResNet18_relu6] Epoch: 062 Train Loss: 0.0981 Train Acc: 0.9660 Eval Loss: 0.4512 Eval Acc: 0.8848 (LR: 0.001000)
[2025-05-13 23:38:19,574]: [ResNet18_relu6] Epoch: 063 Train Loss: 0.0939 Train Acc: 0.9665 Eval Loss: 0.4063 Eval Acc: 0.8966 (LR: 0.001000)
[2025-05-13 23:42:35,029]: [ResNet18_relu6] Epoch: 064 Train Loss: 0.0924 Train Acc: 0.9682 Eval Loss: 0.3954 Eval Acc: 0.8984 (LR: 0.001000)
[2025-05-13 23:48:08,795]: [ResNet18_relu6] Epoch: 065 Train Loss: 0.0886 Train Acc: 0.9688 Eval Loss: 0.3735 Eval Acc: 0.9035 (LR: 0.001000)
[2025-05-13 23:53:18,656]: [ResNet18_relu6] Epoch: 066 Train Loss: 0.0875 Train Acc: 0.9693 Eval Loss: 0.4196 Eval Acc: 0.8932 (LR: 0.001000)
[2025-05-13 23:56:05,569]: [ResNet18_relu6] Epoch: 067 Train Loss: 0.0825 Train Acc: 0.9708 Eval Loss: 0.3965 Eval Acc: 0.8980 (LR: 0.001000)
[2025-05-14 00:01:12,242]: [ResNet18_relu6] Epoch: 068 Train Loss: 0.0837 Train Acc: 0.9698 Eval Loss: 0.4135 Eval Acc: 0.8952 (LR: 0.001000)
[2025-05-14 00:06:49,214]: [ResNet18_relu6] Epoch: 069 Train Loss: 0.0842 Train Acc: 0.9707 Eval Loss: 0.4247 Eval Acc: 0.8939 (LR: 0.001000)
[2025-05-14 00:12:04,479]: [ResNet18_relu6] Epoch: 070 Train Loss: 0.0757 Train Acc: 0.9732 Eval Loss: 0.4233 Eval Acc: 0.8951 (LR: 0.000100)
[2025-05-14 00:17:20,545]: [ResNet18_relu6] Epoch: 071 Train Loss: 0.0513 Train Acc: 0.9832 Eval Loss: 0.3481 Eval Acc: 0.9104 (LR: 0.000100)
[2025-05-14 00:22:31,844]: [ResNet18_relu6] Epoch: 072 Train Loss: 0.0411 Train Acc: 0.9879 Eval Loss: 0.3456 Eval Acc: 0.9117 (LR: 0.000100)
[2025-05-14 00:26:28,091]: [ResNet18_relu6] Epoch: 073 Train Loss: 0.0378 Train Acc: 0.9884 Eval Loss: 0.3559 Eval Acc: 0.9089 (LR: 0.000100)
[2025-05-14 00:29:06,138]: [ResNet18_relu6] Epoch: 074 Train Loss: 0.0361 Train Acc: 0.9892 Eval Loss: 0.3491 Eval Acc: 0.9112 (LR: 0.000100)
[2025-05-14 00:30:31,150]: [ResNet18_relu6] Epoch: 075 Train Loss: 0.0353 Train Acc: 0.9894 Eval Loss: 0.3503 Eval Acc: 0.9120 (LR: 0.000100)
[2025-05-14 00:31:56,759]: [ResNet18_relu6] Epoch: 076 Train Loss: 0.0347 Train Acc: 0.9890 Eval Loss: 0.3509 Eval Acc: 0.9104 (LR: 0.000100)
[2025-05-14 00:33:22,682]: [ResNet18_relu6] Epoch: 077 Train Loss: 0.0326 Train Acc: 0.9907 Eval Loss: 0.3498 Eval Acc: 0.9105 (LR: 0.000100)
[2025-05-14 00:34:44,396]: [ResNet18_relu6] Epoch: 078 Train Loss: 0.0309 Train Acc: 0.9910 Eval Loss: 0.3530 Eval Acc: 0.9120 (LR: 0.000100)
[2025-05-14 00:36:04,387]: [ResNet18_relu6] Epoch: 079 Train Loss: 0.0305 Train Acc: 0.9910 Eval Loss: 0.3496 Eval Acc: 0.9112 (LR: 0.000100)
[2025-05-14 00:37:24,731]: [ResNet18_relu6] Epoch: 080 Train Loss: 0.0309 Train Acc: 0.9911 Eval Loss: 0.3496 Eval Acc: 0.9123 (LR: 0.000100)
[2025-05-14 00:38:47,884]: [ResNet18_relu6] Epoch: 081 Train Loss: 0.0294 Train Acc: 0.9912 Eval Loss: 0.3539 Eval Acc: 0.9114 (LR: 0.000100)
[2025-05-14 00:40:08,243]: [ResNet18_relu6] Epoch: 082 Train Loss: 0.0296 Train Acc: 0.9913 Eval Loss: 0.3525 Eval Acc: 0.9107 (LR: 0.000100)
[2025-05-14 00:41:27,152]: [ResNet18_relu6] Epoch: 083 Train Loss: 0.0304 Train Acc: 0.9913 Eval Loss: 0.3531 Eval Acc: 0.9117 (LR: 0.000100)
[2025-05-14 00:42:46,089]: [ResNet18_relu6] Epoch: 084 Train Loss: 0.0285 Train Acc: 0.9916 Eval Loss: 0.3539 Eval Acc: 0.9100 (LR: 0.000100)
[2025-05-14 00:44:09,170]: [ResNet18_relu6] Epoch: 085 Train Loss: 0.0281 Train Acc: 0.9919 Eval Loss: 0.3574 Eval Acc: 0.9108 (LR: 0.000100)
[2025-05-14 00:45:50,612]: [ResNet18_relu6] Epoch: 086 Train Loss: 0.0282 Train Acc: 0.9919 Eval Loss: 0.3595 Eval Acc: 0.9112 (LR: 0.000100)
[2025-05-14 00:47:27,446]: [ResNet18_relu6] Epoch: 087 Train Loss: 0.0283 Train Acc: 0.9916 Eval Loss: 0.3567 Eval Acc: 0.9104 (LR: 0.000100)
[2025-05-14 00:49:04,279]: [ResNet18_relu6] Epoch: 088 Train Loss: 0.0268 Train Acc: 0.9924 Eval Loss: 0.3584 Eval Acc: 0.9103 (LR: 0.000100)
[2025-05-14 00:50:41,842]: [ResNet18_relu6] Epoch: 089 Train Loss: 0.0274 Train Acc: 0.9927 Eval Loss: 0.3575 Eval Acc: 0.9115 (LR: 0.000100)
[2025-05-14 00:52:22,981]: [ResNet18_relu6] Epoch: 090 Train Loss: 0.0272 Train Acc: 0.9919 Eval Loss: 0.3642 Eval Acc: 0.9111 (LR: 0.000100)
[2025-05-14 00:54:04,504]: [ResNet18_relu6] Epoch: 091 Train Loss: 0.0264 Train Acc: 0.9923 Eval Loss: 0.3603 Eval Acc: 0.9113 (LR: 0.000100)
[2025-05-14 00:55:44,078]: [ResNet18_relu6] Epoch: 092 Train Loss: 0.0246 Train Acc: 0.9934 Eval Loss: 0.3564 Eval Acc: 0.9104 (LR: 0.000100)
[2025-05-14 00:57:21,364]: [ResNet18_relu6] Epoch: 093 Train Loss: 0.0275 Train Acc: 0.9916 Eval Loss: 0.3612 Eval Acc: 0.9112 (LR: 0.000100)
[2025-05-14 00:58:58,627]: [ResNet18_relu6] Epoch: 094 Train Loss: 0.0255 Train Acc: 0.9929 Eval Loss: 0.3648 Eval Acc: 0.9099 (LR: 0.000100)
[2025-05-14 01:00:35,895]: [ResNet18_relu6] Epoch: 095 Train Loss: 0.0249 Train Acc: 0.9929 Eval Loss: 0.3623 Eval Acc: 0.9108 (LR: 0.000100)
[2025-05-14 01:02:12,940]: [ResNet18_relu6] Epoch: 096 Train Loss: 0.0243 Train Acc: 0.9932 Eval Loss: 0.3583 Eval Acc: 0.9127 (LR: 0.000100)
[2025-05-14 01:03:41,933]: [ResNet18_relu6] Epoch: 097 Train Loss: 0.0255 Train Acc: 0.9919 Eval Loss: 0.3611 Eval Acc: 0.9107 (LR: 0.000100)
[2025-05-14 01:05:04,024]: [ResNet18_relu6] Epoch: 098 Train Loss: 0.0246 Train Acc: 0.9929 Eval Loss: 0.3656 Eval Acc: 0.9099 (LR: 0.000100)
[2025-05-14 01:06:25,715]: [ResNet18_relu6] Epoch: 099 Train Loss: 0.0243 Train Acc: 0.9929 Eval Loss: 0.3644 Eval Acc: 0.9124 (LR: 0.000100)
[2025-05-14 01:07:47,640]: [ResNet18_relu6] Epoch: 100 Train Loss: 0.0237 Train Acc: 0.9929 Eval Loss: 0.3659 Eval Acc: 0.9111 (LR: 0.000010)
[2025-05-14 01:09:08,507]: [ResNet18_relu6] Epoch: 101 Train Loss: 0.0223 Train Acc: 0.9940 Eval Loss: 0.3610 Eval Acc: 0.9121 (LR: 0.000010)
[2025-05-14 01:10:33,623]: [ResNet18_relu6] Epoch: 102 Train Loss: 0.0225 Train Acc: 0.9936 Eval Loss: 0.3639 Eval Acc: 0.9120 (LR: 0.000010)
[2025-05-14 01:11:55,919]: [ResNet18_relu6] Epoch: 103 Train Loss: 0.0223 Train Acc: 0.9938 Eval Loss: 0.3630 Eval Acc: 0.9122 (LR: 0.000010)
[2025-05-14 01:13:18,226]: [ResNet18_relu6] Epoch: 104 Train Loss: 0.0229 Train Acc: 0.9933 Eval Loss: 0.3618 Eval Acc: 0.9120 (LR: 0.000010)
[2025-05-14 01:14:39,379]: [ResNet18_relu6] Epoch: 105 Train Loss: 0.0226 Train Acc: 0.9939 Eval Loss: 0.3639 Eval Acc: 0.9117 (LR: 0.000010)
[2025-05-14 01:16:00,419]: [ResNet18_relu6] Epoch: 106 Train Loss: 0.0229 Train Acc: 0.9936 Eval Loss: 0.3618 Eval Acc: 0.9133 (LR: 0.000010)
[2025-05-14 01:17:21,362]: [ResNet18_relu6] Epoch: 107 Train Loss: 0.0226 Train Acc: 0.9939 Eval Loss: 0.3624 Eval Acc: 0.9110 (LR: 0.000010)
[2025-05-14 01:18:40,194]: [ResNet18_relu6] Epoch: 108 Train Loss: 0.0218 Train Acc: 0.9941 Eval Loss: 0.3653 Eval Acc: 0.9127 (LR: 0.000010)
[2025-05-14 01:20:07,068]: [ResNet18_relu6] Epoch: 109 Train Loss: 0.0226 Train Acc: 0.9936 Eval Loss: 0.3626 Eval Acc: 0.9129 (LR: 0.000010)
[2025-05-14 01:21:27,910]: [ResNet18_relu6] Epoch: 110 Train Loss: 0.0211 Train Acc: 0.9942 Eval Loss: 0.3628 Eval Acc: 0.9120 (LR: 0.000010)
[2025-05-14 01:22:46,749]: [ResNet18_relu6] Epoch: 111 Train Loss: 0.0227 Train Acc: 0.9939 Eval Loss: 0.3633 Eval Acc: 0.9129 (LR: 0.000010)
[2025-05-14 01:24:05,287]: [ResNet18_relu6] Epoch: 112 Train Loss: 0.0218 Train Acc: 0.9940 Eval Loss: 0.3638 Eval Acc: 0.9135 (LR: 0.000010)
[2025-05-14 01:25:24,134]: [ResNet18_relu6] Epoch: 113 Train Loss: 0.0217 Train Acc: 0.9943 Eval Loss: 0.3628 Eval Acc: 0.9128 (LR: 0.000010)
[2025-05-14 01:26:43,004]: [ResNet18_relu6] Epoch: 114 Train Loss: 0.0219 Train Acc: 0.9938 Eval Loss: 0.3638 Eval Acc: 0.9128 (LR: 0.000010)
[2025-05-14 01:28:04,204]: [ResNet18_relu6] Epoch: 115 Train Loss: 0.0226 Train Acc: 0.9937 Eval Loss: 0.3635 Eval Acc: 0.9118 (LR: 0.000010)
[2025-05-14 01:29:25,178]: [ResNet18_relu6] Epoch: 116 Train Loss: 0.0217 Train Acc: 0.9938 Eval Loss: 0.3649 Eval Acc: 0.9101 (LR: 0.000010)
[2025-05-14 01:30:44,257]: [ResNet18_relu6] Epoch: 117 Train Loss: 0.0215 Train Acc: 0.9939 Eval Loss: 0.3637 Eval Acc: 0.9119 (LR: 0.000010)
[2025-05-14 01:32:05,272]: [ResNet18_relu6] Epoch: 118 Train Loss: 0.0204 Train Acc: 0.9948 Eval Loss: 0.3637 Eval Acc: 0.9120 (LR: 0.000010)
[2025-05-14 01:33:26,179]: [ResNet18_relu6] Epoch: 119 Train Loss: 0.0223 Train Acc: 0.9936 Eval Loss: 0.3655 Eval Acc: 0.9121 (LR: 0.000010)
[2025-05-14 01:34:49,864]: [ResNet18_relu6] Epoch: 120 Train Loss: 0.0223 Train Acc: 0.9932 Eval Loss: 0.3638 Eval Acc: 0.9116 (LR: 0.000010)
[2025-05-14 01:34:49,864]: [ResNet18_relu6] Best Eval Accuracy: 0.9135
[2025-05-14 01:34:49,940]: 
Training of full-precision model finished!
[2025-05-14 01:34:49,940]: Model Architecture:
[2025-05-14 01:34:49,947]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 01:34:49,948]: 
Model Weights:
[2025-05-14 01:34:49,948]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 01:34:49,986]: Sample Values (25 elements): [-0.13207168877124786, 0.18569083511829376, 0.2582228481769562, 0.05150465667247772, 0.06215021014213562, 0.021942507475614548, 0.22059530019760132, -0.006358141545206308, -0.03726764768362045, 0.0727451890707016, -0.14159740507602692, -0.044539324939250946, -0.0003189854323863983, -0.21559490263462067, 0.12949004769325256, 0.07687351107597351, 0.17018091678619385, 0.09782630205154419, 0.23186610639095306, 0.02568996138870716, -0.036857474595308304, -0.040502022951841354, 0.10864034295082092, 0.1624378114938736, 0.03781510144472122]
[2025-05-14 01:34:50,016]: Mean: 0.00001384
[2025-05-14 01:34:50,030]: Min: -0.33939183
[2025-05-14 01:34:50,034]: Max: 0.35612306
[2025-05-14 01:34:50,034]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 01:34:50,035]: Sample Values (25 elements): [0.9319919943809509, 0.98175448179245, 0.9689877033233643, 0.939965546131134, 0.9315162897109985, 0.9604727625846863, 0.9109077453613281, 0.9166940450668335, 0.9515942931175232, 0.9673910140991211, 0.9383724927902222, 0.987977147102356, 0.9356114268302917, 0.9522731900215149, 0.9209194779396057, 0.969167947769165, 1.008184552192688, 0.9756921529769897, 0.990398645401001, 0.9520004391670227, 0.9835382103919983, 0.9537626504898071, 1.0096144676208496, 1.0357649326324463, 0.9688148498535156]
[2025-05-14 01:34:50,035]: Mean: 0.95694673
[2025-05-14 01:34:50,035]: Min: 0.84303373
[2025-05-14 01:34:50,036]: Max: 1.05538368
[2025-05-14 01:34:50,036]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 01:34:50,036]: Sample Values (25 elements): [-0.03363977000117302, -0.015359746292233467, -0.019399816170334816, 0.05034615099430084, 0.013415095396339893, 0.044824741780757904, 0.024845615029335022, 0.0411243736743927, 0.01871800422668457, 0.028237389400601387, -0.005217906553298235, 0.032847754657268524, -0.026409532874822617, -0.03632784262299538, -0.011697187088429928, -0.011619283817708492, -0.033123694360256195, 0.05510415509343147, -0.01111834216862917, -0.04259413853287697, 0.03476272150874138, 0.0007115030894055963, 0.02406514249742031, 0.008995735086500645, -0.00477222353219986]
[2025-05-14 01:34:50,036]: Mean: -0.00130723
[2025-05-14 01:34:50,037]: Min: -0.12522221
[2025-05-14 01:34:50,037]: Max: 0.12971091
[2025-05-14 01:34:50,037]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 01:34:50,037]: Sample Values (25 elements): [0.9446013569831848, 0.9629989266395569, 0.9506843090057373, 0.9459478259086609, 0.9474606513977051, 0.95335453748703, 0.9674299955368042, 0.949984610080719, 0.9753862023353577, 0.9558141231536865, 0.9405717253684998, 0.9936079382896423, 0.9664466381072998, 0.9641578197479248, 0.9614351391792297, 0.9635202288627625, 0.9744710922241211, 0.9589830040931702, 0.9693632125854492, 0.9502460360527039, 1.0883864164352417, 0.9807114601135254, 0.970080554485321, 0.9563013911247253, 0.9864236116409302]
[2025-05-14 01:34:50,037]: Mean: 0.97184509
[2025-05-14 01:34:50,037]: Min: 0.93655932
[2025-05-14 01:34:50,038]: Max: 1.08838642
[2025-05-14 01:34:50,038]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 01:34:50,038]: Sample Values (25 elements): [-0.030145864933729172, 0.039627570658922195, -0.0144440196454525, -0.0035236382391303778, 0.025077801197767258, -0.029300985857844353, -0.029524091631174088, -0.0012543509947136045, -0.0033938868436962366, -0.03109462559223175, -0.03914221003651619, -0.015002910047769547, -0.026414893567562103, -0.012698938138782978, -0.022317640483379364, 0.05728694424033165, 0.0012867505429312587, -0.0035587751772254705, 0.020680898800492287, 0.016616826876997948, 0.014439976774156094, 0.03277827799320221, -0.02538571134209633, -0.010622052475810051, 0.01880442351102829]
[2025-05-14 01:34:50,038]: Mean: -0.00090825
[2025-05-14 01:34:50,039]: Min: -0.11501443
[2025-05-14 01:34:50,039]: Max: 0.10492913
[2025-05-14 01:34:50,039]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 01:34:50,039]: Sample Values (25 elements): [1.0330497026443481, 0.9729530811309814, 0.9736279249191284, 0.9788811206817627, 0.9713500142097473, 0.9791812300682068, 0.978489339351654, 0.9673904776573181, 0.9625751376152039, 0.9598222374916077, 1.0154774188995361, 0.9832231998443604, 0.9795098304748535, 0.9646108746528625, 0.9893662929534912, 0.9725046157836914, 0.9638056755065918, 0.9955983757972717, 0.969918429851532, 0.9559792876243591, 0.9575245976448059, 0.964773416519165, 0.9632981419563293, 0.9717984199523926, 0.984809160232544]
[2025-05-14 01:34:50,039]: Mean: 0.97868788
[2025-05-14 01:34:50,039]: Min: 0.93881869
[2025-05-14 01:34:50,040]: Max: 1.04455769
[2025-05-14 01:34:50,040]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 01:34:50,040]: Sample Values (25 elements): [0.016069253906607628, -0.04743978753685951, -0.00928952731192112, 0.014273449778556824, -0.03443367779254913, 0.020009486004710197, -0.02619459107518196, 0.010748938657343388, -0.011568103916943073, 0.026596076786518097, -0.025739796459674835, -0.013231119140982628, -0.016653982922434807, 0.027036283165216446, -0.013731501065194607, -0.03002985753118992, 0.0005611785454675555, 0.05415451154112816, -0.03411513939499855, 0.024137208238244057, 0.02943447045981884, 0.02571840211749077, 0.02102254517376423, -0.011738182045519352, 0.010428329929709435]
[2025-05-14 01:34:50,041]: Mean: -0.00113196
[2025-05-14 01:34:50,041]: Min: -0.09077396
[2025-05-14 01:34:50,041]: Max: 0.08246885
[2025-05-14 01:34:50,041]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 01:34:50,041]: Sample Values (25 elements): [0.9759136438369751, 0.9699311852455139, 0.9604809284210205, 0.9738889336585999, 0.962048351764679, 0.970493733882904, 0.9939491152763367, 0.9582430720329285, 0.9671534895896912, 0.9646478295326233, 0.9758524298667908, 0.9733914732933044, 0.9608541131019592, 0.9749332666397095, 0.9871847033500671, 0.9651843309402466, 0.9639279246330261, 0.9791719317436218, 0.9648762941360474, 0.968923032283783, 0.9665963649749756, 0.9671909213066101, 0.9654349088668823, 0.9715025424957275, 0.9793083667755127]
[2025-05-14 01:34:50,041]: Mean: 0.97167099
[2025-05-14 01:34:50,042]: Min: 0.95097911
[2025-05-14 01:34:50,042]: Max: 0.99713957
[2025-05-14 01:34:50,042]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 01:34:50,042]: Sample Values (25 elements): [0.03598599135875702, -0.012689448893070221, -0.00409266073256731, 0.010339884087443352, -0.015984833240509033, 0.01760292612016201, 0.009399721398949623, 0.01873314380645752, 0.004548691213130951, -0.010427256114780903, 0.02737753838300705, -0.023233957588672638, -0.03719078004360199, 0.009555785916745663, 0.006484388839453459, -0.0060462006367743015, -0.016627449542284012, 0.0036588087677955627, -0.04168831184506416, 0.014518017880618572, 0.0407855361700058, -0.021281449124217033, 0.03959593549370766, -0.02394554391503334, 0.0477791503071785]
[2025-05-14 01:34:50,043]: Mean: -0.00034564
[2025-05-14 01:34:50,043]: Min: -0.08140063
[2025-05-14 01:34:50,043]: Max: 0.08078214
[2025-05-14 01:34:50,043]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 01:34:50,043]: Sample Values (25 elements): [1.0003563165664673, 0.9977322816848755, 0.9824957251548767, 0.9740030765533447, 0.9928628206253052, 0.9904999732971191, 0.9710623025894165, 0.9615455865859985, 0.9723492860794067, 0.9795670509338379, 0.9790657162666321, 0.9912979006767273, 0.9675976037979126, 0.9853277206420898, 0.9747999310493469, 0.968599259853363, 0.9883229732513428, 0.990174412727356, 0.9741472601890564, 0.9771568179130554, 0.9882216453552246, 0.9751683473587036, 0.9716117978096008, 0.9927200078964233, 1.0072674751281738]
[2025-05-14 01:34:50,043]: Mean: 0.98285204
[2025-05-14 01:34:50,043]: Min: 0.96154559
[2025-05-14 01:34:50,044]: Max: 1.00945330
[2025-05-14 01:34:50,044]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 01:34:50,044]: Sample Values (25 elements): [-0.04768253490328789, -0.008312360383570194, 0.024448398500680923, -0.02600020542740822, -0.019557351246476173, 0.030806025490164757, 0.04210478812456131, 0.012001379393041134, 0.020396586507558823, 0.00771346827968955, -0.020192274823784828, -0.021309534087777138, 0.031092040240764618, -0.02560610882937908, -0.010268145240843296, 0.021969057619571686, -0.0008118762052617967, -0.020989997312426567, 0.03291581943631172, 0.029649604111909866, 0.0014989383053034544, -0.012747702188789845, 0.0006161665660329163, -0.03746515139937401, -0.048039816319942474]
[2025-05-14 01:34:50,045]: Mean: -0.00044986
[2025-05-14 01:34:50,045]: Min: -0.07023788
[2025-05-14 01:34:50,045]: Max: 0.07205325
[2025-05-14 01:34:50,045]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 01:34:50,045]: Sample Values (25 elements): [0.9808225631713867, 0.9666680693626404, 0.9762200713157654, 0.9677150249481201, 0.9653139114379883, 0.9825023412704468, 0.972426176071167, 0.9690194725990295, 0.9693028926849365, 0.9683804512023926, 0.9761084914207458, 0.9694297909736633, 0.9755805730819702, 0.9665427803993225, 0.9743734002113342, 0.9677929282188416, 0.9666059613227844, 0.9786398410797119, 0.9850848913192749, 0.9730958938598633, 0.9756069779396057, 0.9768804907798767, 0.9614550471305847, 0.9700648188591003, 0.964706301689148]
[2025-05-14 01:34:50,045]: Mean: 0.97173744
[2025-05-14 01:34:50,046]: Min: 0.95739305
[2025-05-14 01:34:50,046]: Max: 0.98619074
[2025-05-14 01:34:50,046]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 01:34:50,047]: Sample Values (25 elements): [0.007638821844011545, -0.02704519033432007, -0.006859331391751766, 0.02596057392656803, 0.004558760207146406, 0.012764167040586472, 0.011947689577937126, 0.008497423492372036, -0.024081772193312645, 0.012438418343663216, -0.022010480985045433, -0.020885813981294632, -0.0021735618356615305, 0.005689444951713085, -0.017161168158054352, -0.0066139777190983295, -0.0048059700056910515, -0.005317137110978365, 0.01028995867818594, 0.01718871481716633, 0.012644674628973007, -0.004822187591344118, -0.021673789247870445, -0.025967691093683243, -0.018070723861455917]
[2025-05-14 01:34:50,047]: Mean: -0.00045367
[2025-05-14 01:34:50,048]: Min: -0.06303569
[2025-05-14 01:34:50,048]: Max: 0.06490874
[2025-05-14 01:34:50,048]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 01:34:50,048]: Sample Values (25 elements): [0.9767796993255615, 0.9657467007637024, 0.9780024290084839, 0.9764222502708435, 0.9608774781227112, 0.9651039242744446, 0.9727433919906616, 0.9759228229522705, 0.9648529291152954, 0.9815641045570374, 0.9726387858390808, 0.9751734137535095, 0.985207736492157, 0.9657668471336365, 0.9923098087310791, 0.9793649911880493, 0.9638190865516663, 0.9739129543304443, 0.9742999076843262, 0.9656781554222107, 0.9593082070350647, 0.9681150913238525, 0.9746873378753662, 0.9714189767837524, 0.972925066947937]
[2025-05-14 01:34:50,048]: Mean: 0.97270072
[2025-05-14 01:34:50,048]: Min: 0.95819134
[2025-05-14 01:34:50,049]: Max: 0.99230981
[2025-05-14 01:34:50,049]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 01:34:50,049]: Sample Values (25 elements): [0.01590822823345661, 0.03535205125808716, 0.03901184722781181, 0.030218573287129402, -0.04198581725358963, -0.030322913080453873, 0.09476573765277863, -0.10439808666706085, -0.023590881377458572, -0.03426172584295273, 0.07772643864154816, -0.11408422142267227, 0.007396150846034288, -0.039249662309885025, 0.037223607301712036, -0.054915376007556915, 0.03717172145843506, 0.08489079773426056, -0.06677430868148804, 0.03155458718538284, 0.10047072172164917, -0.11857657879590988, 0.040067970752716064, 0.11270517855882645, -0.0054419878870248795]
[2025-05-14 01:34:50,049]: Mean: 0.00138367
[2025-05-14 01:34:50,049]: Min: -0.14785311
[2025-05-14 01:34:50,049]: Max: 0.14476538
[2025-05-14 01:34:50,050]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 01:34:50,050]: Sample Values (25 elements): [0.9512061476707458, 0.9548323154449463, 0.9471309781074524, 0.9518934488296509, 0.9652872681617737, 0.9421312808990479, 0.9545280933380127, 0.9534106850624084, 0.9472838640213013, 0.9482742547988892, 0.9627286791801453, 0.9514013528823853, 0.9548680782318115, 0.9556785225868225, 0.9677674770355225, 0.956745445728302, 0.9458699226379395, 0.9611042737960815, 0.9492387771606445, 0.954198956489563, 0.950756311416626, 0.951142430305481, 0.9527794122695923, 0.9569602012634277, 0.9644673466682434]
[2025-05-14 01:34:50,050]: Mean: 0.95526445
[2025-05-14 01:34:50,050]: Min: 0.93096930
[2025-05-14 01:34:50,050]: Max: 0.96998864
[2025-05-14 01:34:50,050]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 01:34:50,052]: Sample Values (25 elements): [-0.0005994181265123188, 0.011066310107707977, 0.002828798722475767, 0.0048917075619101524, -0.005778320133686066, 0.03898371383547783, 0.018410734832286835, -0.005004362668842077, -0.02353259176015854, 0.00907307118177414, 0.019037658348679543, 0.03937619924545288, 0.0005543200531974435, -0.006472067441791296, -0.01022114884108305, 0.017793899402022362, -0.019123122096061707, -0.02091081626713276, -0.015514465980231762, 0.024145090952515602, 0.0017855275655165315, 0.02727172151207924, 0.028180615976452827, 0.007114473730325699, 0.026141734793782234]
[2025-05-14 01:34:50,052]: Mean: -0.00039763
[2025-05-14 01:34:50,053]: Min: -0.05438225
[2025-05-14 01:34:50,053]: Max: 0.06454821
[2025-05-14 01:34:50,053]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 01:34:50,053]: Sample Values (25 elements): [0.9779382944107056, 0.9805471301078796, 0.9675390124320984, 0.9614488482475281, 0.9688848257064819, 0.983022928237915, 0.9851136207580566, 0.963599681854248, 0.9694365859031677, 0.9758234620094299, 0.9724592566490173, 0.9660828709602356, 0.9709979891777039, 0.9646696448326111, 0.9680572748184204, 0.9677596092224121, 0.9684710502624512, 0.9714568853378296, 0.9604255557060242, 0.9782865643501282, 0.9712268710136414, 0.9639179110527039, 0.9710278511047363, 0.972649335861206, 0.9674946069717407]
[2025-05-14 01:34:50,053]: Mean: 0.97172010
[2025-05-14 01:34:50,053]: Min: 0.95948017
[2025-05-14 01:34:50,054]: Max: 0.98964292
[2025-05-14 01:34:50,054]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 01:34:50,055]: Sample Values (25 elements): [0.027431868016719818, 0.009505946189165115, -0.0009281341917812824, -0.029559150338172913, 0.009828283451497555, -0.008614858612418175, -0.017345953732728958, 0.01859487034380436, -0.0038252624217420816, 0.025233784690499306, -0.010936222970485687, -0.030426569283008575, 0.015623199753463268, 0.015304534696042538, 0.0009429424535483122, 0.03271254897117615, 0.011384918354451656, 0.023773526772856712, 0.015747463330626488, -0.015352722257375717, 0.012170201167464256, -0.02292785234749317, -0.031098945066332817, -0.022846292704343796, 0.0009343972196802497]
[2025-05-14 01:34:50,056]: Mean: -0.00018063
[2025-05-14 01:34:50,056]: Min: -0.05684656
[2025-05-14 01:34:50,056]: Max: 0.06731689
[2025-05-14 01:34:50,056]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 01:34:50,056]: Sample Values (25 elements): [1.0033471584320068, 0.978088915348053, 0.9793453216552734, 0.9951009154319763, 0.9758682250976562, 0.9852914810180664, 0.9816330075263977, 0.97507643699646, 0.98390132188797, 0.975646436214447, 0.9747664332389832, 0.9840059280395508, 0.9787744283676147, 0.9832465648651123, 0.9838897585868835, 0.9827009439468384, 0.986833930015564, 0.97115159034729, 0.9742704033851624, 0.985329806804657, 0.9850354790687561, 0.9810357093811035, 0.9952471256256104, 0.9979897737503052, 0.9829959273338318]
[2025-05-14 01:34:50,057]: Mean: 0.98549891
[2025-05-14 01:34:50,057]: Min: 0.97115159
[2025-05-14 01:34:50,057]: Max: 1.01013505
[2025-05-14 01:34:50,057]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 01:34:50,060]: Sample Values (25 elements): [-0.001169324154034257, 0.011380758136510849, 0.021995380520820618, -0.01199159026145935, 0.020723959431052208, -0.01224361453205347, 0.022367427125573158, -0.00932661909610033, 0.032205116003751755, -0.011256562545895576, -0.016319727525115013, 0.02421904355287552, 0.00589978089556098, 0.002277692314237356, -0.004695445764809847, -0.007828868925571442, -0.02622184343636036, -0.009679475799202919, -0.024770041927695274, 0.0013371006352826953, 0.0015873145312070847, -0.019195422530174255, -0.027386676520109177, 0.0022024044301360846, 0.0296779852360487]
[2025-05-14 01:34:50,060]: Mean: -0.00018893
[2025-05-14 01:34:50,060]: Min: -0.05020761
[2025-05-14 01:34:50,060]: Max: 0.05017703
[2025-05-14 01:34:50,060]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 01:34:50,060]: Sample Values (25 elements): [0.9752180576324463, 0.9662697911262512, 0.9717194437980652, 0.9717365503311157, 0.9730554223060608, 0.97160804271698, 0.9761576652526855, 0.9667936563491821, 0.9691903591156006, 0.9649383425712585, 0.9747474789619446, 0.9710224270820618, 0.9661139845848083, 0.9802826642990112, 0.9558542966842651, 0.9738417863845825, 0.9706814885139465, 0.9758450984954834, 0.9667426347732544, 0.9724132418632507, 0.9786762595176697, 0.9693565964698792, 0.9744829535484314, 0.9703667759895325, 0.9701754450798035]
[2025-05-14 01:34:50,061]: Mean: 0.97176754
[2025-05-14 01:34:50,061]: Min: 0.95585430
[2025-05-14 01:34:50,061]: Max: 0.98513997
[2025-05-14 01:34:50,061]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 01:34:50,067]: Sample Values (25 elements): [-0.008653939701616764, 0.01215975172817707, -0.00330846244469285, -0.015285329893231392, -0.022361086681485176, 0.009311392903327942, 0.004155572969466448, -0.02122405171394348, 0.01282932423055172, 0.008029257878661156, -0.006617092061787844, 0.001341888913884759, -0.007478710729628801, 0.007587858941406012, -0.00476421695202589, 0.0030662885401397943, 0.01351823378354311, -0.02175154723227024, -0.0166182778775692, 0.004032745957374573, 0.025918392464518547, 0.019577868282794952, 0.019213106483221054, -0.017455417662858963, 0.008682068437337875]
[2025-05-14 01:34:50,067]: Mean: -0.00036922
[2025-05-14 01:34:50,067]: Min: -0.04051075
[2025-05-14 01:34:50,068]: Max: 0.04499184
[2025-05-14 01:34:50,068]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 01:34:50,068]: Sample Values (25 elements): [0.971953272819519, 0.9761683344841003, 0.9736803770065308, 0.9836556911468506, 0.9734840989112854, 0.9730047583580017, 0.9778091907501221, 0.975683331489563, 0.9770650267601013, 0.9758397936820984, 0.9757108092308044, 0.9674277305603027, 0.9774889945983887, 0.9679449200630188, 0.9777675867080688, 0.9749481081962585, 0.9865182638168335, 0.9762685894966125, 0.9803493618965149, 0.9791390895843506, 0.9705043435096741, 0.9805107712745667, 0.975371778011322, 0.969276487827301, 0.9815397262573242]
[2025-05-14 01:34:50,068]: Mean: 0.97554564
[2025-05-14 01:34:50,068]: Min: 0.96220118
[2025-05-14 01:34:50,068]: Max: 0.98836327
[2025-05-14 01:34:50,068]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 01:34:50,069]: Sample Values (25 elements): [0.08084382116794586, -0.022707220166921616, 0.08562730252742767, 0.03891647607088089, 0.03154582530260086, -0.015757087618112564, 0.0073704044334590435, -0.037107620388269424, 0.0014701054897159338, -0.012852407991886139, -0.028513919562101364, -0.02856079861521721, 0.033346034586429596, -0.05643852427601814, -0.03275632485747337, 0.05277397111058235, 0.07419740408658981, 0.046289414167404175, 0.028241079300642014, -0.08557385951280594, -0.03157700225710869, 0.015910102054476738, -0.06717956811189651, 0.02793882228434086, -0.07578758150339127]
[2025-05-14 01:34:50,069]: Mean: -0.00001468
[2025-05-14 01:34:50,069]: Min: -0.10107810
[2025-05-14 01:34:50,069]: Max: 0.10427763
[2025-05-14 01:34:50,069]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 01:34:50,070]: Sample Values (25 elements): [0.962622344493866, 0.9591975212097168, 0.9566149711608887, 0.9573361277580261, 0.9612414836883545, 0.9602862596511841, 0.9608950614929199, 0.9554092288017273, 0.960435688495636, 0.9634009003639221, 0.9596922397613525, 0.9573784470558167, 0.9586337208747864, 0.9583230018615723, 0.9529240727424622, 0.950808584690094, 0.9595693349838257, 0.9657490849494934, 0.9613334536552429, 0.9586167931556702, 0.9587467312812805, 0.9494888186454773, 0.9568688273429871, 0.95416659116745, 0.9625415205955505]
[2025-05-14 01:34:50,070]: Mean: 0.95872992
[2025-05-14 01:34:50,070]: Min: 0.94948882
[2025-05-14 01:34:50,070]: Max: 0.96985859
[2025-05-14 01:34:50,070]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 01:34:50,075]: Sample Values (25 elements): [-0.00690757529810071, -0.00802005734294653, -0.020057030022144318, -0.022896604612469673, -0.011145524680614471, -0.021702347323298454, -0.011525565758347511, 0.026299038901925087, 0.0009108638041652739, 0.016706353053450584, 0.0055494774132966995, 0.004497581627219915, -0.0038572580087929964, 0.013267271220684052, 0.018074769526720047, 0.013306773267686367, 0.004208743572235107, -0.008513020351529121, -0.012070873752236366, 0.011423680000007153, 0.02362528070807457, -0.01429840549826622, -0.011569206602871418, 0.014420388266444206, -0.0159317534416914]
[2025-05-14 01:34:50,075]: Mean: -0.00034956
[2025-05-14 01:34:50,075]: Min: -0.04326325
[2025-05-14 01:34:50,076]: Max: 0.04342262
[2025-05-14 01:34:50,076]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 01:34:50,076]: Sample Values (25 elements): [0.9800785183906555, 0.975330114364624, 0.9696213006973267, 0.9694935083389282, 0.9714624881744385, 0.9698192477226257, 0.9731564521789551, 0.9697548151016235, 0.9749340415000916, 0.9722798466682434, 0.9753924012184143, 0.9712363481521606, 0.9745556116104126, 0.9749781489372253, 0.9695498943328857, 0.9747049808502197, 0.968799889087677, 0.9683796763420105, 0.9720281362533569, 0.9723316431045532, 0.9664325714111328, 0.9731550216674805, 0.9738081693649292, 0.9714645147323608, 0.9672514200210571]
[2025-05-14 01:34:50,076]: Mean: 0.97173983
[2025-05-14 01:34:50,076]: Min: 0.96250880
[2025-05-14 01:34:50,076]: Max: 0.98330271
[2025-05-14 01:34:50,076]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 01:34:50,081]: Sample Values (25 elements): [-0.0054961806163191795, -0.02042570523917675, 0.01902145706117153, 0.017915142700076103, -0.00831250473856926, 0.02639702521264553, -0.006533322390168905, -0.007295443210750818, 0.00227052578702569, -0.017434552311897278, -0.01609310321509838, -0.019437275826931, -0.016676200553774834, -0.014586534351110458, -0.0015656331088393927, 0.007307530380785465, -0.009327095933258533, -0.006984274834394455, 0.01072486862540245, -0.014404432848095894, -0.0018878891132771969, -0.007499720435589552, -0.009025240316987038, -0.019865768030285835, 0.0003507886140141636]
[2025-05-14 01:34:50,081]: Mean: -0.00007571
[2025-05-14 01:34:50,081]: Min: -0.04043076
[2025-05-14 01:34:50,082]: Max: 0.03960036
[2025-05-14 01:34:50,082]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 01:34:50,082]: Sample Values (25 elements): [0.979331374168396, 0.9847351908683777, 0.9819598197937012, 0.9882609248161316, 0.9761187434196472, 0.983485996723175, 0.9900916218757629, 0.9798543453216553, 0.9837243556976318, 0.9807828664779663, 0.9870249032974243, 0.9798276424407959, 0.9799702763557434, 0.9835442304611206, 0.9871140122413635, 0.9792191982269287, 0.9775248765945435, 0.9716057777404785, 0.979312002658844, 0.9783918261528015, 0.9792176485061646, 0.978942334651947, 0.9738404154777527, 0.9865685105323792, 0.976399838924408]
[2025-05-14 01:34:50,082]: Mean: 0.98087907
[2025-05-14 01:34:50,082]: Min: 0.97160578
[2025-05-14 01:34:50,082]: Max: 0.99537134
[2025-05-14 01:34:50,082]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 01:34:50,095]: Sample Values (25 elements): [-0.020759698003530502, 0.017509542405605316, 0.0035147026646882296, 0.002457035705447197, -0.019064955413341522, -0.02010904997587204, 0.00841876957565546, 0.012767002917826176, -0.0118396682664752, -0.000467427889816463, 0.016953112557530403, -0.008225934579968452, -0.0046396637335419655, 0.007787523325532675, 0.019132329151034355, 0.00026023181271739304, -0.008063881658017635, -0.013424749486148357, -0.019340647384524345, -0.0076263658702373505, -0.014531964436173439, 0.018207283690571785, -0.0016344031319022179, 0.01073778048157692, 0.012138108722865582]
[2025-05-14 01:34:50,095]: Mean: -0.00001114
[2025-05-14 01:34:50,095]: Min: -0.03490295
[2025-05-14 01:34:50,095]: Max: 0.03500130
[2025-05-14 01:34:50,096]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 01:34:50,096]: Sample Values (25 elements): [0.9705817699432373, 0.9702398777008057, 0.9708669781684875, 0.9718118906021118, 0.9710399508476257, 0.9697084426879883, 0.9732329249382019, 0.9714444875717163, 0.9727948307991028, 0.973975419998169, 0.9718722701072693, 0.9694778919219971, 0.9728893041610718, 0.9707719683647156, 0.9743096828460693, 0.9716025590896606, 0.9718660712242126, 0.9697992205619812, 0.9714587926864624, 0.9709238409996033, 0.972093939781189, 0.9689170122146606, 0.9722660779953003, 0.9717470407485962, 0.9705459475517273]
[2025-05-14 01:34:50,096]: Mean: 0.97183418
[2025-05-14 01:34:50,096]: Min: 0.96732020
[2025-05-14 01:34:50,096]: Max: 0.97846103
[2025-05-14 01:34:50,096]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 01:34:50,137]: Sample Values (25 elements): [-0.0035227658227086067, 0.004582980647683144, -0.013898150995373726, 0.014844359830021858, 0.0023301297333091497, -0.007372110616415739, -0.00676162913441658, -0.00910861138254404, -0.008220665156841278, 0.014291206374764442, 0.0034520148765295744, 0.0007900205673649907, 0.006092856638133526, 0.016644323244690895, 0.007329465355724096, -0.0032719385344535112, 0.00179526605643332, 0.011379237286746502, -0.0040361941792070866, 0.01449188869446516, -0.005005140323191881, 0.007570834364742041, 0.009758210740983486, -0.0062469979748129845, 0.0015643317019566894]
[2025-05-14 01:34:50,138]: Mean: -0.00002433
[2025-05-14 01:34:50,138]: Min: -0.02450434
[2025-05-14 01:34:50,138]: Max: 0.02682935
[2025-05-14 01:34:50,138]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 01:34:50,140]: Sample Values (25 elements): [0.977667510509491, 0.9756750464439392, 0.9734029769897461, 0.9729283452033997, 0.9793376326560974, 0.9729942083358765, 0.9761318564414978, 0.974993109703064, 0.974228024482727, 0.9749631881713867, 0.9750985503196716, 0.9776110053062439, 0.9738265872001648, 0.9777747392654419, 0.9749212861061096, 0.976624608039856, 0.9773603677749634, 0.9770837426185608, 0.9774544835090637, 0.9745803475379944, 0.9755123853683472, 0.9730627536773682, 0.9792545437812805, 0.9748459458351135, 0.9784317016601562]
[2025-05-14 01:34:50,140]: Mean: 0.97581166
[2025-05-14 01:34:50,140]: Min: 0.97137678
[2025-05-14 01:34:50,140]: Max: 0.98433530
[2025-05-14 01:34:50,140]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 01:34:50,141]: Sample Values (25 elements): [0.05626997724175453, 0.03458932414650917, 0.030595257878303528, -0.050754278898239136, 0.04450162127614021, 0.011844439432024956, -0.007487836293876171, 0.0468546599149704, 0.051761090755462646, 0.04910759627819061, 0.050344180315732956, -0.015644710510969162, 0.059813160449266434, 0.04175538569688797, 0.035366181284189224, 0.060772623866796494, 0.04675734415650368, -0.035651836544275284, 0.04559877887368202, -0.0023047730792313814, -0.01665876992046833, 0.020600788295269012, 0.016491319984197617, -0.0385267436504364, -0.03672120347619057]
[2025-05-14 01:34:50,142]: Mean: 0.00007684
[2025-05-14 01:34:50,142]: Min: -0.06945500
[2025-05-14 01:34:50,142]: Max: 0.07080992
[2025-05-14 01:34:50,142]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 01:34:50,142]: Sample Values (25 elements): [0.9714199900627136, 0.9700506329536438, 0.9726924896240234, 0.9693591594696045, 0.9724933505058289, 0.9680865406990051, 0.9712622761726379, 0.9685044288635254, 0.9725925922393799, 0.9700677990913391, 0.9697222709655762, 0.9667856097221375, 0.9707009792327881, 0.969816267490387, 0.9689620733261108, 0.9700615406036377, 0.9693386554718018, 0.9696273803710938, 0.9676670432090759, 0.9693010449409485, 0.9721642136573792, 0.9698286056518555, 0.968934953212738, 0.9687382578849792, 0.9699663519859314]
[2025-05-14 01:34:50,142]: Mean: 0.96993959
[2025-05-14 01:34:50,143]: Min: 0.96503854
[2025-05-14 01:34:50,143]: Max: 0.97383761
[2025-05-14 01:34:50,143]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 01:34:50,186]: Sample Values (25 elements): [0.0005518890102393925, 0.0016627163859084249, -0.00534589309245348, 0.007966759614646435, -0.0034056296572089195, -0.010985950008034706, 0.0059953234158456326, 0.006489549297839403, 0.0029493472538888454, 0.01176782138645649, 0.006802054587751627, -0.014770308509469032, 0.003939426504075527, 0.00725385919213295, -0.004734374582767487, -0.008542604744434357, -0.006452023517340422, 0.011068429797887802, 0.014865487813949585, 0.0018128914525732398, -0.001130466116592288, -0.0002867923176381737, -0.00010546416160650551, -0.002214201260358095, -0.008587518706917763]
[2025-05-14 01:34:50,186]: Mean: -0.00010269
[2025-05-14 01:34:50,186]: Min: -0.02491393
[2025-05-14 01:34:50,186]: Max: 0.02489356
[2025-05-14 01:34:50,187]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 01:34:50,187]: Sample Values (25 elements): [0.9708014130592346, 0.9697676301002502, 0.9725438952445984, 0.9702593684196472, 0.9695903062820435, 0.9712650775909424, 0.9762195944786072, 0.9743565917015076, 0.970553994178772, 0.973938524723053, 0.9697496294975281, 0.974500834941864, 0.9713663458824158, 0.9743731021881104, 0.9712908864021301, 0.9718741178512573, 0.9750291705131531, 0.9706023931503296, 0.9733859300613403, 0.9724306464195251, 0.9707313776016235, 0.9718287587165833, 0.9717634320259094, 0.9757938981056213, 0.9723475575447083]
[2025-05-14 01:34:50,187]: Mean: 0.97186172
[2025-05-14 01:34:50,187]: Min: 0.96827936
[2025-05-14 01:34:50,187]: Max: 0.98278320
[2025-05-14 01:34:50,187]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 01:34:50,234]: Sample Values (25 elements): [-0.003050299594178796, 0.0031733307987451553, -0.008806237019598484, -0.007369585335254669, 0.0030401397962123156, -0.006107167806476355, -0.005183496046811342, 0.005436910316348076, 0.0011522624408826232, 0.01118901465088129, 0.0020643644966185093, -0.0029371732380241156, -0.008472681976854801, -0.004841915797442198, 0.009097425267100334, -0.006654569413512945, 0.007640673778951168, -0.00025759657728485763, -0.004131298512220383, -0.006259068381041288, 0.007100964430719614, 0.00569250900298357, -0.009922036901116371, 0.002467768732458353, -0.0006628013215959072]
[2025-05-14 01:34:50,235]: Mean: 0.00002808
[2025-05-14 01:34:50,235]: Min: -0.02012056
[2025-05-14 01:34:50,235]: Max: 0.02222158
[2025-05-14 01:34:50,235]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 01:34:50,236]: Sample Values (25 elements): [0.9818605184555054, 0.9823732376098633, 0.9802427887916565, 0.9786480665206909, 0.9791158437728882, 0.9775047302246094, 0.980392575263977, 0.9797993302345276, 0.9766117334365845, 0.9803441166877747, 0.9742121696472168, 0.9811891913414001, 0.9836538434028625, 0.9815597534179688, 0.9822497367858887, 0.9797804355621338, 0.9812345504760742, 0.979688823223114, 0.9790894389152527, 0.9826120138168335, 0.9813652038574219, 0.9781869053840637, 0.980419397354126, 0.9793765544891357, 0.9811103940010071]
[2025-05-14 01:34:50,236]: Mean: 0.98000705
[2025-05-14 01:34:50,236]: Min: 0.97334719
[2025-05-14 01:34:50,236]: Max: 0.98688769
[2025-05-14 01:34:50,237]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 01:34:50,237]: Sample Values (25 elements): [0.022662591189146042, -0.044982004910707474, 0.030277008190751076, 0.0382247194647789, -0.05336198955774307, 0.060776907950639725, 0.004200212191790342, 0.041929326951503754, 0.05296114459633827, -0.045409779995679855, -0.03245839104056358, 0.03761846572160721, 0.11600970476865768, -0.05011565610766411, -0.06600063294172287, -0.059159014374017715, -0.004379746504127979, -0.0510476753115654, 0.029549941420555115, -0.08640380948781967, -0.033212341368198395, -0.015338768251240253, 7.800340245012194e-05, -0.04049474745988846, 0.03343212977051735]
[2025-05-14 01:34:50,237]: Mean: -0.00026342
[2025-05-14 01:34:50,238]: Min: -0.11476474
[2025-05-14 01:34:50,238]: Max: 0.15021907
[2025-05-14 01:34:50,238]: 


QAT of ResNet18 with relu6 down to 4 bits...
[2025-05-14 01:34:50,570]: [ResNet18_relu6_quantized_4_bits] after configure_qat:
[2025-05-14 01:34:50,719]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 01:36:37,539]: [ResNet18_relu6_quantized_4_bits] Epoch: 001 Train Loss: 0.0992 Train Acc: 0.9639 Eval Loss: 0.4774 Eval Acc: 0.8831 (LR: 0.001000)
[2025-05-14 01:38:24,698]: [ResNet18_relu6_quantized_4_bits] Epoch: 002 Train Loss: 0.1023 Train Acc: 0.9636 Eval Loss: 0.4786 Eval Acc: 0.8866 (LR: 0.001000)
[2025-05-14 01:40:08,049]: [ResNet18_relu6_quantized_4_bits] Epoch: 003 Train Loss: 0.1024 Train Acc: 0.9633 Eval Loss: 0.4316 Eval Acc: 0.8904 (LR: 0.001000)
[2025-05-14 01:41:50,131]: [ResNet18_relu6_quantized_4_bits] Epoch: 004 Train Loss: 0.0950 Train Acc: 0.9664 Eval Loss: 0.4429 Eval Acc: 0.8927 (LR: 0.001000)
[2025-05-14 01:43:39,650]: [ResNet18_relu6_quantized_4_bits] Epoch: 005 Train Loss: 0.0938 Train Acc: 0.9672 Eval Loss: 0.4285 Eval Acc: 0.8962 (LR: 0.001000)
[2025-05-14 01:45:31,705]: [ResNet18_relu6_quantized_4_bits] Epoch: 006 Train Loss: 0.0956 Train Acc: 0.9660 Eval Loss: 0.4145 Eval Acc: 0.8944 (LR: 0.001000)
[2025-05-14 01:47:21,116]: [ResNet18_relu6_quantized_4_bits] Epoch: 007 Train Loss: 0.0873 Train Acc: 0.9689 Eval Loss: 0.4154 Eval Acc: 0.8959 (LR: 0.001000)
[2025-05-14 01:49:12,237]: [ResNet18_relu6_quantized_4_bits] Epoch: 008 Train Loss: 0.0831 Train Acc: 0.9705 Eval Loss: 0.4344 Eval Acc: 0.8936 (LR: 0.001000)
[2025-05-14 01:51:00,086]: [ResNet18_relu6_quantized_4_bits] Epoch: 009 Train Loss: 0.0840 Train Acc: 0.9712 Eval Loss: 0.4230 Eval Acc: 0.8929 (LR: 0.001000)
[2025-05-14 01:52:49,354]: [ResNet18_relu6_quantized_4_bits] Epoch: 010 Train Loss: 0.0861 Train Acc: 0.9687 Eval Loss: 0.4323 Eval Acc: 0.8913 (LR: 0.001000)
[2025-05-14 01:54:37,439]: [ResNet18_relu6_quantized_4_bits] Epoch: 011 Train Loss: 0.0764 Train Acc: 0.9734 Eval Loss: 0.4569 Eval Acc: 0.8888 (LR: 0.001000)
[2025-05-14 01:56:24,066]: [ResNet18_relu6_quantized_4_bits] Epoch: 012 Train Loss: 0.0796 Train Acc: 0.9717 Eval Loss: 0.4532 Eval Acc: 0.8899 (LR: 0.001000)
[2025-05-14 01:58:10,990]: [ResNet18_relu6_quantized_4_bits] Epoch: 013 Train Loss: 0.0784 Train Acc: 0.9727 Eval Loss: 0.4484 Eval Acc: 0.8887 (LR: 0.001000)
[2025-05-14 01:59:58,191]: [ResNet18_relu6_quantized_4_bits] Epoch: 014 Train Loss: 0.0801 Train Acc: 0.9708 Eval Loss: 0.4345 Eval Acc: 0.8955 (LR: 0.001000)
[2025-05-14 02:01:45,540]: [ResNet18_relu6_quantized_4_bits] Epoch: 015 Train Loss: 0.0722 Train Acc: 0.9741 Eval Loss: 0.4210 Eval Acc: 0.8988 (LR: 0.001000)
[2025-05-14 02:03:32,673]: [ResNet18_relu6_quantized_4_bits] Epoch: 016 Train Loss: 0.0713 Train Acc: 0.9747 Eval Loss: 0.4246 Eval Acc: 0.8986 (LR: 0.001000)
[2025-05-14 02:05:19,611]: [ResNet18_relu6_quantized_4_bits] Epoch: 017 Train Loss: 0.0710 Train Acc: 0.9750 Eval Loss: 0.4597 Eval Acc: 0.8924 (LR: 0.001000)
[2025-05-14 02:07:06,365]: [ResNet18_relu6_quantized_4_bits] Epoch: 018 Train Loss: 0.0706 Train Acc: 0.9752 Eval Loss: 0.4577 Eval Acc: 0.8905 (LR: 0.001000)
[2025-05-14 02:08:53,188]: [ResNet18_relu6_quantized_4_bits] Epoch: 019 Train Loss: 0.0742 Train Acc: 0.9729 Eval Loss: 0.4618 Eval Acc: 0.8891 (LR: 0.001000)
[2025-05-14 02:10:40,903]: [ResNet18_relu6_quantized_4_bits] Epoch: 020 Train Loss: 0.0734 Train Acc: 0.9737 Eval Loss: 0.4583 Eval Acc: 0.8884 (LR: 0.001000)
[2025-05-14 02:12:27,840]: [ResNet18_relu6_quantized_4_bits] Epoch: 021 Train Loss: 0.0668 Train Acc: 0.9773 Eval Loss: 0.4203 Eval Acc: 0.8970 (LR: 0.001000)
[2025-05-14 02:14:15,069]: [ResNet18_relu6_quantized_4_bits] Epoch: 022 Train Loss: 0.0669 Train Acc: 0.9764 Eval Loss: 0.4216 Eval Acc: 0.9010 (LR: 0.001000)
[2025-05-14 02:16:02,578]: [ResNet18_relu6_quantized_4_bits] Epoch: 023 Train Loss: 0.0648 Train Acc: 0.9767 Eval Loss: 0.4351 Eval Acc: 0.8972 (LR: 0.001000)
[2025-05-14 02:17:49,776]: [ResNet18_relu6_quantized_4_bits] Epoch: 024 Train Loss: 0.0621 Train Acc: 0.9780 Eval Loss: 0.4546 Eval Acc: 0.8960 (LR: 0.001000)
[2025-05-14 02:19:37,154]: [ResNet18_relu6_quantized_4_bits] Epoch: 025 Train Loss: 0.0641 Train Acc: 0.9779 Eval Loss: 0.4356 Eval Acc: 0.8985 (LR: 0.001000)
[2025-05-14 02:21:23,888]: [ResNet18_relu6_quantized_4_bits] Epoch: 026 Train Loss: 0.0628 Train Acc: 0.9781 Eval Loss: 0.4384 Eval Acc: 0.8949 (LR: 0.001000)
[2025-05-14 02:23:08,523]: [ResNet18_relu6_quantized_4_bits] Epoch: 027 Train Loss: 0.0644 Train Acc: 0.9772 Eval Loss: 0.4218 Eval Acc: 0.8975 (LR: 0.001000)
[2025-05-14 02:25:05,871]: [ResNet18_relu6_quantized_4_bits] Epoch: 028 Train Loss: 0.0601 Train Acc: 0.9784 Eval Loss: 0.4331 Eval Acc: 0.8975 (LR: 0.001000)
[2025-05-14 02:27:06,592]: [ResNet18_relu6_quantized_4_bits] Epoch: 029 Train Loss: 0.0626 Train Acc: 0.9780 Eval Loss: 0.4267 Eval Acc: 0.8996 (LR: 0.001000)
[2025-05-14 02:29:06,460]: [ResNet18_relu6_quantized_4_bits] Epoch: 030 Train Loss: 0.0573 Train Acc: 0.9797 Eval Loss: 0.4230 Eval Acc: 0.8980 (LR: 0.000250)
[2025-05-14 02:31:06,930]: [ResNet18_relu6_quantized_4_bits] Epoch: 031 Train Loss: 0.0348 Train Acc: 0.9890 Eval Loss: 0.3657 Eval Acc: 0.9099 (LR: 0.000250)
[2025-05-14 02:33:06,882]: [ResNet18_relu6_quantized_4_bits] Epoch: 032 Train Loss: 0.0295 Train Acc: 0.9903 Eval Loss: 0.3602 Eval Acc: 0.9120 (LR: 0.000250)
[2025-05-14 02:35:05,185]: [ResNet18_relu6_quantized_4_bits] Epoch: 033 Train Loss: 0.0247 Train Acc: 0.9930 Eval Loss: 0.3611 Eval Acc: 0.9134 (LR: 0.000250)
[2025-05-14 02:37:05,036]: [ResNet18_relu6_quantized_4_bits] Epoch: 034 Train Loss: 0.0243 Train Acc: 0.9924 Eval Loss: 0.3677 Eval Acc: 0.9117 (LR: 0.000250)
[2025-05-14 02:38:52,646]: [ResNet18_relu6_quantized_4_bits] Epoch: 035 Train Loss: 0.0241 Train Acc: 0.9925 Eval Loss: 0.3672 Eval Acc: 0.9117 (LR: 0.000250)
[2025-05-14 02:40:37,558]: [ResNet18_relu6_quantized_4_bits] Epoch: 036 Train Loss: 0.0231 Train Acc: 0.9932 Eval Loss: 0.3712 Eval Acc: 0.9096 (LR: 0.000250)
[2025-05-14 02:42:22,999]: [ResNet18_relu6_quantized_4_bits] Epoch: 037 Train Loss: 0.0234 Train Acc: 0.9929 Eval Loss: 0.3673 Eval Acc: 0.9112 (LR: 0.000250)
[2025-05-14 02:44:12,132]: [ResNet18_relu6_quantized_4_bits] Epoch: 038 Train Loss: 0.0219 Train Acc: 0.9933 Eval Loss: 0.3745 Eval Acc: 0.9103 (LR: 0.000250)
[2025-05-14 02:45:54,842]: [ResNet18_relu6_quantized_4_bits] Epoch: 039 Train Loss: 0.0203 Train Acc: 0.9939 Eval Loss: 0.3770 Eval Acc: 0.9109 (LR: 0.000250)
[2025-05-14 02:47:39,065]: [ResNet18_relu6_quantized_4_bits] Epoch: 040 Train Loss: 0.0214 Train Acc: 0.9931 Eval Loss: 0.3713 Eval Acc: 0.9138 (LR: 0.000250)
[2025-05-14 02:49:33,711]: [ResNet18_relu6_quantized_4_bits] Epoch: 041 Train Loss: 0.0202 Train Acc: 0.9938 Eval Loss: 0.3795 Eval Acc: 0.9103 (LR: 0.000250)
[2025-05-14 02:51:31,719]: [ResNet18_relu6_quantized_4_bits] Epoch: 042 Train Loss: 0.0208 Train Acc: 0.9936 Eval Loss: 0.3874 Eval Acc: 0.9096 (LR: 0.000250)
[2025-05-14 02:53:27,616]: [ResNet18_relu6_quantized_4_bits] Epoch: 043 Train Loss: 0.0205 Train Acc: 0.9935 Eval Loss: 0.3790 Eval Acc: 0.9123 (LR: 0.000250)
[2025-05-14 02:55:19,095]: [ResNet18_relu6_quantized_4_bits] Epoch: 044 Train Loss: 0.0196 Train Acc: 0.9942 Eval Loss: 0.3834 Eval Acc: 0.9119 (LR: 0.000250)
[2025-05-14 02:57:10,829]: [ResNet18_relu6_quantized_4_bits] Epoch: 045 Train Loss: 0.0199 Train Acc: 0.9938 Eval Loss: 0.3891 Eval Acc: 0.9081 (LR: 0.000063)
[2025-05-14 02:59:00,115]: [ResNet18_relu6_quantized_4_bits] Epoch: 046 Train Loss: 0.0178 Train Acc: 0.9947 Eval Loss: 0.3833 Eval Acc: 0.9120 (LR: 0.000063)
[2025-05-14 03:00:48,034]: [ResNet18_relu6_quantized_4_bits] Epoch: 047 Train Loss: 0.0160 Train Acc: 0.9955 Eval Loss: 0.3782 Eval Acc: 0.9130 (LR: 0.000063)
[2025-05-14 03:02:35,802]: [ResNet18_relu6_quantized_4_bits] Epoch: 048 Train Loss: 0.0154 Train Acc: 0.9958 Eval Loss: 0.3760 Eval Acc: 0.9126 (LR: 0.000063)
[2025-05-14 03:04:18,075]: [ResNet18_relu6_quantized_4_bits] Epoch: 049 Train Loss: 0.0156 Train Acc: 0.9954 Eval Loss: 0.3704 Eval Acc: 0.9157 (LR: 0.000063)
[2025-05-14 03:06:06,714]: [ResNet18_relu6_quantized_4_bits] Epoch: 050 Train Loss: 0.0167 Train Acc: 0.9951 Eval Loss: 0.3672 Eval Acc: 0.9164 (LR: 0.000063)
[2025-05-14 03:07:53,303]: [ResNet18_relu6_quantized_4_bits] Epoch: 051 Train Loss: 0.0155 Train Acc: 0.9957 Eval Loss: 0.3738 Eval Acc: 0.9150 (LR: 0.000063)
[2025-05-14 03:09:39,330]: [ResNet18_relu6_quantized_4_bits] Epoch: 052 Train Loss: 0.0145 Train Acc: 0.9962 Eval Loss: 0.3701 Eval Acc: 0.9143 (LR: 0.000063)
[2025-05-14 03:11:25,630]: [ResNet18_relu6_quantized_4_bits] Epoch: 053 Train Loss: 0.0160 Train Acc: 0.9952 Eval Loss: 0.3702 Eval Acc: 0.9122 (LR: 0.000063)
[2025-05-14 03:13:14,425]: [ResNet18_relu6_quantized_4_bits] Epoch: 054 Train Loss: 0.0149 Train Acc: 0.9959 Eval Loss: 0.3703 Eval Acc: 0.9145 (LR: 0.000063)
[2025-05-14 03:15:06,250]: [ResNet18_relu6_quantized_4_bits] Epoch: 055 Train Loss: 0.0150 Train Acc: 0.9962 Eval Loss: 0.3777 Eval Acc: 0.9127 (LR: 0.000063)
[2025-05-14 03:16:56,607]: [ResNet18_relu6_quantized_4_bits] Epoch: 056 Train Loss: 0.0155 Train Acc: 0.9954 Eval Loss: 0.3765 Eval Acc: 0.9136 (LR: 0.000063)
[2025-05-14 03:18:45,299]: [ResNet18_relu6_quantized_4_bits] Epoch: 057 Train Loss: 0.0140 Train Acc: 0.9961 Eval Loss: 0.3753 Eval Acc: 0.9132 (LR: 0.000063)
[2025-05-14 03:20:34,101]: [ResNet18_relu6_quantized_4_bits] Epoch: 058 Train Loss: 0.0142 Train Acc: 0.9962 Eval Loss: 0.3772 Eval Acc: 0.9140 (LR: 0.000063)
[2025-05-14 03:22:25,897]: [ResNet18_relu6_quantized_4_bits] Epoch: 059 Train Loss: 0.0140 Train Acc: 0.9962 Eval Loss: 0.3767 Eval Acc: 0.9153 (LR: 0.000063)
[2025-05-14 03:24:10,504]: [ResNet18_relu6_quantized_4_bits] Epoch: 060 Train Loss: 0.0142 Train Acc: 0.9961 Eval Loss: 0.3714 Eval Acc: 0.9162 (LR: 0.000063)
[2025-05-14 03:24:10,504]: [ResNet18_relu6_quantized_4_bits] Best Eval Accuracy: 0.9164
[2025-05-14 03:24:10,587]: 


Quantization of model down to 4 bits finished
[2025-05-14 03:24:10,587]: Model Architecture:
[2025-05-14 03:24:10,645]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0189], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.143646702170372, max_val=0.13925997912883759)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0161], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12327836453914642, max_val=0.11760447174310684)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09670375287532806, max_val=0.08608412742614746)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0111], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.083341583609581, max_val=0.08387381583452225)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07290135324001312, max_val=0.07627680152654648)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3999], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.997770309448242)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06401516497135162, max_val=0.06469114869832993)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0200], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15097059309482574, max_val=0.14862261712551117)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.058648254722356796, max_val=0.06970135867595673)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3967], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.950608253479004)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0089], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.057916995137929916, max_val=0.07631509751081467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.055772170424461365, max_val=0.05195951089262962)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3967], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.950266361236572)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04264063760638237, max_val=0.04850032553076744)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10463530570268631, max_val=0.10754189640283585)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04543422907590866, max_val=0.04684668406844139)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3959], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.938889026641846)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.043758850544691086, max_val=0.041718993335962296)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.036670032888650894, max_val=0.037074219435453415)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3994], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.990445613861084)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.026830818504095078, max_val=0.028554126620292664)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0094], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06917859613895416, max_val=0.07251330465078354)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0036], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.027422497048974037, max_val=0.026081068441271782)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3982], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.972430229187012)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02083745412528515, max_val=0.022399865090847015)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 03:24:10,645]: 
Model Weights:
[2025-05-14 03:24:10,645]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 03:24:10,646]: Sample Values (25 elements): [0.1465936005115509, -0.12072170525789261, 0.16298535466194153, 0.060313161462545395, -0.10218207538127899, 0.014341768808662891, -0.18692976236343384, -0.1602621078491211, 0.05450737848877907, -0.026779355481266975, -0.048975199460983276, -0.028497377410531044, 0.1586313247680664, 0.04658808186650276, 0.10227514803409576, 0.14489036798477173, -0.03915158659219742, 0.030483368784189224, 0.14784075319766998, -0.1423989087343216, 0.2305232435464859, 0.03973187133669853, 0.11473800987005234, 0.1871826946735382, 0.16223657131195068]
[2025-05-14 03:24:10,646]: Mean: -0.00014525
[2025-05-14 03:24:10,646]: Min: -0.35915616
[2025-05-14 03:24:10,646]: Max: 0.37314099
[2025-05-14 03:24:10,646]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 03:24:10,647]: Sample Values (25 elements): [1.0465954542160034, 0.9492245316505432, 0.9717448353767395, 0.9838615655899048, 0.9491394758224487, 0.9476995468139648, 0.9778968095779419, 0.9437394738197327, 0.9360188245773315, 1.0249619483947754, 1.0007940530776978, 0.9131659269332886, 0.9567400217056274, 0.9482892751693726, 0.999967098236084, 0.9687700271606445, 0.8477925658226013, 0.9084261655807495, 0.9362020492553711, 0.9125269651412964, 0.9274391531944275, 0.935511589050293, 0.8903656005859375, 0.9430745840072632, 0.9755520820617676]
[2025-05-14 03:24:10,647]: Mean: 0.94775802
[2025-05-14 03:24:10,647]: Min: 0.81475866
[2025-05-14 03:24:10,647]: Max: 1.06694829
[2025-05-14 03:24:10,648]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 03:24:10,649]: Sample Values (25 elements): [0.018860427662730217, 0.0, -0.0565812811255455, -0.037720855325460434, -0.018860427662730217, -0.037720855325460434, -0.018860427662730217, -0.037720855325460434, -0.018860427662730217, -0.018860427662730217, -0.037720855325460434, -0.0565812811255455, 0.0, 0.0, 0.0, -0.018860427662730217, 0.018860427662730217, 0.0, 0.0565812811255455, -0.018860427662730217, -0.037720855325460434, -0.037720855325460434, 0.0, -0.037720855325460434, -0.018860427662730217]
[2025-05-14 03:24:10,649]: Mean: -0.00154356
[2025-05-14 03:24:10,649]: Min: -0.15088342
[2025-05-14 03:24:10,650]: Max: 0.13202299
[2025-05-14 03:24:10,650]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 03:24:10,650]: Sample Values (25 elements): [0.9680314064025879, 0.9375678300857544, 0.9469847083091736, 0.9603228569030762, 0.9588693380355835, 0.9519205689430237, 0.9851967692375183, 1.0465670824050903, 0.9805189967155457, 1.0179733037948608, 0.936421811580658, 0.9488037824630737, 0.9570091962814331, 0.9499809741973877, 0.9749081134796143, 0.9760169982910156, 0.9749170541763306, 0.9642242789268494, 0.9551180005073547, 0.9464705586433411, 0.9540799856185913, 1.05292809009552, 0.956806480884552, 0.9687309265136719, 0.9571899771690369]
[2025-05-14 03:24:10,650]: Mean: 0.96400434
[2025-05-14 03:24:10,650]: Min: 0.91990417
[2025-05-14 03:24:10,650]: Max: 1.10066772
[2025-05-14 03:24:10,651]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 03:24:10,652]: Sample Values (25 elements): [0.03211772069334984, 0.04817657917737961, -0.03211772069334984, 0.01605886034667492, 0.03211772069334984, -0.01605886034667492, -0.03211772069334984, 0.01605886034667492, 0.0, -0.04817657917737961, 0.03211772069334984, -0.01605886034667492, 0.01605886034667492, -0.01605886034667492, -0.04817657917737961, -0.03211772069334984, 0.01605886034667492, 0.03211772069334984, 0.03211772069334984, 0.0, 0.01605886034667492, 0.01605886034667492, -0.03211772069334984, 0.0, 0.04817657917737961]
[2025-05-14 03:24:10,652]: Mean: -0.00099758
[2025-05-14 03:24:10,652]: Min: -0.12847088
[2025-05-14 03:24:10,652]: Max: 0.11241202
[2025-05-14 03:24:10,652]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 03:24:10,653]: Sample Values (25 elements): [1.0173897743225098, 0.974276602268219, 0.9548629522323608, 0.9827117323875427, 0.9786144495010376, 0.9541558623313904, 0.9396749138832092, 0.9690953493118286, 0.9734768271446228, 0.981025755405426, 0.9747909903526306, 0.9560371041297913, 0.9571108222007751, 1.0110372304916382, 0.9410709738731384, 0.9640228748321533, 0.9551716446876526, 0.9819321036338806, 0.9998357892036438, 0.9355989694595337, 0.9534211754798889, 0.9721554517745972, 0.9609242677688599, 0.9681496024131775, 0.9869673848152161]
[2025-05-14 03:24:10,653]: Mean: 0.97027862
[2025-05-14 03:24:10,653]: Min: 0.92083222
[2025-05-14 03:24:10,653]: Max: 1.05532849
[2025-05-14 03:24:10,654]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 03:24:10,655]: Sample Values (25 elements): [0.012185853905975819, -0.012185853905975819, 0.024371707811951637, -0.03655756264925003, -0.012185853905975819, 0.0, -0.024371707811951637, -0.012185853905975819, -0.048743415623903275, 0.012185853905975819, -0.012185853905975819, 0.03655756264925003, 0.012185853905975819, -0.012185853905975819, 0.0, -0.024371707811951637, -0.012185853905975819, -0.03655756264925003, -0.024371707811951637, 0.024371707811951637, 0.012185853905975819, -0.03655756264925003, -0.024371707811951637, -0.012185853905975819, 0.0]
[2025-05-14 03:24:10,655]: Mean: -0.00124424
[2025-05-14 03:24:10,655]: Min: -0.09748683
[2025-05-14 03:24:10,655]: Max: 0.08530097
[2025-05-14 03:24:10,655]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 03:24:10,655]: Sample Values (25 elements): [0.9523279666900635, 0.9563667178153992, 0.9617639183998108, 0.9590982794761658, 0.9561429023742676, 0.9645290374755859, 0.9587668776512146, 0.9514948725700378, 0.953741729259491, 0.9567955732345581, 0.9768097400665283, 0.9633535146713257, 0.9618561267852783, 0.9483978748321533, 0.9638010859489441, 0.9652694463729858, 0.9365310072898865, 0.971163809299469, 0.9476533532142639, 0.961800217628479, 0.967017412185669, 0.957302987575531, 0.9579596519470215, 0.976719081401825, 0.9885052442550659]
[2025-05-14 03:24:10,656]: Mean: 0.96094036
[2025-05-14 03:24:10,656]: Min: 0.93653101
[2025-05-14 03:24:10,656]: Max: 0.98850524
[2025-05-14 03:24:10,657]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 03:24:10,657]: Sample Values (25 elements): [0.0, -0.022295372560620308, 0.0, -0.033443059772253036, -0.033443059772253036, -0.033443059772253036, 0.011147686280310154, 0.044590745121240616, -0.022295372560620308, -0.011147686280310154, 0.055738430470228195, 0.011147686280310154, 0.0, 0.011147686280310154, -0.033443059772253036, -0.044590745121240616, 0.033443059772253036, 0.033443059772253036, 0.022295372560620308, -0.044590745121240616, -0.022295372560620308, 0.033443059772253036, 0.055738430470228195, -0.011147686280310154, 0.0]
[2025-05-14 03:24:10,658]: Mean: -0.00039373
[2025-05-14 03:24:10,658]: Min: -0.07803380
[2025-05-14 03:24:10,658]: Max: 0.08918149
[2025-05-14 03:24:10,658]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 03:24:10,658]: Sample Values (25 elements): [0.9702684879302979, 0.9791263937950134, 0.9560905694961548, 0.9620317816734314, 0.9681146740913391, 0.9658671021461487, 0.9637264013290405, 0.9900307655334473, 0.9504426121711731, 0.9816482067108154, 0.9865866899490356, 0.9827574491500854, 0.9691348671913147, 0.9644938707351685, 0.9551694989204407, 0.9631507396697998, 0.9617207646369934, 0.9902576804161072, 0.9887685775756836, 0.9615562558174133, 0.9701173901557922, 0.9641462564468384, 0.9827678799629211, 0.9624909162521362, 0.9632343053817749]
[2025-05-14 03:24:10,658]: Mean: 0.97500914
[2025-05-14 03:24:10,659]: Min: 0.95044261
[2025-05-14 03:24:10,659]: Max: 1.01151550
[2025-05-14 03:24:10,660]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 03:24:10,660]: Sample Values (25 elements): [-0.00994519330561161, 0.01989038661122322, 0.00994519330561161, -0.02983557991683483, -0.02983557991683483, 0.02983557991683483, 0.01989038661122322, -0.02983557991683483, -0.0497259646654129, 0.01989038661122322, 0.02983557991683483, 0.0, 0.00994519330561161, 0.0, 0.01989038661122322, -0.02983557991683483, -0.03978077322244644, 0.01989038661122322, -0.01989038661122322, -0.03978077322244644, 0.0, 0.00994519330561161, -0.00994519330561161, 0.01989038661122322, -0.01989038661122322]
[2025-05-14 03:24:10,661]: Mean: -0.00048291
[2025-05-14 03:24:10,661]: Min: -0.06961636
[2025-05-14 03:24:10,661]: Max: 0.07956155
[2025-05-14 03:24:10,661]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 03:24:10,661]: Sample Values (25 elements): [0.9532669186592102, 0.9551807045936584, 0.9565864205360413, 0.9688290357589722, 0.9533701539039612, 0.9639416933059692, 0.9580597281455994, 0.9568731784820557, 0.9608141779899597, 0.9613817930221558, 0.9725465774536133, 0.9587616920471191, 0.9585580825805664, 0.9696068167686462, 0.9599125981330872, 0.9492649435997009, 0.9632240533828735, 0.9662162661552429, 0.9502558708190918, 0.9682557582855225, 0.9679911136627197, 0.9768081307411194, 0.9554601311683655, 0.959175169467926, 0.967888593673706]
[2025-05-14 03:24:10,661]: Mean: 0.96009058
[2025-05-14 03:24:10,662]: Min: 0.94346702
[2025-05-14 03:24:10,662]: Max: 0.97880650
[2025-05-14 03:24:10,663]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 03:24:10,664]: Sample Values (25 elements): [-0.01716083288192749, 0.025741249322891235, 0.0, 0.0, -0.008580416440963745, -0.01716083288192749, -0.008580416440963745, -0.025741249322891235, -0.01716083288192749, 0.008580416440963745, 0.01716083288192749, -0.01716083288192749, -0.025741249322891235, 0.025741249322891235, 0.01716083288192749, 0.008580416440963745, 0.008580416440963745, 0.025741249322891235, -0.01716083288192749, 0.008580416440963745, -0.008580416440963745, 0.0, 0.0, 0.0, -0.008580416440963745]
[2025-05-14 03:24:10,664]: Mean: -0.00054029
[2025-05-14 03:24:10,664]: Min: -0.06006292
[2025-05-14 03:24:10,665]: Max: 0.06864333
[2025-05-14 03:24:10,665]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 03:24:10,665]: Sample Values (25 elements): [0.9656289219856262, 0.9508025646209717, 0.9663249254226685, 0.9543673992156982, 0.960366427898407, 0.9595254063606262, 0.9666548371315002, 0.9678176045417786, 0.9531034231185913, 0.9613605737686157, 0.9641119241714478, 0.9633210897445679, 0.949385404586792, 0.9496150612831116, 0.965674638748169, 0.9687895178794861, 0.9639832973480225, 0.9547416567802429, 0.9590943455696106, 0.9616310596466064, 0.9634987115859985, 0.952612042427063, 0.9632130265235901, 0.9630743265151978, 0.9576377272605896]
[2025-05-14 03:24:10,665]: Mean: 0.96114922
[2025-05-14 03:24:10,665]: Min: 0.94492173
[2025-05-14 03:24:10,665]: Max: 0.98568016
[2025-05-14 03:24:10,666]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 03:24:10,667]: Sample Values (25 elements): [-0.03994576260447502, 0.11983728408813477, 0.01997288130223751, -0.07989152520895004, -0.03994576260447502, 0.0, 0.11983728408813477, 0.0, 0.07989152520895004, -0.03994576260447502, 0.0, -0.01997288130223751, 0.05991864204406738, -0.05991864204406738, -0.03994576260447502, 0.03994576260447502, -0.01997288130223751, -0.03994576260447502, 0.01997288130223751, 0.03994576260447502, 0.01997288130223751, -0.05991864204406738, 0.07989152520895004, 0.05991864204406738, -0.01997288130223751]
[2025-05-14 03:24:10,667]: Mean: 0.00124587
[2025-05-14 03:24:10,667]: Min: -0.15978305
[2025-05-14 03:24:10,667]: Max: 0.13981017
[2025-05-14 03:24:10,667]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 03:24:10,668]: Sample Values (25 elements): [0.9390057325363159, 0.9444527626037598, 0.9421115517616272, 0.9344197511672974, 0.9469330310821533, 0.9343228936195374, 0.9312785267829895, 0.9519526362419128, 0.9451269507408142, 0.9485692381858826, 0.9525161981582642, 0.9338905215263367, 0.927128791809082, 0.9300142526626587, 0.9408491253852844, 0.9284911751747131, 0.9415305256843567, 0.9403566122055054, 0.9267149567604065, 0.9249066710472107, 0.9466685652732849, 0.9419550895690918, 0.9284971356391907, 0.9365619421005249, 0.9332747459411621]
[2025-05-14 03:24:10,668]: Mean: 0.93700320
[2025-05-14 03:24:10,668]: Min: 0.91039032
[2025-05-14 03:24:10,668]: Max: 0.95571941
[2025-05-14 03:24:10,669]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 03:24:10,671]: Sample Values (25 elements): [-0.008556647226214409, 0.0, 0.008556647226214409, 0.034226588904857635, 0.008556647226214409, 0.008556647226214409, 0.008556647226214409, 0.0, -0.034226588904857635, 0.017113294452428818, -0.008556647226214409, -0.017113294452428818, 0.008556647226214409, 0.025669941678643227, 0.0, 0.008556647226214409, 0.017113294452428818, 0.017113294452428818, 0.017113294452428818, -0.017113294452428818, -0.034226588904857635, 0.017113294452428818, 0.0, -0.025669941678643227, -0.017113294452428818]
[2025-05-14 03:24:10,671]: Mean: -0.00046284
[2025-05-14 03:24:10,671]: Min: -0.05989653
[2025-05-14 03:24:10,671]: Max: 0.06845318
[2025-05-14 03:24:10,671]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 03:24:10,671]: Sample Values (25 elements): [0.9601326584815979, 0.9554522037506104, 0.9596195816993713, 0.9588106870651245, 0.952883780002594, 0.960284948348999, 0.9623848795890808, 0.9531393051147461, 0.9628057479858398, 0.9585642218589783, 0.9466553926467896, 0.9718852043151855, 0.9616391062736511, 0.9679253697395325, 0.9640089273452759, 0.9457347989082336, 0.9595293402671814, 0.9586966037750244, 0.9641832113265991, 0.9664222002029419, 0.9505363702774048, 0.9496516585350037, 0.9695935249328613, 0.9518405199050903, 0.9655662178993225]
[2025-05-14 03:24:10,672]: Mean: 0.95969850
[2025-05-14 03:24:10,672]: Min: 0.94208097
[2025-05-14 03:24:10,672]: Max: 0.98124623
[2025-05-14 03:24:10,673]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 03:24:10,674]: Sample Values (25 elements): [0.0, 0.0178975947201252, 0.0, 0.026846392080187798, 0.0178975947201252, -0.0178975947201252, -0.0178975947201252, 0.0, -0.026846392080187798, 0.0089487973600626, 0.0089487973600626, -0.0178975947201252, -0.026846392080187798, -0.0089487973600626, -0.0089487973600626, -0.026846392080187798, -0.026846392080187798, -0.0089487973600626, 0.0089487973600626, 0.0357951894402504, 0.0178975947201252, 0.0, 0.026846392080187798, 0.0089487973600626, 0.0178975947201252]
[2025-05-14 03:24:10,674]: Mean: -0.00015512
[2025-05-14 03:24:10,675]: Min: -0.05369278
[2025-05-14 03:24:10,675]: Max: 0.08053917
[2025-05-14 03:24:10,675]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 03:24:10,675]: Sample Values (25 elements): [0.9662620425224304, 0.9801824688911438, 0.9736348986625671, 0.9918562173843384, 0.972737729549408, 0.9772509336471558, 0.98538738489151, 0.9633880853652954, 0.9789660573005676, 0.9678940773010254, 0.9698808789253235, 0.9744049310684204, 0.968571126461029, 0.9819402098655701, 0.9731260538101196, 0.9682875275611877, 0.9838904738426208, 0.9740146994590759, 0.9808092713356018, 0.9735479354858398, 0.9715167880058289, 0.970133900642395, 0.9712405204772949, 0.9767175912857056, 0.9987736344337463]
[2025-05-14 03:24:10,675]: Mean: 0.97752416
[2025-05-14 03:24:10,676]: Min: 0.96234971
[2025-05-14 03:24:10,676]: Max: 1.00896454
[2025-05-14 03:24:10,677]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 03:24:10,679]: Sample Values (25 elements): [-0.007182112894952297, -0.02872845157980919, 0.0, 0.014364225789904594, -0.02872845157980919, 0.014364225789904594, -0.014364225789904594, -0.014364225789904594, -0.007182112894952297, -0.014364225789904594, 0.02872845157980919, 0.021546337753534317, -0.021546337753534317, -0.014364225789904594, 0.014364225789904594, 0.02872845157980919, -0.02872845157980919, -0.007182112894952297, 0.007182112894952297, 0.014364225789904594, 0.0, 0.0, 0.021546337753534317, -0.02872845157980919, 0.014364225789904594]
[2025-05-14 03:24:10,679]: Mean: -0.00020055
[2025-05-14 03:24:10,680]: Min: -0.05745690
[2025-05-14 03:24:10,680]: Max: 0.05027479
[2025-05-14 03:24:10,680]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 03:24:10,680]: Sample Values (25 elements): [0.9568658471107483, 0.9580236077308655, 0.9584783315658569, 0.9576785564422607, 0.9631333351135254, 0.9664434790611267, 0.9528174996376038, 0.9536786079406738, 0.9552488327026367, 0.9639953970909119, 0.9564858675003052, 0.960588812828064, 0.9699466228485107, 0.9607862830162048, 0.9649472832679749, 0.9575272798538208, 0.9536473155021667, 0.9554619789123535, 0.9616317749023438, 0.9654079079627991, 0.9635791778564453, 0.9679455757141113, 0.9635428786277771, 0.959855854511261, 0.9596884846687317]
[2025-05-14 03:24:10,680]: Mean: 0.95948637
[2025-05-14 03:24:10,680]: Min: 0.93993711
[2025-05-14 03:24:10,681]: Max: 0.97316098
[2025-05-14 03:24:10,682]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 03:24:10,688]: Sample Values (25 elements): [0.012152115814387798, -0.012152115814387798, 0.006076057907193899, 0.012152115814387798, 0.012152115814387798, 0.024304231628775597, 0.006076057907193899, 0.01822817325592041, -0.012152115814387798, 0.0, 0.012152115814387798, 0.01822817325592041, -0.024304231628775597, 0.006076057907193899, -0.030380290001630783, -0.01822817325592041, -0.006076057907193899, 0.006076057907193899, -0.01822817325592041, -0.01822817325592041, -0.012152115814387798, 0.0, -0.006076057907193899, 0.024304231628775597, 0.012152115814387798]
[2025-05-14 03:24:10,688]: Mean: -0.00040153
[2025-05-14 03:24:10,688]: Min: -0.04253241
[2025-05-14 03:24:10,688]: Max: 0.04860846
[2025-05-14 03:24:10,688]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 03:24:10,689]: Sample Values (25 elements): [0.963049054145813, 0.9698383808135986, 0.9593342542648315, 0.9625679850578308, 0.9676759243011475, 0.9705749154090881, 0.9646545052528381, 0.9680271744728088, 0.9611638188362122, 0.9540243744850159, 0.9687466025352478, 0.9634302854537964, 0.9628832340240479, 0.9615518450737, 0.9642192721366882, 0.9555470943450928, 0.9476213455200195, 0.9674659371376038, 0.9704496264457703, 0.9649282097816467, 0.9710160493850708, 0.96867835521698, 0.9643380045890808, 0.9606221914291382, 0.9618709087371826]
[2025-05-14 03:24:10,689]: Mean: 0.96457851
[2025-05-14 03:24:10,689]: Min: 0.94762135
[2025-05-14 03:24:10,689]: Max: 0.98163515
[2025-05-14 03:24:10,690]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 03:24:10,691]: Sample Values (25 elements): [0.0, 0.01414514984935522, 0.05658059939742088, 0.07072574645280838, 0.02829029969871044, 0.04243544861674309, 0.05658059939742088, -0.04243544861674309, 0.07072574645280838, 0.07072574645280838, -0.01414514984935522, -0.04243544861674309, -0.05658059939742088, 0.04243544861674309, -0.05658059939742088, -0.04243544861674309, 0.02829029969871044, 0.08487089723348618, -0.05658059939742088, -0.02829029969871044, 0.08487089723348618, 0.07072574645280838, 0.01414514984935522, -0.02829029969871044, -0.07072574645280838]
[2025-05-14 03:24:10,691]: Mean: -0.00005784
[2025-05-14 03:24:10,691]: Min: -0.09901605
[2025-05-14 03:24:10,691]: Max: 0.11316120
[2025-05-14 03:24:10,691]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 03:24:10,691]: Sample Values (25 elements): [0.9368249773979187, 0.944384753704071, 0.9400156140327454, 0.9459328055381775, 0.9398716688156128, 0.9360241293907166, 0.9409325122833252, 0.9418190717697144, 0.9442380666732788, 0.9349888563156128, 0.9307007193565369, 0.9332015514373779, 0.9446837902069092, 0.9482265710830688, 0.9434972405433655, 0.9419735670089722, 0.9349326491355896, 0.9385889172554016, 0.9338105320930481, 0.9408530592918396, 0.9439481496810913, 0.9407471418380737, 0.9396325945854187, 0.9498158693313599, 0.9349703192710876]
[2025-05-14 03:24:10,692]: Mean: 0.94137251
[2025-05-14 03:24:10,692]: Min: 0.92998183
[2025-05-14 03:24:10,692]: Max: 0.95453644
[2025-05-14 03:24:10,693]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 03:24:10,699]: Sample Values (25 elements): [0.012304112315177917, 0.006152056157588959, 0.012304112315177917, 0.006152056157588959, -0.024608224630355835, -0.018456168472766876, -0.018456168472766876, 0.012304112315177917, 0.018456168472766876, -0.006152056157588959, 0.012304112315177917, 0.012304112315177917, 0.006152056157588959, 0.0, 0.0, -0.018456168472766876, 0.012304112315177917, -0.018456168472766876, -0.018456168472766876, -0.018456168472766876, -0.006152056157588959, 0.018456168472766876, -0.012304112315177917, 0.018456168472766876, 0.0]
[2025-05-14 03:24:10,699]: Mean: -0.00039049
[2025-05-14 03:24:10,699]: Min: -0.04306439
[2025-05-14 03:24:10,699]: Max: 0.04921645
[2025-05-14 03:24:10,699]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 03:24:10,700]: Sample Values (25 elements): [0.9572067856788635, 0.9630889296531677, 0.9640139937400818, 0.9699803590774536, 0.9660345911979675, 0.9614225625991821, 0.9649046063423157, 0.9575183987617493, 0.9598581194877625, 0.9591655731201172, 0.9625072479248047, 0.9593816995620728, 0.951885461807251, 0.9614403247833252, 0.9573344588279724, 0.9589890241622925, 0.9600395560264587, 0.9672542810440063, 0.9571166634559631, 0.9608560800552368, 0.9613885283470154, 0.9460194706916809, 0.9604268074035645, 0.9524073004722595, 0.9593480229377747]
[2025-05-14 03:24:10,700]: Mean: 0.95940149
[2025-05-14 03:24:10,700]: Min: 0.94601947
[2025-05-14 03:24:10,700]: Max: 0.97223616
[2025-05-14 03:24:10,701]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 03:24:10,707]: Sample Values (25 elements): [0.005698526278138161, -0.022794105112552643, 0.0, 0.011397052556276321, 0.022794105112552643, 0.0, 0.011397052556276321, 0.011397052556276321, -0.005698526278138161, -0.005698526278138161, 0.0, 0.017095578834414482, 0.005698526278138161, -0.011397052556276321, 0.011397052556276321, 0.0, -0.017095578834414482, 0.005698526278138161, -0.005698526278138161, -0.011397052556276321, 0.005698526278138161, -0.011397052556276321, 0.017095578834414482, 0.011397052556276321, 0.017095578834414482]
[2025-05-14 03:24:10,707]: Mean: -0.00008888
[2025-05-14 03:24:10,707]: Min: -0.04558821
[2025-05-14 03:24:10,707]: Max: 0.03988969
[2025-05-14 03:24:10,707]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 03:24:10,708]: Sample Values (25 elements): [0.9675154685974121, 0.977588415145874, 0.9774762392044067, 0.9644098877906799, 0.9777510166168213, 0.9651882648468018, 0.9701812267303467, 0.9672553539276123, 0.9663434028625488, 0.962298572063446, 0.9726724028587341, 0.9715502858161926, 0.9808344841003418, 0.9632811546325684, 0.9797348976135254, 0.9630820751190186, 0.9703971147537231, 0.9768525958061218, 0.97463458776474, 0.966972291469574, 0.9797016382217407, 0.9665196537971497, 0.964564859867096, 0.975000262260437, 0.9751402735710144]
[2025-05-14 03:24:10,708]: Mean: 0.97106314
[2025-05-14 03:24:10,708]: Min: 0.95930088
[2025-05-14 03:24:10,708]: Max: 0.99086511
[2025-05-14 03:24:10,709]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 03:24:10,721]: Sample Values (25 elements): [0.009832573123276234, 0.0, -0.014748860150575638, 0.0, -0.009832573123276234, 0.014748860150575638, 0.0, -0.009832573123276234, -0.009832573123276234, -0.019665146246552467, 0.009832573123276234, -0.014748860150575638, 0.014748860150575638, 0.009832573123276234, -0.019665146246552467, -0.014748860150575638, 0.0, 0.009832573123276234, 0.004916286561638117, -0.009832573123276234, -0.004916286561638117, -0.019665146246552467, 0.004916286561638117, -0.019665146246552467, -0.004916286561638117]
[2025-05-14 03:24:10,722]: Mean: -0.00001108
[2025-05-14 03:24:10,722]: Min: -0.03441400
[2025-05-14 03:24:10,722]: Max: 0.03933029
[2025-05-14 03:24:10,722]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 03:24:10,722]: Sample Values (25 elements): [0.9574568867683411, 0.9582970142364502, 0.9582797288894653, 0.9611741900444031, 0.9586752653121948, 0.960716962814331, 0.9615193009376526, 0.958817183971405, 0.9572490453720093, 0.9595113396644592, 0.9604141712188721, 0.9621102213859558, 0.9605255126953125, 0.9621976017951965, 0.9593287706375122, 0.9573372602462769, 0.9548342227935791, 0.9594631195068359, 0.9586881995201111, 0.958266019821167, 0.9587234854698181, 0.9589663147926331, 0.9601175785064697, 0.9633069038391113, 0.9593318104743958]
[2025-05-14 03:24:10,723]: Mean: 0.95904118
[2025-05-14 03:24:10,723]: Min: 0.95399392
[2025-05-14 03:24:10,723]: Max: 0.96537584
[2025-05-14 03:24:10,724]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 03:24:10,766]: Sample Values (25 elements): [0.01107698678970337, -0.01107698678970337, -0.003692328929901123, 0.0, -0.014769315719604492, 0.0, -0.003692328929901123, 0.007384657859802246, 0.01107698678970337, -0.003692328929901123, 0.0, -0.014769315719604492, -0.007384657859802246, 0.007384657859802246, -0.007384657859802246, -0.007384657859802246, 0.018461644649505615, -0.007384657859802246, -0.003692328929901123, 0.007384657859802246, -0.007384657859802246, 0.01107698678970337, 0.0, -0.003692328929901123, -0.007384657859802246]
[2025-05-14 03:24:10,766]: Mean: -0.00002358
[2025-05-14 03:24:10,767]: Min: -0.02584630
[2025-05-14 03:24:10,767]: Max: 0.02953863
[2025-05-14 03:24:10,767]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 03:24:10,768]: Sample Values (25 elements): [0.9616820216178894, 0.966408908367157, 0.9590949416160583, 0.9638699293136597, 0.9633464813232422, 0.9589137434959412, 0.9621124267578125, 0.9660035967826843, 0.9620562791824341, 0.9627196788787842, 0.9638533592224121, 0.9639621376991272, 0.9601609110832214, 0.9646106362342834, 0.9640477299690247, 0.960024356842041, 0.9646151661872864, 0.9605173468589783, 0.9663965702056885, 0.9650333523750305, 0.9636082649230957, 0.9584219455718994, 0.9639869928359985, 0.9611932039260864, 0.9617211818695068]
[2025-05-14 03:24:10,768]: Mean: 0.96398425
[2025-05-14 03:24:10,769]: Min: 0.95842195
[2025-05-14 03:24:10,769]: Max: 0.97445673
[2025-05-14 03:24:10,770]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 03:24:10,771]: Sample Values (25 elements): [-0.03778452426195145, 0.009446131065487862, -0.009446131065487862, -0.03778452426195145, 0.04723065346479416, 0.03778452426195145, -0.03778452426195145, 0.018892262130975723, -0.009446131065487862, -0.009446131065487862, 0.04723065346479416, 0.03778452426195145, 0.03778452426195145, -0.009446131065487862, -0.03778452426195145, -0.009446131065487862, 0.028338393196463585, 0.03778452426195145, -0.028338393196463585, -0.05667678639292717, 0.018892262130975723, -0.009446131065487862, -0.03778452426195145, -0.009446131065487862, -0.009446131065487862]
[2025-05-14 03:24:10,771]: Mean: 0.00013304
[2025-05-14 03:24:10,771]: Min: -0.06612292
[2025-05-14 03:24:10,772]: Max: 0.07556905
[2025-05-14 03:24:10,772]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 03:24:10,772]: Sample Values (25 elements): [0.95711749792099, 0.9556922912597656, 0.9559139013290405, 0.9569235444068909, 0.9570306539535522, 0.9548797011375427, 0.9565410017967224, 0.955473005771637, 0.9549316167831421, 0.9582176208496094, 0.9560351371765137, 0.9579863548278809, 0.9566996097564697, 0.955204963684082, 0.9567227363586426, 0.9559195041656494, 0.9575113654136658, 0.9562907814979553, 0.9565647840499878, 0.9558694362640381, 0.9567079544067383, 0.9570549726486206, 0.9559372067451477, 0.9526763558387756, 0.9548633694648743]
[2025-05-14 03:24:10,772]: Mean: 0.95631945
[2025-05-14 03:24:10,772]: Min: 0.95115292
[2025-05-14 03:24:10,773]: Max: 0.96139818
[2025-05-14 03:24:10,774]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 03:24:10,813]: Sample Values (25 elements): [0.010700706392526627, -0.007133804261684418, -0.003566902130842209, -0.010700706392526627, 0.007133804261684418, 0.003566902130842209, 0.007133804261684418, 0.003566902130842209, -0.007133804261684418, 0.010700706392526627, -0.003566902130842209, 0.0, -0.003566902130842209, 0.0, -0.003566902130842209, 0.003566902130842209, -0.007133804261684418, -0.003566902130842209, 0.003566902130842209, -0.003566902130842209, -0.017834510654211044, 0.003566902130842209, 0.003566902130842209, -0.010700706392526627, 0.0]
[2025-05-14 03:24:10,813]: Mean: -0.00011307
[2025-05-14 03:24:10,813]: Min: -0.02853522
[2025-05-14 03:24:10,814]: Max: 0.02496831
[2025-05-14 03:24:10,814]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 03:24:10,814]: Sample Values (25 elements): [0.9631638526916504, 0.9571373462677002, 0.9560008645057678, 0.9583761692047119, 0.9616554975509644, 0.9592529535293579, 0.9577270746231079, 0.9587535858154297, 0.9566658735275269, 0.9593371748924255, 0.9575029611587524, 0.9582227468490601, 0.9584642052650452, 0.9567015767097473, 0.9565487504005432, 0.9584106206893921, 0.9578156471252441, 0.9575172066688538, 0.9584426283836365, 0.9607546925544739, 0.9591608643531799, 0.9614779353141785, 0.9618000388145447, 0.958525538444519, 0.9583780765533447]
[2025-05-14 03:24:10,814]: Mean: 0.95887029
[2025-05-14 03:24:10,814]: Min: 0.95454895
[2025-05-14 03:24:10,815]: Max: 0.97252375
[2025-05-14 03:24:10,816]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 03:24:10,848]: Sample Values (25 elements): [0.014412440359592438, -0.01152995228767395, 0.0028824880719184875, -0.01152995228767395, 0.0, -0.005764976143836975, -0.008647464215755463, -0.017294928431510925, 0.0028824880719184875, 0.005764976143836975, 0.0028824880719184875, -0.01152995228767395, 0.014412440359592438, 0.0, -0.01152995228767395, -0.01152995228767395, 0.0028824880719184875, 0.005764976143836975, -0.0028824880719184875, 0.008647464215755463, 0.01152995228767395, -0.0028824880719184875, -0.0028824880719184875, -0.005764976143836975, 0.0]
[2025-05-14 03:24:10,849]: Mean: 0.00003118
[2025-05-14 03:24:10,849]: Min: -0.02017742
[2025-05-14 03:24:10,849]: Max: 0.02305990
[2025-05-14 03:24:10,849]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 03:24:10,850]: Sample Values (25 elements): [0.9710159301757812, 0.9688349366188049, 0.9742781519889832, 0.97515469789505, 0.9668912291526794, 0.9718834161758423, 0.9719051718711853, 0.9684090614318848, 0.9708251357078552, 0.9697065353393555, 0.971523106098175, 0.9666876792907715, 0.9711540937423706, 0.9710065722465515, 0.9700760245323181, 0.9659194946289062, 0.9679403305053711, 0.968268871307373, 0.9677765369415283, 0.9724821448326111, 0.9686302542686462, 0.9707244634628296, 0.9624348878860474, 0.9640381336212158, 0.9711325764656067]
[2025-05-14 03:24:10,850]: Mean: 0.96914124
[2025-05-14 03:24:10,850]: Min: 0.96110004
[2025-05-14 03:24:10,850]: Max: 0.97716218
[2025-05-14 03:24:10,850]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 03:24:10,851]: Sample Values (25 elements): [-0.0027305346447974443, -0.05205849930644035, 0.04905113950371742, -0.09132493287324905, 0.0936557799577713, 0.05140670761466026, -0.0651053935289383, -0.04988674819469452, 0.04691591113805771, 0.047872766852378845, 0.1095503568649292, 0.03077983669936657, 0.10246369242668152, -0.04338499903678894, -0.06219293177127838, 0.09003671258687973, 0.04787750169634819, -0.01266318280249834, -0.06383045017719269, -0.04934556037187576, -0.06573222577571869, 0.050240326672792435, 0.06625805050134659, 0.04688621312379837, 0.09429936856031418]
[2025-05-14 03:24:10,851]: Mean: -0.00025997
[2025-05-14 03:24:10,851]: Min: -0.12329752
[2025-05-14 03:24:10,851]: Max: 0.16111594
[2025-05-14 03:24:10,851]: 


QAT of ResNet18 with relu6 down to 3 bits...
[2025-05-14 03:24:11,072]: [ResNet18_relu6_quantized_3_bits] after configure_qat:
[2025-05-14 03:24:11,120]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 03:25:59,780]: [ResNet18_relu6_quantized_3_bits] Epoch: 001 Train Loss: 0.2249 Train Acc: 0.9205 Eval Loss: 0.5020 Eval Acc: 0.8606 (LR: 0.001000)
[2025-05-14 03:27:48,828]: [ResNet18_relu6_quantized_3_bits] Epoch: 002 Train Loss: 0.1881 Train Acc: 0.9321 Eval Loss: 0.4690 Eval Acc: 0.8711 (LR: 0.001000)
[2025-05-14 03:29:36,783]: [ResNet18_relu6_quantized_3_bits] Epoch: 003 Train Loss: 0.1771 Train Acc: 0.9365 Eval Loss: 0.4518 Eval Acc: 0.8775 (LR: 0.001000)
[2025-05-14 03:31:25,714]: [ResNet18_relu6_quantized_3_bits] Epoch: 004 Train Loss: 0.1715 Train Acc: 0.9398 Eval Loss: 0.4457 Eval Acc: 0.8788 (LR: 0.001000)
[2025-05-14 03:33:07,353]: [ResNet18_relu6_quantized_3_bits] Epoch: 005 Train Loss: 0.1691 Train Acc: 0.9395 Eval Loss: 0.4808 Eval Acc: 0.8666 (LR: 0.001000)
[2025-05-14 03:34:55,102]: [ResNet18_relu6_quantized_3_bits] Epoch: 006 Train Loss: 0.1595 Train Acc: 0.9428 Eval Loss: 0.4585 Eval Acc: 0.8757 (LR: 0.001000)
[2025-05-14 03:36:37,185]: [ResNet18_relu6_quantized_3_bits] Epoch: 007 Train Loss: 0.1564 Train Acc: 0.9433 Eval Loss: 0.4939 Eval Acc: 0.8654 (LR: 0.001000)
[2025-05-14 03:38:19,835]: [ResNet18_relu6_quantized_3_bits] Epoch: 008 Train Loss: 0.1566 Train Acc: 0.9444 Eval Loss: 0.4301 Eval Acc: 0.8823 (LR: 0.001000)
[2025-05-14 03:40:02,438]: [ResNet18_relu6_quantized_3_bits] Epoch: 009 Train Loss: 0.1455 Train Acc: 0.9485 Eval Loss: 0.4380 Eval Acc: 0.8806 (LR: 0.001000)
[2025-05-14 03:41:43,264]: [ResNet18_relu6_quantized_3_bits] Epoch: 010 Train Loss: 0.1441 Train Acc: 0.9480 Eval Loss: 0.4227 Eval Acc: 0.8870 (LR: 0.001000)
[2025-05-14 03:43:23,927]: [ResNet18_relu6_quantized_3_bits] Epoch: 011 Train Loss: 0.1387 Train Acc: 0.9511 Eval Loss: 0.4200 Eval Acc: 0.8870 (LR: 0.001000)
[2025-05-14 03:45:05,771]: [ResNet18_relu6_quantized_3_bits] Epoch: 012 Train Loss: 0.1404 Train Acc: 0.9497 Eval Loss: 0.4129 Eval Acc: 0.8888 (LR: 0.001000)
[2025-05-14 03:46:46,683]: [ResNet18_relu6_quantized_3_bits] Epoch: 013 Train Loss: 0.1344 Train Acc: 0.9519 Eval Loss: 0.4733 Eval Acc: 0.8772 (LR: 0.001000)
[2025-05-14 03:48:30,324]: [ResNet18_relu6_quantized_3_bits] Epoch: 014 Train Loss: 0.1372 Train Acc: 0.9505 Eval Loss: 0.4416 Eval Acc: 0.8827 (LR: 0.001000)
[2025-05-14 03:50:11,222]: [ResNet18_relu6_quantized_3_bits] Epoch: 015 Train Loss: 0.1323 Train Acc: 0.9531 Eval Loss: 0.4223 Eval Acc: 0.8846 (LR: 0.001000)
[2025-05-14 03:51:52,260]: [ResNet18_relu6_quantized_3_bits] Epoch: 016 Train Loss: 0.1287 Train Acc: 0.9543 Eval Loss: 0.4259 Eval Acc: 0.8842 (LR: 0.001000)
[2025-05-14 03:53:34,423]: [ResNet18_relu6_quantized_3_bits] Epoch: 017 Train Loss: 0.1260 Train Acc: 0.9542 Eval Loss: 0.4222 Eval Acc: 0.8875 (LR: 0.001000)
[2025-05-14 03:55:16,113]: [ResNet18_relu6_quantized_3_bits] Epoch: 018 Train Loss: 0.1254 Train Acc: 0.9548 Eval Loss: 0.4401 Eval Acc: 0.8865 (LR: 0.001000)
[2025-05-14 03:56:57,399]: [ResNet18_relu6_quantized_3_bits] Epoch: 019 Train Loss: 0.1186 Train Acc: 0.9582 Eval Loss: 0.4170 Eval Acc: 0.8829 (LR: 0.001000)
[2025-05-14 03:58:39,059]: [ResNet18_relu6_quantized_3_bits] Epoch: 020 Train Loss: 0.1196 Train Acc: 0.9570 Eval Loss: 0.4227 Eval Acc: 0.8899 (LR: 0.001000)
[2025-05-14 04:00:20,963]: [ResNet18_relu6_quantized_3_bits] Epoch: 021 Train Loss: 0.1174 Train Acc: 0.9587 Eval Loss: 0.4476 Eval Acc: 0.8846 (LR: 0.001000)
[2025-05-14 04:02:02,523]: [ResNet18_relu6_quantized_3_bits] Epoch: 022 Train Loss: 0.1184 Train Acc: 0.9574 Eval Loss: 0.4722 Eval Acc: 0.8850 (LR: 0.001000)
[2025-05-14 04:03:41,880]: [ResNet18_relu6_quantized_3_bits] Epoch: 023 Train Loss: 0.1125 Train Acc: 0.9601 Eval Loss: 0.4289 Eval Acc: 0.8869 (LR: 0.001000)
[2025-05-14 04:05:20,781]: [ResNet18_relu6_quantized_3_bits] Epoch: 024 Train Loss: 0.1121 Train Acc: 0.9596 Eval Loss: 0.4412 Eval Acc: 0.8865 (LR: 0.001000)
[2025-05-14 04:07:01,481]: [ResNet18_relu6_quantized_3_bits] Epoch: 025 Train Loss: 0.1121 Train Acc: 0.9604 Eval Loss: 0.4076 Eval Acc: 0.8949 (LR: 0.001000)
[2025-05-14 04:08:45,248]: [ResNet18_relu6_quantized_3_bits] Epoch: 026 Train Loss: 0.1091 Train Acc: 0.9612 Eval Loss: 0.4304 Eval Acc: 0.8879 (LR: 0.001000)
[2025-05-14 04:10:30,661]: [ResNet18_relu6_quantized_3_bits] Epoch: 027 Train Loss: 0.1099 Train Acc: 0.9606 Eval Loss: 0.4022 Eval Acc: 0.8973 (LR: 0.001000)
[2025-05-14 04:12:15,662]: [ResNet18_relu6_quantized_3_bits] Epoch: 028 Train Loss: 0.1065 Train Acc: 0.9616 Eval Loss: 0.4099 Eval Acc: 0.8927 (LR: 0.001000)
[2025-05-14 04:14:03,766]: [ResNet18_relu6_quantized_3_bits] Epoch: 029 Train Loss: 0.1040 Train Acc: 0.9620 Eval Loss: 0.4370 Eval Acc: 0.8897 (LR: 0.001000)
[2025-05-14 04:15:54,624]: [ResNet18_relu6_quantized_3_bits] Epoch: 030 Train Loss: 0.1040 Train Acc: 0.9635 Eval Loss: 0.4145 Eval Acc: 0.8948 (LR: 0.000250)
[2025-05-14 04:17:46,551]: [ResNet18_relu6_quantized_3_bits] Epoch: 031 Train Loss: 0.0708 Train Acc: 0.9752 Eval Loss: 0.3607 Eval Acc: 0.9055 (LR: 0.000250)
[2025-05-14 04:19:35,865]: [ResNet18_relu6_quantized_3_bits] Epoch: 032 Train Loss: 0.0642 Train Acc: 0.9786 Eval Loss: 0.3515 Eval Acc: 0.9055 (LR: 0.000250)
[2025-05-14 04:21:25,714]: [ResNet18_relu6_quantized_3_bits] Epoch: 033 Train Loss: 0.0585 Train Acc: 0.9803 Eval Loss: 0.3720 Eval Acc: 0.9067 (LR: 0.000250)
[2025-05-14 04:23:13,272]: [ResNet18_relu6_quantized_3_bits] Epoch: 034 Train Loss: 0.0543 Train Acc: 0.9819 Eval Loss: 0.3566 Eval Acc: 0.9062 (LR: 0.000250)
[2025-05-14 04:25:00,483]: [ResNet18_relu6_quantized_3_bits] Epoch: 035 Train Loss: 0.0586 Train Acc: 0.9795 Eval Loss: 0.3658 Eval Acc: 0.9036 (LR: 0.000250)
[2025-05-14 04:26:47,564]: [ResNet18_relu6_quantized_3_bits] Epoch: 036 Train Loss: 0.0539 Train Acc: 0.9821 Eval Loss: 0.3691 Eval Acc: 0.9078 (LR: 0.000250)
[2025-05-14 04:28:46,248]: [ResNet18_relu6_quantized_3_bits] Epoch: 037 Train Loss: 0.0532 Train Acc: 0.9822 Eval Loss: 0.3743 Eval Acc: 0.9034 (LR: 0.000250)
[2025-05-14 04:30:33,200]: [ResNet18_relu6_quantized_3_bits] Epoch: 038 Train Loss: 0.0521 Train Acc: 0.9822 Eval Loss: 0.3749 Eval Acc: 0.9051 (LR: 0.000250)
[2025-05-14 04:32:20,001]: [ResNet18_relu6_quantized_3_bits] Epoch: 039 Train Loss: 0.0516 Train Acc: 0.9829 Eval Loss: 0.3766 Eval Acc: 0.9031 (LR: 0.000250)
[2025-05-14 04:34:19,157]: [ResNet18_relu6_quantized_3_bits] Epoch: 040 Train Loss: 0.0521 Train Acc: 0.9827 Eval Loss: 0.3767 Eval Acc: 0.9039 (LR: 0.000250)
[2025-05-14 04:36:20,140]: [ResNet18_relu6_quantized_3_bits] Epoch: 041 Train Loss: 0.0515 Train Acc: 0.9827 Eval Loss: 0.3731 Eval Acc: 0.9062 (LR: 0.000250)
[2025-05-14 04:38:05,670]: [ResNet18_relu6_quantized_3_bits] Epoch: 042 Train Loss: 0.0496 Train Acc: 0.9832 Eval Loss: 0.3771 Eval Acc: 0.9069 (LR: 0.000250)
[2025-05-14 04:39:51,060]: [ResNet18_relu6_quantized_3_bits] Epoch: 043 Train Loss: 0.0492 Train Acc: 0.9833 Eval Loss: 0.3807 Eval Acc: 0.9048 (LR: 0.000250)
[2025-05-14 04:41:42,912]: [ResNet18_relu6_quantized_3_bits] Epoch: 044 Train Loss: 0.0489 Train Acc: 0.9835 Eval Loss: 0.3943 Eval Acc: 0.9035 (LR: 0.000250)
[2025-05-14 04:43:45,093]: [ResNet18_relu6_quantized_3_bits] Epoch: 045 Train Loss: 0.0497 Train Acc: 0.9833 Eval Loss: 0.3833 Eval Acc: 0.9051 (LR: 0.000063)
[2025-05-14 04:45:43,794]: [ResNet18_relu6_quantized_3_bits] Epoch: 046 Train Loss: 0.0463 Train Acc: 0.9844 Eval Loss: 0.3729 Eval Acc: 0.9072 (LR: 0.000063)
[2025-05-14 04:47:43,109]: [ResNet18_relu6_quantized_3_bits] Epoch: 047 Train Loss: 0.0437 Train Acc: 0.9854 Eval Loss: 0.3819 Eval Acc: 0.9061 (LR: 0.000063)
[2025-05-14 04:49:42,032]: [ResNet18_relu6_quantized_3_bits] Epoch: 048 Train Loss: 0.0425 Train Acc: 0.9858 Eval Loss: 0.3719 Eval Acc: 0.9091 (LR: 0.000063)
[2025-05-14 04:51:41,115]: [ResNet18_relu6_quantized_3_bits] Epoch: 049 Train Loss: 0.0411 Train Acc: 0.9871 Eval Loss: 0.3793 Eval Acc: 0.9085 (LR: 0.000063)
[2025-05-14 04:53:40,546]: [ResNet18_relu6_quantized_3_bits] Epoch: 050 Train Loss: 0.0402 Train Acc: 0.9868 Eval Loss: 0.3792 Eval Acc: 0.9073 (LR: 0.000063)
[2025-05-14 04:55:39,181]: [ResNet18_relu6_quantized_3_bits] Epoch: 051 Train Loss: 0.0414 Train Acc: 0.9865 Eval Loss: 0.3760 Eval Acc: 0.9064 (LR: 0.000063)
[2025-05-14 04:57:38,371]: [ResNet18_relu6_quantized_3_bits] Epoch: 052 Train Loss: 0.0388 Train Acc: 0.9869 Eval Loss: 0.3719 Eval Acc: 0.9067 (LR: 0.000063)
[2025-05-14 04:59:37,290]: [ResNet18_relu6_quantized_3_bits] Epoch: 053 Train Loss: 0.0387 Train Acc: 0.9871 Eval Loss: 0.3796 Eval Acc: 0.9055 (LR: 0.000063)
[2025-05-14 05:01:55,597]: [ResNet18_relu6_quantized_3_bits] Epoch: 054 Train Loss: 0.0401 Train Acc: 0.9873 Eval Loss: 0.3770 Eval Acc: 0.9064 (LR: 0.000063)
[2025-05-14 05:03:58,055]: [ResNet18_relu6_quantized_3_bits] Epoch: 055 Train Loss: 0.0407 Train Acc: 0.9863 Eval Loss: 0.3865 Eval Acc: 0.9042 (LR: 0.000063)
[2025-05-14 05:06:00,486]: [ResNet18_relu6_quantized_3_bits] Epoch: 056 Train Loss: 0.0373 Train Acc: 0.9883 Eval Loss: 0.3759 Eval Acc: 0.9058 (LR: 0.000063)
[2025-05-14 05:08:03,603]: [ResNet18_relu6_quantized_3_bits] Epoch: 057 Train Loss: 0.0410 Train Acc: 0.9861 Eval Loss: 0.3733 Eval Acc: 0.9067 (LR: 0.000063)
[2025-05-14 05:10:05,606]: [ResNet18_relu6_quantized_3_bits] Epoch: 058 Train Loss: 0.0408 Train Acc: 0.9865 Eval Loss: 0.3770 Eval Acc: 0.9082 (LR: 0.000063)
[2025-05-14 05:12:08,273]: [ResNet18_relu6_quantized_3_bits] Epoch: 059 Train Loss: 0.0388 Train Acc: 0.9870 Eval Loss: 0.3720 Eval Acc: 0.9083 (LR: 0.000063)
[2025-05-14 05:14:09,773]: [ResNet18_relu6_quantized_3_bits] Epoch: 060 Train Loss: 0.0393 Train Acc: 0.9865 Eval Loss: 0.3625 Eval Acc: 0.9111 (LR: 0.000063)
[2025-05-14 05:14:09,846]: [ResNet18_relu6_quantized_3_bits] Best Eval Accuracy: 0.9111
[2025-05-14 05:14:09,926]: 


Quantization of model down to 3 bits finished
[2025-05-14 05:14:09,926]: Model Architecture:
[2025-05-14 05:14:09,979]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0439], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15095193684101105, max_val=0.15629638731479645)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12131395936012268, max_val=0.12320294231176376)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0276], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0969356819987297, max_val=0.0964890718460083)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0246], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08220844715833664, max_val=0.09001027047634125)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0217], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0748673528432846, max_val=0.07723831385374069)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8562], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.993438720703125)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0197], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06718448549509048, max_val=0.07062828540802002)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0442], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15395833551883698, max_val=0.1556774377822876)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.061615873128175735, max_val=0.07017018646001816)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8554], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.987555503845215)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0585947148501873, max_val=0.0731271356344223)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0153], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05210990458726883, max_val=0.055045317858457565)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8538], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.97627592086792)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0133], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.043377675116062164, max_val=0.04982826113700867)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0305], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10426802188158035, max_val=0.10891535878181458)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0136], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.046620965003967285, max_val=0.04830356314778328)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8557], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.989749908447266)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04485456272959709, max_val=0.04318214952945709)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0110], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03817954659461975, max_val=0.03892623260617256)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8558], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.9905266761779785)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0083], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02960309199988842, max_val=0.02884555794298649)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0205], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07172290980815887, max_val=0.07185903936624527)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02770799957215786, max_val=0.0289397444576025)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8551], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.985445499420166)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020557094365358353, max_val=0.02202206663787365)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 05:14:09,979]: 
Model Weights:
[2025-05-14 05:14:09,979]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 05:14:09,979]: Sample Values (25 elements): [-0.15598899126052856, -0.10876277089118958, -0.14769554138183594, -0.16331015527248383, -0.2812540829181671, -0.07726780325174332, -0.13485312461853027, -0.0018472837982699275, -0.23274067044258118, 0.19343416392803192, 0.1679532825946808, -0.02294384315609932, -0.09152524918317795, -0.2423514723777771, 0.17192602157592773, -0.08498378098011017, 0.06273451447486877, -0.15761145949363708, -0.07567193359136581, -0.05880473554134369, 0.10337076336145401, -0.06402193009853363, -0.17205870151519775, 0.04670007899403572, -0.26071563363075256]
[2025-05-14 05:14:09,980]: Mean: -0.00009433
[2025-05-14 05:14:09,980]: Min: -0.36411121
[2025-05-14 05:14:09,980]: Max: 0.36655295
[2025-05-14 05:14:09,980]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 05:14:09,980]: Sample Values (25 elements): [0.9521709680557251, 0.9368374347686768, 1.0000271797180176, 0.9846482872962952, 0.9527794718742371, 0.9687557816505432, 0.9467686414718628, 1.1121925115585327, 1.0380051136016846, 0.9361276626586914, 1.0082532167434692, 0.9469993710517883, 0.9147425889968872, 0.9729546904563904, 0.9431652426719666, 0.8938589692115784, 0.9679073095321655, 0.8343053460121155, 0.9631363749504089, 0.9459062218666077, 0.9640958309173584, 1.006140112876892, 0.9915120005607605, 1.005765438079834, 1.0321693420410156]
[2025-05-14 05:14:09,981]: Mean: 0.97181332
[2025-05-14 05:14:09,981]: Min: 0.83430535
[2025-05-14 05:14:09,981]: Max: 1.11219251
[2025-05-14 05:14:09,982]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 05:14:09,983]: Sample Values (25 elements): [0.0, -0.04389255493879318, -0.04389255493879318, -0.04389255493879318, 0.04389255493879318, 0.04389255493879318, 0.0, 0.0, 0.0, -0.04389255493879318, 0.0, 0.04389255493879318, 0.0, -0.04389255493879318, 0.0, 0.0, 0.04389255493879318, -0.04389255493879318, 0.0, 0.04389255493879318, -0.04389255493879318, -0.04389255493879318, 0.0, 0.0, 0.04389255493879318]
[2025-05-14 05:14:09,983]: Mean: -0.00182886
[2025-05-14 05:14:09,983]: Min: -0.13167766
[2025-05-14 05:14:09,983]: Max: 0.17557022
[2025-05-14 05:14:09,983]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 05:14:09,983]: Sample Values (25 elements): [0.9648768305778503, 0.9825540781021118, 0.9683505296707153, 0.9425387978553772, 0.9991540908813477, 1.1245250701904297, 0.9611196517944336, 0.9921984672546387, 0.9547434449195862, 0.9784071445465088, 0.9836599230766296, 0.9903331398963928, 0.9424987435340881, 0.974520742893219, 1.0430930852890015, 0.9522702097892761, 1.0951292514801025, 0.9571112394332886, 0.9632211923599243, 0.9661915898323059, 0.9341012239456177, 0.9712122678756714, 0.9659814238548279, 0.9478139281272888, 0.9768252968788147]
[2025-05-14 05:14:09,984]: Mean: 0.97878337
[2025-05-14 05:14:09,984]: Min: 0.92578816
[2025-05-14 05:14:09,984]: Max: 1.14477968
[2025-05-14 05:14:09,985]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 05:14:09,985]: Sample Values (25 elements): [0.0349310077726841, 0.0, -0.0349310077726841, -0.0349310077726841, 0.0349310077726841, 0.0, 0.0, -0.0349310077726841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0349310077726841, 0.0, -0.0349310077726841, 0.0, -0.0349310077726841, 0.0349310077726841, 0.0, 0.0, -0.0349310077726841, 0.0349310077726841, -0.0349310077726841, 0.0]
[2025-05-14 05:14:09,986]: Mean: -0.00124605
[2025-05-14 05:14:09,986]: Min: -0.10479303
[2025-05-14 05:14:09,986]: Max: 0.13972403
[2025-05-14 05:14:09,986]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 05:14:09,986]: Sample Values (25 elements): [0.9691461324691772, 0.9603562355041504, 0.9319682717323303, 1.0257161855697632, 0.9730380773544312, 0.9770541787147522, 0.9463743567466736, 0.9435556530952454, 0.9824782609939575, 0.9858152270317078, 0.9731771945953369, 0.9558572769165039, 0.9900575280189514, 0.9908217787742615, 0.9968453645706177, 0.9505070447921753, 0.967689037322998, 0.9577745199203491, 0.9909486174583435, 0.9725092053413391, 0.9880337119102478, 1.0243561267852783, 0.9583669900894165, 0.9872531294822693, 0.9750339388847351]
[2025-05-14 05:14:09,986]: Mean: 0.98019630
[2025-05-14 05:14:09,987]: Min: 0.93196827
[2025-05-14 05:14:09,987]: Max: 1.08010566
[2025-05-14 05:14:09,988]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 05:14:09,988]: Sample Values (25 elements): [0.0, 0.0, -0.0276320967823267, 0.0, 0.0, 0.0276320967823267, 0.0, 0.0276320967823267, -0.0552641935646534, -0.0552641935646534, -0.0276320967823267, 0.0276320967823267, 0.0276320967823267, 0.0276320967823267, 0.0, 0.0276320967823267, 0.0, 0.0276320967823267, -0.0276320967823267, -0.0552641935646534, 0.0, 0.0276320967823267, -0.0276320967823267, 0.0, 0.0]
[2025-05-14 05:14:09,989]: Mean: -0.00129301
[2025-05-14 05:14:09,989]: Min: -0.11052839
[2025-05-14 05:14:09,989]: Max: 0.08289629
[2025-05-14 05:14:09,989]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 05:14:09,989]: Sample Values (25 elements): [0.9596279859542847, 0.9576273560523987, 0.9630858898162842, 0.9569568037986755, 0.9773295521736145, 0.9831564426422119, 0.9743630886077881, 0.9623222351074219, 0.9719148278236389, 0.9546276330947876, 0.9812225103378296, 0.9603831171989441, 1.001630425453186, 0.9742830395698547, 0.9620419144630432, 0.9727890491485596, 0.948401689529419, 0.964080274105072, 0.9548459649085999, 0.9788413643836975, 0.9680145978927612, 0.9651428461074829, 0.9712129831314087, 0.9667592644691467, 0.9583659768104553]
[2025-05-14 05:14:09,989]: Mean: 0.96842587
[2025-05-14 05:14:09,990]: Min: 0.94131655
[2025-05-14 05:14:09,990]: Max: 1.00665140
[2025-05-14 05:14:09,991]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 05:14:09,991]: Sample Values (25 elements): [0.0, 0.024602729827165604, 0.0, -0.04920545965433121, 0.0, 0.024602729827165604, -0.04920545965433121, -0.04920545965433121, -0.024602729827165604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04920545965433121, 0.0, 0.0, 0.0, 0.024602729827165604, 0.0, 0.04920545965433121, -0.04920545965433121, 0.024602729827165604, -0.04920545965433121, 0.0]
[2025-05-14 05:14:09,991]: Mean: -0.00040244
[2025-05-14 05:14:09,992]: Min: -0.07380819
[2025-05-14 05:14:09,992]: Max: 0.09841092
[2025-05-14 05:14:09,992]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 05:14:09,992]: Sample Values (25 elements): [0.9905157685279846, 1.000770092010498, 0.9661381244659424, 1.0064632892608643, 0.9725701212882996, 0.9985741972923279, 0.998505175113678, 0.9950350522994995, 0.9682934880256653, 0.9716765284538269, 0.9615880846977234, 1.00050950050354, 0.9907083511352539, 0.9870297312736511, 0.9672355651855469, 1.0130773782730103, 0.9932787418365479, 0.9579426646232605, 0.9728312492370605, 0.9947850704193115, 0.9672518968582153, 0.9804143905639648, 0.9669634103775024, 1.0014845132827759, 0.999232292175293]
[2025-05-14 05:14:09,992]: Mean: 0.98319519
[2025-05-14 05:14:09,992]: Min: 0.95794266
[2025-05-14 05:14:09,993]: Max: 1.02439928
[2025-05-14 05:14:09,994]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 05:14:09,995]: Sample Values (25 elements): [-0.0217293594032526, -0.0217293594032526, 0.0, -0.0217293594032526, 0.0434587188065052, -0.0217293594032526, -0.0434587188065052, 0.0217293594032526, -0.0217293594032526, 0.0434587188065052, 0.0, -0.0217293594032526, 0.0217293594032526, 0.0, 0.0217293594032526, 0.0434587188065052, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0217293594032526, 0.0, 0.0, 0.0]
[2025-05-14 05:14:09,995]: Mean: -0.00049042
[2025-05-14 05:14:09,995]: Min: -0.06518808
[2025-05-14 05:14:09,995]: Max: 0.08691744
[2025-05-14 05:14:09,995]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 05:14:09,995]: Sample Values (25 elements): [0.9698416590690613, 0.9728021025657654, 0.9610327482223511, 0.9762536287307739, 0.9610373973846436, 0.9746947288513184, 0.9703311920166016, 0.9727870225906372, 0.944408655166626, 0.9699476361274719, 0.9613733887672424, 0.9727480411529541, 0.9601930379867554, 0.9642744064331055, 0.9759829640388489, 0.9621427059173584, 0.9636667370796204, 0.9676668047904968, 0.9594787955284119, 0.9678402543067932, 0.9562354683876038, 0.9693140387535095, 0.965499997138977, 0.9633918404579163, 0.9808897376060486]
[2025-05-14 05:14:09,996]: Mean: 0.96522486
[2025-05-14 05:14:09,996]: Min: 0.94440866
[2025-05-14 05:14:09,996]: Max: 0.98666561
[2025-05-14 05:14:09,997]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 05:14:09,998]: Sample Values (25 elements): [0.01968754082918167, -0.01968754082918167, 0.0, 0.0, 0.01968754082918167, 0.0, -0.01968754082918167, 0.0, 0.0, -0.01968754082918167, -0.03937508165836334, -0.01968754082918167, 0.01968754082918167, 0.01968754082918167, -0.01968754082918167, -0.01968754082918167, -0.01968754082918167, -0.01968754082918167, 0.0, 0.01968754082918167, 0.0, 0.0, -0.01968754082918167, 0.01968754082918167, -0.03937508165836334]
[2025-05-14 05:14:09,999]: Mean: -0.00049814
[2025-05-14 05:14:09,999]: Min: -0.05906262
[2025-05-14 05:14:09,999]: Max: 0.07875016
[2025-05-14 05:14:09,999]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 05:14:09,999]: Sample Values (25 elements): [0.9511645436286926, 0.9596639275550842, 0.9548461437225342, 0.9873846769332886, 0.9616872072219849, 0.9638861417770386, 0.9600623846054077, 0.9530684351921082, 0.9604012370109558, 0.9701525568962097, 0.9644694328308105, 0.9632164835929871, 0.9546438455581665, 0.9637131690979004, 0.9586877822875977, 0.9517265558242798, 0.9541284441947937, 0.9748258590698242, 0.9719584584236145, 0.966493546962738, 0.9589571952819824, 0.9555701613426208, 0.9660622477531433, 0.9645626544952393, 0.9692526459693909]
[2025-05-14 05:14:10,000]: Mean: 0.96609586
[2025-05-14 05:14:10,000]: Min: 0.94783103
[2025-05-14 05:14:10,000]: Max: 0.99053144
[2025-05-14 05:14:10,001]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 05:14:10,001]: Sample Values (25 elements): [-0.04423369839787483, -0.08846739679574966, 0.08846739679574966, -0.08846739679574966, 0.08846739679574966, 0.04423369839787483, -0.04423369839787483, 0.0, 0.08846739679574966, 0.1327010989189148, -0.1327010989189148, -0.08846739679574966, 0.08846739679574966, 0.0, 0.0, 0.04423369839787483, 0.04423369839787483, -0.08846739679574966, -0.08846739679574966, 0.04423369839787483, 0.1327010989189148, -0.04423369839787483, 0.0, -0.08846739679574966, 0.08846739679574966]
[2025-05-14 05:14:10,002]: Mean: 0.00142010
[2025-05-14 05:14:10,002]: Min: -0.13270110
[2025-05-14 05:14:10,002]: Max: 0.17693479
[2025-05-14 05:14:10,002]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 05:14:10,002]: Sample Values (25 elements): [0.9332582354545593, 0.9309170842170715, 0.9353650808334351, 0.9514229893684387, 0.9268246293067932, 0.9420782923698425, 0.9222393035888672, 0.9200291037559509, 0.9378523230552673, 0.9448629021644592, 0.9280177354812622, 0.9223142862319946, 0.9436411261558533, 0.9358804225921631, 0.9399755597114563, 0.9258017539978027, 0.9483896493911743, 0.9331731200218201, 0.9350824952125549, 0.940401017665863, 0.9340334534645081, 0.9265593886375427, 0.9327288866043091, 0.9444995522499084, 0.9326324462890625]
[2025-05-14 05:14:10,002]: Mean: 0.93534487
[2025-05-14 05:14:10,003]: Min: 0.90037984
[2025-05-14 05:14:10,003]: Max: 0.95955211
[2025-05-14 05:14:10,004]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 05:14:10,005]: Sample Values (25 elements): [0.018826594576239586, -0.018826594576239586, 0.018826594576239586, 0.018826594576239586, -0.018826594576239586, 0.0, 0.0, 0.018826594576239586, 0.0, -0.018826594576239586, 0.0, -0.018826594576239586, 0.018826594576239586, 0.0, 0.018826594576239586, -0.018826594576239586, 0.0, 0.03765318915247917, -0.018826594576239586, -0.018826594576239586, 0.018826594576239586, -0.03765318915247917, 0.0, -0.018826594576239586, 0.0]
[2025-05-14 05:14:10,005]: Mean: -0.00045223
[2025-05-14 05:14:10,006]: Min: -0.05647978
[2025-05-14 05:14:10,006]: Max: 0.07530638
[2025-05-14 05:14:10,006]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 05:14:10,006]: Sample Values (25 elements): [0.9764755964279175, 0.9585627317428589, 0.9613324403762817, 0.9577562212944031, 0.9633177518844604, 0.9647331237792969, 0.981259286403656, 0.9593734741210938, 0.9674623012542725, 0.9629928469657898, 0.9526932239532471, 0.9648852348327637, 0.9540792107582092, 0.9779236912727356, 0.9574325084686279, 0.9606536030769348, 0.9760702848434448, 0.9580578207969666, 0.9704800248146057, 0.9628081917762756, 0.959857165813446, 0.9640156626701355, 0.9571250081062317, 0.9563112258911133, 0.9827532768249512]
[2025-05-14 05:14:10,006]: Mean: 0.96396577
[2025-05-14 05:14:10,006]: Min: 0.94501925
[2025-05-14 05:14:10,007]: Max: 0.99458426
[2025-05-14 05:14:10,008]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 05:14:10,009]: Sample Values (25 elements): [0.0, -0.018817437812685966, -0.018817437812685966, 0.018817437812685966, -0.018817437812685966, 0.0, 0.018817437812685966, 0.018817437812685966, 0.018817437812685966, -0.018817437812685966, -0.018817437812685966, 0.0, -0.018817437812685966, 0.0, 0.018817437812685966, 0.018817437812685966, -0.018817437812685966, -0.018817437812685966, 0.018817437812685966, -0.03763487562537193, -0.018817437812685966, 0.018817437812685966, -0.018817437812685966, -0.018817437812685966, 0.03763487562537193]
[2025-05-14 05:14:10,009]: Mean: -0.00013872
[2025-05-14 05:14:10,009]: Min: -0.05645231
[2025-05-14 05:14:10,010]: Max: 0.07526975
[2025-05-14 05:14:10,010]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 05:14:10,010]: Sample Values (25 elements): [1.0048108100891113, 0.9733784794807434, 0.9807617664337158, 0.9842533469200134, 0.9722058176994324, 0.9751181602478027, 0.973531186580658, 0.9872037768363953, 0.9848684072494507, 0.980198323726654, 0.9670599699020386, 0.9925130009651184, 0.9928085207939148, 0.9866346120834351, 0.9951800107955933, 0.9800930023193359, 0.9817072749137878, 0.9812182188034058, 0.9959379434585571, 0.9777960181236267, 0.9879717230796814, 0.9680509567260742, 0.9870094060897827, 0.9809065461158752, 0.9976389408111572]
[2025-05-14 05:14:10,010]: Mean: 0.98238528
[2025-05-14 05:14:10,010]: Min: 0.96620578
[2025-05-14 05:14:10,010]: Max: 1.01605272
[2025-05-14 05:14:10,012]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 05:14:10,014]: Sample Values (25 elements): [0.0, 0.0, -0.030615761876106262, 0.015307880938053131, 0.015307880938053131, 0.015307880938053131, 0.0, -0.015307880938053131, -0.015307880938053131, 0.015307880938053131, 0.0, -0.015307880938053131, 0.015307880938053131, 0.015307880938053131, -0.015307880938053131, 0.0, 0.015307880938053131, -0.030615761876106262, -0.015307880938053131, 0.0, 0.0, 0.015307880938053131, 0.0, 0.015307880938053131, 0.0]
[2025-05-14 05:14:10,014]: Mean: -0.00017057
[2025-05-14 05:14:10,015]: Min: -0.04592364
[2025-05-14 05:14:10,015]: Max: 0.06123152
[2025-05-14 05:14:10,015]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 05:14:10,015]: Sample Values (25 elements): [0.9668556451797485, 0.9660625457763672, 0.9623584747314453, 0.9673376083374023, 0.9611334800720215, 0.9421960711479187, 0.9595160484313965, 0.970009446144104, 0.9615733027458191, 0.9660419821739197, 0.9621863961219788, 0.960082471370697, 0.9582139253616333, 0.9659788012504578, 0.9632043242454529, 0.9574727416038513, 0.9643697142601013, 0.9700947403907776, 0.9629795551300049, 0.961825430393219, 0.9669062495231628, 0.962958574295044, 0.9641866683959961, 0.959399402141571, 0.9587774276733398]
[2025-05-14 05:14:10,015]: Mean: 0.96279460
[2025-05-14 05:14:10,016]: Min: 0.94219607
[2025-05-14 05:14:10,016]: Max: 0.98123825
[2025-05-14 05:14:10,017]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 05:14:10,025]: Sample Values (25 elements): [0.013315152376890182, -0.026630304753780365, 0.013315152376890182, 0.0, 0.0, 0.0, -0.026630304753780365, 0.0, -0.026630304753780365, 0.013315152376890182, 0.0, -0.013315152376890182, 0.0, 0.0, -0.013315152376890182, 0.0, 0.013315152376890182, 0.013315152376890182, -0.013315152376890182, 0.026630304753780365, -0.013315152376890182, -0.013315152376890182, -0.026630304753780365, 0.0, 0.026630304753780365]
[2025-05-14 05:14:10,025]: Mean: -0.00034639
[2025-05-14 05:14:10,026]: Min: -0.03994546
[2025-05-14 05:14:10,026]: Max: 0.05326061
[2025-05-14 05:14:10,026]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 05:14:10,026]: Sample Values (25 elements): [0.9674853086471558, 0.965722918510437, 0.9629503488540649, 0.9674258232116699, 0.9791638255119324, 0.9699850678443909, 0.9659458994865417, 0.9813941121101379, 0.9777178764343262, 0.9658847451210022, 0.9652786254882812, 0.9623333215713501, 0.9585954546928406, 0.9697124361991882, 0.964435338973999, 0.9727065563201904, 0.9716511368751526, 0.9735434651374817, 0.9636580944061279, 0.9674047827720642, 0.9685742259025574, 0.9654388427734375, 0.9643212556838989, 0.9577260613441467, 0.9609330296516418]
[2025-05-14 05:14:10,026]: Mean: 0.96809185
[2025-05-14 05:14:10,026]: Min: 0.95288467
[2025-05-14 05:14:10,027]: Max: 0.98545724
[2025-05-14 05:14:10,028]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 05:14:10,028]: Sample Values (25 elements): [0.060909464955329895, 0.030454732477664948, 0.09136419743299484, 0.060909464955329895, 0.060909464955329895, 0.0, 0.0, 0.060909464955329895, -0.030454732477664948, 0.060909464955329895, 0.030454732477664948, 0.060909464955329895, -0.060909464955329895, 0.0, 0.060909464955329895, 0.030454732477664948, 0.030454732477664948, 0.030454732477664948, -0.060909464955329895, 0.0, -0.030454732477664948, 0.030454732477664948, -0.030454732477664948, 0.0, 0.030454732477664948]
[2025-05-14 05:14:10,028]: Mean: -0.00004275
[2025-05-14 05:14:10,029]: Min: -0.09136420
[2025-05-14 05:14:10,029]: Max: 0.12181893
[2025-05-14 05:14:10,029]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 05:14:10,029]: Sample Values (25 elements): [0.9283859133720398, 0.9404173493385315, 0.9399348497390747, 0.9444862604141235, 0.9397372603416443, 0.9429315328598022, 0.9344005584716797, 0.9378566145896912, 0.9485304951667786, 0.944830596446991, 0.9436140060424805, 0.9404810667037964, 0.9351789951324463, 0.9378108382225037, 0.9361087083816528, 0.9531696438789368, 0.9369751214981079, 0.9404900670051575, 0.933475911617279, 0.9383176565170288, 0.9412721991539001, 0.9351645708084106, 0.930990993976593, 0.9305862188339233, 0.9348195791244507]
[2025-05-14 05:14:10,030]: Mean: 0.94009733
[2025-05-14 05:14:10,030]: Min: 0.92838591
[2025-05-14 05:14:10,030]: Max: 0.95650482
[2025-05-14 05:14:10,031]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 05:14:10,038]: Sample Values (25 elements): [-0.013560673221945763, 0.013560673221945763, -0.027121346443891525, 0.0, 0.013560673221945763, -0.013560673221945763, -0.013560673221945763, 0.0, -0.027121346443891525, -0.013560673221945763, 0.0, 0.0, 0.013560673221945763, -0.013560673221945763, -0.027121346443891525, -0.013560673221945763, 0.013560673221945763, 0.013560673221945763, 0.013560673221945763, 0.013560673221945763, 0.013560673221945763, 0.0, -0.013560673221945763, 0.013560673221945763, 0.0]
[2025-05-14 05:14:10,038]: Mean: -0.00035811
[2025-05-14 05:14:10,038]: Min: -0.04068202
[2025-05-14 05:14:10,039]: Max: 0.05424269
[2025-05-14 05:14:10,039]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 05:14:10,039]: Sample Values (25 elements): [0.9693568348884583, 0.9688165187835693, 0.9605045914649963, 0.9621667265892029, 0.9664283394813538, 0.9590505957603455, 0.9631378054618835, 0.9633870720863342, 0.9642204642295837, 0.965218722820282, 0.9595606327056885, 0.9591136574745178, 0.9575536847114563, 0.9660835862159729, 0.9645012021064758, 0.9579659104347229, 0.9684998393058777, 0.9611063599586487, 0.9573631286621094, 0.9596765637397766, 0.960638165473938, 0.961747944355011, 0.9646734595298767, 0.9598660469055176, 0.9496310353279114]
[2025-05-14 05:14:10,039]: Mean: 0.96223927
[2025-05-14 05:14:10,039]: Min: 0.94953138
[2025-05-14 05:14:10,040]: Max: 0.97857618
[2025-05-14 05:14:10,041]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 05:14:10,047]: Sample Values (25 elements): [0.012576674111187458, -0.012576674111187458, -0.012576674111187458, 0.012576674111187458, 0.025153348222374916, 0.0, -0.012576674111187458, 0.0, -0.012576674111187458, 0.012576674111187458, 0.0, 0.0, 0.012576674111187458, 0.012576674111187458, -0.012576674111187458, 0.0, -0.025153348222374916, -0.025153348222374916, 0.0, -0.012576674111187458, -0.012576674111187458, -0.012576674111187458, -0.012576674111187458, 0.025153348222374916, 0.012576674111187458]
[2025-05-14 05:14:10,047]: Mean: -0.00009657
[2025-05-14 05:14:10,047]: Min: -0.05030670
[2025-05-14 05:14:10,047]: Max: 0.03773002
[2025-05-14 05:14:10,047]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 05:14:10,048]: Sample Values (25 elements): [0.9673118591308594, 0.9827606081962585, 0.9687409996986389, 0.9686944484710693, 0.9710116386413574, 0.975146472454071, 0.9822397232055664, 0.9801411628723145, 0.9802514910697937, 0.9729127883911133, 0.9801645874977112, 0.9721903800964355, 0.9652531147003174, 0.969385027885437, 0.976692795753479, 0.9688754677772522, 0.9641332626342773, 0.977766215801239, 0.9724636077880859, 0.9637416005134583, 0.9683660268783569, 0.9704148769378662, 0.9683669209480286, 0.9754742383956909, 0.968876838684082]
[2025-05-14 05:14:10,048]: Mean: 0.97342253
[2025-05-14 05:14:10,048]: Min: 0.95929915
[2025-05-14 05:14:10,048]: Max: 0.99306285
[2025-05-14 05:14:10,049]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 05:14:10,064]: Sample Values (25 elements): [0.011015097610652447, 0.0, 0.0, 0.0, 0.0, -0.022030195221304893, 0.011015097610652447, -0.011015097610652447, 0.011015097610652447, 0.022030195221304893, 0.011015097610652447, 0.0, 0.011015097610652447, 0.0, 0.022030195221304893, 0.0, 0.011015097610652447, 0.0, 0.011015097610652447, 0.0, 0.011015097610652447, 0.022030195221304893, 0.0, -0.011015097610652447, -0.011015097610652447]
[2025-05-14 05:14:10,064]: Mean: -0.00000038
[2025-05-14 05:14:10,065]: Min: -0.03304529
[2025-05-14 05:14:10,065]: Max: 0.04406039
[2025-05-14 05:14:10,065]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 05:14:10,065]: Sample Values (25 elements): [0.9629307389259338, 0.9594382643699646, 0.9569023847579956, 0.9623951315879822, 0.9675617814064026, 0.9578696489334106, 0.9598575830459595, 0.9612098336219788, 0.9628134369850159, 0.9592047333717346, 0.9636245369911194, 0.9633272886276245, 0.9572742581367493, 0.9592309594154358, 0.9594876170158386, 0.9600773453712463, 0.9584867358207703, 0.9586225152015686, 0.9625105857849121, 0.9620475172996521, 0.9608041048049927, 0.9601958990097046, 0.9585159420967102, 0.9633737206459045, 0.9578782916069031]
[2025-05-14 05:14:10,065]: Mean: 0.96020663
[2025-05-14 05:14:10,066]: Min: 0.95383692
[2025-05-14 05:14:10,066]: Max: 0.97015411
[2025-05-14 05:14:10,067]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 05:14:10,111]: Sample Values (25 elements): [-0.008349820040166378, 0.008349820040166378, 0.0, -0.008349820040166378, 0.016699640080332756, -0.008349820040166378, -0.008349820040166378, 0.008349820040166378, -0.008349820040166378, 0.008349820040166378, 0.008349820040166378, -0.008349820040166378, -0.008349820040166378, 0.008349820040166378, -0.008349820040166378, 0.0, -0.008349820040166378, 0.008349820040166378, 0.0, 0.008349820040166378, -0.008349820040166378, -0.008349820040166378, -0.008349820040166378, -0.008349820040166378, 0.0]
[2025-05-14 05:14:10,112]: Mean: -0.00002935
[2025-05-14 05:14:10,112]: Min: -0.03339928
[2025-05-14 05:14:10,112]: Max: 0.02504946
[2025-05-14 05:14:10,112]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 05:14:10,113]: Sample Values (25 elements): [0.9644601941108704, 0.9610090255737305, 0.9642298817634583, 0.9660333395004272, 0.9639542102813721, 0.9705742001533508, 0.965412974357605, 0.9684933423995972, 0.9660824537277222, 0.9609900116920471, 0.9663601517677307, 0.9604555368423462, 0.9669361710548401, 0.9673328995704651, 0.9621664881706238, 0.9612151384353638, 0.9662047624588013, 0.9626936316490173, 0.966322660446167, 0.9677379727363586, 0.9639528393745422, 0.9619795680046082, 0.9683051705360413, 0.9623486399650574, 0.9642322659492493]
[2025-05-14 05:14:10,114]: Mean: 0.96455210
[2025-05-14 05:14:10,114]: Min: 0.95864058
[2025-05-14 05:14:10,114]: Max: 0.97616637
[2025-05-14 05:14:10,115]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 05:14:10,116]: Sample Values (25 elements): [0.020511707291007042, -0.020511707291007042, -0.06153512001037598, -0.06153512001037598, -0.041023414582014084, 0.0, -0.020511707291007042, 0.0, 0.0, -0.020511707291007042, 0.06153512001037598, 0.020511707291007042, 0.041023414582014084, -0.020511707291007042, 0.020511707291007042, -0.06153512001037598, 0.041023414582014084, -0.041023414582014084, 0.020511707291007042, -0.06153512001037598, 0.0, 0.041023414582014084, -0.041023414582014084, 0.06153512001037598, 0.041023414582014084]
[2025-05-14 05:14:10,117]: Mean: 0.00013067
[2025-05-14 05:14:10,117]: Min: -0.06153512
[2025-05-14 05:14:10,117]: Max: 0.08204683
[2025-05-14 05:14:10,117]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 05:14:10,117]: Sample Values (25 elements): [0.9575274586677551, 0.9550278186798096, 0.9560838937759399, 0.9571797251701355, 0.9539611339569092, 0.956704318523407, 0.957219123840332, 0.9547156095504761, 0.9546657800674438, 0.9570515155792236, 0.9578937292098999, 0.9558978080749512, 0.9577236175537109, 0.9528747200965881, 0.9535611271858215, 0.9572807550430298, 0.9558159708976746, 0.9548271894454956, 0.9575560092926025, 0.9553216695785522, 0.9553120732307434, 0.9562731385231018, 0.9583873748779297, 0.9563079476356506, 0.9562004804611206]
[2025-05-14 05:14:10,117]: Mean: 0.95611715
[2025-05-14 05:14:10,117]: Min: 0.95012766
[2025-05-14 05:14:10,118]: Max: 0.96127772
[2025-05-14 05:14:10,119]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 05:14:10,159]: Sample Values (25 elements): [-0.008092528209090233, 0.008092528209090233, 0.0, 0.0, 0.008092528209090233, 0.0, -0.008092528209090233, -0.008092528209090233, 0.016185056418180466, -0.008092528209090233, 0.008092528209090233, 0.0, 0.008092528209090233, 0.0, -0.016185056418180466, 0.0, -0.008092528209090233, -0.008092528209090233, 0.008092528209090233, -0.008092528209090233, 0.008092528209090233, -0.008092528209090233, 0.008092528209090233, 0.0, 0.0]
[2025-05-14 05:14:10,159]: Mean: -0.00010902
[2025-05-14 05:14:10,159]: Min: -0.02427758
[2025-05-14 05:14:10,160]: Max: 0.03237011
[2025-05-14 05:14:10,160]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 05:14:10,160]: Sample Values (25 elements): [0.959547221660614, 0.9584596157073975, 0.9595733284950256, 0.957205593585968, 0.9563083648681641, 0.9583012461662292, 0.9593338370323181, 0.9624184370040894, 0.9636952877044678, 0.9597153663635254, 0.960627555847168, 0.9606802463531494, 0.9584347605705261, 0.9600549340248108, 0.9629219174385071, 0.9583507180213928, 0.9568634629249573, 0.9571422934532166, 0.9576632380485535, 0.971298336982727, 0.9585393071174622, 0.9587710499763489, 0.9565351009368896, 0.9595731496810913, 0.966139554977417]
[2025-05-14 05:14:10,160]: Mean: 0.95924854
[2025-05-14 05:14:10,160]: Min: 0.95485747
[2025-05-14 05:14:10,160]: Max: 0.97375888
[2025-05-14 05:14:10,162]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 05:14:10,200]: Sample Values (25 elements): [-0.006082737352699041, 0.006082737352699041, 0.006082737352699041, 0.0, -0.006082737352699041, -0.012165474705398083, 0.0, 0.012165474705398083, -0.012165474705398083, -0.012165474705398083, -0.006082737352699041, 0.006082737352699041, 0.012165474705398083, -0.012165474705398083, 0.018248211592435837, 0.0, 0.0, 0.012165474705398083, -0.012165474705398083, 0.012165474705398083, -0.006082737352699041, 0.0, -0.006082737352699041, 0.012165474705398083, 0.0]
[2025-05-14 05:14:10,200]: Mean: 0.00004902
[2025-05-14 05:14:10,201]: Min: -0.01824821
[2025-05-14 05:14:10,201]: Max: 0.02433095
[2025-05-14 05:14:10,201]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 05:14:10,201]: Sample Values (25 elements): [0.9668734669685364, 0.9675074219703674, 0.966459333896637, 0.9686955809593201, 0.9662686586380005, 0.9684114456176758, 0.9678123593330383, 0.9669267535209656, 0.9641928672790527, 0.9691926836967468, 0.9667347073554993, 0.9693410992622375, 0.9648082256317139, 0.9710158705711365, 0.9634087681770325, 0.968610942363739, 0.9736658334732056, 0.9691582322120667, 0.970079243183136, 0.969021737575531, 0.9693097472190857, 0.9712533950805664, 0.9747139811515808, 0.9657949209213257, 0.9667456746101379]
[2025-05-14 05:14:10,201]: Mean: 0.96866405
[2025-05-14 05:14:10,201]: Min: 0.96073765
[2025-05-14 05:14:10,202]: Max: 0.97667277
[2025-05-14 05:14:10,202]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 05:14:10,202]: Sample Values (25 elements): [-0.07584936916828156, -0.022890880703926086, -0.03493363410234451, 0.08812572062015533, -0.06699312478303909, -0.07910428196191788, 0.12359373271465302, -0.03712525591254234, -0.028131680563092232, 0.10361229628324509, -0.08693414181470871, 0.08356469869613647, 0.041692204773426056, 0.070530965924263, -0.04253048077225685, -0.03915395960211754, 0.09385712444782257, 0.09148779511451721, 0.0727878138422966, -0.05388551950454712, 0.09452249109745026, 0.012357819825410843, 0.05137593671679497, 0.0741732195019722, -0.08169364929199219]
[2025-05-14 05:14:10,202]: Mean: -0.00025994
[2025-05-14 05:14:10,202]: Min: -0.12269092
[2025-05-14 05:14:10,203]: Max: 0.15687399
[2025-05-14 05:14:10,203]: 


QAT of ResNet18 with relu6 down to 2 bits...
[2025-05-14 05:14:10,432]: [ResNet18_relu6_quantized_2_bits] after configure_qat:
[2025-05-14 05:14:10,479]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 05:16:09,644]: [ResNet18_relu6_quantized_2_bits] Epoch: 001 Train Loss: 0.9280 Train Acc: 0.6858 Eval Loss: 0.7586 Eval Acc: 0.7473 (LR: 0.001000)
[2025-05-14 05:18:08,682]: [ResNet18_relu6_quantized_2_bits] Epoch: 002 Train Loss: 0.6657 Train Acc: 0.7663 Eval Loss: 0.6592 Eval Acc: 0.7803 (LR: 0.001000)
[2025-05-14 05:20:07,760]: [ResNet18_relu6_quantized_2_bits] Epoch: 003 Train Loss: 0.5906 Train Acc: 0.7928 Eval Loss: 0.6001 Eval Acc: 0.7990 (LR: 0.001000)
[2025-05-14 05:22:06,612]: [ResNet18_relu6_quantized_2_bits] Epoch: 004 Train Loss: 0.5513 Train Acc: 0.8069 Eval Loss: 0.6088 Eval Acc: 0.7953 (LR: 0.001000)
[2025-05-14 05:24:05,517]: [ResNet18_relu6_quantized_2_bits] Epoch: 005 Train Loss: 0.5236 Train Acc: 0.8164 Eval Loss: 0.5722 Eval Acc: 0.8099 (LR: 0.001000)
[2025-05-14 05:26:04,615]: [ResNet18_relu6_quantized_2_bits] Epoch: 006 Train Loss: 0.4998 Train Acc: 0.8237 Eval Loss: 0.5601 Eval Acc: 0.8128 (LR: 0.001000)
[2025-05-14 05:28:03,680]: [ResNet18_relu6_quantized_2_bits] Epoch: 007 Train Loss: 0.4861 Train Acc: 0.8284 Eval Loss: 0.5469 Eval Acc: 0.8181 (LR: 0.001000)
[2025-05-14 05:30:02,986]: [ResNet18_relu6_quantized_2_bits] Epoch: 008 Train Loss: 0.4678 Train Acc: 0.8355 Eval Loss: 0.5449 Eval Acc: 0.8238 (LR: 0.001000)
[2025-05-14 05:32:02,210]: [ResNet18_relu6_quantized_2_bits] Epoch: 009 Train Loss: 0.4560 Train Acc: 0.8407 Eval Loss: 0.5390 Eval Acc: 0.8188 (LR: 0.001000)
[2025-05-14 05:34:01,341]: [ResNet18_relu6_quantized_2_bits] Epoch: 010 Train Loss: 0.4432 Train Acc: 0.8442 Eval Loss: 0.5246 Eval Acc: 0.8223 (LR: 0.001000)
[2025-05-14 05:36:00,442]: [ResNet18_relu6_quantized_2_bits] Epoch: 011 Train Loss: 0.4353 Train Acc: 0.8453 Eval Loss: 0.5066 Eval Acc: 0.8325 (LR: 0.001000)
[2025-05-14 05:37:59,716]: [ResNet18_relu6_quantized_2_bits] Epoch: 012 Train Loss: 0.4236 Train Acc: 0.8489 Eval Loss: 0.5077 Eval Acc: 0.8302 (LR: 0.001000)
[2025-05-14 05:39:52,365]: [ResNet18_relu6_quantized_2_bits] Epoch: 013 Train Loss: 0.4197 Train Acc: 0.8503 Eval Loss: 0.4781 Eval Acc: 0.8415 (LR: 0.001000)
[2025-05-14 05:41:40,588]: [ResNet18_relu6_quantized_2_bits] Epoch: 014 Train Loss: 0.4076 Train Acc: 0.8572 Eval Loss: 0.5005 Eval Acc: 0.8357 (LR: 0.001000)
[2025-05-14 05:43:24,888]: [ResNet18_relu6_quantized_2_bits] Epoch: 015 Train Loss: 0.4013 Train Acc: 0.8586 Eval Loss: 0.4778 Eval Acc: 0.8425 (LR: 0.001000)
[2025-05-14 05:45:08,727]: [ResNet18_relu6_quantized_2_bits] Epoch: 016 Train Loss: 0.3990 Train Acc: 0.8599 Eval Loss: 0.4746 Eval Acc: 0.8415 (LR: 0.001000)
[2025-05-14 05:46:48,121]: [ResNet18_relu6_quantized_2_bits] Epoch: 017 Train Loss: 0.3850 Train Acc: 0.8640 Eval Loss: 0.4900 Eval Acc: 0.8370 (LR: 0.001000)
[2025-05-14 05:48:27,509]: [ResNet18_relu6_quantized_2_bits] Epoch: 018 Train Loss: 0.3851 Train Acc: 0.8639 Eval Loss: 0.4722 Eval Acc: 0.8490 (LR: 0.001000)
[2025-05-14 05:50:07,037]: [ResNet18_relu6_quantized_2_bits] Epoch: 019 Train Loss: 0.3773 Train Acc: 0.8675 Eval Loss: 0.5051 Eval Acc: 0.8363 (LR: 0.001000)
[2025-05-14 05:51:52,559]: [ResNet18_relu6_quantized_2_bits] Epoch: 020 Train Loss: 0.3734 Train Acc: 0.8686 Eval Loss: 0.4978 Eval Acc: 0.8392 (LR: 0.001000)
[2025-05-14 05:53:36,656]: [ResNet18_relu6_quantized_2_bits] Epoch: 021 Train Loss: 0.3657 Train Acc: 0.8700 Eval Loss: 0.4894 Eval Acc: 0.8399 (LR: 0.001000)
[2025-05-14 05:55:20,217]: [ResNet18_relu6_quantized_2_bits] Epoch: 022 Train Loss: 0.3574 Train Acc: 0.8747 Eval Loss: 0.4954 Eval Acc: 0.8381 (LR: 0.001000)
[2025-05-14 05:57:00,869]: [ResNet18_relu6_quantized_2_bits] Epoch: 023 Train Loss: 0.3597 Train Acc: 0.8723 Eval Loss: 0.5010 Eval Acc: 0.8363 (LR: 0.001000)
[2025-05-14 05:58:46,221]: [ResNet18_relu6_quantized_2_bits] Epoch: 024 Train Loss: 0.3477 Train Acc: 0.8763 Eval Loss: 0.4838 Eval Acc: 0.8471 (LR: 0.001000)
[2025-05-14 06:00:32,496]: [ResNet18_relu6_quantized_2_bits] Epoch: 025 Train Loss: 0.3428 Train Acc: 0.8791 Eval Loss: 0.4664 Eval Acc: 0.8498 (LR: 0.001000)
[2025-05-14 06:02:18,871]: [ResNet18_relu6_quantized_2_bits] Epoch: 026 Train Loss: 0.3487 Train Acc: 0.8756 Eval Loss: 0.4506 Eval Acc: 0.8562 (LR: 0.001000)
[2025-05-14 06:04:05,547]: [ResNet18_relu6_quantized_2_bits] Epoch: 027 Train Loss: 0.3389 Train Acc: 0.8795 Eval Loss: 0.4529 Eval Acc: 0.8524 (LR: 0.001000)
[2025-05-14 06:05:51,869]: [ResNet18_relu6_quantized_2_bits] Epoch: 028 Train Loss: 0.3365 Train Acc: 0.8816 Eval Loss: 0.4750 Eval Acc: 0.8473 (LR: 0.001000)
[2025-05-14 06:07:37,813]: [ResNet18_relu6_quantized_2_bits] Epoch: 029 Train Loss: 0.3325 Train Acc: 0.8819 Eval Loss: 0.4472 Eval Acc: 0.8577 (LR: 0.001000)
[2025-05-14 06:09:22,018]: [ResNet18_relu6_quantized_2_bits] Epoch: 030 Train Loss: 0.3278 Train Acc: 0.8841 Eval Loss: 0.4331 Eval Acc: 0.8585 (LR: 0.000250)
[2025-05-14 06:11:04,419]: [ResNet18_relu6_quantized_2_bits] Epoch: 031 Train Loss: 0.2905 Train Acc: 0.8973 Eval Loss: 0.4111 Eval Acc: 0.8667 (LR: 0.000250)
[2025-05-14 06:12:46,908]: [ResNet18_relu6_quantized_2_bits] Epoch: 032 Train Loss: 0.2815 Train Acc: 0.9004 Eval Loss: 0.4063 Eval Acc: 0.8705 (LR: 0.000250)
[2025-05-14 06:14:29,405]: [ResNet18_relu6_quantized_2_bits] Epoch: 033 Train Loss: 0.2816 Train Acc: 0.8994 Eval Loss: 0.3943 Eval Acc: 0.8679 (LR: 0.000250)
[2025-05-14 06:16:11,758]: [ResNet18_relu6_quantized_2_bits] Epoch: 034 Train Loss: 0.2808 Train Acc: 0.8985 Eval Loss: 0.4113 Eval Acc: 0.8655 (LR: 0.000250)
[2025-05-14 06:17:54,210]: [ResNet18_relu6_quantized_2_bits] Epoch: 035 Train Loss: 0.2797 Train Acc: 0.9001 Eval Loss: 0.4159 Eval Acc: 0.8631 (LR: 0.000250)
[2025-05-14 06:19:36,707]: [ResNet18_relu6_quantized_2_bits] Epoch: 036 Train Loss: 0.2770 Train Acc: 0.9017 Eval Loss: 0.4005 Eval Acc: 0.8682 (LR: 0.000250)
[2025-05-14 06:21:19,178]: [ResNet18_relu6_quantized_2_bits] Epoch: 037 Train Loss: 0.2752 Train Acc: 0.9027 Eval Loss: 0.4000 Eval Acc: 0.8729 (LR: 0.000250)
[2025-05-14 06:23:01,399]: [ResNet18_relu6_quantized_2_bits] Epoch: 038 Train Loss: 0.2723 Train Acc: 0.9053 Eval Loss: 0.4115 Eval Acc: 0.8686 (LR: 0.000250)
[2025-05-14 06:24:43,729]: [ResNet18_relu6_quantized_2_bits] Epoch: 039 Train Loss: 0.2736 Train Acc: 0.9027 Eval Loss: 0.3965 Eval Acc: 0.8732 (LR: 0.000250)
[2025-05-14 06:26:29,759]: [ResNet18_relu6_quantized_2_bits] Epoch: 040 Train Loss: 0.2717 Train Acc: 0.9032 Eval Loss: 0.4157 Eval Acc: 0.8634 (LR: 0.000250)
[2025-05-14 06:28:13,287]: [ResNet18_relu6_quantized_2_bits] Epoch: 041 Train Loss: 0.2719 Train Acc: 0.9032 Eval Loss: 0.4125 Eval Acc: 0.8667 (LR: 0.000250)
[2025-05-14 06:30:13,241]: [ResNet18_relu6_quantized_2_bits] Epoch: 042 Train Loss: 0.2704 Train Acc: 0.9035 Eval Loss: 0.4058 Eval Acc: 0.8702 (LR: 0.000250)
[2025-05-14 06:32:10,656]: [ResNet18_relu6_quantized_2_bits] Epoch: 043 Train Loss: 0.2655 Train Acc: 0.9060 Eval Loss: 0.4276 Eval Acc: 0.8656 (LR: 0.000250)
[2025-05-14 06:34:09,679]: [ResNet18_relu6_quantized_2_bits] Epoch: 044 Train Loss: 0.2603 Train Acc: 0.9076 Eval Loss: 0.4068 Eval Acc: 0.8715 (LR: 0.000250)
[2025-05-14 06:36:08,235]: [ResNet18_relu6_quantized_2_bits] Epoch: 045 Train Loss: 0.2644 Train Acc: 0.9062 Eval Loss: 0.4197 Eval Acc: 0.8720 (LR: 0.000063)
[2025-05-14 06:38:06,987]: [ResNet18_relu6_quantized_2_bits] Epoch: 046 Train Loss: 0.2565 Train Acc: 0.9075 Eval Loss: 0.3981 Eval Acc: 0.8720 (LR: 0.000063)
[2025-05-14 06:40:05,477]: [ResNet18_relu6_quantized_2_bits] Epoch: 047 Train Loss: 0.2499 Train Acc: 0.9111 Eval Loss: 0.3914 Eval Acc: 0.8763 (LR: 0.000063)
[2025-05-14 06:42:04,550]: [ResNet18_relu6_quantized_2_bits] Epoch: 048 Train Loss: 0.2492 Train Acc: 0.9111 Eval Loss: 0.3987 Eval Acc: 0.8717 (LR: 0.000063)
[2025-05-14 06:44:02,865]: [ResNet18_relu6_quantized_2_bits] Epoch: 049 Train Loss: 0.2490 Train Acc: 0.9122 Eval Loss: 0.3889 Eval Acc: 0.8750 (LR: 0.000063)
[2025-05-14 06:45:51,371]: [ResNet18_relu6_quantized_2_bits] Epoch: 050 Train Loss: 0.2460 Train Acc: 0.9117 Eval Loss: 0.3970 Eval Acc: 0.8746 (LR: 0.000063)
[2025-05-14 06:47:34,531]: [ResNet18_relu6_quantized_2_bits] Epoch: 051 Train Loss: 0.2511 Train Acc: 0.9105 Eval Loss: 0.3898 Eval Acc: 0.8771 (LR: 0.000063)
[2025-05-14 06:49:18,288]: [ResNet18_relu6_quantized_2_bits] Epoch: 052 Train Loss: 0.2475 Train Acc: 0.9108 Eval Loss: 0.4066 Eval Acc: 0.8693 (LR: 0.000063)
[2025-05-14 06:51:04,293]: [ResNet18_relu6_quantized_2_bits] Epoch: 053 Train Loss: 0.2475 Train Acc: 0.9110 Eval Loss: 0.4021 Eval Acc: 0.8733 (LR: 0.000063)
[2025-05-14 06:52:47,230]: [ResNet18_relu6_quantized_2_bits] Epoch: 054 Train Loss: 0.2429 Train Acc: 0.9134 Eval Loss: 0.3921 Eval Acc: 0.8769 (LR: 0.000063)
[2025-05-14 06:54:30,259]: [ResNet18_relu6_quantized_2_bits] Epoch: 055 Train Loss: 0.2472 Train Acc: 0.9116 Eval Loss: 0.3963 Eval Acc: 0.8758 (LR: 0.000063)
[2025-05-14 06:56:13,242]: [ResNet18_relu6_quantized_2_bits] Epoch: 056 Train Loss: 0.2504 Train Acc: 0.9127 Eval Loss: 0.3990 Eval Acc: 0.8720 (LR: 0.000063)
[2025-05-14 06:57:56,243]: [ResNet18_relu6_quantized_2_bits] Epoch: 057 Train Loss: 0.2430 Train Acc: 0.9133 Eval Loss: 0.3966 Eval Acc: 0.8725 (LR: 0.000063)
[2025-05-14 06:59:39,250]: [ResNet18_relu6_quantized_2_bits] Epoch: 058 Train Loss: 0.2458 Train Acc: 0.9121 Eval Loss: 0.3965 Eval Acc: 0.8743 (LR: 0.000063)
[2025-05-14 07:01:17,570]: [ResNet18_relu6_quantized_2_bits] Epoch: 059 Train Loss: 0.2448 Train Acc: 0.9133 Eval Loss: 0.3980 Eval Acc: 0.8731 (LR: 0.000063)
[2025-05-14 07:02:55,506]: [ResNet18_relu6_quantized_2_bits] Epoch: 060 Train Loss: 0.2421 Train Acc: 0.9136 Eval Loss: 0.3954 Eval Acc: 0.8751 (LR: 0.000063)
[2025-05-14 07:02:55,507]: [ResNet18_relu6_quantized_2_bits] Best Eval Accuracy: 0.8771
[2025-05-14 07:02:55,594]: 


Quantization of model down to 2 bits finished
[2025-05-14 07:02:55,594]: Model Architecture:
[2025-05-14 07:02:55,643]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): ReLU6(inplace=True)
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1291], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1671495884656906, max_val=0.2202131152153015)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0875], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12580306828022003, max_val=0.13677893579006195)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0707], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10436691343784332, max_val=0.10773254185914993)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0613], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08550137281417847, max_val=0.09829827398061752)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0540], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08096089214086533, max_val=0.08100467175245285)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.999958038330078)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0471], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06668483465909958, max_val=0.07446561753749847)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1104], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16530637443065643, max_val=0.16590352356433868)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0430], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05975418537855148, max_val=0.06922906637191772)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9992], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.997503280639648)
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0440], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05752953141927719, max_val=0.07446562498807907)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0366], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05247940868139267, max_val=0.05730463191866875)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9986], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.995893478393555)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0298], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04426989704370499, max_val=0.045221976935863495)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0737], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10716653615236282, max_val=0.11381234973669052)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04705756530165672, max_val=0.04719819501042366)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.999959945678711)
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0294], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04462645947933197, max_val=0.0436931848526001)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.039246171712875366, max_val=0.04030478000640869)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9997], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.999133110046387)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0187], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.026925047859549522, max_val=0.029170284047722816)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0489], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07075341045856476, max_val=0.07609343528747559)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0187], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.027507150545716286, max_val=0.028671542182564735)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9992], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.99747371673584)
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02023027464747429, max_val=0.02209051139652729)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): ReLU6(
          inplace=True
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 07:02:55,643]: 
Model Weights:
[2025-05-14 07:02:55,643]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 07:02:55,643]: Sample Values (25 elements): [0.17178042232990265, 0.18987992405891418, -0.170465350151062, -0.23193226754665375, 0.011637965217232704, -0.12083540856838226, -0.082899309694767, -0.14216868579387665, 0.07235458493232727, 0.03615126013755798, -0.0858675166964531, -0.0808277577161789, 0.06731570512056351, 0.01768297329545021, -0.0066154892556369305, 0.0727616474032402, 0.1481575220823288, 0.08640822023153305, 0.23654575645923615, -0.015061765909194946, -0.04800733923912048, 0.12017481029033661, -0.12917520105838776, -0.03185883164405823, 0.11487868428230286]
[2025-05-14 07:02:55,644]: Mean: 0.00047331
[2025-05-14 07:02:55,644]: Min: -0.36481836
[2025-05-14 07:02:55,644]: Max: 0.35744652
[2025-05-14 07:02:55,644]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 07:02:55,644]: Sample Values (25 elements): [1.0812036991119385, 0.9601947069168091, 1.1443003416061401, 0.8614438772201538, 1.158492922782898, 1.1234511137008667, 1.2180936336517334, 1.2724833488464355, 1.0027555227279663, 1.2691373825073242, 1.0301744937896729, 1.1554616689682007, 1.035020351409912, 1.0216434001922607, 0.9109259247779846, 1.1890865564346313, 1.0672650337219238, 1.009127140045166, 0.9667928218841553, 1.036045789718628, 0.9556416273117065, 1.2919520139694214, 1.3824769258499146, 1.0945408344268799, 0.9738989472389221]
[2025-05-14 07:02:55,644]: Mean: 1.06757927
[2025-05-14 07:02:55,645]: Min: 0.86144388
[2025-05-14 07:02:55,645]: Max: 1.38247693
[2025-05-14 07:02:55,646]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 07:02:55,646]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 07:02:55,646]: Mean: -0.00097723
[2025-05-14 07:02:55,647]: Min: -0.12912087
[2025-05-14 07:02:55,647]: Max: 0.25824174
[2025-05-14 07:02:55,647]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 07:02:55,647]: Sample Values (25 elements): [0.9961971640586853, 0.9798779487609863, 0.9671370983123779, 1.0202267169952393, 0.9759477972984314, 0.9703041911125183, 0.9683613181114197, 1.0023208856582642, 1.230803370475769, 0.9765040874481201, 0.999770998954773, 0.9921945333480835, 0.9479597806930542, 1.0147970914840698, 1.1402838230133057, 0.9948171973228455, 1.0092321634292603, 1.0234614610671997, 1.0566213130950928, 0.9820916652679443, 1.0273131132125854, 1.140609622001648, 1.2620383501052856, 0.9358072876930237, 1.003787875175476]
[2025-05-14 07:02:55,647]: Mean: 1.01177597
[2025-05-14 07:02:55,647]: Min: 0.93580729
[2025-05-14 07:02:55,648]: Max: 1.26203835
[2025-05-14 07:02:55,648]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 07:02:55,649]: Sample Values (25 elements): [0.0, 0.08752735704183578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08752735704183578, 0.0, 0.0, 0.0, 0.0, -0.08752735704183578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 07:02:55,649]: Mean: -0.00169290
[2025-05-14 07:02:55,649]: Min: -0.08752736
[2025-05-14 07:02:55,649]: Max: 0.17505471
[2025-05-14 07:02:55,649]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 07:02:55,650]: Sample Values (25 elements): [0.9594720005989075, 1.012164831161499, 1.0050321817398071, 1.0406285524368286, 0.9886822700500488, 0.9852364659309387, 0.965410053730011, 1.0039596557617188, 0.975687563419342, 0.9795331358909607, 0.9840602278709412, 1.005036473274231, 1.0353847742080688, 0.9706349968910217, 0.9801064729690552, 1.016127109527588, 1.0192838907241821, 1.0517587661743164, 0.9983987212181091, 1.0968610048294067, 0.9799874424934387, 1.0052833557128906, 0.9831749796867371, 0.9592944979667664, 0.970172107219696]
[2025-05-14 07:02:55,650]: Mean: 0.99161482
[2025-05-14 07:02:55,650]: Min: 0.94595981
[2025-05-14 07:02:55,650]: Max: 1.09686100
[2025-05-14 07:02:55,651]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 07:02:55,652]: Sample Values (25 elements): [0.0, 0.0, -0.0707000195980072, 0.0, 0.0, 0.0, 0.0, 0.0707000195980072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0707000195980072, 0.0707000195980072, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 07:02:55,652]: Mean: -0.00146908
[2025-05-14 07:02:55,652]: Min: -0.07070002
[2025-05-14 07:02:55,652]: Max: 0.14140004
[2025-05-14 07:02:55,652]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 07:02:55,652]: Sample Values (25 elements): [0.9802708625793457, 0.990127444267273, 1.0385265350341797, 1.003176212310791, 0.9963760375976562, 0.9696499705314636, 0.9643441438674927, 0.9929790496826172, 0.9677529335021973, 0.9674139618873596, 1.0026403665542603, 1.0078394412994385, 0.9816991686820984, 0.9700830578804016, 0.9743000864982605, 0.981312096118927, 0.9560627937316895, 0.9799614548683167, 0.9995369911193848, 0.9745445251464844, 1.0141773223876953, 0.9794411659240723, 0.986941397190094, 1.002111554145813, 1.0025101900100708]
[2025-05-14 07:02:55,653]: Mean: 0.98775196
[2025-05-14 07:02:55,653]: Min: 0.95606279
[2025-05-14 07:02:55,653]: Max: 1.03852654
[2025-05-14 07:02:55,654]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 07:02:55,654]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.06126659736037254, 0.06126659736037254, 0.0, 0.06126659736037254, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06126659736037254, -0.06126659736037254, 0.0, -0.06126659736037254, 0.0, 0.0, 0.0, -0.06126659736037254, -0.06126659736037254, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 07:02:55,654]: Mean: -0.00074622
[2025-05-14 07:02:55,655]: Min: -0.06126660
[2025-05-14 07:02:55,655]: Max: 0.12253319
[2025-05-14 07:02:55,655]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 07:02:55,655]: Sample Values (25 elements): [1.0315855741500854, 1.0499314069747925, 0.9942207932472229, 0.9822433590888977, 1.001681923866272, 1.0158525705337524, 0.9987414479255676, 0.998980700969696, 1.0173635482788086, 0.9961369633674622, 1.0160760879516602, 0.9912639856338501, 0.9876890182495117, 0.9734336733818054, 0.9852294325828552, 0.9950567483901978, 1.0082831382751465, 1.0313217639923096, 1.0180848836898804, 0.978573203086853, 0.9898636937141418, 0.9762529134750366, 1.0326474905014038, 0.9934141039848328, 0.9911538362503052]
[2025-05-14 07:02:55,655]: Mean: 1.00176454
[2025-05-14 07:02:55,655]: Min: 0.96522009
[2025-05-14 07:02:55,656]: Max: 1.04993141
[2025-05-14 07:02:55,656]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 07:02:55,657]: Sample Values (25 elements): [0.0, -0.05398852750658989, 0.0, 0.0, 0.0, -0.05398852750658989, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05398852750658989, -0.05398852750658989, 0.0, 0.0, 0.0, -0.05398852750658989, 0.0, 0.05398852750658989, 0.0, -0.05398852750658989, 0.0, 0.0, -0.05398852750658989, 0.0]
[2025-05-14 07:02:55,657]: Mean: -0.00062023
[2025-05-14 07:02:55,658]: Min: -0.05398853
[2025-05-14 07:02:55,658]: Max: 0.10797706
[2025-05-14 07:02:55,658]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 07:02:55,658]: Sample Values (25 elements): [1.0002840757369995, 0.9955506920814514, 0.9720715880393982, 0.9829269051551819, 0.9685282707214355, 0.9675577878952026, 0.9799913167953491, 1.007180094718933, 0.9729645848274231, 0.9898166060447693, 0.980385959148407, 0.9946050643920898, 0.9765108227729797, 0.9671154022216797, 0.9759931564331055, 0.9941251277923584, 0.9670975804328918, 0.992110550403595, 0.9753596782684326, 0.9562281370162964, 0.9808934330940247, 0.98245769739151, 0.9913198947906494, 0.968173623085022, 1.0107802152633667]
[2025-05-14 07:02:55,658]: Mean: 0.97951853
[2025-05-14 07:02:55,658]: Min: 0.95615703
[2025-05-14 07:02:55,658]: Max: 1.01202047
[2025-05-14 07:02:55,659]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 07:02:55,661]: Sample Values (25 elements): [0.047050099819898605, 0.0, 0.0, 0.0, 0.047050099819898605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.047050099819898605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.047050099819898605, 0.0]
[2025-05-14 07:02:55,661]: Mean: -0.00058487
[2025-05-14 07:02:55,661]: Min: -0.04705010
[2025-05-14 07:02:55,661]: Max: 0.09410020
[2025-05-14 07:02:55,661]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 07:02:55,662]: Sample Values (25 elements): [0.9621654152870178, 0.9912048578262329, 0.9806099534034729, 0.9679721593856812, 0.9856622815132141, 0.9626280069351196, 0.9941571354866028, 0.9891049265861511, 0.9659298062324524, 0.9822880625724792, 0.987799346446991, 1.0010747909545898, 0.9778866767883301, 0.9773847460746765, 0.9833781719207764, 0.9601797461509705, 0.9669389128684998, 0.9857592582702637, 0.9587776064872742, 0.9559592604637146, 1.0021395683288574, 0.9788898825645447, 0.9764431715011597, 0.9820173978805542, 0.9871847033500671]
[2025-05-14 07:02:55,662]: Mean: 0.98115468
[2025-05-14 07:02:55,662]: Min: 0.95595926
[2025-05-14 07:02:55,662]: Max: 1.01771891
[2025-05-14 07:02:55,663]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 07:02:55,663]: Sample Values (25 elements): [0.0, -0.11040329933166504, 0.0, -0.11040329933166504, 0.11040329933166504, 0.0, 0.0, -0.11040329933166504, -0.11040329933166504, 0.11040329933166504, 0.0, 0.0, -0.11040329933166504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11040329933166504, 0.0, -0.11040329933166504, 0.11040329933166504]
[2025-05-14 07:02:55,663]: Mean: 0.00018868
[2025-05-14 07:02:55,664]: Min: -0.11040330
[2025-05-14 07:02:55,664]: Max: 0.22080660
[2025-05-14 07:02:55,664]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 07:02:55,664]: Sample Values (25 elements): [0.9378114938735962, 0.9393607378005981, 0.9327284097671509, 0.9215274453163147, 0.9249604940414429, 0.9427647590637207, 0.9236915111541748, 0.9459039568901062, 0.9305529594421387, 0.9332232475280762, 0.9379454255104065, 0.9245855808258057, 0.93011873960495, 0.93752121925354, 0.9299402832984924, 0.9462792277336121, 0.9181587100028992, 0.9211465716362, 0.9356323480606079, 0.9151507019996643, 0.9334403276443481, 0.9435071349143982, 0.9446359276771545, 0.9145581722259521, 0.946790337562561]
[2025-05-14 07:02:55,664]: Mean: 0.93568927
[2025-05-14 07:02:55,664]: Min: 0.89609581
[2025-05-14 07:02:55,664]: Max: 0.96594983
[2025-05-14 07:02:55,665]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 07:02:55,667]: Sample Values (25 elements): [0.0, 0.0, 0.04299439117312431, 0.04299439117312431, 0.0, 0.04299439117312431, 0.0, -0.04299439117312431, 0.0, 0.04299439117312431, 0.0, -0.04299439117312431, 0.0, 0.0, 0.0, 0.04299439117312431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04299439117312431, 0.0, -0.04299439117312431, 0.0]
[2025-05-14 07:02:55,667]: Mean: -0.00045719
[2025-05-14 07:02:55,667]: Min: -0.04299439
[2025-05-14 07:02:55,667]: Max: 0.08598878
[2025-05-14 07:02:55,667]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 07:02:55,668]: Sample Values (25 elements): [1.0166736841201782, 0.9856739640235901, 0.9905182719230652, 0.9701626896858215, 0.9554524421691895, 0.9894205331802368, 0.9667538404464722, 0.9754148721694946, 0.9712886810302734, 0.9655829071998596, 0.9619795083999634, 0.9916173219680786, 0.972888171672821, 0.9614230394363403, 0.9676523804664612, 0.9935893416404724, 0.9663213491439819, 0.9756929874420166, 0.9779232740402222, 0.9838072657585144, 0.988279402256012, 0.9763585329055786, 0.9767944812774658, 0.9692437052726746, 0.9776867628097534]
[2025-05-14 07:02:55,668]: Mean: 0.97465992
[2025-05-14 07:02:55,668]: Min: 0.94852263
[2025-05-14 07:02:55,668]: Max: 1.01667368
[2025-05-14 07:02:55,669]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 07:02:55,670]: Sample Values (25 elements): [-0.04399837553501129, -0.04399837553501129, 0.0, 0.0, -0.04399837553501129, 0.0, 0.04399837553501129, 0.0, 0.0, 0.0, 0.04399837553501129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04399837553501129, 0.04399837553501129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 07:02:55,670]: Mean: -0.00011846
[2025-05-14 07:02:55,671]: Min: -0.04399838
[2025-05-14 07:02:55,671]: Max: 0.08799675
[2025-05-14 07:02:55,671]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 07:02:55,671]: Sample Values (25 elements): [0.9963432550430298, 1.0040892362594604, 0.9825246930122375, 0.978152334690094, 0.9775950312614441, 0.977812647819519, 1.0252842903137207, 0.9860904216766357, 0.987795352935791, 1.0090218782424927, 1.000229001045227, 0.9924768805503845, 0.9850782752037048, 1.0033001899719238, 0.994990885257721, 0.9936748147010803, 0.9883089661598206, 1.0213488340377808, 1.0260107517242432, 1.0010656118392944, 1.0039440393447876, 0.9748120903968811, 1.0021178722381592, 0.9861189723014832, 0.9979387521743774]
[2025-05-14 07:02:55,671]: Mean: 0.99469048
[2025-05-14 07:02:55,671]: Min: 0.97371882
[2025-05-14 07:02:55,672]: Max: 1.03439057
[2025-05-14 07:02:55,672]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 07:02:55,675]: Sample Values (25 elements): [0.03659471496939659, 0.0, 0.03659471496939659, -0.03659471496939659, -0.03659471496939659, 0.0, 0.03659471496939659, 0.03659471496939659, 0.0, 0.0, 0.0, 0.0, 0.03659471496939659, 0.0, 0.0, 0.03659471496939659, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03659471496939659, 0.0, 0.0, -0.03659471496939659]
[2025-05-14 07:02:55,675]: Mean: -0.00012570
[2025-05-14 07:02:55,676]: Min: -0.03659471
[2025-05-14 07:02:55,676]: Max: 0.07318943
[2025-05-14 07:02:55,676]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 07:02:55,676]: Sample Values (25 elements): [0.969566285610199, 0.9741783738136292, 0.9831147789955139, 0.9665650129318237, 0.9604796767234802, 0.97713303565979, 0.9735530614852905, 0.9693547487258911, 0.9678741693496704, 0.9679310917854309, 0.9742125868797302, 0.9728410243988037, 0.9659135341644287, 0.9764006733894348, 0.9718649983406067, 0.9730514883995056, 0.9715604782104492, 0.9749560952186584, 0.9760171175003052, 0.9747108221054077, 0.9675649404525757, 0.9719024300575256, 0.9774468541145325, 0.972186267375946, 0.9683073163032532]
[2025-05-14 07:02:55,676]: Mean: 0.97207725
[2025-05-14 07:02:55,676]: Min: 0.95377898
[2025-05-14 07:02:55,676]: Max: 1.00314331
[2025-05-14 07:02:55,677]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 07:02:55,683]: Sample Values (25 elements): [0.0, -0.02983066253364086, 0.0, 0.02983066253364086, -0.02983066253364086, 0.0, -0.02983066253364086, 0.0, 0.0, -0.02983066253364086, -0.02983066253364086, 0.02983066253364086, -0.02983066253364086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.02983066253364086, 0.0]
[2025-05-14 07:02:55,683]: Mean: -0.00030664
[2025-05-14 07:02:55,683]: Min: -0.02983066
[2025-05-14 07:02:55,683]: Max: 0.05966133
[2025-05-14 07:02:55,684]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 07:02:55,684]: Sample Values (25 elements): [0.9740818738937378, 0.9963685870170593, 0.9687404632568359, 0.9626979231834412, 0.982526957988739, 0.9797569513320923, 0.9707938432693481, 0.9757147431373596, 0.971881091594696, 0.9828067421913147, 0.9741096496582031, 0.9726062417030334, 0.9852334260940552, 0.9652543067932129, 0.9665077328681946, 0.9878353476524353, 0.9681208729743958, 0.9662875533103943, 0.9710711240768433, 0.9891172647476196, 0.9741626977920532, 0.9742756485939026, 0.9705893993377686, 0.9735609292984009, 0.9786432385444641]
[2025-05-14 07:02:55,684]: Mean: 0.97684872
[2025-05-14 07:02:55,684]: Min: 0.96141797
[2025-05-14 07:02:55,684]: Max: 1.00437045
[2025-05-14 07:02:55,685]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 07:02:55,686]: Sample Values (25 elements): [-0.07365968078374863, 0.0, -0.07365968078374863, 0.07365968078374863, -0.07365968078374863, -0.07365968078374863, 0.0, 0.0, 0.07365968078374863, 0.0, -0.07365968078374863, 0.0, 0.0, 0.07365968078374863, 0.07365968078374863, 0.0, 0.07365968078374863, -0.07365968078374863, -0.07365968078374863, -0.07365968078374863, 0.0, 0.07365968078374863, 0.0, 0.07365968078374863, 0.0]
[2025-05-14 07:02:55,686]: Mean: -0.00010565
[2025-05-14 07:02:55,686]: Min: -0.07365968
[2025-05-14 07:02:55,686]: Max: 0.14731936
[2025-05-14 07:02:55,686]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 07:02:55,686]: Sample Values (25 elements): [0.9434372186660767, 0.9335772395133972, 0.9489299058914185, 0.9365311861038208, 0.9406819343566895, 0.9343295693397522, 0.9328175187110901, 0.9397457242012024, 0.930823802947998, 0.954013466835022, 0.9471668601036072, 0.9402230381965637, 0.9341615438461304, 0.9449867606163025, 0.9465042948722839, 0.9330255389213562, 0.9357404708862305, 0.9326729774475098, 0.9482365250587463, 0.9418037533760071, 0.9340164065361023, 0.9404706954956055, 0.9307321906089783, 0.9343233108520508, 0.9420396089553833]
[2025-05-14 07:02:55,687]: Mean: 0.94023818
[2025-05-14 07:02:55,687]: Min: 0.92525494
[2025-05-14 07:02:55,687]: Max: 0.95504749
[2025-05-14 07:02:55,688]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 07:02:55,694]: Sample Values (25 elements): [-0.031418561935424805, 0.0, 0.0, 0.0, 0.0, -0.031418561935424805, 0.0, 0.0, 0.031418561935424805, -0.031418561935424805, 0.0, 0.0, 0.0, 0.0, -0.031418561935424805, -0.031418561935424805, 0.0, 0.0, 0.0, 0.031418561935424805, 0.031418561935424805, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 07:02:55,694]: Mean: -0.00025579
[2025-05-14 07:02:55,694]: Min: -0.03141856
[2025-05-14 07:02:55,694]: Max: 0.06283712
[2025-05-14 07:02:55,695]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 07:02:55,695]: Sample Values (25 elements): [0.9580585360527039, 0.9600865840911865, 0.9584881067276001, 0.9604495763778687, 0.9698612689971924, 0.9652918577194214, 0.9671700596809387, 0.9646294116973877, 0.9664525985717773, 0.975089430809021, 0.9737701416015625, 0.9689121246337891, 0.9598912000656128, 0.9833576679229736, 0.9721748232841492, 0.9883648157119751, 0.9768786430358887, 0.9740972518920898, 0.9638325572013855, 0.9595812559127808, 0.9722900986671448, 0.9746664762496948, 0.9701340198516846, 0.9711157083511353, 0.9620675444602966]
[2025-05-14 07:02:55,695]: Mean: 0.96894121
[2025-05-14 07:02:55,695]: Min: 0.95472646
[2025-05-14 07:02:55,695]: Max: 0.99645942
[2025-05-14 07:02:55,696]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 07:02:55,702]: Sample Values (25 elements): [0.0, 0.0, -0.029439836740493774, -0.029439836740493774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029439836740493774, 0.0, 0.0, 0.0, 0.0, 0.0, -0.029439836740493774, 0.0, 0.0, 0.0, -0.029439836740493774, -0.029439836740493774, 0.0, 0.0]
[2025-05-14 07:02:55,702]: Mean: -0.00009189
[2025-05-14 07:02:55,702]: Min: -0.05887967
[2025-05-14 07:02:55,702]: Max: 0.02943984
[2025-05-14 07:02:55,702]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 07:02:55,703]: Sample Values (25 elements): [0.9779653549194336, 0.9753852486610413, 0.9855086207389832, 0.9816022515296936, 0.9917222857475281, 0.9836830496788025, 0.9728441834449768, 0.990471601486206, 0.9815793633460999, 0.9773166179656982, 0.9728297591209412, 0.9749018549919128, 0.9728841185569763, 0.979896605014801, 0.973154604434967, 0.9764496684074402, 0.9710420370101929, 0.9790003895759583, 0.9712080359458923, 0.9892747402191162, 0.9766912460327148, 0.9780893921852112, 0.9689909219741821, 0.9717300534248352, 0.9908729195594788]
[2025-05-14 07:02:55,703]: Mean: 0.97911251
[2025-05-14 07:02:55,703]: Min: 0.96324193
[2025-05-14 07:02:55,703]: Max: 1.00190580
[2025-05-14 07:02:55,704]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 07:02:55,716]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.026517003774642944, 0.0, -0.026517003774642944, 0.0, 0.0, 0.0, 0.026517003774642944, 0.0, -0.026517003774642944, -0.026517003774642944, -0.026517003774642944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.026517003774642944, 0.026517003774642944, 0.026517003774642944]
[2025-05-14 07:02:55,716]: Mean: 0.00005345
[2025-05-14 07:02:55,716]: Min: -0.02651700
[2025-05-14 07:02:55,717]: Max: 0.05303401
[2025-05-14 07:02:55,717]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 07:02:55,717]: Sample Values (25 elements): [0.9618696570396423, 0.9619300961494446, 0.959828794002533, 0.9668302536010742, 0.9601460099220276, 0.9683682918548584, 0.968869686126709, 0.9571567177772522, 0.9683539867401123, 0.9624624848365784, 0.9604851603507996, 0.9629710912704468, 0.9652276039123535, 0.9694152474403381, 0.9638046026229858, 0.9563015103340149, 0.9604300856590271, 0.9610150456428528, 0.9643152356147766, 0.962760329246521, 0.9632121920585632, 0.9644837379455566, 0.970653235912323, 0.9619078040122986, 0.9604227542877197]
[2025-05-14 07:02:55,717]: Mean: 0.96386105
[2025-05-14 07:02:55,717]: Min: 0.95630151
[2025-05-14 07:02:55,718]: Max: 0.98079795
[2025-05-14 07:02:55,719]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 07:02:55,760]: Sample Values (25 elements): [0.0, 0.01869843155145645, -0.01869843155145645, 0.0, 0.01869843155145645, 0.0, 0.0, 0.0, 0.0, 0.0, -0.01869843155145645, 0.0, -0.01869843155145645, 0.0, 0.0, 0.0, 0.01869843155145645, 0.0, 0.0, 0.0, -0.01869843155145645, 0.0, 0.0, -0.01869843155145645, 0.01869843155145645]
[2025-05-14 07:02:55,761]: Mean: -0.00004800
[2025-05-14 07:02:55,761]: Min: -0.01869843
[2025-05-14 07:02:55,761]: Max: 0.03739686
[2025-05-14 07:02:55,761]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 07:02:55,762]: Sample Values (25 elements): [0.9656282067298889, 0.965797483921051, 0.9659857749938965, 0.9679476618766785, 0.9686789512634277, 0.9618741273880005, 0.9686316251754761, 0.963880181312561, 0.9632254838943481, 0.9685493111610413, 0.9641540050506592, 0.9617820978164673, 0.9644960761070251, 0.9664745330810547, 0.9595034718513489, 0.9780367612838745, 0.9687278866767883, 0.9658183455467224, 0.9668979644775391, 0.9682209491729736, 0.9637393355369568, 0.9628143310546875, 0.9650253057479858, 0.9683336019515991, 0.9643328189849854]
[2025-05-14 07:02:55,763]: Mean: 0.96616745
[2025-05-14 07:02:55,763]: Min: 0.95908159
[2025-05-14 07:02:55,763]: Max: 0.98254615
[2025-05-14 07:02:55,764]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 07:02:55,765]: Sample Values (25 elements): [-0.04894891381263733, 0.0, -0.04894891381263733, 0.04894891381263733, 0.04894891381263733, 0.0, 0.0, 0.0, -0.04894891381263733, 0.04894891381263733, 0.0, 0.04894891381263733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04894891381263733, 0.0, 0.0, 0.0, 0.0, -0.04894891381263733, 0.0, 0.04894891381263733]
[2025-05-14 07:02:55,765]: Mean: 0.00023042
[2025-05-14 07:02:55,766]: Min: -0.04894891
[2025-05-14 07:02:55,766]: Max: 0.09789783
[2025-05-14 07:02:55,766]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 07:02:55,766]: Sample Values (25 elements): [0.9558650255203247, 0.9523723125457764, 0.9552024602890015, 0.9560033679008484, 0.9549039006233215, 0.9542567729949951, 0.956445574760437, 0.9567556977272034, 0.9534099102020264, 0.9571900367736816, 0.9542750716209412, 0.9562368392944336, 0.9582796692848206, 0.9565843939781189, 0.9524096250534058, 0.9552865624427795, 0.9534171223640442, 0.957817792892456, 0.9599630832672119, 0.9575753808021545, 0.9569818377494812, 0.9559460878372192, 0.9546604156494141, 0.9541618227958679, 0.9562039971351624]
[2025-05-14 07:02:55,766]: Mean: 0.95601135
[2025-05-14 07:02:55,766]: Min: 0.94997275
[2025-05-14 07:02:55,766]: Max: 0.96243179
[2025-05-14 07:02:55,767]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 07:02:55,809]: Sample Values (25 elements): [0.0, 0.0, 0.018726179376244545, 0.0, 0.018726179376244545, 0.0, 0.0, 0.018726179376244545, 0.0, 0.018726179376244545, 0.0, 0.0, -0.018726179376244545, 0.0, 0.0, 0.0, 0.0, -0.018726179376244545, 0.0, 0.0, 0.0, -0.018726179376244545, 0.018726179376244545, 0.018726179376244545, -0.018726179376244545]
[2025-05-14 07:02:55,810]: Mean: -0.00011394
[2025-05-14 07:02:55,810]: Min: -0.01872618
[2025-05-14 07:02:55,810]: Max: 0.03745236
[2025-05-14 07:02:55,810]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 07:02:55,810]: Sample Values (25 elements): [0.9608657360076904, 0.9611994028091431, 0.9610627889633179, 0.9576773047447205, 0.9583940505981445, 0.9610590934753418, 0.9639660120010376, 0.9594053626060486, 0.9588847160339355, 0.9603708386421204, 0.9611062407493591, 0.962666392326355, 0.9584593772888184, 0.9595263600349426, 0.9566954374313354, 0.9617999792098999, 0.9591224789619446, 0.9636726379394531, 0.9584292769432068, 0.958850085735321, 0.9581681489944458, 0.957108199596405, 0.9613246917724609, 0.9604289531707764, 0.9590699672698975]
[2025-05-14 07:02:55,810]: Mean: 0.96024781
[2025-05-14 07:02:55,811]: Min: 0.95587605
[2025-05-14 07:02:55,811]: Max: 0.97609764
[2025-05-14 07:02:55,812]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 07:02:55,850]: Sample Values (25 elements): [-0.01410691998898983, -0.01410691998898983, 0.01410691998898983, 0.01410691998898983, -0.01410691998898983, 0.0, 0.0, -0.01410691998898983, 0.0, 0.01410691998898983, 0.0, -0.01410691998898983, 0.0, -0.01410691998898983, 0.0, 0.0, 0.0, 0.0, -0.01410691998898983, -0.01410691998898983, 0.0, 0.01410691998898983, 0.01410691998898983, -0.01410691998898983, 0.0]
[2025-05-14 07:02:55,850]: Mean: 0.00005518
[2025-05-14 07:02:55,851]: Min: -0.01410692
[2025-05-14 07:02:55,851]: Max: 0.02821384
[2025-05-14 07:02:55,851]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 07:02:55,851]: Sample Values (25 elements): [0.9696608781814575, 0.9686232209205627, 0.9687125086784363, 0.9681998491287231, 0.9660271406173706, 0.9651581645011902, 0.9670679569244385, 0.962417483329773, 0.9626815319061279, 0.9672756791114807, 0.9641079306602478, 0.9644437432289124, 0.9665989279747009, 0.9682921171188354, 0.9670736193656921, 0.9682407975196838, 0.9664015173912048, 0.9656143188476562, 0.9640930891036987, 0.9691377878189087, 0.9683346748352051, 0.965830385684967, 0.9675355553627014, 0.9687689542770386, 0.9665369987487793]
[2025-05-14 07:02:55,851]: Mean: 0.96631879
[2025-05-14 07:02:55,852]: Min: 0.95985436
[2025-05-14 07:02:55,852]: Max: 0.97311687
[2025-05-14 07:02:55,852]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 07:02:55,852]: Sample Values (25 elements): [0.05004136636853218, 0.029414130374789238, 0.11328493803739548, -0.05441105365753174, 0.0685800239443779, 0.024200985208153725, -0.04292655736207962, -0.058076344430446625, -0.08273664861917496, 0.041380833834409714, -0.018210943788290024, -0.07845813781023026, 0.052244625985622406, 0.09215766936540604, 0.08510376513004303, 0.026105176657438278, -0.019763225689530373, -0.07011318951845169, -0.06147390976548195, 0.05496906116604805, -0.06970010697841644, 0.09937085211277008, -0.09753581136465073, -0.06396634131669998, -0.04110818728804588]
[2025-05-14 07:02:55,852]: Mean: -0.00025991
[2025-05-14 07:02:55,852]: Min: -0.11431593
[2025-05-14 07:02:55,853]: Max: 0.14152671
