[2025-05-12 08:36:47,232]: 
Training ResNet20 with parametrized_relu
[2025-05-12 08:38:46,432]: [ResNet20_parametrized_relu] Epoch: 001 Train Loss: 1.8210 Train Acc: 0.3172 Eval Loss: 1.5664 Eval Acc: 0.4109 (LR: 0.001000)
[2025-05-12 08:40:26,482]: [ResNet20_parametrized_relu] Epoch: 002 Train Loss: 1.5066 Train Acc: 0.4367 Eval Loss: 1.3733 Eval Acc: 0.4909 (LR: 0.001000)
[2025-05-12 08:42:09,953]: [ResNet20_parametrized_relu] Epoch: 003 Train Loss: 1.3455 Train Acc: 0.5075 Eval Loss: 1.2477 Eval Acc: 0.5500 (LR: 0.001000)
[2025-05-12 08:44:08,535]: [ResNet20_parametrized_relu] Epoch: 004 Train Loss: 1.2358 Train Acc: 0.5513 Eval Loss: 1.1984 Eval Acc: 0.5661 (LR: 0.001000)
[2025-05-12 08:45:58,542]: [ResNet20_parametrized_relu] Epoch: 005 Train Loss: 1.1359 Train Acc: 0.5909 Eval Loss: 1.1210 Eval Acc: 0.5974 (LR: 0.001000)
[2025-05-12 08:47:42,723]: [ResNet20_parametrized_relu] Epoch: 006 Train Loss: 1.0684 Train Acc: 0.6173 Eval Loss: 1.0758 Eval Acc: 0.6126 (LR: 0.001000)
[2025-05-12 08:49:04,978]: [ResNet20_parametrized_relu] Epoch: 007 Train Loss: 1.0182 Train Acc: 0.6368 Eval Loss: 0.9762 Eval Acc: 0.6592 (LR: 0.001000)
[2025-05-12 08:50:25,582]: [ResNet20_parametrized_relu] Epoch: 008 Train Loss: 0.9697 Train Acc: 0.6515 Eval Loss: 0.9779 Eval Acc: 0.6543 (LR: 0.001000)
[2025-05-12 08:51:50,570]: [ResNet20_parametrized_relu] Epoch: 009 Train Loss: 0.9287 Train Acc: 0.6708 Eval Loss: 0.8750 Eval Acc: 0.6901 (LR: 0.001000)
[2025-05-12 08:53:22,579]: [ResNet20_parametrized_relu] Epoch: 010 Train Loss: 0.8967 Train Acc: 0.6824 Eval Loss: 0.9352 Eval Acc: 0.6779 (LR: 0.001000)
[2025-05-12 08:54:53,604]: [ResNet20_parametrized_relu] Epoch: 011 Train Loss: 0.8641 Train Acc: 0.6933 Eval Loss: 0.9057 Eval Acc: 0.6905 (LR: 0.001000)
[2025-05-12 08:56:24,048]: [ResNet20_parametrized_relu] Epoch: 012 Train Loss: 0.8399 Train Acc: 0.7043 Eval Loss: 0.8156 Eval Acc: 0.7158 (LR: 0.001000)
[2025-05-12 08:57:30,686]: [ResNet20_parametrized_relu] Epoch: 013 Train Loss: 0.8061 Train Acc: 0.7137 Eval Loss: 0.8225 Eval Acc: 0.7091 (LR: 0.001000)
[2025-05-12 09:00:00,394]: [ResNet20_parametrized_relu] Epoch: 014 Train Loss: 0.7843 Train Acc: 0.7244 Eval Loss: 0.7757 Eval Acc: 0.7249 (LR: 0.001000)
[2025-05-12 09:02:23,217]: [ResNet20_parametrized_relu] Epoch: 015 Train Loss: 0.7613 Train Acc: 0.7325 Eval Loss: 0.8030 Eval Acc: 0.7177 (LR: 0.001000)
[2025-05-12 09:04:33,280]: [ResNet20_parametrized_relu] Epoch: 016 Train Loss: 0.7329 Train Acc: 0.7440 Eval Loss: 0.6995 Eval Acc: 0.7557 (LR: 0.001000)
[2025-05-12 09:06:52,085]: [ResNet20_parametrized_relu] Epoch: 017 Train Loss: 0.7192 Train Acc: 0.7492 Eval Loss: 0.7371 Eval Acc: 0.7465 (LR: 0.001000)
[2025-05-12 09:08:59,467]: [ResNet20_parametrized_relu] Epoch: 018 Train Loss: 0.6929 Train Acc: 0.7578 Eval Loss: 0.7463 Eval Acc: 0.7455 (LR: 0.001000)
[2025-05-12 09:11:38,337]: [ResNet20_parametrized_relu] Epoch: 019 Train Loss: 0.6759 Train Acc: 0.7641 Eval Loss: 0.7345 Eval Acc: 0.7547 (LR: 0.001000)
[2025-05-12 09:14:08,723]: [ResNet20_parametrized_relu] Epoch: 020 Train Loss: 0.6594 Train Acc: 0.7682 Eval Loss: 0.6861 Eval Acc: 0.7675 (LR: 0.001000)
[2025-05-12 09:16:04,657]: [ResNet20_parametrized_relu] Epoch: 021 Train Loss: 0.6450 Train Acc: 0.7714 Eval Loss: 0.6736 Eval Acc: 0.7718 (LR: 0.001000)
[2025-05-12 09:18:12,347]: [ResNet20_parametrized_relu] Epoch: 022 Train Loss: 0.6296 Train Acc: 0.7812 Eval Loss: 0.6947 Eval Acc: 0.7637 (LR: 0.001000)
[2025-05-12 09:20:40,793]: [ResNet20_parametrized_relu] Epoch: 023 Train Loss: 0.6198 Train Acc: 0.7837 Eval Loss: 0.6295 Eval Acc: 0.7857 (LR: 0.001000)
[2025-05-12 09:23:09,677]: [ResNet20_parametrized_relu] Epoch: 024 Train Loss: 0.6018 Train Acc: 0.7893 Eval Loss: 0.6455 Eval Acc: 0.7735 (LR: 0.001000)
[2025-05-12 09:25:42,911]: [ResNet20_parametrized_relu] Epoch: 025 Train Loss: 0.5926 Train Acc: 0.7928 Eval Loss: 0.6290 Eval Acc: 0.7864 (LR: 0.001000)
[2025-05-12 09:27:55,025]: [ResNet20_parametrized_relu] Epoch: 026 Train Loss: 0.5777 Train Acc: 0.7982 Eval Loss: 0.5892 Eval Acc: 0.8029 (LR: 0.001000)
[2025-05-12 09:30:08,431]: [ResNet20_parametrized_relu] Epoch: 027 Train Loss: 0.5690 Train Acc: 0.8024 Eval Loss: 0.5971 Eval Acc: 0.7958 (LR: 0.001000)
[2025-05-12 09:32:33,035]: [ResNet20_parametrized_relu] Epoch: 028 Train Loss: 0.5652 Train Acc: 0.8021 Eval Loss: 0.6344 Eval Acc: 0.7869 (LR: 0.001000)
[2025-05-12 09:35:08,115]: [ResNet20_parametrized_relu] Epoch: 029 Train Loss: 0.5462 Train Acc: 0.8117 Eval Loss: 0.6002 Eval Acc: 0.7939 (LR: 0.001000)
[2025-05-12 09:37:24,582]: [ResNet20_parametrized_relu] Epoch: 030 Train Loss: 0.5425 Train Acc: 0.8117 Eval Loss: 0.5734 Eval Acc: 0.8053 (LR: 0.001000)
[2025-05-12 09:39:32,987]: [ResNet20_parametrized_relu] Epoch: 031 Train Loss: 0.5317 Train Acc: 0.8155 Eval Loss: 0.6055 Eval Acc: 0.7951 (LR: 0.001000)
[2025-05-12 09:41:28,810]: [ResNet20_parametrized_relu] Epoch: 032 Train Loss: 0.5250 Train Acc: 0.8173 Eval Loss: 0.5653 Eval Acc: 0.8050 (LR: 0.001000)
[2025-05-12 09:43:35,551]: [ResNet20_parametrized_relu] Epoch: 033 Train Loss: 0.5144 Train Acc: 0.8206 Eval Loss: 0.5840 Eval Acc: 0.8043 (LR: 0.001000)
[2025-05-12 09:45:02,894]: [ResNet20_parametrized_relu] Epoch: 034 Train Loss: 0.5092 Train Acc: 0.8219 Eval Loss: 0.5668 Eval Acc: 0.8097 (LR: 0.001000)
[2025-05-12 09:46:44,809]: [ResNet20_parametrized_relu] Epoch: 035 Train Loss: 0.5061 Train Acc: 0.8243 Eval Loss: 0.5233 Eval Acc: 0.8214 (LR: 0.001000)
[2025-05-12 09:48:27,711]: [ResNet20_parametrized_relu] Epoch: 036 Train Loss: 0.4938 Train Acc: 0.8286 Eval Loss: 0.5540 Eval Acc: 0.8151 (LR: 0.001000)
[2025-05-12 09:50:14,762]: [ResNet20_parametrized_relu] Epoch: 037 Train Loss: 0.4869 Train Acc: 0.8298 Eval Loss: 0.5720 Eval Acc: 0.8078 (LR: 0.001000)
[2025-05-12 09:51:54,913]: [ResNet20_parametrized_relu] Epoch: 038 Train Loss: 0.4788 Train Acc: 0.8344 Eval Loss: 0.5566 Eval Acc: 0.8179 (LR: 0.001000)
[2025-05-12 09:53:27,437]: [ResNet20_parametrized_relu] Epoch: 039 Train Loss: 0.4720 Train Acc: 0.8357 Eval Loss: 0.5466 Eval Acc: 0.8151 (LR: 0.001000)
[2025-05-12 09:55:12,088]: [ResNet20_parametrized_relu] Epoch: 040 Train Loss: 0.4680 Train Acc: 0.8379 Eval Loss: 0.5664 Eval Acc: 0.8075 (LR: 0.001000)
[2025-05-12 09:57:02,715]: [ResNet20_parametrized_relu] Epoch: 041 Train Loss: 0.4642 Train Acc: 0.8388 Eval Loss: 0.5314 Eval Acc: 0.8255 (LR: 0.001000)
[2025-05-12 09:58:49,146]: [ResNet20_parametrized_relu] Epoch: 042 Train Loss: 0.4567 Train Acc: 0.8423 Eval Loss: 0.5186 Eval Acc: 0.8237 (LR: 0.001000)
[2025-05-12 10:00:56,261]: [ResNet20_parametrized_relu] Epoch: 043 Train Loss: 0.4531 Train Acc: 0.8421 Eval Loss: 0.5026 Eval Acc: 0.8280 (LR: 0.001000)
[2025-05-12 10:02:47,326]: [ResNet20_parametrized_relu] Epoch: 044 Train Loss: 0.4455 Train Acc: 0.8453 Eval Loss: 0.5331 Eval Acc: 0.8227 (LR: 0.001000)
[2025-05-12 10:04:47,688]: [ResNet20_parametrized_relu] Epoch: 045 Train Loss: 0.4399 Train Acc: 0.8456 Eval Loss: 0.5471 Eval Acc: 0.8199 (LR: 0.001000)
[2025-05-12 10:06:53,036]: [ResNet20_parametrized_relu] Epoch: 046 Train Loss: 0.4354 Train Acc: 0.8470 Eval Loss: 0.5223 Eval Acc: 0.8297 (LR: 0.001000)
[2025-05-12 10:08:45,141]: [ResNet20_parametrized_relu] Epoch: 047 Train Loss: 0.4334 Train Acc: 0.8488 Eval Loss: 0.5115 Eval Acc: 0.8296 (LR: 0.001000)
[2025-05-12 10:10:40,624]: [ResNet20_parametrized_relu] Epoch: 048 Train Loss: 0.4281 Train Acc: 0.8506 Eval Loss: 0.5480 Eval Acc: 0.8221 (LR: 0.001000)
[2025-05-12 10:12:44,245]: [ResNet20_parametrized_relu] Epoch: 049 Train Loss: 0.4285 Train Acc: 0.8513 Eval Loss: 0.4994 Eval Acc: 0.8365 (LR: 0.001000)
[2025-05-12 10:14:46,699]: [ResNet20_parametrized_relu] Epoch: 050 Train Loss: 0.4157 Train Acc: 0.8552 Eval Loss: 0.5043 Eval Acc: 0.8319 (LR: 0.001000)
[2025-05-12 10:16:44,372]: [ResNet20_parametrized_relu] Epoch: 051 Train Loss: 0.4081 Train Acc: 0.8585 Eval Loss: 0.4792 Eval Acc: 0.8414 (LR: 0.001000)
[2025-05-12 10:18:45,809]: [ResNet20_parametrized_relu] Epoch: 052 Train Loss: 0.4050 Train Acc: 0.8600 Eval Loss: 0.5290 Eval Acc: 0.8283 (LR: 0.001000)
[2025-05-12 10:20:49,686]: [ResNet20_parametrized_relu] Epoch: 053 Train Loss: 0.4046 Train Acc: 0.8590 Eval Loss: 0.5171 Eval Acc: 0.8280 (LR: 0.001000)
[2025-05-12 10:22:32,055]: [ResNet20_parametrized_relu] Epoch: 054 Train Loss: 0.3981 Train Acc: 0.8606 Eval Loss: 0.4885 Eval Acc: 0.8334 (LR: 0.001000)
[2025-05-12 10:24:13,952]: [ResNet20_parametrized_relu] Epoch: 055 Train Loss: 0.3991 Train Acc: 0.8601 Eval Loss: 0.5872 Eval Acc: 0.8139 (LR: 0.001000)
[2025-05-12 10:25:40,666]: [ResNet20_parametrized_relu] Epoch: 056 Train Loss: 0.3878 Train Acc: 0.8647 Eval Loss: 0.4788 Eval Acc: 0.8431 (LR: 0.001000)
[2025-05-12 10:27:10,808]: [ResNet20_parametrized_relu] Epoch: 057 Train Loss: 0.3895 Train Acc: 0.8644 Eval Loss: 0.4971 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-12 10:28:49,975]: [ResNet20_parametrized_relu] Epoch: 058 Train Loss: 0.3801 Train Acc: 0.8676 Eval Loss: 0.5004 Eval Acc: 0.8351 (LR: 0.001000)
[2025-05-12 10:30:33,305]: [ResNet20_parametrized_relu] Epoch: 059 Train Loss: 0.3821 Train Acc: 0.8669 Eval Loss: 0.4934 Eval Acc: 0.8408 (LR: 0.001000)
[2025-05-12 10:32:19,660]: [ResNet20_parametrized_relu] Epoch: 060 Train Loss: 0.3770 Train Acc: 0.8695 Eval Loss: 0.4778 Eval Acc: 0.8429 (LR: 0.001000)
[2025-05-12 10:33:57,205]: [ResNet20_parametrized_relu] Epoch: 061 Train Loss: 0.3769 Train Acc: 0.8675 Eval Loss: 0.5149 Eval Acc: 0.8294 (LR: 0.001000)
[2025-05-12 10:35:24,173]: [ResNet20_parametrized_relu] Epoch: 062 Train Loss: 0.3735 Train Acc: 0.8678 Eval Loss: 0.4760 Eval Acc: 0.8447 (LR: 0.001000)
[2025-05-12 10:37:03,214]: [ResNet20_parametrized_relu] Epoch: 063 Train Loss: 0.3660 Train Acc: 0.8729 Eval Loss: 0.4687 Eval Acc: 0.8453 (LR: 0.001000)
[2025-05-12 10:38:40,590]: [ResNet20_parametrized_relu] Epoch: 064 Train Loss: 0.3637 Train Acc: 0.8737 Eval Loss: 0.4880 Eval Acc: 0.8374 (LR: 0.001000)
[2025-05-12 10:40:13,398]: [ResNet20_parametrized_relu] Epoch: 065 Train Loss: 0.3602 Train Acc: 0.8744 Eval Loss: 0.4793 Eval Acc: 0.8461 (LR: 0.001000)
[2025-05-12 10:41:57,125]: [ResNet20_parametrized_relu] Epoch: 066 Train Loss: 0.3536 Train Acc: 0.8756 Eval Loss: 0.5200 Eval Acc: 0.8300 (LR: 0.001000)
[2025-05-12 10:43:48,403]: [ResNet20_parametrized_relu] Epoch: 067 Train Loss: 0.3519 Train Acc: 0.8783 Eval Loss: 0.4969 Eval Acc: 0.8402 (LR: 0.001000)
[2025-05-12 10:45:51,912]: [ResNet20_parametrized_relu] Epoch: 068 Train Loss: 0.3540 Train Acc: 0.8766 Eval Loss: 0.4945 Eval Acc: 0.8408 (LR: 0.001000)
[2025-05-12 10:47:39,582]: [ResNet20_parametrized_relu] Epoch: 069 Train Loss: 0.3504 Train Acc: 0.8776 Eval Loss: 0.4613 Eval Acc: 0.8484 (LR: 0.001000)
[2025-05-12 10:49:40,990]: [ResNet20_parametrized_relu] Epoch: 070 Train Loss: 0.3412 Train Acc: 0.8808 Eval Loss: 0.4807 Eval Acc: 0.8436 (LR: 0.000100)
[2025-05-12 10:51:23,697]: [ResNet20_parametrized_relu] Epoch: 071 Train Loss: 0.3059 Train Acc: 0.8944 Eval Loss: 0.4033 Eval Acc: 0.8658 (LR: 0.000100)
[2025-05-12 10:53:08,466]: [ResNet20_parametrized_relu] Epoch: 072 Train Loss: 0.2954 Train Acc: 0.8971 Eval Loss: 0.4034 Eval Acc: 0.8650 (LR: 0.000100)
[2025-05-12 10:55:05,375]: [ResNet20_parametrized_relu] Epoch: 073 Train Loss: 0.2920 Train Acc: 0.8988 Eval Loss: 0.3968 Eval Acc: 0.8677 (LR: 0.000100)
[2025-05-12 10:57:04,051]: [ResNet20_parametrized_relu] Epoch: 074 Train Loss: 0.2929 Train Acc: 0.8986 Eval Loss: 0.4005 Eval Acc: 0.8653 (LR: 0.000100)
[2025-05-12 10:59:06,686]: [ResNet20_parametrized_relu] Epoch: 075 Train Loss: 0.2928 Train Acc: 0.8972 Eval Loss: 0.4011 Eval Acc: 0.8637 (LR: 0.000100)
[2025-05-12 11:01:05,904]: [ResNet20_parametrized_relu] Epoch: 076 Train Loss: 0.2885 Train Acc: 0.9017 Eval Loss: 0.3969 Eval Acc: 0.8675 (LR: 0.000100)
[2025-05-12 11:03:04,090]: [ResNet20_parametrized_relu] Epoch: 077 Train Loss: 0.2887 Train Acc: 0.9007 Eval Loss: 0.3975 Eval Acc: 0.8673 (LR: 0.000100)
[2025-05-12 11:04:45,648]: [ResNet20_parametrized_relu] Epoch: 078 Train Loss: 0.2836 Train Acc: 0.9019 Eval Loss: 0.3964 Eval Acc: 0.8667 (LR: 0.000100)
[2025-05-12 11:06:04,766]: [ResNet20_parametrized_relu] Epoch: 079 Train Loss: 0.2821 Train Acc: 0.9027 Eval Loss: 0.4006 Eval Acc: 0.8669 (LR: 0.000100)
[2025-05-12 11:07:10,469]: [ResNet20_parametrized_relu] Epoch: 080 Train Loss: 0.2835 Train Acc: 0.9021 Eval Loss: 0.3945 Eval Acc: 0.8685 (LR: 0.000100)
[2025-05-12 11:08:00,462]: [ResNet20_parametrized_relu] Epoch: 081 Train Loss: 0.2848 Train Acc: 0.9011 Eval Loss: 0.3983 Eval Acc: 0.8685 (LR: 0.000100)
[2025-05-12 11:08:53,545]: [ResNet20_parametrized_relu] Epoch: 082 Train Loss: 0.2843 Train Acc: 0.9008 Eval Loss: 0.3980 Eval Acc: 0.8671 (LR: 0.000100)
[2025-05-12 11:09:45,702]: [ResNet20_parametrized_relu] Epoch: 083 Train Loss: 0.2850 Train Acc: 0.9013 Eval Loss: 0.3970 Eval Acc: 0.8688 (LR: 0.000100)
[2025-05-12 11:10:35,728]: [ResNet20_parametrized_relu] Epoch: 084 Train Loss: 0.2851 Train Acc: 0.9022 Eval Loss: 0.4001 Eval Acc: 0.8669 (LR: 0.000100)
[2025-05-12 11:11:30,545]: [ResNet20_parametrized_relu] Epoch: 085 Train Loss: 0.2784 Train Acc: 0.9039 Eval Loss: 0.3963 Eval Acc: 0.8682 (LR: 0.000100)
[2025-05-12 11:12:19,704]: [ResNet20_parametrized_relu] Epoch: 086 Train Loss: 0.2821 Train Acc: 0.9026 Eval Loss: 0.3982 Eval Acc: 0.8685 (LR: 0.000100)
[2025-05-12 11:13:12,834]: [ResNet20_parametrized_relu] Epoch: 087 Train Loss: 0.2808 Train Acc: 0.9022 Eval Loss: 0.4008 Eval Acc: 0.8677 (LR: 0.000100)
[2025-05-12 11:14:05,165]: [ResNet20_parametrized_relu] Epoch: 088 Train Loss: 0.2803 Train Acc: 0.9029 Eval Loss: 0.3994 Eval Acc: 0.8676 (LR: 0.000100)
[2025-05-12 11:14:54,756]: [ResNet20_parametrized_relu] Epoch: 089 Train Loss: 0.2813 Train Acc: 0.9018 Eval Loss: 0.3988 Eval Acc: 0.8694 (LR: 0.000100)
[2025-05-12 11:15:49,017]: [ResNet20_parametrized_relu] Epoch: 090 Train Loss: 0.2789 Train Acc: 0.9039 Eval Loss: 0.3975 Eval Acc: 0.8697 (LR: 0.000100)
[2025-05-12 11:16:40,149]: [ResNet20_parametrized_relu] Epoch: 091 Train Loss: 0.2738 Train Acc: 0.9069 Eval Loss: 0.4012 Eval Acc: 0.8662 (LR: 0.000100)
[2025-05-12 11:17:31,679]: [ResNet20_parametrized_relu] Epoch: 092 Train Loss: 0.2747 Train Acc: 0.9043 Eval Loss: 0.3951 Eval Acc: 0.8707 (LR: 0.000100)
[2025-05-12 11:18:24,270]: [ResNet20_parametrized_relu] Epoch: 093 Train Loss: 0.2793 Train Acc: 0.9027 Eval Loss: 0.4004 Eval Acc: 0.8685 (LR: 0.000100)
[2025-05-12 11:19:16,209]: [ResNet20_parametrized_relu] Epoch: 094 Train Loss: 0.2764 Train Acc: 0.9049 Eval Loss: 0.3979 Eval Acc: 0.8695 (LR: 0.000100)
[2025-05-12 11:20:10,281]: [ResNet20_parametrized_relu] Epoch: 095 Train Loss: 0.2771 Train Acc: 0.9030 Eval Loss: 0.3992 Eval Acc: 0.8702 (LR: 0.000100)
[2025-05-12 11:21:02,889]: [ResNet20_parametrized_relu] Epoch: 096 Train Loss: 0.2775 Train Acc: 0.9041 Eval Loss: 0.3995 Eval Acc: 0.8688 (LR: 0.000100)
[2025-05-12 11:21:51,758]: [ResNet20_parametrized_relu] Epoch: 097 Train Loss: 0.2732 Train Acc: 0.9057 Eval Loss: 0.3979 Eval Acc: 0.8692 (LR: 0.000100)
[2025-05-12 11:22:43,876]: [ResNet20_parametrized_relu] Epoch: 098 Train Loss: 0.2723 Train Acc: 0.9064 Eval Loss: 0.4038 Eval Acc: 0.8665 (LR: 0.000100)
[2025-05-12 11:23:36,131]: [ResNet20_parametrized_relu] Epoch: 099 Train Loss: 0.2704 Train Acc: 0.9060 Eval Loss: 0.4024 Eval Acc: 0.8672 (LR: 0.000100)
[2025-05-12 11:24:29,383]: [ResNet20_parametrized_relu] Epoch: 100 Train Loss: 0.2723 Train Acc: 0.9057 Eval Loss: 0.3994 Eval Acc: 0.8685 (LR: 0.000010)
[2025-05-12 11:25:21,880]: [ResNet20_parametrized_relu] Epoch: 101 Train Loss: 0.2671 Train Acc: 0.9071 Eval Loss: 0.3963 Eval Acc: 0.8704 (LR: 0.000010)
[2025-05-12 11:26:10,609]: [ResNet20_parametrized_relu] Epoch: 102 Train Loss: 0.2688 Train Acc: 0.9076 Eval Loss: 0.3972 Eval Acc: 0.8708 (LR: 0.000010)
[2025-05-12 11:27:03,720]: [ResNet20_parametrized_relu] Epoch: 103 Train Loss: 0.2656 Train Acc: 0.9076 Eval Loss: 0.3960 Eval Acc: 0.8706 (LR: 0.000010)
[2025-05-12 11:27:57,828]: [ResNet20_parametrized_relu] Epoch: 104 Train Loss: 0.2663 Train Acc: 0.9080 Eval Loss: 0.3936 Eval Acc: 0.8713 (LR: 0.000010)
[2025-05-12 11:28:50,240]: [ResNet20_parametrized_relu] Epoch: 105 Train Loss: 0.2678 Train Acc: 0.9058 Eval Loss: 0.3962 Eval Acc: 0.8701 (LR: 0.000010)
[2025-05-12 11:29:43,569]: [ResNet20_parametrized_relu] Epoch: 106 Train Loss: 0.2663 Train Acc: 0.9077 Eval Loss: 0.3939 Eval Acc: 0.8720 (LR: 0.000010)
[2025-05-12 11:30:34,284]: [ResNet20_parametrized_relu] Epoch: 107 Train Loss: 0.2645 Train Acc: 0.9077 Eval Loss: 0.3946 Eval Acc: 0.8713 (LR: 0.000010)
[2025-05-12 11:31:26,755]: [ResNet20_parametrized_relu] Epoch: 108 Train Loss: 0.2666 Train Acc: 0.9072 Eval Loss: 0.3957 Eval Acc: 0.8702 (LR: 0.000010)
[2025-05-12 11:32:20,704]: [ResNet20_parametrized_relu] Epoch: 109 Train Loss: 0.2659 Train Acc: 0.9090 Eval Loss: 0.3965 Eval Acc: 0.8701 (LR: 0.000010)
[2025-05-12 11:33:10,274]: [ResNet20_parametrized_relu] Epoch: 110 Train Loss: 0.2636 Train Acc: 0.9088 Eval Loss: 0.3957 Eval Acc: 0.8707 (LR: 0.000010)
[2025-05-12 11:34:01,732]: [ResNet20_parametrized_relu] Epoch: 111 Train Loss: 0.2665 Train Acc: 0.9073 Eval Loss: 0.3954 Eval Acc: 0.8702 (LR: 0.000010)
[2025-05-12 11:34:53,764]: [ResNet20_parametrized_relu] Epoch: 112 Train Loss: 0.2647 Train Acc: 0.9081 Eval Loss: 0.3923 Eval Acc: 0.8711 (LR: 0.000010)
[2025-05-12 11:35:44,915]: [ResNet20_parametrized_relu] Epoch: 113 Train Loss: 0.2644 Train Acc: 0.9074 Eval Loss: 0.3961 Eval Acc: 0.8706 (LR: 0.000010)
[2025-05-12 11:36:39,480]: [ResNet20_parametrized_relu] Epoch: 114 Train Loss: 0.2647 Train Acc: 0.9079 Eval Loss: 0.3963 Eval Acc: 0.8706 (LR: 0.000010)
[2025-05-12 11:37:28,465]: [ResNet20_parametrized_relu] Epoch: 115 Train Loss: 0.2667 Train Acc: 0.9069 Eval Loss: 0.3975 Eval Acc: 0.8698 (LR: 0.000010)
[2025-05-12 11:38:19,939]: [ResNet20_parametrized_relu] Epoch: 116 Train Loss: 0.2644 Train Acc: 0.9095 Eval Loss: 0.3933 Eval Acc: 0.8710 (LR: 0.000010)
[2025-05-12 11:39:14,476]: [ResNet20_parametrized_relu] Epoch: 117 Train Loss: 0.2661 Train Acc: 0.9076 Eval Loss: 0.3945 Eval Acc: 0.8715 (LR: 0.000010)
[2025-05-12 11:40:03,985]: [ResNet20_parametrized_relu] Epoch: 118 Train Loss: 0.2669 Train Acc: 0.9066 Eval Loss: 0.3963 Eval Acc: 0.8714 (LR: 0.000010)
[2025-05-12 11:40:58,176]: [ResNet20_parametrized_relu] Epoch: 119 Train Loss: 0.2639 Train Acc: 0.9080 Eval Loss: 0.3967 Eval Acc: 0.8705 (LR: 0.000010)
[2025-05-12 11:41:50,615]: [ResNet20_parametrized_relu] Epoch: 120 Train Loss: 0.2664 Train Acc: 0.9074 Eval Loss: 0.3962 Eval Acc: 0.8708 (LR: 0.000010)
[2025-05-12 11:41:50,616]: [ResNet20_parametrized_relu] Best Eval Accuracy: 0.8720
[2025-05-12 11:41:50,673]: 
Training of full-precision model finished!
[2025-05-12 11:41:50,673]: Model Architecture:
[2025-05-12 11:41:50,675]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:41:50,675]: 
Model Weights:
[2025-05-12 11:41:50,675]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 11:41:50,687]: Sample Values (25 elements): [-0.06436207890510559, -0.09629852324724197, -0.032617583870887756, 0.020976927131414413, 0.3280933201313019, 0.1714528203010559, 0.07287982106208801, 0.024452006444334984, 0.428836464881897, -0.1132296621799469, -0.4781910479068756, 0.22886405885219574, 0.21487775444984436, -0.04867495223879814, -0.0007488218834623694, -0.3098532557487488, 0.002852143719792366, 0.07070043683052063, 0.22392761707305908, -0.0007478127954527736, -0.1036151796579361, -0.011242722161114216, 0.20919206738471985, -0.14141865074634552, 0.06447741389274597]
[2025-05-12 11:41:50,692]: Mean: -0.00075731
[2025-05-12 11:41:50,700]: Min: -0.47819105
[2025-05-12 11:41:50,700]: Max: 0.50618821
[2025-05-12 11:41:50,700]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,701]: Sample Values (16 elements): [1.2430561780929565, 0.9598503708839417, 0.9687211513519287, 0.9572049379348755, 0.947361409664154, 0.8501778244972229, 0.8284057974815369, 0.8424370884895325, 0.8938419222831726, 0.9566875100135803, 1.0001932382583618, 0.9725598692893982, 1.0653867721557617, 0.8976213335990906, 0.8087921142578125, 1.0505260229110718]
[2025-05-12 11:41:50,701]: Mean: 0.95267648
[2025-05-12 11:41:50,702]: Min: 0.80879211
[2025-05-12 11:41:50,702]: Max: 1.24305618
[2025-05-12 11:41:50,702]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:41:50,703]: Sample Values (25 elements): [-0.013262518681585789, 0.021771671250462532, 0.029567724093794823, 0.04201963543891907, 0.030476072803139687, -0.02571512944996357, 0.0014958917163312435, -0.04232805222272873, -0.14742815494537354, -0.0646614208817482, -0.009222373366355896, 0.027753500267863274, -0.028004484251141548, -0.013653875328600407, 0.03782003000378609, -0.08906421065330505, -0.03014451079070568, -0.001783436513505876, 0.04511271417140961, -0.1104457899928093, 0.1202624961733818, -0.17600616812705994, 0.059174444526433945, 0.049736592918634415, -0.016882240772247314]
[2025-05-12 11:41:50,704]: Mean: -0.00322819
[2025-05-12 11:41:50,704]: Min: -0.27393064
[2025-05-12 11:41:50,704]: Max: 0.31497335
[2025-05-12 11:41:50,704]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,706]: Sample Values (16 elements): [1.0002115964889526, 0.8464566469192505, 1.032573938369751, 1.0923678874969482, 0.9560545682907104, 0.9156171083450317, 0.8908082246780396, 1.0732142925262451, 0.9107470512390137, 0.8723635077476501, 1.0387275218963623, 0.9086358547210693, 1.0630589723587036, 0.9586597681045532, 0.9594577550888062, 1.0177515745162964]
[2025-05-12 11:41:50,706]: Mean: 0.97104418
[2025-05-12 11:41:50,706]: Min: 0.84645665
[2025-05-12 11:41:50,707]: Max: 1.09236789
[2025-05-12 11:41:50,707]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:41:50,708]: Sample Values (25 elements): [-0.10758021473884583, 0.03014773689210415, -0.04763195663690567, -0.056219637393951416, 0.0909159779548645, 0.021693293005228043, 0.06632042676210403, -0.04353286698460579, -0.010696989484131336, -0.07105635851621628, 0.03445878624916077, -0.039546649903059006, 0.008973564021289349, -0.07896391302347183, -0.001872561639174819, 0.0695285052061081, 0.020010292530059814, -0.007638920098543167, -0.005375900771468878, 0.00465270783752203, 0.026953278109431267, 0.05453875660896301, -0.07216209918260574, 0.0505865141749382, 0.0864633321762085]
[2025-05-12 11:41:50,708]: Mean: -0.00403508
[2025-05-12 11:41:50,709]: Min: -0.18747710
[2025-05-12 11:41:50,709]: Max: 0.30448192
[2025-05-12 11:41:50,709]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,710]: Sample Values (16 elements): [1.0023123025894165, 1.004181146621704, 0.9773551225662231, 0.9276778697967529, 0.8958728909492493, 0.8512574434280396, 1.0115892887115479, 1.081568956375122, 1.0190342664718628, 1.0082066059112549, 1.0063047409057617, 0.9717221856117249, 0.9399155378341675, 1.008081078529358, 1.067319631576538, 0.9894755482673645]
[2025-05-12 11:41:50,711]: Mean: 0.98511714
[2025-05-12 11:41:50,711]: Min: 0.85125744
[2025-05-12 11:41:50,711]: Max: 1.08156896
[2025-05-12 11:41:50,711]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:41:50,712]: Sample Values (25 elements): [-0.02485046535730362, -0.03363744914531708, 0.010622386820614338, -0.07828948646783829, 0.07276041060686111, 0.04024779424071312, 0.013650411739945412, -0.03472098708152771, -0.024147382006049156, -0.06236989423632622, -0.012710072100162506, -0.06835015118122101, 0.0036138202995061874, 0.010044500231742859, -0.05629744753241539, -0.04354675114154816, 0.003112780163064599, -0.0012823310680687428, -0.08510543406009674, -0.03681003302335739, -0.09767603129148483, -0.09821469336748123, -0.026188790798187256, 0.0017566626193001866, 0.06960669159889221]
[2025-05-12 11:41:50,713]: Mean: -0.00191560
[2025-05-12 11:41:50,713]: Min: -0.21741386
[2025-05-12 11:41:50,713]: Max: 0.21090879
[2025-05-12 11:41:50,713]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,714]: Sample Values (16 elements): [0.9309837818145752, 1.0226064920425415, 0.9567648768424988, 0.9857028126716614, 1.088183045387268, 0.8882611989974976, 0.9676828384399414, 1.003599762916565, 0.9183611273765564, 0.9534460306167603, 0.9681246876716614, 0.9200684428215027, 0.944804847240448, 0.9754365086555481, 1.0517466068267822, 0.9371623396873474]
[2025-05-12 11:41:50,715]: Mean: 0.96955848
[2025-05-12 11:41:50,715]: Min: 0.88826120
[2025-05-12 11:41:50,715]: Max: 1.08818305
[2025-05-12 11:41:50,715]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:41:50,716]: Sample Values (25 elements): [0.055808521807193756, 0.0681384727358818, -0.027652600780129433, -0.04677744209766388, 0.04386720433831215, 0.027375543490052223, 0.11235298216342926, 0.0368933267891407, 0.01919572800397873, -0.05891422554850578, 0.051389582455158234, 0.07044106721878052, 0.07067437469959259, -0.04903900995850563, 0.04944821819663048, 0.018922973424196243, -0.03528052195906639, 0.023545261472463608, 0.007757594343274832, -0.05113856494426727, -0.07686356455087662, 0.08471108973026276, 0.03874548152089119, 0.020320095121860504, -0.01431011501699686]
[2025-05-12 11:41:50,717]: Mean: -0.00193827
[2025-05-12 11:41:50,717]: Min: -0.23105906
[2025-05-12 11:41:50,717]: Max: 0.20018804
[2025-05-12 11:41:50,717]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,718]: Sample Values (16 elements): [0.9396681785583496, 0.9622308611869812, 0.9342868328094482, 0.9656307101249695, 1.0713601112365723, 0.9778590798377991, 0.9480745196342468, 0.9545878767967224, 0.998440146446228, 0.968774676322937, 1.0189766883850098, 1.0302727222442627, 0.9531506896018982, 0.9344507455825806, 0.9858942627906799, 0.8433511853218079]
[2025-05-12 11:41:50,718]: Mean: 0.96793807
[2025-05-12 11:41:50,718]: Min: 0.84335119
[2025-05-12 11:41:50,719]: Max: 1.07136011
[2025-05-12 11:41:50,719]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:41:50,719]: Sample Values (25 elements): [-0.0250077061355114, 0.035894688218832016, -0.031535565853118896, 0.037459347397089005, 0.020433608442544937, 0.0744522288441658, -0.0991031602025032, 0.011758144944906235, 0.011732179671525955, -0.025472033768892288, -0.02043311297893524, 0.0839318037033081, -0.0465158075094223, 0.08777324110269547, 0.006698137614876032, 0.006096868310123682, 0.029559893533587456, 0.088923878967762, -0.11949323862791061, -0.01186240091919899, 0.038988906890153885, 0.019051184877753258, 0.08603903651237488, 0.019207557663321495, 0.11527028679847717]
[2025-05-12 11:41:50,720]: Mean: -0.00221836
[2025-05-12 11:41:50,720]: Min: -0.18446767
[2025-05-12 11:41:50,720]: Max: 0.16952880
[2025-05-12 11:41:50,720]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,721]: Sample Values (16 elements): [1.010339617729187, 0.9983024001121521, 0.9644893407821655, 0.973227858543396, 0.9550133347511292, 0.9487204551696777, 0.9935898780822754, 0.9787437915802002, 0.9671927690505981, 0.9926844835281372, 0.9685603976249695, 0.9659408330917358, 0.9620246887207031, 0.946472704410553, 0.9577217102050781, 0.9661732316017151]
[2025-05-12 11:41:50,721]: Mean: 0.97182488
[2025-05-12 11:41:50,722]: Min: 0.94647270
[2025-05-12 11:41:50,722]: Max: 1.01033962
[2025-05-12 11:41:50,722]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:41:50,722]: Sample Values (25 elements): [0.03731295093894005, 0.03626173734664917, -0.01232842169702053, 0.06055525317788124, 0.0006075284909456968, 0.0017046058783307672, -0.00426827697083354, 0.08950955420732498, -0.009240253828465939, -0.040195781737565994, 0.08300270885229111, -0.12709137797355652, 0.03974425420165062, 0.002684739651158452, 0.024819517508149147, -0.05878683552145958, 0.0031119780614972115, 0.025780443102121353, -0.11831622570753098, -0.0346703827381134, 0.028453800827264786, 0.03575035557150841, -0.04604463651776314, 0.04942353069782257, 0.0089664151892066]
[2025-05-12 11:41:50,723]: Mean: -0.00041091
[2025-05-12 11:41:50,723]: Min: -0.18317582
[2025-05-12 11:41:50,724]: Max: 0.17718346
[2025-05-12 11:41:50,724]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:41:50,724]: Sample Values (16 elements): [0.9504164457321167, 0.8556841015815735, 0.9751031994819641, 1.0011701583862305, 0.9392253756523132, 0.9298421144485474, 0.8970547318458557, 0.9897846579551697, 1.0149266719818115, 0.9455967545509338, 0.9124501943588257, 0.9771863222122192, 0.9671403169631958, 1.0282843112945557, 0.9172340035438538, 0.9482249617576599]
[2025-05-12 11:41:50,724]: Mean: 0.95308280
[2025-05-12 11:41:50,725]: Min: 0.85568410
[2025-05-12 11:41:50,725]: Max: 1.02828431
[2025-05-12 11:41:50,725]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 11:41:50,726]: Sample Values (25 elements): [0.05865606293082237, -0.01952161081135273, 0.053604934364557266, -0.028176914900541306, -0.03603813052177429, -0.09400627017021179, 0.007087242789566517, -0.06923392415046692, 0.007285940460860729, -0.006982570048421621, -0.032643042504787445, -0.019430601969361305, 0.08724451810121536, 0.10989326238632202, 0.015078680589795113, -0.010261038318276405, -0.07336536049842834, -0.028574470430612564, 0.01146736554801464, -0.039785876870155334, 0.06552754342556, -0.02259117178618908, 0.02572789415717125, 0.039672959595918655, -0.023848695680499077]
[2025-05-12 11:41:50,726]: Mean: -0.00098609
[2025-05-12 11:41:50,726]: Min: -0.18193787
[2025-05-12 11:41:50,727]: Max: 0.16666998
[2025-05-12 11:41:50,727]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,729]: Sample Values (25 elements): [0.9728989005088806, 0.9746856093406677, 0.9581893086433411, 0.9544619917869568, 0.9626060724258423, 0.977342963218689, 0.9642017483711243, 0.964609682559967, 0.9901612997055054, 0.9725660681724548, 0.9531207084655762, 1.017035722732544, 0.9651893377304077, 0.947131872177124, 1.0137486457824707, 0.9731554388999939, 0.9627593159675598, 0.9945154786109924, 0.948237419128418, 0.9766324758529663, 0.9743441939353943, 0.9917309880256653, 0.9888250827789307, 0.9505130648612976, 0.9548676609992981]
[2025-05-12 11:41:50,733]: Mean: 0.97169793
[2025-05-12 11:41:50,735]: Min: 0.93355346
[2025-05-12 11:41:50,740]: Max: 1.01703572
[2025-05-12 11:41:50,740]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:41:50,759]: Sample Values (25 elements): [0.034935396164655685, 0.0011458121007308364, -0.005234191659837961, -0.018361123278737068, -0.048506077378988266, -0.020413121208548546, 0.018924597650766373, -0.011401276104152203, 0.029845653101801872, -0.07804694771766663, -0.05447939410805702, -0.05630616471171379, -0.03080594539642334, -0.058109745383262634, 0.028280112892389297, 0.027174698188900948, -0.028838926926255226, -0.03563873842358589, 0.048135098069906235, 0.005227094981819391, 0.011983114294707775, 0.0035791730042546988, 0.002551816403865814, 0.048851802945137024, -0.05678248405456543]
[2025-05-12 11:41:50,763]: Mean: -0.00093054
[2025-05-12 11:41:50,764]: Min: -0.12598930
[2025-05-12 11:41:50,764]: Max: 0.16458426
[2025-05-12 11:41:50,764]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,765]: Sample Values (25 elements): [1.02189302444458, 0.9269327521324158, 0.9527417421340942, 0.9652697443962097, 0.9730444550514221, 0.9922412633895874, 0.9514957666397095, 0.9840880036354065, 0.9512498378753662, 1.0095794200897217, 0.9675741791725159, 0.9516953229904175, 0.988722562789917, 0.9730170965194702, 0.9669920802116394, 0.9478282928466797, 1.0122723579406738, 1.0132460594177246, 1.01026451587677, 1.055152177810669, 0.9442159533500671, 0.9623810052871704, 1.0145485401153564, 1.0212112665176392, 0.9525666236877441]
[2025-05-12 11:41:50,765]: Mean: 0.98277593
[2025-05-12 11:41:50,766]: Min: 0.92693275
[2025-05-12 11:41:50,766]: Max: 1.05515218
[2025-05-12 11:41:50,767]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 11:41:50,767]: Sample Values (25 elements): [-0.205091655254364, -0.14476442337036133, 0.10672134906053543, 0.08580523729324341, -0.1081094741821289, -0.1530836671590805, -0.05141216143965721, -0.16148723661899567, 0.14547912776470184, 0.06890438497066498, -0.14981837570667267, -0.041317202150821686, -0.027675550431013107, 0.01372443325817585, -0.11120928823947906, -0.020491672679781914, 0.15572918951511383, 0.1770382672548294, -0.09838788956403732, -0.12013855576515198, 0.13611193001270294, 0.16448622941970825, 0.01261097565293312, 0.20571665465831757, 0.158249169588089]
[2025-05-12 11:41:50,767]: Mean: 0.00168708
[2025-05-12 11:41:50,768]: Min: -0.31387970
[2025-05-12 11:41:50,768]: Max: 0.28007078
[2025-05-12 11:41:50,768]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,769]: Sample Values (25 elements): [0.9060972332954407, 0.8725706338882446, 0.893828809261322, 0.9139752984046936, 0.9502488970756531, 0.9136358499526978, 0.9300501942634583, 0.9189324378967285, 0.903080940246582, 0.9010387063026428, 0.8746629953384399, 0.905501663684845, 0.9129195213317871, 0.9013932943344116, 0.9088836908340454, 0.9186943769454956, 0.8743655681610107, 0.9547652006149292, 0.8971183896064758, 0.9242067933082581, 0.9078094363212585, 0.9286694526672363, 0.8634954690933228, 0.9191957712173462, 0.9037672281265259]
[2025-05-12 11:41:50,770]: Mean: 0.91222972
[2025-05-12 11:41:50,770]: Min: 0.86349547
[2025-05-12 11:41:50,771]: Max: 0.97983336
[2025-05-12 11:41:50,771]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:41:50,772]: Sample Values (25 elements): [-0.006157790310680866, -0.008522583171725273, 0.01142742671072483, -0.07858531922101974, -0.09993007034063339, 0.040204621851444244, -0.04342920333147049, -0.000498926208820194, 0.04110389202833176, -0.006786426063627005, 0.027715668082237244, -0.04378035292029381, 0.030451709404587746, -0.051550909876823425, -0.01422157697379589, 0.029579490423202515, -0.058853816241025925, 0.030612874776124954, -0.015677355229854584, 0.015328843146562576, -0.011551192961633205, -0.019382843747735023, -0.010929752141237259, -0.026576610282063484, -0.022518575191497803]
[2025-05-12 11:41:50,772]: Mean: -0.00222503
[2025-05-12 11:41:50,772]: Min: -0.12503557
[2025-05-12 11:41:50,773]: Max: 0.12165887
[2025-05-12 11:41:50,773]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,775]: Sample Values (25 elements): [0.95659339427948, 0.9513917565345764, 0.9480975270271301, 0.9722079634666443, 0.9640841484069824, 0.9608256220817566, 0.9920947551727295, 0.9524314403533936, 0.9981428384780884, 1.0010749101638794, 0.9861554503440857, 0.9675872921943665, 0.9632796049118042, 0.9716713428497314, 0.9471304416656494, 0.994493842124939, 0.9873960614204407, 0.9399385452270508, 0.974129319190979, 0.967471718788147, 0.9601837396621704, 0.9536382555961609, 1.014420747756958, 0.9807115793228149, 0.9825949668884277]
[2025-05-12 11:41:50,775]: Mean: 0.97160208
[2025-05-12 11:41:50,776]: Min: 0.93993855
[2025-05-12 11:41:50,776]: Max: 1.01442075
[2025-05-12 11:41:50,776]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:41:50,777]: Sample Values (25 elements): [-0.01636495254933834, 0.08417683839797974, -0.002880629850551486, -0.02016541175544262, 0.04663378745317459, 0.03349064663052559, -0.00871984101831913, -0.017110230401158333, 0.062416061758995056, -0.044823311269283295, 0.012994566932320595, -0.014040120877325535, -0.002859084866940975, 0.07678453624248505, 0.009884160943329334, -0.019812043756246567, -0.010916496627032757, 0.045688696205616, -0.023954560980200768, 0.028139552101492882, -0.05367491766810417, 0.015492244623601437, 0.01536752562969923, -0.004504987504333258, 0.00490282429382205]
[2025-05-12 11:41:50,778]: Mean: -0.00121005
[2025-05-12 11:41:50,779]: Min: -0.13112369
[2025-05-12 11:41:50,779]: Max: 0.12554242
[2025-05-12 11:41:50,779]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,780]: Sample Values (25 elements): [0.9318374395370483, 0.9996078014373779, 0.985945999622345, 0.9528476595878601, 0.9797118306159973, 0.9645252227783203, 0.9938090443611145, 0.9599841237068176, 0.9670428037643433, 0.9687173366546631, 1.0043867826461792, 0.976115345954895, 0.9977681040763855, 0.9752975702285767, 0.9803425669670105, 0.972366988658905, 0.9563036561012268, 0.9512315392494202, 0.9877090454101562, 1.013630986213684, 0.9442805647850037, 0.9728806614875793, 0.9916374087333679, 1.0158103704452515, 0.954227089881897]
[2025-05-12 11:41:50,781]: Mean: 0.97541916
[2025-05-12 11:41:50,782]: Min: 0.93183744
[2025-05-12 11:41:50,782]: Max: 1.01581037
[2025-05-12 11:41:50,782]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:41:50,784]: Sample Values (25 elements): [-0.026348993182182312, -0.05686014145612717, 0.013004896230995655, -0.015808165073394775, -0.01984320394694805, 0.03129881992936134, -0.006801160983741283, -0.02187190391123295, -0.008575156331062317, -0.08113854378461838, -0.0015826158924028277, -0.0019152904860675335, -0.03962858393788338, -0.013233674690127373, -0.03429785370826721, 0.005353687796741724, -0.04704597219824791, 0.025120196864008904, -0.04640563577413559, 0.004323501605540514, -0.050214312970638275, -0.04977201670408249, 0.02953529730439186, -0.041571661829948425, -0.04391021281480789]
[2025-05-12 11:41:50,784]: Mean: -0.00094384
[2025-05-12 11:41:50,784]: Min: -0.12115288
[2025-05-12 11:41:50,785]: Max: 0.12434416
[2025-05-12 11:41:50,785]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,786]: Sample Values (25 elements): [0.9688048958778381, 0.9873008131980896, 1.0238362550735474, 0.9517298340797424, 0.9729419350624084, 0.9832863211631775, 0.9865154027938843, 0.9556698203086853, 0.9619671106338501, 0.9563565254211426, 0.9535115361213684, 0.9795183539390564, 0.9633942246437073, 0.9475094676017761, 0.9984612464904785, 0.9718592762947083, 0.9622876644134521, 0.9355331659317017, 0.9757969379425049, 0.9820850491523743, 0.9662827849388123, 0.9891219735145569, 0.945597231388092, 0.9730382561683655, 0.9830851554870605]
[2025-05-12 11:41:50,786]: Mean: 0.97165024
[2025-05-12 11:41:50,787]: Min: 0.93553317
[2025-05-12 11:41:50,788]: Max: 1.02383626
[2025-05-12 11:41:50,788]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:41:50,789]: Sample Values (25 elements): [-0.059873007237911224, 0.0695282369852066, -0.06235801428556442, -0.07443235069513321, -0.012805123813450336, 0.06802941858768463, 0.06524074077606201, -0.03140063211321831, 0.054177116602659225, 0.04683266952633858, -0.03335370868444443, -0.029902024194598198, -0.033208541572093964, -0.028539706021547318, 0.03237445652484894, -0.03687921166419983, 0.013065315783023834, 0.012476260773837566, 0.04267127439379692, 0.03406936302781105, -0.053604111075401306, -0.00941530056297779, -0.03954226151108742, 0.009170183911919594, 0.052897337824106216]
[2025-05-12 11:41:50,789]: Mean: 0.00052167
[2025-05-12 11:41:50,790]: Min: -0.11946633
[2025-05-12 11:41:50,790]: Max: 0.13245963
[2025-05-12 11:41:50,790]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:41:50,791]: Sample Values (25 elements): [0.9812632203102112, 1.0099016427993774, 1.0186896324157715, 0.9643857479095459, 1.0186132192611694, 1.0124917030334473, 0.9869580864906311, 0.966322660446167, 1.000461220741272, 0.9762206673622131, 0.9618485569953918, 0.9879164695739746, 1.0096981525421143, 0.9910255074501038, 0.9851462244987488, 1.0376956462860107, 1.0160645246505737, 1.0081326961517334, 0.9979482293128967, 0.9866126775741577, 0.9882932305335999, 1.0271817445755005, 1.006949543952942, 0.9539529085159302, 1.016845464706421]
[2025-05-12 11:41:50,792]: Mean: 0.99654549
[2025-05-12 11:41:50,792]: Min: 0.95395291
[2025-05-12 11:41:50,793]: Max: 1.03769565
[2025-05-12 11:41:50,793]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 11:41:50,795]: Sample Values (25 elements): [0.053181182593107224, 0.04295189678668976, -0.021157125011086464, 0.03293038532137871, 0.023041145876049995, 0.06219881400465965, -0.029538342729210854, -0.012891438789665699, -0.019276212900877, -0.016262361779808998, -0.004443296696990728, 0.08551879227161407, 0.02557639591395855, -0.013290254399180412, -0.008336738683283329, -0.04042843356728554, -0.06317785382270813, -0.011176164261996746, 0.026729486882686615, -0.00090035330504179, 0.004471639171242714, 0.022580409422516823, 0.054580751806497574, -0.0030780015513300896, -0.03288497403264046]
[2025-05-12 11:41:50,795]: Mean: -0.00058798
[2025-05-12 11:41:50,795]: Min: -0.11562694
[2025-05-12 11:41:50,795]: Max: 0.11214695
[2025-05-12 11:41:50,795]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,796]: Sample Values (25 elements): [0.9578802585601807, 0.970597505569458, 0.9732090830802917, 0.9787715077400208, 0.9913946986198425, 0.9713283777236938, 0.9584020972251892, 0.9633899331092834, 0.9680368900299072, 0.9738367795944214, 0.962317705154419, 0.9862384796142578, 0.9608867168426514, 0.9557812213897705, 0.9690662622451782, 0.9921503663063049, 0.9756835699081421, 0.9450735449790955, 0.9737995862960815, 0.9678845405578613, 0.9569379687309265, 0.9751508831977844, 0.9920512437820435, 0.9912063479423523, 0.9753997921943665]
[2025-05-12 11:41:50,797]: Mean: 0.97159809
[2025-05-12 11:41:50,797]: Min: 0.94507354
[2025-05-12 11:41:50,797]: Max: 1.00967681
[2025-05-12 11:41:50,797]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:41:50,798]: Sample Values (25 elements): [0.010637735016644001, -0.03645812347531319, 0.010247661732137203, -0.030419599264860153, -0.008284914307296276, -0.042400915175676346, -0.028450435027480125, -0.003727173199877143, -0.0015968895750120282, -0.0012295340420678258, 0.03418207913637161, -0.0033839596435427666, 0.04180152341723442, 0.009639894589781761, 0.05246063321828842, 0.012929624877870083, 0.03899897634983063, 0.04496777430176735, -0.04041158780455589, -0.02218346856534481, 0.012863421812653542, 0.01096348650753498, -0.03819718211889267, -0.043983835726976395, -0.022208429872989655]
[2025-05-12 11:41:50,798]: Mean: -0.00026081
[2025-05-12 11:41:50,799]: Min: -0.09949955
[2025-05-12 11:41:50,800]: Max: 0.09325034
[2025-05-12 11:41:50,800]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,801]: Sample Values (25 elements): [1.0066425800323486, 1.0072635412216187, 1.0188077688217163, 0.9964084029197693, 1.0484561920166016, 0.9852910041809082, 0.973335862159729, 0.9752316474914551, 0.9961562752723694, 0.9933841228485107, 0.9949144124984741, 1.0038179159164429, 1.0433745384216309, 1.0304505825042725, 1.0361192226409912, 1.0266263484954834, 0.9936521053314209, 1.018259048461914, 0.9942198991775513, 0.9811758995056152, 0.9871854782104492, 1.0116591453552246, 1.0168702602386475, 1.012905478477478, 1.020209789276123]
[2025-05-12 11:41:50,801]: Mean: 1.00349557
[2025-05-12 11:41:50,801]: Min: 0.96076381
[2025-05-12 11:41:50,802]: Max: 1.04845619
[2025-05-12 11:41:50,802]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 11:41:50,802]: Sample Values (25 elements): [0.09554269909858704, -0.057508133351802826, -0.12996378540992737, 0.030517954379320145, 0.11237531155347824, 0.11867844313383102, -0.08915531635284424, 0.10000459849834442, -0.011568429879844189, 0.09123418480157852, -0.16476096212863922, -0.08313050121068954, 0.12574414908885956, -0.12045805156230927, -0.03475627303123474, -0.09957857429981232, 0.1417817324399948, -0.08568652719259262, -0.07592758536338806, 0.085479736328125, -0.02753305993974209, -0.15558341145515442, 0.04402999207377434, -0.016023801639676094, 0.10561216622591019]
[2025-05-12 11:41:50,803]: Mean: 0.00148882
[2025-05-12 11:41:50,803]: Min: -0.20814741
[2025-05-12 11:41:50,803]: Max: 0.21122712
[2025-05-12 11:41:50,803]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,804]: Sample Values (25 elements): [0.9336782693862915, 0.956515908241272, 0.9330602884292603, 0.911666750907898, 0.9606467485427856, 0.9299629926681519, 0.9289214015007019, 0.9599343538284302, 0.9429218173027039, 0.9250670671463013, 0.9526882171630859, 0.9435678720474243, 0.9070600867271423, 0.935316264629364, 0.9402784109115601, 0.9348659515380859, 0.9465710520744324, 0.9563322067260742, 0.9332983493804932, 0.9338551759719849, 0.9389222264289856, 0.942183256149292, 0.9301720261573792, 0.9561026096343994, 0.9147250652313232]
[2025-05-12 11:41:50,804]: Mean: 0.93494165
[2025-05-12 11:41:50,805]: Min: 0.90706009
[2025-05-12 11:41:50,805]: Max: 0.97455585
[2025-05-12 11:41:50,805]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:41:50,807]: Sample Values (25 elements): [-0.003546276595443487, -0.006780724972486496, -0.02874457836151123, 0.004308507777750492, -0.029316376894712448, -0.022417884320020676, -0.010477638803422451, 0.04835730791091919, 0.0037436371203511953, 0.05317135527729988, 0.01746731996536255, 0.017907485365867615, -0.00957189965993166, -0.022951578721404076, -0.0012239146744832397, -0.0001947228447534144, -0.01337332185357809, -0.002258401596918702, -0.034768857061862946, 0.02455335296690464, 0.009850643575191498, -0.003114310558885336, -0.030421895906329155, -0.01908385194838047, -0.017829569056630135]
[2025-05-12 11:41:50,807]: Mean: -0.00118431
[2025-05-12 11:41:50,807]: Min: -0.09691218
[2025-05-12 11:41:50,808]: Max: 0.10789420
[2025-05-12 11:41:50,808]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,808]: Sample Values (25 elements): [0.9665386080741882, 0.9759840369224548, 0.9643003940582275, 0.9674827456474304, 0.9599757194519043, 0.9772202968597412, 0.9629353284835815, 0.9747185111045837, 0.9720816612243652, 0.9873481392860413, 0.9703432321548462, 0.9694743752479553, 0.9835489988327026, 0.9826270937919617, 0.9908202290534973, 1.001299262046814, 0.9846527576446533, 0.9646388292312622, 0.962780237197876, 0.9629552960395813, 0.9754703640937805, 0.9718275666236877, 0.9709458947181702, 0.9838964939117432, 1.0016634464263916]
[2025-05-12 11:41:50,809]: Mean: 0.97107959
[2025-05-12 11:41:50,809]: Min: 0.94059527
[2025-05-12 11:41:50,810]: Max: 1.00166345
[2025-05-12 11:41:50,810]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:41:50,811]: Sample Values (25 elements): [-0.023950617760419846, 0.02936464734375477, 0.036410439759492874, 0.019990799948573112, -0.007797528058290482, 0.00860424991697073, 0.033931512385606766, -0.05094590410590172, 0.043615568429231644, 0.023599786683917046, 0.0037299045361578465, -0.04270032420754433, 0.01079266332089901, 0.017385756596922874, 0.01836465112864971, 0.019431080669164658, -0.03530929237604141, -0.03812000900506973, -0.01569848321378231, 0.015855619683861732, 0.0014712171396240592, -0.015940457582473755, 0.02145726978778839, 0.041474487632513046, 0.012509429827332497]
[2025-05-12 11:41:50,811]: Mean: 0.00022253
[2025-05-12 11:41:50,812]: Min: -0.08234599
[2025-05-12 11:41:50,812]: Max: 0.09859828
[2025-05-12 11:41:50,812]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,813]: Sample Values (25 elements): [1.02023184299469, 0.9948903322219849, 1.0330005884170532, 1.045142650604248, 1.0181783437728882, 1.014093041419983, 1.0155706405639648, 1.028853178024292, 1.030885100364685, 1.0242836475372314, 1.0175303220748901, 1.0319900512695312, 1.020378589630127, 1.029689073562622, 1.0150563716888428, 1.0025825500488281, 1.0482171773910522, 1.030434489250183, 1.0368170738220215, 1.0173122882843018, 1.0136313438415527, 1.0258017778396606, 1.0098756551742554, 1.0475040674209595, 1.0357277393341064]
[2025-05-12 11:41:50,813]: Mean: 1.02534664
[2025-05-12 11:41:50,814]: Min: 0.99489033
[2025-05-12 11:41:50,815]: Max: 1.06709027
[2025-05-12 11:41:50,815]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:41:50,816]: Sample Values (25 elements): [-0.01935223676264286, -0.02787473239004612, -0.005895625334233046, -0.014152588322758675, 0.007786779198795557, -0.03806092590093613, -0.015359039418399334, -0.002624178072437644, -0.011524347588419914, 0.02173282392323017, 0.02646842785179615, -0.02444823458790779, -0.006422423291951418, 0.000634624739177525, -0.03305167704820633, 0.015545785427093506, 0.024008329957723618, -0.02146601863205433, -0.001300776726566255, 0.04972868040204048, 0.006413793656975031, -0.020367445424199104, 0.00376341724768281, -0.0032706342171877623, -0.04245666041970253]
[2025-05-12 11:41:50,816]: Mean: -0.00030871
[2025-05-12 11:41:50,817]: Min: -0.07636463
[2025-05-12 11:41:50,817]: Max: 0.08017401
[2025-05-12 11:41:50,817]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,818]: Sample Values (25 elements): [0.9696444272994995, 0.9671094417572021, 0.964471161365509, 0.971114456653595, 0.9702498316764832, 0.9711195826530457, 0.971268892288208, 0.9572657942771912, 0.9901928305625916, 0.9711159467697144, 0.9637680053710938, 0.9679244160652161, 0.97795170545578, 0.9810957312583923, 0.9714807271957397, 0.9719879031181335, 0.9667342305183411, 0.9674062728881836, 0.9666349291801453, 0.9667210578918457, 0.9797049164772034, 0.9730925559997559, 0.9602814316749573, 0.9706172943115234, 0.9715263247489929]
[2025-05-12 11:41:50,818]: Mean: 0.97156298
[2025-05-12 11:41:50,819]: Min: 0.95144230
[2025-05-12 11:41:50,820]: Max: 1.01145017
[2025-05-12 11:41:50,820]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:41:50,820]: Sample Values (25 elements): [-0.025429418310523033, 0.002422626595944166, 0.030823729932308197, -0.005184081383049488, 0.029522042721509933, 0.0171050988137722, -0.01436850056052208, -0.02659829519689083, 0.015149971470236778, -0.002418999560177326, -0.006668272893875837, -0.02323656901717186, -0.02625167928636074, -0.013866609893739223, 0.01641131192445755, 0.015181000344455242, 0.02372993528842926, -0.028804855421185493, 0.024293553084135056, -0.042858365923166275, -0.0457189716398716, 0.01507301814854145, 0.01685786433517933, -0.00592674920335412, 0.009634650312364101]
[2025-05-12 11:41:50,822]: Mean: 0.00039574
[2025-05-12 11:41:50,822]: Min: -0.06743822
[2025-05-12 11:41:50,822]: Max: 0.07642981
[2025-05-12 11:41:50,822]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:41:50,823]: Sample Values (25 elements): [1.0591533184051514, 1.0460960865020752, 1.0726218223571777, 1.0388509035110474, 1.066094994544983, 1.0426199436187744, 1.0602517127990723, 1.0465281009674072, 1.0437352657318115, 1.046536922454834, 1.0888725519180298, 1.0817104578018188, 1.0931967496871948, 1.0481913089752197, 1.0625072717666626, 1.0615290403366089, 1.0691813230514526, 1.0536627769470215, 1.0631617307662964, 1.0523931980133057, 1.0534977912902832, 1.072257161140442, 1.0779963731765747, 1.1278955936431885, 1.0598942041397095]
[2025-05-12 11:41:50,824]: Mean: 1.06084204
[2025-05-12 11:41:50,824]: Min: 1.02035522
[2025-05-12 11:41:50,825]: Max: 1.12789559
[2025-05-12 11:41:50,825]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 11:41:50,826]: Sample Values (25 elements): [-0.14939604699611664, -0.1333162635564804, 0.30013740062713623, -0.2328731268644333, -0.3300550580024719, 0.2936088442802429, 0.18818457424640656, 0.24397747218608856, 0.004452700726687908, 0.02105281874537468, -0.08265867829322815, -0.2061743289232254, 0.015183175913989544, 0.0007409887621179223, -0.13757996261119843, -0.28004035353660583, -0.1460852324962616, -0.19301505386829376, -0.06233429163694382, -0.13771098852157593, -0.13134072721004486, 0.016908269375562668, -0.1147519126534462, 0.07124771177768707, -0.18707944452762604]
[2025-05-12 11:41:50,826]: Mean: 0.00018762
[2025-05-12 11:41:50,827]: Min: -0.33357048
[2025-05-12 11:41:50,827]: Max: 0.54764146
[2025-05-12 11:41:50,827]: 


QAT of ResNet20 with parametrized_relu down to 4 bits...
[2025-05-12 11:41:51,191]: [ResNet20_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-05-12 11:41:51,387]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:43:51,042]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.4018 Train Acc: 0.8580 Eval Loss: 0.5679 Eval Acc: 0.8178 (LR: 0.001000)
[2025-05-12 11:45:51,285]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.3968 Train Acc: 0.8620 Eval Loss: 0.5362 Eval Acc: 0.8275 (LR: 0.001000)
[2025-05-12 11:47:47,883]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.3948 Train Acc: 0.8619 Eval Loss: 0.5349 Eval Acc: 0.8283 (LR: 0.001000)
[2025-05-12 11:49:44,919]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.3911 Train Acc: 0.8629 Eval Loss: 0.5048 Eval Acc: 0.8360 (LR: 0.001000)
[2025-05-12 11:51:41,408]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.3913 Train Acc: 0.8629 Eval Loss: 0.5245 Eval Acc: 0.8290 (LR: 0.001000)
[2025-05-12 11:53:39,355]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.3854 Train Acc: 0.8658 Eval Loss: 0.5069 Eval Acc: 0.8360 (LR: 0.001000)
[2025-05-12 11:55:36,801]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.3789 Train Acc: 0.8655 Eval Loss: 0.5097 Eval Acc: 0.8364 (LR: 0.001000)
[2025-05-12 11:57:35,486]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.3862 Train Acc: 0.8635 Eval Loss: 0.5141 Eval Acc: 0.8360 (LR: 0.001000)
[2025-05-12 11:59:32,604]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.3807 Train Acc: 0.8663 Eval Loss: 0.5253 Eval Acc: 0.8344 (LR: 0.001000)
[2025-05-12 12:01:29,561]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.3787 Train Acc: 0.8668 Eval Loss: 0.5330 Eval Acc: 0.8301 (LR: 0.001000)
[2025-05-12 12:03:27,641]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.3750 Train Acc: 0.8683 Eval Loss: 0.5006 Eval Acc: 0.8372 (LR: 0.001000)
[2025-05-12 12:05:25,859]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.3756 Train Acc: 0.8663 Eval Loss: 0.4950 Eval Acc: 0.8378 (LR: 0.001000)
[2025-05-12 12:07:26,567]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.3746 Train Acc: 0.8692 Eval Loss: 0.4929 Eval Acc: 0.8389 (LR: 0.001000)
[2025-05-12 12:09:25,628]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.3638 Train Acc: 0.8717 Eval Loss: 0.6239 Eval Acc: 0.7981 (LR: 0.001000)
[2025-05-12 12:11:25,886]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.3624 Train Acc: 0.8723 Eval Loss: 0.4971 Eval Acc: 0.8382 (LR: 0.001000)
[2025-05-12 12:13:23,863]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.3657 Train Acc: 0.8728 Eval Loss: 0.4566 Eval Acc: 0.8512 (LR: 0.001000)
[2025-05-12 12:15:20,442]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.3651 Train Acc: 0.8726 Eval Loss: 0.5165 Eval Acc: 0.8289 (LR: 0.001000)
[2025-05-12 12:17:16,029]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.3563 Train Acc: 0.8752 Eval Loss: 0.4842 Eval Acc: 0.8418 (LR: 0.001000)
[2025-05-12 12:19:13,706]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.3555 Train Acc: 0.8754 Eval Loss: 0.5356 Eval Acc: 0.8260 (LR: 0.001000)
[2025-05-12 12:21:11,719]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.3560 Train Acc: 0.8749 Eval Loss: 0.4839 Eval Acc: 0.8440 (LR: 0.001000)
[2025-05-12 12:23:08,381]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.3530 Train Acc: 0.8755 Eval Loss: 0.4927 Eval Acc: 0.8383 (LR: 0.001000)
[2025-05-12 12:25:02,857]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.3563 Train Acc: 0.8753 Eval Loss: 0.5798 Eval Acc: 0.8141 (LR: 0.001000)
[2025-05-12 12:26:57,854]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.3501 Train Acc: 0.8783 Eval Loss: 0.5203 Eval Acc: 0.8299 (LR: 0.001000)
[2025-05-12 12:28:53,122]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.3482 Train Acc: 0.8788 Eval Loss: 0.5140 Eval Acc: 0.8349 (LR: 0.001000)
[2025-05-12 12:30:48,107]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.3460 Train Acc: 0.8784 Eval Loss: 0.4648 Eval Acc: 0.8479 (LR: 0.001000)
[2025-05-12 12:32:43,835]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.3398 Train Acc: 0.8818 Eval Loss: 0.4626 Eval Acc: 0.8497 (LR: 0.001000)
[2025-05-12 12:34:36,734]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.3453 Train Acc: 0.8791 Eval Loss: 0.4383 Eval Acc: 0.8546 (LR: 0.001000)
[2025-05-12 12:36:27,988]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.3367 Train Acc: 0.8816 Eval Loss: 0.5086 Eval Acc: 0.8386 (LR: 0.001000)
[2025-05-12 12:38:12,771]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.3335 Train Acc: 0.8835 Eval Loss: 0.4523 Eval Acc: 0.8546 (LR: 0.001000)
[2025-05-12 12:39:53,125]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.3412 Train Acc: 0.8798 Eval Loss: 0.4819 Eval Acc: 0.8416 (LR: 0.000250)
[2025-05-12 12:41:32,961]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.2972 Train Acc: 0.8972 Eval Loss: 0.4090 Eval Acc: 0.8667 (LR: 0.000250)
[2025-05-12 12:43:12,519]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.2896 Train Acc: 0.8997 Eval Loss: 0.4079 Eval Acc: 0.8644 (LR: 0.000250)
[2025-05-12 12:44:52,568]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.2845 Train Acc: 0.8995 Eval Loss: 0.3938 Eval Acc: 0.8702 (LR: 0.000250)
[2025-05-12 12:46:42,206]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.2826 Train Acc: 0.9021 Eval Loss: 0.4080 Eval Acc: 0.8654 (LR: 0.000250)
[2025-05-12 12:48:35,936]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.2842 Train Acc: 0.9011 Eval Loss: 0.3982 Eval Acc: 0.8742 (LR: 0.000250)
[2025-05-12 12:50:31,747]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.2805 Train Acc: 0.9031 Eval Loss: 0.3944 Eval Acc: 0.8672 (LR: 0.000250)
[2025-05-12 12:52:27,263]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.2799 Train Acc: 0.9029 Eval Loss: 0.3955 Eval Acc: 0.8703 (LR: 0.000250)
[2025-05-12 12:54:22,647]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.2813 Train Acc: 0.9002 Eval Loss: 0.3965 Eval Acc: 0.8706 (LR: 0.000250)
[2025-05-12 12:56:17,237]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.2801 Train Acc: 0.9011 Eval Loss: 0.4039 Eval Acc: 0.8679 (LR: 0.000250)
[2025-05-12 12:58:15,134]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.2790 Train Acc: 0.9020 Eval Loss: 0.4232 Eval Acc: 0.8642 (LR: 0.000250)
[2025-05-12 13:00:12,051]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.2783 Train Acc: 0.9031 Eval Loss: 0.4069 Eval Acc: 0.8644 (LR: 0.000250)
[2025-05-12 13:02:05,987]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.2788 Train Acc: 0.9030 Eval Loss: 0.4250 Eval Acc: 0.8619 (LR: 0.000250)
[2025-05-12 13:03:59,555]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.2800 Train Acc: 0.9030 Eval Loss: 0.4190 Eval Acc: 0.8637 (LR: 0.000250)
[2025-05-12 13:05:53,361]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.2770 Train Acc: 0.9021 Eval Loss: 0.4035 Eval Acc: 0.8692 (LR: 0.000250)
[2025-05-12 13:07:48,394]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.2775 Train Acc: 0.9033 Eval Loss: 0.4061 Eval Acc: 0.8665 (LR: 0.000063)
[2025-05-12 13:09:43,437]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.2633 Train Acc: 0.9069 Eval Loss: 0.3841 Eval Acc: 0.8718 (LR: 0.000063)
[2025-05-12 13:11:39,021]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.2616 Train Acc: 0.9092 Eval Loss: 0.3811 Eval Acc: 0.8741 (LR: 0.000063)
[2025-05-12 13:13:33,819]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.2594 Train Acc: 0.9099 Eval Loss: 0.3945 Eval Acc: 0.8705 (LR: 0.000063)
[2025-05-12 13:15:26,805]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.2586 Train Acc: 0.9091 Eval Loss: 0.3849 Eval Acc: 0.8747 (LR: 0.000063)
[2025-05-12 13:17:17,208]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.2623 Train Acc: 0.9087 Eval Loss: 0.3837 Eval Acc: 0.8743 (LR: 0.000063)
[2025-05-12 13:19:07,194]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.2566 Train Acc: 0.9104 Eval Loss: 0.3887 Eval Acc: 0.8727 (LR: 0.000063)
[2025-05-12 13:20:58,530]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.2593 Train Acc: 0.9094 Eval Loss: 0.3935 Eval Acc: 0.8714 (LR: 0.000063)
[2025-05-12 13:22:50,715]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.2557 Train Acc: 0.9097 Eval Loss: 0.3856 Eval Acc: 0.8736 (LR: 0.000063)
[2025-05-12 13:24:42,387]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.2556 Train Acc: 0.9102 Eval Loss: 0.3956 Eval Acc: 0.8727 (LR: 0.000063)
[2025-05-12 13:26:34,868]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.2559 Train Acc: 0.9096 Eval Loss: 0.3919 Eval Acc: 0.8724 (LR: 0.000063)
[2025-05-12 13:28:26,075]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.2573 Train Acc: 0.9094 Eval Loss: 0.3907 Eval Acc: 0.8707 (LR: 0.000063)
[2025-05-12 13:30:16,769]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.2576 Train Acc: 0.9097 Eval Loss: 0.3850 Eval Acc: 0.8733 (LR: 0.000063)
[2025-05-12 13:32:07,422]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.2587 Train Acc: 0.9099 Eval Loss: 0.3887 Eval Acc: 0.8717 (LR: 0.000063)
[2025-05-12 13:33:59,564]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.2571 Train Acc: 0.9107 Eval Loss: 0.3780 Eval Acc: 0.8745 (LR: 0.000063)
[2025-05-12 13:35:53,604]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.2562 Train Acc: 0.9104 Eval Loss: 0.3897 Eval Acc: 0.8730 (LR: 0.000063)
[2025-05-12 13:35:53,611]: [ResNet20_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.8747
[2025-05-12 13:35:53,690]: 


Quantization of model down to 4 bits finished
[2025-05-12 13:35:53,690]: Model Architecture:
[2025-05-12 13:35:53,894]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3821], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.731510639190674)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0444], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3122681975364685, max_val=0.35417842864990234)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3827], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.741099834442139)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21701116859912872, max_val=0.3748849332332611)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3814], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.720986366271973)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0346], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26867783069610596, max_val=0.2500728368759155)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3836], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.754556179046631)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0319], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27663591504096985, max_val=0.20115786790847778)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3801], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.701534271240234)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0263], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21036741137504578, max_val=0.18466444313526154)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3835], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.752838134765625)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0249], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19115421175956726, max_val=0.18200871348381042)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3875], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.81196403503418)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0260], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20304350554943085, max_val=0.18621571362018585)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3766], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.648465156555176)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0220], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14262494444847107, max_val=0.1875373274087906)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0417], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3230758309364319, max_val=0.30263713002204895)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3840], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.759726524353027)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0201], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15041667222976685, max_val=0.15041755139827728)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3707], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.56071662902832)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0198], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14726299047470093, max_val=0.1493149846792221)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3832], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.747481822967529)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0183], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13622435927391052, max_val=0.138507679104805)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.356601715087891)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1347925066947937, max_val=0.15659724175930023)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3950], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.925135612487793)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0173], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13332682847976685, max_val=0.1267283856868744)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3718], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.5764546394348145)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0145], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11164914071559906, max_val=0.10543686896562576)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0296], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22205662727355957, max_val=0.22197988629341125)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3853], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7788987159729)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0159], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11917691677808762, max_val=0.11951279640197754)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3823], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.733951568603516)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09261135756969452, max_val=0.09876624494791031)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3812], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.718204021453857)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0116], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08679071068763733, max_val=0.08682004362344742)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3831], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7460455894470215)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0100], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07097459584474564, max_val=0.07946830987930298)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4128], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.192137718200684)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:35:53,895]: 
Model Weights:
[2025-05-12 13:35:53,895]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 13:35:53,897]: Sample Values (25 elements): [0.1519983857870102, -0.055180419236421585, 0.06807522475719452, 0.12652689218521118, 0.19010137021541595, 0.2578957676887512, 0.1618332415819168, -0.3075520694255829, 0.1310673952102661, -0.04787954315543175, -0.07244408130645752, 0.1698356568813324, -0.14410074055194855, -0.14248257875442505, 0.11634287983179092, 0.008659499697387218, -0.21676898002624512, -0.03725462406873703, 0.0054551223292946815, -0.22497208416461945, 0.1169218122959137, 0.1314125806093216, -0.04618975520133972, -0.15139563381671906, 0.24383533000946045]
[2025-05-12 13:35:53,897]: Mean: -0.00062940
[2025-05-12 13:35:53,898]: Min: -0.54127246
[2025-05-12 13:35:53,898]: Max: 0.56563783
[2025-05-12 13:35:53,899]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 13:35:53,899]: Sample Values (16 elements): [1.055456519126892, 1.0974785089492798, 1.3968535661697388, 1.0684995651245117, 0.9855932593345642, 1.024771809577942, 1.2860783338546753, 0.987606942653656, 0.7991656064987183, 1.2384147644042969, 1.03328537940979, 0.8590532541275024, 0.9399407505989075, 0.8555580377578735, 0.9520860910415649, 0.8610422015190125]
[2025-05-12 13:35:53,900]: Mean: 1.02755523
[2025-05-12 13:35:53,901]: Min: 0.79916561
[2025-05-12 13:35:53,901]: Max: 1.39685357
[2025-05-12 13:35:53,906]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:35:53,911]: Sample Values (25 elements): [0.08885964006185532, 0.0, 0.0, 0.04442982003092766, 0.0, 0.0, 0.0, -0.04442982003092766, 0.0, -0.04442982003092766, 0.04442982003092766, -0.04442982003092766, 0.0, 0.13328945636749268, -0.04442982003092766, -0.08885964006185532, 0.0, 0.0, 0.04442982003092766, 0.04442982003092766, -0.04442982003092766, 0.08885964006185532, -0.04442982003092766, 0.08885964006185532, 0.0]
[2025-05-12 13:35:53,912]: Mean: -0.00435813
[2025-05-12 13:35:53,917]: Min: -0.31100875
[2025-05-12 13:35:53,922]: Max: 0.35543856
[2025-05-12 13:35:53,922]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:35:53,941]: Sample Values (16 elements): [0.9553000926971436, 1.0179102420806885, 1.0565922260284424, 1.0895065069198608, 0.9317233562469482, 0.9592368006706238, 1.186579942703247, 0.8778197765350342, 0.9762812852859497, 0.9329957962036133, 0.9039035439491272, 1.126503586769104, 1.143097162246704, 1.0907779932022095, 0.8922380805015564, 0.8136753439903259]
[2025-05-12 13:35:53,941]: Mean: 0.99713391
[2025-05-12 13:35:53,942]: Min: 0.81367534
[2025-05-12 13:35:53,943]: Max: 1.18657994
[2025-05-12 13:35:53,946]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:35:53,948]: Sample Values (25 elements): [0.03945973142981529, 0.03945973142981529, 0.03945973142981529, 0.07891946285963058, 0.0, -0.11837919056415558, 0.0, -0.03945973142981529, -0.11837919056415558, -0.03945973142981529, 0.0, -0.03945973142981529, 0.0, -0.03945973142981529, 0.0, -0.03945973142981529, 0.0, 0.11837919056415558, -0.03945973142981529, -0.03945973142981529, 0.03945973142981529, -0.03945973142981529, 0.0, -0.03945973142981529, 0.0]
[2025-05-12 13:35:53,950]: Mean: -0.00515511
[2025-05-12 13:35:53,950]: Min: -0.19729866
[2025-05-12 13:35:53,951]: Max: 0.35513759
[2025-05-12 13:35:53,951]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:35:53,952]: Sample Values (16 elements): [0.9528108239173889, 1.010756254196167, 0.886416494846344, 0.9836286306381226, 1.041702151298523, 1.0018290281295776, 1.0127118825912476, 1.0448780059814453, 0.8863562345504761, 1.0223640203475952, 1.0916544198989868, 0.824100911617279, 0.9961851239204407, 1.0017625093460083, 0.9863113760948181, 1.065578579902649]
[2025-05-12 13:35:53,952]: Mean: 0.98806542
[2025-05-12 13:35:53,953]: Min: 0.82410091
[2025-05-12 13:35:53,955]: Max: 1.09165442
[2025-05-12 13:35:53,958]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:35:53,959]: Sample Values (25 elements): [0.13833332061767578, -0.17291665077209473, -0.034583330154418945, 0.06916666030883789, 0.0, 0.034583330154418945, 0.10374999046325684, 0.034583330154418945, 0.0, 0.034583330154418945, -0.06916666030883789, 0.10374999046325684, 0.0, -0.06916666030883789, -0.10374999046325684, -0.034583330154418945, -0.034583330154418945, 0.0, -0.034583330154418945, 0.06916666030883789, -0.10374999046325684, 0.034583330154418945, 0.034583330154418945, 0.0, 0.0]
[2025-05-12 13:35:53,961]: Mean: -0.00232657
[2025-05-12 13:35:53,961]: Min: -0.27666664
[2025-05-12 13:35:53,962]: Max: 0.24208331
[2025-05-12 13:35:53,962]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:35:53,964]: Sample Values (16 elements): [0.9780824780464172, 1.183470368385315, 0.9726505875587463, 0.9642402529716492, 0.9338938593864441, 0.9148170948028564, 1.1373969316482544, 1.0293846130371094, 0.9592384099960327, 0.9332383871078491, 0.8684996366500854, 1.0304521322250366, 0.918121874332428, 0.945484459400177, 0.9227799773216248, 1.0932594537734985]
[2025-05-12 13:35:53,964]: Mean: 0.98656321
[2025-05-12 13:35:53,965]: Min: 0.86849964
[2025-05-12 13:35:53,966]: Max: 1.18347037
[2025-05-12 13:35:53,970]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:35:53,972]: Sample Values (25 elements): [0.06370586156845093, 0.0, 0.0, -0.031852930784225464, -0.031852930784225464, 0.031852930784225464, -0.031852930784225464, -0.09555879235267639, 0.031852930784225464, 0.06370586156845093, 0.031852930784225464, 0.09555879235267639, 0.0, -0.09555879235267639, 0.0, 0.0, 0.031852930784225464, 0.0, -0.06370586156845093, 0.0, 0.06370586156845093, 0.06370586156845093, 0.031852930784225464, 0.031852930784225464, 0.031852930784225464]
[2025-05-12 13:35:53,972]: Mean: -0.00223966
[2025-05-12 13:35:53,974]: Min: -0.28667638
[2025-05-12 13:35:53,974]: Max: 0.19111758
[2025-05-12 13:35:53,974]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:35:53,975]: Sample Values (16 elements): [0.9806503057479858, 0.9348943829536438, 0.9437283277511597, 0.9461193680763245, 0.9144512414932251, 1.1104741096496582, 0.7895230054855347, 0.9466465711593628, 1.0148900747299194, 0.9467875361442566, 0.99317467212677, 1.0218182802200317, 1.0399287939071655, 0.965137779712677, 0.8999541997909546, 0.9858091473579407]
[2025-05-12 13:35:53,976]: Mean: 0.96462429
[2025-05-12 13:35:53,976]: Min: 0.78952301
[2025-05-12 13:35:53,977]: Max: 1.11047411
[2025-05-12 13:35:53,980]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:35:53,981]: Sample Values (25 elements): [-0.05267081782221794, -0.07900622487068176, 0.0, 0.0, -0.05267081782221794, 0.05267081782221794, 0.05267081782221794, 0.02633540891110897, 0.02633540891110897, 0.02633540891110897, -0.05267081782221794, -0.02633540891110897, 0.13167704641819, 0.0, 0.02633540891110897, -0.02633540891110897, 0.07900622487068176, 0.0, -0.02633540891110897, 0.05267081782221794, 0.02633540891110897, 0.0, 0.05267081782221794, -0.05267081782221794, -0.02633540891110897]
[2025-05-12 13:35:53,982]: Mean: -0.00257182
[2025-05-12 13:35:53,982]: Min: -0.21068327
[2025-05-12 13:35:53,983]: Max: 0.18434787
[2025-05-12 13:35:53,983]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:35:53,984]: Sample Values (16 elements): [0.9704861044883728, 0.9599961638450623, 0.9631679058074951, 0.9620299935340881, 0.9349113702774048, 1.0025780200958252, 0.9307110905647278, 1.0301250219345093, 0.9901288151741028, 0.974337100982666, 0.9610141515731812, 0.9623805284500122, 0.9550840258598328, 0.9811347723007202, 0.9703264832496643, 0.9928768873214722]
[2025-05-12 13:35:53,984]: Mean: 0.97133052
[2025-05-12 13:35:53,984]: Min: 0.93071109
[2025-05-12 13:35:53,985]: Max: 1.03012502
[2025-05-12 13:35:53,987]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:35:53,988]: Sample Values (25 elements): [0.07463257014751434, -0.12438760697841644, 0.02487752214074135, -0.0497550442814827, -0.0497550442814827, 0.0497550442814827, 0.07463257014751434, 0.02487752214074135, 0.0497550442814827, -0.02487752214074135, 0.07463257014751434, 0.0, 0.07463257014751434, 0.0995100885629654, -0.0497550442814827, -0.0995100885629654, -0.07463257014751434, 0.02487752214074135, 0.0497550442814827, -0.02487752214074135, 0.0, -0.02487752214074135, -0.0497550442814827, 0.0995100885629654, -0.0497550442814827]
[2025-05-12 13:35:53,996]: Mean: 0.00004319
[2025-05-12 13:35:53,999]: Min: -0.19902018
[2025-05-12 13:35:54,006]: Max: 0.17414266
[2025-05-12 13:35:54,006]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:35:54,022]: Sample Values (16 elements): [1.047762393951416, 0.9357730150222778, 0.9099515080451965, 0.9749702215194702, 0.9657914638519287, 0.9922840595245361, 0.9824123382568359, 1.0194171667099, 0.8932966589927673, 0.9267739653587341, 1.0308725833892822, 0.8935046792030334, 0.9384202361106873, 0.9513266682624817, 0.8242595195770264, 0.9001490473747253]
[2025-05-12 13:35:54,025]: Mean: 0.94918537
[2025-05-12 13:35:54,025]: Min: 0.82425952
[2025-05-12 13:35:54,026]: Max: 1.04776239
[2025-05-12 13:35:54,028]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 13:35:54,029]: Sample Values (25 elements): [-0.05190132558345795, -0.025950662791728973, -0.07785198837518692, 0.05190132558345795, 0.05190132558345795, -0.025950662791728973, -0.025950662791728973, -0.05190132558345795, 0.0, -0.025950662791728973, 0.025950662791728973, -0.025950662791728973, 0.025950662791728973, 0.07785198837518692, 0.1038026511669159, -0.025950662791728973, -0.07785198837518692, 0.1038026511669159, -0.025950662791728973, -0.1038026511669159, -0.05190132558345795, -0.025950662791728973, 0.05190132558345795, -0.05190132558345795, 0.025950662791728973]
[2025-05-12 13:35:54,030]: Mean: -0.00104749
[2025-05-12 13:35:54,030]: Min: -0.20760530
[2025-05-12 13:35:54,030]: Max: 0.18165463
[2025-05-12 13:35:54,030]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,031]: Sample Values (25 elements): [1.0000855922698975, 0.9492818713188171, 0.9772303700447083, 0.9826934337615967, 1.0003154277801514, 0.97954261302948, 0.9881226420402527, 0.9422217607498169, 0.9939290285110474, 1.0233384370803833, 0.9692835807800293, 0.946053147315979, 0.9677096009254456, 0.9572404623031616, 0.9739344120025635, 0.944628119468689, 0.9835281372070312, 0.9559211134910583, 0.9626517295837402, 0.9823870062828064, 0.9694210290908813, 0.9749495983123779, 0.9314675331115723, 1.0263417959213257, 0.9493318796157837]
[2025-05-12 13:35:54,031]: Mean: 0.97211629
[2025-05-12 13:35:54,032]: Min: 0.93146753
[2025-05-12 13:35:54,032]: Max: 1.02634180
[2025-05-12 13:35:54,035]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:35:54,036]: Sample Values (25 elements): [0.0, -0.022010764107108116, -0.022010764107108116, 0.08804305642843246, 0.022010764107108116, 0.0, 0.0, -0.04402152821421623, 0.0, -0.022010764107108116, 0.022010764107108116, 0.0, 0.04402152821421623, 0.0, 0.04402152821421623, -0.04402152821421623, -0.0660322904586792, 0.022010764107108116, 0.0, -0.0660322904586792, 0.022010764107108116, -0.04402152821421623, 0.0, -0.04402152821421623, 0.022010764107108116]
[2025-05-12 13:35:54,036]: Mean: -0.00105803
[2025-05-12 13:35:54,038]: Min: -0.13206458
[2025-05-12 13:35:54,038]: Max: 0.19809687
[2025-05-12 13:35:54,039]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,039]: Sample Values (25 elements): [0.9165366888046265, 0.9855573773384094, 1.0623509883880615, 1.0118894577026367, 1.0160592794418335, 0.9476298093795776, 1.0207401514053345, 0.9977982640266418, 0.9992867112159729, 0.926156222820282, 0.9475018382072449, 0.9593077301979065, 0.9973585605621338, 0.961310625076294, 1.0009862184524536, 1.0230154991149902, 0.9653805494308472, 0.9496542811393738, 0.9639974236488342, 1.020334005355835, 1.0324015617370605, 0.9311413168907166, 0.930827796459198, 1.028002142906189, 1.0151071548461914]
[2025-05-12 13:35:54,040]: Mean: 0.98103970
[2025-05-12 13:35:54,040]: Min: 0.91653669
[2025-05-12 13:35:54,040]: Max: 1.06235099
[2025-05-12 13:35:54,044]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 13:35:54,044]: Sample Values (25 elements): [-0.08342836797237396, -0.29199928045272827, -0.2502850890159607, 0.08342836797237396, 0.16685673594474792, 0.0, 0.12514254450798035, 0.08342836797237396, 0.16685673594474792, 0.0, 0.16685673594474792, -0.08342836797237396, 0.16685673594474792, 0.12514254450798035, 0.08342836797237396, 0.04171418398618698, 0.08342836797237396, -0.04171418398618698, 0.12514254450798035, -0.2085709273815155, -0.12514254450798035, -0.08342836797237396, 0.08342836797237396, -0.29199928045272827, 0.0]
[2025-05-12 13:35:54,045]: Mean: 0.00097768
[2025-05-12 13:35:54,045]: Min: -0.33371347
[2025-05-12 13:35:54,045]: Max: 0.29199928
[2025-05-12 13:35:54,045]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,046]: Sample Values (25 elements): [0.8902915120124817, 0.8246880173683167, 0.9292178750038147, 0.9020795822143555, 0.8751164674758911, 0.9649639129638672, 0.8932308554649353, 0.8679803609848022, 0.8641519546508789, 0.8806638717651367, 0.8682698607444763, 0.8746303915977478, 0.8580286502838135, 0.8964347839355469, 0.8880429267883301, 0.8843476176261902, 0.895016610622406, 0.825456976890564, 0.9089152812957764, 0.888914167881012, 0.8691047430038452, 0.8597263693809509, 0.8561320304870605, 0.8658620715141296, 0.8506923913955688]
[2025-05-12 13:35:54,046]: Mean: 0.87839907
[2025-05-12 13:35:54,048]: Min: 0.81832302
[2025-05-12 13:35:54,048]: Max: 0.96496391
[2025-05-12 13:35:54,053]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:35:54,054]: Sample Values (25 elements): [0.0, 0.020055599510669708, -0.040111199021339417, 0.08022239804267883, -0.020055599510669708, 0.020055599510669708, -0.020055599510669708, -0.08022239804267883, -0.020055599510669708, 0.10027799755334854, 0.0, 0.040111199021339417, 0.020055599510669708, 0.0, 0.020055599510669708, 0.040111199021339417, -0.020055599510669708, 0.0, 0.0, 0.060166798532009125, -0.060166798532009125, 0.0, 0.040111199021339417, 0.0, 0.0]
[2025-05-12 13:35:54,055]: Mean: -0.00260923
[2025-05-12 13:35:54,055]: Min: -0.14038920
[2025-05-12 13:35:54,055]: Max: 0.16044480
[2025-05-12 13:35:54,056]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,056]: Sample Values (25 elements): [0.949721097946167, 0.9715924859046936, 0.9331408739089966, 0.9508668184280396, 0.9535250663757324, 0.9733281135559082, 0.9378604292869568, 0.9642724394798279, 0.9917286038398743, 0.9557865262031555, 0.9874643087387085, 0.9935908913612366, 0.9535441398620605, 0.9878833889961243, 0.9448626637458801, 0.9998453855514526, 0.9610047936439514, 1.023984432220459, 1.0038031339645386, 1.0013155937194824, 0.9607373476028442, 0.9842559695243835, 0.9486732482910156, 0.9537287354469299, 0.9483070373535156]
[2025-05-12 13:35:54,058]: Mean: 0.96867019
[2025-05-12 13:35:54,058]: Min: 0.92366064
[2025-05-12 13:35:54,058]: Max: 1.02398443
[2025-05-12 13:35:54,062]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:35:54,066]: Sample Values (25 elements): [-0.019771911203861237, 0.03954382240772247, -0.05931573361158371, 0.03954382240772247, 0.0, 0.019771911203861237, 0.03954382240772247, 0.05931573361158371, 0.03954382240772247, -0.07908764481544495, -0.05931573361158371, -0.019771911203861237, -0.019771911203861237, -0.03954382240772247, -0.019771911203861237, 0.0, 0.03954382240772247, 0.0, -0.03954382240772247, -0.11863146722316742, 0.019771911203861237, -0.03954382240772247, 0.019771911203861237, -0.05931573361158371, -0.07908764481544495]
[2025-05-12 13:35:54,070]: Mean: -0.00139879
[2025-05-12 13:35:54,074]: Min: -0.13840339
[2025-05-12 13:35:54,087]: Max: 0.15817529
[2025-05-12 13:35:54,087]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,098]: Sample Values (25 elements): [1.0161118507385254, 1.013096570968628, 0.9618409872055054, 0.9842281937599182, 0.9645918607711792, 0.9874805808067322, 1.0126227140426636, 0.9622436165809631, 1.0107187032699585, 0.9283966422080994, 0.9512356519699097, 0.97260981798172, 0.9392608404159546, 0.9263219237327576, 0.943743884563446, 0.9697642922401428, 0.9799585342407227, 0.9999897480010986, 0.9639138579368591, 0.9612537622451782, 0.9921640753746033, 0.9797971248626709, 0.906624436378479, 0.9469717144966125, 0.9652904272079468]
[2025-05-12 13:35:54,098]: Mean: 0.96656150
[2025-05-12 13:35:54,099]: Min: 0.90662444
[2025-05-12 13:35:54,099]: Max: 1.01611185
[2025-05-12 13:35:54,102]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:35:54,103]: Sample Values (25 elements): [0.0, 0.018315471708774567, 0.0549464151263237, 0.0, 0.018315471708774567, 0.018315471708774567, 0.03663094341754913, 0.03663094341754913, 0.03663094341754913, 0.0549464151263237, 0.0549464151263237, -0.018315471708774567, -0.018315471708774567, -0.03663094341754913, 0.03663094341754913, 0.018315471708774567, 0.03663094341754913, -0.0549464151263237, -0.0549464151263237, -0.018315471708774567, 0.03663094341754913, -0.018315471708774567, 0.03663094341754913, -0.03663094341754913, 0.018315471708774567]
[2025-05-12 13:35:54,104]: Mean: -0.00108311
[2025-05-12 13:35:54,104]: Min: -0.12820831
[2025-05-12 13:35:54,105]: Max: 0.14652377
[2025-05-12 13:35:54,105]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,107]: Sample Values (25 elements): [0.9672457575798035, 0.9161739945411682, 0.9419025778770447, 0.9550511240959167, 0.9669765830039978, 0.9508020281791687, 0.967698335647583, 0.9556032419204712, 0.9484865069389343, 0.953168511390686, 1.006754994392395, 0.9579480290412903, 0.9795316457748413, 0.963626503944397, 0.9703156352043152, 0.9883161187171936, 0.9299669861793518, 0.9411906003952026, 0.9594331383705139, 0.9861463308334351, 0.9696056842803955, 0.9931588172912598, 0.9505376219749451, 0.9668862819671631, 0.9938292503356934]
[2025-05-12 13:35:54,107]: Mean: 0.96549875
[2025-05-12 13:35:54,107]: Min: 0.91617399
[2025-05-12 13:35:54,108]: Max: 1.03326261
[2025-05-12 13:35:54,128]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:35:54,144]: Sample Values (25 elements): [-0.03885197266936302, 0.058277957141399384, 0.01942598633468151, -0.058277957141399384, 0.03885197266936302, -0.058277957141399384, 0.07770394533872604, -0.07770394533872604, -0.03885197266936302, -0.058277957141399384, -0.01942598633468151, -0.01942598633468151, -0.058277957141399384, 0.0, 0.03885197266936302, -0.03885197266936302, -0.03885197266936302, 0.0, 0.03885197266936302, -0.03885197266936302, 0.0, -0.01942598633468151, -0.03885197266936302, -0.01942598633468151, 0.01942598633468151]
[2025-05-12 13:35:54,147]: Mean: 0.00037098
[2025-05-12 13:35:54,147]: Min: -0.13598190
[2025-05-12 13:35:54,148]: Max: 0.15540789
[2025-05-12 13:35:54,148]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:35:54,149]: Sample Values (25 elements): [1.02076256275177, 1.012757420539856, 1.0194894075393677, 0.9543530344963074, 0.9461683630943298, 1.016338586807251, 0.9825571775436401, 1.0074647665023804, 0.9906396269798279, 1.0328670740127563, 0.9894446730613708, 1.0505515336990356, 1.0049490928649902, 0.9526312947273254, 0.9800105690956116, 0.9977157711982727, 1.0078901052474976, 0.968956470489502, 1.0052570104599, 1.022546648979187, 0.9511234164237976, 1.0373402833938599, 0.9892391562461853, 0.9547496438026428, 0.9676199555397034]
[2025-05-12 13:35:54,149]: Mean: 0.99601835
[2025-05-12 13:35:54,149]: Min: 0.94616836
[2025-05-12 13:35:54,150]: Max: 1.05055153
[2025-05-12 13:35:54,152]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 13:35:54,153]: Sample Values (25 elements): [-0.01733706332743168, 0.03467412665486336, 0.01733706332743168, -0.03467412665486336, -0.08668531477451324, -0.03467412665486336, -0.052011191844940186, 0.01733706332743168, 0.01733706332743168, -0.052011191844940186, -0.052011191844940186, -0.03467412665486336, -0.01733706332743168, 0.0, -0.01733706332743168, -0.03467412665486336, -0.01733706332743168, -0.01733706332743168, 0.052011191844940186, 0.052011191844940186, -0.052011191844940186, 0.01733706332743168, 0.0, 0.03467412665486336, -0.03467412665486336]
[2025-05-12 13:35:54,153]: Mean: -0.00075624
[2025-05-12 13:35:54,153]: Min: -0.13869651
[2025-05-12 13:35:54,153]: Max: 0.12135945
[2025-05-12 13:35:54,153]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,155]: Sample Values (25 elements): [0.9676800966262817, 1.0139243602752686, 0.9547690153121948, 0.9747039675712585, 0.9641469120979309, 0.9658319354057312, 0.9693165421485901, 0.933485209941864, 0.9332351088523865, 0.935337245464325, 0.966424286365509, 0.9543409943580627, 0.9395849108695984, 0.9829016923904419, 0.9689710140228271, 0.944789469242096, 0.9996584057807922, 0.9665629267692566, 0.9881232976913452, 0.9713337421417236, 0.9982959628105164, 0.9465818405151367, 0.944584846496582, 0.9595125913619995, 0.941693127155304]
[2025-05-12 13:35:54,155]: Mean: 0.96568751
[2025-05-12 13:35:54,155]: Min: 0.93323511
[2025-05-12 13:35:54,155]: Max: 1.01392436
[2025-05-12 13:35:54,159]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:35:54,160]: Sample Values (25 elements): [0.0434173122048378, -0.028944874182343483, -0.0434173122048378, 0.0, -0.014472437091171741, -0.014472437091171741, 0.028944874182343483, 0.0, 0.014472437091171741, 0.014472437091171741, 0.028944874182343483, 0.014472437091171741, 0.057889748364686966, -0.014472437091171741, 0.028944874182343483, 0.028944874182343483, 0.014472437091171741, -0.028944874182343483, 0.014472437091171741, -0.028944874182343483, 0.0, -0.014472437091171741, -0.014472437091171741, 0.028944874182343483, -0.0434173122048378]
[2025-05-12 13:35:54,161]: Mean: -0.00038827
[2025-05-12 13:35:54,161]: Min: -0.11577950
[2025-05-12 13:35:54,161]: Max: 0.10130706
[2025-05-12 13:35:54,161]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,163]: Sample Values (25 elements): [0.9970627427101135, 0.9681439399719238, 0.9707598090171814, 0.9736064672470093, 1.0347007513046265, 0.9953255653381348, 1.0083441734313965, 1.0434508323669434, 1.0134332180023193, 1.0059136152267456, 0.9961867928504944, 0.9888443350791931, 0.9756345748901367, 1.0290712118148804, 0.9973058700561523, 1.0234884023666382, 1.0073847770690918, 1.0048383474349976, 1.01204252243042, 0.9779180288314819, 0.9678605794906616, 1.015480875968933, 1.0061473846435547, 1.0040334463119507, 1.0471911430358887]
[2025-05-12 13:35:54,163]: Mean: 1.00614643
[2025-05-12 13:35:54,163]: Min: 0.93537372
[2025-05-12 13:35:54,164]: Max: 1.08129394
[2025-05-12 13:35:54,166]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 13:35:54,167]: Sample Values (25 elements): [-0.02960246242582798, 0.02960246242582798, 0.08880738914012909, -0.14801231026649475, -0.08880738914012909, 0.05920492485165596, -0.08880738914012909, -0.17761477828025818, -0.05920492485165596, 0.02960246242582798, 0.11840984970331192, 0.05920492485165596, 0.08880738914012909, 0.05920492485165596, 0.08880738914012909, 0.17761477828025818, 0.0, 0.11840984970331192, 0.08880738914012909, -0.08880738914012909, -0.05920492485165596, 0.02960246242582798, 0.14801231026649475, 0.0, -0.17761477828025818]
[2025-05-12 13:35:54,167]: Mean: 0.00118525
[2025-05-12 13:35:54,168]: Min: -0.23681970
[2025-05-12 13:35:54,169]: Max: 0.20721723
[2025-05-12 13:35:54,169]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,169]: Sample Values (25 elements): [0.9101252555847168, 0.8913156986236572, 0.9339826107025146, 0.901160717010498, 0.9086154103279114, 0.9030212163925171, 0.8902204036712646, 0.8990682363510132, 0.9340825080871582, 0.9092296957969666, 0.9100238680839539, 0.9087381362915039, 0.9007638692855835, 0.9009172916412354, 0.9185565114021301, 0.8780150413513184, 0.9022117853164673, 0.9027952551841736, 0.9016355276107788, 0.9086310863494873, 0.8872056007385254, 0.9269784092903137, 0.917269766330719, 0.9011236429214478, 0.8992911577224731]
[2025-05-12 13:35:54,169]: Mean: 0.90392184
[2025-05-12 13:35:54,170]: Min: 0.87278801
[2025-05-12 13:35:54,171]: Max: 0.96610653
[2025-05-12 13:35:54,173]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:35:54,174]: Sample Values (25 elements): [0.015912702307105064, 0.0, 0.03182540461421013, 0.0, -0.03182540461421013, -0.03182540461421013, -0.015912702307105064, 0.03182540461421013, 0.06365080922842026, -0.015912702307105064, -0.015912702307105064, 0.03182540461421013, -0.015912702307105064, 0.0, -0.015912702307105064, 0.03182540461421013, 0.0, 0.06365080922842026, 0.047738105058670044, 0.0, 0.0, 0.0, -0.015912702307105064, 0.047738105058670044, 0.03182540461421013]
[2025-05-12 13:35:54,174]: Mean: -0.00135584
[2025-05-12 13:35:54,174]: Min: -0.11138891
[2025-05-12 13:35:54,176]: Max: 0.12730162
[2025-05-12 13:35:54,176]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,176]: Sample Values (25 elements): [0.9672356247901917, 0.9786077737808228, 1.0082967281341553, 0.9655359387397766, 0.9651368260383606, 0.9618125557899475, 0.9699486494064331, 0.9672699570655823, 0.9627987742424011, 0.9748242497444153, 0.9631412029266357, 0.9575209021568298, 0.9642248153686523, 0.9532989263534546, 0.9527638554573059, 0.9645600914955139, 0.9615194797515869, 0.9586238861083984, 0.9657440781593323, 0.9537366628646851, 0.9632979035377502, 0.9648852348327637, 0.9640556573867798, 0.9744175672531128, 0.9440224766731262]
[2025-05-12 13:35:54,176]: Mean: 0.96476990
[2025-05-12 13:35:54,177]: Min: 0.91584885
[2025-05-12 13:35:54,177]: Max: 1.00829673
[2025-05-12 13:35:54,180]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:35:54,181]: Sample Values (25 elements): [-0.012758512049913406, 0.012758512049913406, -0.025517024099826813, 0.0, -0.025517024099826813, 0.0, -0.025517024099826813, -0.025517024099826813, -0.051034048199653625, 0.025517024099826813, 0.051034048199653625, 0.0, -0.03827553614974022, -0.051034048199653625, 0.012758512049913406, 0.03827553614974022, 0.03827553614974022, -0.025517024099826813, 0.012758512049913406, -0.07655107229948044, 0.025517024099826813, 0.0, -0.025517024099826813, 0.025517024099826813, 0.0]
[2025-05-12 13:35:54,181]: Mean: 0.00015194
[2025-05-12 13:35:54,182]: Min: -0.08930959
[2025-05-12 13:35:54,182]: Max: 0.10206810
[2025-05-12 13:35:54,182]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,182]: Sample Values (25 elements): [1.0230379104614258, 1.044950246810913, 1.0448025465011597, 1.0063185691833496, 1.0301580429077148, 1.028643250465393, 1.0297961235046387, 1.0249603986740112, 1.0317752361297607, 0.9972363710403442, 1.0385137796401978, 1.0142568349838257, 1.0436198711395264, 1.0152220726013184, 1.0088715553283691, 1.031760573387146, 1.023680567741394, 1.0138858556747437, 1.0223084688186646, 1.0474849939346313, 1.0334270000457764, 1.028313159942627, 1.0239567756652832, 1.043441653251648, 1.0200223922729492]
[2025-05-12 13:35:54,183]: Mean: 1.02705705
[2025-05-12 13:35:54,183]: Min: 0.99723637
[2025-05-12 13:35:54,183]: Max: 1.07800925
[2025-05-12 13:35:54,186]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:35:54,186]: Sample Values (25 elements): [0.0, -0.0115740317851305, 0.023148063570261, -0.03472209721803665, 0.0115740317851305, -0.0115740317851305, -0.023148063570261, 0.03472209721803665, -0.0115740317851305, 0.0, 0.0115740317851305, -0.03472209721803665, 0.0, 0.046296127140522, -0.0115740317851305, -0.0115740317851305, 0.023148063570261, 0.0115740317851305, -0.023148063570261, 0.0115740317851305, -0.03472209721803665, -0.023148063570261, 0.0, 0.023148063570261, 0.0115740317851305]
[2025-05-12 13:35:54,187]: Mean: -0.00027409
[2025-05-12 13:35:54,187]: Min: -0.08101822
[2025-05-12 13:35:54,187]: Max: 0.09259225
[2025-05-12 13:35:54,187]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,188]: Sample Values (25 elements): [0.9607199430465698, 0.9571872353553772, 0.9537693858146667, 0.9698368906974792, 0.960323691368103, 0.9366838932037354, 0.9991127252578735, 0.9604071378707886, 0.9534610509872437, 0.971687376499176, 0.9471422433853149, 0.9581424593925476, 0.9636566042900085, 0.9487132430076599, 0.9408911466598511, 0.95536869764328, 0.9625200629234314, 0.9574484825134277, 0.947281539440155, 0.9721118807792664, 0.9712139368057251, 0.9856622219085693, 0.953242301940918, 0.96370530128479, 0.9585698246955872]
[2025-05-12 13:35:54,189]: Mean: 0.95953327
[2025-05-12 13:35:54,189]: Min: 0.93668389
[2025-05-12 13:35:54,189]: Max: 0.99911273
[2025-05-12 13:35:54,191]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:35:54,192]: Sample Values (25 elements): [0.0, -0.010029532946646214, 0.020059065893292427, -0.030088599771261215, 0.0, 0.030088599771261215, 0.030088599771261215, -0.030088599771261215, 0.0, 0.020059065893292427, -0.040118131786584854, -0.020059065893292427, -0.010029532946646214, 0.020059065893292427, 0.030088599771261215, 0.0, 0.020059065893292427, 0.05014766380190849, -0.020059065893292427, 0.010029532946646214, 0.0, 0.0, -0.010029532946646214, -0.010029532946646214, 0.040118131786584854]
[2025-05-12 13:35:54,192]: Mean: 0.00051775
[2025-05-12 13:35:54,193]: Min: -0.07020673
[2025-05-12 13:35:54,193]: Max: 0.08023626
[2025-05-12 13:35:54,193]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:35:54,194]: Sample Values (25 elements): [1.0378347635269165, 1.0852903127670288, 1.0893594026565552, 1.0539089441299438, 1.0949560403823853, 1.086896300315857, 1.0698026418685913, 1.0766308307647705, 1.0761207342147827, 1.0522981882095337, 1.0959711074829102, 1.0726234912872314, 1.0610734224319458, 1.0898452997207642, 1.0829815864562988, 1.0902936458587646, 1.0549981594085693, 1.0816287994384766, 1.0753828287124634, 1.0583252906799316, 1.106650710105896, 1.0972723960876465, 1.0684846639633179, 1.0716114044189453, 1.0626131296157837]
[2025-05-12 13:35:54,194]: Mean: 1.07438147
[2025-05-12 13:35:54,194]: Min: 1.02603495
[2025-05-12 13:35:54,194]: Max: 1.15014362
[2025-05-12 13:35:54,194]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 13:35:54,196]: Sample Values (25 elements): [0.1196196973323822, 0.04767875000834465, -0.23160693049430847, -0.20047926902770996, 0.048018380999565125, -0.056993816047906876, -0.25930002331733704, 0.08668561279773712, -0.22185510396957397, -0.18870338797569275, 0.22151561081409454, 0.04564681276679039, 0.022294925525784492, 0.0700182095170021, 0.28712016344070435, -0.07682693004608154, -0.18192645907402039, -0.24617834389209747, 0.30435916781425476, -0.05934189260005951, -0.15587276220321655, -0.08605974167585373, -0.11509333550930023, -0.1806376576423645, -0.023160796612501144]
[2025-05-12 13:35:54,196]: Mean: 0.00018498
[2025-05-12 13:35:54,196]: Min: -0.36660242
[2025-05-12 13:35:54,197]: Max: 0.58487296
[2025-05-12 13:35:54,197]: 


QAT of ResNet20 with parametrized_relu down to 3 bits...
[2025-05-12 13:35:54,616]: [ResNet20_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-05-12 13:35:54,797]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:37:48,988]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.5996 Train Acc: 0.7890 Eval Loss: 0.7295 Eval Acc: 0.7605 (LR: 0.001000)
[2025-05-12 13:39:44,672]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.5513 Train Acc: 0.8071 Eval Loss: 0.7075 Eval Acc: 0.7725 (LR: 0.001000)
[2025-05-12 13:41:40,195]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.5377 Train Acc: 0.8113 Eval Loss: 0.7568 Eval Acc: 0.7628 (LR: 0.001000)
[2025-05-12 13:43:35,717]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.5323 Train Acc: 0.8146 Eval Loss: 0.7163 Eval Acc: 0.7663 (LR: 0.001000)
[2025-05-12 13:45:32,154]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.5199 Train Acc: 0.8187 Eval Loss: 0.6284 Eval Acc: 0.7889 (LR: 0.001000)
[2025-05-12 13:47:29,344]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.5192 Train Acc: 0.8166 Eval Loss: 0.6426 Eval Acc: 0.7862 (LR: 0.001000)
[2025-05-12 13:49:30,503]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.5114 Train Acc: 0.8212 Eval Loss: 0.6054 Eval Acc: 0.7962 (LR: 0.001000)
[2025-05-12 13:51:28,822]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.5109 Train Acc: 0.8208 Eval Loss: 0.7017 Eval Acc: 0.7728 (LR: 0.001000)
[2025-05-12 13:53:30,249]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.5043 Train Acc: 0.8254 Eval Loss: 0.6203 Eval Acc: 0.7921 (LR: 0.001000)
[2025-05-12 13:55:28,992]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.4982 Train Acc: 0.8241 Eval Loss: 0.6365 Eval Acc: 0.7895 (LR: 0.001000)
[2025-05-12 13:57:24,858]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.4917 Train Acc: 0.8271 Eval Loss: 0.6020 Eval Acc: 0.8025 (LR: 0.001000)
[2025-05-12 13:59:21,379]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.4887 Train Acc: 0.8294 Eval Loss: 0.6051 Eval Acc: 0.8027 (LR: 0.001000)
[2025-05-12 14:01:15,698]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.4896 Train Acc: 0.8273 Eval Loss: 0.6029 Eval Acc: 0.7998 (LR: 0.001000)
[2025-05-12 14:03:10,149]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.4915 Train Acc: 0.8258 Eval Loss: 0.5537 Eval Acc: 0.8190 (LR: 0.001000)
[2025-05-12 14:05:04,655]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.4839 Train Acc: 0.8303 Eval Loss: 0.6118 Eval Acc: 0.8006 (LR: 0.001000)
[2025-05-12 14:07:00,271]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.4783 Train Acc: 0.8324 Eval Loss: 0.5696 Eval Acc: 0.8125 (LR: 0.001000)
[2025-05-12 14:08:54,339]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.4799 Train Acc: 0.8312 Eval Loss: 0.6138 Eval Acc: 0.7983 (LR: 0.001000)
[2025-05-12 14:10:50,244]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.4754 Train Acc: 0.8326 Eval Loss: 0.5447 Eval Acc: 0.8229 (LR: 0.001000)
[2025-05-12 14:12:48,252]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.4707 Train Acc: 0.8346 Eval Loss: 0.5952 Eval Acc: 0.8015 (LR: 0.001000)
[2025-05-12 14:14:46,445]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.4658 Train Acc: 0.8362 Eval Loss: 0.5933 Eval Acc: 0.8027 (LR: 0.001000)
[2025-05-12 14:16:42,812]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.4736 Train Acc: 0.8346 Eval Loss: 0.5802 Eval Acc: 0.8045 (LR: 0.001000)
[2025-05-12 14:18:39,129]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.4652 Train Acc: 0.8378 Eval Loss: 0.5980 Eval Acc: 0.8017 (LR: 0.001000)
[2025-05-12 14:20:36,880]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.4621 Train Acc: 0.8376 Eval Loss: 0.5227 Eval Acc: 0.8316 (LR: 0.001000)
[2025-05-12 14:22:33,967]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.4589 Train Acc: 0.8383 Eval Loss: 0.5359 Eval Acc: 0.8248 (LR: 0.001000)
[2025-05-12 14:24:44,229]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.4573 Train Acc: 0.8412 Eval Loss: 0.6550 Eval Acc: 0.7931 (LR: 0.001000)
[2025-05-12 14:26:40,566]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.4546 Train Acc: 0.8418 Eval Loss: 0.6617 Eval Acc: 0.7916 (LR: 0.001000)
[2025-05-12 14:28:41,183]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.4481 Train Acc: 0.8428 Eval Loss: 0.5479 Eval Acc: 0.8187 (LR: 0.001000)
[2025-05-12 14:30:48,697]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.4568 Train Acc: 0.8404 Eval Loss: 0.5237 Eval Acc: 0.8280 (LR: 0.001000)
[2025-05-12 14:32:55,910]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.4535 Train Acc: 0.8409 Eval Loss: 0.6466 Eval Acc: 0.7918 (LR: 0.001000)
[2025-05-12 14:34:58,136]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.4551 Train Acc: 0.8395 Eval Loss: 0.5382 Eval Acc: 0.8240 (LR: 0.000250)
[2025-05-12 14:36:57,145]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.4052 Train Acc: 0.8584 Eval Loss: 0.4752 Eval Acc: 0.8418 (LR: 0.000250)
[2025-05-12 14:39:20,337]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.3982 Train Acc: 0.8597 Eval Loss: 0.4530 Eval Acc: 0.8490 (LR: 0.000250)
[2025-05-12 14:41:52,139]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.4022 Train Acc: 0.8599 Eval Loss: 0.4830 Eval Acc: 0.8362 (LR: 0.000250)
[2025-05-12 14:44:20,483]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.3967 Train Acc: 0.8617 Eval Loss: 0.4722 Eval Acc: 0.8412 (LR: 0.000250)
[2025-05-12 14:46:49,034]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.3999 Train Acc: 0.8585 Eval Loss: 0.4952 Eval Acc: 0.8319 (LR: 0.000250)
[2025-05-12 14:49:17,675]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.3956 Train Acc: 0.8616 Eval Loss: 0.4539 Eval Acc: 0.8473 (LR: 0.000250)
[2025-05-12 14:51:47,467]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.3940 Train Acc: 0.8629 Eval Loss: 0.4726 Eval Acc: 0.8440 (LR: 0.000250)
[2025-05-12 14:54:20,851]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.3966 Train Acc: 0.8604 Eval Loss: 0.4514 Eval Acc: 0.8458 (LR: 0.000250)
[2025-05-12 14:56:22,645]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.3963 Train Acc: 0.8615 Eval Loss: 0.4844 Eval Acc: 0.8351 (LR: 0.000250)
[2025-05-12 14:58:20,671]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.3939 Train Acc: 0.8617 Eval Loss: 0.5233 Eval Acc: 0.8244 (LR: 0.000250)
[2025-05-12 15:00:21,463]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.3929 Train Acc: 0.8619 Eval Loss: 0.4882 Eval Acc: 0.8389 (LR: 0.000250)
[2025-05-12 15:02:22,334]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.3931 Train Acc: 0.8620 Eval Loss: 0.4690 Eval Acc: 0.8431 (LR: 0.000250)
[2025-05-12 15:04:23,414]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.3976 Train Acc: 0.8627 Eval Loss: 0.4627 Eval Acc: 0.8437 (LR: 0.000250)
[2025-05-12 15:06:24,065]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.3920 Train Acc: 0.8629 Eval Loss: 0.4580 Eval Acc: 0.8493 (LR: 0.000250)
[2025-05-12 15:08:24,942]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.3933 Train Acc: 0.8636 Eval Loss: 0.4706 Eval Acc: 0.8440 (LR: 0.000063)
[2025-05-12 15:10:26,631]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.3720 Train Acc: 0.8693 Eval Loss: 0.4389 Eval Acc: 0.8503 (LR: 0.000063)
[2025-05-12 15:12:27,930]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.3671 Train Acc: 0.8719 Eval Loss: 0.4311 Eval Acc: 0.8574 (LR: 0.000063)
[2025-05-12 15:14:25,644]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.3705 Train Acc: 0.8700 Eval Loss: 0.4531 Eval Acc: 0.8485 (LR: 0.000063)
[2025-05-12 15:16:23,368]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.3706 Train Acc: 0.8710 Eval Loss: 0.4333 Eval Acc: 0.8545 (LR: 0.000063)
[2025-05-12 15:18:25,136]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.3718 Train Acc: 0.8703 Eval Loss: 0.4410 Eval Acc: 0.8517 (LR: 0.000063)
[2025-05-12 15:20:23,322]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.3688 Train Acc: 0.8725 Eval Loss: 0.4419 Eval Acc: 0.8535 (LR: 0.000063)
[2025-05-12 15:22:22,820]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.3682 Train Acc: 0.8695 Eval Loss: 0.4417 Eval Acc: 0.8511 (LR: 0.000063)
[2025-05-12 15:24:22,119]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.3703 Train Acc: 0.8715 Eval Loss: 0.4495 Eval Acc: 0.8519 (LR: 0.000063)
[2025-05-12 15:26:17,509]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.3698 Train Acc: 0.8707 Eval Loss: 0.4533 Eval Acc: 0.8485 (LR: 0.000063)
[2025-05-12 15:28:11,604]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.3713 Train Acc: 0.8691 Eval Loss: 0.4445 Eval Acc: 0.8533 (LR: 0.000063)
[2025-05-12 15:30:05,622]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.3730 Train Acc: 0.8688 Eval Loss: 0.4481 Eval Acc: 0.8533 (LR: 0.000063)
[2025-05-12 15:32:01,448]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.3751 Train Acc: 0.8692 Eval Loss: 0.4675 Eval Acc: 0.8442 (LR: 0.000063)
[2025-05-12 15:33:56,548]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.3675 Train Acc: 0.8717 Eval Loss: 0.4407 Eval Acc: 0.8520 (LR: 0.000063)
[2025-05-12 15:35:53,005]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.3668 Train Acc: 0.8718 Eval Loss: 0.4414 Eval Acc: 0.8538 (LR: 0.000063)
[2025-05-12 15:37:47,108]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.3680 Train Acc: 0.8714 Eval Loss: 0.4394 Eval Acc: 0.8509 (LR: 0.000063)
[2025-05-12 15:37:47,109]: [ResNet20_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.8574
[2025-05-12 15:37:47,195]: 


Quantization of model down to 3 bits finished
[2025-05-12 15:37:47,195]: Model Architecture:
[2025-05-12 15:37:47,460]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8166], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.715993881225586)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1002], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3493863344192505, max_val=0.3521778881549835)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8201], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.74071741104126)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0898], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23544391989707947, max_val=0.3929295837879181)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8123], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.686066150665283)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0775], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.271359384059906, max_val=0.27136245369911194)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8221], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.75496244430542)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0694], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28245311975479126, max_val=0.20334410667419434)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8149], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.704010009765625)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0588], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20834970474243164, max_val=0.2030552625656128)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8217], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.751862049102783)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0540], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18900254368782043, max_val=0.18925738334655762)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8374], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.862049102783203)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0568], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2060195952653885, max_val=0.19178061187267303)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.635272026062012)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0487], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13574442267417908, max_val=0.20512278378009796)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0935], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32725873589515686, max_val=0.3272804021835327)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8239], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.767433166503906)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0433], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14665454626083374, max_val=0.15650467574596405)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8132], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.692488670349121)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0442], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15468737483024597, max_val=0.1548219919204712)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8242], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.76957893371582)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0401], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13448382914066315, max_val=0.14632399380207062)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.634945392608643)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0441], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14763011038303375, max_val=0.1609719693660736)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8526], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.967947483062744)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13817207515239716, max_val=0.1313067376613617)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8068], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.6479172706604)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0329], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11630412191152573, max_val=0.11422483623027802)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0644], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22543518245220184, max_val=0.22543326020240784)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8273], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.79125452041626)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0330], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11456526815891266, max_val=0.11610856652259827)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8218], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.752411365509033)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0281], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09239158034324646, max_val=0.10450371354818344)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8210], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7471208572387695)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08933304995298386, max_val=0.08970119059085846)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8216], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.751549243927002)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0213], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06892803311347961, max_val=0.07986404746770859)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8842], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.189070224761963)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 15:37:47,460]: 
Model Weights:
[2025-05-12 15:37:47,460]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 15:37:47,461]: Sample Values (25 elements): [-0.2638068199157715, 0.20743155479431152, 0.06450081616640091, 0.26780518889427185, 0.09206843376159668, -0.3732471466064453, 0.13258185982704163, -0.10174702107906342, 0.0958075225353241, -0.06012062728404999, -0.12796224653720856, 0.10484693199396133, 0.05298681929707527, 0.05269641801714897, 0.1296544373035431, 0.10092711448669434, -0.015563765540719032, -0.12131277471780777, -0.18113264441490173, -0.19796662032604218, -0.005383198615163565, -0.2675235867500305, -0.09556282311677933, 0.11590681970119476, 0.032418034970760345]
[2025-05-12 15:37:47,461]: Mean: -0.00099953
[2025-05-12 15:37:47,462]: Min: -0.53635484
[2025-05-12 15:37:47,462]: Max: 0.55956739
[2025-05-12 15:37:47,462]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,463]: Sample Values (16 elements): [1.2920080423355103, 1.6370400190353394, 1.170393943786621, 1.1000899076461792, 1.038195013999939, 1.1315665245056152, 1.0393962860107422, 0.9289221167564392, 1.4629422426223755, 0.942510724067688, 1.2148422002792358, 0.9670217633247375, 0.9396417140960693, 1.1052824258804321, 1.5585532188415527, 1.0909994840621948]
[2025-05-12 15:37:47,463]: Mean: 1.16371274
[2025-05-12 15:37:47,464]: Min: 0.92892212
[2025-05-12 15:37:47,464]: Max: 1.63704002
[2025-05-12 15:37:47,467]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 15:37:47,468]: Sample Values (25 elements): [0.0, 0.0, -0.10022345930337906, 0.10022345930337906, 0.10022345930337906, 0.0, 0.0, 0.10022345930337906, 0.10022345930337906, 0.0, -0.10022345930337906, -0.10022345930337906, 0.0, -0.10022345930337906, 0.0, 0.0, 0.0, 0.0, -0.10022345930337906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 15:37:47,468]: Mean: -0.00487197
[2025-05-12 15:37:47,468]: Min: -0.30067039
[2025-05-12 15:37:47,470]: Max: 0.40089384
[2025-05-12 15:37:47,470]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,470]: Sample Values (16 elements): [1.2720133066177368, 1.2037692070007324, 0.9350633025169373, 1.0197827816009521, 0.9873940944671631, 1.144310712814331, 0.8669936060905457, 1.1537195444107056, 0.9011162519454956, 0.9627620577812195, 1.2327638864517212, 1.017137885093689, 0.9415600299835205, 1.1208066940307617, 1.0693228244781494, 1.0156382322311401]
[2025-05-12 15:37:47,471]: Mean: 1.05275965
[2025-05-12 15:37:47,471]: Min: 0.86699361
[2025-05-12 15:37:47,471]: Max: 1.27201331
[2025-05-12 15:37:47,475]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 15:37:47,476]: Sample Values (25 elements): [0.17953529953956604, -0.08976764976978302, 0.0, 0.08976764976978302, -0.08976764976978302, 0.0, 0.0, 0.08976764976978302, -0.08976764976978302, 0.0, 0.0, 0.0, -0.08976764976978302, 0.17953529953956604, -0.08976764976978302, 0.0, -0.08976764976978302, 0.0, 0.0, -0.08976764976978302, -0.08976764976978302, 0.0, -0.08976764976978302, 0.08976764976978302, -0.08976764976978302]
[2025-05-12 15:37:47,476]: Mean: -0.00588321
[2025-05-12 15:37:47,476]: Min: -0.26930296
[2025-05-12 15:37:47,477]: Max: 0.35907060
[2025-05-12 15:37:47,477]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,478]: Sample Values (16 elements): [1.0861661434173584, 1.0151777267456055, 1.0671635866165161, 1.0639876127243042, 1.0357791185379028, 1.1437528133392334, 1.020377278327942, 0.8557873368263245, 1.0443941354751587, 0.8944118618965149, 1.095664381980896, 1.0285784006118774, 1.0285942554473877, 0.8939017057418823, 1.0592005252838135, 1.0251619815826416]
[2025-05-12 15:37:47,478]: Mean: 1.02238119
[2025-05-12 15:37:47,479]: Min: 0.85578734
[2025-05-12 15:37:47,480]: Max: 1.14375281
[2025-05-12 15:37:47,482]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 15:37:47,484]: Sample Values (25 elements): [0.0, 0.07753171771764755, 0.07753171771764755, -0.1550634354352951, 0.1550634354352951, 0.07753171771764755, 0.0, -0.07753171771764755, 0.07753171771764755, 0.07753171771764755, -0.07753171771764755, 0.0, -0.07753171771764755, 0.0, 0.0, 0.0, -0.07753171771764755, 0.0, 0.0, 0.07753171771764755, -0.07753171771764755, 0.0, 0.0, 0.07753171771764755, 0.0]
[2025-05-12 15:37:47,485]: Mean: -0.00164889
[2025-05-12 15:37:47,485]: Min: -0.23259515
[2025-05-12 15:37:47,486]: Max: 0.31012687
[2025-05-12 15:37:47,486]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,486]: Sample Values (16 elements): [0.8967587351799011, 1.2984689474105835, 1.0090988874435425, 1.0035618543624878, 0.9823416471481323, 0.9685232043266296, 0.9949356913566589, 1.099430799484253, 0.9423543214797974, 1.0915330648422241, 0.9909477233886719, 0.9263960123062134, 0.9845300912857056, 1.19163179397583, 0.9493815898895264, 1.2396793365478516]
[2025-05-12 15:37:47,487]: Mean: 1.03559828
[2025-05-12 15:37:47,487]: Min: 0.89675874
[2025-05-12 15:37:47,488]: Max: 1.29846895
[2025-05-12 15:37:47,491]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 15:37:47,492]: Sample Values (25 elements): [0.0, -0.06939958781003952, 0.0, 0.0, 0.0, 0.0, 0.06939958781003952, 0.06939958781003952, 0.06939958781003952, 0.0, 0.06939958781003952, 0.06939958781003952, 0.06939958781003952, -0.06939958781003952, -0.06939958781003952, 0.0, -0.06939958781003952, 0.0, 0.06939958781003952, 0.06939958781003952, -0.06939958781003952, 0.06939958781003952, -0.06939958781003952, 0.06939958781003952, 0.0]
[2025-05-12 15:37:47,492]: Mean: -0.00213862
[2025-05-12 15:37:47,493]: Min: -0.27759835
[2025-05-12 15:37:47,493]: Max: 0.20819876
[2025-05-12 15:37:47,493]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,493]: Sample Values (16 elements): [1.0076125860214233, 0.9888907670974731, 1.0306771993637085, 0.9800527095794678, 1.0195661783218384, 0.973688006401062, 1.0592797994613647, 0.9979792833328247, 0.8966851234436035, 1.0830365419387817, 0.9870457649230957, 1.1429471969604492, 0.7867848873138428, 1.001301646232605, 0.9053979516029358, 0.9760145545005798]
[2025-05-12 15:37:47,494]: Mean: 0.98980999
[2025-05-12 15:37:47,494]: Min: 0.78678489
[2025-05-12 15:37:47,495]: Max: 1.14294720
[2025-05-12 15:37:47,497]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 15:37:47,498]: Sample Values (25 elements): [-0.0587722584605217, 0.1175445169210434, 0.0587722584605217, 0.0587722584605217, 0.0587722584605217, -0.0587722584605217, 0.0587722584605217, 0.0587722584605217, 0.0, 0.0, 0.0587722584605217, -0.0587722584605217, -0.0587722584605217, -0.1175445169210434, 0.1175445169210434, 0.0587722584605217, 0.1175445169210434, 0.1763167679309845, 0.0587722584605217, -0.0587722584605217, 0.0, -0.1763167679309845, 0.0587722584605217, 0.0, 0.0587722584605217]
[2025-05-12 15:37:47,498]: Mean: -0.00283148
[2025-05-12 15:37:47,499]: Min: -0.23508903
[2025-05-12 15:37:47,499]: Max: 0.17631677
[2025-05-12 15:37:47,499]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,499]: Sample Values (16 elements): [0.9805454015731812, 0.9741239547729492, 1.041306734085083, 0.9884704351425171, 0.993480384349823, 0.9950573444366455, 1.0219660997390747, 0.948688805103302, 1.0040256977081299, 0.9991341233253479, 1.0651968717575073, 0.9765064716339111, 0.9422330260276794, 1.0253958702087402, 0.9843081831932068, 1.0017114877700806]
[2025-05-12 15:37:47,500]: Mean: 0.99638438
[2025-05-12 15:37:47,500]: Min: 0.94223303
[2025-05-12 15:37:47,500]: Max: 1.06519687
[2025-05-12 15:37:47,513]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 15:37:47,520]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.05403716489672661, 0.0, -0.10807432979345322, 0.0, 0.05403716489672661, -0.05403716489672661, 0.10807432979345322, 0.0, 0.0, -0.05403716489672661, 0.0, -0.10807432979345322, 0.05403716489672661, 0.10807432979345322, 0.0, -0.05403716489672661, 0.0, 0.10807432979345322, -0.05403716489672661, -0.10807432979345322, 0.05403716489672661, 0.0]
[2025-05-12 15:37:47,535]: Mean: -0.00056289
[2025-05-12 15:37:47,539]: Min: -0.16211149
[2025-05-12 15:37:47,539]: Max: 0.21614866
[2025-05-12 15:37:47,539]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 15:37:47,540]: Sample Values (16 elements): [0.9993638396263123, 0.9526656866073608, 0.9861170053482056, 0.9464425444602966, 1.053223729133606, 0.9968733787536621, 1.082495093345642, 0.9116324186325073, 0.949835479259491, 0.9175382256507874, 1.0339986085891724, 0.9687334299087524, 0.9446701407432556, 0.9036257863044739, 0.98216712474823, 0.8316975235939026]
[2025-05-12 15:37:47,540]: Mean: 0.96631753
[2025-05-12 15:37:47,540]: Min: 0.83169752
[2025-05-12 15:37:47,541]: Max: 1.08249509
[2025-05-12 15:37:47,543]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 15:37:47,544]: Sample Values (25 elements): [-0.056828491389751434, -0.056828491389751434, 0.0, -0.056828491389751434, 0.0, 0.0, 0.11365698277950287, 0.0, 0.0, 0.056828491389751434, -0.056828491389751434, 0.0, 0.0, -0.11365698277950287, 0.0, -0.056828491389751434, 0.0, -0.056828491389751434, 0.0, 0.11365698277950287, 0.056828491389751434, 0.0, 0.056828491389751434, -0.056828491389751434, 0.0]
[2025-05-12 15:37:47,545]: Mean: -0.00067829
[2025-05-12 15:37:47,545]: Min: -0.22731397
[2025-05-12 15:37:47,545]: Max: 0.17048547
[2025-05-12 15:37:47,545]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,546]: Sample Values (25 elements): [1.036644697189331, 0.954944908618927, 0.983401358127594, 0.985097348690033, 1.0131855010986328, 0.9985123872756958, 1.0133723020553589, 1.0072294473648071, 1.07522714138031, 0.9727967977523804, 0.9835671782493591, 0.9947425127029419, 1.0143673419952393, 1.0149143934249878, 0.9607133269309998, 0.9784800410270691, 1.014838457107544, 1.0607235431671143, 0.9660093784332275, 1.0234696865081787, 0.9733814597129822, 0.9913085699081421, 1.0007293224334717, 1.0036967992782593, 0.9665836691856384]
[2025-05-12 15:37:47,547]: Mean: 1.00088882
[2025-05-12 15:37:47,548]: Min: 0.95494491
[2025-05-12 15:37:47,548]: Max: 1.07522714
[2025-05-12 15:37:47,550]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 15:37:47,551]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.04869534820318222, 0.0, 0.0, 0.0, 0.0, 0.09739069640636444, 0.0, 0.0, 0.04869534820318222, -0.04869534820318222, -0.04869534820318222, -0.09739069640636444, 0.0, 0.04869534820318222, -0.04869534820318222, -0.04869534820318222, 0.04869534820318222, -0.04869534820318222, 0.0, -0.04869534820318222]
[2025-05-12 15:37:47,551]: Mean: -0.00095636
[2025-05-12 15:37:47,551]: Min: -0.14608604
[2025-05-12 15:37:47,552]: Max: 0.19478139
[2025-05-12 15:37:47,552]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,553]: Sample Values (25 elements): [0.9619215726852417, 1.0593959093093872, 0.9829353094100952, 1.0937684774398804, 0.9741498231887817, 0.9371316432952881, 1.0276926755905151, 1.0447386503219604, 0.9715136289596558, 0.9304310083389282, 1.000620722770691, 0.9547944068908691, 0.9434540867805481, 1.046902060508728, 1.0506831407546997, 0.9760175943374634, 0.94098961353302, 1.0460166931152344, 1.016851544380188, 1.016795039176941, 1.0278050899505615, 0.9898874759674072, 1.0636615753173828, 1.0229495763778687, 1.0035260915756226]
[2025-05-12 15:37:47,554]: Mean: 1.00416529
[2025-05-12 15:37:47,554]: Min: 0.93043101
[2025-05-12 15:37:47,555]: Max: 1.09376848
[2025-05-12 15:37:47,557]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 15:37:47,558]: Sample Values (25 elements): [0.09350559860467911, 0.0, -0.18701119720935822, 0.0, 0.0, 0.0, -0.18701119720935822, 0.0, 0.09350559860467911, -0.09350559860467911, 0.2805168032646179, 0.0, -0.09350559860467911, -0.18701119720935822, 0.0, 0.09350559860467911, 0.09350559860467911, -0.18701119720935822, -0.09350559860467911, -0.09350559860467911, -0.18701119720935822, 0.09350559860467911, -0.18701119720935822, 0.18701119720935822, -0.18701119720935822]
[2025-05-12 15:37:47,559]: Mean: 0.00073051
[2025-05-12 15:37:47,559]: Min: -0.28051680
[2025-05-12 15:37:47,560]: Max: 0.37402239
[2025-05-12 15:37:47,560]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,561]: Sample Values (25 elements): [0.9031987190246582, 0.9131649732589722, 0.8874381184577942, 0.9367380738258362, 0.8932977914810181, 0.80858314037323, 0.858893096446991, 0.9135912656784058, 0.8141263723373413, 0.8829823732376099, 0.8577407002449036, 0.9144781827926636, 0.8547928333282471, 0.8172022700309753, 0.9047477841377258, 0.8967758417129517, 0.8681601285934448, 0.9169871807098389, 0.8959901332855225, 0.8805334568023682, 0.8364111185073853, 0.9001729488372803, 0.8663517832756042, 0.8699503540992737, 0.9331689476966858]
[2025-05-12 15:37:47,561]: Mean: 0.88179255
[2025-05-12 15:37:47,561]: Min: 0.80858314
[2025-05-12 15:37:47,561]: Max: 0.97778183
[2025-05-12 15:37:47,564]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 15:37:47,565]: Sample Values (25 elements): [0.0, -0.04330836981534958, 0.04330836981534958, 0.08661673963069916, 0.0, 0.04330836981534958, 0.04330836981534958, -0.12992510199546814, 0.04330836981534958, 0.0, 0.0, 0.04330836981534958, 0.0, 0.0, -0.08661673963069916, -0.08661673963069916, 0.04330836981534958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12992510199546814, 0.04330836981534958, 0.04330836981534958]
[2025-05-12 15:37:47,566]: Mean: -0.00234023
[2025-05-12 15:37:47,566]: Min: -0.12992510
[2025-05-12 15:37:47,566]: Max: 0.17323348
[2025-05-12 15:37:47,566]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,567]: Sample Values (25 elements): [0.9961369037628174, 0.9561766982078552, 0.9803024530410767, 0.9758467078208923, 1.059395432472229, 0.9501697421073914, 0.9782465100288391, 1.0286005735397339, 0.9758560657501221, 1.0198527574539185, 1.0042670965194702, 0.9378969073295593, 1.0378060340881348, 1.0202525854110718, 1.0135414600372314, 0.9420046210289001, 0.9639090895652771, 0.9934621453285217, 0.9859621524810791, 0.9937886595726013, 0.9853227734565735, 0.9689416885375977, 0.9967299699783325, 0.9677567481994629, 1.0334504842758179]
[2025-05-12 15:37:47,568]: Mean: 0.99170458
[2025-05-12 15:37:47,568]: Min: 0.93789691
[2025-05-12 15:37:47,568]: Max: 1.05939543
[2025-05-12 15:37:47,570]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 15:37:47,571]: Sample Values (25 elements): [0.04421572387218475, -0.04421572387218475, -0.04421572387218475, 0.04421572387218475, 0.0, 0.0, 0.0, 0.04421572387218475, 0.0, -0.04421572387218475, 0.0, -0.04421572387218475, -0.04421572387218475, 0.0884314477443695, -0.04421572387218475, 0.0, -0.04421572387218475, 0.04421572387218475, -0.04421572387218475, 0.0, 0.0, -0.04421572387218475, 0.0, 0.04421572387218475, 0.04421572387218475]
[2025-05-12 15:37:47,571]: Mean: -0.00144411
[2025-05-12 15:37:47,573]: Min: -0.13264717
[2025-05-12 15:37:47,573]: Max: 0.17686290
[2025-05-12 15:37:47,573]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,573]: Sample Values (25 elements): [0.9779106378555298, 1.0179779529571533, 1.0186691284179688, 0.9563131928443909, 0.9736137986183167, 0.9873543977737427, 0.9975523948669434, 0.93031245470047, 1.0269032716751099, 0.9818768501281738, 1.0125601291656494, 0.9538750648498535, 0.9696910977363586, 0.990829586982727, 1.0101901292800903, 0.9733176231384277, 0.9474047422409058, 1.006393313407898, 0.9848304390907288, 1.0331535339355469, 0.9415106773376465, 0.9574875831604004, 0.9852924942970276, 0.9148452877998352, 1.0308091640472412]
[2025-05-12 15:37:47,574]: Mean: 0.97988915
[2025-05-12 15:37:47,574]: Min: 0.91484529
[2025-05-12 15:37:47,574]: Max: 1.03315353
[2025-05-12 15:37:47,576]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 15:37:47,577]: Sample Values (25 elements): [0.0, 0.04011540114879608, -0.04011540114879608, 0.04011540114879608, 0.0, 0.0, 0.04011540114879608, 0.0, 0.08023080229759216, -0.12034620344638824, 0.0, 0.04011540114879608, -0.04011540114879608, 0.04011540114879608, 0.0, 0.0, 0.0, -0.08023080229759216, -0.04011540114879608, 0.08023080229759216, -0.04011540114879608, -0.08023080229759216, 0.0, 0.0, 0.04011540114879608]
[2025-05-12 15:37:47,577]: Mean: -0.00106644
[2025-05-12 15:37:47,577]: Min: -0.12034620
[2025-05-12 15:37:47,578]: Max: 0.16046160
[2025-05-12 15:37:47,578]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,578]: Sample Values (25 elements): [0.9273548722267151, 0.9959567189216614, 1.0229518413543701, 0.972945511341095, 0.9860804080963135, 0.9431647062301636, 0.970477819442749, 0.9703710079193115, 0.968450129032135, 0.9739657640457153, 0.9638881087303162, 0.977250874042511, 0.9992737174034119, 0.9854152798652649, 1.011717438697815, 0.9842143654823303, 0.982216477394104, 0.9645286202430725, 0.9744752645492554, 0.9732361435890198, 1.0013819932937622, 1.0326018333435059, 0.9666182994842529, 0.9810771346092224, 0.9559561014175415]
[2025-05-12 15:37:47,579]: Mean: 0.98225439
[2025-05-12 15:37:47,579]: Min: 0.92735487
[2025-05-12 15:37:47,579]: Max: 1.06584156
[2025-05-12 15:37:47,581]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 15:37:47,582]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.04408596456050873, 0.0, 0.0, 0.04408596456050873, 0.0, -0.08817192912101746, 0.0, 0.04408596456050873, 0.04408596456050873, 0.0, 0.04408596456050873, 0.04408596456050873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04408596456050873, -0.04408596456050873, -0.04408596456050873, 0.04408596456050873, -0.08817192912101746]
[2025-05-12 15:37:47,582]: Mean: 0.00038747
[2025-05-12 15:37:47,582]: Min: -0.13225789
[2025-05-12 15:37:47,582]: Max: 0.17634386
[2025-05-12 15:37:47,583]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 15:37:47,583]: Sample Values (25 elements): [1.0242067575454712, 1.0151593685150146, 1.0009263753890991, 1.0308903455734253, 0.9799395203590393, 0.9782537817955017, 0.9505046010017395, 1.0041488409042358, 1.03727388381958, 0.9885813593864441, 0.9701855778694153, 1.0033317804336548, 1.031627893447876, 1.0121010541915894, 1.069116473197937, 1.005616307258606, 1.0371426343917847, 0.9786327481269836, 1.0446829795837402, 1.0084365606307983, 0.9585539102554321, 1.026260256767273, 0.9859760999679565, 1.0070399045944214, 1.0360082387924194]
[2025-05-12 15:37:47,583]: Mean: 1.00607145
[2025-05-12 15:37:47,584]: Min: 0.95050460
[2025-05-12 15:37:47,584]: Max: 1.06911647
[2025-05-12 15:37:47,587]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 15:37:47,590]: Sample Values (25 elements): [0.0, -0.03849710151553154, -0.03849710151553154, 0.03849710151553154, -0.03849710151553154, 0.07699420303106308, 0.0, 0.0, 0.03849710151553154, -0.03849710151553154, 0.0, 0.03849710151553154, 0.0, 0.0, 0.03849710151553154, -0.03849710151553154, 0.0, 0.0, -0.03849710151553154, 0.07699420303106308, -0.03849710151553154, 0.03849710151553154, 0.0, -0.03849710151553154, -0.03849710151553154]
[2025-05-12 15:37:47,593]: Mean: -0.00065791
[2025-05-12 15:37:47,597]: Min: -0.15398841
[2025-05-12 15:37:47,599]: Max: 0.11549130
[2025-05-12 15:37:47,599]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,623]: Sample Values (25 elements): [0.9888948202133179, 1.0096611976623535, 0.9938482046127319, 0.9490606784820557, 1.0160948038101196, 0.9712345600128174, 0.9832504987716675, 0.9833589792251587, 0.9520354866981506, 0.987879753112793, 0.9815935492515564, 1.0389009714126587, 0.9979912042617798, 0.9740087389945984, 1.0064009428024292, 0.972179114818573, 0.9777234792709351, 0.9620066285133362, 0.989149272441864, 0.9847553968429565, 0.9923526644706726, 1.010291337966919, 1.0143134593963623, 1.020096778869629, 0.9399914741516113]
[2025-05-12 15:37:47,624]: Mean: 0.98370034
[2025-05-12 15:37:47,624]: Min: 0.93999147
[2025-05-12 15:37:47,624]: Max: 1.03937292
[2025-05-12 15:37:47,626]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 15:37:47,633]: Sample Values (25 elements): [0.0, -0.03293260559439659, 0.06586521118879318, 0.0, 0.0, -0.03293260559439659, -0.03293260559439659, -0.03293260559439659, 0.03293260559439659, 0.0, 0.03293260559439659, 0.03293260559439659, 0.0, 0.0, 0.03293260559439659, 0.0, -0.06586521118879318, 0.03293260559439659, 0.0, 0.0, 0.0, 0.03293260559439659, -0.03293260559439659, 0.0, 0.03293260559439659]
[2025-05-12 15:37:47,638]: Mean: -0.00042345
[2025-05-12 15:37:47,645]: Min: -0.13173042
[2025-05-12 15:37:47,654]: Max: 0.09879781
[2025-05-12 15:37:47,654]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,665]: Sample Values (25 elements): [1.0304573774337769, 1.0143206119537354, 1.0228346586227417, 0.9898967146873474, 1.064620852470398, 1.0246670246124268, 1.0160683393478394, 0.9888923764228821, 0.9943130016326904, 1.0402151346206665, 0.9989365339279175, 0.992302656173706, 1.0118545293807983, 0.9401827454566956, 1.0308297872543335, 0.9745063781738281, 1.0245485305786133, 1.0832771062850952, 1.0205614566802979, 1.0041576623916626, 1.016563057899475, 1.0160049200057983, 1.020260214805603, 0.9742289185523987, 1.0169113874435425]
[2025-05-12 15:37:47,665]: Mean: 1.01589584
[2025-05-12 15:37:47,666]: Min: 0.94018275
[2025-05-12 15:37:47,667]: Max: 1.08327711
[2025-05-12 15:37:47,670]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 15:37:47,671]: Sample Values (25 elements): [-0.0644097849726677, 0.0, -0.0644097849726677, -0.0644097849726677, 0.1288195699453354, 0.0, 0.1288195699453354, -0.1288195699453354, 0.0644097849726677, 0.0644097849726677, -0.0644097849726677, -0.1288195699453354, 0.0644097849726677, -0.0644097849726677, 0.0644097849726677, 0.0, -0.19322934746742249, 0.1288195699453354, -0.0644097849726677, -0.0644097849726677, 0.0644097849726677, 0.0, 0.1288195699453354, 0.1288195699453354, 0.0644097849726677]
[2025-05-12 15:37:47,672]: Mean: 0.00160395
[2025-05-12 15:37:47,674]: Min: -0.25763914
[2025-05-12 15:37:47,675]: Max: 0.19322935
[2025-05-12 15:37:47,675]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,675]: Sample Values (25 elements): [0.9030806422233582, 0.9056174755096436, 0.9131665825843811, 0.9030277729034424, 0.8972798585891724, 0.8863517045974731, 0.9166543483734131, 0.890480637550354, 0.9020920991897583, 0.9169573187828064, 0.9299681186676025, 0.9053683876991272, 0.8917368650436401, 0.9299782514572144, 0.9358348250389099, 0.8761298656463623, 0.9246127605438232, 0.9379488825798035, 0.8747245669364929, 0.8948462605476379, 0.8985170722007751, 0.8971682786941528, 0.9143072366714478, 0.902866542339325, 0.9128583669662476]
[2025-05-12 15:37:47,676]: Mean: 0.90439850
[2025-05-12 15:37:47,676]: Min: 0.87212062
[2025-05-12 15:37:47,677]: Max: 0.96519518
[2025-05-12 15:37:47,681]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 15:37:47,683]: Sample Values (25 elements): [0.0, 0.03295334056019783, -0.03295334056019783, -0.03295334056019783, 0.0, 0.0, 0.0, 0.03295334056019783, 0.0, 0.03295334056019783, 0.03295334056019783, 0.03295334056019783, 0.03295334056019783, 0.0, 0.0, -0.03295334056019783, 0.03295334056019783, -0.06590668112039566, 0.0, 0.06590668112039566, -0.03295334056019783, -0.03295334056019783, 0.03295334056019783, 0.03295334056019783, 0.06590668112039566]
[2025-05-12 15:37:47,684]: Mean: -0.00109684
[2025-05-12 15:37:47,684]: Min: -0.09886003
[2025-05-12 15:37:47,685]: Max: 0.13181336
[2025-05-12 15:37:47,685]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,687]: Sample Values (25 elements): [0.9795472621917725, 0.9311535954475403, 0.970668375492096, 1.0145316123962402, 1.0196771621704102, 0.9515367746353149, 0.9666872620582581, 0.9718835353851318, 0.9882630109786987, 0.9803563952445984, 0.9789218902587891, 0.9510428309440613, 0.9900934100151062, 0.9869242906570435, 1.0084776878356934, 0.9747837781906128, 0.9456324577331543, 0.9866385459899902, 0.9923485517501831, 0.9973430037498474, 0.9956023097038269, 1.0043456554412842, 0.9656176567077637, 0.9965257048606873, 0.9484072923660278]
[2025-05-12 15:37:47,687]: Mean: 0.98131585
[2025-05-12 15:37:47,687]: Min: 0.93115360
[2025-05-12 15:37:47,689]: Max: 1.02016330
[2025-05-12 15:37:47,693]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 15:37:47,694]: Sample Values (25 elements): [-0.028127873316407204, 0.0, 0.0, -0.028127873316407204, -0.028127873316407204, 0.028127873316407204, 0.05625574663281441, -0.028127873316407204, 0.0, 0.0, -0.028127873316407204, 0.028127873316407204, -0.028127873316407204, 0.028127873316407204, 0.0, -0.028127873316407204, -0.028127873316407204, -0.028127873316407204, 0.028127873316407204, 0.028127873316407204, 0.028127873316407204, -0.028127873316407204, -0.05625574663281441, 0.0, 0.0]
[2025-05-12 15:37:47,694]: Mean: 0.00027316
[2025-05-12 15:37:47,695]: Min: -0.08438362
[2025-05-12 15:37:47,696]: Max: 0.11251149
[2025-05-12 15:37:47,696]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,697]: Sample Values (25 elements): [1.0312893390655518, 1.005465030670166, 1.0339839458465576, 1.035772442817688, 1.0215461254119873, 1.0353350639343262, 1.0350793600082397, 1.0245312452316284, 1.0392639636993408, 1.0256184339523315, 1.0164319276809692, 1.0282868146896362, 1.0390039682388306, 1.0266162157058716, 1.017965316772461, 1.007146954536438, 1.0113849639892578, 1.019052505493164, 1.0411211252212524, 1.0372909307479858, 1.0363408327102661, 1.027340054512024, 1.0238271951675415, 0.999653160572052, 0.9908238649368286]
[2025-05-12 15:37:47,698]: Mean: 1.02521086
[2025-05-12 15:37:47,698]: Min: 0.99082386
[2025-05-12 15:37:47,699]: Max: 1.08765602
[2025-05-12 15:37:47,703]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 15:37:47,704]: Sample Values (25 elements): [0.025576354935765266, -0.05115270987153053, -0.025576354935765266, 0.025576354935765266, 0.025576354935765266, -0.025576354935765266, 0.0, 0.025576354935765266, -0.025576354935765266, -0.025576354935765266, 0.025576354935765266, 0.0, 0.025576354935765266, -0.025576354935765266, -0.025576354935765266, 0.025576354935765266, -0.025576354935765266, -0.025576354935765266, 0.025576354935765266, 0.0, 0.0, -0.025576354935765266, 0.025576354935765266, 0.0, 0.05115270987153053]
[2025-05-12 15:37:47,704]: Mean: -0.00012835
[2025-05-12 15:37:47,705]: Min: -0.07672907
[2025-05-12 15:37:47,706]: Max: 0.10230542
[2025-05-12 15:37:47,706]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,707]: Sample Values (25 elements): [0.9510053396224976, 0.9742311835289001, 0.961310863494873, 0.9587588906288147, 0.9749423265457153, 0.9642938375473022, 0.9452394247055054, 0.9644472599029541, 0.9978653788566589, 0.967117428779602, 0.9563456177711487, 0.9728864431381226, 0.9595516324043274, 0.9707134366035461, 0.9688867330551147, 0.9637877941131592, 0.9858174920082092, 0.9629213809967041, 0.9640050530433655, 0.950075626373291, 0.9518721103668213, 0.9599781632423401, 0.9611716270446777, 0.954735279083252, 0.9580100774765015]
[2025-05-12 15:37:47,707]: Mean: 0.96318781
[2025-05-12 15:37:47,708]: Min: 0.93913269
[2025-05-12 15:37:47,708]: Max: 1.00280726
[2025-05-12 15:37:47,712]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 15:37:47,715]: Sample Values (25 elements): [-0.021256037056446075, 0.021256037056446075, -0.021256037056446075, 0.021256037056446075, 0.021256037056446075, 0.0, 0.021256037056446075, -0.021256037056446075, 0.021256037056446075, -0.04251207411289215, 0.021256037056446075, 0.021256037056446075, -0.04251207411289215, 0.0, 0.021256037056446075, 0.0, 0.021256037056446075, -0.04251207411289215, 0.021256037056446075, 0.021256037056446075, -0.021256037056446075, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 15:37:47,715]: Mean: 0.00069308
[2025-05-12 15:37:47,717]: Min: -0.06376811
[2025-05-12 15:37:47,722]: Max: 0.08502415
[2025-05-12 15:37:47,722]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 15:37:47,737]: Sample Values (25 elements): [1.0946428775787354, 1.0608898401260376, 1.0186009407043457, 1.0659143924713135, 1.0386643409729004, 1.0428192615509033, 1.0662521123886108, 1.0286451578140259, 1.043431043624878, 1.0526268482208252, 1.0537720918655396, 1.0701371431350708, 1.0791593790054321, 1.06985604763031, 1.0583035945892334, 1.0653592348098755, 1.0431833267211914, 1.086227297782898, 1.0757781267166138, 1.0508034229278564, 1.0737042427062988, 1.0587409734725952, 1.074212670326233, 1.089282751083374, 1.0787293910980225]
[2025-05-12 15:37:47,745]: Mean: 1.06239963
[2025-05-12 15:37:47,748]: Min: 1.01860094
[2025-05-12 15:37:47,749]: Max: 1.13250577
[2025-05-12 15:37:47,749]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 15:37:47,751]: Sample Values (25 elements): [-0.3027738630771637, 0.16826561093330383, 0.18925075232982635, -0.280227392911911, -0.32029232382774353, 0.04072040691971779, -0.13610611855983734, 0.25968438386917114, -0.16856157779693604, -0.10127950459718704, -0.13987557590007782, 0.1274988353252411, 0.2795329689979553, -0.12251872569322586, 0.1966787725687027, 0.5055265426635742, -0.1151835098862648, 0.0531366765499115, -0.15561355650424957, -0.15302489697933197, -0.15013714134693146, 0.062308117747306824, -0.3138430118560791, 0.10673226416110992, 0.4116440713405609]
[2025-05-12 15:37:47,751]: Mean: 0.00018501
[2025-05-12 15:37:47,752]: Min: -0.35731870
[2025-05-12 15:37:47,752]: Max: 0.55943298
[2025-05-12 15:37:47,752]: 


QAT of ResNet20 with parametrized_relu down to 2 bits...
[2025-05-12 15:37:48,278]: [ResNet20_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-05-12 15:37:48,509]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 15:39:42,913]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 1.4869 Train Acc: 0.4760 Eval Loss: 1.2542 Eval Acc: 0.5599 (LR: 0.001000)
[2025-05-12 15:41:37,267]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 1.1508 Train Acc: 0.5879 Eval Loss: 1.1610 Eval Acc: 0.5993 (LR: 0.001000)
[2025-05-12 15:43:31,215]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.0573 Train Acc: 0.6252 Eval Loss: 1.0742 Eval Acc: 0.6212 (LR: 0.001000)
[2025-05-12 15:45:25,327]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.0037 Train Acc: 0.6445 Eval Loss: 1.2220 Eval Acc: 0.5892 (LR: 0.001000)
[2025-05-12 15:47:19,084]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 0.9633 Train Acc: 0.6568 Eval Loss: 1.0540 Eval Acc: 0.6375 (LR: 0.001000)
[2025-05-12 15:49:18,049]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 0.9364 Train Acc: 0.6680 Eval Loss: 0.9953 Eval Acc: 0.6522 (LR: 0.001000)
[2025-05-12 15:51:20,657]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 0.9223 Train Acc: 0.6737 Eval Loss: 1.0617 Eval Acc: 0.6360 (LR: 0.001000)
[2025-05-12 15:53:24,187]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 0.8964 Train Acc: 0.6847 Eval Loss: 1.1059 Eval Acc: 0.6254 (LR: 0.001000)
[2025-05-12 15:55:28,532]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 0.8854 Train Acc: 0.6896 Eval Loss: 1.0006 Eval Acc: 0.6615 (LR: 0.001000)
[2025-05-12 15:57:34,011]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 0.8739 Train Acc: 0.6919 Eval Loss: 0.9165 Eval Acc: 0.6851 (LR: 0.001000)
[2025-05-12 15:59:42,779]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 0.8618 Train Acc: 0.6989 Eval Loss: 1.2458 Eval Acc: 0.5960 (LR: 0.001000)
[2025-05-12 16:01:48,959]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 0.8490 Train Acc: 0.6999 Eval Loss: 0.8809 Eval Acc: 0.6966 (LR: 0.001000)
[2025-05-12 16:03:56,410]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 0.8419 Train Acc: 0.7024 Eval Loss: 0.9714 Eval Acc: 0.6657 (LR: 0.001000)
[2025-05-12 16:05:59,818]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 0.8250 Train Acc: 0.7112 Eval Loss: 0.8363 Eval Acc: 0.7099 (LR: 0.001000)
[2025-05-12 16:08:03,691]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 0.8148 Train Acc: 0.7126 Eval Loss: 0.8921 Eval Acc: 0.6952 (LR: 0.001000)
[2025-05-12 16:10:08,493]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 0.8127 Train Acc: 0.7146 Eval Loss: 0.9789 Eval Acc: 0.6775 (LR: 0.001000)
[2025-05-12 16:12:11,956]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 0.8096 Train Acc: 0.7144 Eval Loss: 0.8460 Eval Acc: 0.7100 (LR: 0.001000)
[2025-05-12 16:14:14,767]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 0.8010 Train Acc: 0.7201 Eval Loss: 1.0822 Eval Acc: 0.6493 (LR: 0.001000)
[2025-05-12 16:16:16,479]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 0.7980 Train Acc: 0.7183 Eval Loss: 1.0326 Eval Acc: 0.6639 (LR: 0.001000)
[2025-05-12 16:18:11,167]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 0.7933 Train Acc: 0.7209 Eval Loss: 1.1506 Eval Acc: 0.6444 (LR: 0.001000)
[2025-05-12 16:20:05,292]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 0.7893 Train Acc: 0.7227 Eval Loss: 0.9514 Eval Acc: 0.6857 (LR: 0.001000)
[2025-05-12 16:21:59,809]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 0.7780 Train Acc: 0.7249 Eval Loss: 0.8783 Eval Acc: 0.6995 (LR: 0.001000)
[2025-05-12 16:23:55,134]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 0.7779 Train Acc: 0.7273 Eval Loss: 0.8531 Eval Acc: 0.7087 (LR: 0.001000)
[2025-05-12 16:25:49,172]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 0.7739 Train Acc: 0.7285 Eval Loss: 0.8846 Eval Acc: 0.7024 (LR: 0.001000)
[2025-05-12 16:27:45,492]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 0.7714 Train Acc: 0.7302 Eval Loss: 0.9252 Eval Acc: 0.6882 (LR: 0.001000)
[2025-05-12 16:29:44,728]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 0.7735 Train Acc: 0.7295 Eval Loss: 0.9089 Eval Acc: 0.6983 (LR: 0.001000)
[2025-05-12 16:31:42,809]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 0.7656 Train Acc: 0.7293 Eval Loss: 0.8573 Eval Acc: 0.7068 (LR: 0.001000)
[2025-05-12 16:33:40,701]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 0.7641 Train Acc: 0.7316 Eval Loss: 0.9028 Eval Acc: 0.6957 (LR: 0.001000)
[2025-05-12 16:35:42,449]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 0.7573 Train Acc: 0.7335 Eval Loss: 0.8085 Eval Acc: 0.7247 (LR: 0.001000)
[2025-05-12 16:37:43,492]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 0.7515 Train Acc: 0.7389 Eval Loss: 0.8805 Eval Acc: 0.7021 (LR: 0.000250)
[2025-05-12 16:39:40,934]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 0.7074 Train Acc: 0.7519 Eval Loss: 0.6979 Eval Acc: 0.7591 (LR: 0.000250)
[2025-05-12 16:42:38,710]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 0.7048 Train Acc: 0.7531 Eval Loss: 0.8304 Eval Acc: 0.7205 (LR: 0.000250)
[2025-05-12 16:45:16,393]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 0.7024 Train Acc: 0.7553 Eval Loss: 0.7641 Eval Acc: 0.7384 (LR: 0.000250)
[2025-05-12 16:47:27,864]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 0.7079 Train Acc: 0.7517 Eval Loss: 0.8070 Eval Acc: 0.7207 (LR: 0.000250)
[2025-05-12 16:49:41,075]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 0.7067 Train Acc: 0.7514 Eval Loss: 0.7820 Eval Acc: 0.7332 (LR: 0.000250)
[2025-05-12 16:51:53,504]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 0.7070 Train Acc: 0.7518 Eval Loss: 0.7885 Eval Acc: 0.7268 (LR: 0.000250)
[2025-05-12 16:54:06,107]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 0.7139 Train Acc: 0.7492 Eval Loss: 0.7696 Eval Acc: 0.7361 (LR: 0.000250)
[2025-05-12 16:55:49,443]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 0.7060 Train Acc: 0.7528 Eval Loss: 0.7396 Eval Acc: 0.7468 (LR: 0.000250)
[2025-05-12 16:57:38,118]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 0.7071 Train Acc: 0.7526 Eval Loss: 0.8724 Eval Acc: 0.7097 (LR: 0.000250)
[2025-05-12 16:59:14,250]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 0.7042 Train Acc: 0.7528 Eval Loss: 0.8228 Eval Acc: 0.7210 (LR: 0.000250)
[2025-05-12 17:01:00,676]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 0.7043 Train Acc: 0.7525 Eval Loss: 0.8230 Eval Acc: 0.7168 (LR: 0.000250)
[2025-05-12 17:02:36,107]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 0.7112 Train Acc: 0.7497 Eval Loss: 0.9137 Eval Acc: 0.7006 (LR: 0.000250)
[2025-05-12 17:04:22,457]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 0.7094 Train Acc: 0.7518 Eval Loss: 0.7335 Eval Acc: 0.7505 (LR: 0.000250)
[2025-05-12 17:05:57,911]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 0.7069 Train Acc: 0.7522 Eval Loss: 0.7436 Eval Acc: 0.7493 (LR: 0.000250)
[2025-05-12 17:07:44,020]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 0.7059 Train Acc: 0.7547 Eval Loss: 0.8364 Eval Acc: 0.7152 (LR: 0.000063)
[2025-05-12 17:09:19,722]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 0.6711 Train Acc: 0.7651 Eval Loss: 0.7237 Eval Acc: 0.7556 (LR: 0.000063)
[2025-05-12 17:11:06,153]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 0.6762 Train Acc: 0.7657 Eval Loss: 0.7253 Eval Acc: 0.7476 (LR: 0.000063)
[2025-05-12 17:12:41,708]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 0.6805 Train Acc: 0.7614 Eval Loss: 0.7482 Eval Acc: 0.7415 (LR: 0.000063)
[2025-05-12 17:14:28,074]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 0.6768 Train Acc: 0.7623 Eval Loss: 0.7257 Eval Acc: 0.7527 (LR: 0.000063)
[2025-05-12 17:16:03,516]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 0.6858 Train Acc: 0.7599 Eval Loss: 0.7082 Eval Acc: 0.7541 (LR: 0.000063)
[2025-05-12 17:17:50,713]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 0.6841 Train Acc: 0.7598 Eval Loss: 0.6945 Eval Acc: 0.7622 (LR: 0.000063)
[2025-05-12 17:19:30,888]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 0.6844 Train Acc: 0.7599 Eval Loss: 0.8337 Eval Acc: 0.7165 (LR: 0.000063)
[2025-05-12 17:21:19,143]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 0.6825 Train Acc: 0.7602 Eval Loss: 0.7123 Eval Acc: 0.7570 (LR: 0.000063)
[2025-05-12 17:23:01,174]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 0.6874 Train Acc: 0.7599 Eval Loss: 0.7902 Eval Acc: 0.7225 (LR: 0.000063)
[2025-05-12 17:24:45,621]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 0.6790 Train Acc: 0.7625 Eval Loss: 0.7481 Eval Acc: 0.7408 (LR: 0.000063)
[2025-05-12 17:26:23,646]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 0.6835 Train Acc: 0.7603 Eval Loss: 0.7670 Eval Acc: 0.7409 (LR: 0.000063)
[2025-05-12 17:28:10,866]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 0.6791 Train Acc: 0.7616 Eval Loss: 0.7278 Eval Acc: 0.7493 (LR: 0.000063)
[2025-05-12 17:29:47,315]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 0.6810 Train Acc: 0.7624 Eval Loss: 0.8388 Eval Acc: 0.7245 (LR: 0.000063)
[2025-05-12 17:31:39,582]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 0.6886 Train Acc: 0.7572 Eval Loss: 0.7341 Eval Acc: 0.7494 (LR: 0.000063)
[2025-05-12 17:33:20,600]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 0.6859 Train Acc: 0.7588 Eval Loss: 0.7861 Eval Acc: 0.7358 (LR: 0.000063)
[2025-05-12 17:33:20,600]: [ResNet20_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.7622
[2025-05-12 17:33:20,679]: 


Quantization of model down to 2 bits finished
[2025-05-12 17:33:20,679]: Model Architecture:
[2025-05-12 17:33:20,733]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9026], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.707818984985352)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2432], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3647790551185608, max_val=0.3647935688495636)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9151], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.745210647583008)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1877], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21213923394680023, max_val=0.3509978950023651)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8945], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.683553218841553)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2163], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32452088594436646, max_val=0.3245013654232025)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9183], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.754880905151367)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1660], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28284478187561035, max_val=0.21513381600379944)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9325], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.797643661499023)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1322], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20696820318698883, max_val=0.18966446816921234)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9144], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.743108749389648)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1321], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19815780222415924, max_val=0.19825854897499084)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9968], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.990394115447998)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1295], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19361713528633118, max_val=0.19479477405548096)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9184], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.75511360168457)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1152], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1606472134590149, max_val=0.18488860130310059)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2388], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3570515811443329, max_val=0.3592068552970886)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9262], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.778717994689941)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0993], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14815843105316162, max_val=0.14976148307323456)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9162], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.748553276062012)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0993], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14896833896636963, max_val=0.14891576766967773)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9405], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.821491241455078)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0986], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14682292938232422, max_val=0.14889034628868103)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9146], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.743935585021973)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1000], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1465853601694107, max_val=0.15342068672180176)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0127], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.037992000579834)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0925], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13870246708393097, max_val=0.13868029415607452)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9060], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7180633544921875)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0738], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1123073622584343, max_val=0.10905160009860992)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1584], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23716798424720764, max_val=0.23817101120948792)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9417], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.825138092041016)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0767], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11111074686050415, max_val=0.11899945139884949)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.756269931793213)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0666], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0905279740691185, max_val=0.10940646380186081)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9420], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.826092720031738)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0570], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0827995240688324, max_val=0.08829700946807861)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9183], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.754801273345947)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0492], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06550872325897217, max_val=0.08223235607147217)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0617], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.185125350952148)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 17:33:20,734]: 
Model Weights:
[2025-05-12 17:33:20,734]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 17:33:20,734]: Sample Values (25 elements): [0.016721675172448158, 0.31150496006011963, 0.12241436541080475, -0.0583098866045475, 0.3056955337524414, -0.10163935273885727, -0.12608736753463745, 0.29941055178642273, -0.04799990728497505, -0.11232830584049225, -0.34862929582595825, -0.06978218257427216, -0.10309126228094101, 0.15740998089313507, 0.2388569414615631, 0.03691327944397926, -0.30218830704689026, -0.08147583901882172, 0.058318451046943665, -0.19395282864570618, 0.19810062646865845, -0.06651219725608826, -0.05305130407214165, -0.06332672387361526, -0.09747415035963058]
[2025-05-12 17:33:20,734]: Mean: -0.00033178
[2025-05-12 17:33:20,734]: Min: -0.54597533
[2025-05-12 17:33:20,735]: Max: 0.52872145
[2025-05-12 17:33:20,735]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,735]: Sample Values (16 elements): [1.4637714624404907, 1.1713848114013672, 1.1752415895462036, 0.9143071174621582, 1.0111693143844604, 1.9112634658813477, 1.2365176677703857, 1.1858834028244019, 1.0474203824996948, 0.9094856977462769, 1.8386651277542114, 1.3062552213668823, 1.346092700958252, 1.0717271566390991, 1.1754872798919678, 1.5820512771606445]
[2025-05-12 17:33:20,735]: Mean: 1.27167034
[2025-05-12 17:33:20,735]: Min: 0.90948570
[2025-05-12 17:33:20,735]: Max: 1.91126347
[2025-05-12 17:33:20,736]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 17:33:20,737]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2431909292936325, 0.0, 0.0, 0.2431909292936325, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2431909292936325, 0.0, 0.0]
[2025-05-12 17:33:20,737]: Mean: -0.00116107
[2025-05-12 17:33:20,737]: Min: -0.24319093
[2025-05-12 17:33:20,737]: Max: 0.48638186
[2025-05-12 17:33:20,737]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,737]: Sample Values (16 elements): [1.1024340391159058, 1.323708415031433, 1.3120088577270508, 1.185737133026123, 1.0135374069213867, 0.9082353115081787, 1.0503050088882446, 1.0882158279418945, 1.053684949874878, 1.0181267261505127, 0.9898104667663574, 1.128965139389038, 0.935758113861084, 0.9019736051559448, 1.2224721908569336, 1.086668848991394]
[2025-05-12 17:33:20,738]: Mean: 1.08260262
[2025-05-12 17:33:20,738]: Min: 0.90197361
[2025-05-12 17:33:20,738]: Max: 1.32370842
[2025-05-12 17:33:20,739]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 17:33:20,739]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18771252036094666, 0.0, 0.0, 0.0, 0.0, -0.18771252036094666, 0.0, -0.18771252036094666, 0.0, 0.0, 0.18771252036094666, -0.18771252036094666, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,739]: Mean: -0.00578454
[2025-05-12 17:33:20,740]: Min: -0.18771252
[2025-05-12 17:33:20,740]: Max: 0.37542504
[2025-05-12 17:33:20,740]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,740]: Sample Values (16 elements): [1.0865654945373535, 0.9129028916358948, 1.119510293006897, 1.0625543594360352, 1.049038290977478, 1.0577812194824219, 1.1352678537368774, 0.9389221668243408, 0.941784679889679, 1.0229593515396118, 1.1186894178390503, 1.039208173751831, 1.0270166397094727, 0.8853588104248047, 1.072746753692627, 1.0763037204742432]
[2025-05-12 17:33:20,740]: Mean: 1.03416312
[2025-05-12 17:33:20,740]: Min: 0.88535881
[2025-05-12 17:33:20,741]: Max: 1.13526785
[2025-05-12 17:33:20,741]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 17:33:20,742]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.21634075045585632, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21634075045585632, 0.0]
[2025-05-12 17:33:20,742]: Mean: 0.00037559
[2025-05-12 17:33:20,742]: Min: -0.21634075
[2025-05-12 17:33:20,742]: Max: 0.21634075
[2025-05-12 17:33:20,742]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,743]: Sample Values (16 elements): [0.9643049836158752, 0.9548100829124451, 1.2676692008972168, 1.0015418529510498, 1.1125361919403076, 0.9866810441017151, 1.4351670742034912, 1.036799669265747, 1.2800893783569336, 1.0014535188674927, 1.018155813217163, 0.9543403387069702, 0.9557246565818787, 1.1306507587432861, 0.9566089510917664, 0.9861211776733398]
[2025-05-12 17:33:20,743]: Mean: 1.06516600
[2025-05-12 17:33:20,743]: Min: 0.95434034
[2025-05-12 17:33:20,743]: Max: 1.43516707
[2025-05-12 17:33:20,744]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 17:33:20,744]: Sample Values (25 elements): [-0.16599291563034058, -0.16599291563034058, 0.0, 0.0, 0.16599291563034058, 0.0, 0.0, 0.0, 0.0, -0.16599291563034058, 0.0, -0.16599291563034058, 0.0, -0.16599291563034058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,744]: Mean: -0.00561955
[2025-05-12 17:33:20,745]: Min: -0.33198583
[2025-05-12 17:33:20,745]: Max: 0.16599292
[2025-05-12 17:33:20,745]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,745]: Sample Values (16 elements): [1.1800546646118164, 1.0194423198699951, 1.0663557052612305, 0.9218213558197021, 0.9778692126274109, 1.0311137437820435, 0.9328198432922363, 1.0237141847610474, 0.9235371947288513, 1.0225296020507812, 1.0309337377548218, 1.1033482551574707, 1.0726147890090942, 0.8444334268569946, 1.0664379596710205, 1.0759838819503784]
[2025-05-12 17:33:20,745]: Mean: 1.01831317
[2025-05-12 17:33:20,745]: Min: 0.84443343
[2025-05-12 17:33:20,745]: Max: 1.18005466
[2025-05-12 17:33:20,746]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 17:33:20,747]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.13221105933189392, 0.0, 0.0, -0.13221105933189392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13221105933189392, -0.13221105933189392, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,747]: Mean: -0.00321346
[2025-05-12 17:33:20,747]: Min: -0.26442212
[2025-05-12 17:33:20,747]: Max: 0.13221106
[2025-05-12 17:33:20,747]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,748]: Sample Values (16 elements): [0.9926306009292603, 1.0345938205718994, 1.0502982139587402, 0.9725179076194763, 1.0556812286376953, 1.0078394412994385, 1.061528205871582, 1.043688178062439, 1.0435621738433838, 1.0147114992141724, 1.0566632747650146, 1.0561532974243164, 1.0117814540863037, 0.9455304741859436, 1.113983392715454, 0.9858759045600891]
[2025-05-12 17:33:20,748]: Mean: 1.02794003
[2025-05-12 17:33:20,748]: Min: 0.94553047
[2025-05-12 17:33:20,748]: Max: 1.11398339
[2025-05-12 17:33:20,749]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 17:33:20,749]: Sample Values (25 elements): [0.0, -0.1321389079093933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1321389079093933, 0.0, 0.1321389079093933, 0.0, 0.0, -0.1321389079093933, 0.0, 0.0, -0.1321389079093933, -0.1321389079093933, -0.1321389079093933]
[2025-05-12 17:33:20,750]: Mean: -0.00143380
[2025-05-12 17:33:20,750]: Min: -0.13213891
[2025-05-12 17:33:20,750]: Max: 0.26427782
[2025-05-12 17:33:20,750]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 17:33:20,750]: Sample Values (16 elements): [1.0117565393447876, 0.9773485660552979, 0.9580080509185791, 0.976719081401825, 0.8587381839752197, 0.989965558052063, 0.9465500116348267, 0.9492737054824829, 1.0451741218566895, 1.0470716953277588, 1.0139269828796387, 0.9919649362564087, 1.0135101079940796, 1.1194231510162354, 1.002242088317871, 0.9315878748893738]
[2025-05-12 17:33:20,750]: Mean: 0.98957884
[2025-05-12 17:33:20,751]: Min: 0.85873818
[2025-05-12 17:33:20,751]: Max: 1.11942315
[2025-05-12 17:33:20,752]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 17:33:20,752]: Sample Values (25 elements): [0.0, 0.0, -0.12947073578834534, 0.0, 0.0, 0.12947073578834534, 0.0, 0.0, 0.0, 0.0, -0.12947073578834534, 0.0, -0.12947073578834534, 0.0, 0.0, 0.12947073578834534, 0.0, 0.0, -0.12947073578834534, 0.0, 0.0, 0.12947073578834534, 0.0, 0.12947073578834534, 0.0]
[2025-05-12 17:33:20,752]: Mean: 0.00028097
[2025-05-12 17:33:20,752]: Min: -0.12947074
[2025-05-12 17:33:20,753]: Max: 0.25894147
[2025-05-12 17:33:20,753]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,753]: Sample Values (25 elements): [1.051679015159607, 0.9989835619926453, 1.008668065071106, 1.0893961191177368, 1.061240553855896, 1.0658881664276123, 1.0879889726638794, 1.1052178144454956, 1.0307992696762085, 1.0602452754974365, 1.003608226776123, 1.0271766185760498, 1.044887900352478, 1.0070874691009521, 1.0681703090667725, 1.0367974042892456, 1.0484033823013306, 1.132377028465271, 1.0924668312072754, 1.0820897817611694, 1.0616858005523682, 1.0944684743881226, 1.0089627504348755, 1.0712040662765503, 1.002995252609253]
[2025-05-12 17:33:20,753]: Mean: 1.05031633
[2025-05-12 17:33:20,753]: Min: 0.99898356
[2025-05-12 17:33:20,753]: Max: 1.13237703
[2025-05-12 17:33:20,754]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 17:33:20,755]: Sample Values (25 elements): [0.0, -0.11517870426177979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11517870426177979, 0.0, 0.0, 0.0, -0.11517870426177979, 0.11517870426177979, -0.11517870426177979, 0.0, 0.0, 0.0, 0.0, 0.11517870426177979]
[2025-05-12 17:33:20,755]: Mean: -0.00036243
[2025-05-12 17:33:20,755]: Min: -0.11517870
[2025-05-12 17:33:20,755]: Max: 0.23035741
[2025-05-12 17:33:20,755]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,755]: Sample Values (25 elements): [1.000840187072754, 1.0971604585647583, 1.0683902502059937, 1.043044090270996, 1.117854118347168, 1.0627179145812988, 1.151515245437622, 1.0629966259002686, 1.0023088455200195, 1.087737798690796, 1.0314230918884277, 0.9780651330947876, 1.0728096961975098, 1.0245829820632935, 1.0566800832748413, 1.061676025390625, 1.0191725492477417, 1.0002167224884033, 1.0838974714279175, 1.0758708715438843, 1.0765753984451294, 1.0529536008834839, 0.9816347360610962, 1.069007396697998, 1.1156209707260132]
[2025-05-12 17:33:20,756]: Mean: 1.05317652
[2025-05-12 17:33:20,756]: Min: 0.97806513
[2025-05-12 17:33:20,756]: Max: 1.15151525
[2025-05-12 17:33:20,757]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 17:33:20,757]: Sample Values (25 elements): [0.0, -0.23875296115875244, -0.23875296115875244, -0.23875296115875244, 0.23875296115875244, -0.23875296115875244, 0.23875296115875244, 0.0, 0.0, 0.0, 0.0, -0.23875296115875244, 0.0, 0.0, -0.23875296115875244, -0.23875296115875244, 0.0, 0.23875296115875244, 0.0, -0.23875296115875244, 0.0, 0.0, -0.23875296115875244, 0.0, 0.0]
[2025-05-12 17:33:20,757]: Mean: 0.00466314
[2025-05-12 17:33:20,757]: Min: -0.23875296
[2025-05-12 17:33:20,758]: Max: 0.47750592
[2025-05-12 17:33:20,758]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,758]: Sample Values (25 elements): [0.8417030572891235, 1.0095709562301636, 0.8799770474433899, 0.8995702266693115, 0.9190124273300171, 0.8766340017318726, 0.9243457317352295, 0.9148893356323242, 0.8089424967765808, 0.8684880137443542, 0.9285928010940552, 0.961008608341217, 0.8033297657966614, 0.9169955253601074, 0.8555165529251099, 0.940848171710968, 0.9038638472557068, 0.9136636853218079, 0.9410383105278015, 0.9268678426742554, 0.8977050185203552, 0.9302717447280884, 0.9456555247306824, 0.9464340806007385, 0.9132224917411804]
[2025-05-12 17:33:20,758]: Mean: 0.90104210
[2025-05-12 17:33:20,758]: Min: 0.80332977
[2025-05-12 17:33:20,758]: Max: 1.00957096
[2025-05-12 17:33:20,759]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 17:33:20,760]: Sample Values (25 elements): [-0.09930682182312012, 0.09930682182312012, 0.0, -0.09930682182312012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09930682182312012, -0.09930682182312012, 0.09930682182312012, 0.0, 0.09930682182312012, 0.0, 0.0, 0.0, 0.09930682182312012, 0.0, 0.0, -0.09930682182312012, 0.0, 0.0, -0.09930682182312012]
[2025-05-12 17:33:20,760]: Mean: -0.00155167
[2025-05-12 17:33:20,760]: Min: -0.09930682
[2025-05-12 17:33:20,760]: Max: 0.19861364
[2025-05-12 17:33:20,760]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,760]: Sample Values (25 elements): [0.9974997639656067, 1.0646421909332275, 1.051858901977539, 0.9569599628448486, 1.0266326665878296, 1.0186140537261963, 1.0047920942306519, 1.0589665174484253, 1.0157281160354614, 1.0282572507858276, 1.051424264907837, 1.058497667312622, 1.0107473134994507, 1.008525013923645, 0.9928299784660339, 0.9896400570869446, 0.9987285733222961, 0.977922797203064, 1.0130248069763184, 1.0139843225479126, 1.0503654479980469, 1.044100046157837, 0.9741645455360413, 1.000076174736023, 1.0867602825164795]
[2025-05-12 17:33:20,761]: Mean: 1.02527046
[2025-05-12 17:33:20,761]: Min: 0.95695996
[2025-05-12 17:33:20,761]: Max: 1.11293471
[2025-05-12 17:33:20,762]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 17:33:20,762]: Sample Values (25 elements): [-0.09929477423429489, 0.0, 0.09929477423429489, 0.0, -0.09929477423429489, 0.0, 0.0, 0.09929477423429489, 0.09929477423429489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09929477423429489, 0.09929477423429489, 0.0, 0.0, -0.09929477423429489, 0.0, 0.0]
[2025-05-12 17:33:20,762]: Mean: -0.00062490
[2025-05-12 17:33:20,763]: Min: -0.19858955
[2025-05-12 17:33:20,763]: Max: 0.09929477
[2025-05-12 17:33:20,763]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,763]: Sample Values (25 elements): [1.037076473236084, 1.022876262664795, 1.000938057899475, 1.002151608467102, 0.9139319062232971, 1.0563547611236572, 1.0045616626739502, 1.0149651765823364, 1.0253937244415283, 1.0050396919250488, 1.0485354661941528, 1.0543571710586548, 0.9845395684242249, 0.9576308131217957, 1.0410974025726318, 1.0608769655227661, 0.9769041538238525, 0.9722723364830017, 1.0181185007095337, 1.0069912672042847, 0.9635091423988342, 1.0276665687561035, 1.055186152458191, 1.0530176162719727, 1.033808708190918]
[2025-05-12 17:33:20,763]: Mean: 1.00546932
[2025-05-12 17:33:20,763]: Min: 0.91393191
[2025-05-12 17:33:20,764]: Max: 1.06087697
[2025-05-12 17:33:20,764]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 17:33:20,765]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.09857114404439926, 0.0, 0.0, 0.09857114404439926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09857114404439926, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,765]: Mean: -0.00083426
[2025-05-12 17:33:20,765]: Min: -0.09857114
[2025-05-12 17:33:20,765]: Max: 0.19714229
[2025-05-12 17:33:20,765]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,766]: Sample Values (25 elements): [0.998439371585846, 1.0083794593811035, 0.9892093539237976, 1.000709056854248, 1.0520423650741577, 1.0544830560684204, 0.9767625331878662, 1.004984974861145, 1.0126006603240967, 1.013138771057129, 0.9993916153907776, 1.033105492591858, 1.0157650709152222, 1.0324760675430298, 0.9681345224380493, 1.0215790271759033, 0.9546205401420593, 1.022372841835022, 0.9819256663322449, 1.003866195678711, 1.0342891216278076, 1.126049518585205, 1.0019185543060303, 0.9798345565795898, 1.0072680711746216]
[2025-05-12 17:33:20,766]: Mean: 1.01156151
[2025-05-12 17:33:20,766]: Min: 0.95462054
[2025-05-12 17:33:20,766]: Max: 1.12604952
[2025-05-12 17:33:20,767]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 17:33:20,767]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1000019758939743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1000019758939743, -0.1000019758939743, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,768]: Mean: 0.00083552
[2025-05-12 17:33:20,768]: Min: -0.10000198
[2025-05-12 17:33:20,768]: Max: 0.20000395
[2025-05-12 17:33:20,768]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 17:33:20,768]: Sample Values (25 elements): [0.9802637696266174, 1.0248501300811768, 0.9838436245918274, 1.0898182392120361, 1.0087157487869263, 1.0355247259140015, 1.0545068979263306, 1.0350477695465088, 1.0449397563934326, 1.0401356220245361, 1.0146042108535767, 1.0615818500518799, 1.0345923900604248, 0.9806973934173584, 0.981911301612854, 1.0209020376205444, 1.037785530090332, 0.9906569719314575, 1.0022883415222168, 1.0281672477722168, 1.0385140180587769, 1.0510460138320923, 1.0749428272247314, 1.0255126953125, 1.0157793760299683]
[2025-05-12 17:33:20,768]: Mean: 1.02971244
[2025-05-12 17:33:20,768]: Min: 0.98026377
[2025-05-12 17:33:20,769]: Max: 1.08981824
[2025-05-12 17:33:20,770]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 17:33:20,770]: Sample Values (25 elements): [0.0, -0.0924610123038292, 0.0924610123038292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0924610123038292, -0.0924610123038292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0924610123038292, 0.0, -0.0924610123038292, 0.0]
[2025-05-12 17:33:20,770]: Mean: 0.00034111
[2025-05-12 17:33:20,770]: Min: -0.18492202
[2025-05-12 17:33:20,770]: Max: 0.09246101
[2025-05-12 17:33:20,770]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,771]: Sample Values (25 elements): [1.019376516342163, 1.0412324666976929, 0.9666320085525513, 1.0202308893203735, 1.0062143802642822, 1.0097204446792603, 1.044153094291687, 1.006212830543518, 1.0132544040679932, 1.0280227661132812, 1.0193268060684204, 1.1126818656921387, 1.0559967756271362, 0.984062135219574, 1.0319267511367798, 0.9920445680618286, 0.9721140265464783, 0.9891946911811829, 1.0344982147216797, 1.0230416059494019, 1.0026838779449463, 1.027714490890503, 1.0071587562561035, 1.0099492073059082, 1.0127129554748535]
[2025-05-12 17:33:20,771]: Mean: 1.01617646
[2025-05-12 17:33:20,771]: Min: 0.96663201
[2025-05-12 17:33:20,771]: Max: 1.11268187
[2025-05-12 17:33:20,772]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 17:33:20,773]: Sample Values (25 elements): [0.07378634810447693, 0.0, 0.0, 0.0, 0.0, 0.07378634810447693, 0.0, 0.07378634810447693, 0.0, 0.07378634810447693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07378634810447693, 0.0, 0.0]
[2025-05-12 17:33:20,773]: Mean: -0.00010809
[2025-05-12 17:33:20,773]: Min: -0.14757270
[2025-05-12 17:33:20,773]: Max: 0.07378635
[2025-05-12 17:33:20,773]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,774]: Sample Values (25 elements): [1.0027676820755005, 1.0211514234542847, 1.0352939367294312, 1.021518349647522, 1.012402892112732, 1.0187523365020752, 0.9982901215553284, 1.0573662519454956, 1.0113294124603271, 1.0499240159988403, 1.0244807004928589, 0.9860360622406006, 1.0647010803222656, 1.0345053672790527, 1.0219649076461792, 1.0052241086959839, 1.0378515720367432, 0.9897304177284241, 1.0208266973495483, 1.0495243072509766, 1.0393311977386475, 1.0485771894454956, 1.0213985443115234, 1.098601222038269, 1.013953685760498]
[2025-05-12 17:33:20,774]: Mean: 1.03585124
[2025-05-12 17:33:20,774]: Min: 0.98603606
[2025-05-12 17:33:20,774]: Max: 1.11575794
[2025-05-12 17:33:20,775]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 17:33:20,775]: Sample Values (25 elements): [0.0, 0.0, 0.15844640135765076, 0.0, 0.15844640135765076, -0.15844640135765076, 0.15844640135765076, -0.15844640135765076, 0.15844640135765076, 0.15844640135765076, 0.15844640135765076, 0.15844640135765076, 0.0, 0.0, -0.15844640135765076, 0.0, 0.0, 0.0, 0.0, -0.15844640135765076, 0.0, -0.15844640135765076, 0.0, 0.0, -0.15844640135765076]
[2025-05-12 17:33:20,775]: Mean: 0.00332676
[2025-05-12 17:33:20,776]: Min: -0.15844640
[2025-05-12 17:33:20,776]: Max: 0.31689280
[2025-05-12 17:33:20,776]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,776]: Sample Values (25 elements): [0.9270308017730713, 0.9361847043037415, 0.9003772139549255, 0.9050940275192261, 0.9183056950569153, 0.8881151676177979, 0.9551689624786377, 0.8814941644668579, 0.9260090589523315, 0.9167781472206116, 0.9227114319801331, 0.8767262101173401, 0.9220052361488342, 0.9158488512039185, 0.8937261700630188, 0.9144087433815002, 0.9377613067626953, 0.903950572013855, 0.9139845371246338, 0.906014621257782, 0.9039830565452576, 0.9166952967643738, 0.9292412400245667, 0.9012275338172913, 0.9078261852264404]
[2025-05-12 17:33:20,776]: Mean: 0.91319346
[2025-05-12 17:33:20,776]: Min: 0.86407781
[2025-05-12 17:33:20,777]: Max: 0.97809625
[2025-05-12 17:33:20,778]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 17:33:20,778]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.07670341432094574, 0.0, 0.0, 0.0, 0.07670341432094574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07670341432094574, 0.0, 0.0, 0.0, 0.07670341432094574, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,779]: Mean: -0.00043071
[2025-05-12 17:33:20,779]: Min: -0.07670341
[2025-05-12 17:33:20,779]: Max: 0.15340683
[2025-05-12 17:33:20,779]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,779]: Sample Values (25 elements): [0.9776674509048462, 1.0900651216506958, 1.000131368637085, 0.9961471557617188, 1.025591492652893, 0.987968921661377, 1.0062587261199951, 0.9935925602912903, 1.0376557111740112, 1.000211238861084, 0.992730438709259, 1.0112180709838867, 1.0153898000717163, 1.0080965757369995, 0.959815263748169, 0.9921315908432007, 0.9978647232055664, 0.9998445510864258, 0.971398651599884, 0.9780378341674805, 1.037150502204895, 1.0500510931015015, 1.0077321529388428, 1.072308897972107, 1.0048644542694092]
[2025-05-12 17:33:20,779]: Mean: 1.00452101
[2025-05-12 17:33:20,780]: Min: 0.95015293
[2025-05-12 17:33:20,780]: Max: 1.09006512
[2025-05-12 17:33:20,781]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 17:33:20,781]: Sample Values (25 elements): [0.0, -0.06664475798606873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06664475798606873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06664475798606873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 17:33:20,781]: Mean: 0.00045558
[2025-05-12 17:33:20,782]: Min: -0.06664476
[2025-05-12 17:33:20,782]: Max: 0.13328952
[2025-05-12 17:33:20,782]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,782]: Sample Values (25 elements): [1.003670334815979, 1.0354492664337158, 1.0194910764694214, 1.0009002685546875, 1.0032985210418701, 1.0087989568710327, 1.0156954526901245, 1.040266990661621, 1.0041797161102295, 1.0472699403762817, 1.0120779275894165, 1.0482392311096191, 1.017381191253662, 1.0254125595092773, 1.0327059030532837, 1.048563838005066, 1.0217009782791138, 1.0310407876968384, 1.0394916534423828, 1.0099033117294312, 1.0258010625839233, 1.061371922492981, 1.0234142541885376, 1.022874116897583, 1.0083099603652954]
[2025-05-12 17:33:20,782]: Mean: 1.02453768
[2025-05-12 17:33:20,782]: Min: 0.99050516
[2025-05-12 17:33:20,783]: Max: 1.09056938
[2025-05-12 17:33:20,784]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 17:33:20,784]: Sample Values (25 elements): [0.0, -0.057032108306884766, 0.0, 0.0, 0.0, -0.057032108306884766, -0.057032108306884766, -0.057032108306884766, 0.0, 0.057032108306884766, 0.0, 0.0, 0.0, 0.0, 0.057032108306884766, 0.057032108306884766, -0.057032108306884766, 0.057032108306884766, 0.0, 0.0, 0.0, -0.057032108306884766, -0.057032108306884766, 0.0, 0.0]
[2025-05-12 17:33:20,784]: Mean: -0.00018875
[2025-05-12 17:33:20,784]: Min: -0.05703211
[2025-05-12 17:33:20,785]: Max: 0.11406422
[2025-05-12 17:33:20,785]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,785]: Sample Values (25 elements): [0.9903252720832825, 0.9715593457221985, 0.9726899862289429, 0.981412410736084, 0.9531168937683105, 0.9755055904388428, 0.9812251925468445, 0.9670829176902771, 0.9789308309555054, 0.9696942567825317, 0.9507730007171631, 0.980104386806488, 0.9913132786750793, 0.98121577501297, 0.9730482697486877, 0.96843421459198, 0.9877446293830872, 0.975628674030304, 0.9694327116012573, 0.9627295136451721, 1.0391887426376343, 0.9584932327270508, 0.9683758616447449, 0.9828982949256897, 0.9760841131210327]
[2025-05-12 17:33:20,785]: Mean: 0.97414416
[2025-05-12 17:33:20,785]: Min: 0.94844836
[2025-05-12 17:33:20,785]: Max: 1.03918874
[2025-05-12 17:33:20,786]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 17:33:20,787]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.049247097223997116, 0.0, 0.0, -0.049247097223997116, 0.0, 0.0, 0.0, 0.049247097223997116, -0.049247097223997116, 0.0, 0.049247097223997116, 0.0, -0.049247097223997116, 0.0, -0.049247097223997116, 0.0, 0.0, -0.049247097223997116, -0.049247097223997116, 0.0, 0.0]
[2025-05-12 17:33:20,787]: Mean: 0.00053036
[2025-05-12 17:33:20,787]: Min: -0.04924710
[2025-05-12 17:33:20,787]: Max: 0.09849419
[2025-05-12 17:33:20,787]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 17:33:20,788]: Sample Values (25 elements): [1.051847219467163, 1.0103459358215332, 1.0414689779281616, 1.0584392547607422, 1.0382052659988403, 1.0521873235702515, 1.0656499862670898, 1.0270709991455078, 1.050801396369934, 1.0353862047195435, 1.0396907329559326, 1.0578296184539795, 1.0356976985931396, 1.0445197820663452, 1.0433882474899292, 1.046934723854065, 1.0189213752746582, 1.0244466066360474, 1.0374679565429688, 1.0443748235702515, 1.0370460748672485, 1.0596725940704346, 1.0637882947921753, 1.0327837467193604, 1.0673848390579224]
[2025-05-12 17:33:20,788]: Mean: 1.04273272
[2025-05-12 17:33:20,788]: Min: 1.00490475
[2025-05-12 17:33:20,788]: Max: 1.09158432
[2025-05-12 17:33:20,788]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 17:33:20,789]: Sample Values (25 elements): [-0.2393696904182434, -0.05521974340081215, -0.02137155272066593, 0.15974228084087372, 0.3736693859100342, -0.0637209415435791, -0.1099969744682312, -0.11589004844427109, 0.018646154552698135, 0.18358924984931946, 0.05503268539905548, -0.005856705363839865, -0.0497693233191967, 0.05658293142914772, 0.27129077911376953, 0.22955921292304993, -0.19084173440933228, -0.28922000527381897, 0.249297633767128, 0.30711308121681213, -0.12764818966388702, 0.32811614871025085, 0.029071003198623657, 0.2049357295036316, 0.2507972717285156]
[2025-05-12 17:33:20,789]: Mean: 0.00018507
[2025-05-12 17:33:20,789]: Min: -0.32496515
[2025-05-12 17:33:20,789]: Max: 0.50932640
