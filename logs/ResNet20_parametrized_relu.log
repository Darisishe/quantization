[2025-06-13 18:42:46,724]: 
Training ResNet20 with parametrized_relu
[2025-06-13 18:43:50,868]: [ResNet20_parametrized_relu] Epoch: 001 Train Loss: 1.4936 Train Acc: 0.4466 Eval Loss: 1.2760 Eval Acc: 0.5406 (LR: 0.00100000)
[2025-06-13 18:44:45,303]: [ResNet20_parametrized_relu] Epoch: 002 Train Loss: 1.1016 Train Acc: 0.6038 Eval Loss: 1.2726 Eval Acc: 0.5590 (LR: 0.00100000)
[2025-06-13 18:45:34,688]: [ResNet20_parametrized_relu] Epoch: 003 Train Loss: 0.9525 Train Acc: 0.6595 Eval Loss: 0.9705 Eval Acc: 0.6715 (LR: 0.00100000)
[2025-06-13 18:46:26,746]: [ResNet20_parametrized_relu] Epoch: 004 Train Loss: 0.8447 Train Acc: 0.7030 Eval Loss: 0.8583 Eval Acc: 0.7043 (LR: 0.00100000)
[2025-06-13 18:47:25,415]: [ResNet20_parametrized_relu] Epoch: 005 Train Loss: 0.7657 Train Acc: 0.7313 Eval Loss: 0.8830 Eval Acc: 0.7039 (LR: 0.00100000)
[2025-06-13 18:48:24,070]: [ResNet20_parametrized_relu] Epoch: 006 Train Loss: 0.7175 Train Acc: 0.7498 Eval Loss: 0.8671 Eval Acc: 0.7219 (LR: 0.00100000)
[2025-06-13 18:49:19,125]: [ResNet20_parametrized_relu] Epoch: 007 Train Loss: 0.6698 Train Acc: 0.7673 Eval Loss: 0.6609 Eval Acc: 0.7748 (LR: 0.00100000)
[2025-06-13 18:50:08,992]: [ResNet20_parametrized_relu] Epoch: 008 Train Loss: 0.6314 Train Acc: 0.7811 Eval Loss: 0.6895 Eval Acc: 0.7698 (LR: 0.00100000)
[2025-06-13 18:51:01,367]: [ResNet20_parametrized_relu] Epoch: 009 Train Loss: 0.6121 Train Acc: 0.7875 Eval Loss: 0.7144 Eval Acc: 0.7670 (LR: 0.00100000)
[2025-06-13 18:51:59,789]: [ResNet20_parametrized_relu] Epoch: 010 Train Loss: 0.5794 Train Acc: 0.7985 Eval Loss: 0.6911 Eval Acc: 0.7696 (LR: 0.00100000)
[2025-06-13 18:52:59,769]: [ResNet20_parametrized_relu] Epoch: 011 Train Loss: 0.5625 Train Acc: 0.8030 Eval Loss: 0.5661 Eval Acc: 0.8146 (LR: 0.00100000)
[2025-06-13 18:53:52,816]: [ResNet20_parametrized_relu] Epoch: 012 Train Loss: 0.5409 Train Acc: 0.8115 Eval Loss: 0.6212 Eval Acc: 0.7913 (LR: 0.00100000)
[2025-06-13 18:54:42,121]: [ResNet20_parametrized_relu] Epoch: 013 Train Loss: 0.5209 Train Acc: 0.8208 Eval Loss: 0.6543 Eval Acc: 0.7913 (LR: 0.00100000)
[2025-06-13 18:55:34,280]: [ResNet20_parametrized_relu] Epoch: 014 Train Loss: 0.5070 Train Acc: 0.8242 Eval Loss: 0.5677 Eval Acc: 0.8177 (LR: 0.00100000)
[2025-06-13 18:56:33,456]: [ResNet20_parametrized_relu] Epoch: 015 Train Loss: 0.4931 Train Acc: 0.8280 Eval Loss: 0.5412 Eval Acc: 0.8188 (LR: 0.00100000)
[2025-06-13 18:57:33,326]: [ResNet20_parametrized_relu] Epoch: 016 Train Loss: 0.4781 Train Acc: 0.8346 Eval Loss: 0.5620 Eval Acc: 0.8167 (LR: 0.00100000)
[2025-06-13 18:58:26,435]: [ResNet20_parametrized_relu] Epoch: 017 Train Loss: 0.4644 Train Acc: 0.8391 Eval Loss: 0.4900 Eval Acc: 0.8352 (LR: 0.00100000)
[2025-06-13 18:59:17,232]: [ResNet20_parametrized_relu] Epoch: 018 Train Loss: 0.4562 Train Acc: 0.8399 Eval Loss: 0.5991 Eval Acc: 0.8126 (LR: 0.00100000)
[2025-06-13 19:00:13,046]: [ResNet20_parametrized_relu] Epoch: 019 Train Loss: 0.4398 Train Acc: 0.8471 Eval Loss: 0.5427 Eval Acc: 0.8238 (LR: 0.00100000)
[2025-06-13 19:01:19,008]: [ResNet20_parametrized_relu] Epoch: 020 Train Loss: 0.4295 Train Acc: 0.8497 Eval Loss: 0.4837 Eval Acc: 0.8408 (LR: 0.00100000)
[2025-06-13 19:02:27,009]: [ResNet20_parametrized_relu] Epoch: 021 Train Loss: 0.4203 Train Acc: 0.8545 Eval Loss: 0.5383 Eval Acc: 0.8202 (LR: 0.00100000)
[2025-06-13 19:03:26,156]: [ResNet20_parametrized_relu] Epoch: 022 Train Loss: 0.4132 Train Acc: 0.8557 Eval Loss: 0.5169 Eval Acc: 0.8355 (LR: 0.00100000)
[2025-06-13 19:04:22,423]: [ResNet20_parametrized_relu] Epoch: 023 Train Loss: 0.4040 Train Acc: 0.8580 Eval Loss: 0.4693 Eval Acc: 0.8453 (LR: 0.00100000)
[2025-06-13 19:05:25,237]: [ResNet20_parametrized_relu] Epoch: 024 Train Loss: 0.4019 Train Acc: 0.8600 Eval Loss: 0.5258 Eval Acc: 0.8284 (LR: 0.00100000)
[2025-06-13 19:06:31,298]: [ResNet20_parametrized_relu] Epoch: 025 Train Loss: 0.3899 Train Acc: 0.8637 Eval Loss: 0.4255 Eval Acc: 0.8554 (LR: 0.00100000)
[2025-06-13 19:07:40,047]: [ResNet20_parametrized_relu] Epoch: 026 Train Loss: 0.3831 Train Acc: 0.8667 Eval Loss: 0.4864 Eval Acc: 0.8426 (LR: 0.00100000)
[2025-06-13 19:08:34,298]: [ResNet20_parametrized_relu] Epoch: 027 Train Loss: 0.3762 Train Acc: 0.8689 Eval Loss: 0.4545 Eval Acc: 0.8521 (LR: 0.00100000)
[2025-06-13 19:09:37,212]: [ResNet20_parametrized_relu] Epoch: 028 Train Loss: 0.3728 Train Acc: 0.8705 Eval Loss: 0.4732 Eval Acc: 0.8471 (LR: 0.00100000)
[2025-06-13 19:10:39,749]: [ResNet20_parametrized_relu] Epoch: 029 Train Loss: 0.3688 Train Acc: 0.8704 Eval Loss: 0.4195 Eval Acc: 0.8597 (LR: 0.00100000)
[2025-06-13 19:11:45,854]: [ResNet20_parametrized_relu] Epoch: 030 Train Loss: 0.3619 Train Acc: 0.8748 Eval Loss: 0.4474 Eval Acc: 0.8525 (LR: 0.00100000)
[2025-06-13 19:12:52,049]: [ResNet20_parametrized_relu] Epoch: 031 Train Loss: 0.3548 Train Acc: 0.8757 Eval Loss: 0.4220 Eval Acc: 0.8631 (LR: 0.00100000)
[2025-06-13 19:13:50,017]: [ResNet20_parametrized_relu] Epoch: 032 Train Loss: 0.3499 Train Acc: 0.8784 Eval Loss: 0.4591 Eval Acc: 0.8523 (LR: 0.00100000)
[2025-06-13 19:14:48,547]: [ResNet20_parametrized_relu] Epoch: 033 Train Loss: 0.3509 Train Acc: 0.8765 Eval Loss: 0.4731 Eval Acc: 0.8430 (LR: 0.00100000)
[2025-06-13 19:15:54,202]: [ResNet20_parametrized_relu] Epoch: 034 Train Loss: 0.3392 Train Acc: 0.8826 Eval Loss: 0.4379 Eval Acc: 0.8557 (LR: 0.00100000)
[2025-06-13 19:17:07,154]: [ResNet20_parametrized_relu] Epoch: 035 Train Loss: 0.3409 Train Acc: 0.8805 Eval Loss: 0.4384 Eval Acc: 0.8576 (LR: 0.00010000)
[2025-06-13 19:17:58,801]: [ResNet20_parametrized_relu] Epoch: 036 Train Loss: 0.2763 Train Acc: 0.9044 Eval Loss: 0.3388 Eval Acc: 0.8879 (LR: 0.00010000)
[2025-06-13 19:18:51,010]: [ResNet20_parametrized_relu] Epoch: 037 Train Loss: 0.2551 Train Acc: 0.9130 Eval Loss: 0.3364 Eval Acc: 0.8890 (LR: 0.00010000)
[2025-06-13 19:19:46,316]: [ResNet20_parametrized_relu] Epoch: 038 Train Loss: 0.2474 Train Acc: 0.9132 Eval Loss: 0.3359 Eval Acc: 0.8907 (LR: 0.00010000)
[2025-06-13 19:20:46,417]: [ResNet20_parametrized_relu] Epoch: 039 Train Loss: 0.2395 Train Acc: 0.9148 Eval Loss: 0.3366 Eval Acc: 0.8917 (LR: 0.00010000)
[2025-06-13 19:22:08,823]: [ResNet20_parametrized_relu] Epoch: 040 Train Loss: 0.2332 Train Acc: 0.9191 Eval Loss: 0.3414 Eval Acc: 0.8909 (LR: 0.00010000)
[2025-06-13 19:23:22,895]: [ResNet20_parametrized_relu] Epoch: 041 Train Loss: 0.2309 Train Acc: 0.9200 Eval Loss: 0.3391 Eval Acc: 0.8900 (LR: 0.00010000)
[2025-06-13 19:24:36,154]: [ResNet20_parametrized_relu] Epoch: 042 Train Loss: 0.2250 Train Acc: 0.9203 Eval Loss: 0.3404 Eval Acc: 0.8897 (LR: 0.00010000)
[2025-06-13 19:25:57,231]: [ResNet20_parametrized_relu] Epoch: 043 Train Loss: 0.2258 Train Acc: 0.9205 Eval Loss: 0.3401 Eval Acc: 0.8897 (LR: 0.00010000)
[2025-06-13 19:27:18,095]: [ResNet20_parametrized_relu] Epoch: 044 Train Loss: 0.2239 Train Acc: 0.9225 Eval Loss: 0.3368 Eval Acc: 0.8924 (LR: 0.00001000)
[2025-06-13 19:28:18,958]: [ResNet20_parametrized_relu] Epoch: 045 Train Loss: 0.2152 Train Acc: 0.9250 Eval Loss: 0.3351 Eval Acc: 0.8936 (LR: 0.00001000)
[2025-06-13 19:29:13,619]: [ResNet20_parametrized_relu] Epoch: 046 Train Loss: 0.2115 Train Acc: 0.9254 Eval Loss: 0.3346 Eval Acc: 0.8931 (LR: 0.00001000)
[2025-06-13 19:30:15,712]: [ResNet20_parametrized_relu] Epoch: 047 Train Loss: 0.2144 Train Acc: 0.9251 Eval Loss: 0.3332 Eval Acc: 0.8941 (LR: 0.00001000)
[2025-06-13 19:31:17,535]: [ResNet20_parametrized_relu] Epoch: 048 Train Loss: 0.2151 Train Acc: 0.9240 Eval Loss: 0.3328 Eval Acc: 0.8946 (LR: 0.00001000)
[2025-06-13 19:32:18,377]: [ResNet20_parametrized_relu] Epoch: 049 Train Loss: 0.2116 Train Acc: 0.9269 Eval Loss: 0.3354 Eval Acc: 0.8938 (LR: 0.00001000)
[2025-06-13 19:33:17,133]: [ResNet20_parametrized_relu] Epoch: 050 Train Loss: 0.2114 Train Acc: 0.9262 Eval Loss: 0.3344 Eval Acc: 0.8941 (LR: 0.00001000)
[2025-06-13 19:34:20,970]: [ResNet20_parametrized_relu] Epoch: 051 Train Loss: 0.2076 Train Acc: 0.9275 Eval Loss: 0.3337 Eval Acc: 0.8935 (LR: 0.00001000)
[2025-06-13 19:35:22,105]: [ResNet20_parametrized_relu] Epoch: 052 Train Loss: 0.2085 Train Acc: 0.9278 Eval Loss: 0.3355 Eval Acc: 0.8940 (LR: 0.00001000)
[2025-06-13 19:36:27,024]: [ResNet20_parametrized_relu] Epoch: 053 Train Loss: 0.2044 Train Acc: 0.9294 Eval Loss: 0.3312 Eval Acc: 0.8947 (LR: 0.00001000)
[2025-06-13 19:37:28,813]: [ResNet20_parametrized_relu] Epoch: 054 Train Loss: 0.2098 Train Acc: 0.9279 Eval Loss: 0.3326 Eval Acc: 0.8949 (LR: 0.00001000)
[2025-06-13 19:38:20,372]: [ResNet20_parametrized_relu] Epoch: 055 Train Loss: 0.2067 Train Acc: 0.9297 Eval Loss: 0.3313 Eval Acc: 0.8954 (LR: 0.00001000)
[2025-06-13 19:39:13,111]: [ResNet20_parametrized_relu] Epoch: 056 Train Loss: 0.2141 Train Acc: 0.9255 Eval Loss: 0.3329 Eval Acc: 0.8950 (LR: 0.00001000)
[2025-06-13 19:40:11,772]: [ResNet20_parametrized_relu] Epoch: 057 Train Loss: 0.2112 Train Acc: 0.9267 Eval Loss: 0.3326 Eval Acc: 0.8939 (LR: 0.00001000)
[2025-06-13 19:41:16,832]: [ResNet20_parametrized_relu] Epoch: 058 Train Loss: 0.2093 Train Acc: 0.9273 Eval Loss: 0.3350 Eval Acc: 0.8956 (LR: 0.00001000)
[2025-06-13 19:42:18,116]: [ResNet20_parametrized_relu] Epoch: 059 Train Loss: 0.2081 Train Acc: 0.9276 Eval Loss: 0.3348 Eval Acc: 0.8946 (LR: 0.00000100)
[2025-06-13 19:43:11,019]: [ResNet20_parametrized_relu] Epoch: 060 Train Loss: 0.2046 Train Acc: 0.9274 Eval Loss: 0.3358 Eval Acc: 0.8944 (LR: 0.00000100)
[2025-06-13 19:44:07,919]: [ResNet20_parametrized_relu] Epoch: 061 Train Loss: 0.2069 Train Acc: 0.9288 Eval Loss: 0.3335 Eval Acc: 0.8951 (LR: 0.00000100)
[2025-06-13 19:45:11,911]: [ResNet20_parametrized_relu] Epoch: 062 Train Loss: 0.2089 Train Acc: 0.9270 Eval Loss: 0.3367 Eval Acc: 0.8944 (LR: 0.00000100)
[2025-06-13 19:46:14,497]: [ResNet20_parametrized_relu] Epoch: 063 Train Loss: 0.2073 Train Acc: 0.9285 Eval Loss: 0.3343 Eval Acc: 0.8958 (LR: 0.00000100)
[2025-06-13 19:47:13,293]: [ResNet20_parametrized_relu] Epoch: 064 Train Loss: 0.2065 Train Acc: 0.9284 Eval Loss: 0.3357 Eval Acc: 0.8938 (LR: 0.00000100)
[2025-06-13 19:48:04,521]: [ResNet20_parametrized_relu] Epoch: 065 Train Loss: 0.2052 Train Acc: 0.9293 Eval Loss: 0.3356 Eval Acc: 0.8949 (LR: 0.00000010)
[2025-06-13 19:48:59,329]: [ResNet20_parametrized_relu] Epoch: 066 Train Loss: 0.2057 Train Acc: 0.9282 Eval Loss: 0.3357 Eval Acc: 0.8936 (LR: 0.00000010)
[2025-06-13 19:49:58,767]: [ResNet20_parametrized_relu] Epoch: 067 Train Loss: 0.2079 Train Acc: 0.9272 Eval Loss: 0.3343 Eval Acc: 0.8948 (LR: 0.00000010)
[2025-06-13 19:51:00,091]: [ResNet20_parametrized_relu] Epoch: 068 Train Loss: 0.2058 Train Acc: 0.9274 Eval Loss: 0.3345 Eval Acc: 0.8953 (LR: 0.00000010)
[2025-06-13 19:52:00,048]: [ResNet20_parametrized_relu] Epoch: 069 Train Loss: 0.2040 Train Acc: 0.9281 Eval Loss: 0.3354 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 19:52:50,888]: [ResNet20_parametrized_relu] Epoch: 070 Train Loss: 0.2070 Train Acc: 0.9277 Eval Loss: 0.3359 Eval Acc: 0.8937 (LR: 0.00000010)
[2025-06-13 19:53:44,543]: [ResNet20_parametrized_relu] Epoch: 071 Train Loss: 0.2085 Train Acc: 0.9258 Eval Loss: 0.3359 Eval Acc: 0.8948 (LR: 0.00000010)
[2025-06-13 19:54:44,273]: [ResNet20_parametrized_relu] Epoch: 072 Train Loss: 0.2052 Train Acc: 0.9291 Eval Loss: 0.3344 Eval Acc: 0.8940 (LR: 0.00000010)
[2025-06-13 19:55:45,544]: [ResNet20_parametrized_relu] Epoch: 073 Train Loss: 0.2068 Train Acc: 0.9281 Eval Loss: 0.3347 Eval Acc: 0.8953 (LR: 0.00000010)
[2025-06-13 19:56:40,198]: [ResNet20_parametrized_relu] Epoch: 074 Train Loss: 0.2068 Train Acc: 0.9278 Eval Loss: 0.3356 Eval Acc: 0.8947 (LR: 0.00000010)
[2025-06-13 19:57:28,837]: [ResNet20_parametrized_relu] Epoch: 075 Train Loss: 0.2071 Train Acc: 0.9270 Eval Loss: 0.3354 Eval Acc: 0.8941 (LR: 0.00000010)
[2025-06-13 19:58:24,553]: [ResNet20_parametrized_relu] Epoch: 076 Train Loss: 0.2056 Train Acc: 0.9293 Eval Loss: 0.3331 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 19:59:25,923]: [ResNet20_parametrized_relu] Epoch: 077 Train Loss: 0.2045 Train Acc: 0.9288 Eval Loss: 0.3359 Eval Acc: 0.8952 (LR: 0.00000010)
[2025-06-13 20:00:28,225]: [ResNet20_parametrized_relu] Epoch: 078 Train Loss: 0.2026 Train Acc: 0.9285 Eval Loss: 0.3347 Eval Acc: 0.8949 (LR: 0.00000010)
[2025-06-13 20:01:23,859]: [ResNet20_parametrized_relu] Epoch: 079 Train Loss: 0.2098 Train Acc: 0.9267 Eval Loss: 0.3364 Eval Acc: 0.8951 (LR: 0.00000010)
[2025-06-13 20:02:14,169]: [ResNet20_parametrized_relu] Epoch: 080 Train Loss: 0.2071 Train Acc: 0.9276 Eval Loss: 0.3353 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 20:03:07,591]: [ResNet20_parametrized_relu] Epoch: 081 Train Loss: 0.2073 Train Acc: 0.9272 Eval Loss: 0.3340 Eval Acc: 0.8965 (LR: 0.00000010)
[2025-06-13 20:04:06,894]: [ResNet20_parametrized_relu] Epoch: 082 Train Loss: 0.2077 Train Acc: 0.9266 Eval Loss: 0.3364 Eval Acc: 0.8938 (LR: 0.00000010)
[2025-06-13 20:05:05,997]: [ResNet20_parametrized_relu] Epoch: 083 Train Loss: 0.2075 Train Acc: 0.9283 Eval Loss: 0.3358 Eval Acc: 0.8943 (LR: 0.00000010)
[2025-06-13 20:05:50,234]: [ResNet20_parametrized_relu] Epoch: 084 Train Loss: 0.2049 Train Acc: 0.9297 Eval Loss: 0.3342 Eval Acc: 0.8951 (LR: 0.00000010)
[2025-06-13 20:06:34,238]: [ResNet20_parametrized_relu] Epoch: 085 Train Loss: 0.2068 Train Acc: 0.9283 Eval Loss: 0.3342 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-06-13 20:07:18,128]: [ResNet20_parametrized_relu] Epoch: 086 Train Loss: 0.2072 Train Acc: 0.9278 Eval Loss: 0.3362 Eval Acc: 0.8943 (LR: 0.00000010)
[2025-06-13 20:08:02,205]: [ResNet20_parametrized_relu] Epoch: 087 Train Loss: 0.2060 Train Acc: 0.9286 Eval Loss: 0.3346 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 20:08:43,368]: [ResNet20_parametrized_relu] Epoch: 088 Train Loss: 0.2071 Train Acc: 0.9283 Eval Loss: 0.3367 Eval Acc: 0.8929 (LR: 0.00000010)
[2025-06-13 20:09:27,761]: [ResNet20_parametrized_relu] Epoch: 089 Train Loss: 0.2078 Train Acc: 0.9277 Eval Loss: 0.3329 Eval Acc: 0.8955 (LR: 0.00000010)
[2025-06-13 20:10:08,467]: [ResNet20_parametrized_relu] Epoch: 090 Train Loss: 0.2049 Train Acc: 0.9285 Eval Loss: 0.3336 Eval Acc: 0.8948 (LR: 0.00000010)
[2025-06-13 20:10:52,580]: [ResNet20_parametrized_relu] Epoch: 091 Train Loss: 0.2048 Train Acc: 0.9276 Eval Loss: 0.3343 Eval Acc: 0.8955 (LR: 0.00000010)
[2025-06-13 20:11:33,883]: [ResNet20_parametrized_relu] Epoch: 092 Train Loss: 0.2064 Train Acc: 0.9277 Eval Loss: 0.3342 Eval Acc: 0.8947 (LR: 0.00000010)
[2025-06-13 20:12:17,977]: [ResNet20_parametrized_relu] Epoch: 093 Train Loss: 0.2042 Train Acc: 0.9282 Eval Loss: 0.3353 Eval Acc: 0.8956 (LR: 0.00000010)
[2025-06-13 20:13:01,455]: [ResNet20_parametrized_relu] Epoch: 094 Train Loss: 0.2066 Train Acc: 0.9278 Eval Loss: 0.3350 Eval Acc: 0.8942 (LR: 0.00000010)
[2025-06-13 20:13:45,601]: [ResNet20_parametrized_relu] Epoch: 095 Train Loss: 0.2062 Train Acc: 0.9276 Eval Loss: 0.3357 Eval Acc: 0.8940 (LR: 0.00000010)
[2025-06-13 20:14:29,599]: [ResNet20_parametrized_relu] Epoch: 096 Train Loss: 0.2065 Train Acc: 0.9274 Eval Loss: 0.3336 Eval Acc: 0.8955 (LR: 0.00000010)
[2025-06-13 20:15:14,568]: [ResNet20_parametrized_relu] Epoch: 097 Train Loss: 0.2070 Train Acc: 0.9275 Eval Loss: 0.3346 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-06-13 20:16:01,900]: [ResNet20_parametrized_relu] Epoch: 098 Train Loss: 0.2053 Train Acc: 0.9281 Eval Loss: 0.3357 Eval Acc: 0.8946 (LR: 0.00000010)
[2025-06-13 20:16:43,804]: [ResNet20_parametrized_relu] Epoch: 099 Train Loss: 0.2051 Train Acc: 0.9276 Eval Loss: 0.3351 Eval Acc: 0.8954 (LR: 0.00000010)
[2025-06-13 20:17:28,349]: [ResNet20_parametrized_relu] Epoch: 100 Train Loss: 0.2068 Train Acc: 0.9284 Eval Loss: 0.3338 Eval Acc: 0.8949 (LR: 0.00000010)
[2025-06-13 20:17:28,349]: [ResNet20_parametrized_relu] Best Eval Accuracy: 0.8965
[2025-06-13 20:17:28,675]: 
Training of full-precision model finished!
[2025-06-13 20:17:28,675]: Model Architecture:
[2025-06-13 20:17:28,800]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 20:17:28,834]: 
Model Weights:
[2025-06-13 20:17:28,834]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-13 20:17:29,125]: Sample Values (25 elements): [0.1067488044500351, 0.25428593158721924, 0.04976958781480789, -0.1391768604516983, 0.10856697708368301, 0.15545284748077393, 0.07285924255847931, -0.03122006170451641, 0.043388333171606064, -0.27053335309028625, -0.25400590896606445, -0.23759350180625916, 0.05130861699581146, -0.0608731284737587, 0.05777660757303238, 0.16507205367088318, -0.10543083399534225, 0.07007724791765213, -0.21153730154037476, -0.19017282128334045, -0.027903474867343903, -0.10707729309797287, -0.07333295047283173, -0.07431307435035706, -0.21832717955112457]
[2025-06-13 20:17:29,144]: Mean: -0.00091038
[2025-06-13 20:17:29,163]: Min: -0.48560286
[2025-06-13 20:17:29,178]: Max: 0.48041451
[2025-06-13 20:17:29,178]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,180]: Sample Values (16 elements): [0.968957781791687, 0.936716616153717, 1.0089060068130493, 0.908546507358551, 0.7584648132324219, 0.9059939980506897, 0.9062372446060181, 0.9184964299201965, 0.9874240159988403, 0.9070501923561096, 0.9189741015434265, 0.9082505106925964, 0.6684331893920898, 0.8275966048240662, 0.7917234301567078, 0.8075250387191772]
[2025-06-13 20:17:29,181]: Mean: 0.88308108
[2025-06-13 20:17:29,181]: Min: 0.66843319
[2025-06-13 20:17:29,181]: Max: 1.00890601
[2025-06-13 20:17:29,181]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:17:29,182]: Sample Values (25 elements): [0.08461505174636841, -0.23089514672756195, 0.011863034218549728, -0.011474494822323322, -0.09933702647686005, -0.03093235194683075, 0.036984432488679886, 0.047708023339509964, -0.08693955093622208, -0.032874271273612976, 0.08284949511289597, -0.06879673153162003, -0.15041151642799377, 0.2725239396095276, -0.039041049778461456, 0.0033230294939130545, -0.028266699984669685, 0.09743060171604156, 0.02971319481730461, 0.0691307783126831, 0.05863402411341667, 0.12818384170532227, -0.0047195241786539555, -0.04588526114821434, -0.06275258213281631]
[2025-06-13 20:17:29,182]: Mean: -0.01088321
[2025-06-13 20:17:29,183]: Min: -0.44570777
[2025-06-13 20:17:29,183]: Max: 0.39290354
[2025-06-13 20:17:29,183]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,183]: Sample Values (16 elements): [0.7783845067024231, 0.7803898453712463, 0.8131895661354065, 0.9105955362319946, 0.7719273567199707, 0.8799302577972412, 0.9215510487556458, 0.990725040435791, 0.8460538983345032, 1.0574045181274414, 0.8634337782859802, 0.67652827501297, 0.7807523012161255, 0.930715799331665, 0.7819332480430603, 0.7843421101570129]
[2025-06-13 20:17:29,183]: Mean: 0.84799105
[2025-06-13 20:17:29,184]: Min: 0.67652828
[2025-06-13 20:17:29,184]: Max: 1.05740452
[2025-06-13 20:17:29,184]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:17:29,184]: Sample Values (25 elements): [0.03637576475739479, -0.01693468727171421, 0.1578199714422226, -0.10989217460155487, -0.0834188312292099, -0.08286531269550323, -0.005045044701546431, -0.11618281155824661, -0.06413841992616653, -0.11947407573461533, 0.06776425242424011, 0.09163686633110046, -0.0018648913828656077, 0.04631051421165466, -0.049731288105249405, -0.11835898458957672, -0.02556830272078514, -0.015327895991504192, -0.08359367400407791, -0.18296295404434204, 0.02389759011566639, -0.12238770723342896, 0.06197173520922661, -0.17961402237415314, -0.007529685273766518]
[2025-06-13 20:17:29,185]: Mean: -0.00744500
[2025-06-13 20:17:29,185]: Min: -0.38682497
[2025-06-13 20:17:29,185]: Max: 0.49516007
[2025-06-13 20:17:29,185]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,185]: Sample Values (16 elements): [0.7055186033248901, 0.8060411214828491, 0.8096676468849182, 0.9197766184806824, 0.8017814755439758, 0.8433328866958618, 0.8858973383903503, 0.849908173084259, 0.7549711465835571, 0.7950726747512817, 0.8036367297172546, 0.7832070589065552, 1.0465055704116821, 0.9473091959953308, 0.7847628593444824, 0.6871052384376526]
[2025-06-13 20:17:29,186]: Mean: 0.82653093
[2025-06-13 20:17:29,186]: Min: 0.68710524
[2025-06-13 20:17:29,186]: Max: 1.04650557
[2025-06-13 20:17:29,186]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:17:29,187]: Sample Values (25 elements): [-0.08862664550542831, 0.023832743987441063, -0.10629529505968094, -0.01723586954176426, -0.05957107990980148, 0.030047588050365448, -0.014790517278015614, 0.20587578415870667, 0.002273957245051861, -0.04548758640885353, 0.03738244250416756, -0.1361497938632965, 0.03162183612585068, -0.07280608266592026, -0.14508157968521118, -0.14250148832798004, 0.04100203886628151, -0.15090934932231903, 0.19627928733825684, 0.009674199856817722, -0.01675783097743988, 0.2546953856945038, -0.006660038605332375, -0.1638031005859375, -0.04537747800350189]
[2025-06-13 20:17:29,187]: Mean: -0.00869492
[2025-06-13 20:17:29,187]: Min: -0.41275409
[2025-06-13 20:17:29,187]: Max: 0.39912033
[2025-06-13 20:17:29,187]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,188]: Sample Values (16 elements): [0.8549959659576416, 0.7356809377670288, 0.8843116164207458, 1.1048277616500854, 0.8000420331954956, 0.8574069142341614, 0.9130880236625671, 0.8705633878707886, 0.7914199829101562, 0.8125285506248474, 0.7284436821937561, 0.7058939933776855, 0.7944570183753967, 0.8182823061943054, 0.924776017665863, 0.7991659045219421]
[2025-06-13 20:17:29,188]: Mean: 0.83724278
[2025-06-13 20:17:29,188]: Min: 0.70589399
[2025-06-13 20:17:29,188]: Max: 1.10482776
[2025-06-13 20:17:29,189]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:17:29,189]: Sample Values (25 elements): [0.05585942417383194, -0.02630675584077835, -0.03732787072658539, 0.18600943684577942, -0.08845912665128708, -0.013484706170856953, -0.2317730039358139, -0.08263104408979416, 0.07682383805513382, -0.20028680562973022, 0.02367032878100872, -0.12190600484609604, 0.10066622495651245, 0.08892156183719635, -0.2270233929157257, 0.16913679242134094, 0.17169132828712463, 0.06997428089380264, 0.22871074080467224, 0.14404334127902985, 0.14757785201072693, -0.035113222897052765, 0.08403483778238297, -0.05329657718539238, -0.06450686603784561]
[2025-06-13 20:17:29,189]: Mean: -0.00193423
[2025-06-13 20:17:29,189]: Min: -0.52278119
[2025-06-13 20:17:29,190]: Max: 0.38324273
[2025-06-13 20:17:29,190]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,190]: Sample Values (16 elements): [0.7740052938461304, 0.6818972229957581, 0.6590186357498169, 0.7965243458747864, 0.9361626505851746, 0.9130762219429016, 0.6623981595039368, 0.6032522916793823, 0.7286701202392578, 0.7189255952835083, 0.7653128504753113, 0.8612355589866638, 0.8237065076828003, 0.6362219452857971, 0.7916839718818665, 0.6746975183486938]
[2025-06-13 20:17:29,190]: Mean: 0.75167435
[2025-06-13 20:17:29,190]: Min: 0.60325229
[2025-06-13 20:17:29,191]: Max: 0.93616265
[2025-06-13 20:17:29,191]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:17:29,191]: Sample Values (25 elements): [0.3279731869697571, -0.1579127013683319, 0.07121699303388596, 0.10281579196453094, 0.03325962647795677, -0.056272320449352264, 0.08053907752037048, 0.2829531729221344, 0.0022576777264475822, 0.05842907726764679, -0.03747700899839401, -0.07462917268276215, 0.07178448140621185, -0.13249893486499786, -0.2828579246997833, -0.03425319492816925, -0.10236285626888275, 0.10238200426101685, -0.010936780832707882, 0.013753842562437057, -0.0858427882194519, -0.1299685835838318, -0.06507887691259384, 0.06604909896850586, 0.05228717625141144]
[2025-06-13 20:17:29,191]: Mean: -0.00353562
[2025-06-13 20:17:29,192]: Min: -0.46927100
[2025-06-13 20:17:29,192]: Max: 0.57049423
[2025-06-13 20:17:29,192]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,192]: Sample Values (16 elements): [0.7444087266921997, 0.6273875832557678, 0.9985221028327942, 1.138839602470398, 0.8627943992614746, 0.7674591541290283, 0.8839742541313171, 0.6436029076576233, 0.9045183062553406, 0.8729280829429626, 0.87357497215271, 0.6952399611473083, 1.3091825246810913, 0.6154391169548035, 0.8916791677474976, 0.8418294787406921]
[2025-06-13 20:17:29,192]: Mean: 0.85446125
[2025-06-13 20:17:29,193]: Min: 0.61543912
[2025-06-13 20:17:29,193]: Max: 1.30918252
[2025-06-13 20:17:29,193]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 20:17:29,193]: Sample Values (25 elements): [0.22113405168056488, 0.21429568529129028, 0.05191617086529732, -0.05785762891173363, 0.062366027384996414, 0.08757275342941284, -0.010312026366591454, 0.08803726732730865, -0.006003738846629858, 0.03684241324663162, 0.017535358667373657, -0.12436452507972717, -0.017644153907895088, -0.08794249594211578, -0.005737450905144215, 0.05822271853685379, 0.058742422610521317, -0.07488596439361572, -0.3443412780761719, 0.03191675618290901, 0.18805699050426483, -0.10177777707576752, -0.1085476279258728, -0.07608763128519058, 0.028694530948996544]
[2025-06-13 20:17:29,193]: Mean: -0.00497013
[2025-06-13 20:17:29,194]: Min: -0.53674173
[2025-06-13 20:17:29,194]: Max: 0.38009736
[2025-06-13 20:17:29,194]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-13 20:17:29,194]: Sample Values (16 elements): [0.6454048752784729, 0.8909807801246643, 0.9171702265739441, 0.8445619940757751, 0.8890946507453918, 0.6775560975074768, 1.2290270328521729, 0.7243120074272156, 0.6067250370979309, 0.9863712787628174, 0.75583416223526, 0.8399744629859924, 1.0931968688964844, 0.9115357398986816, 0.6447908282279968, 0.9362316727638245]
[2025-06-13 20:17:29,195]: Mean: 0.84954798
[2025-06-13 20:17:29,195]: Min: 0.60672504
[2025-06-13 20:17:29,195]: Max: 1.22902703
[2025-06-13 20:17:29,195]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-13 20:17:29,195]: Sample Values (25 elements): [0.13063332438468933, -0.03733501210808754, -0.15570704638957977, 0.019757244735956192, -0.25569280982017517, 0.039177972823381424, -0.031353484839200974, 0.03475586324930191, 0.019114067777991295, -0.10406430065631866, -0.1364823430776596, -0.026798676699399948, 0.034169621765613556, 0.20904743671417236, -0.08679933100938797, 0.11665943264961243, -0.07915449142456055, -0.16360127925872803, 0.12617532908916473, 0.056283656507730484, 0.13908639550209045, 0.015538630075752735, 0.11916962265968323, -0.16228465735912323, -0.17845559120178223]
[2025-06-13 20:17:29,196]: Mean: -0.00502885
[2025-06-13 20:17:29,196]: Min: -0.42822361
[2025-06-13 20:17:29,196]: Max: 0.37077588
[2025-06-13 20:17:29,196]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,196]: Sample Values (25 elements): [0.7611072659492493, 0.7889547944068909, 1.0052151679992676, 0.7166484594345093, 0.8356148600578308, 0.8831023573875427, 0.7395138144493103, 0.7854520678520203, 0.9176831245422363, 0.7131494879722595, 0.7945278882980347, 0.754127025604248, 0.8424067497253418, 0.8217408061027527, 0.8051640391349792, 0.8981090188026428, 0.8467356562614441, 0.7150640487670898, 0.9072235226631165, 0.8582364320755005, 0.7050371170043945, 0.6533177495002747, 0.7618129849433899, 0.8123615384101868, 0.7923913598060608]
[2025-06-13 20:17:29,197]: Mean: 0.82013857
[2025-06-13 20:17:29,197]: Min: 0.65331775
[2025-06-13 20:17:29,197]: Max: 1.00521517
[2025-06-13 20:17:29,197]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:17:29,198]: Sample Values (25 elements): [-0.09408695250749588, -0.052316758781671524, 0.09934844821691513, 0.02648962289094925, 0.15992696583271027, -0.12971320748329163, 0.07415013760328293, -0.024456536397337914, -0.047064945101737976, 0.07605905085802078, 0.028724592179059982, -0.005630039144307375, 0.009046440944075584, -0.08771007508039474, 0.05392588675022125, 0.00608636112883687, 0.16372013092041016, -0.02775983326137066, 0.10336515307426453, -0.038335129618644714, 0.1269414871931076, 0.05696416273713112, 0.03904194384813309, 0.03371763229370117, -0.12828417122364044]
[2025-06-13 20:17:29,198]: Mean: -0.00058713
[2025-06-13 20:17:29,198]: Min: -0.38908020
[2025-06-13 20:17:29,198]: Max: 0.34921288
[2025-06-13 20:17:29,198]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,199]: Sample Values (25 elements): [0.8581005334854126, 0.9249609708786011, 0.9155518412590027, 1.009184718132019, 0.8765222430229187, 0.820214033126831, 0.8415201902389526, 0.8660762310028076, 0.9340314269065857, 0.8405782580375671, 0.8008824586868286, 0.982746958732605, 0.9331682920455933, 1.0073447227478027, 0.9531744122505188, 0.8350614309310913, 0.9712673425674438, 0.9560443162918091, 0.8569356203079224, 0.8445734977722168, 0.8602586984634399, 0.8970665335655212, 0.8562713265419006, 0.8043603301048279, 0.8689683675765991]
[2025-06-13 20:17:29,199]: Mean: 0.88039339
[2025-06-13 20:17:29,199]: Min: 0.76472074
[2025-06-13 20:17:29,200]: Max: 1.00918472
[2025-06-13 20:17:29,200]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-13 20:17:29,200]: Sample Values (25 elements): [-0.10171115398406982, -0.04060772433876991, 0.14036165177822113, 0.06003018096089363, -0.2305673509836197, -0.16746263206005096, -0.14711341261863708, 0.2777473032474518, 0.28863605856895447, 0.05411396920681, 0.11882348358631134, -0.05748261138796806, -0.1928890496492386, -0.06385485827922821, -0.05003334581851959, -0.16748332977294922, -0.0794442892074585, 0.016883477568626404, 0.07178771495819092, 0.08487173914909363, 0.03452254459261894, 0.18651308119297028, -0.02007710188627243, -0.034247349947690964, 0.03856023773550987]
[2025-06-13 20:17:29,200]: Mean: 0.00370500
[2025-06-13 20:17:29,200]: Min: -0.44485474
[2025-06-13 20:17:29,201]: Max: 0.44740924
[2025-06-13 20:17:29,201]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,201]: Sample Values (25 elements): [0.7318719029426575, 0.9157834649085999, 0.7303686738014221, 0.6991510391235352, 0.7520686984062195, 0.5735045075416565, 0.5938790440559387, 0.7250317931175232, 0.5069108605384827, 0.6769416332244873, 0.7911640405654907, 0.5742673277854919, 0.6127366423606873, 0.47421371936798096, 0.8085516691207886, 0.631141722202301, 0.5778058171272278, 0.672719419002533, 0.6655994653701782, 0.6473667025566101, 0.8427590727806091, 0.9268669486045837, 0.6258588433265686, 0.7612675428390503, 0.5981629490852356]
[2025-06-13 20:17:29,201]: Mean: 0.67680776
[2025-06-13 20:17:29,201]: Min: 0.47421372
[2025-06-13 20:17:29,202]: Max: 0.92686695
[2025-06-13 20:17:29,202]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:17:29,202]: Sample Values (25 elements): [-0.032574303448200226, 0.07449057698249817, -0.23432676494121552, -0.03721797466278076, -0.004814245272427797, 0.011483483947813511, 0.03001520410180092, -0.11826027929782867, -0.006769876461476088, -0.11662673950195312, 0.06216637045145035, -0.07668165862560272, -0.15579068660736084, 0.08907494693994522, -0.012838638387620449, 0.08624473214149475, 0.26339101791381836, 0.03127880394458771, -0.03023085743188858, -0.002114369533956051, -0.22979333996772766, 0.015380216762423515, 0.0907900482416153, 0.0022025711368769407, -0.03146388381719589]
[2025-06-13 20:17:29,203]: Mean: -0.01072054
[2025-06-13 20:17:29,203]: Min: -0.36548689
[2025-06-13 20:17:29,203]: Max: 0.33396554
[2025-06-13 20:17:29,203]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,203]: Sample Values (25 elements): [0.8713682889938354, 0.7855319976806641, 0.840811014175415, 0.7548495531082153, 0.7120166420936584, 0.9169097542762756, 0.7846432328224182, 0.8302103281021118, 0.7391116619110107, 0.7980608940124512, 0.8484265804290771, 0.7724243998527527, 0.9018241763114929, 0.8298128843307495, 0.791714072227478, 0.76591956615448, 0.8476053476333618, 0.7435512542724609, 0.8259666562080383, 0.8435680866241455, 0.7916241884231567, 0.729154646396637, 0.8041064739227295, 1.0655884742736816, 0.7388424277305603]
[2025-06-13 20:17:29,204]: Mean: 0.80217695
[2025-06-13 20:17:29,204]: Min: 0.67029899
[2025-06-13 20:17:29,204]: Max: 1.06558847
[2025-06-13 20:17:29,204]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:17:29,205]: Sample Values (25 elements): [0.09364793449640274, -0.05583644658327103, 0.027769839391112328, -0.17961323261260986, -0.009530466981232166, 0.11525778472423553, 0.049522362649440765, 0.02980988286435604, 0.020189741626381874, 0.0027409372851252556, -0.02016771212220192, -0.041199441999197006, 0.06051439791917801, -0.017611384391784668, -0.1444341242313385, -0.09432496875524521, 0.0031140721403062344, -0.0929187461733818, 0.10471347719430923, -0.0088002048432827, 0.09027845412492752, 0.0700923502445221, 0.09139883518218994, -0.02149597555398941, -0.08654133230447769]
[2025-06-13 20:17:29,205]: Mean: -0.00429995
[2025-06-13 20:17:29,205]: Min: -0.35959092
[2025-06-13 20:17:29,206]: Max: 0.33456701
[2025-06-13 20:17:29,206]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,206]: Sample Values (25 elements): [0.8158029913902283, 0.8669784665107727, 0.7794460654258728, 0.7150508761405945, 0.738329291343689, 0.8431593775749207, 0.7846163511276245, 0.8046596646308899, 0.8190954327583313, 0.8050405979156494, 0.682482898235321, 0.8241878747940063, 0.6695289015769958, 0.7220104932785034, 0.8669596910476685, 0.7326217293739319, 0.8259400129318237, 0.6769998669624329, 0.7333579063415527, 0.7210951447486877, 0.7946059107780457, 0.7046336531639099, 0.6911655068397522, 0.8646803498268127, 0.825951874256134]
[2025-06-13 20:17:29,206]: Mean: 0.76986736
[2025-06-13 20:17:29,206]: Min: 0.66952890
[2025-06-13 20:17:29,207]: Max: 0.86697847
[2025-06-13 20:17:29,207]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:17:29,207]: Sample Values (25 elements): [0.06907746195793152, 0.08249133825302124, 0.0693691298365593, 0.008662017993628979, 0.08165192604064941, -0.07030075788497925, -0.0448574461042881, -0.3646237552165985, 0.02535416930913925, -0.17480702698230743, 0.0033140196464955807, -0.03842023015022278, 0.0009537246078252792, -0.05844464525580406, 0.0366109237074852, -0.011776713654398918, 0.0037697849329560995, -0.060975734144449234, 0.19761104881763458, 0.0033729183487594128, -0.08074508607387543, 0.12207994610071182, 0.062642902135849, 0.13978247344493866, -0.026303982362151146]
[2025-06-13 20:17:29,207]: Mean: -0.01220787
[2025-06-13 20:17:29,208]: Min: -0.36586824
[2025-06-13 20:17:29,208]: Max: 0.34019956
[2025-06-13 20:17:29,208]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,208]: Sample Values (25 elements): [0.6537410020828247, 0.7069435119628906, 0.8318207859992981, 0.6694386005401611, 0.6640002727508545, 0.7955562472343445, 0.71867835521698, 0.7558199167251587, 0.6718975901603699, 0.7457608580589294, 0.7864078879356384, 0.7813054919242859, 0.6936066150665283, 0.6853960156440735, 0.7744637131690979, 0.9132615923881531, 0.7708762884140015, 0.6615451574325562, 0.7607037425041199, 0.846454381942749, 0.8240602612495422, 0.7748081088066101, 0.8475902080535889, 0.751071572303772, 0.7906938195228577]
[2025-06-13 20:17:29,209]: Mean: 0.76493210
[2025-06-13 20:17:29,209]: Min: 0.65374100
[2025-06-13 20:17:29,209]: Max: 0.91326159
[2025-06-13 20:17:29,209]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 20:17:29,210]: Sample Values (25 elements): [0.05395814776420593, 0.1251889169216156, 0.19013556838035583, -0.10486268252134323, -0.08488613367080688, -0.005672129336744547, -0.006573448423296213, 0.0512254536151886, 0.1226775199174881, 0.07209857553243637, 0.013326913118362427, 0.0012022490845993161, 0.0897524282336235, -0.044311970472335815, 0.11035963147878647, 0.09390486776828766, -0.22681008279323578, 0.03474126383662224, 0.06926622986793518, 0.005697053391486406, -0.032740816473960876, -0.08815783262252808, -0.09110783040523529, -0.13791371881961823, -0.02754989266395569]
[2025-06-13 20:17:29,210]: Mean: -0.00391353
[2025-06-13 20:17:29,210]: Min: -0.34730130
[2025-06-13 20:17:29,210]: Max: 0.42612395
[2025-06-13 20:17:29,210]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-13 20:17:29,211]: Sample Values (25 elements): [0.6471714377403259, 0.7848259210586548, 0.6589125394821167, 0.7486064434051514, 0.6824498176574707, 1.0594396591186523, 0.7412205338478088, 0.6985143423080444, 0.7005650997161865, 0.6376149654388428, 0.6409434676170349, 0.626636266708374, 0.8187866806983948, 0.7290129661560059, 0.6204688549041748, 0.7681659460067749, 0.7371465563774109, 0.6839873194694519, 0.8035019040107727, 0.7690637707710266, 0.8058730959892273, 0.8027852177619934, 0.7852731347084045, 0.7968872785568237, 0.7572647333145142]
[2025-06-13 20:17:29,211]: Mean: 0.73513800
[2025-06-13 20:17:29,211]: Min: 0.62046885
[2025-06-13 20:17:29,211]: Max: 1.05943966
[2025-06-13 20:17:29,211]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-13 20:17:29,212]: Sample Values (25 elements): [0.19493438303470612, 0.1323569267988205, -0.17601588368415833, 0.07086975127458572, -0.007033697795122862, -0.11747937649488449, 0.15958262979984283, 0.031234584748744965, -0.14993658661842346, -0.0005953258369117975, 0.11363819986581802, 0.11732993274927139, 0.031737565994262695, -0.14772331714630127, -0.09490081667900085, -0.10726866126060486, -0.02031089924275875, 0.07598274946212769, 0.08607085049152374, 0.1179039478302002, -0.08758986741304398, -0.04075741767883301, 0.047252923250198364, 0.050403282046318054, 0.06577258557081223]
[2025-06-13 20:17:29,212]: Mean: -0.00549009
[2025-06-13 20:17:29,212]: Min: -0.35043901
[2025-06-13 20:17:29,213]: Max: 0.37497956
[2025-06-13 20:17:29,213]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,218]: Sample Values (25 elements): [0.6780036091804504, 0.7464636564254761, 0.816504955291748, 0.7421135306358337, 0.8222349286079407, 0.8629436492919922, 0.6730754971504211, 0.7864071726799011, 0.9991430640220642, 0.7136902809143066, 0.7552513480186462, 0.8473454713821411, 0.7993249297142029, 0.7778199315071106, 0.7702020406723022, 0.8000746369361877, 0.8675361275672913, 0.7717208862304688, 0.7005711197853088, 0.7963056564331055, 0.7640373706817627, 0.8163114190101624, 0.8372896313667297, 0.666761577129364, 0.7865822911262512]
[2025-06-13 20:17:29,220]: Mean: 0.74796134
[2025-06-13 20:17:29,222]: Min: 0.64456052
[2025-06-13 20:17:29,224]: Max: 0.99914306
[2025-06-13 20:17:29,224]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:17:29,229]: Sample Values (25 elements): [0.009955364279448986, 0.008436114527285099, -0.06364491581916809, 0.08756884932518005, 0.08303838223218918, 0.02979631908237934, -0.08050739020109177, 0.061852503567934036, -0.09443199634552002, 0.10677558928728104, 0.13796184957027435, -0.005076319444924593, -0.06731412559747696, -0.029498089104890823, 0.06041661649942398, 0.011831089854240417, -0.02374950982630253, 0.009107645601034164, -0.011973272077739239, -0.06464722752571106, 0.02563348039984703, 0.09195388853549957, 0.10444668680429459, 0.02885851450264454, -0.11784374713897705]
[2025-06-13 20:17:29,231]: Mean: -0.00544963
[2025-06-13 20:17:29,235]: Min: -0.35998449
[2025-06-13 20:17:29,240]: Max: 0.32287812
[2025-06-13 20:17:29,240]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,249]: Sample Values (25 elements): [0.8517513871192932, 0.9036489129066467, 0.764521062374115, 0.7771017551422119, 0.9768432974815369, 0.7699485421180725, 0.8230164647102356, 1.0229525566101074, 0.9925259947776794, 0.9744792580604553, 0.8037958741188049, 0.9442631006240845, 0.9902534484863281, 0.8991327881813049, 0.9684083461761475, 1.028180718421936, 0.9000990390777588, 0.8700548410415649, 0.8617750406265259, 0.8628160357475281, 0.8878389000892639, 0.8432149887084961, 0.9994603395462036, 0.9934914112091064, 0.9888002872467041]
[2025-06-13 20:17:29,254]: Mean: 0.90409446
[2025-06-13 20:17:29,259]: Min: 0.71875358
[2025-06-13 20:17:29,259]: Max: 1.09999835
[2025-06-13 20:17:29,259]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-13 20:17:29,284]: Sample Values (25 elements): [0.19318003952503204, 0.019048260524868965, -0.23870623111724854, -0.037045642733573914, 0.16999100148677826, 0.20308911800384521, 0.2082582414150238, 0.12978504598140717, -0.044016867876052856, -0.03651954606175423, 0.2584782838821411, 0.17948618531227112, -0.03790628910064697, 0.1884385645389557, -0.11772612482309341, -0.02395552024245262, 0.14374475181102753, -0.09613104909658432, -0.1663772165775299, -0.04867652803659439, -0.12237302958965302, 0.059549666941165924, 0.10537324845790863, 0.10638660192489624, 0.22966957092285156]
[2025-06-13 20:17:29,285]: Mean: -0.00343718
[2025-06-13 20:17:29,285]: Min: -0.35625842
[2025-06-13 20:17:29,285]: Max: 0.40166312
[2025-06-13 20:17:29,285]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,286]: Sample Values (25 elements): [0.5150562524795532, 0.6855621933937073, 0.5716856718063354, 0.5510473847389221, 0.623293399810791, 0.5490815043449402, 0.5177277326583862, 0.6460001468658447, 0.6811360120773315, 0.5673965215682983, 0.654026210308075, 0.6378373503684998, 0.7040002346038818, 0.6640484929084778, 0.6140540838241577, 0.6704971790313721, 0.600773274898529, 0.6142170429229736, 0.6529316902160645, 0.6533890962600708, 0.40002936124801636, 0.44748634099960327, 0.5947517156600952, 0.6266525983810425, 0.49882039427757263]
[2025-06-13 20:17:29,286]: Mean: 0.60031617
[2025-06-13 20:17:29,286]: Min: 0.40002936
[2025-06-13 20:17:29,286]: Max: 0.78867501
[2025-06-13 20:17:29,286]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:17:29,287]: Sample Values (25 elements): [-0.0774255245923996, 0.023106172680854797, -0.05086030811071396, 0.09631676971912384, -0.062104713171720505, -0.013406938873231411, 0.07523217052221298, 0.06757237017154694, 0.1655801236629486, -0.05113794654607773, 0.08560134470462799, 0.042524829506874084, -0.04466450586915016, -6.052946628187783e-05, -0.005920095834881067, -0.07948265969753265, -0.10622501373291016, -0.05491924658417702, -0.013648522086441517, -0.09347623586654663, 0.0033985013142228127, -0.09658537805080414, 0.01822752133011818, 0.005774752236902714, -0.0070417532697319984]
[2025-06-13 20:17:29,287]: Mean: -0.00635755
[2025-06-13 20:17:29,288]: Min: -0.32973671
[2025-06-13 20:17:29,288]: Max: 0.31642824
[2025-06-13 20:17:29,288]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,288]: Sample Values (25 elements): [0.8027773499488831, 0.7399917244911194, 0.6401844620704651, 0.6528661251068115, 0.7249794602394104, 0.7885344624519348, 0.6441085338592529, 0.7227445244789124, 0.5344998836517334, 0.7023207545280457, 0.7908294796943665, 0.7546801567077637, 0.7704731822013855, 0.7262897491455078, 0.7641049027442932, 0.7770894765853882, 0.6610108613967896, 0.6896761655807495, 0.6139868497848511, 0.7457798719406128, 0.7646723985671997, 0.5744447708129883, 0.7005105018615723, 0.7313342094421387, 0.8909584283828735]
[2025-06-13 20:17:29,288]: Mean: 0.71517813
[2025-06-13 20:17:29,288]: Min: 0.53449988
[2025-06-13 20:17:29,289]: Max: 0.90186548
[2025-06-13 20:17:29,289]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:17:29,289]: Sample Values (25 elements): [-0.034640055149793625, -0.03878689184784889, -0.024466725066304207, 0.023150764405727386, -0.030957844108343124, 0.0028537914622575045, -0.0423881821334362, 0.06073547154664993, 0.07798808813095093, 0.0015538916923105717, 0.0005425068666227162, -0.016373157501220703, -0.10570091009140015, -0.0003516737197060138, 0.14322194457054138, 0.018567979335784912, 0.05798753350973129, -0.03453392907977104, -0.016588961705565453, 0.03206481412053108, -0.042415764182806015, -0.09646042436361313, -0.01683982089161873, 0.02436339668929577, -0.036023035645484924]
[2025-06-13 20:17:29,290]: Mean: 0.00013813
[2025-06-13 20:17:29,290]: Min: -0.24267861
[2025-06-13 20:17:29,290]: Max: 0.25796765
[2025-06-13 20:17:29,290]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,291]: Sample Values (25 elements): [0.8735895156860352, 1.0401380062103271, 0.938295841217041, 0.953377902507782, 0.9608285427093506, 0.9700978994369507, 0.9947419762611389, 1.0249582529067993, 1.0011776685714722, 0.896823525428772, 0.8984509706497192, 1.1139216423034668, 1.006224513053894, 1.0311514139175415, 0.9209614992141724, 1.0285038948059082, 1.0022423267364502, 1.0714939832687378, 0.9230136275291443, 0.9579141139984131, 1.0019123554229736, 1.1272144317626953, 0.9986258745193481, 0.9822143912315369, 0.9258489608764648]
[2025-06-13 20:17:29,291]: Mean: 0.99508107
[2025-06-13 20:17:29,291]: Min: 0.83402836
[2025-06-13 20:17:29,291]: Max: 1.21319222
[2025-06-13 20:17:29,291]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:17:29,292]: Sample Values (25 elements): [0.00022143340902402997, 0.10982978343963623, -0.08798717707395554, 0.07350531965494156, -0.0027402464766055346, 0.027940088883042336, -0.009058470837771893, -0.01646457239985466, -0.06975588202476501, -0.07942096143960953, 0.05973834916949272, -0.0014343566726893187, -0.010937009938061237, 0.021001212298870087, -0.010466291569173336, 8.976143430094405e-40, -2.135306744315284e-15, -0.09512466192245483, -0.08573701232671738, 0.11312729865312576, 0.011502553708851337, 0.05831679329276085, -0.016998372972011566, 0.007011514622718096, -0.0034875485580414534]
[2025-06-13 20:17:29,292]: Mean: -0.00203588
[2025-06-13 20:17:29,292]: Min: -0.22891115
[2025-06-13 20:17:29,293]: Max: 0.23202266
[2025-06-13 20:17:29,293]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,293]: Sample Values (25 elements): [0.7876954674720764, 0.4107592701911926, 0.7465966939926147, 0.8990514278411865, 0.9448012113571167, 0.8010817766189575, 0.6546055674552917, 0.6019436120986938, 0.4732157588005066, 0.7621060609817505, 0.5536157488822937, 0.5347660183906555, 0.8039567470550537, 0.8078761696815491, 0.7507151961326599, 0.4135494828224182, 0.7418714165687561, 0.44430282711982727, 1.9345758176837757e-14, 0.6685871481895447, 0.6775587797164917, 0.6091923713684082, 0.6835399270057678, 0.7507031559944153, 0.6427088975906372]
[2025-06-13 20:17:29,294]: Mean: 0.57300162
[2025-06-13 20:17:29,294]: Min: 0.00000000
[2025-06-13 20:17:29,294]: Max: 0.94480121
[2025-06-13 20:17:29,294]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 20:17:29,295]: Sample Values (25 elements): [-0.05803189054131508, 0.008891820907592773, -0.10605315119028091, -0.011806037276983261, 0.021696489304304123, -0.04696575552225113, -0.005182674154639244, 0.02798948623239994, 4.904684754983292e-41, -0.028966685757040977, 0.01308450661599636, 0.02045009657740593, 0.0730677917599678, -0.008287668228149414, 0.05784158036112785, -0.052128780633211136, 0.015914374962449074, -0.01836252398788929, 0.02101999707520008, 0.0373445600271225, -0.0014122349675744772, -0.05262588709592819, -0.04614967852830887, 0.01010026317089796, 0.04204251617193222]
[2025-06-13 20:17:29,295]: Mean: 0.00326111
[2025-06-13 20:17:29,295]: Min: -0.16844057
[2025-06-13 20:17:29,295]: Max: 0.19100805
[2025-06-13 20:17:29,295]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-13 20:17:29,296]: Sample Values (25 elements): [1.043733835220337, 1.064138650894165, 1.1152584552764893, 0.9596187472343445, 1.0244523286819458, 1.0707018375396729, 0.9324912428855896, 1.0019145011901855, 0.950789213180542, 1.1449086666107178, 0.9215171933174133, 0.8969942331314087, 0.984326958656311, 0.9823653101921082, 0.9627234935760498, 1.0988013744354248, 0.8966938853263855, 0.9024462699890137, 1.153650164604187, 0.9932000637054443, 0.966659665107727, 0.9026159644126892, 0.9330387711524963, 0.8090163469314575, 1.0107700824737549]
[2025-06-13 20:17:29,296]: Mean: 0.98572069
[2025-06-13 20:17:29,296]: Min: 0.80901635
[2025-06-13 20:17:29,297]: Max: 1.15365016
[2025-06-13 20:17:29,297]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-13 20:17:29,297]: Sample Values (25 elements): [0.20071512460708618, 0.16421987116336823, 0.1739114373922348, 0.05442385375499725, 0.30159085988998413, -0.10414304584264755, 0.11210643500089645, 0.2030072659254074, 0.191507950425148, -0.06777115911245346, 0.3123827874660492, -0.35449451208114624, -0.44074198603630066, -0.5108590722084045, 0.26711276173591614, 0.33570924401283264, -0.29935213923454285, -0.4204350709915161, 0.13608212769031525, -0.44198736548423767, -0.033867064863443375, 0.2565212547779083, -0.3328417241573334, -0.001220410573296249, -0.3323898911476135]
[2025-06-13 20:17:29,297]: Mean: -0.05788873
[2025-06-13 20:17:29,298]: Min: -0.67281646
[2025-06-13 20:17:29,298]: Max: 0.37785742
[2025-06-13 20:17:29,298]: Checkpoint of model at path [checkpoint/ResNet20_parametrized_relu.ckpt] will be used for QAT
[2025-06-13 20:17:29,298]: 


QAT of ResNet20 with parametrized_relu down to 4 bits...
[2025-06-13 20:17:30,467]: [ResNet20_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-06-13 20:17:31,032]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 20:18:46,622]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.3498 Train Acc: 0.8775 Eval Loss: 0.6127 Eval Acc: 0.8126 (LR: 0.00100000)
[2025-06-13 20:20:08,692]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.3484 Train Acc: 0.8778 Eval Loss: 0.4726 Eval Acc: 0.8477 (LR: 0.00100000)
[2025-06-13 20:21:31,231]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.3497 Train Acc: 0.8782 Eval Loss: 0.4396 Eval Acc: 0.8527 (LR: 0.00100000)
[2025-06-13 20:22:51,869]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.3471 Train Acc: 0.8787 Eval Loss: 0.5184 Eval Acc: 0.8366 (LR: 0.00100000)
[2025-06-13 20:24:12,085]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.3413 Train Acc: 0.8815 Eval Loss: 0.5688 Eval Acc: 0.8120 (LR: 0.00100000)
[2025-06-13 20:25:31,636]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.3454 Train Acc: 0.8791 Eval Loss: 0.4582 Eval Acc: 0.8519 (LR: 0.00100000)
[2025-06-13 20:26:47,926]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.3369 Train Acc: 0.8824 Eval Loss: 0.5048 Eval Acc: 0.8392 (LR: 0.00100000)
[2025-06-13 20:28:04,310]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.3417 Train Acc: 0.8806 Eval Loss: 0.6489 Eval Acc: 0.8095 (LR: 0.00100000)
[2025-06-13 20:29:19,625]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.3384 Train Acc: 0.8829 Eval Loss: 0.4303 Eval Acc: 0.8612 (LR: 0.00100000)
[2025-06-13 20:30:35,945]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.3314 Train Acc: 0.8840 Eval Loss: 0.4467 Eval Acc: 0.8515 (LR: 0.00100000)
[2025-06-13 20:31:54,103]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.3334 Train Acc: 0.8843 Eval Loss: 0.4254 Eval Acc: 0.8639 (LR: 0.00100000)
[2025-06-13 20:33:10,430]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.3238 Train Acc: 0.8857 Eval Loss: 0.4662 Eval Acc: 0.8478 (LR: 0.00100000)
[2025-06-13 20:34:24,836]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.3287 Train Acc: 0.8851 Eval Loss: 0.4111 Eval Acc: 0.8632 (LR: 0.00100000)
[2025-06-13 20:35:39,131]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.3252 Train Acc: 0.8849 Eval Loss: 0.5404 Eval Acc: 0.8331 (LR: 0.00100000)
[2025-06-13 20:36:51,894]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.3272 Train Acc: 0.8860 Eval Loss: 0.4446 Eval Acc: 0.8576 (LR: 0.00100000)
[2025-06-13 20:38:05,062]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.3205 Train Acc: 0.8864 Eval Loss: 0.4598 Eval Acc: 0.8572 (LR: 0.00100000)
[2025-06-13 20:39:16,052]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.3181 Train Acc: 0.8877 Eval Loss: 0.4252 Eval Acc: 0.8617 (LR: 0.00100000)
[2025-06-13 20:40:29,354]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.3137 Train Acc: 0.8901 Eval Loss: 0.4368 Eval Acc: 0.8599 (LR: 0.00100000)
[2025-06-13 20:41:41,692]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.3159 Train Acc: 0.8893 Eval Loss: 0.4484 Eval Acc: 0.8584 (LR: 0.00010000)
[2025-06-13 20:42:53,513]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.2472 Train Acc: 0.9139 Eval Loss: 0.3312 Eval Acc: 0.8922 (LR: 0.00010000)
[2025-06-13 20:44:06,698]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.2270 Train Acc: 0.9204 Eval Loss: 0.3259 Eval Acc: 0.8938 (LR: 0.00010000)
[2025-06-13 20:45:22,008]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.2196 Train Acc: 0.9228 Eval Loss: 0.3354 Eval Acc: 0.8933 (LR: 0.00010000)
[2025-06-13 20:46:35,826]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.2159 Train Acc: 0.9235 Eval Loss: 0.3371 Eval Acc: 0.8933 (LR: 0.00010000)
[2025-06-13 20:47:46,828]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.2127 Train Acc: 0.9257 Eval Loss: 0.3329 Eval Acc: 0.8912 (LR: 0.00010000)
[2025-06-13 20:48:59,925]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.2115 Train Acc: 0.9267 Eval Loss: 0.3349 Eval Acc: 0.8946 (LR: 0.00010000)
[2025-06-13 20:50:13,698]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.2065 Train Acc: 0.9282 Eval Loss: 0.3309 Eval Acc: 0.8936 (LR: 0.00010000)
[2025-06-13 20:51:25,088]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.2059 Train Acc: 0.9280 Eval Loss: 0.3258 Eval Acc: 0.8959 (LR: 0.00010000)
[2025-06-13 20:52:39,881]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.1998 Train Acc: 0.9302 Eval Loss: 0.3325 Eval Acc: 0.8948 (LR: 0.00010000)
[2025-06-13 20:53:53,860]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.2012 Train Acc: 0.9294 Eval Loss: 0.3284 Eval Acc: 0.8957 (LR: 0.00010000)
[2025-06-13 20:55:07,861]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.1976 Train Acc: 0.9305 Eval Loss: 0.3301 Eval Acc: 0.8943 (LR: 0.00010000)
[2025-06-13 20:56:20,098]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.1971 Train Acc: 0.9318 Eval Loss: 0.3319 Eval Acc: 0.8937 (LR: 0.00010000)
[2025-06-13 20:57:31,118]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.1963 Train Acc: 0.9308 Eval Loss: 0.3467 Eval Acc: 0.8937 (LR: 0.00010000)
[2025-06-13 20:58:41,691]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.1913 Train Acc: 0.9329 Eval Loss: 0.3428 Eval Acc: 0.8920 (LR: 0.00001000)
[2025-06-13 20:59:55,612]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.1824 Train Acc: 0.9360 Eval Loss: 0.3290 Eval Acc: 0.8978 (LR: 0.00001000)
[2025-06-13 21:01:17,757]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.1789 Train Acc: 0.9377 Eval Loss: 0.3266 Eval Acc: 0.8975 (LR: 0.00001000)
[2025-06-13 21:02:38,183]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.1780 Train Acc: 0.9362 Eval Loss: 0.3253 Eval Acc: 0.8958 (LR: 0.00001000)
[2025-06-13 21:03:53,353]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.1800 Train Acc: 0.9372 Eval Loss: 0.3297 Eval Acc: 0.8964 (LR: 0.00001000)
[2025-06-13 21:05:06,332]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.1801 Train Acc: 0.9374 Eval Loss: 0.3302 Eval Acc: 0.8972 (LR: 0.00001000)
[2025-06-13 21:06:19,238]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.1762 Train Acc: 0.9384 Eval Loss: 0.3312 Eval Acc: 0.8968 (LR: 0.00001000)
[2025-06-13 21:07:34,009]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.1781 Train Acc: 0.9375 Eval Loss: 0.3323 Eval Acc: 0.8971 (LR: 0.00001000)
[2025-06-13 21:08:46,949]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.1774 Train Acc: 0.9381 Eval Loss: 0.3254 Eval Acc: 0.8989 (LR: 0.00001000)
[2025-06-13 21:10:00,107]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.1762 Train Acc: 0.9387 Eval Loss: 0.3266 Eval Acc: 0.8976 (LR: 0.00000100)
[2025-06-13 21:11:13,853]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.1710 Train Acc: 0.9396 Eval Loss: 0.3268 Eval Acc: 0.8975 (LR: 0.00000100)
[2025-06-13 21:12:29,413]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.1749 Train Acc: 0.9382 Eval Loss: 0.3283 Eval Acc: 0.8973 (LR: 0.00000100)
[2025-06-13 21:13:42,785]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.1754 Train Acc: 0.9384 Eval Loss: 0.3286 Eval Acc: 0.8967 (LR: 0.00000100)
[2025-06-13 21:14:56,209]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.1743 Train Acc: 0.9396 Eval Loss: 0.3269 Eval Acc: 0.8990 (LR: 0.00000100)
[2025-06-13 21:16:10,285]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.1749 Train Acc: 0.9389 Eval Loss: 0.3205 Eval Acc: 0.8997 (LR: 0.00000100)
[2025-06-13 21:17:30,103]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.1721 Train Acc: 0.9403 Eval Loss: 0.3238 Eval Acc: 0.8989 (LR: 0.00000100)
[2025-06-13 21:18:51,756]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.1742 Train Acc: 0.9373 Eval Loss: 0.3229 Eval Acc: 0.8998 (LR: 0.00000100)
[2025-06-13 21:20:16,556]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.1753 Train Acc: 0.9392 Eval Loss: 0.3271 Eval Acc: 0.8964 (LR: 0.00000100)
[2025-06-13 21:21:57,319]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.1770 Train Acc: 0.9388 Eval Loss: 0.3264 Eval Acc: 0.8970 (LR: 0.00000100)
[2025-06-13 21:23:37,673]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.1720 Train Acc: 0.9403 Eval Loss: 0.3256 Eval Acc: 0.8991 (LR: 0.00000100)
[2025-06-13 21:25:18,733]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.1729 Train Acc: 0.9401 Eval Loss: 0.3265 Eval Acc: 0.8970 (LR: 0.00000010)
[2025-06-13 21:26:52,798]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.1709 Train Acc: 0.9402 Eval Loss: 0.3279 Eval Acc: 0.8985 (LR: 0.00000010)
[2025-06-13 21:28:26,938]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.1728 Train Acc: 0.9385 Eval Loss: 0.3256 Eval Acc: 0.8989 (LR: 0.00000010)
[2025-06-13 21:30:09,501]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.1710 Train Acc: 0.9413 Eval Loss: 0.3267 Eval Acc: 0.8990 (LR: 0.00000010)
[2025-06-13 21:31:49,714]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.1738 Train Acc: 0.9397 Eval Loss: 0.3258 Eval Acc: 0.8983 (LR: 0.00000010)
[2025-06-13 21:33:25,679]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.1735 Train Acc: 0.9395 Eval Loss: 0.3255 Eval Acc: 0.8978 (LR: 0.00000010)
[2025-06-13 21:34:56,019]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.1714 Train Acc: 0.9411 Eval Loss: 0.3261 Eval Acc: 0.8978 (LR: 0.00000010)
[2025-06-13 21:36:26,327]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.1714 Train Acc: 0.9403 Eval Loss: 0.3294 Eval Acc: 0.8950 (LR: 0.00000010)
[2025-06-13 21:36:26,327]: [ResNet20_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.8998
[2025-06-13 21:36:26,484]: 


Quantization of model down to 4 bits finished
[2025-06-13 21:36:26,484]: Model Architecture:
[2025-06-13 21:36:26,853]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2085], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.1278200149536133)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0639], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49237221479415894, max_val=0.46675366163253784)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1755], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0757], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5264360904693604, max_val=0.6084041595458984)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2563], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0648], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4864233732223511, max_val=0.48578768968582153)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1616], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0695], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5834327936172485, max_val=0.45964670181274414)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2896], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0925], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6035770177841187, max_val=0.784385085105896)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1587], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0733], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.628052294254303, max_val=0.4718922972679138)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2984], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0668], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49401456117630005, max_val=0.5081250667572021)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1779], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0593], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41775399446487427, max_val=0.471499502658844)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0629], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4195881485939026, max_val=0.5243706703186035)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2527], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0514], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3868691921234131, max_val=0.38448435068130493)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1565], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0549], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3697919547557831, max_val=0.4543244242668152)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2832], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0593], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4092983603477478, max_val=0.4795643091201782)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1432], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0608], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41227102279663086, max_val=0.49962788820266724)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3922978639602661, max_val=0.39008384943008423)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1810], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0517], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3857764005661011, max_val=0.3895755410194397)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0531], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3674721121788025, max_val=0.42870038747787476)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2491], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0489], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3698427677154541, max_val=0.3643200993537903)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1509], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2733307480812073, max_val=0.3051568269729614)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2216], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0359], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2366625815629959, max_val=0.30226290225982666)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1169], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0255], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19533458352088928, max_val=0.18672078847885132)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3502], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 21:36:26,854]: 
Model Weights:
[2025-06-13 21:36:26,854]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-13 21:36:26,855]: Sample Values (25 elements): [-0.09870892018079758, -0.31893664598464966, 0.14327123761177063, 0.1197093203663826, 0.1294156163930893, 0.03694590553641319, -0.22342325747013092, -0.11425027251243591, -0.13414402306079865, -0.26405513286590576, -0.03783871605992317, 0.19830147922039032, -0.042820870876312256, 0.2482847422361374, -0.040980689227581024, -0.04864172264933586, -0.09448327124118805, -0.06475071609020233, -0.12640181183815002, 0.01819312572479248, 0.010463796555995941, 0.21690797805786133, -0.20896916091442108, 0.11269961297512054, 0.02691940777003765]
[2025-06-13 21:36:26,865]: Mean: -0.00108555
[2025-06-13 21:36:26,865]: Min: -0.58788890
[2025-06-13 21:36:26,866]: Max: 0.58989823
[2025-06-13 21:36:26,866]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,866]: Sample Values (16 elements): [0.7976654171943665, 0.7453218698501587, 0.6543321013450623, 0.918285071849823, 0.8069187998771667, 0.990980327129364, 1.0576893091201782, 0.9840953946113586, 0.8726955056190491, 1.0299028158187866, 1.0285288095474243, 0.9586361050605774, 1.0669509172439575, 0.9754035472869873, 0.9529026746749878, 1.0812833309173584]
[2025-06-13 21:36:26,866]: Mean: 0.93259948
[2025-06-13 21:36:26,867]: Min: 0.65433210
[2025-06-13 21:36:26,867]: Max: 1.08128333
[2025-06-13 21:36:26,868]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:36:26,869]: Sample Values (25 elements): [-0.25576692819595337, -0.19182519614696503, 0.0, 0.0, 0.0, -0.12788346409797668, 0.19182519614696503, 0.06394173204898834, -0.06394173204898834, -0.12788346409797668, -0.25576692819595337, 0.19182519614696503, 0.0, -0.25576692819595337, -0.19182519614696503, 0.06394173204898834, 0.0, -0.06394173204898834, 0.0, 0.0, -0.19182519614696503, 0.12788346409797668, 0.0, 0.0, 0.0]
[2025-06-13 21:36:26,869]: Mean: -0.01112875
[2025-06-13 21:36:26,869]: Min: -0.51153386
[2025-06-13 21:36:26,869]: Max: 0.44759214
[2025-06-13 21:36:26,869]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,870]: Sample Values (16 elements): [0.6150534152984619, 0.7473087310791016, 1.0065135955810547, 0.9269800782203674, 0.7300369143486023, 0.9595314860343933, 0.7371941208839417, 0.729167640209198, 0.7675919532775879, 0.8868353962898254, 0.7918052673339844, 0.892208993434906, 0.7570711374282837, 0.8436705470085144, 0.9041197299957275, 0.720720648765564]
[2025-06-13 21:36:26,870]: Mean: 0.81348813
[2025-06-13 21:36:26,870]: Min: 0.61505342
[2025-06-13 21:36:26,870]: Max: 1.00651360
[2025-06-13 21:36:26,872]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:36:26,872]: Sample Values (25 elements): [0.0, -0.22696805000305176, 0.15131203830242157, 0.15131203830242157, 0.07565601915121078, 0.0, -0.15131203830242157, -0.22696805000305176, 0.0, 0.07565601915121078, 0.15131203830242157, -0.15131203830242157, 0.30262407660484314, 0.30262407660484314, -0.4539361000061035, 0.07565601915121078, 0.07565601915121078, 0.15131203830242157, 0.07565601915121078, 0.15131203830242157, 0.15131203830242157, -0.15131203830242157, -0.30262407660484314, -0.22696805000305176, 0.0]
[2025-06-13 21:36:26,873]: Mean: -0.00814353
[2025-06-13 21:36:26,873]: Min: -0.52959216
[2025-06-13 21:36:26,873]: Max: 0.60524815
[2025-06-13 21:36:26,873]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,873]: Sample Values (16 elements): [0.927842915058136, 0.5759384632110596, 0.7228085398674011, 1.0152337551116943, 0.7035034894943237, 0.7642067670822144, 0.7533177733421326, 0.6872580647468567, 0.6116917729377747, 0.7210315465927124, 0.7628101706504822, 0.7558316588401794, 0.7222736477851868, 0.7298743724822998, 0.9466027617454529, 0.8770567774772644]
[2025-06-13 21:36:26,874]: Mean: 0.76733011
[2025-06-13 21:36:26,874]: Min: 0.57593846
[2025-06-13 21:36:26,874]: Max: 1.01523376
[2025-06-13 21:36:26,875]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:36:26,876]: Sample Values (25 elements): [0.06481407582759857, 0.0, -0.4536985158920288, 0.12962815165519714, 0.19444222748279572, 0.12962815165519714, 0.0, -0.06481407582759857, 0.0, 0.0, -0.12962815165519714, 0.2592563033103943, -0.06481407582759857, 0.0, -0.12962815165519714, 0.0, 0.12962815165519714, -0.2592563033103943, 0.12962815165519714, 0.0, 0.12962815165519714, -0.2592563033103943, 0.2592563033103943, 0.0, -0.12962815165519714]
[2025-06-13 21:36:26,876]: Mean: -0.00990215
[2025-06-13 21:36:26,876]: Min: -0.51851261
[2025-06-13 21:36:26,877]: Max: 0.45369852
[2025-06-13 21:36:26,877]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,877]: Sample Values (16 elements): [0.8067095875740051, 0.7635400891304016, 0.8015890717506409, 0.7369810342788696, 0.811141848564148, 0.7727346420288086, 0.6813390254974365, 0.8813191652297974, 0.6769959330558777, 0.8553521037101746, 0.7320042252540588, 0.7017248868942261, 0.7038838863372803, 0.7430116534233093, 1.097776174545288, 0.8552618622779846]
[2025-06-13 21:36:26,877]: Mean: 0.78883529
[2025-06-13 21:36:26,878]: Min: 0.67699593
[2025-06-13 21:36:26,878]: Max: 1.09777617
[2025-06-13 21:36:26,879]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:36:26,879]: Sample Values (25 elements): [0.0, 0.0, 0.20861591398715973, -0.41723182797431946, 0.06953863799571991, -0.06953863799571991, 0.06953863799571991, 0.13907727599143982, 0.13907727599143982, -0.06953863799571991, 0.13907727599143982, -0.06953863799571991, 0.0, 0.13907727599143982, 0.0, -0.06953863799571991, 0.13907727599143982, -0.06953863799571991, 0.0, 0.06953863799571991, -0.13907727599143982, 0.13907727599143982, 0.06953863799571991, -0.20861591398715973, 0.13907727599143982]
[2025-06-13 21:36:26,880]: Mean: -0.00060363
[2025-06-13 21:36:26,880]: Min: -0.55630910
[2025-06-13 21:36:26,880]: Max: 0.48677045
[2025-06-13 21:36:26,880]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,881]: Sample Values (16 elements): [0.6556692123413086, 0.8677915930747986, 0.6854621171951294, 0.5787731409072876, 0.6050899028778076, 0.6736862063407898, 0.8995545506477356, 0.5004297494888306, 0.7747142314910889, 0.636699914932251, 0.7698200345039368, 0.7316413521766663, 0.6105936765670776, 0.8098360300064087, 0.7120189666748047, 0.6128697991371155]
[2025-06-13 21:36:26,881]: Mean: 0.69529063
[2025-06-13 21:36:26,881]: Min: 0.50042975
[2025-06-13 21:36:26,881]: Max: 0.89955455
[2025-06-13 21:36:26,883]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:36:26,883]: Sample Values (25 elements): [0.18506161868572235, 0.18506161868572235, 0.09253080934286118, -0.3701232373714447, 0.18506161868572235, 0.09253080934286118, 0.0, 0.0, 0.09253080934286118, 0.18506161868572235, 0.09253080934286118, -0.18506161868572235, 0.0, -0.3701232373714447, 0.0, -0.18506161868572235, 0.0, 0.3701232373714447, -0.09253080934286118, -0.09253080934286118, 0.18506161868572235, -0.18506161868572235, 0.09253080934286118, 0.18506161868572235, 0.09253080934286118]
[2025-06-13 21:36:26,883]: Mean: -0.00389561
[2025-06-13 21:36:26,884]: Min: -0.64771569
[2025-06-13 21:36:26,884]: Max: 0.74024647
[2025-06-13 21:36:26,884]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,884]: Sample Values (16 elements): [0.8076357841491699, 0.7433517575263977, 0.8837705850601196, 0.8524757027626038, 0.5699794292449951, 0.6208352446556091, 0.8592383861541748, 0.9328314065933228, 0.5797300934791565, 1.3226009607315063, 0.7734458446502686, 0.8851802945137024, 0.8514039516448975, 0.6034525036811829, 1.1089938879013062, 0.8792851567268372]
[2025-06-13 21:36:26,884]: Mean: 0.82963824
[2025-06-13 21:36:26,885]: Min: 0.56997943
[2025-06-13 21:36:26,885]: Max: 1.32260096
[2025-06-13 21:36:26,887]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 21:36:26,887]: Sample Values (25 elements): [0.21998892724514008, -0.07332964241504669, -0.07332964241504669, -0.07332964241504669, 0.14665928483009338, 0.0, 0.0, 0.14665928483009338, 0.07332964241504669, -0.14665928483009338, -0.07332964241504669, 0.14665928483009338, 0.0, 0.0, 0.14665928483009338, 0.07332964241504669, 0.0, 0.0, -0.21998892724514008, 0.29331856966018677, 0.07332964241504669, 0.0, 0.07332964241504669, -0.29331856966018677, -0.07332964241504669]
[2025-06-13 21:36:26,887]: Mean: -0.00614263
[2025-06-13 21:36:26,888]: Min: -0.65996677
[2025-06-13 21:36:26,888]: Max: 0.43997785
[2025-06-13 21:36:26,888]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-13 21:36:26,888]: Sample Values (16 elements): [0.7970466017723083, 0.6310706734657288, 0.5855727195739746, 0.7633230090141296, 0.5400124788284302, 1.1474738121032715, 0.9182281494140625, 0.9557433724403381, 0.5577459931373596, 0.6750434041023254, 0.8743093609809875, 1.2879823446273804, 0.8472381830215454, 0.8758772015571594, 0.7826411128044128, 0.6409915685653687]
[2025-06-13 21:36:26,889]: Mean: 0.80501878
[2025-06-13 21:36:26,889]: Min: 0.54001248
[2025-06-13 21:36:26,889]: Max: 1.28798234
[2025-06-13 21:36:26,890]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-13 21:36:26,891]: Sample Values (25 elements): [0.4008558690547943, 0.06680931150913239, 0.0, -0.13361862301826477, 0.06680931150913239, 0.13361862301826477, 0.13361862301826477, -0.06680931150913239, -0.13361862301826477, 0.06680931150913239, 0.13361862301826477, 0.13361862301826477, 0.06680931150913239, -0.06680931150913239, -0.06680931150913239, 0.0, 0.06680931150913239, -0.06680931150913239, -0.06680931150913239, 0.13361862301826477, -0.13361862301826477, 0.0, -0.06680931150913239, 0.06680931150913239, 0.0]
[2025-06-13 21:36:26,891]: Mean: -0.00588641
[2025-06-13 21:36:26,891]: Min: -0.46766520
[2025-06-13 21:36:26,892]: Max: 0.53447449
[2025-06-13 21:36:26,892]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,892]: Sample Values (25 elements): [0.795768678188324, 0.8320093750953674, 0.6927717924118042, 0.8677794933319092, 0.8892585039138794, 0.6857141852378845, 0.7816320061683655, 0.7683840990066528, 0.8193041086196899, 0.7700268626213074, 0.6719251871109009, 0.8160225749015808, 0.675776481628418, 0.8265316486358643, 0.7948156595230103, 0.7906646728515625, 0.9669869542121887, 0.6797733306884766, 0.9793338775634766, 0.8299250602722168, 0.7832843661308289, 0.7581632137298584, 0.7570182085037231, 0.7078760266304016, 0.6750854849815369]
[2025-06-13 21:36:26,892]: Mean: 0.78404111
[2025-06-13 21:36:26,893]: Min: 0.67192519
[2025-06-13 21:36:26,893]: Max: 0.97933388
[2025-06-13 21:36:26,899]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:36:26,902]: Sample Values (25 elements): [0.05928356945514679, 0.11856713891029358, -0.11856713891029358, 0.0, 0.11856713891029358, 0.0, 0.0, -0.17785070836544037, 0.17785070836544037, 0.0, 0.0, 0.11856713891029358, -0.05928356945514679, 0.05928356945514679, -0.05928356945514679, 0.0, 0.35570141673088074, -0.05928356945514679, -0.11856713891029358, 0.17785070836544037, 0.11856713891029358, -0.11856713891029358, -0.11856713891029358, -0.23713427782058716, 0.05928356945514679]
[2025-06-13 21:36:26,904]: Mean: -0.00045672
[2025-06-13 21:36:26,907]: Min: -0.41498500
[2025-06-13 21:36:26,909]: Max: 0.47426856
[2025-06-13 21:36:26,909]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,914]: Sample Values (25 elements): [0.7733104228973389, 0.8154301047325134, 0.8118155002593994, 0.8209511637687683, 0.9579259157180786, 0.8492506146430969, 0.7098502516746521, 0.9224665760993958, 0.8294476270675659, 0.8838154077529907, 0.8862910866737366, 1.0070204734802246, 0.8021389245986938, 0.8160752654075623, 0.9619226455688477, 0.8407953381538391, 0.7696967124938965, 0.8086432814598083, 0.9192047119140625, 0.7965238094329834, 0.8086694478988647, 0.773883581161499, 0.7474966049194336, 0.8489570021629333, 0.9318370223045349]
[2025-06-13 21:36:26,919]: Mean: 0.84239328
[2025-06-13 21:36:26,927]: Min: 0.70985025
[2025-06-13 21:36:26,932]: Max: 1.00702047
[2025-06-13 21:36:26,945]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-13 21:36:26,945]: Sample Values (25 elements): [-0.18879178166389465, 0.0, -0.06293059140443802, -0.3146529495716095, 0.12586118280887604, 0.44051414728164673, 0.06293059140443802, 0.18879178166389465, 0.0, 0.18879178166389465, 0.06293059140443802, 0.12586118280887604, -0.18879178166389465, 0.0, 0.12586118280887604, 0.2517223656177521, -0.12586118280887604, 0.06293059140443802, -0.12586118280887604, -0.06293059140443802, -0.18879178166389465, 0.18879178166389465, 0.2517223656177521, -0.06293059140443802, 0.0]
[2025-06-13 21:36:26,945]: Mean: 0.00663721
[2025-06-13 21:36:26,945]: Min: -0.44051415
[2025-06-13 21:36:26,946]: Max: 0.50344473
[2025-06-13 21:36:26,946]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,946]: Sample Values (25 elements): [0.577326774597168, 0.5593153238296509, 0.5081835389137268, 0.6030097603797913, 0.917413592338562, 0.5941389203071594, 0.6035022735595703, 0.6331072449684143, 0.5659464001655579, 0.6457831263542175, 0.622927725315094, 0.5941904783248901, 0.537254273891449, 0.6107405424118042, 0.48276475071907043, 0.5166009068489075, 0.6507022380828857, 0.42638251185417175, 0.5070870518684387, 0.6412500143051147, 0.7353895902633667, 0.4370516836643219, 0.5058329105377197, 0.486944317817688, 0.6242169141769409]
[2025-06-13 21:36:26,947]: Mean: 0.59427822
[2025-06-13 21:36:26,947]: Min: 0.42638251
[2025-06-13 21:36:26,947]: Max: 0.91741359
[2025-06-13 21:36:26,949]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:36:26,949]: Sample Values (25 elements): [-0.1028471440076828, 0.0, -0.1542707085609436, -0.1542707085609436, 0.0, 0.0, -0.1028471440076828, 0.0514235720038414, 0.0514235720038414, 0.1028471440076828, 0.0, 0.1542707085609436, 0.0, 0.0514235720038414, 0.0514235720038414, 0.0, -0.1028471440076828, 0.1028471440076828, 0.1028471440076828, -0.0514235720038414, -0.1028471440076828, -0.1028471440076828, 0.1542707085609436, 0.1542707085609436, -0.1542707085609436]
[2025-06-13 21:36:26,950]: Mean: -0.01132702
[2025-06-13 21:36:26,950]: Min: -0.41138858
[2025-06-13 21:36:26,950]: Max: 0.35996500
[2025-06-13 21:36:26,951]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,951]: Sample Values (25 elements): [0.7263568043708801, 0.8718541860580444, 0.7901490926742554, 0.7855451107025146, 0.6315066814422607, 0.7666752338409424, 0.6012217402458191, 0.7110822796821594, 0.827237606048584, 0.7943987846374512, 0.7283158302307129, 0.7003920674324036, 0.8424943685531616, 0.7844483256340027, 0.7029823064804077, 0.7648971676826477, 0.762248694896698, 0.7656195759773254, 1.0712326765060425, 0.7150859236717224, 0.7958060503005981, 0.7517266273498535, 0.6906513571739197, 0.6164799928665161, 0.7710086107254028]
[2025-06-13 21:36:26,951]: Mean: 0.75110006
[2025-06-13 21:36:26,952]: Min: 0.60122174
[2025-06-13 21:36:26,952]: Max: 1.07123268
[2025-06-13 21:36:26,954]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:36:26,955]: Sample Values (25 elements): [-0.054941095411777496, -0.054941095411777496, 0.054941095411777496, 0.21976438164710999, 0.10988219082355499, 0.0, 0.0, 0.10988219082355499, -0.054941095411777496, -0.10988219082355499, -0.10988219082355499, -0.054941095411777496, 0.0, 0.10988219082355499, -0.054941095411777496, -0.16482329368591309, 0.054941095411777496, -0.10988219082355499, -0.054941095411777496, -0.054941095411777496, 0.0, 0.054941095411777496, -0.054941095411777496, -0.054941095411777496, 0.0]
[2025-06-13 21:36:26,955]: Mean: -0.00190172
[2025-06-13 21:36:26,955]: Min: -0.38458768
[2025-06-13 21:36:26,955]: Max: 0.43952876
[2025-06-13 21:36:26,955]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,956]: Sample Values (25 elements): [0.7139328718185425, 0.7417810559272766, 0.6211951375007629, 0.6987742781639099, 0.6526212692260742, 0.6832947134971619, 0.798861563205719, 0.8657932281494141, 0.7688161134719849, 0.7409394979476929, 0.7454404234886169, 0.6318414211273193, 0.6866461634635925, 0.7894768118858337, 0.7274373173713684, 0.639247477054596, 0.7222634553909302, 0.694554328918457, 0.7703925371170044, 0.5973082184791565, 0.6839921474456787, 0.7528706192970276, 0.8128141164779663, 0.7137540578842163, 0.6225330829620361]
[2025-06-13 21:36:26,956]: Mean: 0.71044433
[2025-06-13 21:36:26,956]: Min: 0.59730822
[2025-06-13 21:36:26,957]: Max: 0.86579323
[2025-06-13 21:36:26,959]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:36:26,959]: Sample Values (25 elements): [0.05925751477479935, 0.1185150295495987, 0.05925751477479935, -0.05925751477479935, 0.05925751477479935, 0.1185150295495987, 0.05925751477479935, 0.1185150295495987, 0.0, -0.05925751477479935, 0.2370300590991974, -0.05925751477479935, 0.17777255177497864, 0.0, 0.05925751477479935, 0.1185150295495987, -0.05925751477479935, -0.05925751477479935, 0.0, 0.0, 0.05925751477479935, 0.1185150295495987, -0.17777255177497864, 0.05925751477479935, -0.05925751477479935]
[2025-06-13 21:36:26,959]: Mean: -0.01340624
[2025-06-13 21:36:26,960]: Min: -0.41480261
[2025-06-13 21:36:26,960]: Max: 0.47406012
[2025-06-13 21:36:26,960]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,960]: Sample Values (25 elements): [0.7669482827186584, 0.7501538395881653, 0.6327045559883118, 0.7691578269004822, 0.7029005289077759, 0.6092198491096497, 0.604986310005188, 0.6755847930908203, 0.6863237619400024, 0.7692255973815918, 0.6715508699417114, 0.7024144530296326, 0.7206661105155945, 0.8508498072624207, 0.5927483439445496, 0.5823121070861816, 0.8344899415969849, 0.8542507886886597, 0.5738165378570557, 0.6800860166549683, 0.7538968920707703, 0.6407709717750549, 0.6954573392868042, 0.7206532955169678, 0.7268596887588501]
[2025-06-13 21:36:26,961]: Mean: 0.70658511
[2025-06-13 21:36:26,962]: Min: 0.57381654
[2025-06-13 21:36:26,962]: Max: 0.85425079
[2025-06-13 21:36:26,963]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 21:36:26,964]: Sample Values (25 elements): [0.06079326570034027, 0.06079326570034027, -0.06079326570034027, -0.12158653140068054, -0.06079326570034027, -0.1823797971010208, 0.0, -0.06079326570034027, 0.1823797971010208, -0.12158653140068054, 0.06079326570034027, 0.12158653140068054, 0.12158653140068054, -0.12158653140068054, -0.06079326570034027, -0.06079326570034027, 0.0, 0.06079326570034027, 0.1823797971010208, 0.12158653140068054, -0.12158653140068054, 0.06079326570034027, 0.06079326570034027, -0.06079326570034027, -0.06079326570034027]
[2025-06-13 21:36:26,964]: Mean: -0.00457797
[2025-06-13 21:36:26,964]: Min: -0.42555285
[2025-06-13 21:36:26,964]: Max: 0.48634613
[2025-06-13 21:36:26,964]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-13 21:36:26,965]: Sample Values (25 elements): [0.773722231388092, 0.6108182668685913, 0.7378665208816528, 0.588462233543396, 0.6407989263534546, 0.6894825100898743, 0.6600485444068909, 0.7270213961601257, 0.669213056564331, 0.613795816898346, 0.6594196557998657, 0.7447469234466553, 0.5131736397743225, 0.7199011445045471, 0.7078680396080017, 0.6493582129478455, 0.6830955147743225, 0.5685323476791382, 0.6994975805282593, 0.639100193977356, 0.6606206893920898, 0.6094226837158203, 0.5873804688453674, 0.7148578763008118, 0.6920466423034668]
[2025-06-13 21:36:26,965]: Mean: 0.67096341
[2025-06-13 21:36:26,966]: Min: 0.51317364
[2025-06-13 21:36:26,966]: Max: 1.04158974
[2025-06-13 21:36:26,967]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-13 21:36:26,968]: Sample Values (25 elements): [-0.1043175682425499, 0.0, -0.1043175682425499, -0.1043175682425499, -0.05215878412127495, -0.2086351364850998, 0.1043175682425499, -0.05215878412127495, 0.0, 0.1043175682425499, 0.1043175682425499, 0.0, -0.05215878412127495, 0.2086351364850998, -0.05215878412127495, 0.1043175682425499, -0.15647634863853455, 0.15647634863853455, 0.1043175682425499, 0.0, -0.05215878412127495, 0.1043175682425499, 0.1043175682425499, 0.0, -0.05215878412127495]
[2025-06-13 21:36:26,968]: Mean: -0.00547282
[2025-06-13 21:36:26,969]: Min: -0.41727027
[2025-06-13 21:36:26,969]: Max: 0.36511150
[2025-06-13 21:36:26,969]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,970]: Sample Values (25 elements): [0.7082707285881042, 0.6605443358421326, 0.7148841619491577, 0.8203842639923096, 0.6183863282203674, 0.6722971200942993, 0.6851890087127686, 0.8167404532432556, 0.7102705836296082, 0.7685726881027222, 0.7892657518386841, 0.6420623064041138, 0.6853165626525879, 0.7156297564506531, 0.7022750973701477, 0.7658135890960693, 0.7362416982650757, 0.7267082333564758, 0.6786100268363953, 0.6812061667442322, 0.6187306046485901, 0.6471285223960876, 0.7118015289306641, 0.7230322360992432, 0.7597509026527405]
[2025-06-13 21:36:26,970]: Mean: 0.71566612
[2025-06-13 21:36:26,970]: Min: 0.59703046
[2025-06-13 21:36:26,970]: Max: 1.01277161
[2025-06-13 21:36:26,972]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:36:26,973]: Sample Values (25 elements): [-0.051690131425857544, -0.051690131425857544, 0.051690131425857544, 0.051690131425857544, 0.051690131425857544, -0.051690131425857544, 0.0, -0.20676052570343018, -0.051690131425857544, -0.051690131425857544, 0.051690131425857544, -0.15507039427757263, -0.051690131425857544, -0.051690131425857544, 0.051690131425857544, 0.0, 0.10338026285171509, -0.10338026285171509, 0.10338026285171509, 0.051690131425857544, -0.051690131425857544, 0.20676052570343018, -0.10338026285171509, -0.10338026285171509, -0.10338026285171509]
[2025-06-13 21:36:26,973]: Mean: -0.00649211
[2025-06-13 21:36:26,973]: Min: -0.36183092
[2025-06-13 21:36:26,974]: Max: 0.41352105
[2025-06-13 21:36:26,974]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,974]: Sample Values (25 elements): [0.9832406640052795, 0.8536517024040222, 0.9191534519195557, 0.8128620982170105, 0.8020567297935486, 0.8411818146705627, 1.0159549713134766, 0.8981806039810181, 0.8632709383964539, 0.8303967714309692, 0.9811822772026062, 0.8121670484542847, 0.8944641947746277, 0.6631513237953186, 0.9706519246101379, 0.8676807880401611, 1.0042572021484375, 0.8759129047393799, 0.8529722094535828, 0.8359565734863281, 0.91827392578125, 0.911466658115387, 0.8285543322563171, 0.8373667597770691, 1.0600570440292358]
[2025-06-13 21:36:26,974]: Mean: 0.87349743
[2025-06-13 21:36:26,974]: Min: 0.66315132
[2025-06-13 21:36:26,975]: Max: 1.06005704
[2025-06-13 21:36:26,976]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-13 21:36:26,977]: Sample Values (25 elements): [-0.053078170865774155, 0.15923450887203217, -0.15923450887203217, 0.053078170865774155, 0.053078170865774155, -0.053078170865774155, -0.10615634173154831, -0.15923450887203217, -0.053078170865774155, -0.10615634173154831, -0.15923450887203217, -0.10615634173154831, 0.21231268346309662, 0.21231268346309662, -0.2653908431529999, -0.15923450887203217, -0.15923450887203217, 0.10615634173154831, 0.15923450887203217, 0.0, -0.053078170865774155, 0.10615634173154831, -0.15923450887203217, -0.21231268346309662, -0.2653908431529999]
[2025-06-13 21:36:26,977]: Mean: -0.00458732
[2025-06-13 21:36:26,977]: Min: -0.37154719
[2025-06-13 21:36:26,977]: Max: 0.42462537
[2025-06-13 21:36:26,977]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,978]: Sample Values (25 elements): [0.6195948123931885, 0.5665828585624695, 0.3478958308696747, 0.3064834177494049, 0.3887627124786377, 0.5245603919029236, 0.542244553565979, 0.42305880784988403, 0.5394647717475891, 0.3562023937702179, 0.6330908536911011, 0.4228074550628662, 0.4615355134010315, 0.6054284572601318, 0.5443694591522217, 0.5654404163360596, 0.5614529848098755, 0.532223105430603, 0.556548535823822, 0.5265970230102539, 0.39748474955558777, 0.48151400685310364, 0.6579228043556213, 0.43798041343688965, 0.30620715022087097]
[2025-06-13 21:36:26,978]: Mean: 0.49682784
[2025-06-13 21:36:26,978]: Min: 0.30620715
[2025-06-13 21:36:26,978]: Max: 0.65792280
[2025-06-13 21:36:26,981]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:36:26,982]: Sample Values (25 elements): [0.09788838773965836, 0.04894419386982918, -0.14683258533477783, 0.14683258533477783, 0.0, 0.04894419386982918, 0.0, 0.0, -0.14683258533477783, 0.04894419386982918, 0.0, 0.0, -0.04894419386982918, -0.04894419386982918, 0.09788838773965836, -0.04894419386982918, -0.04894419386982918, 0.0, 0.14683258533477783, 0.0, 0.14683258533477783, 0.09788838773965836, -0.04894419386982918, 0.14683258533477783, -0.04894419386982918]
[2025-06-13 21:36:26,982]: Mean: -0.00643003
[2025-06-13 21:36:26,982]: Min: -0.39155355
[2025-06-13 21:36:26,982]: Max: 0.34260935
[2025-06-13 21:36:26,982]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,983]: Sample Values (25 elements): [0.7250995635986328, 0.6874791383743286, 0.6100536584854126, 0.7146576046943665, 0.7202653884887695, 0.6168827414512634, 0.6489603519439697, 0.7022228837013245, 0.6090353727340698, 0.709883451461792, 0.6450250744819641, 0.6870445609092712, 0.7387176752090454, 0.7044199705123901, 0.7126917839050293, 0.6540697813034058, 0.689795732498169, 0.7300739288330078, 0.7327490448951721, 0.6943846344947815, 0.6286980509757996, 0.6755726337432861, 0.6358168125152588, 0.6176337599754333, 0.6843286156654358]
[2025-06-13 21:36:26,984]: Mean: 0.66377711
[2025-06-13 21:36:26,984]: Min: 0.49227673
[2025-06-13 21:36:26,985]: Max: 0.85522336
[2025-06-13 21:36:26,986]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:36:26,987]: Sample Values (25 elements): [-0.07713168114423752, 0.07713168114423752, 0.03856584057211876, 0.07713168114423752, 0.0, 0.0, -0.03856584057211876, 0.11569751799106598, 0.0, 0.03856584057211876, 0.0, 0.0, 0.0, -0.03856584057211876, 0.11569751799106598, 0.03856584057211876, -0.1928292065858841, 0.0, -0.03856584057211876, -0.03856584057211876, 0.07713168114423752, 0.15426336228847504, 0.0, -0.03856584057211876, 0.03856584057211876]
[2025-06-13 21:36:26,987]: Mean: -0.00045717
[2025-06-13 21:36:26,988]: Min: -0.26996088
[2025-06-13 21:36:26,988]: Max: 0.30852672
[2025-06-13 21:36:26,988]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,988]: Sample Values (25 elements): [0.9461368322372437, 0.8733513355255127, 0.8148813843727112, 1.1033793687820435, 0.962266743183136, 0.9920476078987122, 0.9530385732650757, 1.0705857276916504, 0.9835644364356995, 0.8063319325447083, 0.9195132255554199, 0.9138848185539246, 0.8494082689285278, 0.9396560788154602, 0.945513904094696, 0.8901166319847107, 0.840740442276001, 1.122690200805664, 0.9869003295898438, 0.9735470414161682, 0.8187416791915894, 0.8439954519271851, 0.9363220930099487, 1.1296603679656982, 0.8648504018783569]
[2025-06-13 21:36:26,988]: Mean: 0.94388425
[2025-06-13 21:36:26,989]: Min: 0.68399411
[2025-06-13 21:36:26,989]: Max: 1.25235379
[2025-06-13 21:36:26,991]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:36:26,991]: Sample Values (25 elements): [-0.03592836484313011, 0.0, 0.07185672968626022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03592836484313011, 0.07185672968626022, 0.10778509080410004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10778509080410004, 0.0, -0.07185672968626022, 0.0, 0.03592836484313011, -0.03592836484313011, 0.17964182794094086]
[2025-06-13 21:36:26,992]: Mean: -0.00154087
[2025-06-13 21:36:26,992]: Min: -0.25149855
[2025-06-13 21:36:26,992]: Max: 0.28742692
[2025-06-13 21:36:26,992]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,993]: Sample Values (25 elements): [5.749289755385689e-08, 5.820012911880263e-41, 0.46326807141304016, 0.7475496530532837, 0.5112901926040649, 0.639465868473053, 0.3014918267726898, 0.38328874111175537, 0.6677333116531372, 0.45749932527542114, 0.5740760564804077, 0.7233647108078003, 0.5899052023887634, 6.067342090833593e-41, 5.404948306747252e-41, -5.700902542412653e-41, 0.6940388679504395, 0.609706699848175, 0.596398651599884, -5.170370943819278e-41, 0.0005213827826082706, 0.6496210098266602, 0.5622883439064026, -6.018997293814387e-41, 0.5549548864364624]
[2025-06-13 21:36:26,993]: Mean: 0.46246111
[2025-06-13 21:36:26,993]: Min: -0.00000000
[2025-06-13 21:36:26,993]: Max: 0.88413322
[2025-06-13 21:36:26,995]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 21:36:26,996]: Sample Values (25 elements): [0.0, 0.02547035738825798, 0.02547035738825798, 0.0, -0.02547035738825798, -0.05094071477651596, 0.0, -0.02547035738825798, 0.05094071477651596, -0.02547035738825798, 0.0, 0.0, 0.0, 0.07641106843948364, 0.0, 0.05094071477651596, 0.0, -0.02547035738825798, 0.0, -0.05094071477651596, 0.10188142955303192, -0.02547035738825798, -0.02547035738825798, 0.05094071477651596, -0.02547035738825798]
[2025-06-13 21:36:26,996]: Mean: 0.00279549
[2025-06-13 21:36:26,996]: Min: -0.20376286
[2025-06-13 21:36:26,997]: Max: 0.17829250
[2025-06-13 21:36:26,997]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-13 21:36:26,997]: Sample Values (25 elements): [0.9433040618896484, 1.1124966144561768, 0.8564373850822449, 1.011236310005188, 0.8946216106414795, 0.9983657002449036, 1.0007777214050293, 0.983923077583313, 0.9719828963279724, 1.0935053825378418, 0.7866291403770447, 0.9225145578384399, 0.9507225155830383, 1.0970958471298218, 0.8744014501571655, 1.051041603088379, 0.8356459140777588, 0.9678270816802979, 0.8336572051048279, 0.950268566608429, 0.928410530090332, 0.9186672568321228, 0.865627110004425, 0.8984525203704834, 0.8482494950294495]
[2025-06-13 21:36:26,998]: Mean: 0.93506640
[2025-06-13 21:36:26,998]: Min: 0.73624551
[2025-06-13 21:36:26,998]: Max: 1.11249661
[2025-06-13 21:36:26,998]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-13 21:36:26,999]: Sample Values (25 elements): [-0.2366138994693756, -0.565018355846405, -0.5551360845565796, -0.2962406277656555, 0.306253045797348, 0.1954936385154724, -0.3801548480987549, 0.3641114830970764, 0.16012591123580933, 0.15567058324813843, -0.06037599965929985, -0.5002527832984924, 0.17461518943309784, 0.24734431505203247, 0.03134233132004738, -0.06501054763793945, 0.022813159972429276, -0.13126562535762787, -0.415987491607666, -0.2486894726753235, -0.2651250958442688, -0.1984270066022873, 0.10243627429008484, -0.4447917342185974, -0.23319938778877258]
[2025-06-13 21:36:26,999]: Mean: -0.06972664
[2025-06-13 21:36:26,999]: Min: -0.82235289
[2025-06-13 21:36:26,999]: Max: 0.45055941
[2025-06-13 21:36:26,999]: 


QAT of ResNet20 with parametrized_relu down to 3 bits...
[2025-06-13 21:36:27,333]: [ResNet20_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-06-13 21:36:27,417]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 21:37:48,492]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.4314 Train Acc: 0.8484 Eval Loss: 0.7403 Eval Acc: 0.7798 (LR: 0.00100000)
[2025-06-13 21:39:10,143]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.4237 Train Acc: 0.8527 Eval Loss: 0.7188 Eval Acc: 0.7819 (LR: 0.00100000)
[2025-06-13 21:40:30,550]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.4249 Train Acc: 0.8529 Eval Loss: 0.6982 Eval Acc: 0.7802 (LR: 0.00100000)
[2025-06-13 21:41:47,620]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.4222 Train Acc: 0.8534 Eval Loss: 0.5783 Eval Acc: 0.8138 (LR: 0.00100000)
[2025-06-13 21:43:04,906]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.4226 Train Acc: 0.8510 Eval Loss: 0.5776 Eval Acc: 0.8189 (LR: 0.00100000)
[2025-06-13 21:44:21,548]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.4243 Train Acc: 0.8535 Eval Loss: 0.5400 Eval Acc: 0.8301 (LR: 0.00100000)
[2025-06-13 21:45:56,574]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.4220 Train Acc: 0.8522 Eval Loss: 0.5874 Eval Acc: 0.8100 (LR: 0.00100000)
[2025-06-13 21:47:22,315]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.4174 Train Acc: 0.8551 Eval Loss: 0.6017 Eval Acc: 0.8070 (LR: 0.00100000)
[2025-06-13 21:48:44,116]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.4156 Train Acc: 0.8550 Eval Loss: 0.5771 Eval Acc: 0.8119 (LR: 0.00100000)
[2025-06-13 21:50:08,051]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.4181 Train Acc: 0.8557 Eval Loss: 0.5860 Eval Acc: 0.8143 (LR: 0.00100000)
[2025-06-13 21:51:29,354]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.4095 Train Acc: 0.8580 Eval Loss: 0.4983 Eval Acc: 0.8386 (LR: 0.00100000)
[2025-06-13 21:52:50,296]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.4091 Train Acc: 0.8574 Eval Loss: 0.6953 Eval Acc: 0.7908 (LR: 0.00100000)
[2025-06-13 21:54:12,276]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.4070 Train Acc: 0.8590 Eval Loss: 0.4991 Eval Acc: 0.8335 (LR: 0.00100000)
[2025-06-13 21:55:52,026]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.4054 Train Acc: 0.8592 Eval Loss: 0.7245 Eval Acc: 0.7876 (LR: 0.00100000)
[2025-06-13 21:57:29,640]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.4093 Train Acc: 0.8593 Eval Loss: 0.5194 Eval Acc: 0.8304 (LR: 0.00100000)
[2025-06-13 21:59:06,381]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.4036 Train Acc: 0.8596 Eval Loss: 0.4843 Eval Acc: 0.8431 (LR: 0.00100000)
[2025-06-13 22:00:37,558]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.3967 Train Acc: 0.8616 Eval Loss: 0.5192 Eval Acc: 0.8273 (LR: 0.00100000)
[2025-06-13 22:02:18,433]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.3975 Train Acc: 0.8605 Eval Loss: 0.4484 Eval Acc: 0.8574 (LR: 0.00100000)
[2025-06-13 22:03:59,770]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.3924 Train Acc: 0.8629 Eval Loss: 0.4463 Eval Acc: 0.8474 (LR: 0.00100000)
[2025-06-13 22:05:42,665]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.3963 Train Acc: 0.8622 Eval Loss: 0.4989 Eval Acc: 0.8364 (LR: 0.00100000)
[2025-06-13 22:07:05,823]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.3990 Train Acc: 0.8602 Eval Loss: 0.6255 Eval Acc: 0.8083 (LR: 0.00100000)
[2025-06-13 22:08:28,877]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.3917 Train Acc: 0.8627 Eval Loss: 0.6477 Eval Acc: 0.7968 (LR: 0.00100000)
[2025-06-13 22:09:51,895]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.3868 Train Acc: 0.8643 Eval Loss: 0.5202 Eval Acc: 0.8341 (LR: 0.00100000)
[2025-06-13 22:11:17,770]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.3864 Train Acc: 0.8658 Eval Loss: 0.5067 Eval Acc: 0.8325 (LR: 0.00100000)
[2025-06-13 22:12:43,036]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.3842 Train Acc: 0.8658 Eval Loss: 0.4465 Eval Acc: 0.8521 (LR: 0.00010000)
[2025-06-13 22:14:11,238]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.3130 Train Acc: 0.8923 Eval Loss: 0.3584 Eval Acc: 0.8796 (LR: 0.00010000)
[2025-06-13 22:15:56,438]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.2903 Train Acc: 0.8985 Eval Loss: 0.3612 Eval Acc: 0.8789 (LR: 0.00010000)
[2025-06-13 22:17:26,313]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.2854 Train Acc: 0.9011 Eval Loss: 0.3410 Eval Acc: 0.8847 (LR: 0.00010000)
[2025-06-13 22:18:53,776]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.2759 Train Acc: 0.9043 Eval Loss: 0.3718 Eval Acc: 0.8773 (LR: 0.00010000)
[2025-06-13 22:20:21,656]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.2718 Train Acc: 0.9050 Eval Loss: 0.3558 Eval Acc: 0.8848 (LR: 0.00010000)
[2025-06-13 22:21:51,174]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.2728 Train Acc: 0.9056 Eval Loss: 0.3598 Eval Acc: 0.8806 (LR: 0.00010000)
[2025-06-13 22:23:19,527]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.2704 Train Acc: 0.9034 Eval Loss: 0.3577 Eval Acc: 0.8814 (LR: 0.00010000)
[2025-06-13 22:24:49,243]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.2685 Train Acc: 0.9065 Eval Loss: 0.3570 Eval Acc: 0.8833 (LR: 0.00010000)
[2025-06-13 22:26:17,325]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.2659 Train Acc: 0.9066 Eval Loss: 0.3566 Eval Acc: 0.8811 (LR: 0.00001000)
[2025-06-13 22:27:45,106]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.2534 Train Acc: 0.9124 Eval Loss: 0.3381 Eval Acc: 0.8887 (LR: 0.00001000)
[2025-06-13 22:29:13,576]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.2502 Train Acc: 0.9114 Eval Loss: 0.3465 Eval Acc: 0.8869 (LR: 0.00001000)
[2025-06-13 22:30:41,703]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.2452 Train Acc: 0.9150 Eval Loss: 0.3792 Eval Acc: 0.8789 (LR: 0.00001000)
[2025-06-13 22:32:06,354]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.2471 Train Acc: 0.9134 Eval Loss: 0.3347 Eval Acc: 0.8867 (LR: 0.00001000)
[2025-06-13 22:33:30,082]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.2441 Train Acc: 0.9161 Eval Loss: 0.3428 Eval Acc: 0.8870 (LR: 0.00001000)
[2025-06-13 22:34:59,009]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.2444 Train Acc: 0.9142 Eval Loss: 0.3443 Eval Acc: 0.8872 (LR: 0.00001000)
[2025-06-13 22:36:28,308]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.2459 Train Acc: 0.9131 Eval Loss: 0.3403 Eval Acc: 0.8872 (LR: 0.00001000)
[2025-06-13 22:37:56,613]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.2427 Train Acc: 0.9157 Eval Loss: 0.3423 Eval Acc: 0.8859 (LR: 0.00001000)
[2025-06-13 22:39:24,980]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.2412 Train Acc: 0.9144 Eval Loss: 0.3374 Eval Acc: 0.8902 (LR: 0.00001000)
[2025-06-13 22:40:52,955]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.2418 Train Acc: 0.9153 Eval Loss: 0.3343 Eval Acc: 0.8899 (LR: 0.00001000)
[2025-06-13 22:42:22,633]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.2403 Train Acc: 0.9164 Eval Loss: 0.3511 Eval Acc: 0.8856 (LR: 0.00001000)
[2025-06-13 22:43:50,785]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.2396 Train Acc: 0.9170 Eval Loss: 0.3335 Eval Acc: 0.8913 (LR: 0.00001000)
[2025-06-13 22:45:18,366]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.2410 Train Acc: 0.9168 Eval Loss: 0.3434 Eval Acc: 0.8887 (LR: 0.00001000)
[2025-06-13 22:46:46,184]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.2413 Train Acc: 0.9156 Eval Loss: 0.3461 Eval Acc: 0.8897 (LR: 0.00001000)
[2025-06-13 22:48:12,400]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.2389 Train Acc: 0.9164 Eval Loss: 0.3382 Eval Acc: 0.8896 (LR: 0.00001000)
[2025-06-13 22:49:41,923]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.2377 Train Acc: 0.9167 Eval Loss: 0.3487 Eval Acc: 0.8870 (LR: 0.00001000)
[2025-06-13 22:51:07,641]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.2389 Train Acc: 0.9150 Eval Loss: 0.3467 Eval Acc: 0.8880 (LR: 0.00001000)
[2025-06-13 22:52:38,901]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.2398 Train Acc: 0.9166 Eval Loss: 0.3345 Eval Acc: 0.8907 (LR: 0.00000100)
[2025-06-13 22:54:19,016]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.2344 Train Acc: 0.9198 Eval Loss: 0.3300 Eval Acc: 0.8905 (LR: 0.00000100)
[2025-06-13 22:55:57,028]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.2324 Train Acc: 0.9189 Eval Loss: 0.3314 Eval Acc: 0.8908 (LR: 0.00000100)
[2025-06-13 22:57:36,576]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.2352 Train Acc: 0.9164 Eval Loss: 0.3300 Eval Acc: 0.8921 (LR: 0.00000100)
[2025-06-13 22:59:14,564]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.2335 Train Acc: 0.9197 Eval Loss: 0.3345 Eval Acc: 0.8893 (LR: 0.00000100)
[2025-06-13 23:00:51,407]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.2364 Train Acc: 0.9169 Eval Loss: 0.3287 Eval Acc: 0.8919 (LR: 0.00000100)
[2025-06-13 23:02:29,160]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.2323 Train Acc: 0.9189 Eval Loss: 0.3253 Eval Acc: 0.8925 (LR: 0.00000100)
[2025-06-13 23:04:05,107]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.2387 Train Acc: 0.9152 Eval Loss: 0.3287 Eval Acc: 0.8922 (LR: 0.00000100)
[2025-06-13 23:05:41,654]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.2380 Train Acc: 0.9159 Eval Loss: 0.3282 Eval Acc: 0.8933 (LR: 0.00000100)
[2025-06-13 23:05:41,736]: [ResNet20_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.8933
[2025-06-13 23:05:41,936]: 


Quantization of model down to 3 bits finished
[2025-06-13 23:05:41,936]: Model Architecture:
[2025-06-13 23:05:42,127]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4824], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.3765816688537598)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1645], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5756596922874451, max_val=0.5756545066833496)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3872], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1779], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6246631145477295, max_val=0.6203001141548157)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6036], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1644], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5805715322494507, max_val=0.570224940776825)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3577], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1821], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.682637095451355, max_val=0.5917913913726807)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6888], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2681], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7647237181663513, max_val=1.1118967533111572)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3237], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1851], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6919574737548828, max_val=0.603705883026123)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7071], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1612], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5563075542449951, max_val=0.5719743967056274)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4192], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1416], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.44795602560043335, max_val=0.5432748198509216)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1692], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5782284140586853, max_val=0.6062295436859131)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5849], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1194], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4103981554508209, max_val=0.4251687526702881)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3529], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1219], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4245527982711792, max_val=0.4287119209766388)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6663], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1329], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4703117311000824, max_val=0.45992857217788696)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3299], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1361], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4252127408981323, max_val=0.527305543422699)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7163], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1307], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40849483013153076, max_val=0.5065416693687439)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4276], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1221], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4174114167690277, max_val=0.43757331371307373)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1350], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.449741005897522, max_val=0.49541181325912476)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5828], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1167], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41514286398887634, max_val=0.40200895071029663)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3638], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0901], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3199145197868347, max_val=0.31082040071487427)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5083], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0812], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2779584228992462, max_val=0.29036858677864075)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2661], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0613], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21427202224731445, max_val=0.21469634771347046)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7457], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 23:05:42,127]: 
Model Weights:
[2025-06-13 23:05:42,127]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-13 23:05:42,128]: Sample Values (25 elements): [0.016502197831869125, 0.05077934265136719, -0.15756738185882568, 0.14780113101005554, 0.2153596431016922, -0.33280372619628906, 0.006385618355125189, 0.04183183237910271, -0.15524590015411377, -0.0229994785040617, -0.07125834375619888, 0.10174163430929184, 0.004955396521836519, 0.1508074253797531, 0.10057926177978516, -0.1483963429927826, 0.061478808522224426, -0.19797658920288086, -0.3990219533443451, -0.20978911221027374, -0.09692024439573288, 0.0635700672864914, -0.08396243304014206, 0.19415034353733063, -0.02811683528125286]
[2025-06-13 23:05:42,128]: Mean: -0.00078477
[2025-06-13 23:05:42,129]: Min: -0.60576439
[2025-06-13 23:05:42,129]: Max: 0.56712437
[2025-06-13 23:05:42,129]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,130]: Sample Values (16 elements): [0.973598301410675, 1.2778723239898682, 1.3629214763641357, 1.2833747863769531, 1.0771774053573608, 1.378132700920105, 0.7610889673233032, 1.2846201658248901, 1.1561204195022583, 0.9513272047042847, 1.2320044040679932, 1.3946032524108887, 1.4945929050445557, 1.1637110710144043, 1.3293994665145874, 1.0936925411224365]
[2025-06-13 23:05:42,130]: Mean: 1.20088983
[2025-06-13 23:05:42,130]: Min: 0.76108897
[2025-06-13 23:05:42,130]: Max: 1.49459291
[2025-06-13 23:05:42,132]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 23:05:42,132]: Sample Values (25 elements): [0.16447344422340393, 0.0, 0.0, 0.0, -0.16447344422340393, 0.16447344422340393, 0.0, 0.16447344422340393, 0.16447344422340393, 0.16447344422340393, -0.32894688844680786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32894688844680786, -0.16447344422340393, -0.16447344422340393, 0.0, 0.0, -0.32894688844680786, 0.0]
[2025-06-13 23:05:42,132]: Mean: -0.01456275
[2025-06-13 23:05:42,132]: Min: -0.49342033
[2025-06-13 23:05:42,133]: Max: 0.49342033
[2025-06-13 23:05:42,133]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,133]: Sample Values (16 elements): [0.879578709602356, 0.9659919738769531, 1.0030198097229004, 1.0989494323730469, 0.8066822290420532, 0.7516560554504395, 0.649756908416748, 0.8096928596496582, 1.0553380250930786, 0.8068282008171082, 0.7678810358047485, 0.8144240975379944, 0.8015745878219604, 0.9188401103019714, 1.0686153173446655, 1.0237079858779907]
[2025-06-13 23:05:42,133]: Mean: 0.88890862
[2025-06-13 23:05:42,133]: Min: 0.64975691
[2025-06-13 23:05:42,134]: Max: 1.09894943
[2025-06-13 23:05:42,135]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 23:05:42,135]: Sample Values (25 elements): [-0.17785188555717468, 0.0, -0.17785188555717468, 0.0, -0.17785188555717468, -0.17785188555717468, 0.17785188555717468, -0.17785188555717468, -0.17785188555717468, -0.17785188555717468, 0.17785188555717468, 0.0, 0.0, 0.0, -0.17785188555717468, -0.35570377111434937, 0.0, -0.17785188555717468, -0.35570377111434937, -0.35570377111434937, 0.17785188555717468, 0.0, 0.17785188555717468, 0.17785188555717468, -0.17785188555717468]
[2025-06-13 23:05:42,135]: Mean: -0.00370525
[2025-06-13 23:05:42,136]: Min: -0.71140754
[2025-06-13 23:05:42,136]: Max: 0.53355563
[2025-06-13 23:05:42,136]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,136]: Sample Values (16 elements): [0.8350275158882141, 1.0504673719406128, 0.7778682708740234, 0.8275556564331055, 1.127090334892273, 0.7319378852844238, 0.6098637580871582, 0.8427768349647522, 0.86508709192276, 0.8812674880027771, 0.6696420907974243, 0.7867084741592407, 0.8139290809631348, 0.8254204392433167, 1.0117872953414917, 0.7912993431091309]
[2025-06-13 23:05:42,136]: Mean: 0.84048307
[2025-06-13 23:05:42,137]: Min: 0.60986376
[2025-06-13 23:05:42,137]: Max: 1.12709033
[2025-06-13 23:05:42,138]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 23:05:42,139]: Sample Values (25 elements): [-0.16439950466156006, 0.16439950466156006, 0.0, 0.3287990093231201, 0.16439950466156006, 0.0, 0.16439950466156006, 0.0, -0.16439950466156006, -0.16439950466156006, 0.16439950466156006, 0.0, 0.0, 0.0, -0.16439950466156006, 0.16439950466156006, 0.0, 0.0, -0.16439950466156006, 0.0, 0.0, -0.16439950466156006, 0.0, -0.16439950466156006, 0.0]
[2025-06-13 23:05:42,139]: Mean: -0.01077445
[2025-06-13 23:05:42,140]: Min: -0.65759802
[2025-06-13 23:05:42,140]: Max: 0.49319851
[2025-06-13 23:05:42,140]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,140]: Sample Values (16 elements): [0.8173220753669739, 0.7698521614074707, 0.713494598865509, 0.7083570957183838, 0.9036264419555664, 0.6717954277992249, 0.7510091066360474, 0.7599924206733704, 0.883000910282135, 1.2472985982894897, 0.8561911582946777, 0.862135648727417, 0.7892153859138489, 0.8443356156349182, 0.7270481586456299, 0.9467393159866333]
[2025-06-13 23:05:42,141]: Mean: 0.82821333
[2025-06-13 23:05:42,141]: Min: 0.67179543
[2025-06-13 23:05:42,141]: Max: 1.24729860
[2025-06-13 23:05:42,142]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 23:05:42,142]: Sample Values (25 elements): [0.18206121027469635, -0.18206121027469635, 0.18206121027469635, -0.18206121027469635, 0.0, 0.0, -0.18206121027469635, 0.0, 0.18206121027469635, 0.0, 0.0, 0.18206121027469635, 0.0, 0.0, 0.3641224205493927, 0.0, 0.0, 0.0, -0.18206121027469635, 0.0, 0.0, -0.18206121027469635, -0.18206121027469635, 0.0, 0.18206121027469635]
[2025-06-13 23:05:42,143]: Mean: -0.00110627
[2025-06-13 23:05:42,143]: Min: -0.72824484
[2025-06-13 23:05:42,143]: Max: 0.54618365
[2025-06-13 23:05:42,143]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,143]: Sample Values (16 elements): [0.7958450317382812, 0.6517595052719116, 0.7504345774650574, 0.6239216923713684, 0.8932291269302368, 0.9025189876556396, 0.6091300845146179, 0.8515805006027222, 0.6867806315422058, 0.5735200047492981, 0.8619590997695923, 0.6248979568481445, 0.6704933643341064, 0.654859185218811, 1.075973391532898, 0.6785003542900085]
[2025-06-13 23:05:42,144]: Mean: 0.74408770
[2025-06-13 23:05:42,144]: Min: 0.57352000
[2025-06-13 23:05:42,144]: Max: 1.07597339
[2025-06-13 23:05:42,145]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 23:05:42,145]: Sample Values (25 elements): [0.0, 0.0, -0.2680886685848236, 0.0, 0.0, -0.5361773371696472, -0.2680886685848236, 0.0, -0.2680886685848236, 0.0, -0.2680886685848236, 0.0, 0.0, 0.2680886685848236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2680886685848236, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 23:05:42,145]: Mean: -0.00477068
[2025-06-13 23:05:42,146]: Min: -0.80426598
[2025-06-13 23:05:42,146]: Max: 1.07235467
[2025-06-13 23:05:42,146]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,146]: Sample Values (16 elements): [0.8963849544525146, 0.6293235421180725, 0.6457623243331909, 0.9766620993614197, 0.9893426299095154, 0.9246299862861633, 0.975976824760437, 0.9056510329246521, 0.5630785822868347, 0.8780273795127869, 0.8926751613616943, 0.6923803687095642, 1.4498369693756104, 1.158990740776062, 0.7444065809249878, 0.8252288103103638]
[2025-06-13 23:05:42,146]: Mean: 0.88427234
[2025-06-13 23:05:42,147]: Min: 0.56307858
[2025-06-13 23:05:42,147]: Max: 1.44983697
[2025-06-13 23:05:42,148]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-13 23:05:42,149]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.18509477376937866, -0.18509477376937866, 0.0, 0.0, -0.18509477376937866, -0.3701895475387573, 0.0, 0.0, 0.0, 0.0, -0.18509477376937866, 0.18509477376937866, 0.18509477376937866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18509477376937866]
[2025-06-13 23:05:42,149]: Mean: -0.00674825
[2025-06-13 23:05:42,149]: Min: -0.74037910
[2025-06-13 23:05:42,149]: Max: 0.55528432
[2025-06-13 23:05:42,149]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-13 23:05:42,150]: Sample Values (16 elements): [1.2740262746810913, 0.8673460483551025, 1.4579472541809082, 0.9800178408622742, 1.0466070175170898, 0.9254463911056519, 0.6434754133224487, 0.5714324712753296, 0.8585665225982666, 0.5408633947372437, 0.5971835255622864, 0.9869311451911926, 0.6452399492263794, 0.6307736039161682, 0.8175145983695984, 0.8274760246276855]
[2025-06-13 23:05:42,150]: Mean: 0.85442799
[2025-06-13 23:05:42,150]: Min: 0.54086339
[2025-06-13 23:05:42,150]: Max: 1.45794725
[2025-06-13 23:05:42,151]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-13 23:05:42,152]: Sample Values (25 elements): [0.16118311882019043, 0.16118311882019043, 0.32236623764038086, 0.16118311882019043, 0.32236623764038086, 0.16118311882019043, 0.16118311882019043, -0.16118311882019043, 0.0, 0.16118311882019043, 0.32236623764038086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16118311882019043, 0.16118311882019043, 0.16118311882019043, 0.0, 0.0, 0.16118311882019043, -0.16118311882019043, -0.32236623764038086, 0.0]
[2025-06-13 23:05:42,152]: Mean: -0.00521187
[2025-06-13 23:05:42,152]: Min: -0.48354936
[2025-06-13 23:05:42,152]: Max: 0.64473248
[2025-06-13 23:05:42,152]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,152]: Sample Values (25 elements): [0.8773682117462158, 0.7802548408508301, 1.034119725227356, 0.8208639621734619, 0.9286456108093262, 0.8853104114532471, 0.8516387939453125, 0.8910672068595886, 0.8293259739875793, 0.7886456847190857, 0.8313204646110535, 0.8709936738014221, 0.8281221389770508, 0.8536226749420166, 0.8388153910636902, 0.6988442540168762, 0.947513997554779, 0.7226939797401428, 0.9343998432159424, 0.7657062411308289, 0.8691868185997009, 0.7550166845321655, 0.7424651980400085, 0.853060245513916, 0.857826292514801]
[2025-06-13 23:05:42,152]: Mean: 0.85010350
[2025-06-13 23:05:42,153]: Min: 0.69884425
[2025-06-13 23:05:42,153]: Max: 1.03859091
[2025-06-13 23:05:42,154]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 23:05:42,154]: Sample Values (25 elements): [0.0, -0.14160440862178802, 0.0, 0.0, 0.14160440862178802, 0.0, 0.14160440862178802, 0.0, 0.0, -0.14160440862178802, -0.14160440862178802, 0.14160440862178802, 0.0, 0.0, -0.28320881724357605, 0.28320881724357605, 0.0, -0.14160440862178802, -0.14160440862178802, 0.14160440862178802, -0.14160440862178802, 0.0, -0.14160440862178802, -0.14160440862178802, 0.0]
[2025-06-13 23:05:42,154]: Mean: -0.00067606
[2025-06-13 23:05:42,154]: Min: -0.42481321
[2025-06-13 23:05:42,155]: Max: 0.56641763
[2025-06-13 23:05:42,155]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,155]: Sample Values (25 elements): [0.9221817255020142, 0.8981074690818787, 0.9390738606452942, 0.7793143391609192, 0.8917195796966553, 0.7858272194862366, 0.922915518283844, 0.9797160625457764, 0.8607363104820251, 0.9132175445556641, 0.9302165508270264, 0.9405879378318787, 0.9640275239944458, 0.8697561621665955, 1.0592989921569824, 0.9388526082038879, 0.9654003977775574, 0.871327817440033, 0.9076429605484009, 0.8933113813400269, 0.9962888360023499, 0.8919907212257385, 1.0154832601547241, 0.8774999380111694, 0.900226354598999]
[2025-06-13 23:05:42,155]: Mean: 0.92525983
[2025-06-13 23:05:42,155]: Min: 0.77931434
[2025-06-13 23:05:42,155]: Max: 1.08081365
[2025-06-13 23:05:42,157]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-13 23:05:42,157]: Sample Values (25 elements): [0.16920825839042664, 0.0, 0.0, 0.0, 0.0, -0.16920825839042664, 0.0, 0.0, 0.16920825839042664, -0.5076247453689575, -0.5076247453689575, 0.16920825839042664, -0.16920825839042664, 0.0, 0.33841651678085327, -0.5076247453689575, 0.0, -0.16920825839042664, 0.16920825839042664, 0.33841651678085327, -0.16920825839042664, 0.0, 0.16920825839042664, 0.0, 0.0]
[2025-06-13 23:05:42,158]: Mean: 0.01123649
[2025-06-13 23:05:42,158]: Min: -0.50762475
[2025-06-13 23:05:42,158]: Max: 0.67683303
[2025-06-13 23:05:42,158]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,158]: Sample Values (25 elements): [0.6745452880859375, 0.7992112040519714, 0.6012943983078003, 0.46508103609085083, 0.7254725694656372, 0.5214834213256836, 0.47095799446105957, 0.6512773633003235, 0.5658899545669556, 0.4759349822998047, 0.6340122222900391, 0.44410595297813416, 0.4539996385574341, 0.4710862338542938, 0.5338299870491028, 0.5499322414398193, 0.5017529726028442, 0.6505571603775024, 0.6257888674736023, 0.7520421147346497, 0.6520009636878967, 0.5274545550346375, 0.5914326310157776, 0.9248504638671875, 0.5889806747436523]
[2025-06-13 23:05:42,159]: Mean: 0.59901118
[2025-06-13 23:05:42,159]: Min: 0.44410595
[2025-06-13 23:05:42,159]: Max: 0.96398300
[2025-06-13 23:05:42,161]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 23:05:42,161]: Sample Values (25 elements): [0.11936668306589127, 0.0, 0.0, 0.0, -0.11936668306589127, 0.0, -0.11936668306589127, -0.3581000566482544, 0.0, -0.23873336613178253, 0.0, 0.11936668306589127, 0.11936668306589127, -0.11936668306589127, 0.0, -0.11936668306589127, 0.11936668306589127, -0.11936668306589127, 0.11936668306589127, 0.0, 0.11936668306589127, -0.11936668306589127, 0.0, -0.23873336613178253, 0.0]
[2025-06-13 23:05:42,162]: Mean: -0.01227861
[2025-06-13 23:05:42,162]: Min: -0.35810006
[2025-06-13 23:05:42,162]: Max: 0.47746673
[2025-06-13 23:05:42,162]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,162]: Sample Values (25 elements): [0.8452930450439453, 0.6598498225212097, 0.7575404644012451, 0.711094081401825, 0.8088640570640564, 0.8204013705253601, 0.7807197570800781, 0.6987992525100708, 0.813199520111084, 0.815913200378418, 0.7580761313438416, 0.8149672746658325, 0.747938871383667, 0.920922040939331, 0.8252790570259094, 0.8017863631248474, 0.69251948595047, 0.838899552822113, 0.888899564743042, 0.6360868215560913, 1.102199673652649, 0.7157998085021973, 0.7104997038841248, 0.6022720336914062, 0.777504026889801]
[2025-06-13 23:05:42,163]: Mean: 0.76845139
[2025-06-13 23:05:42,163]: Min: 0.59874171
[2025-06-13 23:05:42,163]: Max: 1.10219967
[2025-06-13 23:05:42,164]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 23:05:42,165]: Sample Values (25 elements): [0.0, -0.12189497798681259, -0.12189497798681259, 0.12189497798681259, -0.12189497798681259, 0.12189497798681259, -0.12189497798681259, 0.0, 0.0, -0.12189497798681259, 0.0, -0.12189497798681259, 0.0, -0.12189497798681259, 0.0, -0.12189497798681259, 0.0, -0.12189497798681259, -0.12189497798681259, 0.24378995597362518, 0.12189497798681259, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 23:05:42,165]: Mean: -0.00234108
[2025-06-13 23:05:42,165]: Min: -0.36568493
[2025-06-13 23:05:42,165]: Max: 0.48757991
[2025-06-13 23:05:42,165]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,166]: Sample Values (25 elements): [0.6909453272819519, 0.7179857492446899, 0.8062317967414856, 0.6805993914604187, 0.8654313683509827, 0.6902510523796082, 0.8015570640563965, 0.7115082144737244, 0.6220493316650391, 0.8128570914268494, 0.7050292491912842, 0.7306352257728577, 0.807689905166626, 0.6502153277397156, 0.7368771433830261, 0.829882800579071, 0.8512439727783203, 0.8226003646850586, 0.769234836101532, 0.7617886662483215, 0.8694296479225159, 0.7356183528900146, 0.6142284274101257, 0.7636812329292297, 0.9084281921386719]
[2025-06-13 23:05:42,166]: Mean: 0.75425422
[2025-06-13 23:05:42,166]: Min: 0.61422843
[2025-06-13 23:05:42,166]: Max: 0.90881336
[2025-06-13 23:05:42,168]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 23:05:42,168]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.1328914612531662, -0.1328914612531662, 0.0, -0.1328914612531662, 0.0, 0.1328914612531662, 0.0, 0.0, 0.0, 0.0, -0.1328914612531662, -0.1328914612531662, 0.1328914612531662, 0.0, -0.1328914612531662, -0.1328914612531662, 0.1328914612531662, 0.1328914612531662, -0.3986743688583374, 0.0, 0.1328914612531662, -0.1328914612531662]
[2025-06-13 23:05:42,169]: Mean: -0.01362657
[2025-06-13 23:05:42,169]: Min: -0.53156585
[2025-06-13 23:05:42,169]: Max: 0.39867437
[2025-06-13 23:05:42,169]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,170]: Sample Values (25 elements): [0.7308303117752075, 0.8197368383407593, 0.6991554498672485, 0.825365424156189, 0.676771879196167, 0.7591410279273987, 0.7303487062454224, 0.7331200242042542, 0.8572314977645874, 0.7850611209869385, 0.8602374792098999, 0.6108773946762085, 0.7295640707015991, 0.6043711304664612, 0.6963610649108887, 0.7461773157119751, 0.6877521276473999, 0.7177373766899109, 0.7817602753639221, 0.6577742099761963, 0.5433760285377502, 0.7360584735870361, 0.5329196453094482, 0.6804377436637878, 0.72598797082901]
[2025-06-13 23:05:42,170]: Mean: 0.70959091
[2025-06-13 23:05:42,170]: Min: 0.53291965
[2025-06-13 23:05:42,170]: Max: 0.86023748
[2025-06-13 23:05:42,172]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-13 23:05:42,172]: Sample Values (25 elements): [0.0, -0.13607405126094818, 0.13607405126094818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13607405126094818, 0.0, 0.27214810252189636, 0.0, 0.0, -0.13607405126094818, 0.13607405126094818, -0.13607405126094818, 0.13607405126094818, 0.13607405126094818, -0.13607405126094818, 0.0, 0.0, 0.13607405126094818, 0.0, 0.13607405126094818, 0.0]
[2025-06-13 23:05:42,172]: Mean: -0.00450332
[2025-06-13 23:05:42,172]: Min: -0.40822214
[2025-06-13 23:05:42,172]: Max: 0.54429621
[2025-06-13 23:05:42,173]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-13 23:05:42,173]: Sample Values (25 elements): [0.6935175061225891, 0.6710085272789001, 0.6189202666282654, 0.7448673844337463, 0.6658968925476074, 0.6931180357933044, 0.7231687307357788, 0.567959725856781, 0.5474550724029541, 0.613805890083313, 0.6485916972160339, 0.7944884300231934, 0.6365863680839539, 0.7625370025634766, 0.5843386650085449, 0.7248948812484741, 0.706623375415802, 1.1045784950256348, 0.8135458827018738, 0.747799277305603, 0.539678156375885, 0.5450194478034973, 0.6413854956626892, 0.6823139786720276, 0.6641252040863037]
[2025-06-13 23:05:42,173]: Mean: 0.68989998
[2025-06-13 23:05:42,173]: Min: 0.53967816
[2025-06-13 23:05:42,173]: Max: 1.10457850
[2025-06-13 23:05:42,174]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-13 23:05:42,175]: Sample Values (25 elements): [0.13071949779987335, 0.13071949779987335, 0.2614389955997467, 0.0, 0.0, 0.0, 0.0, -0.13071949779987335, 0.0, 0.2614389955997467, 0.13071949779987335, 0.0, -0.13071949779987335, 0.13071949779987335, 0.0, 0.0, 0.13071949779987335, -0.2614389955997467, 0.13071949779987335, -0.13071949779987335, -0.13071949779987335, 0.0, 0.0, -0.2614389955997467, 0.13071949779987335]
[2025-06-13 23:05:42,175]: Mean: -0.00407080
[2025-06-13 23:05:42,175]: Min: -0.39215851
[2025-06-13 23:05:42,176]: Max: 0.52287799
[2025-06-13 23:05:42,176]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,176]: Sample Values (25 elements): [0.7454040050506592, 0.8007394671440125, 0.8141389489173889, 0.6504966616630554, 0.8142857551574707, 0.6854067444801331, 0.7441979646682739, 0.8731637597084045, 0.8446779847145081, 0.714795708656311, 0.835663914680481, 0.7809529900550842, 0.7375714778900146, 0.8831284046173096, 0.8107728362083435, 0.7396982312202454, 0.8018984198570251, 0.7082847952842712, 0.7433707118034363, 0.9302416443824768, 0.7563485503196716, 0.8409467935562134, 0.6496371626853943, 0.7448241710662842, 0.7599385380744934]
[2025-06-13 23:05:42,176]: Mean: 0.78095144
[2025-06-13 23:05:42,176]: Min: 0.64963716
[2025-06-13 23:05:42,177]: Max: 1.16943479
[2025-06-13 23:05:42,178]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 23:05:42,179]: Sample Values (25 elements): [-0.12214068323373795, 0.0, 0.0, 0.0, 0.12214068323373795, 0.0, 0.12214068323373795, 0.0, 0.0, 0.0, 0.12214068323373795, 0.0, 0.0, 0.0, 0.12214068323373795, 0.12214068323373795, 0.12214068323373795, 0.0, 0.0, -0.12214068323373795, 0.0, 0.0, -0.12214068323373795, 0.0, 0.12214068323373795]
[2025-06-13 23:05:42,179]: Mean: -0.00685517
[2025-06-13 23:05:42,179]: Min: -0.36642206
[2025-06-13 23:05:42,180]: Max: 0.48856273
[2025-06-13 23:05:42,180]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,180]: Sample Values (25 elements): [0.7471728920936584, 0.894808828830719, 0.8763450980186462, 0.9041215777397156, 0.911065399646759, 0.9621084332466125, 0.9568960666656494, 0.9440329670906067, 0.8188730478286743, 0.9244324564933777, 0.9513735771179199, 0.8124368786811829, 0.9328408241271973, 1.1524044275283813, 0.956710934638977, 0.9056283831596375, 0.8102438449859619, 1.0603140592575073, 0.8733096718788147, 1.0239447355270386, 0.9272586107254028, 1.0344226360321045, 0.861785352230072, 0.7257863879203796, 1.0630146265029907]
[2025-06-13 23:05:42,180]: Mean: 0.91570872
[2025-06-13 23:05:42,180]: Min: 0.69417202
[2025-06-13 23:05:42,181]: Max: 1.15240443
[2025-06-13 23:05:42,182]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-13 23:05:42,196]: Sample Values (25 elements): [0.13502183556556702, -0.13502183556556702, 0.0, -0.27004367113113403, 0.27004367113113403, 0.0, -0.13502183556556702, -0.13502183556556702, 0.13502183556556702, 0.13502183556556702, 0.0, 0.27004367113113403, 0.0, -0.27004367113113403, 0.0, 0.27004367113113403, 0.0, 0.27004367113113403, -0.27004367113113403, 0.0, -0.13502183556556702, -0.27004367113113403, -0.13502183556556702, 0.13502183556556702, -0.40506550669670105]
[2025-06-13 23:05:42,196]: Mean: -0.00731808
[2025-06-13 23:05:42,196]: Min: -0.40506551
[2025-06-13 23:05:42,196]: Max: 0.54008734
[2025-06-13 23:05:42,196]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,197]: Sample Values (25 elements): [0.3812979757785797, 0.4563486576080322, 0.4540296196937561, 0.5633590817451477, 0.3797626793384552, 0.2707541286945343, 0.5349651575088501, 0.5276821255683899, 0.6602299809455872, 0.48807644844055176, 0.4174562096595764, 0.5945240259170532, 0.5483837127685547, 0.510026216506958, 0.33320510387420654, 0.46473821997642517, 0.41513580083847046, 0.5348555445671082, 0.4443965554237366, 0.5801253318786621, 0.6002741456031799, 0.5281533002853394, 0.38955506682395935, 0.4601523280143738, 0.4370187520980835]
[2025-06-13 23:05:42,197]: Mean: 0.48589844
[2025-06-13 23:05:42,197]: Min: 0.26125780
[2025-06-13 23:05:42,197]: Max: 0.68475717
[2025-06-13 23:05:42,198]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 23:05:42,199]: Sample Values (25 elements): [0.0, -0.11673595756292343, 0.0, 0.11673595756292343, -0.11673595756292343, 0.11673595756292343, 0.0, -0.23347191512584686, 0.11673595756292343, -0.11673595756292343, 0.11673595756292343, -0.11673595756292343, 0.0, -0.11673595756292343, -0.11673595756292343, 0.0, 0.0, 0.11673595756292343, 0.11673595756292343, 0.11673595756292343, 0.23347191512584686, 0.0, 0.11673595756292343, 0.0, -0.11673595756292343]
[2025-06-13 23:05:42,199]: Mean: -0.00598500
[2025-06-13 23:05:42,199]: Min: -0.46694383
[2025-06-13 23:05:42,199]: Max: 0.35020787
[2025-06-13 23:05:42,199]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,200]: Sample Values (25 elements): [0.7172446250915527, 0.6873742938041687, 0.6945874691009521, 0.688774585723877, 0.6102332472801208, 0.8560698628425598, 0.7016161680221558, 0.6573408842086792, 0.7938330769538879, 0.8750817775726318, 0.7199751734733582, 0.6922450065612793, 0.6968090534210205, 0.6228411197662354, 0.6434698104858398, 0.6931155323982239, 0.6130260825157166, 0.728350818157196, 0.6363552808761597, 0.7720029950141907, 0.7278136610984802, 0.9114179015159607, 0.7058663964271545, 0.6066009998321533, 0.7190736532211304]
[2025-06-13 23:05:42,200]: Mean: 0.68692321
[2025-06-13 23:05:42,200]: Min: 0.53144860
[2025-06-13 23:05:42,200]: Max: 0.91141790
[2025-06-13 23:05:42,202]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 23:05:42,202]: Sample Values (25 elements): [-0.09010501205921173, 0.09010501205921173, 0.0, 0.0, 0.09010501205921173, -0.18021002411842346, -0.09010501205921173, 0.0, 0.0, 0.09010501205921173, -0.09010501205921173, 0.0, 0.0, 0.09010501205921173, 0.0, 0.0, -0.09010501205921173, 0.0, 0.0, 0.0, 0.09010501205921173, -0.09010501205921173, -0.09010501205921173, 0.0, 0.0]
[2025-06-13 23:05:42,203]: Mean: -0.00155943
[2025-06-13 23:05:42,203]: Min: -0.36042005
[2025-06-13 23:05:42,203]: Max: 0.27031505
[2025-06-13 23:05:42,203]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,204]: Sample Values (25 elements): [0.8095979690551758, 0.9182446599006653, 0.7258132100105286, 0.8159005045890808, 0.8319653272628784, 0.9181435108184814, 0.9634265303611755, 0.8900085687637329, 0.8980727195739746, 0.9789851307868958, 0.9297386407852173, 0.8498992323875427, 0.9519286751747131, 0.8549425005912781, 0.9692752361297607, 0.8342905044555664, 1.0594995021820068, 0.975513219833374, 0.9581997990608215, 0.8604843616485596, 1.0291879177093506, 1.1183196306228638, 0.9811878204345703, 0.9013739824295044, 1.0627645254135132]
[2025-06-13 23:05:42,204]: Mean: 0.92227328
[2025-06-13 23:05:42,204]: Min: 0.72581321
[2025-06-13 23:05:42,204]: Max: 1.23588264
[2025-06-13 23:05:42,205]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 23:05:42,206]: Sample Values (25 elements): [-0.08118955790996552, 0.0, 0.0, 0.0, -0.08118955790996552, 0.0, -0.08118955790996552, 0.08118955790996552, -0.08118955790996552, 0.0, 0.0, -0.08118955790996552, -0.08118955790996552, 0.08118955790996552, 0.0, 0.08118955790996552, 0.08118955790996552, -0.16237911581993103, 0.08118955790996552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-13 23:05:42,206]: Mean: -0.00094483
[2025-06-13 23:05:42,206]: Min: -0.24356867
[2025-06-13 23:05:42,206]: Max: 0.32475823
[2025-06-13 23:05:42,206]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,207]: Sample Values (25 elements): [0.6977214217185974, 0.4003274738788605, 0.5945718884468079, 0.44320014119148254, 5.750009912652843e-10, 0.5634322762489319, 0.605029821395874, 0.6650963425636292, 0.6458584666252136, 1.6330929497598845e-08, 0.7267835736274719, 0.6624443531036377, 0.5717347264289856, 0.4856377840042114, 0.6078813076019287, 0.799080491065979, 1.673224519836162e-32, 0.3851791322231293, 0.6620044708251953, 0.5787830352783203, 9.981474752316951e-27, 0.4646323621273041, 0.7469150424003601, 0.7698894143104553, 0.4710521996021271]
[2025-06-13 23:05:42,207]: Mean: 0.44315961
[2025-06-13 23:05:42,207]: Min: -0.00000000
[2025-06-13 23:05:42,207]: Max: 0.83566707
[2025-06-13 23:05:42,208]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-13 23:05:42,209]: Sample Values (25 elements): [0.0, 0.0, -0.061281196773052216, -0.061281196773052216, 0.0, 0.0, 0.061281196773052216, 0.061281196773052216, 0.0, 0.061281196773052216, 0.061281196773052216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.061281196773052216, 0.061281196773052216, 0.0, -0.061281196773052216, -0.061281196773052216]
[2025-06-13 23:05:42,209]: Mean: 0.00295069
[2025-06-13 23:05:42,209]: Min: -0.18384358
[2025-06-13 23:05:42,209]: Max: 0.24512479
[2025-06-13 23:05:42,209]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-13 23:05:42,210]: Sample Values (25 elements): [0.9595613479614258, 0.951411247253418, 0.8799231052398682, 0.9260364174842834, 0.7664890289306641, 0.7606123089790344, 0.968234658241272, 0.698957085609436, 0.790180504322052, 0.9099781513214111, 1.007951259613037, 0.903310239315033, 0.8389530181884766, 0.8537782430648804, 0.8934037089347839, 0.8867613673210144, 1.0231138467788696, 0.815614640712738, 0.8120102882385254, 0.820020854473114, 0.9387197494506836, 0.9915302991867065, 0.8548287749290466, 0.7660412192344666, 0.8546615839004517]
[2025-06-13 23:05:42,210]: Mean: 0.88023776
[2025-06-13 23:05:42,210]: Min: 0.69609177
[2025-06-13 23:05:42,211]: Max: 1.08913267
[2025-06-13 23:05:42,211]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-13 23:05:42,211]: Sample Values (25 elements): [0.30967310070991516, -0.4747316837310791, 0.0819694846868515, 0.1850285679101944, -0.6511073112487793, 0.22710077464580536, 0.1720806509256363, 0.23915937542915344, 0.20289970934391022, 0.2739427983760834, -0.17620155215263367, 0.05741241201758385, 0.3644605576992035, 0.2949323058128357, 0.22444722056388855, 0.17165607213974, -0.3256993889808655, -0.2365346997976303, -0.4479675889015198, -0.278527170419693, 0.3209057152271271, 0.06837595254182816, -0.30232053995132446, 0.013282698579132557, 0.0892866775393486]
[2025-06-13 23:05:42,211]: Mean: -0.06709804
[2025-06-13 23:05:42,211]: Min: -0.80293900
[2025-06-13 23:05:42,212]: Max: 0.44681090
[2025-06-13 23:05:42,212]: 


QAT of ResNet20 with parametrized_relu down to 2 bits...
[2025-06-13 23:05:42,429]: [ResNet20_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-06-13 23:05:42,501]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-13 23:07:18,998]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 0.9425 Train Acc: 0.6757 Eval Loss: 0.8678 Eval Acc: 0.7134 (LR: 0.00100000)
[2025-06-13 23:08:53,738]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 0.7406 Train Acc: 0.7416 Eval Loss: 0.8201 Eval Acc: 0.7243 (LR: 0.00100000)
[2025-06-13 23:10:46,297]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 0.7138 Train Acc: 0.7518 Eval Loss: 1.1738 Eval Acc: 0.6376 (LR: 0.00100000)
[2025-06-13 23:12:41,392]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 0.6961 Train Acc: 0.7568 Eval Loss: 1.2526 Eval Acc: 0.6309 (LR: 0.00100000)
[2025-06-13 23:14:17,330]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 0.6858 Train Acc: 0.7614 Eval Loss: 0.9093 Eval Acc: 0.6948 (LR: 0.00100000)
[2025-06-13 23:15:53,698]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 0.6792 Train Acc: 0.7626 Eval Loss: 1.0176 Eval Acc: 0.6911 (LR: 0.00100000)
[2025-06-13 23:17:27,797]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 0.6787 Train Acc: 0.7641 Eval Loss: 0.7845 Eval Acc: 0.7409 (LR: 0.00100000)
[2025-06-13 23:19:01,014]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 0.6676 Train Acc: 0.7657 Eval Loss: 0.8697 Eval Acc: 0.7126 (LR: 0.00100000)
[2025-06-13 23:20:35,915]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 0.6750 Train Acc: 0.7652 Eval Loss: 0.8967 Eval Acc: 0.7035 (LR: 0.00100000)
[2025-06-13 23:22:11,798]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 0.6617 Train Acc: 0.7699 Eval Loss: 0.9136 Eval Acc: 0.7114 (LR: 0.00100000)
[2025-06-13 23:23:48,070]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 0.6556 Train Acc: 0.7728 Eval Loss: 0.9349 Eval Acc: 0.7019 (LR: 0.00100000)
[2025-06-13 23:25:22,690]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 0.6618 Train Acc: 0.7700 Eval Loss: 0.7140 Eval Acc: 0.7632 (LR: 0.00100000)
[2025-06-13 23:26:57,230]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 0.6468 Train Acc: 0.7742 Eval Loss: 0.8292 Eval Acc: 0.7224 (LR: 0.00100000)
[2025-06-13 23:28:29,881]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 0.6488 Train Acc: 0.7741 Eval Loss: 0.7113 Eval Acc: 0.7618 (LR: 0.00100000)
[2025-06-13 23:29:59,530]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 0.6523 Train Acc: 0.7732 Eval Loss: 0.8906 Eval Acc: 0.7208 (LR: 0.00100000)
[2025-06-13 23:31:30,399]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 0.6476 Train Acc: 0.7740 Eval Loss: 1.0128 Eval Acc: 0.6585 (LR: 0.00100000)
[2025-06-13 23:32:59,529]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 0.6418 Train Acc: 0.7775 Eval Loss: 0.8972 Eval Acc: 0.7192 (LR: 0.00100000)
[2025-06-13 23:34:27,557]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 0.6431 Train Acc: 0.7775 Eval Loss: 0.7022 Eval Acc: 0.7638 (LR: 0.00100000)
[2025-06-13 23:35:56,215]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 0.6456 Train Acc: 0.7747 Eval Loss: 1.1629 Eval Acc: 0.6534 (LR: 0.00100000)
[2025-06-13 23:37:22,216]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 0.6403 Train Acc: 0.7779 Eval Loss: 1.1007 Eval Acc: 0.6515 (LR: 0.00100000)
[2025-06-13 23:38:47,941]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 0.6343 Train Acc: 0.7781 Eval Loss: 0.7056 Eval Acc: 0.7597 (LR: 0.00100000)
[2025-06-13 23:40:12,658]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 0.6381 Train Acc: 0.7771 Eval Loss: 1.0558 Eval Acc: 0.6747 (LR: 0.00100000)
[2025-06-13 23:41:39,157]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 0.6303 Train Acc: 0.7796 Eval Loss: 1.0282 Eval Acc: 0.6697 (LR: 0.00100000)
[2025-06-13 23:43:07,465]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 0.6291 Train Acc: 0.7823 Eval Loss: 0.7343 Eval Acc: 0.7544 (LR: 0.00010000)
[2025-06-13 23:44:34,089]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 0.5403 Train Acc: 0.8135 Eval Loss: 0.5396 Eval Acc: 0.8178 (LR: 0.00010000)
[2025-06-13 23:46:01,495]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 0.5146 Train Acc: 0.8219 Eval Loss: 0.5352 Eval Acc: 0.8197 (LR: 0.00010000)
[2025-06-13 23:47:26,740]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 0.5100 Train Acc: 0.8234 Eval Loss: 0.7562 Eval Acc: 0.7519 (LR: 0.00010000)
[2025-06-13 23:48:52,395]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 0.5100 Train Acc: 0.8215 Eval Loss: 0.5904 Eval Acc: 0.8032 (LR: 0.00010000)
[2025-06-13 23:50:17,788]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 0.5084 Train Acc: 0.8230 Eval Loss: 0.5275 Eval Acc: 0.8182 (LR: 0.00010000)
[2025-06-13 23:51:46,267]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 0.5050 Train Acc: 0.8257 Eval Loss: 0.6725 Eval Acc: 0.7779 (LR: 0.00010000)
[2025-06-13 23:53:15,051]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 0.5094 Train Acc: 0.8220 Eval Loss: 0.5133 Eval Acc: 0.8236 (LR: 0.00010000)
[2025-06-13 23:54:43,728]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 0.5066 Train Acc: 0.8237 Eval Loss: 0.6448 Eval Acc: 0.7896 (LR: 0.00010000)
[2025-06-13 23:56:11,116]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 0.5038 Train Acc: 0.8238 Eval Loss: 0.6211 Eval Acc: 0.7938 (LR: 0.00010000)
[2025-06-13 23:57:36,952]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 0.5067 Train Acc: 0.8234 Eval Loss: 0.5545 Eval Acc: 0.8062 (LR: 0.00010000)
[2025-06-13 23:59:02,792]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 0.5061 Train Acc: 0.8233 Eval Loss: 0.5920 Eval Acc: 0.7963 (LR: 0.00010000)
[2025-06-14 00:00:31,653]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 0.5081 Train Acc: 0.8227 Eval Loss: 0.6287 Eval Acc: 0.7824 (LR: 0.00010000)
[2025-06-14 00:01:59,763]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 0.5110 Train Acc: 0.8219 Eval Loss: 0.6641 Eval Acc: 0.7833 (LR: 0.00001000)
[2025-06-14 00:03:27,910]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 0.4741 Train Acc: 0.8333 Eval Loss: 0.4870 Eval Acc: 0.8389 (LR: 0.00001000)
[2025-06-14 00:04:56,115]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 0.4686 Train Acc: 0.8366 Eval Loss: 0.5016 Eval Acc: 0.8293 (LR: 0.00001000)
[2025-06-14 00:06:25,420]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 0.4635 Train Acc: 0.8399 Eval Loss: 0.5240 Eval Acc: 0.8247 (LR: 0.00001000)
[2025-06-14 00:07:53,877]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 0.4681 Train Acc: 0.8380 Eval Loss: 0.6157 Eval Acc: 0.7909 (LR: 0.00001000)
[2025-06-14 00:09:24,218]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 0.4622 Train Acc: 0.8402 Eval Loss: 0.5341 Eval Acc: 0.8219 (LR: 0.00001000)
[2025-06-14 00:10:56,919]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 0.4664 Train Acc: 0.8382 Eval Loss: 0.7680 Eval Acc: 0.7477 (LR: 0.00001000)
[2025-06-14 00:12:29,490]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 0.4677 Train Acc: 0.8353 Eval Loss: 0.5431 Eval Acc: 0.8204 (LR: 0.00000100)
[2025-06-14 00:13:59,490]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 0.4576 Train Acc: 0.8402 Eval Loss: 0.4622 Eval Acc: 0.8378 (LR: 0.00000100)
[2025-06-14 00:15:29,185]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 0.4510 Train Acc: 0.8429 Eval Loss: 0.4649 Eval Acc: 0.8445 (LR: 0.00000100)
[2025-06-14 00:17:10,509]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 0.4541 Train Acc: 0.8414 Eval Loss: 0.4863 Eval Acc: 0.8315 (LR: 0.00000100)
[2025-06-14 00:18:59,302]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 0.4500 Train Acc: 0.8461 Eval Loss: 0.4767 Eval Acc: 0.8381 (LR: 0.00000100)
[2025-06-14 00:20:40,762]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 0.4576 Train Acc: 0.8398 Eval Loss: 0.4955 Eval Acc: 0.8345 (LR: 0.00000100)
[2025-06-14 00:22:10,854]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 0.4504 Train Acc: 0.8437 Eval Loss: 0.5071 Eval Acc: 0.8319 (LR: 0.00000100)
[2025-06-14 00:23:41,088]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 0.4515 Train Acc: 0.8440 Eval Loss: 0.4964 Eval Acc: 0.8316 (LR: 0.00000010)
[2025-06-14 00:25:09,836]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 0.4459 Train Acc: 0.8441 Eval Loss: 0.5003 Eval Acc: 0.8295 (LR: 0.00000010)
[2025-06-14 00:26:45,356]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 0.4444 Train Acc: 0.8462 Eval Loss: 0.4758 Eval Acc: 0.8377 (LR: 0.00000010)
[2025-06-14 00:27:47,367]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 0.4439 Train Acc: 0.8449 Eval Loss: 0.4621 Eval Acc: 0.8403 (LR: 0.00000010)
[2025-06-14 00:28:49,045]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 0.4462 Train Acc: 0.8462 Eval Loss: 0.5336 Eval Acc: 0.8185 (LR: 0.00000010)
[2025-06-14 00:29:50,414]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 0.4462 Train Acc: 0.8449 Eval Loss: 0.4861 Eval Acc: 0.8384 (LR: 0.00000010)
[2025-06-14 00:30:53,497]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 0.4470 Train Acc: 0.8457 Eval Loss: 0.4597 Eval Acc: 0.8442 (LR: 0.00000010)
[2025-06-14 00:31:52,808]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 0.4495 Train Acc: 0.8439 Eval Loss: 0.4617 Eval Acc: 0.8417 (LR: 0.00000010)
[2025-06-14 00:32:51,994]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 0.4475 Train Acc: 0.8429 Eval Loss: 0.4514 Eval Acc: 0.8429 (LR: 0.00000010)
[2025-06-14 00:33:53,627]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 0.4496 Train Acc: 0.8445 Eval Loss: 0.5068 Eval Acc: 0.8271 (LR: 0.00000010)
[2025-06-14 00:33:53,628]: [ResNet20_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.8445
[2025-06-14 00:33:53,697]: 


Quantization of model down to 2 bits finished
[2025-06-14 00:33:53,698]: Model Architecture:
[2025-06-14 00:33:53,812]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3446], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.033750534057617)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4758], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7118998765945435, max_val=0.7155381441116333)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8815], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4730], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7095524072647095, max_val=0.7094341516494751)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.7042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4233], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6343417167663574, max_val=0.6356214880943298)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8894], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4852], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7277780771255493, max_val=0.7278143763542175)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0471], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9874], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2267639636993408, max_val=1.735413908958435)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8918], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5172], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7757871150970459, max_val=0.7757028341293335)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1947], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4657], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6973041296005249, max_val=0.6998081207275391)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2427], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3810], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5061613321304321, max_val=0.6367303133010864)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5495], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8236440420150757, max_val=0.8249751329421997)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6479], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3281], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49216321110725403, max_val=0.49216562509536743)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0515], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3306], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45048391819000244, max_val=0.5412571430206299)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9786], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3570], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5354558229446411, max_val=0.5354548692703247)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3464], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5085731148719788, max_val=0.530505895614624)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2894], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3393], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43848639726638794, max_val=0.5794609785079956)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3248], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3290], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4886781573295593, max_val=0.49817532300949097)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4152], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6177126169204712, max_val=0.6277445554733276)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6919], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3431], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4995095729827881, max_val=0.5296574831008911)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1781], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2297], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.342251181602478, max_val=0.34696096181869507)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5795], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2442], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3534635603427887, max_val=0.37919121980667114)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7625], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1665], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.265922486782074, max_val=0.2335207760334015)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9274], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-06-14 00:33:53,813]: 
Model Weights:
[2025-06-14 00:33:53,813]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-06-14 00:33:53,813]: Sample Values (25 elements): [-0.05962609499692917, -0.02799830585718155, -0.12554189562797546, 0.09011352807283401, 0.29073768854141235, 0.10424338281154633, -0.060454901307821274, -0.11277570575475693, -0.3242146372795105, 0.024960385635495186, 0.0033817922230809927, 0.21204350888729095, -0.08953201770782471, -0.17830879986286163, 0.2738986611366272, -0.13468104600906372, 0.20082736015319824, -0.12273187935352325, 0.2971706986427307, -0.16809362173080444, 0.08721365034580231, -0.20453453063964844, -0.3077014982700348, 0.2355465590953827, -0.006846831180155277]
[2025-06-14 00:33:53,813]: Mean: -0.00098593
[2025-06-14 00:33:53,814]: Min: -0.61914867
[2025-06-14 00:33:53,814]: Max: 0.48998678
[2025-06-14 00:33:53,814]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,814]: Sample Values (16 elements): [2.1712687015533447, 2.3431315422058105, 2.2089686393737793, 1.818902611732483, 1.4880045652389526, 1.4824477434158325, 1.8973591327667236, 2.3876967430114746, 1.739761233329773, 1.6379896402359009, 2.643390417098999, 1.0924936532974243, 2.171111822128296, 1.4546688795089722, 1.6171996593475342, 2.3381969928741455]
[2025-06-14 00:33:53,814]: Mean: 1.90578699
[2025-06-14 00:33:53,814]: Min: 1.09249365
[2025-06-14 00:33:53,814]: Max: 2.64339042
[2025-06-14 00:33:53,815]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 00:33:53,816]: Sample Values (25 elements): [0.4758126735687256, 0.0, 0.0, -0.4758126735687256, -0.4758126735687256, 0.0, 0.0, -0.4758126735687256, 0.0, 0.0, 0.4758126735687256, 0.4758126735687256, 0.0, -0.4758126735687256, 0.0, 0.0, 0.4758126735687256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4758126735687256, 0.0]
[2025-06-14 00:33:53,816]: Mean: -0.00433683
[2025-06-14 00:33:53,816]: Min: -0.47581267
[2025-06-14 00:33:53,816]: Max: 0.95162535
[2025-06-14 00:33:53,816]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,817]: Sample Values (16 elements): [0.9719951748847961, 1.1250348091125488, 1.0273821353912354, 0.9583587646484375, 0.7845998406410217, 1.243371844291687, 0.9847832322120667, 1.196897029876709, 1.0538506507873535, 1.335575819015503, 1.194632649421692, 0.8464440703392029, 1.4128789901733398, 0.7240728735923767, 0.8730326890945435, 1.1352657079696655]
[2025-06-14 00:33:53,817]: Mean: 1.05426097
[2025-06-14 00:33:53,817]: Min: 0.72407287
[2025-06-14 00:33:53,817]: Max: 1.41287899
[2025-06-14 00:33:53,818]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 00:33:53,818]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.4729955196380615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4729955196380615, 0.0, 0.0, 0.4729955196380615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4729955196380615, 0.0, 0.4729955196380615]
[2025-06-14 00:33:53,818]: Mean: 0.00123176
[2025-06-14 00:33:53,819]: Min: -0.94599104
[2025-06-14 00:33:53,819]: Max: 0.47299552
[2025-06-14 00:33:53,819]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,819]: Sample Values (16 elements): [0.9484695196151733, 0.8035106658935547, 1.1022242307662964, 0.935011088848114, 0.6743536591529846, 1.2489897012710571, 1.456465721130371, 1.0518323183059692, 0.8540154695510864, 0.9508656859397888, 1.0199458599090576, 1.0490683317184448, 0.9503585696220398, 0.920760989189148, 0.8750010132789612, 1.0772191286087036]
[2025-06-14 00:33:53,819]: Mean: 0.99488080
[2025-06-14 00:33:53,819]: Min: 0.67435366
[2025-06-14 00:33:53,819]: Max: 1.45646572
[2025-06-14 00:33:53,820]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 00:33:53,821]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.42332109808921814, 0.0, 0.0, -0.42332109808921814, 0.42332109808921814, 0.0, 0.0, 0.0, 0.0, 0.0, -0.42332109808921814, 0.0, -0.42332109808921814, 0.0, 0.42332109808921814, 0.0, -0.42332109808921814]
[2025-06-14 00:33:53,821]: Mean: -0.00698186
[2025-06-14 00:33:53,821]: Min: -0.42332110
[2025-06-14 00:33:53,821]: Max: 0.84664220
[2025-06-14 00:33:53,821]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,821]: Sample Values (16 elements): [0.8657006621360779, 1.2264527082443237, 1.036910891532898, 1.0411120653152466, 0.9422306418418884, 1.0538277626037598, 1.1127159595489502, 0.8362531065940857, 0.8761469125747681, 1.1416943073272705, 0.897640585899353, 1.6226974725723267, 0.8826007843017578, 1.0097682476043701, 0.850482165813446, 0.9036982655525208]
[2025-06-14 00:33:53,822]: Mean: 1.01874578
[2025-06-14 00:33:53,822]: Min: 0.83625311
[2025-06-14 00:33:53,822]: Max: 1.62269747
[2025-06-14 00:33:53,823]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 00:33:53,823]: Sample Values (25 elements): [0.0, 0.0, 0.4851974844932556, 0.0, 0.0, 0.0, 0.4851974844932556, 0.0, 0.0, 0.4851974844932556, 0.4851974844932556, 0.0, 0.0, 0.0, -0.4851974844932556, 0.0, 0.4851974844932556, 0.0, 0.0, -0.4851974844932556, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,823]: Mean: 0.00484355
[2025-06-14 00:33:53,823]: Min: -0.48519748
[2025-06-14 00:33:53,824]: Max: 0.97039497
[2025-06-14 00:33:53,824]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,824]: Sample Values (16 elements): [0.6662815809249878, 1.0725178718566895, 0.9599770307540894, 1.1679203510284424, 1.229867935180664, 1.2035185098648071, 1.2727140188217163, 0.9175205826759338, 0.8087048530578613, 1.0205973386764526, 0.8402736186981201, 0.8250450491905212, 0.9979340434074402, 1.522382140159607, 0.6204049587249756, 0.7545034289360046]
[2025-06-14 00:33:53,824]: Mean: 0.99251020
[2025-06-14 00:33:53,824]: Min: 0.62040496
[2025-06-14 00:33:53,824]: Max: 1.52238214
[2025-06-14 00:33:53,825]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 00:33:53,826]: Sample Values (25 elements): [0.9873926043510437, 0.0, 0.0, 0.9873926043510437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9873926043510437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,826]: Mean: 0.00342845
[2025-06-14 00:33:53,826]: Min: -0.98739260
[2025-06-14 00:33:53,826]: Max: 1.97478521
[2025-06-14 00:33:53,826]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,826]: Sample Values (16 elements): [0.7514394521713257, 1.1985129117965698, 1.1518759727478027, 1.0139881372451782, 1.5791879892349243, 1.050174593925476, 0.5633237957954407, 1.4338136911392212, 0.6384496688842773, 0.7479621767997742, 1.03461492061615, 1.0072704553604126, 0.8940590620040894, 1.0812638998031616, 0.8218196034431458, 0.9705933928489685]
[2025-06-14 00:33:53,826]: Mean: 0.99614686
[2025-06-14 00:33:53,827]: Min: 0.56332380
[2025-06-14 00:33:53,827]: Max: 1.57918799
[2025-06-14 00:33:53,828]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-06-14 00:33:53,828]: Sample Values (25 elements): [0.0, -0.5171633362770081, -0.5171633362770081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5171633362770081, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,828]: Mean: -0.00404034
[2025-06-14 00:33:53,828]: Min: -1.03432667
[2025-06-14 00:33:53,828]: Max: 0.51716334
[2025-06-14 00:33:53,828]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-06-14 00:33:53,829]: Sample Values (16 elements): [0.6708799004554749, 0.8237602710723877, 0.7259488701820374, 1.3699315786361694, 1.5529916286468506, 1.4587823152542114, 1.0027546882629395, 0.7350176572799683, 0.734930694103241, 1.1498335599899292, 1.0945106744766235, 1.2324212789535522, 0.7750667333602905, 0.9120027422904968, 1.723930835723877, 0.6482127904891968]
[2025-06-14 00:33:53,829]: Mean: 1.03818595
[2025-06-14 00:33:53,829]: Min: 0.64821279
[2025-06-14 00:33:53,829]: Max: 1.72393084
[2025-06-14 00:33:53,830]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-06-14 00:33:53,830]: Sample Values (25 elements): [-0.465704083442688, 0.0, 0.0, 0.0, 0.465704083442688, 0.0, 0.0, -0.465704083442688, -0.465704083442688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.465704083442688, 0.0, 0.0, -0.465704083442688, 0.465704083442688]
[2025-06-14 00:33:53,831]: Mean: -0.00070745
[2025-06-14 00:33:53,831]: Min: -0.46570408
[2025-06-14 00:33:53,831]: Max: 0.93140817
[2025-06-14 00:33:53,831]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,831]: Sample Values (25 elements): [1.0986238718032837, 1.1725232601165771, 1.2354472875595093, 0.9909475445747375, 1.2138559818267822, 1.0183496475219727, 1.219274878501892, 1.1350741386413574, 1.0779473781585693, 0.9861608743667603, 1.126185655593872, 1.2326703071594238, 1.20864999294281, 1.028424859046936, 1.16238272190094, 1.030949354171753, 1.165017008781433, 1.3288626670837402, 0.9474503993988037, 1.0483477115631104, 1.0244446992874146, 1.0645500421524048, 1.0449579954147339, 1.1573739051818848, 1.186387538909912]
[2025-06-14 00:33:53,831]: Mean: 1.11710787
[2025-06-14 00:33:53,832]: Min: 0.94665879
[2025-06-14 00:33:53,832]: Max: 1.32886267
[2025-06-14 00:33:53,833]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 00:33:53,833]: Sample Values (25 elements): [0.38096389174461365, 0.0, 0.38096389174461365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38096389174461365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38096389174461365, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,833]: Mean: 0.00165349
[2025-06-14 00:33:53,833]: Min: -0.38096389
[2025-06-14 00:33:53,833]: Max: 0.76192778
[2025-06-14 00:33:53,833]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,834]: Sample Values (25 elements): [1.1607725620269775, 1.2046781778335571, 1.2465920448303223, 1.2787110805511475, 1.1862208843231201, 1.1617474555969238, 1.122880458831787, 1.2129496335983276, 1.0991414785385132, 1.246558666229248, 1.1948888301849365, 1.3937132358551025, 1.1892602443695068, 1.2061536312103271, 1.4066381454467773, 1.2822747230529785, 1.1495312452316284, 1.212411642074585, 1.2811875343322754, 1.2301925420761108, 1.2425992488861084, 1.2224280834197998, 1.2197551727294922, 1.2931421995162964, 1.2465147972106934]
[2025-06-14 00:33:53,834]: Mean: 1.21921802
[2025-06-14 00:33:53,834]: Min: 1.04511094
[2025-06-14 00:33:53,834]: Max: 1.40663815
[2025-06-14 00:33:53,835]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-06-14 00:33:53,835]: Sample Values (25 elements): [0.0, 0.0, -0.5495397448539734, 0.0, 0.0, 0.0, -0.5495397448539734, 0.0, 0.0, -0.5495397448539734, 0.0, 0.0, 0.0, 0.0, 0.5495397448539734, 0.5495397448539734, 0.0, 0.5495397448539734, 0.0, -0.5495397448539734, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,836]: Mean: 0.01502648
[2025-06-14 00:33:53,836]: Min: -0.54953974
[2025-06-14 00:33:53,836]: Max: 1.09907949
[2025-06-14 00:33:53,836]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,836]: Sample Values (25 elements): [0.6405059695243835, 0.5905077457427979, 0.5118289589881897, 0.5917863249778748, 0.6638525128364563, 0.5327864289283752, 1.0084360837936401, 0.7990353107452393, 0.5326781868934631, 0.8408100605010986, 0.6451965570449829, 0.6431508660316467, 0.6255525350570679, 0.7059803009033203, 0.5577265620231628, 0.8056410551071167, 1.1171382665634155, 0.5347944498062134, 0.7251718044281006, 0.8163145184516907, 0.5181998014450073, 0.41472095251083374, 0.6043225526809692, 0.5801687836647034, 0.6415731906890869]
[2025-06-14 00:33:53,836]: Mean: 0.65518522
[2025-06-14 00:33:53,836]: Min: 0.41472095
[2025-06-14 00:33:53,837]: Max: 1.11713827
[2025-06-14 00:33:53,838]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 00:33:53,838]: Sample Values (25 elements): [-0.32810962200164795, -0.32810962200164795, 0.32810962200164795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32810962200164795, 0.32810962200164795, 0.0, -0.32810962200164795, 0.0, -0.32810962200164795, 0.0, 0.0, 0.0, 0.0, 0.32810962200164795, 0.0, 0.0, 0.0, 0.0, -0.32810962200164795]
[2025-06-14 00:33:53,838]: Mean: -0.01174872
[2025-06-14 00:33:53,838]: Min: -0.32810962
[2025-06-14 00:33:53,838]: Max: 0.32810962
[2025-06-14 00:33:53,838]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,839]: Sample Values (25 elements): [1.0203814506530762, 0.8202900886535645, 0.8570079207420349, 0.9213791489601135, 0.7666817307472229, 1.307427167892456, 0.8650344610214233, 1.0128432512283325, 0.9837862253189087, 0.8945624232292175, 0.7524005174636841, 0.7791463136672974, 1.0692095756530762, 0.9412072896957397, 1.0323446989059448, 0.9388079643249512, 0.9571014046669006, 0.9604319930076599, 0.8163923621177673, 0.8729301691055298, 0.9166718125343323, 0.7808415293693542, 0.995651364326477, 1.0480340719223022, 0.8060507774353027]
[2025-06-14 00:33:53,839]: Mean: 0.92023718
[2025-06-14 00:33:53,839]: Min: 0.66879213
[2025-06-14 00:33:53,839]: Max: 1.30742717
[2025-06-14 00:33:53,840]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 00:33:53,840]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.33058035373687744, 0.0, 0.0, 0.33058035373687744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33058035373687744, 0.0, 0.0]
[2025-06-14 00:33:53,841]: Mean: -0.00200873
[2025-06-14 00:33:53,841]: Min: -0.33058035
[2025-06-14 00:33:53,841]: Max: 0.66116071
[2025-06-14 00:33:53,841]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,887]: Sample Values (25 elements): [0.7514705061912537, 0.9883714318275452, 0.9555159211158752, 0.966316282749176, 1.0547305345535278, 1.1841048002243042, 0.9968152046203613, 1.0386918783187866, 1.0120047330856323, 0.9243288636207581, 0.9464671015739441, 1.0486869812011719, 0.8581029176712036, 1.1032531261444092, 1.153086543083191, 0.8677641153335571, 0.934812068939209, 0.7983295917510986, 0.988447904586792, 0.8513310551643372, 1.0542421340942383, 1.1432843208312988, 1.100988745689392, 0.8963753581047058, 0.9992184042930603]
[2025-06-14 00:33:53,887]: Mean: 0.98201692
[2025-06-14 00:33:53,887]: Min: 0.75147051
[2025-06-14 00:33:53,888]: Max: 1.18410480
[2025-06-14 00:33:53,889]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 00:33:53,889]: Sample Values (25 elements): [0.0, -0.35697025060653687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35697025060653687, 0.0, 0.0, -0.35697025060653687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35697025060653687, 0.0, 0.35697025060653687]
[2025-06-14 00:33:53,889]: Mean: -0.01169759
[2025-06-14 00:33:53,889]: Min: -0.35697025
[2025-06-14 00:33:53,890]: Max: 0.35697025
[2025-06-14 00:33:53,890]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,890]: Sample Values (25 elements): [0.8786609172821045, 0.7475546598434448, 0.8304687142372131, 0.66584312915802, 0.7862040996551514, 0.9580283761024475, 0.826698362827301, 0.8278836011886597, 0.7369391322135925, 0.9004260301589966, 0.961496889591217, 0.821709930896759, 0.7503981590270996, 0.6409594416618347, 0.9276208281517029, 0.8698996901512146, 0.7356146574020386, 1.0256662368774414, 0.7724360227584839, 0.7998286485671997, 0.6986456513404846, 0.7046011090278625, 0.7228102684020996, 0.9021521806716919, 0.7458184361457825]
[2025-06-14 00:33:53,890]: Mean: 0.81142378
[2025-06-14 00:33:53,890]: Min: 0.64095944
[2025-06-14 00:33:53,890]: Max: 1.02566624
[2025-06-14 00:33:53,891]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-06-14 00:33:53,892]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3463596701622009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3463596701622009, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,892]: Mean: -0.00037582
[2025-06-14 00:33:53,892]: Min: -0.34635967
[2025-06-14 00:33:53,892]: Max: 0.69271934
[2025-06-14 00:33:53,892]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-06-14 00:33:53,893]: Sample Values (25 elements): [0.9010179042816162, 0.9712233543395996, 1.1007845401763916, 1.0417299270629883, 0.918346107006073, 0.8568717241287231, 0.8714249730110168, 1.1572740077972412, 0.9392163753509521, 0.8184890747070312, 1.0606932640075684, 1.2084153890609741, 0.9315631985664368, 0.8470730781555176, 0.9656400680541992, 0.8607494235038757, 0.9183644652366638, 0.8450537323951721, 0.8181504011154175, 1.359725832939148, 0.9955717921257019, 1.034040093421936, 0.8995235562324524, 0.8342773914337158, 0.8025646209716797]
[2025-06-14 00:33:53,893]: Mean: 0.94431847
[2025-06-14 00:33:53,893]: Min: 0.78057122
[2025-06-14 00:33:53,893]: Max: 1.35972583
[2025-06-14 00:33:53,894]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-06-14 00:33:53,895]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33931583166122437, -0.33931583166122437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.33931583166122437, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,895]: Mean: 0.00112295
[2025-06-14 00:33:53,895]: Min: -0.33931583
[2025-06-14 00:33:53,895]: Max: 0.67863166
[2025-06-14 00:33:53,895]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,896]: Sample Values (25 elements): [1.084743857383728, 1.1306602954864502, 1.11898672580719, 0.9517847895622253, 1.011966586112976, 0.969364583492279, 1.071608066558838, 0.9586367607116699, 0.8599754571914673, 0.970504105091095, 1.2877860069274902, 1.099770188331604, 1.152774453163147, 1.1226729154586792, 1.0137951374053955, 0.9838988184928894, 1.0696405172348022, 0.9241729378700256, 1.0784169435501099, 1.0358014106750488, 1.0628901720046997, 0.9672277569770813, 1.1469557285308838, 1.0236660242080688, 1.1436513662338257]
[2025-06-14 00:33:53,896]: Mean: 1.04271626
[2025-06-14 00:33:53,896]: Min: 0.83313072
[2025-06-14 00:33:53,896]: Max: 1.60637796
[2025-06-14 00:33:53,897]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 00:33:53,898]: Sample Values (25 elements): [0.0, 0.0, 0.3289511799812317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3289511799812317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3289511799812317, 0.0, 0.0]
[2025-06-14 00:33:53,898]: Mean: -0.00208807
[2025-06-14 00:33:53,898]: Min: -0.32895118
[2025-06-14 00:33:53,898]: Max: 0.65790236
[2025-06-14 00:33:53,898]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,898]: Sample Values (25 elements): [1.075910210609436, 0.9959582090377808, 1.090757966041565, 1.4267315864562988, 1.3308148384094238, 0.8608230352401733, 0.9978204965591431, 1.0024775266647339, 1.0628905296325684, 1.1759001016616821, 1.2708988189697266, 0.9815943241119385, 1.1424025297164917, 1.232129454612732, 0.9586323499679565, 1.1286163330078125, 1.0285537242889404, 1.1217365264892578, 1.075590968132019, 1.1287802457809448, 1.4108543395996094, 0.9792660474777222, 1.1410540342330933, 1.1300033330917358, 1.2462912797927856]
[2025-06-14 00:33:53,898]: Mean: 1.10041225
[2025-06-14 00:33:53,899]: Min: 0.84479892
[2025-06-14 00:33:53,899]: Max: 1.42673159
[2025-06-14 00:33:53,900]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-06-14 00:33:53,900]: Sample Values (25 elements): [-0.4151524007320404, 0.0, 0.0, 0.0, 0.0, -0.4151524007320404, 0.0, -0.4151524007320404, 0.4151524007320404, 0.0, 0.0, 0.0, 0.0, 0.4151524007320404, 0.0, -0.4151524007320404, -0.4151524007320404, 0.0, -0.4151524007320404, 0.0, 0.0, 0.0, 0.0, -0.4151524007320404, -0.4151524007320404]
[2025-06-14 00:33:53,900]: Mean: 0.00121627
[2025-06-14 00:33:53,900]: Min: -0.41515240
[2025-06-14 00:33:53,901]: Max: 0.83030480
[2025-06-14 00:33:53,901]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,901]: Sample Values (25 elements): [0.6243389248847961, 0.7215319275856018, 0.5972712635993958, 0.6614448428153992, 0.49715912342071533, 0.4395328462123871, 0.5607582330703735, 0.4338306188583374, 0.5678073167800903, 0.5426244735717773, 0.48110201954841614, 0.4100440740585327, 0.48067399859428406, 0.7653210163116455, 0.38798555731773376, 0.705533504486084, 0.5019552111625671, 0.5939143300056458, 0.5234122276306152, 0.41715484857559204, 0.5109543800354004, 0.6025716066360474, 0.44951871037483215, 0.7446393966674805, 0.6389339566230774]
[2025-06-14 00:33:53,901]: Mean: 0.56040484
[2025-06-14 00:33:53,901]: Min: 0.29661089
[2025-06-14 00:33:53,901]: Max: 0.76883709
[2025-06-14 00:33:53,902]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 00:33:53,903]: Sample Values (25 elements): [0.0, 0.0, -0.34305569529533386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,903]: Mean: -0.00023265
[2025-06-14 00:33:53,903]: Min: -0.34305570
[2025-06-14 00:33:53,903]: Max: 0.68611139
[2025-06-14 00:33:53,903]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,904]: Sample Values (25 elements): [0.8317939639091492, 0.8242624998092651, 0.9179489612579346, 0.9875050187110901, 0.9419859051704407, 1.0044325590133667, 0.7183380722999573, 0.8388378620147705, 0.785296618938446, 0.808038592338562, 0.7533280253410339, 0.8104047775268555, 0.6833192110061646, 0.7823546528816223, 0.7906531095504761, 0.8667336106300354, 0.7607100605964661, 0.6760947108268738, 0.8254651427268982, 0.6641173958778381, 0.7368836998939514, 0.9519600868225098, 1.0288517475128174, 0.7774597406387329, 0.7856734395027161]
[2025-06-14 00:33:53,904]: Mean: 0.82957566
[2025-06-14 00:33:53,904]: Min: 0.66411740
[2025-06-14 00:33:53,904]: Max: 1.06205666
[2025-06-14 00:33:53,905]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 00:33:53,906]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.22973738610744476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22973738610744476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.22973738610744476]
[2025-06-14 00:33:53,906]: Mean: -0.00198178
[2025-06-14 00:33:53,906]: Min: -0.22973739
[2025-06-14 00:33:53,906]: Max: 0.45947477
[2025-06-14 00:33:53,906]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,906]: Sample Values (25 elements): [0.8764500617980957, 0.971696138381958, 0.9084025621414185, 0.8657371401786804, 0.9894664287567139, 0.7684465646743774, 0.8133023977279663, 1.0312764644622803, 0.6805344820022583, 0.9375460743904114, 0.8344978094100952, 0.9323838353157043, 0.8992522954940796, 1.1050435304641724, 1.0038659572601318, 1.0776090621948242, 1.04451584815979, 0.9659756422042847, 1.0072643756866455, 1.1023874282836914, 0.9309731125831604, 0.9470503330230713, 1.0337255001068115, 0.9107309579849243, 0.8475791215896606]
[2025-06-14 00:33:53,906]: Mean: 0.93870592
[2025-06-14 00:33:53,907]: Min: 0.68053448
[2025-06-14 00:33:53,907]: Max: 1.21343291
[2025-06-14 00:33:53,908]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 00:33:53,908]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.24421826004981995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24421826004981995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24421826004981995, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,908]: Mean: -0.00015900
[2025-06-14 00:33:53,909]: Min: -0.24421826
[2025-06-14 00:33:53,909]: Max: 0.48843652
[2025-06-14 00:33:53,909]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,909]: Sample Values (25 elements): [0.6639869809150696, 0.62187659740448, 0.6051788926124573, 5.954397434609013e-41, 0.5928763747215271, 0.6498690843582153, 0.5858272314071655, 0.7780672311782837, 0.5941247344017029, 0.6392367482185364, 0.42077121138572693, 0.5700607299804688, 0.6968377828598022, -6.018997293814387e-41, 0.72893887758255, 0.7180715799331665, 0.6363106369972229, 0.8567771911621094, 0.2742059528827667, 0.056599490344524384, 0.8130127787590027, 5.819732652187398e-41, 0.2652333378791809, 0.5530838370323181, -4.911270857765619e-41]
[2025-06-14 00:33:53,909]: Mean: 0.47969946
[2025-06-14 00:33:53,909]: Min: -0.00000000
[2025-06-14 00:33:53,910]: Max: 0.85677719
[2025-06-14 00:33:53,911]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-06-14 00:33:53,911]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.16648109257221222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16648109257221222, -0.16648109257221222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-06-14 00:33:53,911]: Mean: 0.00134579
[2025-06-14 00:33:53,911]: Min: -0.33296219
[2025-06-14 00:33:53,912]: Max: 0.16648109
[2025-06-14 00:33:53,912]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-06-14 00:33:53,912]: Sample Values (25 elements): [0.8923265337944031, 0.9417048096656799, 0.8313424587249756, 1.031665563583374, 1.074331283569336, 0.9483235478401184, 1.0126675367355347, 0.9276875257492065, 1.0270837545394897, 0.8514244556427002, 1.0006318092346191, 0.709748387336731, 0.8486502766609192, 0.9784618616104126, 0.829450249671936, 0.9881842732429504, 0.9204968214035034, 0.7722578644752502, 1.0186657905578613, 0.903217613697052, 0.7821894884109497, 0.9741829633712769, 1.049906611442566, 0.8940951228141785, 0.7925465106964111]
[2025-06-14 00:33:53,912]: Mean: 0.90796638
[2025-06-14 00:33:53,912]: Min: 0.67858499
[2025-06-14 00:33:53,912]: Max: 1.12840295
[2025-06-14 00:33:53,912]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-06-14 00:33:53,913]: Sample Values (25 elements): [-0.2750956416130066, 0.03433475270867348, 0.04769165441393852, -0.16750560700893402, -0.13144855201244354, -0.013925614766776562, -0.5264209508895874, -0.1419820487499237, 0.29980701208114624, -0.13692431151866913, -0.20981097221374512, -0.1641531139612198, 0.1885492503643036, -0.3423506021499634, 0.09083776921033859, 0.08933574706315994, -0.008080129511654377, -0.3291996121406555, -0.007165529299527407, -0.2050880342721939, -0.07577536255121231, -0.09863434731960297, -0.23626407980918884, 0.32239705324172974, -0.01862500235438347]
[2025-06-14 00:33:53,913]: Mean: -0.05862130
[2025-06-14 00:33:53,913]: Min: -0.73214042
[2025-06-14 00:33:53,913]: Max: 0.43578935
