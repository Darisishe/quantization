[2025-05-21 21:24:12,209]: Checkpoint of model at path [checkpoint/ResNet20_relu6.ckpt] will be used for QAT
[2025-05-21 21:24:12,209]: 


QAT of ResNet20 with parametrized_relu down to 2 bits...
[2025-05-21 21:24:12,356]: [ResNet20_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-05-21 21:24:12,399]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-21 21:25:11,496]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 1.3989 Train Acc: 0.4981 Eval Loss: 1.2114 Eval Acc: 0.5814 (LR: 0.010000)
[2025-05-21 21:26:09,881]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 1.0843 Train Acc: 0.6140 Eval Loss: 1.1737 Eval Acc: 0.6014 (LR: 0.010000)
[2025-05-21 21:27:08,598]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 0.9892 Train Acc: 0.6513 Eval Loss: 1.0853 Eval Acc: 0.6322 (LR: 0.010000)
[2025-05-21 21:28:07,143]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 0.9383 Train Acc: 0.6699 Eval Loss: 1.0014 Eval Acc: 0.6622 (LR: 0.010000)
[2025-05-21 21:29:05,987]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 0.8924 Train Acc: 0.6867 Eval Loss: 0.9180 Eval Acc: 0.6817 (LR: 0.010000)
[2025-05-21 21:30:04,589]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 0.8733 Train Acc: 0.6926 Eval Loss: 1.0769 Eval Acc: 0.6474 (LR: 0.010000)
[2025-05-21 21:31:03,370]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 0.8522 Train Acc: 0.7026 Eval Loss: 0.9360 Eval Acc: 0.6819 (LR: 0.010000)
[2025-05-21 21:32:01,963]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 0.8381 Train Acc: 0.7067 Eval Loss: 1.1390 Eval Acc: 0.6273 (LR: 0.010000)
[2025-05-21 21:33:00,158]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 0.8110 Train Acc: 0.7155 Eval Loss: 0.8583 Eval Acc: 0.7130 (LR: 0.010000)
[2025-05-21 21:33:56,720]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 0.8110 Train Acc: 0.7143 Eval Loss: 0.8922 Eval Acc: 0.6920 (LR: 0.010000)
[2025-05-21 21:34:53,093]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 0.7919 Train Acc: 0.7227 Eval Loss: 1.0097 Eval Acc: 0.6558 (LR: 0.010000)
[2025-05-21 21:35:49,126]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 0.7759 Train Acc: 0.7290 Eval Loss: 0.9859 Eval Acc: 0.6708 (LR: 0.010000)
[2025-05-21 21:36:45,429]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 0.7608 Train Acc: 0.7339 Eval Loss: 0.8002 Eval Acc: 0.7274 (LR: 0.010000)
[2025-05-21 21:37:41,453]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 0.7560 Train Acc: 0.7363 Eval Loss: 1.0583 Eval Acc: 0.6687 (LR: 0.010000)
[2025-05-21 21:38:37,467]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 0.7525 Train Acc: 0.7368 Eval Loss: 0.8890 Eval Acc: 0.6895 (LR: 0.001000)
[2025-05-21 21:39:33,778]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 0.6709 Train Acc: 0.7648 Eval Loss: 0.6520 Eval Acc: 0.7795 (LR: 0.001000)
[2025-05-21 21:40:30,171]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 0.6497 Train Acc: 0.7721 Eval Loss: 0.7010 Eval Acc: 0.7588 (LR: 0.001000)
[2025-05-21 21:41:26,544]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 0.6465 Train Acc: 0.7746 Eval Loss: 0.6494 Eval Acc: 0.7779 (LR: 0.001000)
[2025-05-21 21:42:22,597]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 0.6412 Train Acc: 0.7763 Eval Loss: 0.6363 Eval Acc: 0.7777 (LR: 0.001000)
[2025-05-21 21:43:19,143]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 0.6414 Train Acc: 0.7764 Eval Loss: 0.6840 Eval Acc: 0.7626 (LR: 0.001000)
[2025-05-21 21:44:15,245]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 0.6413 Train Acc: 0.7767 Eval Loss: 0.7108 Eval Acc: 0.7598 (LR: 0.001000)
[2025-05-21 21:45:11,426]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 0.6389 Train Acc: 0.7778 Eval Loss: 0.7890 Eval Acc: 0.7332 (LR: 0.001000)
[2025-05-21 21:46:07,677]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 0.6313 Train Acc: 0.7774 Eval Loss: 0.6791 Eval Acc: 0.7705 (LR: 0.001000)
[2025-05-21 21:47:03,779]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 0.6414 Train Acc: 0.7776 Eval Loss: 0.6513 Eval Acc: 0.7783 (LR: 0.001000)
[2025-05-21 21:47:59,943]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 0.6383 Train Acc: 0.7772 Eval Loss: 0.6795 Eval Acc: 0.7664 (LR: 0.001000)
[2025-05-21 21:48:56,237]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 0.6362 Train Acc: 0.7802 Eval Loss: 0.6840 Eval Acc: 0.7755 (LR: 0.001000)
[2025-05-21 21:49:52,175]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 0.6308 Train Acc: 0.7799 Eval Loss: 0.7429 Eval Acc: 0.7472 (LR: 0.001000)
[2025-05-21 21:50:48,382]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 0.6347 Train Acc: 0.7792 Eval Loss: 0.8994 Eval Acc: 0.7128 (LR: 0.001000)
[2025-05-21 21:51:44,383]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 0.6375 Train Acc: 0.7792 Eval Loss: 0.6523 Eval Acc: 0.7768 (LR: 0.001000)
[2025-05-21 21:52:40,560]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 0.6317 Train Acc: 0.7782 Eval Loss: 0.7067 Eval Acc: 0.7673 (LR: 0.000100)
[2025-05-21 21:53:36,996]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 0.6087 Train Acc: 0.7875 Eval Loss: 0.6010 Eval Acc: 0.7958 (LR: 0.000100)
[2025-05-21 21:54:34,198]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 0.6010 Train Acc: 0.7915 Eval Loss: 0.6193 Eval Acc: 0.7910 (LR: 0.000100)
[2025-05-21 21:55:30,844]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 0.5972 Train Acc: 0.7916 Eval Loss: 0.6085 Eval Acc: 0.7893 (LR: 0.000100)
[2025-05-21 21:56:27,483]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 0.5987 Train Acc: 0.7915 Eval Loss: 0.6418 Eval Acc: 0.7855 (LR: 0.000100)
[2025-05-21 21:57:24,294]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 0.6024 Train Acc: 0.7898 Eval Loss: 0.6286 Eval Acc: 0.7856 (LR: 0.000100)
[2025-05-21 21:58:21,149]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 0.6057 Train Acc: 0.7875 Eval Loss: 0.6201 Eval Acc: 0.7878 (LR: 0.000100)
[2025-05-21 21:59:17,810]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 0.6020 Train Acc: 0.7889 Eval Loss: 0.6588 Eval Acc: 0.7779 (LR: 0.000100)
[2025-05-21 22:00:14,681]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 0.6009 Train Acc: 0.7917 Eval Loss: 0.6345 Eval Acc: 0.7833 (LR: 0.000100)
[2025-05-21 22:01:11,252]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 0.6014 Train Acc: 0.7902 Eval Loss: 0.6600 Eval Acc: 0.7764 (LR: 0.000100)
[2025-05-21 22:02:08,340]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 0.6027 Train Acc: 0.7888 Eval Loss: 0.6662 Eval Acc: 0.7733 (LR: 0.000100)
[2025-05-21 22:03:04,889]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 0.5994 Train Acc: 0.7912 Eval Loss: 0.6027 Eval Acc: 0.7941 (LR: 0.000100)
[2025-05-21 22:04:01,795]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 0.6068 Train Acc: 0.7888 Eval Loss: 0.6846 Eval Acc: 0.7655 (LR: 0.000100)
[2025-05-21 22:04:58,459]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 0.6028 Train Acc: 0.7913 Eval Loss: 0.6875 Eval Acc: 0.7745 (LR: 0.000100)
[2025-05-21 22:05:54,898]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 0.6064 Train Acc: 0.7890 Eval Loss: 0.6160 Eval Acc: 0.7924 (LR: 0.000100)
[2025-05-21 22:06:51,629]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 0.6043 Train Acc: 0.7896 Eval Loss: 0.5933 Eval Acc: 0.7975 (LR: 0.000010)
[2025-05-21 22:07:48,448]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 0.5906 Train Acc: 0.7923 Eval Loss: 0.5897 Eval Acc: 0.7997 (LR: 0.000010)
[2025-05-21 22:08:45,069]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 0.5906 Train Acc: 0.7938 Eval Loss: 0.5775 Eval Acc: 0.8040 (LR: 0.000010)
[2025-05-21 22:09:41,657]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 0.5871 Train Acc: 0.7956 Eval Loss: 0.5836 Eval Acc: 0.8018 (LR: 0.000010)
[2025-05-21 22:10:38,198]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 0.5868 Train Acc: 0.7953 Eval Loss: 0.6103 Eval Acc: 0.7923 (LR: 0.000010)
[2025-05-21 22:11:34,822]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 0.5914 Train Acc: 0.7936 Eval Loss: 0.6004 Eval Acc: 0.7970 (LR: 0.000010)
[2025-05-21 22:12:31,628]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 0.5930 Train Acc: 0.7941 Eval Loss: 0.6116 Eval Acc: 0.7934 (LR: 0.000010)
[2025-05-21 22:13:28,285]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 0.5880 Train Acc: 0.7960 Eval Loss: 0.6061 Eval Acc: 0.7912 (LR: 0.000010)
[2025-05-21 22:14:24,872]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 0.5887 Train Acc: 0.7952 Eval Loss: 0.6330 Eval Acc: 0.7869 (LR: 0.000010)
[2025-05-21 22:15:21,427]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 0.5941 Train Acc: 0.7914 Eval Loss: 0.5920 Eval Acc: 0.7950 (LR: 0.000010)
[2025-05-21 22:16:17,809]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 0.5947 Train Acc: 0.7934 Eval Loss: 0.6285 Eval Acc: 0.7844 (LR: 0.000010)
[2025-05-21 22:17:14,694]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 0.5931 Train Acc: 0.7914 Eval Loss: 0.6078 Eval Acc: 0.7967 (LR: 0.000010)
[2025-05-21 22:18:11,551]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 0.5922 Train Acc: 0.7915 Eval Loss: 0.6134 Eval Acc: 0.7911 (LR: 0.000010)
[2025-05-21 22:19:07,813]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 0.5899 Train Acc: 0.7947 Eval Loss: 0.6113 Eval Acc: 0.7936 (LR: 0.000010)
[2025-05-21 22:20:04,133]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 0.5931 Train Acc: 0.7948 Eval Loss: 0.5966 Eval Acc: 0.7952 (LR: 0.000010)
[2025-05-21 22:21:00,785]: [ResNet20_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 0.5971 Train Acc: 0.7903 Eval Loss: 0.6238 Eval Acc: 0.7908 (LR: 0.000010)
[2025-05-21 22:21:00,786]: [ResNet20_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.8040
[2025-05-21 22:21:00,844]: 


Quantization of model down to 2 bits finished
[2025-05-21 22:21:00,845]: Model Architecture:
[2025-05-21 22:21:00,903]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3004], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40880125761032104, max_val=0.4922564923763275)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8713], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2193], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3284749388694763, max_val=0.3292820453643799)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8872], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4529], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7677104473114014, max_val=0.5909804105758667)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8706], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2137], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32062506675720215, max_val=0.3206217885017395)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9991], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2193], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.322994202375412, max_val=0.33492255210876465)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8741], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1930], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27531829476356506, max_val=0.3037473261356354)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1212], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1711], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2500304877758026, max_val=0.26314663887023926)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8744], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1519], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21737763285636902, max_val=0.23838624358177185)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3054], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45813462138175964, max_val=0.45813024044036865)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9084], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1923], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26718199253082275, max_val=0.3097101151943207)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8762], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1328], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19499172270298004, max_val=0.20355072617530823)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9831], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1396], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2025228589773178, max_val=0.2163056582212448)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8753], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1179], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16103067994117737, max_val=0.1925847828388214)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1699], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1275], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1827440857887268, max_val=0.19964243471622467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8764], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1186], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16774806380271912, max_val=0.1882018744945526)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1973], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2905130982398987, max_val=0.3013545274734497)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9583], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1151], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16308581829071045, max_val=0.18225717544555664)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8807], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1010], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13572116196155548, max_val=0.16727882623672485)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9738], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0855], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1157975047826767, max_val=0.1406821310520172)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8764], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0640], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0956571102142334, max_val=0.09625165164470673)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0116], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-21 22:21:00,903]: 
Model Weights:
[2025-05-21 22:21:00,904]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-21 22:21:00,904]: Sample Values (25 elements): [0.06509476900100708, 0.09911550581455231, 0.08706247061491013, 0.025220837444067, 0.17664657533168793, -0.18583224713802338, -0.08477350324392319, -0.23222306370735168, -0.2254883348941803, 0.1334609091281891, 0.3282099664211273, 0.16550195217132568, -0.14512021839618683, -0.10992815345525742, 0.2990555763244629, -0.33647558093070984, -0.23700247704982758, 0.16741633415222168, -0.6773437857627869, -0.023721231147646904, -0.14440779387950897, 0.04398597776889801, -0.16565079987049103, 0.059459008276462555, -0.34736284613609314]
[2025-05-21 22:21:00,904]: Mean: -0.00171005
[2025-05-21 22:21:00,904]: Min: -0.67734379
[2025-05-21 22:21:00,904]: Max: 0.91401309
[2025-05-21 22:21:00,904]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,905]: Sample Values (16 elements): [1.8568291664123535, 1.9312074184417725, 2.1266884803771973, 2.1201374530792236, 1.3960119485855103, 1.3067512512207031, 1.6152044534683228, 1.6455739736557007, 1.5454075336456299, 1.1801775693893433, 1.1878952980041504, 1.6344634294509888, 1.9506475925445557, 1.8364934921264648, 1.3478997945785522, 1.657043218612671]
[2025-05-21 22:21:00,905]: Mean: 1.64615202
[2025-05-21 22:21:00,905]: Min: 1.18017757
[2025-05-21 22:21:00,905]: Max: 2.12668848
[2025-05-21 22:21:00,906]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 22:21:00,906]: Sample Values (25 elements): [-0.3003526031970978, 0.0, 0.0, -0.3003526031970978, 0.0, 0.3003526031970978, -0.3003526031970978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3003526031970978, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,907]: Mean: -0.00482337
[2025-05-21 22:21:00,907]: Min: -0.30035260
[2025-05-21 22:21:00,907]: Max: 0.60070521
[2025-05-21 22:21:00,907]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,907]: Sample Values (16 elements): [1.1609019041061401, 1.9231328964233398, 1.0590027570724487, 1.0627599954605103, 1.0070499181747437, 1.3328336477279663, 1.0609372854232788, 1.1665643453598022, 1.1719776391983032, 1.3592796325683594, 1.1232906579971313, 1.1448942422866821, 0.903168261051178, 1.2986985445022583, 1.168841004371643, 1.1578147411346436]
[2025-05-21 22:21:00,907]: Mean: 1.19382167
[2025-05-21 22:21:00,907]: Min: 0.90316826
[2025-05-21 22:21:00,908]: Max: 1.92313290
[2025-05-21 22:21:00,909]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 22:21:00,909]: Sample Values (25 elements): [-0.2192523181438446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2192523181438446, 0.0, 0.0, 0.0, -0.2192523181438446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,909]: Mean: -0.00399679
[2025-05-21 22:21:00,909]: Min: -0.21925232
[2025-05-21 22:21:00,910]: Max: 0.43850464
[2025-05-21 22:21:00,910]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,910]: Sample Values (16 elements): [1.067630648612976, 1.0504897832870483, 1.0495132207870483, 1.3563604354858398, 0.7686346769332886, 1.131308913230896, 1.1762174367904663, 0.8766817450523376, 1.1546763181686401, 1.2733250856399536, 0.9172032475471497, 1.1968094110488892, 1.363292932510376, 0.9883363246917725, 1.0017725229263306, 1.1372238397598267]
[2025-05-21 22:21:00,910]: Mean: 1.09434223
[2025-05-21 22:21:00,910]: Min: 0.76863468
[2025-05-21 22:21:00,910]: Max: 1.36329293
[2025-05-21 22:21:00,911]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 22:21:00,912]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,912]: Mean: 0.00491425
[2025-05-21 22:21:00,912]: Min: -0.90579391
[2025-05-21 22:21:00,912]: Max: 0.45289695
[2025-05-21 22:21:00,912]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,912]: Sample Values (16 elements): [1.4750645160675049, 1.5796889066696167, 0.9255180954933167, 0.9777600169181824, 1.0097360610961914, 0.9403005838394165, 1.2774192094802856, 1.0665867328643799, 0.987635612487793, 1.4881551265716553, 1.0439221858978271, 0.969817042350769, 0.9860672354698181, 0.9508689045906067, 0.9441432952880859, 1.035219669342041]
[2025-05-21 22:21:00,913]: Mean: 1.10361898
[2025-05-21 22:21:00,913]: Min: 0.92551810
[2025-05-21 22:21:00,913]: Max: 1.57968891
[2025-05-21 22:21:00,914]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 22:21:00,914]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.213748961687088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,914]: Mean: 0.00046387
[2025-05-21 22:21:00,914]: Min: -0.42749792
[2025-05-21 22:21:00,915]: Max: 0.21374896
[2025-05-21 22:21:00,915]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,915]: Sample Values (16 elements): [0.8403709530830383, 0.9028534889221191, 0.8878969550132751, 0.8499002456665039, 0.9395564794540405, 0.8965879082679749, 1.3096836805343628, 0.7959061861038208, 1.0004786252975464, 0.9124200344085693, 0.9002696871757507, 1.240878701210022, 0.8098470568656921, 1.0315386056900024, 0.9306138753890991, 1.1703327894210815]
[2025-05-21 22:21:00,915]: Mean: 0.96369594
[2025-05-21 22:21:00,915]: Min: 0.79590619
[2025-05-21 22:21:00,915]: Max: 1.30968368
[2025-05-21 22:21:00,917]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 22:21:00,917]: Sample Values (25 elements): [0.0, 0.0, 0.2193056046962738, -0.2193056046962738, -0.2193056046962738, -0.2193056046962738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2193056046962738, 0.2193056046962738, 0.0, 0.0, 0.0, 0.0, -0.2193056046962738, 0.0, -0.2193056046962738, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,918]: Mean: -0.00409294
[2025-05-21 22:21:00,918]: Min: -0.21930560
[2025-05-21 22:21:00,918]: Max: 0.43861121
[2025-05-21 22:21:00,918]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,918]: Sample Values (16 elements): [1.0083166360855103, 1.097762942314148, 0.909437358379364, 1.293588638305664, 1.1262214183807373, 0.9883445501327515, 1.236945629119873, 0.8972949981689453, 1.3025680780410767, 1.131226897239685, 1.0049664974212646, 1.3005861043930054, 1.2075858116149902, 1.1092485189437866, 1.2834038734436035, 0.9839123487472534]
[2025-05-21 22:21:00,918]: Mean: 1.11758804
[2025-05-21 22:21:00,919]: Min: 0.89729500
[2025-05-21 22:21:00,919]: Max: 1.30256808
[2025-05-21 22:21:00,920]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 22:21:00,920]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.19302187860012054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19302187860012054, 0.0, 0.0, -0.19302187860012054, 0.0, 0.19302187860012054, -0.19302187860012054, -0.19302187860012054, 0.19302187860012054, 0.0, 0.0, -0.19302187860012054, 0.0, 0.0]
[2025-05-21 22:21:00,920]: Mean: 0.00619949
[2025-05-21 22:21:00,920]: Min: -0.19302188
[2025-05-21 22:21:00,920]: Max: 0.38604376
[2025-05-21 22:21:00,920]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-21 22:21:00,921]: Sample Values (16 elements): [1.1154688596725464, 0.9831627011299133, 1.0277940034866333, 1.0711995363235474, 0.8625280857086182, 0.9387376308441162, 1.0694079399108887, 0.9849081635475159, 1.2320961952209473, 1.04621422290802, 1.077012300491333, 0.982734203338623, 1.200828194618225, 0.9973939657211304, 1.0526787042617798, 1.501150369644165]
[2025-05-21 22:21:00,921]: Mean: 1.07145715
[2025-05-21 22:21:00,921]: Min: 0.86252809
[2025-05-21 22:21:00,921]: Max: 1.50115037
[2025-05-21 22:21:00,922]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-21 22:21:00,923]: Sample Values (25 elements): [0.0, -0.17105905711650848, 0.0, 0.0, 0.0, 0.0, 0.17105905711650848, 0.0, 0.0, 0.0, 0.0, 0.17105905711650848, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.17105905711650848, 0.17105905711650848, 0.17105905711650848, 0.0]
[2025-05-21 22:21:00,923]: Mean: -0.00267280
[2025-05-21 22:21:00,923]: Min: -0.17105906
[2025-05-21 22:21:00,923]: Max: 0.34211811
[2025-05-21 22:21:00,923]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,923]: Sample Values (25 elements): [1.1458250284194946, 1.1983381509780884, 1.10689115524292, 1.168542742729187, 1.0725600719451904, 1.2902988195419312, 1.233569622039795, 1.1334693431854248, 1.3670843839645386, 1.1863632202148438, 1.1563971042633057, 1.2266380786895752, 1.1756782531738281, 1.0479736328125, 1.1434296369552612, 1.4015189409255981, 1.29641592502594, 1.0985740423202515, 1.1735368967056274, 1.2951196432113647, 1.0575485229492188, 1.086687684059143, 1.0596628189086914, 1.230208158493042, 1.1438019275665283]
[2025-05-21 22:21:00,924]: Mean: 1.17537665
[2025-05-21 22:21:00,924]: Min: 1.04797363
[2025-05-21 22:21:00,924]: Max: 1.40151894
[2025-05-21 22:21:00,925]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 22:21:00,925]: Sample Values (25 elements): [0.0, 0.0, -0.15192128717899323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.15192128717899323, 0.0, 0.0, 0.0, 0.0, -0.15192128717899323, 0.0, 0.0, 0.0, 0.0, -0.15192128717899323, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,925]: Mean: -0.00514317
[2025-05-21 22:21:00,925]: Min: -0.15192129
[2025-05-21 22:21:00,926]: Max: 0.30384257
[2025-05-21 22:21:00,926]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,926]: Sample Values (25 elements): [1.1857526302337646, 1.20540189743042, 1.1537784337997437, 1.1265575885772705, 1.2215166091918945, 1.171371340751648, 1.1000125408172607, 1.1949515342712402, 1.1868728399276733, 1.1926347017288208, 1.1522209644317627, 1.0992276668548584, 1.1945228576660156, 1.1751558780670166, 1.1561251878738403, 1.227099895477295, 1.1050338745117188, 1.1169785261154175, 1.1911206245422363, 1.085999846458435, 1.2291077375411987, 1.1350512504577637, 1.1224679946899414, 1.1314572095870972, 1.1438992023468018]
[2025-05-21 22:21:00,926]: Mean: 1.16092825
[2025-05-21 22:21:00,926]: Min: 1.06811094
[2025-05-21 22:21:00,926]: Max: 1.34864068
[2025-05-21 22:21:00,927]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-21 22:21:00,928]: Sample Values (25 elements): [0.0, -0.3054216504096985, -0.3054216504096985, 0.0, -0.3054216504096985, 0.3054216504096985, 0.0, 0.3054216504096985, 0.0, -0.3054216504096985, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3054216504096985, -0.3054216504096985, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3054216504096985, 0.0, 0.3054216504096985]
[2025-05-21 22:21:00,928]: Mean: -0.01372011
[2025-05-21 22:21:00,928]: Min: -0.61084330
[2025-05-21 22:21:00,928]: Max: 0.30542165
[2025-05-21 22:21:00,928]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,928]: Sample Values (25 elements): [0.8116973638534546, 0.8027461171150208, 0.9794000387191772, 0.8269739151000977, 0.8919683694839478, 0.8726404905319214, 0.8020188808441162, 0.8564730882644653, 0.9217194318771362, 0.9572213888168335, 0.9169749021530151, 0.8736693263053894, 0.7542238235473633, 0.8545660972595215, 0.8375421762466431, 0.7573020458221436, 0.8257131576538086, 0.8050463199615479, 0.8456361293792725, 0.9755814671516418, 0.9273301362991333, 0.9305792450904846, 0.828231692314148, 0.8097522258758545, 0.8627026081085205]
[2025-05-21 22:21:00,928]: Mean: 0.85493457
[2025-05-21 22:21:00,929]: Min: 0.74423909
[2025-05-21 22:21:00,929]: Max: 1.01402938
[2025-05-21 22:21:00,930]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 22:21:00,930]: Sample Values (25 elements): [0.0, -0.19229738414287567, -0.19229738414287567, 0.0, -0.19229738414287567, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19229738414287567, 0.0, 0.0, 0.0, 0.0, 0.0, -0.19229738414287567, 0.0, 0.0, 0.0, 0.0, -0.19229738414287567]
[2025-05-21 22:21:00,930]: Mean: -0.00016692
[2025-05-21 22:21:00,931]: Min: -0.19229738
[2025-05-21 22:21:00,931]: Max: 0.38459477
[2025-05-21 22:21:00,931]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,931]: Sample Values (25 elements): [1.0256439447402954, 1.0592976808547974, 1.1597039699554443, 1.0086913108825684, 1.1865673065185547, 1.1494895219802856, 1.101393699645996, 0.9947660565376282, 1.129947304725647, 1.0392234325408936, 1.027284026145935, 1.2433805465698242, 1.2148605585098267, 0.954790472984314, 1.1004747152328491, 1.096183180809021, 1.1299344301223755, 1.09200918674469, 1.1164225339889526, 0.9382844567298889, 1.302048921585083, 1.0583138465881348, 1.0455365180969238, 1.0893385410308838, 1.128441333770752]
[2025-05-21 22:21:00,931]: Mean: 1.08753026
[2025-05-21 22:21:00,931]: Min: 0.90947396
[2025-05-21 22:21:00,932]: Max: 1.30204892
[2025-05-21 22:21:00,932]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 22:21:00,933]: Sample Values (25 elements): [0.0, 0.0, -0.13284748792648315, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13284748792648315, 0.0, 0.0, 0.13284748792648315, 0.0, 0.0, -0.13284748792648315, 0.0, 0.0, 0.13284748792648315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,933]: Mean: -0.00138383
[2025-05-21 22:21:00,933]: Min: -0.13284749
[2025-05-21 22:21:00,933]: Max: 0.26569498
[2025-05-21 22:21:00,933]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,934]: Sample Values (25 elements): [1.1119199991226196, 1.054754376411438, 1.0069843530654907, 1.032378077507019, 1.090040683746338, 1.1539427042007446, 0.9707324504852295, 1.0590248107910156, 0.9825485348701477, 0.912533164024353, 1.0074080228805542, 0.9528802037239075, 1.0742522478103638, 1.0216410160064697, 0.8468642830848694, 0.9963263273239136, 0.8682126998901367, 0.9427217841148376, 1.0481946468353271, 1.1633821725845337, 1.062257170677185, 0.9731502532958984, 0.9743949174880981, 1.0690006017684937, 1.0340677499771118]
[2025-05-21 22:21:00,934]: Mean: 1.01966131
[2025-05-21 22:21:00,934]: Min: 0.84686428
[2025-05-21 22:21:00,934]: Max: 1.16338217
[2025-05-21 22:21:00,935]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 22:21:00,936]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13960951566696167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13960951566696167, -0.13960951566696167, 0.0, 0.0, 0.0, 0.13960951566696167, -0.13960951566696167, 0.0]
[2025-05-21 22:21:00,936]: Mean: -0.00145427
[2025-05-21 22:21:00,936]: Min: -0.13960952
[2025-05-21 22:21:00,936]: Max: 0.27921903
[2025-05-21 22:21:00,936]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,937]: Sample Values (25 elements): [1.0567563772201538, 0.9713321924209595, 0.9925008416175842, 1.0258315801620483, 1.0415304899215698, 1.0362162590026855, 1.1175174713134766, 1.0321580171585083, 0.9851342439651489, 1.0263735055923462, 1.1167577505111694, 1.047708511352539, 1.0198739767074585, 0.926459550857544, 1.0013614892959595, 1.08931303024292, 1.044188380241394, 1.044636845588684, 1.0478650331497192, 0.8947513699531555, 1.0365599393844604, 0.9310939908027649, 1.1035022735595703, 1.0435686111450195, 1.0256316661834717]
[2025-05-21 22:21:00,937]: Mean: 1.03499222
[2025-05-21 22:21:00,937]: Min: 0.89475137
[2025-05-21 22:21:00,937]: Max: 1.18430066
[2025-05-21 22:21:00,938]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 22:21:00,939]: Sample Values (25 elements): [-0.11787183582782745, -0.11787183582782745, -0.11787183582782745, -0.11787183582782745, 0.0, 0.0, 0.0, 0.0, 0.11787183582782745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11787183582782745, 0.0, 0.0, 0.0, 0.0, 0.11787183582782745, 0.0, 0.0, 0.11787183582782745, -0.11787183582782745, 0.0]
[2025-05-21 22:21:00,939]: Mean: -0.00193128
[2025-05-21 22:21:00,939]: Min: -0.11787184
[2025-05-21 22:21:00,939]: Max: 0.23574367
[2025-05-21 22:21:00,939]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-21 22:21:00,939]: Sample Values (25 elements): [1.0030966997146606, 1.037850260734558, 0.9980000257492065, 1.0862840414047241, 1.068740963935852, 1.0147316455841064, 0.9576225280761719, 0.9163577556610107, 1.0358175039291382, 1.0267266035079956, 1.0060516595840454, 0.9480398297309875, 0.9305051565170288, 1.1117322444915771, 0.9790171384811401, 1.0435353517532349, 0.9750370383262634, 1.0080044269561768, 0.9972032904624939, 1.034308671951294, 1.0290361642837524, 0.965490996837616, 1.0022438764572144, 1.1118425130844116, 0.9949765205383301]
[2025-05-21 22:21:00,940]: Mean: 1.01280260
[2025-05-21 22:21:00,940]: Min: 0.91635776
[2025-05-21 22:21:00,940]: Max: 1.11184251
[2025-05-21 22:21:00,941]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-21 22:21:00,941]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12746217846870422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12746217846870422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12746217846870422]
[2025-05-21 22:21:00,941]: Mean: 0.00246183
[2025-05-21 22:21:00,942]: Min: -0.12746218
[2025-05-21 22:21:00,942]: Max: 0.25492436
[2025-05-21 22:21:00,942]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,942]: Sample Values (25 elements): [1.0322612524032593, 1.0112899541854858, 1.0260770320892334, 0.9848030209541321, 1.0493522882461548, 0.9255828261375427, 1.203479528427124, 1.081075668334961, 1.0244956016540527, 1.0633060932159424, 1.0354491472244263, 1.0181448459625244, 1.0142285823822021, 1.115914225578308, 1.0441834926605225, 1.0317999124526978, 1.0018069744110107, 1.033365249633789, 1.1370949745178223, 1.0673195123672485, 1.023060917854309, 1.1075060367584229, 1.1142842769622803, 1.1422722339630127, 1.0650527477264404]
[2025-05-21 22:21:00,942]: Mean: 1.06978989
[2025-05-21 22:21:00,942]: Min: 0.92558283
[2025-05-21 22:21:00,943]: Max: 1.20347953
[2025-05-21 22:21:00,944]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 22:21:00,944]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.11864998191595078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11864998191595078, -0.11864998191595078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,944]: Mean: -0.00054072
[2025-05-21 22:21:00,945]: Min: -0.11864998
[2025-05-21 22:21:00,945]: Max: 0.23729996
[2025-05-21 22:21:00,945]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,945]: Sample Values (25 elements): [1.1074979305267334, 1.0975008010864258, 0.9731088280677795, 0.9448971152305603, 1.019654393196106, 1.1518102884292603, 1.0078742504119873, 0.9570509195327759, 1.052155613899231, 1.0849976539611816, 1.072688341140747, 1.0765806436538696, 1.000272512435913, 1.0475128889083862, 1.0470714569091797, 1.0747374296188354, 1.070355772972107, 1.0559462308883667, 1.0677464008331299, 1.113404631614685, 1.0175272226333618, 1.003575086593628, 1.0636297464370728, 1.165834903717041, 1.0718859434127808]
[2025-05-21 22:21:00,945]: Mean: 1.05258214
[2025-05-21 22:21:00,945]: Min: 0.93452805
[2025-05-21 22:21:00,946]: Max: 1.16583490
[2025-05-21 22:21:00,947]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-21 22:21:00,947]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.19728922843933105, 0.0, 0.19728922843933105, 0.0, 0.19728922843933105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19728922843933105, 0.0, -0.19728922843933105, 0.0, 0.19728922843933105, 0.0, 0.19728922843933105, 0.0, 0.0, -0.19728922843933105, 0.0, 0.19728922843933105]
[2025-05-21 22:21:00,947]: Mean: 0.00423864
[2025-05-21 22:21:00,947]: Min: -0.19728923
[2025-05-21 22:21:00,947]: Max: 0.39457846
[2025-05-21 22:21:00,948]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,948]: Sample Values (25 elements): [0.8308504819869995, 0.8466746211051941, 0.7377325892448425, 0.8523333668708801, 0.8292360901832581, 0.8536018133163452, 0.8473434448242188, 0.7934396862983704, 0.8921830058097839, 0.808623194694519, 0.7370012998580933, 0.7453083992004395, 0.8817989826202393, 0.875118613243103, 0.9362408518791199, 0.8824531435966492, 0.8624370694160461, 0.8825414776802063, 0.7944482564926147, 0.8224764466285706, 0.8847951889038086, 0.8817527294158936, 0.8716496825218201, 0.8765575885772705, 0.788946807384491]
[2025-05-21 22:21:00,948]: Mean: 0.84879285
[2025-05-21 22:21:00,948]: Min: 0.73700130
[2025-05-21 22:21:00,948]: Max: 0.94879490
[2025-05-21 22:21:00,949]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 22:21:00,950]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.11511432379484177, 0.0, 0.11511432379484177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511432379484177]
[2025-05-21 22:21:00,950]: Mean: -0.00029665
[2025-05-21 22:21:00,950]: Min: -0.11511432
[2025-05-21 22:21:00,951]: Max: 0.23022865
[2025-05-21 22:21:00,951]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,951]: Sample Values (25 elements): [0.9923707842826843, 1.068350911140442, 0.9972988367080688, 1.074817419052124, 0.9197915196418762, 0.9376397728919983, 0.9007191061973572, 0.9893758893013, 1.0635616779327393, 1.0029326677322388, 1.0340876579284668, 0.9236737489700317, 1.1579128503799438, 1.0636730194091797, 0.9956781268119812, 1.0373414754867554, 0.9899548292160034, 0.9555774927139282, 1.1205466985702515, 0.9975974559783936, 1.0493985414505005, 1.0298686027526855, 1.0185105800628662, 1.0491811037063599, 1.0162407159805298]
[2025-05-21 22:21:00,951]: Mean: 1.01736140
[2025-05-21 22:21:00,951]: Min: 0.89743316
[2025-05-21 22:21:00,952]: Max: 1.15791285
[2025-05-21 22:21:00,953]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 22:21:00,954]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10100001096725464, -0.10100001096725464, 0.0, 0.0, 0.0, 0.0, 0.10100001096725464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10100001096725464, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,954]: Mean: 0.00086852
[2025-05-21 22:21:00,954]: Min: -0.10100001
[2025-05-21 22:21:00,955]: Max: 0.20200002
[2025-05-21 22:21:00,955]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,955]: Sample Values (25 elements): [0.9363180994987488, 0.9808266758918762, 0.9693735241889954, 0.9873226881027222, 0.9110274910926819, 0.9500629305839539, 0.9969906806945801, 0.9710114002227783, 0.965920090675354, 1.0430653095245361, 1.0238701105117798, 1.0371105670928955, 0.9525905251502991, 0.938113272190094, 0.9794198870658875, 1.019426703453064, 0.9270864129066467, 0.9371364712715149, 0.9405332207679749, 1.056046962738037, 0.9712255597114563, 0.9049992561340332, 1.0779597759246826, 0.9386103749275208, 0.9580071568489075]
[2025-05-21 22:21:00,955]: Mean: 0.98040485
[2025-05-21 22:21:00,955]: Min: 0.90499926
[2025-05-21 22:21:00,955]: Max: 1.10152268
[2025-05-21 22:21:00,956]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 22:21:00,957]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.08549320697784424, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08549320697784424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,957]: Mean: -0.00073749
[2025-05-21 22:21:00,958]: Min: -0.08549321
[2025-05-21 22:21:00,958]: Max: 0.17098641
[2025-05-21 22:21:00,958]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,958]: Sample Values (25 elements): [0.9559858441352844, 0.9484611749649048, 0.9395291209220886, 1.012528896331787, 0.9544553160667419, 0.9291846752166748, 0.8935182690620422, 0.9423385262489319, 0.9585975408554077, 1.0116440057754517, 0.9623813033103943, 0.8915890455245972, 0.890850841999054, 0.9224944114685059, 0.9435985684394836, 0.9662806391716003, 0.9870229959487915, 0.9411347508430481, 0.9319964051246643, 0.975287675857544, 0.9713977575302124, 0.9519032835960388, 0.8896597027778625, 0.9529368281364441, 0.9384716749191284]
[2025-05-21 22:21:00,958]: Mean: 0.95325649
[2025-05-21 22:21:00,958]: Min: 0.88636607
[2025-05-21 22:21:00,959]: Max: 1.03553402
[2025-05-21 22:21:00,960]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 22:21:00,960]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.06396959722042084, 0.0, 0.0, 0.0, -0.06396959722042084, 0.06396959722042084, 0.06396959722042084, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06396959722042084, 0.0, 0.0, -0.06396959722042084, 0.0, 0.06396959722042084, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 22:21:00,960]: Mean: -0.00053273
[2025-05-21 22:21:00,961]: Min: -0.06396960
[2025-05-21 22:21:00,961]: Max: 0.12793919
[2025-05-21 22:21:00,961]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-21 22:21:00,961]: Sample Values (25 elements): [0.949621319770813, 0.978586733341217, 0.9866039156913757, 1.009485125541687, 0.9513913989067078, 0.9771565198898315, 1.0064265727996826, 1.045934796333313, 0.9386364817619324, 0.965627133846283, 0.9740236401557922, 0.9978798031806946, 0.9642624258995056, 1.018880844116211, 0.9946168661117554, 0.9703798890113831, 0.9716188907623291, 0.9934591054916382, 1.0145848989486694, 0.9635524153709412, 0.9667454361915588, 0.9772565364837646, 0.9874070882797241, 0.9908948540687561, 0.9962476491928101]
[2025-05-21 22:21:00,961]: Mean: 0.98869628
[2025-05-21 22:21:00,961]: Min: 0.93863648
[2025-05-21 22:21:00,962]: Max: 1.06259274
[2025-05-21 22:21:00,962]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-21 22:21:00,962]: Sample Values (25 elements): [-0.007125310134142637, -0.2031187117099762, 0.31201404333114624, -0.19057099521160126, 0.1662323772907257, 0.1104823499917984, -0.21250472962856293, -0.24704265594482422, -0.19853129982948303, -0.26423752307891846, -0.0546482652425766, -0.15998457372188568, -0.16147764027118683, 0.07458674162626266, 0.2184554934501648, -0.19033105671405792, 0.04470063000917435, -0.06497912108898163, 0.1868576556444168, 0.314617782831192, 0.2103932797908783, 0.0038860265631228685, 0.3219600021839142, -0.12302238494157791, 0.5476964712142944]
[2025-05-21 22:21:00,962]: Mean: -0.00341572
[2025-05-21 22:21:00,962]: Min: -0.44633365
[2025-05-21 22:21:00,963]: Max: 0.58816940
[2025-05-21 22:21:00,963]: 


QAT of ResNet20 with parametrized_relu down to 3 bits...
[2025-05-21 22:21:01,069]: [ResNet20_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-05-21 22:21:01,099]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-21 22:21:57,620]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.8582 Train Acc: 0.6995 Eval Loss: 1.1087 Eval Acc: 0.6621 (LR: 0.010000)
[2025-05-21 22:22:54,020]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.7141 Train Acc: 0.7507 Eval Loss: 1.1091 Eval Acc: 0.6817 (LR: 0.010000)
[2025-05-21 22:23:50,153]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.6661 Train Acc: 0.7671 Eval Loss: 0.7861 Eval Acc: 0.7416 (LR: 0.010000)
[2025-05-21 22:24:46,430]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.6344 Train Acc: 0.7805 Eval Loss: 0.7119 Eval Acc: 0.7642 (LR: 0.010000)
[2025-05-21 22:25:42,626]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.6093 Train Acc: 0.7891 Eval Loss: 0.6596 Eval Acc: 0.7722 (LR: 0.010000)
[2025-05-21 22:26:38,738]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.5925 Train Acc: 0.7955 Eval Loss: 0.6755 Eval Acc: 0.7746 (LR: 0.010000)
[2025-05-21 22:27:34,937]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.5681 Train Acc: 0.8036 Eval Loss: 0.7293 Eval Acc: 0.7631 (LR: 0.010000)
[2025-05-21 22:28:31,049]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.5542 Train Acc: 0.8073 Eval Loss: 0.6239 Eval Acc: 0.7946 (LR: 0.010000)
[2025-05-21 22:29:27,298]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.5426 Train Acc: 0.8105 Eval Loss: 0.6147 Eval Acc: 0.7943 (LR: 0.010000)
[2025-05-21 22:30:23,382]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.5343 Train Acc: 0.8135 Eval Loss: 0.5481 Eval Acc: 0.8141 (LR: 0.010000)
[2025-05-21 22:31:19,555]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.5191 Train Acc: 0.8187 Eval Loss: 0.6975 Eval Acc: 0.7802 (LR: 0.010000)
[2025-05-21 22:32:15,771]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.5121 Train Acc: 0.8221 Eval Loss: 0.6087 Eval Acc: 0.7985 (LR: 0.010000)
[2025-05-21 22:33:12,188]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.5010 Train Acc: 0.8246 Eval Loss: 0.5812 Eval Acc: 0.8026 (LR: 0.010000)
[2025-05-21 22:34:08,531]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.4968 Train Acc: 0.8267 Eval Loss: 0.6351 Eval Acc: 0.7948 (LR: 0.010000)
[2025-05-21 22:35:04,833]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.4838 Train Acc: 0.8334 Eval Loss: 0.5052 Eval Acc: 0.8315 (LR: 0.001000)
[2025-05-21 22:36:00,925]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.4148 Train Acc: 0.8551 Eval Loss: 0.4307 Eval Acc: 0.8523 (LR: 0.001000)
[2025-05-21 22:36:57,351]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.3909 Train Acc: 0.8651 Eval Loss: 0.4174 Eval Acc: 0.8571 (LR: 0.001000)
[2025-05-21 22:37:53,487]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.3847 Train Acc: 0.8678 Eval Loss: 0.4168 Eval Acc: 0.8565 (LR: 0.001000)
[2025-05-21 22:38:49,776]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.3849 Train Acc: 0.8665 Eval Loss: 0.4125 Eval Acc: 0.8613 (LR: 0.001000)
[2025-05-21 22:39:46,611]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.3785 Train Acc: 0.8693 Eval Loss: 0.4016 Eval Acc: 0.8630 (LR: 0.001000)
[2025-05-21 22:40:42,865]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.3764 Train Acc: 0.8689 Eval Loss: 0.4344 Eval Acc: 0.8572 (LR: 0.001000)
[2025-05-21 22:41:38,867]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.3752 Train Acc: 0.8699 Eval Loss: 0.4363 Eval Acc: 0.8515 (LR: 0.001000)
[2025-05-21 22:42:35,034]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.3763 Train Acc: 0.8696 Eval Loss: 0.4249 Eval Acc: 0.8582 (LR: 0.001000)
[2025-05-21 22:43:31,401]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.3752 Train Acc: 0.8699 Eval Loss: 0.4115 Eval Acc: 0.8586 (LR: 0.001000)
[2025-05-21 22:44:27,789]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.3760 Train Acc: 0.8690 Eval Loss: 0.4143 Eval Acc: 0.8583 (LR: 0.001000)
[2025-05-21 22:45:24,178]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.3740 Train Acc: 0.8702 Eval Loss: 0.4155 Eval Acc: 0.8581 (LR: 0.001000)
[2025-05-21 22:46:20,408]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.3733 Train Acc: 0.8705 Eval Loss: 0.4101 Eval Acc: 0.8627 (LR: 0.001000)
[2025-05-21 22:47:16,777]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.3690 Train Acc: 0.8715 Eval Loss: 0.4196 Eval Acc: 0.8602 (LR: 0.001000)
[2025-05-21 22:48:12,886]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.3680 Train Acc: 0.8725 Eval Loss: 0.4150 Eval Acc: 0.8655 (LR: 0.001000)
[2025-05-21 22:49:09,168]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.3651 Train Acc: 0.8722 Eval Loss: 0.4255 Eval Acc: 0.8575 (LR: 0.000100)
[2025-05-21 22:50:05,128]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.3537 Train Acc: 0.8771 Eval Loss: 0.3964 Eval Acc: 0.8652 (LR: 0.000100)
[2025-05-21 22:51:03,013]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.3485 Train Acc: 0.8783 Eval Loss: 0.3950 Eval Acc: 0.8662 (LR: 0.000100)
[2025-05-21 22:52:01,733]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.3419 Train Acc: 0.8803 Eval Loss: 0.4090 Eval Acc: 0.8663 (LR: 0.000100)
[2025-05-21 22:53:00,615]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.3419 Train Acc: 0.8811 Eval Loss: 0.3966 Eval Acc: 0.8660 (LR: 0.000100)
[2025-05-21 22:53:59,341]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.3436 Train Acc: 0.8804 Eval Loss: 0.3856 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-21 22:54:57,472]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.3499 Train Acc: 0.8767 Eval Loss: 0.3918 Eval Acc: 0.8679 (LR: 0.000100)
[2025-05-21 22:55:56,018]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.3470 Train Acc: 0.8806 Eval Loss: 0.3868 Eval Acc: 0.8710 (LR: 0.000100)
[2025-05-21 22:56:54,737]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.3460 Train Acc: 0.8801 Eval Loss: 0.3999 Eval Acc: 0.8679 (LR: 0.000100)
[2025-05-21 22:57:53,353]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.3419 Train Acc: 0.8814 Eval Loss: 0.4023 Eval Acc: 0.8642 (LR: 0.000100)
[2025-05-21 22:58:52,646]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.3435 Train Acc: 0.8814 Eval Loss: 0.3878 Eval Acc: 0.8681 (LR: 0.000100)
[2025-05-21 22:59:51,535]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.3440 Train Acc: 0.8818 Eval Loss: 0.4047 Eval Acc: 0.8634 (LR: 0.000100)
[2025-05-21 23:00:50,387]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.3428 Train Acc: 0.8806 Eval Loss: 0.3905 Eval Acc: 0.8689 (LR: 0.000100)
[2025-05-21 23:01:48,759]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.3419 Train Acc: 0.8810 Eval Loss: 0.3986 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-21 23:02:46,914]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.3474 Train Acc: 0.8794 Eval Loss: 0.4069 Eval Acc: 0.8644 (LR: 0.000100)
[2025-05-21 23:03:45,182]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.3417 Train Acc: 0.8811 Eval Loss: 0.3889 Eval Acc: 0.8711 (LR: 0.000010)
[2025-05-21 23:04:44,134]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.3392 Train Acc: 0.8828 Eval Loss: 0.3904 Eval Acc: 0.8672 (LR: 0.000010)
[2025-05-21 23:05:42,578]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.3367 Train Acc: 0.8824 Eval Loss: 0.3870 Eval Acc: 0.8691 (LR: 0.000010)
[2025-05-21 23:06:41,465]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.3367 Train Acc: 0.8827 Eval Loss: 0.3861 Eval Acc: 0.8708 (LR: 0.000010)
[2025-05-21 23:07:40,047]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.3381 Train Acc: 0.8825 Eval Loss: 0.3888 Eval Acc: 0.8697 (LR: 0.000010)
[2025-05-21 23:08:38,856]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.3369 Train Acc: 0.8834 Eval Loss: 0.3885 Eval Acc: 0.8696 (LR: 0.000010)
[2025-05-21 23:09:37,472]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.3374 Train Acc: 0.8816 Eval Loss: 0.3903 Eval Acc: 0.8701 (LR: 0.000010)
[2025-05-21 23:10:35,995]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.3374 Train Acc: 0.8829 Eval Loss: 0.3911 Eval Acc: 0.8682 (LR: 0.000010)
[2025-05-21 23:11:34,690]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.3367 Train Acc: 0.8832 Eval Loss: 0.3990 Eval Acc: 0.8668 (LR: 0.000010)
[2025-05-21 23:12:33,441]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.3387 Train Acc: 0.8829 Eval Loss: 0.3854 Eval Acc: 0.8691 (LR: 0.000010)
[2025-05-21 23:13:32,687]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.3381 Train Acc: 0.8824 Eval Loss: 0.3817 Eval Acc: 0.8742 (LR: 0.000010)
[2025-05-21 23:14:32,210]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.3354 Train Acc: 0.8847 Eval Loss: 0.3875 Eval Acc: 0.8702 (LR: 0.000010)
[2025-05-21 23:15:30,825]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.3371 Train Acc: 0.8838 Eval Loss: 0.3822 Eval Acc: 0.8715 (LR: 0.000010)
[2025-05-21 23:16:29,268]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.3342 Train Acc: 0.8841 Eval Loss: 0.3896 Eval Acc: 0.8727 (LR: 0.000010)
[2025-05-21 23:17:28,174]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.3373 Train Acc: 0.8819 Eval Loss: 0.3898 Eval Acc: 0.8704 (LR: 0.000010)
[2025-05-21 23:18:27,054]: [ResNet20_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.3363 Train Acc: 0.8835 Eval Loss: 0.3919 Eval Acc: 0.8687 (LR: 0.000010)
[2025-05-21 23:18:27,054]: [ResNet20_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.8742
[2025-05-21 23:18:27,124]: 


Quantization of model down to 3 bits finished
[2025-05-21 23:18:27,125]: Model Architecture:
[2025-05-21 23:18:27,208]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1357], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4735552668571472, max_val=0.47657835483551025)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8022], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1098], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3720899820327759, max_val=0.3964521288871765)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7799], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1886], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8101602792739868, max_val=0.509989321231842)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7996], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1218], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47175225615501404, max_val=0.3808123469352722)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8177], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0865], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.304722398519516, max_val=0.3006001114845276)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0860], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23348119854927063, max_val=0.36879202723503113)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8642], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0742], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2650383412837982, max_val=0.25410184264183044)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0700], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22269222140312195, max_val=0.2672630846500397)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1225], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45928141474723816, max_val=0.39794495701789856)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8120], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0809], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2601805627346039, max_val=0.30618375539779663)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8036], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0621], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2059634029865265, max_val=0.22853849828243256)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8236], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0616], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21572345495224, max_val=0.2156626433134079)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0578], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.185777947306633, max_val=0.21903203427791595)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8743], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0553], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19349916279315948, max_val=0.19349798560142517)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1768651306629181, max_val=0.20877403020858765)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0764], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25735342502593994, max_val=0.27718856930732727)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8169], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0537], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1824186146259308, max_val=0.19322283565998077)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0389], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12721894681453705, max_val=0.14527486264705658)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8132], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0358], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12049828469753265, max_val=0.1301465630531311)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8043], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0252], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08479878306388855, max_val=0.0917234718799591)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8492], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-21 23:18:27,208]: 
Model Weights:
[2025-05-21 23:18:27,208]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-21 23:18:27,208]: Sample Values (25 elements): [0.1392696499824524, -0.06882154941558838, 0.4003177881240845, 0.21193040907382965, -0.13673999905586243, 0.15208488702774048, -0.051851872354745865, 0.1623401641845703, -0.19871702790260315, -0.27280667424201965, -0.3105238676071167, 0.11510960012674332, -0.46575161814689636, -0.04364956542849541, 0.35756465792655945, 0.22819654643535614, -0.08347360044717789, -0.1351199597120285, -0.08606018871068954, -0.07273698598146439, 0.12349174171686172, 0.21701815724372864, 0.048364900052547455, 0.2612461447715759, 0.2448096126317978]
[2025-05-21 23:18:27,208]: Mean: -0.00296834
[2025-05-21 23:18:27,209]: Min: -0.73241013
[2025-05-21 23:18:27,209]: Max: 0.98673463
[2025-05-21 23:18:27,209]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,209]: Sample Values (16 elements): [1.0913203954696655, 1.7383321523666382, 1.252339482307434, 1.4180500507354736, 1.6769601106643677, 0.8960990905761719, 1.4699796438217163, 1.4969044923782349, 1.088695764541626, 1.5594483613967896, 1.4653785228729248, 1.1967111825942993, 1.2369000911712646, 1.5267115831375122, 1.4091968536376953, 1.7272279262542725]
[2025-05-21 23:18:27,209]: Mean: 1.39064097
[2025-05-21 23:18:27,209]: Min: 0.89609909
[2025-05-21 23:18:27,209]: Max: 1.73833215
[2025-05-21 23:18:27,211]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 23:18:27,211]: Sample Values (25 elements): [-0.13573339581489563, -0.13573339581489563, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13573339581489563, -0.13573339581489563, 0.0, -0.13573339581489563, 0.0, -0.13573339581489563, 0.0, -0.13573339581489563, 0.0, -0.13573339581489563, -0.13573339581489563, -0.13573339581489563, -0.13573339581489563, 0.0, -0.27146679162979126, 0.27146679162979126, 0.13573339581489563, -0.13573339581489563]
[2025-05-21 23:18:27,212]: Mean: -0.01354977
[2025-05-21 23:18:27,212]: Min: -0.40720019
[2025-05-21 23:18:27,212]: Max: 0.54293358
[2025-05-21 23:18:27,213]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,213]: Sample Values (16 elements): [0.8781538605690002, 1.1290475130081177, 0.8239971399307251, 1.00575852394104, 1.079759955406189, 1.2417160272598267, 1.110687494277954, 1.1035937070846558, 1.7186424732208252, 1.3226008415222168, 0.9942402839660645, 1.128649353981018, 1.0607839822769165, 1.2314847707748413, 1.162064790725708, 1.1262298822402954]
[2025-05-21 23:18:27,213]: Mean: 1.13233817
[2025-05-21 23:18:27,213]: Min: 0.82399714
[2025-05-21 23:18:27,213]: Max: 1.71864247
[2025-05-21 23:18:27,214]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 23:18:27,215]: Sample Values (25 elements): [0.10979174077510834, 0.10979174077510834, 0.0, 0.10979174077510834, 0.0, 0.0, -0.10979174077510834, -0.10979174077510834, 0.0, 0.10979174077510834, -0.10979174077510834, 0.0, -0.10979174077510834, -0.10979174077510834, 0.0, 0.0, 0.0, 0.10979174077510834, 0.10979174077510834, 0.0, 0.0, 0.0, 0.0, -0.10979174077510834, 0.21958348155021667]
[2025-05-21 23:18:27,215]: Mean: -0.00567067
[2025-05-21 23:18:27,215]: Min: -0.32937521
[2025-05-21 23:18:27,215]: Max: 0.43916696
[2025-05-21 23:18:27,215]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,216]: Sample Values (16 elements): [0.9395033717155457, 1.014225959777832, 1.065290093421936, 1.0531628131866455, 1.0316241979599, 1.0115290880203247, 1.3156895637512207, 1.144344687461853, 1.0822749137878418, 0.871924877166748, 1.3229650259017944, 1.1789380311965942, 1.1015102863311768, 1.1493195295333862, 0.9829904437065125, 1.029067039489746]
[2025-05-21 23:18:27,216]: Mean: 1.08089757
[2025-05-21 23:18:27,216]: Min: 0.87192488
[2025-05-21 23:18:27,216]: Max: 1.32296503
[2025-05-21 23:18:27,217]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 23:18:27,218]: Sample Values (25 elements): [-0.1885928213596344, -0.1885928213596344, 0.0, 0.0, 0.0, 0.0, -0.1885928213596344, 0.1885928213596344, 0.0, 0.0, 0.0, -0.1885928213596344, 0.0, -0.1885928213596344, 0.0, -0.1885928213596344, 0.0, 0.0, 0.0, 0.1885928213596344, 0.0, 0.0, -0.1885928213596344, 0.0, 0.0]
[2025-05-21 23:18:27,218]: Mean: -0.00196451
[2025-05-21 23:18:27,218]: Min: -0.75437129
[2025-05-21 23:18:27,218]: Max: 0.56577849
[2025-05-21 23:18:27,219]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,219]: Sample Values (16 elements): [1.0522030591964722, 0.8247736692428589, 1.1972261667251587, 1.0190736055374146, 0.8626964688301086, 1.64663827419281, 1.328957200050354, 1.5311708450317383, 0.9414608478546143, 1.2495447397232056, 1.050907015800476, 0.9001497030258179, 1.3202909231185913, 1.0679255723953247, 0.9772731065750122, 0.9520140290260315]
[2025-05-21 23:18:27,220]: Mean: 1.12014413
[2025-05-21 23:18:27,220]: Min: 0.82477367
[2025-05-21 23:18:27,220]: Max: 1.64663827
[2025-05-21 23:18:27,221]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 23:18:27,221]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.12179495394229889, 0.0, 0.12179495394229889, 0.0, 0.0, -0.24358990788459778, -0.12179495394229889, 0.0, 0.0, 0.0, 0.0, 0.12179495394229889, 0.12179495394229889, 0.0, -0.12179495394229889, -0.12179495394229889, 0.0, 0.0, 0.12179495394229889, 0.0, 0.0]
[2025-05-21 23:18:27,222]: Mean: -0.00179732
[2025-05-21 23:18:27,222]: Min: -0.48717982
[2025-05-21 23:18:27,222]: Max: 0.36538488
[2025-05-21 23:18:27,222]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,222]: Sample Values (16 elements): [1.1050740480422974, 1.5314823389053345, 0.8618893623352051, 0.8950467109680176, 0.9571219086647034, 1.1098800897598267, 1.050906777381897, 0.980302631855011, 0.8909868597984314, 0.8213488459587097, 0.9753465056419373, 1.0440192222595215, 0.9460214972496033, 1.3766180276870728, 1.102803111076355, 0.9984100461006165]
[2025-05-21 23:18:27,222]: Mean: 1.04045367
[2025-05-21 23:18:27,223]: Min: 0.82134885
[2025-05-21 23:18:27,223]: Max: 1.53148234
[2025-05-21 23:18:27,224]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 23:18:27,224]: Sample Values (25 elements): [0.0, -0.17294928431510925, -0.17294928431510925, 0.08647464215755463, 0.0, -0.08647464215755463, 0.0, 0.0, 0.0, -0.08647464215755463, 0.0, -0.08647464215755463, -0.08647464215755463, 0.08647464215755463, -0.08647464215755463, 0.0, 0.08647464215755463, 0.0, -0.08647464215755463, 0.17294928431510925, 0.17294928431510925, 0.0, -0.08647464215755463, 0.0, 0.0]
[2025-05-21 23:18:27,225]: Mean: -0.00615531
[2025-05-21 23:18:27,225]: Min: -0.34589857
[2025-05-21 23:18:27,225]: Max: 0.25942391
[2025-05-21 23:18:27,225]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,225]: Sample Values (16 elements): [0.9352468848228455, 0.9352491497993469, 1.1913427114486694, 1.146519660949707, 1.0142030715942383, 1.0903146266937256, 1.052871584892273, 1.0001217126846313, 0.8800375461578369, 0.9609102010726929, 1.088897705078125, 1.0392365455627441, 0.9642778038978577, 1.1361083984375, 1.1724810600280762, 0.8541610240936279]
[2025-05-21 23:18:27,225]: Mean: 1.02887368
[2025-05-21 23:18:27,226]: Min: 0.85416102
[2025-05-21 23:18:27,226]: Max: 1.19134271
[2025-05-21 23:18:27,227]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-21 23:18:27,227]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.08603903651237488, 0.0, 0.0, 0.0, 0.0, 0.08603903651237488, 0.08603903651237488, 0.0, -0.08603903651237488, 0.08603903651237488, 0.08603903651237488, -0.08603903651237488, -0.08603903651237488, 0.0, -0.08603903651237488, 0.08603903651237488, 0.0, 0.0, 0.08603903651237488, 0.08603903651237488, -0.08603903651237488, 0.0]
[2025-05-21 23:18:27,228]: Mean: 0.00728195
[2025-05-21 23:18:27,228]: Min: -0.25811711
[2025-05-21 23:18:27,228]: Max: 0.34415615
[2025-05-21 23:18:27,228]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-21 23:18:27,228]: Sample Values (16 elements): [0.9043929576873779, 0.9328432679176331, 1.0676518678665161, 0.8560739755630493, 1.2558248043060303, 0.9788058996200562, 1.0862303972244263, 0.8302014470100403, 0.93126380443573, 0.8957464694976807, 0.8608736991882324, 0.7399519085884094, 0.8797987699508667, 1.0186941623687744, 0.9742779731750488, 0.9696627855300903]
[2025-05-21 23:18:27,228]: Mean: 0.94889343
[2025-05-21 23:18:27,229]: Min: 0.73995191
[2025-05-21 23:18:27,229]: Max: 1.25582480
[2025-05-21 23:18:27,230]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-21 23:18:27,230]: Sample Values (25 elements): [-0.07416289299726486, 0.0, 0.07416289299726486, -0.07416289299726486, 0.14832578599452972, 0.0, 0.0, 0.07416289299726486, 0.222488671541214, 0.07416289299726486, -0.07416289299726486, -0.07416289299726486, -0.07416289299726486, 0.0, -0.14832578599452972, 0.07416289299726486, -0.14832578599452972, 0.14832578599452972, 0.0, 0.0, -0.07416289299726486, 0.0, 0.222488671541214, 0.0, 0.0]
[2025-05-21 23:18:27,231]: Mean: -0.00455471
[2025-05-21 23:18:27,231]: Min: -0.29665157
[2025-05-21 23:18:27,231]: Max: 0.22248867
[2025-05-21 23:18:27,232]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,232]: Sample Values (25 elements): [1.0809805393218994, 1.0634554624557495, 1.0445202589035034, 1.094921350479126, 1.0019932985305786, 1.0523563623428345, 1.165457010269165, 0.986876904964447, 0.9826831817626953, 1.1074349880218506, 0.9777213931083679, 0.9702522158622742, 1.131831169128418, 1.086663007736206, 1.078070044517517, 1.0622544288635254, 1.1745471954345703, 1.0847100019454956, 1.0937483310699463, 1.1378986835479736, 1.0447996854782104, 0.9675623774528503, 1.050751805305481, 1.0715285539627075, 1.1558003425598145]
[2025-05-21 23:18:27,232]: Mean: 1.05929756
[2025-05-21 23:18:27,233]: Min: 0.96756238
[2025-05-21 23:18:27,233]: Max: 1.17454720
[2025-05-21 23:18:27,235]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 23:18:27,235]: Sample Values (25 elements): [-0.06999361515045166, -0.06999361515045166, -0.13998723030090332, -0.13998723030090332, -0.06999361515045166, -0.06999361515045166, 0.13998723030090332, 0.0, 0.0, -0.06999361515045166, -0.06999361515045166, -0.06999361515045166, 0.06999361515045166, 0.0, 0.0, 0.0, -0.06999361515045166, 0.0, 0.06999361515045166, 0.06999361515045166, -0.06999361515045166, -0.06999361515045166, -0.06999361515045166, 0.0, -0.06999361515045166]
[2025-05-21 23:18:27,235]: Mean: -0.00587837
[2025-05-21 23:18:27,236]: Min: -0.20998085
[2025-05-21 23:18:27,236]: Max: 0.27997446
[2025-05-21 23:18:27,236]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,236]: Sample Values (25 elements): [0.870552122592926, 1.0319557189941406, 1.0760456323623657, 1.0082957744598389, 0.9870250225067139, 1.0867329835891724, 1.0443620681762695, 1.2055039405822754, 1.0972100496292114, 1.1274389028549194, 0.9890319108963013, 0.9637662172317505, 1.1071009635925293, 1.0451748371124268, 0.9955479502677917, 0.9634576439857483, 1.0590629577636719, 0.9485234022140503, 0.9975346326828003, 1.1150410175323486, 1.1182268857955933, 1.0629805326461792, 1.034916639328003, 1.03695547580719, 1.0979622602462769]
[2025-05-21 23:18:27,236]: Mean: 1.04583502
[2025-05-21 23:18:27,237]: Min: 0.87055212
[2025-05-21 23:18:27,237]: Max: 1.20550394
[2025-05-21 23:18:27,238]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-21 23:18:27,239]: Sample Values (25 elements): [0.12246091663837433, 0.24492183327674866, 0.0, 0.12246091663837433, -0.12246091663837433, 0.12246091663837433, 0.12246091663837433, 0.12246091663837433, 0.24492183327674866, 0.24492183327674866, 0.0, -0.24492183327674866, 0.12246091663837433, -0.12246091663837433, -0.12246091663837433, 0.0, 0.3673827648162842, -0.12246091663837433, -0.12246091663837433, -0.24492183327674866, -0.24492183327674866, -0.12246091663837433, -0.12246091663837433, -0.24492183327674866, 0.12246091663837433]
[2025-05-21 23:18:27,239]: Mean: -0.01961288
[2025-05-21 23:18:27,239]: Min: -0.48984367
[2025-05-21 23:18:27,240]: Max: 0.36738276
[2025-05-21 23:18:27,240]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,240]: Sample Values (25 elements): [0.6585013270378113, 0.7927457690238953, 0.8521690368652344, 0.9148006439208984, 0.7215023040771484, 0.9180263876914978, 0.8440057039260864, 0.8751322031021118, 0.7975272536277771, 0.7330489754676819, 0.7648470997810364, 0.7098923325538635, 0.8391667008399963, 0.6995593905448914, 0.7938991189002991, 0.7940406799316406, 0.7620471119880676, 0.7979812622070312, 0.7001354694366455, 0.7738627195358276, 0.7897228002548218, 0.7355911135673523, 0.7702523469924927, 0.8328304290771484, 0.7459680438041687]
[2025-05-21 23:18:27,240]: Mean: 0.79030848
[2025-05-21 23:18:27,241]: Min: 0.65850133
[2025-05-21 23:18:27,241]: Max: 0.91802639
[2025-05-21 23:18:27,242]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 23:18:27,242]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.08090920746326447, 0.0, -0.08090920746326447, 0.08090920746326447, -0.08090920746326447, 0.0, -0.08090920746326447, 0.0, 0.08090920746326447, 0.08090920746326447, 0.08090920746326447, 0.0, 0.0, 0.0, -0.08090920746326447, 0.0, -0.08090920746326447, -0.08090920746326447, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 23:18:27,242]: Mean: -0.00207189
[2025-05-21 23:18:27,243]: Min: -0.24272762
[2025-05-21 23:18:27,243]: Max: 0.32363683
[2025-05-21 23:18:27,243]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,243]: Sample Values (25 elements): [1.0697669982910156, 1.0552871227264404, 0.8898705244064331, 0.8977818489074707, 0.9295012354850769, 1.0330756902694702, 1.165959119796753, 0.9502223134040833, 1.0065155029296875, 0.9689189195632935, 0.9359892010688782, 0.9992288947105408, 1.0183287858963013, 1.0538657903671265, 0.9960321187973022, 1.0001764297485352, 0.8707518577575684, 1.1083588600158691, 1.04494047164917, 0.9304418563842773, 1.0782512426376343, 1.1868948936462402, 1.0247564315795898, 0.9597353935241699, 1.15846848487854]
[2025-05-21 23:18:27,243]: Mean: 1.00370622
[2025-05-21 23:18:27,244]: Min: 0.82972515
[2025-05-21 23:18:27,244]: Max: 1.18689489
[2025-05-21 23:18:27,246]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 23:18:27,246]: Sample Values (25 elements): [0.062071703374385834, 0.0, 0.12414340674877167, -0.062071703374385834, 0.12414340674877167, 0.0, 0.0, 0.0, 0.062071703374385834, 0.062071703374385834, 0.062071703374385834, 0.0, 0.062071703374385834, 0.0, 0.0, 0.0, 0.0, 0.0, -0.062071703374385834, -0.062071703374385834, 0.062071703374385834, -0.062071703374385834, 0.0, 0.0, 0.0]
[2025-05-21 23:18:27,247]: Mean: -0.00122581
[2025-05-21 23:18:27,247]: Min: -0.18621510
[2025-05-21 23:18:27,248]: Max: 0.24828681
[2025-05-21 23:18:27,248]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,248]: Sample Values (25 elements): [1.1090633869171143, 0.998524010181427, 1.0386911630630493, 0.9680760502815247, 0.9804696440696716, 0.7669464349746704, 1.0519756078720093, 0.9037176370620728, 0.9461429715156555, 0.909322202205658, 1.0126497745513916, 1.0920844078063965, 0.8458799123764038, 0.8830991983413696, 0.9157634973526001, 0.9225530028343201, 0.9924902319908142, 0.9934059381484985, 0.8810474276542664, 1.0072051286697388, 0.7857109904289246, 0.9702550172805786, 1.002191185951233, 0.9971320033073425, 0.8237029910087585]
[2025-05-21 23:18:27,248]: Mean: 0.96253514
[2025-05-21 23:18:27,249]: Min: 0.76694643
[2025-05-21 23:18:27,249]: Max: 1.11047971
[2025-05-21 23:18:27,250]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 23:18:27,250]: Sample Values (25 elements): [0.061626583337783813, -0.061626583337783813, 0.061626583337783813, 0.0, 0.0, 0.0, 0.12325316667556763, 0.0, 0.061626583337783813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.061626583337783813, 0.061626583337783813, -0.12325316667556763, 0.061626583337783813, -0.061626583337783813, 0.061626583337783813, 0.12325316667556763, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 23:18:27,250]: Mean: -0.00393190
[2025-05-21 23:18:27,251]: Min: -0.24650633
[2025-05-21 23:18:27,251]: Max: 0.18487975
[2025-05-21 23:18:27,251]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,251]: Sample Values (25 elements): [0.9336740374565125, 0.8948348164558411, 0.9139412045478821, 0.9718142151832581, 0.8865360021591187, 0.9895438551902771, 1.006184458732605, 0.9410354495048523, 1.0455635786056519, 0.9943426251411438, 0.9514087438583374, 1.0495496988296509, 1.0203375816345215, 1.0117062330245972, 0.8972795009613037, 0.9340367913246155, 1.041152000427246, 0.9483453631401062, 1.0373682975769043, 0.9025425314903259, 0.9708470702171326, 0.988978922367096, 0.9837407469749451, 0.9806559085845947, 0.983763575553894]
[2025-05-21 23:18:27,251]: Mean: 0.97029710
[2025-05-21 23:18:27,252]: Min: 0.86517274
[2025-05-21 23:18:27,252]: Max: 1.04954970
[2025-05-21 23:18:27,253]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-21 23:18:27,254]: Sample Values (25 elements): [0.1734900176525116, 0.05783000588417053, -0.05783000588417053, 0.05783000588417053, 0.0, 0.05783000588417053, -0.05783000588417053, 0.0, -0.05783000588417053, 0.05783000588417053, 0.0, -0.05783000588417053, 0.11566001176834106, 0.05783000588417053, 0.11566001176834106, 0.0, 0.1734900176525116, 0.05783000588417053, 0.05783000588417053, 0.0, -0.11566001176834106, -0.05783000588417053, -0.05783000588417053, -0.05783000588417053, 0.0]
[2025-05-21 23:18:27,254]: Mean: 0.00007530
[2025-05-21 23:18:27,255]: Min: -0.17349002
[2025-05-21 23:18:27,255]: Max: 0.23132002
[2025-05-21 23:18:27,255]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-21 23:18:27,255]: Sample Values (25 elements): [0.9525097012519836, 0.9688073396682739, 1.0387569665908813, 0.9864469766616821, 0.9873892068862915, 0.8437289595603943, 0.9776554107666016, 0.9054514169692993, 0.8877071142196655, 0.91746985912323, 0.9778081178665161, 0.8976635932922363, 0.9515783190727234, 0.9629969000816345, 1.0366982221603394, 0.8861321210861206, 0.8388326168060303, 0.9513005614280701, 0.8954334855079651, 0.9701024293899536, 0.9286352396011353, 1.018792986869812, 1.0159459114074707, 1.0072996616363525, 0.9639223217964172]
[2025-05-21 23:18:27,256]: Mean: 0.95281136
[2025-05-21 23:18:27,256]: Min: 0.83883262
[2025-05-21 23:18:27,256]: Max: 1.12152350
[2025-05-21 23:18:27,257]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-21 23:18:27,258]: Sample Values (25 elements): [-0.11057060956954956, 0.05528530478477478, -0.05528530478477478, 0.05528530478477478, 0.0, 0.05528530478477478, 0.0, 0.0, -0.05528530478477478, 0.0, 0.05528530478477478, -0.05528530478477478, 0.0, -0.05528530478477478, 0.0, 0.0, -0.05528530478477478, -0.05528530478477478, 0.05528530478477478, 0.0, -0.05528530478477478, -0.05528530478477478, 0.0, 0.05528530478477478, 0.0]
[2025-05-21 23:18:27,258]: Mean: 0.00043792
[2025-05-21 23:18:27,258]: Min: -0.16585591
[2025-05-21 23:18:27,258]: Max: 0.16585591
[2025-05-21 23:18:27,258]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,259]: Sample Values (25 elements): [0.9697741270065308, 1.0768513679504395, 0.9849786758422852, 0.9712185263633728, 0.9292427897453308, 0.9462367296218872, 0.9457752108573914, 0.9414933323860168, 1.1104722023010254, 0.9755793213844299, 0.9723425507545471, 1.0033795833587646, 1.093005895614624, 0.9204162359237671, 1.0840826034545898, 0.9316262006759644, 0.8776882290840149, 0.955715000629425, 0.9310096502304077, 0.9830222129821777, 0.9966247081756592, 0.9977641701698303, 0.9854021668434143, 1.0644488334655762, 0.9497258067131042]
[2025-05-21 23:18:27,259]: Mean: 0.98433036
[2025-05-21 23:18:27,260]: Min: 0.87768823
[2025-05-21 23:18:27,260]: Max: 1.11047220
[2025-05-21 23:18:27,262]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 23:18:27,263]: Sample Values (25 elements): [0.0, 0.0, 0.05509132146835327, 0.0, 0.05509132146835327, -0.05509132146835327, -0.05509132146835327, 0.0, 0.0, 0.0, 0.16527396440505981, 0.05509132146835327, 0.05509132146835327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05509132146835327, 0.0, -0.05509132146835327]
[2025-05-21 23:18:27,263]: Mean: -0.00194876
[2025-05-21 23:18:27,263]: Min: -0.16527396
[2025-05-21 23:18:27,264]: Max: 0.22036529
[2025-05-21 23:18:27,264]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,264]: Sample Values (25 elements): [0.9491581916809082, 0.9715494513511658, 1.0080074071884155, 1.0092966556549072, 0.9921900629997253, 1.038537859916687, 1.0840582847595215, 0.9873539805412292, 0.9782136678695679, 0.993636429309845, 0.9706736207008362, 1.0256801843643188, 0.9266098141670227, 0.9839268326759338, 0.9871495366096497, 1.0538036823272705, 0.9366574883460999, 1.0268806219100952, 1.001887321472168, 0.994988739490509, 1.0334244966506958, 0.992352306842804, 1.0124797821044922, 1.0221738815307617, 1.021523356437683]
[2025-05-21 23:18:27,264]: Mean: 1.00366497
[2025-05-21 23:18:27,265]: Min: 0.86100686
[2025-05-21 23:18:27,265]: Max: 1.12875473
[2025-05-21 23:18:27,266]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-21 23:18:27,267]: Sample Values (25 elements): [-0.07636313885450363, -0.15272627770900726, 0.0, 0.15272627770900726, 0.0, 0.0, 0.07636313885450363, 0.0, 0.15272627770900726, 0.07636313885450363, -0.2290894091129303, 0.15272627770900726, -0.07636313885450363, -0.2290894091129303, -0.15272627770900726, 0.0, 0.07636313885450363, 0.07636313885450363, 0.0, -0.15272627770900726, 0.0, 0.2290894091129303, 0.0, 0.15272627770900726, 0.07636313885450363]
[2025-05-21 23:18:27,267]: Mean: 0.00238635
[2025-05-21 23:18:27,267]: Min: -0.22908941
[2025-05-21 23:18:27,267]: Max: 0.30545256
[2025-05-21 23:18:27,267]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,268]: Sample Values (25 elements): [0.7722552418708801, 0.8061272501945496, 0.8384316563606262, 0.8060494661331177, 0.7717742919921875, 0.8179755210876465, 0.7621990442276001, 0.8156159520149231, 0.8189894556999207, 0.78136146068573, 0.8603611588478088, 0.7767423987388611, 0.8402450084686279, 0.8701173067092896, 0.7709798812866211, 0.8016510605812073, 0.8380581140518188, 0.8078860640525818, 0.83296138048172, 0.7932305335998535, 0.7943134903907776, 0.7113534212112427, 0.8558046817779541, 0.8427128791809082, 0.8554657697677612]
[2025-05-21 23:18:27,268]: Mean: 0.80810636
[2025-05-21 23:18:27,268]: Min: 0.71135342
[2025-05-21 23:18:27,268]: Max: 0.90746909
[2025-05-21 23:18:27,270]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 23:18:27,270]: Sample Values (25 elements): [0.0, 0.10732613503932953, 0.0, 0.0, 0.053663067519664764, -0.053663067519664764, 0.0, -0.053663067519664764, 0.0, 0.0, -0.053663067519664764, 0.0, 0.0, -0.053663067519664764, 0.0, 0.0, 0.0, -0.053663067519664764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 23:18:27,270]: Mean: -0.00199286
[2025-05-21 23:18:27,270]: Min: -0.16098920
[2025-05-21 23:18:27,271]: Max: 0.21465227
[2025-05-21 23:18:27,271]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,271]: Sample Values (25 elements): [0.9945679903030396, 0.9644440412521362, 0.8883562684059143, 0.9385988712310791, 0.9479621052742004, 0.9806565642356873, 0.9720055460929871, 1.0356898307800293, 0.9566644430160522, 0.9382753968238831, 0.9590657353401184, 0.9675671458244324, 1.007167935371399, 0.8905037641525269, 0.9498561024665833, 0.8848189115524292, 0.9505646228790283, 0.9938195943832397, 0.9880184531211853, 0.9485945105552673, 0.9112082123756409, 0.9524645209312439, 0.9477410316467285, 1.0071622133255005, 0.9750998616218567]
[2025-05-21 23:18:27,271]: Mean: 0.96105987
[2025-05-21 23:18:27,271]: Min: 0.84958780
[2025-05-21 23:18:27,271]: Max: 1.09102547
[2025-05-21 23:18:27,272]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 23:18:27,274]: Sample Values (25 elements): [0.03892768546938896, 0.03892768546938896, 0.03892768546938896, -0.03892768546938896, 0.03892768546938896, -0.03892768546938896, 0.03892768546938896, 0.07785537093877792, 0.0, 0.0, 0.0, 0.03892768546938896, 0.0, -0.03892768546938896, 0.0, 0.0, 0.07785537093877792, 0.0, -0.03892768546938896, -0.03892768546938896, 0.03892768546938896, 0.03892768546938896, 0.03892768546938896, 0.03892768546938896, 0.07785537093877792]
[2025-05-21 23:18:27,274]: Mean: 0.00047414
[2025-05-21 23:18:27,274]: Min: -0.11678305
[2025-05-21 23:18:27,274]: Max: 0.15571074
[2025-05-21 23:18:27,275]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,275]: Sample Values (25 elements): [0.9287384748458862, 0.9541096091270447, 0.9620728492736816, 0.968593180179596, 0.9019267559051514, 0.988968014717102, 0.950221836566925, 0.986117422580719, 0.9245813488960266, 0.9988850951194763, 1.0365735292434692, 0.9439606666564941, 0.9036327600479126, 0.9795234799385071, 0.9623492956161499, 0.9447775483131409, 1.069361925125122, 1.0050790309906006, 1.0121463537216187, 1.0285519361495972, 1.0256315469741821, 1.0093032121658325, 1.0163562297821045, 0.9509775042533875, 0.9471279382705688]
[2025-05-21 23:18:27,275]: Mean: 0.97407609
[2025-05-21 23:18:27,276]: Min: 0.90192676
[2025-05-21 23:18:27,276]: Max: 1.06936193
[2025-05-21 23:18:27,277]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 23:18:27,278]: Sample Values (25 elements): [0.035806406289339066, 0.0, 0.0, -0.035806406289339066, 0.0, -0.035806406289339066, 0.035806406289339066, -0.07161281257867813, -0.035806406289339066, 0.0, 0.0, 0.0, 0.035806406289339066, -0.035806406289339066, 0.0, 0.035806406289339066, 0.0, -0.035806406289339066, 0.0, -0.035806406289339066, -0.035806406289339066, 0.035806406289339066, 0.035806406289339066, 0.0, -0.035806406289339066]
[2025-05-21 23:18:27,278]: Mean: -0.00080230
[2025-05-21 23:18:27,278]: Min: -0.10741922
[2025-05-21 23:18:27,278]: Max: 0.14322563
[2025-05-21 23:18:27,278]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,279]: Sample Values (25 elements): [0.9022771120071411, 0.8849266171455383, 0.9070892930030823, 0.9237043857574463, 0.902043879032135, 0.8940242528915405, 0.9321383237838745, 0.9238648414611816, 0.9018568396568298, 0.9193441867828369, 0.9059422016143799, 0.9039760231971741, 0.8987324833869934, 0.9481688141822815, 0.8884120583534241, 0.9227414131164551, 0.9432377219200134, 0.8765845894813538, 0.9285266995429993, 0.9568486213684082, 0.936826765537262, 0.9352602958679199, 0.9211693406105042, 0.8974982500076294, 0.9476999044418335]
[2025-05-21 23:18:27,279]: Mean: 0.91755843
[2025-05-21 23:18:27,279]: Min: 0.85397911
[2025-05-21 23:18:27,279]: Max: 0.96909159
[2025-05-21 23:18:27,281]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 23:18:27,282]: Sample Values (25 elements): [0.0, 0.050434939563274384, 0.0, -0.025217469781637192, 0.050434939563274384, 0.0, -0.025217469781637192, 0.0, -0.025217469781637192, 0.0, 0.025217469781637192, 0.025217469781637192, 0.0, -0.025217469781637192, 0.0, 0.025217469781637192, 0.0, -0.025217469781637192, -0.025217469781637192, 0.050434939563274384, 0.025217469781637192, -0.050434939563274384, 0.0, -0.025217469781637192, -0.025217469781637192]
[2025-05-21 23:18:27,282]: Mean: 0.00075658
[2025-05-21 23:18:27,283]: Min: -0.07565241
[2025-05-21 23:18:27,283]: Max: 0.10086988
[2025-05-21 23:18:27,283]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-21 23:18:27,284]: Sample Values (25 elements): [1.0214568376541138, 1.038080096244812, 1.0076038837432861, 1.073620080947876, 0.9889789819717407, 1.060762643814087, 1.0390052795410156, 1.036756992340088, 0.9965452551841736, 0.9949683547019958, 1.0180062055587769, 1.00234854221344, 1.0114084482192993, 1.0138145685195923, 0.9851654171943665, 1.0302839279174805, 1.0307068824768066, 1.0091012716293335, 1.0749849081039429, 1.0299928188323975, 1.0157126188278198, 1.018319845199585, 1.0586212873458862, 1.0250542163848877, 1.0616151094436646]
[2025-05-21 23:18:27,284]: Mean: 1.02523565
[2025-05-21 23:18:27,284]: Min: 0.96868366
[2025-05-21 23:18:27,284]: Max: 1.09440136
[2025-05-21 23:18:27,284]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-21 23:18:27,285]: Sample Values (25 elements): [-0.33313891291618347, 0.4421927332878113, -0.21075300872325897, 0.027559969574213028, -0.03285321593284607, -0.36649858951568604, 0.35810571908950806, -0.08956371247768402, -0.1425510048866272, 0.18639391660690308, -0.217096209526062, 0.42802292108535767, 0.11611239612102509, -0.18481247127056122, 0.10654902458190918, 0.33337414264678955, -0.17610514163970947, -0.13761718571186066, -0.27718615531921387, -0.15343625843524933, 0.22302615642547607, -0.15738023817539215, 0.4815313518047333, 0.21112412214279175, -0.13990023732185364]
[2025-05-21 23:18:27,285]: Mean: -0.00341579
[2025-05-21 23:18:27,285]: Min: -0.45831314
[2025-05-21 23:18:27,285]: Max: 0.66329265
[2025-05-21 23:18:27,285]: 


QAT of ResNet20 with parametrized_relu down to 4 bits...
[2025-05-21 23:18:27,397]: [ResNet20_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-05-21 23:18:27,429]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-21 23:19:26,373]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.7554 Train Acc: 0.7376 Eval Loss: 1.5158 Eval Acc: 0.5940 (LR: 0.010000)
[2025-05-21 23:20:24,939]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.6146 Train Acc: 0.7878 Eval Loss: 0.8244 Eval Acc: 0.7496 (LR: 0.010000)
[2025-05-21 23:21:23,589]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.5729 Train Acc: 0.8011 Eval Loss: 0.6767 Eval Acc: 0.7700 (LR: 0.010000)
[2025-05-21 23:22:22,013]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.5425 Train Acc: 0.8121 Eval Loss: 0.5919 Eval Acc: 0.8031 (LR: 0.010000)
[2025-05-21 23:23:20,717]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.5157 Train Acc: 0.8226 Eval Loss: 0.5860 Eval Acc: 0.8037 (LR: 0.010000)
[2025-05-21 23:24:19,307]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.4952 Train Acc: 0.8279 Eval Loss: 0.6615 Eval Acc: 0.7794 (LR: 0.010000)
[2025-05-21 23:25:17,429]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.4865 Train Acc: 0.8315 Eval Loss: 0.7167 Eval Acc: 0.7791 (LR: 0.010000)
[2025-05-21 23:26:15,311]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.4733 Train Acc: 0.8373 Eval Loss: 0.5397 Eval Acc: 0.8180 (LR: 0.010000)
[2025-05-21 23:27:13,692]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.4597 Train Acc: 0.8414 Eval Loss: 0.5543 Eval Acc: 0.8182 (LR: 0.010000)
[2025-05-21 23:28:11,844]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.4406 Train Acc: 0.8459 Eval Loss: 0.5862 Eval Acc: 0.8142 (LR: 0.010000)
[2025-05-21 23:29:10,104]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.4358 Train Acc: 0.8473 Eval Loss: 0.5188 Eval Acc: 0.8269 (LR: 0.010000)
[2025-05-21 23:30:08,559]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.4262 Train Acc: 0.8518 Eval Loss: 0.5523 Eval Acc: 0.8220 (LR: 0.010000)
[2025-05-21 23:31:06,667]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.4183 Train Acc: 0.8535 Eval Loss: 0.5406 Eval Acc: 0.8303 (LR: 0.010000)
[2025-05-21 23:32:04,997]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.4124 Train Acc: 0.8570 Eval Loss: 0.5244 Eval Acc: 0.8322 (LR: 0.010000)
[2025-05-21 23:33:03,502]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.4035 Train Acc: 0.8588 Eval Loss: 0.4535 Eval Acc: 0.8473 (LR: 0.001000)
[2025-05-21 23:34:01,661]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.3284 Train Acc: 0.8861 Eval Loss: 0.3715 Eval Acc: 0.8759 (LR: 0.001000)
[2025-05-21 23:34:59,903]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.3093 Train Acc: 0.8933 Eval Loss: 0.3632 Eval Acc: 0.8786 (LR: 0.001000)
[2025-05-21 23:35:57,960]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.3019 Train Acc: 0.8960 Eval Loss: 0.3744 Eval Acc: 0.8738 (LR: 0.001000)
[2025-05-21 23:36:56,333]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.2962 Train Acc: 0.8970 Eval Loss: 0.3697 Eval Acc: 0.8771 (LR: 0.001000)
[2025-05-21 23:37:55,036]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.2938 Train Acc: 0.8990 Eval Loss: 0.3662 Eval Acc: 0.8805 (LR: 0.001000)
[2025-05-21 23:38:53,598]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.2941 Train Acc: 0.8967 Eval Loss: 0.3685 Eval Acc: 0.8806 (LR: 0.001000)
[2025-05-21 23:39:51,883]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.2876 Train Acc: 0.8997 Eval Loss: 0.3710 Eval Acc: 0.8774 (LR: 0.001000)
[2025-05-21 23:40:50,349]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.2863 Train Acc: 0.8998 Eval Loss: 0.3673 Eval Acc: 0.8777 (LR: 0.001000)
[2025-05-21 23:41:48,542]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.2811 Train Acc: 0.9030 Eval Loss: 0.3677 Eval Acc: 0.8777 (LR: 0.001000)
[2025-05-21 23:42:47,095]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.2825 Train Acc: 0.9035 Eval Loss: 0.3707 Eval Acc: 0.8773 (LR: 0.001000)
[2025-05-21 23:43:45,099]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.2837 Train Acc: 0.9011 Eval Loss: 0.3679 Eval Acc: 0.8774 (LR: 0.001000)
[2025-05-21 23:44:43,800]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.2808 Train Acc: 0.9013 Eval Loss: 0.3749 Eval Acc: 0.8742 (LR: 0.001000)
[2025-05-21 23:45:42,173]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.2816 Train Acc: 0.9026 Eval Loss: 0.3719 Eval Acc: 0.8783 (LR: 0.001000)
[2025-05-21 23:46:40,680]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.2802 Train Acc: 0.9032 Eval Loss: 0.3729 Eval Acc: 0.8778 (LR: 0.001000)
[2025-05-21 23:47:39,032]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.2749 Train Acc: 0.9039 Eval Loss: 0.3730 Eval Acc: 0.8772 (LR: 0.000100)
[2025-05-21 23:48:38,049]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.2659 Train Acc: 0.9067 Eval Loss: 0.3554 Eval Acc: 0.8786 (LR: 0.000100)
[2025-05-21 23:49:36,821]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.2679 Train Acc: 0.9075 Eval Loss: 0.3578 Eval Acc: 0.8828 (LR: 0.000100)
[2025-05-21 23:50:35,578]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.2597 Train Acc: 0.9096 Eval Loss: 0.3592 Eval Acc: 0.8816 (LR: 0.000100)
[2025-05-21 23:51:34,034]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.2609 Train Acc: 0.9096 Eval Loss: 0.3564 Eval Acc: 0.8818 (LR: 0.000100)
[2025-05-21 23:52:32,272]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.2593 Train Acc: 0.9096 Eval Loss: 0.3540 Eval Acc: 0.8820 (LR: 0.000100)
[2025-05-21 23:53:30,535]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.2590 Train Acc: 0.9109 Eval Loss: 0.3537 Eval Acc: 0.8842 (LR: 0.000100)
[2025-05-21 23:54:29,139]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.2599 Train Acc: 0.9104 Eval Loss: 0.3522 Eval Acc: 0.8851 (LR: 0.000100)
[2025-05-21 23:55:27,687]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.2589 Train Acc: 0.9081 Eval Loss: 0.3551 Eval Acc: 0.8811 (LR: 0.000100)
[2025-05-21 23:56:25,991]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.2548 Train Acc: 0.9108 Eval Loss: 0.3548 Eval Acc: 0.8819 (LR: 0.000100)
[2025-05-21 23:57:25,192]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.2604 Train Acc: 0.9087 Eval Loss: 0.3535 Eval Acc: 0.8858 (LR: 0.000100)
[2025-05-21 23:58:23,675]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.2588 Train Acc: 0.9094 Eval Loss: 0.3505 Eval Acc: 0.8848 (LR: 0.000100)
[2025-05-21 23:59:22,297]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.2602 Train Acc: 0.9093 Eval Loss: 0.3524 Eval Acc: 0.8854 (LR: 0.000100)
[2025-05-22 00:00:21,051]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.2571 Train Acc: 0.9098 Eval Loss: 0.3547 Eval Acc: 0.8826 (LR: 0.000100)
[2025-05-22 00:01:19,417]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.2550 Train Acc: 0.9106 Eval Loss: 0.3575 Eval Acc: 0.8816 (LR: 0.000100)
[2025-05-22 00:02:18,340]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.2559 Train Acc: 0.9108 Eval Loss: 0.3527 Eval Acc: 0.8838 (LR: 0.000010)
[2025-05-22 00:03:17,036]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.2580 Train Acc: 0.9119 Eval Loss: 0.3551 Eval Acc: 0.8818 (LR: 0.000010)
[2025-05-22 00:04:15,793]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.2570 Train Acc: 0.9114 Eval Loss: 0.3509 Eval Acc: 0.8822 (LR: 0.000010)
[2025-05-22 00:05:14,597]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.2555 Train Acc: 0.9112 Eval Loss: 0.3516 Eval Acc: 0.8832 (LR: 0.000010)
[2025-05-22 00:06:13,165]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.2576 Train Acc: 0.9110 Eval Loss: 0.3579 Eval Acc: 0.8839 (LR: 0.000010)
[2025-05-22 00:07:12,029]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.2525 Train Acc: 0.9131 Eval Loss: 0.3545 Eval Acc: 0.8833 (LR: 0.000010)
[2025-05-22 00:08:10,666]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.2513 Train Acc: 0.9122 Eval Loss: 0.3550 Eval Acc: 0.8815 (LR: 0.000010)
[2025-05-22 00:09:09,529]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.2517 Train Acc: 0.9137 Eval Loss: 0.3549 Eval Acc: 0.8838 (LR: 0.000010)
[2025-05-22 00:10:06,542]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.2554 Train Acc: 0.9120 Eval Loss: 0.3538 Eval Acc: 0.8817 (LR: 0.000010)
[2025-05-22 00:11:02,838]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.2548 Train Acc: 0.9113 Eval Loss: 0.3524 Eval Acc: 0.8827 (LR: 0.000010)
[2025-05-22 00:11:58,754]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.2545 Train Acc: 0.9113 Eval Loss: 0.3539 Eval Acc: 0.8840 (LR: 0.000010)
[2025-05-22 00:12:54,826]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.2552 Train Acc: 0.9112 Eval Loss: 0.3525 Eval Acc: 0.8837 (LR: 0.000010)
[2025-05-22 00:13:51,310]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.2537 Train Acc: 0.9110 Eval Loss: 0.3473 Eval Acc: 0.8846 (LR: 0.000010)
[2025-05-22 00:14:48,556]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.2525 Train Acc: 0.9121 Eval Loss: 0.3517 Eval Acc: 0.8820 (LR: 0.000010)
[2025-05-22 00:15:44,223]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.2530 Train Acc: 0.9124 Eval Loss: 0.3561 Eval Acc: 0.8831 (LR: 0.000010)
[2025-05-22 00:16:40,518]: [ResNet20_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.2560 Train Acc: 0.9099 Eval Loss: 0.3515 Eval Acc: 0.8830 (LR: 0.000010)
[2025-05-22 00:16:40,518]: [ResNet20_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.8858
[2025-05-22 00:16:40,576]: 


Quantization of model down to 4 bits finished
[2025-05-22 00:16:40,576]: Model Architecture:
[2025-05-22 00:16:40,631]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0602], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4537796080112457, max_val=0.44994479417800903)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3740], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0531], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43698152899742126, max_val=0.3587712347507477)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3659], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0821], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7611862421035767, max_val=0.470145583152771)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3740], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0535], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.45274290442466736, max_val=0.34965837001800537)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3752], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0391], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2952578663825989, max_val=0.29114118218421936)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3748], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0344], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23101834952831268, max_val=0.28535181283950806)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3865], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0318], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2387697696685791, max_val=0.23783817887306213)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3748], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0342], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22110193967819214, max_val=0.2919522821903229)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0509], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4113409221172333, max_val=0.35200318694114685)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3770], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0378], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25513264536857605, max_val=0.31169724464416504)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3749], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0322], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21732905507087708, max_val=0.2661384344100952)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3796], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0293], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2128531038761139, max_val=0.2259879857301712)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3749], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0235], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17596283555030823, max_val=0.17578959465026855)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3952], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0243], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1764543354511261, max_val=0.18860657513141632)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3748], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0258], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1699548363685608, max_val=0.21659520268440247)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0378], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2718123197555542, max_val=0.29463398456573486)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3779], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0250], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1794048398733139, max_val=0.1953984498977661)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3747], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0190], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13977593183517456, max_val=0.14502373337745667)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3755], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0171], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1222662702202797, max_val=0.13453759253025055)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3752], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09088800102472305, max_val=0.09805409610271454)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3938], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-22 00:16:40,642]: 
Model Weights:
[2025-05-22 00:16:40,642]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-22 00:16:40,642]: Sample Values (25 elements): [-0.1078486442565918, 0.1350180059671402, -0.1902039349079132, 0.2558635175228119, 0.23460787534713745, 0.44832828640937805, 0.3033508062362671, -0.3275943696498871, 0.1015990599989891, -0.12210527807474136, -0.060851745307445526, -0.23426449298858643, 0.2322501689195633, 0.3152020573616028, -0.2194884866476059, 0.13583838939666748, -0.32684141397476196, -0.01871267706155777, -0.6990774273872375, 0.36645811796188354, 0.34946972131729126, 0.6698700785636902, 0.08422230184078217, -0.052760716527700424, 0.03690601885318756]
[2025-05-22 00:16:40,642]: Mean: -0.00387564
[2025-05-22 00:16:40,643]: Min: -0.70500338
[2025-05-22 00:16:40,643]: Max: 1.00367641
[2025-05-22 00:16:40,643]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,643]: Sample Values (16 elements): [0.9117791652679443, 0.8770321607589722, 0.8303817510604858, 1.133016586303711, 1.189487099647522, 1.148451566696167, 1.169032335281372, 1.133916974067688, 1.3614786863327026, 1.0024383068084717, 1.184410572052002, 0.870747983455658, 1.1327635049819946, 1.1474707126617432, 1.2961934804916382, 1.0153322219848633]
[2025-05-22 00:16:40,643]: Mean: 1.08774579
[2025-05-22 00:16:40,643]: Min: 0.83038175
[2025-05-22 00:16:40,644]: Max: 1.36147869
[2025-05-22 00:16:40,645]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 00:16:40,645]: Sample Values (25 elements): [0.0, -0.3012414574623108, -0.18074487149715424, 0.06024828925728798, 0.0, 0.12049657851457596, 0.06024828925728798, -0.06024828925728798, 0.06024828925728798, -0.12049657851457596, 0.0, 0.0, 0.06024828925728798, -0.06024828925728798, -0.06024828925728798, -0.06024828925728798, -0.12049657851457596, 0.0, -0.12049657851457596, 0.24099315702915192, 0.06024828925728798, 0.0, -0.24099315702915192, -0.12049657851457596, -0.18074487149715424]
[2025-05-22 00:16:40,645]: Mean: -0.01257788
[2025-05-22 00:16:40,645]: Min: -0.48198631
[2025-05-22 00:16:40,645]: Max: 0.42173803
[2025-05-22 00:16:40,645]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,646]: Sample Values (16 elements): [0.7681272625923157, 0.9738441705703735, 0.8573170900344849, 0.9808205962181091, 0.8090786337852478, 1.0091235637664795, 1.0284136533737183, 1.0708155632019043, 1.022687554359436, 1.0026695728302002, 1.5428200960159302, 0.9802160859107971, 1.0473309755325317, 1.208540678024292, 0.9767041206359863, 0.9241412878036499]
[2025-05-22 00:16:40,646]: Mean: 1.01266575
[2025-05-22 00:16:40,646]: Min: 0.76812726
[2025-05-22 00:16:40,646]: Max: 1.54282010
[2025-05-22 00:16:40,647]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 00:16:40,647]: Sample Values (25 elements): [0.15915057063102722, 0.0, -0.05305019021034241, 0.05305019021034241, 0.15915057063102722, 0.05305019021034241, -0.05305019021034241, 0.0, 0.0, -0.10610038042068481, 0.10610038042068481, 0.10610038042068481, -0.05305019021034241, 0.05305019021034241, -0.10610038042068481, 0.0, 0.05305019021034241, 0.0, 0.31830114126205444, 0.15915057063102722, 0.0, -0.10610038042068481, -0.05305019021034241, -0.15915057063102722, 0.0]
[2025-05-22 00:16:40,647]: Mean: -0.00679245
[2025-05-22 00:16:40,648]: Min: -0.42440152
[2025-05-22 00:16:40,648]: Max: 0.37135133
[2025-05-22 00:16:40,648]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,648]: Sample Values (16 elements): [0.9749199748039246, 1.2602895498275757, 0.9741387963294983, 0.9863230586051941, 0.9853661060333252, 0.9499316215515137, 0.9321873188018799, 0.9015507102012634, 1.1297376155853271, 0.9555295705795288, 1.1270909309387207, 1.031840205192566, 1.0380464792251587, 0.9785493612289429, 1.0164700746536255, 0.8643524646759033]
[2025-05-22 00:16:40,648]: Mean: 1.00664520
[2025-05-22 00:16:40,648]: Min: 0.86435246
[2025-05-22 00:16:40,649]: Max: 1.26028955
[2025-05-22 00:16:40,649]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 00:16:40,650]: Sample Values (25 elements): [0.0, -0.08208879083395004, 0.08208879083395004, 0.0, 0.08208879083395004, -0.08208879083395004, 0.08208879083395004, -0.08208879083395004, 0.08208879083395004, 0.0, 0.0, -0.08208879083395004, 0.08208879083395004, 0.08208879083395004, -0.24626636505126953, 0.0, -0.16417758166790009, 0.16417758166790009, 0.0, 0.0, 0.08208879083395004, -0.16417758166790009, 0.0, 0.0, 0.16417758166790009]
[2025-05-22 00:16:40,650]: Mean: -0.00185270
[2025-05-22 00:16:40,650]: Min: -0.73879910
[2025-05-22 00:16:40,650]: Max: 0.49253273
[2025-05-22 00:16:40,650]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,650]: Sample Values (16 elements): [0.8865045309066772, 1.4103304147720337, 0.9447039365768433, 0.7421282529830933, 0.7361166477203369, 0.8573715686798096, 0.8991833329200745, 1.1514695882797241, 0.9245203137397766, 1.1357040405273438, 1.4185776710510254, 0.9197948575019836, 0.9156448245048523, 1.18741774559021, 0.872187614440918, 1.003907561302185]
[2025-05-22 00:16:40,651]: Mean: 1.00034761
[2025-05-22 00:16:40,651]: Min: 0.73611665
[2025-05-22 00:16:40,651]: Max: 1.41857767
[2025-05-22 00:16:40,652]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 00:16:40,652]: Sample Values (25 elements): [-0.21397365629673004, -0.16048024594783783, -0.05349341407418251, 0.05349341407418251, 0.05349341407418251, 0.0, 0.10698682814836502, 0.10698682814836502, -0.05349341407418251, -0.05349341407418251, -0.4279473125934601, -0.10698682814836502, 0.0, -0.05349341407418251, 0.10698682814836502, -0.05349341407418251, 0.05349341407418251, 0.0, -0.05349341407418251, 0.0, -0.05349341407418251, 0.10698682814836502, 0.05349341407418251, -0.10698682814836502, 0.10698682814836502]
[2025-05-22 00:16:40,652]: Mean: -0.00225211
[2025-05-22 00:16:40,653]: Min: -0.42794731
[2025-05-22 00:16:40,653]: Max: 0.37445390
[2025-05-22 00:16:40,653]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,653]: Sample Values (16 elements): [1.0399961471557617, 0.8816711902618408, 1.034170150756836, 1.3287057876586914, 0.9274609684944153, 0.9854952692985535, 0.8270979523658752, 0.8809397220611572, 0.8291406631469727, 0.921745777130127, 1.1023638248443604, 1.5104178190231323, 1.0653783082962036, 0.8701657652854919, 1.0022157430648804, 0.7795166373252869]
[2025-05-22 00:16:40,653]: Mean: 0.99915510
[2025-05-22 00:16:40,653]: Min: 0.77951664
[2025-05-22 00:16:40,653]: Max: 1.51041782
[2025-05-22 00:16:40,654]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 00:16:40,655]: Sample Values (25 elements): [-0.07818654179573059, 0.07818654179573059, -0.07818654179573059, -0.07818654179573059, 0.039093270897865295, 0.07818654179573059, -0.039093270897865295, -0.07818654179573059, -0.11727981269359589, -0.039093270897865295, -0.07818654179573059, 0.07818654179573059, -0.07818654179573059, -0.23455962538719177, 0.039093270897865295, 0.0, 0.0, 0.0, 0.11727981269359589, -0.07818654179573059, 0.039093270897865295, -0.15637308359146118, 0.11727981269359589, 0.0, -0.11727981269359589]
[2025-05-22 00:16:40,655]: Mean: -0.00576897
[2025-05-22 00:16:40,655]: Min: -0.31274617
[2025-05-22 00:16:40,655]: Max: 0.27365291
[2025-05-22 00:16:40,655]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,655]: Sample Values (16 elements): [0.8892443776130676, 0.948774516582489, 1.0612632036209106, 1.0574426651000977, 0.8691588640213013, 0.9806563854217529, 0.9567075967788696, 0.9042959213256836, 1.0707628726959229, 0.937880277633667, 1.0233649015426636, 0.9036769866943359, 0.9466552138328552, 0.9319782257080078, 0.8010340332984924, 0.9439619183540344]
[2025-05-22 00:16:40,656]: Mean: 0.95167863
[2025-05-22 00:16:40,656]: Min: 0.80103403
[2025-05-22 00:16:40,656]: Max: 1.07076287
[2025-05-22 00:16:40,657]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-22 00:16:40,657]: Sample Values (25 elements): [-0.0688493624329567, -0.10327404737472534, 0.03442468121647835, -0.0688493624329567, 0.10327404737472534, 0.1376987248659134, 0.0688493624329567, -0.03442468121647835, -0.10327404737472534, 0.10327404737472534, 0.0, 0.0, 0.0, -0.1376987248659134, 0.0688493624329567, 0.03442468121647835, 0.0, 0.0, -0.03442468121647835, -0.0688493624329567, -0.0688493624329567, 0.1376987248659134, -0.0688493624329567, -0.0688493624329567, -0.03442468121647835]
[2025-05-22 00:16:40,657]: Mean: 0.00714193
[2025-05-22 00:16:40,657]: Min: -0.24097277
[2025-05-22 00:16:40,658]: Max: 0.27539745
[2025-05-22 00:16:40,658]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-22 00:16:40,658]: Sample Values (16 elements): [0.8972280025482178, 0.8618245124816895, 0.7780842781066895, 0.9678353071212769, 0.7941441535949707, 0.8630169034004211, 0.9171286821365356, 0.950672447681427, 0.8639490604400635, 0.9820284247398376, 1.1156079769134521, 0.770995557308197, 0.9808856248855591, 0.7510547637939453, 0.8937926888465881, 0.9879869222640991]
[2025-05-22 00:16:40,658]: Mean: 0.89851475
[2025-05-22 00:16:40,658]: Min: 0.75105476
[2025-05-22 00:16:40,658]: Max: 1.11560798
[2025-05-22 00:16:40,659]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-22 00:16:40,660]: Sample Values (25 elements): [0.03177386894822121, -0.06354773789644241, 0.12709547579288483, -0.03177386894822121, 0.09532161056995392, -0.03177386894822121, -0.06354773789644241, 0.09532161056995392, 0.03177386894822121, 0.09532161056995392, -0.03177386894822121, 0.0, -0.12709547579288483, -0.06354773789644241, 0.0, -0.06354773789644241, 0.0, 0.03177386894822121, 0.03177386894822121, -0.06354773789644241, -0.06354773789644241, 0.0, 0.06354773789644241, -0.06354773789644241, -0.09532161056995392]
[2025-05-22 00:16:40,660]: Mean: -0.00444062
[2025-05-22 00:16:40,660]: Min: -0.25419095
[2025-05-22 00:16:40,660]: Max: 0.22241709
[2025-05-22 00:16:40,660]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,660]: Sample Values (25 elements): [0.9121968746185303, 0.849328339099884, 0.9853105545043945, 0.9034472107887268, 0.9701375365257263, 1.0340142250061035, 0.8827571868896484, 0.95569908618927, 1.0965335369110107, 0.9092411994934082, 1.0088047981262207, 0.9189456701278687, 1.019700050354004, 1.0110089778900146, 0.9690556526184082, 1.0050257444381714, 0.9530528783798218, 1.026328206062317, 0.8218099474906921, 1.0802407264709473, 0.955843985080719, 0.9565427303314209, 0.9908496737480164, 0.9786531925201416, 1.0176705121994019]
[2025-05-22 00:16:40,660]: Mean: 0.96283495
[2025-05-22 00:16:40,661]: Min: 0.82180995
[2025-05-22 00:16:40,661]: Max: 1.09653354
[2025-05-22 00:16:40,662]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 00:16:40,662]: Sample Values (25 elements): [0.10261083394289017, -0.03420361131429672, -0.03420361131429672, 0.10261083394289017, -0.06840722262859344, -0.20522166788578033, 0.0, -0.03420361131429672, 0.0, -0.1368144452571869, 0.1368144452571869, 0.0, 0.0, -0.03420361131429672, -0.06840722262859344, -0.03420361131429672, -0.06840722262859344, 0.0, -0.06840722262859344, -0.03420361131429672, -0.03420361131429672, -0.1368144452571869, -0.06840722262859344, 0.03420361131429672, 0.03420361131429672]
[2025-05-22 00:16:40,662]: Mean: -0.00595297
[2025-05-22 00:16:40,662]: Min: -0.20522167
[2025-05-22 00:16:40,663]: Max: 0.30783251
[2025-05-22 00:16:40,663]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,663]: Sample Values (25 elements): [1.0802173614501953, 0.8969730138778687, 0.9371364116668701, 0.969741940498352, 1.05815851688385, 1.0389940738677979, 0.9144045114517212, 0.987711489200592, 1.0158655643463135, 0.9324283599853516, 0.9778246283531189, 0.9355311393737793, 1.051024079322815, 0.9308240413665771, 1.0156307220458984, 1.025488257408142, 0.812286913394928, 0.9522466063499451, 0.9940102100372314, 1.1304811239242554, 0.9702781438827515, 1.0858311653137207, 0.9892642498016357, 1.0132694244384766, 0.9348637461662292]
[2025-05-22 00:16:40,663]: Mean: 0.98234320
[2025-05-22 00:16:40,663]: Min: 0.81228691
[2025-05-22 00:16:40,663]: Max: 1.13048112
[2025-05-22 00:16:40,664]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-22 00:16:40,665]: Sample Values (25 elements): [0.0, 0.15266884863376617, -0.05088961496949196, -0.10177922993898392, 0.20355845987796783, -0.2544480860233307, -0.2544480860233307, -0.2544480860233307, -0.10177922993898392, 0.10177922993898392, -0.15266884863376617, -0.10177922993898392, 0.15266884863376617, -0.05088961496949196, -0.356227308511734, -0.15266884863376617, 0.10177922993898392, 0.30533769726753235, -0.20355845987796783, -0.05088961496949196, -0.2544480860233307, 0.2544480860233307, 0.30533769726753235, -0.05088961496949196, 0.05088961496949196]
[2025-05-22 00:16:40,665]: Mean: -0.01769209
[2025-05-22 00:16:40,665]: Min: -0.40711692
[2025-05-22 00:16:40,665]: Max: 0.35622731
[2025-05-22 00:16:40,665]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,665]: Sample Values (25 elements): [0.6790775060653687, 0.7817420363426208, 0.6946315169334412, 0.8536695241928101, 0.6787219047546387, 0.7574092149734497, 0.9007712006568909, 0.8207850456237793, 0.7346243858337402, 0.8774646520614624, 0.7319020628929138, 0.8022474050521851, 0.7008131742477417, 0.7687380313873291, 0.7022676467895508, 0.7680366039276123, 0.9023357033729553, 0.8090632557868958, 0.7554372549057007, 0.713848888874054, 0.8339517116546631, 0.7791720628738403, 0.7171630263328552, 0.8243768811225891, 0.7946021556854248]
[2025-05-22 00:16:40,665]: Mean: 0.77408105
[2025-05-22 00:16:40,666]: Min: 0.67872190
[2025-05-22 00:16:40,666]: Max: 0.90233570
[2025-05-22 00:16:40,667]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 00:16:40,667]: Sample Values (25 elements): [0.1889432966709137, 0.03778865933418274, -0.07557731866836548, 0.03778865933418274, -0.07557731866836548, 0.0, 0.03778865933418274, 0.03778865933418274, 0.03778865933418274, 0.0, -0.03778865933418274, 0.03778865933418274, -0.03778865933418274, 0.11336597800254822, -0.03778865933418274, 0.0, -0.07557731866836548, 0.0, -0.03778865933418274, 0.03778865933418274, -0.11336597800254822, 0.03778865933418274, -0.03778865933418274, 0.07557731866836548, 0.03778865933418274]
[2025-05-22 00:16:40,667]: Mean: -0.00278413
[2025-05-22 00:16:40,667]: Min: -0.26452062
[2025-05-22 00:16:40,667]: Max: 0.30230927
[2025-05-22 00:16:40,668]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,668]: Sample Values (25 elements): [0.8089997172355652, 1.0266345739364624, 0.8453877568244934, 0.8667646050453186, 0.9554993510246277, 1.003715991973877, 0.9694742560386658, 0.9879680871963501, 0.9623492956161499, 0.9826813340187073, 0.9651666283607483, 1.0893570184707642, 0.951099157333374, 0.946463942527771, 0.8990247249603271, 0.980542778968811, 0.9723602533340454, 0.9084303379058838, 0.8632833361625671, 0.9685958623886108, 0.8573171496391296, 0.8487443923950195, 1.0225751399993896, 0.8325446844100952, 1.0663223266601562]
[2025-05-22 00:16:40,668]: Mean: 0.93751711
[2025-05-22 00:16:40,668]: Min: 0.80899972
[2025-05-22 00:16:40,668]: Max: 1.08935702
[2025-05-22 00:16:40,669]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 00:16:40,670]: Sample Values (25 elements): [-0.0644623413681984, -0.1611558496952057, 0.09669351577758789, 0.0, -0.0644623413681984, 0.0644623413681984, -0.0644623413681984, -0.09669351577758789, -0.1289246827363968, -0.0322311706840992, -0.0644623413681984, -0.0644623413681984, -0.0322311706840992, 0.0322311706840992, 0.0322311706840992, -0.0644623413681984, 0.0, -0.0644623413681984, -0.09669351577758789, 0.0, 0.0322311706840992, 0.0, 0.0, 0.0644623413681984, -0.0644623413681984]
[2025-05-22 00:16:40,670]: Mean: -0.00202494
[2025-05-22 00:16:40,670]: Min: -0.22561820
[2025-05-22 00:16:40,670]: Max: 0.25784937
[2025-05-22 00:16:40,670]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,671]: Sample Values (25 elements): [0.9534130096435547, 0.751795768737793, 0.8404877781867981, 0.9472653865814209, 0.93046635389328, 0.9055166244506836, 0.9900327324867249, 0.8483323454856873, 0.9309241771697998, 0.9348798990249634, 0.9820396304130554, 0.9354997873306274, 0.9010469317436218, 0.7519186735153198, 0.9796867370605469, 0.9879379272460938, 1.0271965265274048, 0.9243209958076477, 0.8718870878219604, 1.024753451347351, 0.8661424517631531, 0.9447875022888184, 1.0548263788223267, 0.8072361946105957, 1.0275450944900513]
[2025-05-22 00:16:40,671]: Mean: 0.92573822
[2025-05-22 00:16:40,671]: Min: 0.75179577
[2025-05-22 00:16:40,671]: Max: 1.06219387
[2025-05-22 00:16:40,672]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 00:16:40,672]: Sample Values (25 elements): [0.0, 0.0, -0.14628036320209503, 0.0, -0.029256071895360947, -0.029256071895360947, 0.029256071895360947, -0.05851214379072189, 0.0, -0.05851214379072189, -0.029256071895360947, 0.14628036320209503, 0.0, -0.05851214379072189, 0.0, 0.05851214379072189, 0.029256071895360947, 0.029256071895360947, -0.05851214379072189, -0.05851214379072189, 0.0, 0.05851214379072189, 0.029256071895360947, 0.08776821196079254, 0.0]
[2025-05-22 00:16:40,673]: Mean: -0.00434905
[2025-05-22 00:16:40,673]: Min: -0.20479250
[2025-05-22 00:16:40,673]: Max: 0.23404858
[2025-05-22 00:16:40,673]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,673]: Sample Values (25 elements): [0.8663014769554138, 0.9322077035903931, 0.8777225613594055, 0.980389416217804, 0.9474201798439026, 0.9494593143463135, 0.9508575797080994, 0.9403734803199768, 1.0201064348220825, 0.8327134847640991, 0.9271834492683411, 0.9860859513282776, 0.9799695611000061, 0.8495001792907715, 0.9189021587371826, 0.8297780752182007, 0.9461796283721924, 0.9781526327133179, 0.8103753924369812, 0.9433609843254089, 0.9122371673583984, 0.9186986088752747, 0.9565095901489258, 0.9604448676109314, 0.9225505590438843]
[2025-05-22 00:16:40,673]: Mean: 0.92582577
[2025-05-22 00:16:40,673]: Min: 0.81037539
[2025-05-22 00:16:40,674]: Max: 1.02010643
[2025-05-22 00:16:40,675]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-22 00:16:40,675]: Sample Values (25 elements): [-0.04690033569931984, 0.07035050541162491, 0.0, -0.11725083738565445, -0.02345016784965992, 0.07035050541162491, 0.02345016784965992, 0.07035050541162491, -0.02345016784965992, -0.04690033569931984, 0.0, 0.04690033569931984, 0.07035050541162491, -0.09380067139863968, -0.09380067139863968, -0.04690033569931984, 0.0, -0.02345016784965992, -0.02345016784965992, -0.02345016784965992, 0.09380067139863968, 0.0, -0.04690033569931984, 0.04690033569931984, 0.07035050541162491]
[2025-05-22 00:16:40,675]: Mean: -0.00040458
[2025-05-22 00:16:40,675]: Min: -0.18760134
[2025-05-22 00:16:40,675]: Max: 0.16415118
[2025-05-22 00:16:40,675]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-22 00:16:40,676]: Sample Values (25 elements): [0.8940185904502869, 1.0021153688430786, 1.0074981451034546, 0.9265869855880737, 0.9167039394378662, 0.9186167120933533, 0.9043973684310913, 0.9491889476776123, 0.8865058422088623, 1.0371794700622559, 1.0123200416564941, 1.0394134521484375, 0.9534645676612854, 0.9073733687400818, 0.8303640484809875, 0.9505929350852966, 0.9940797090530396, 0.8687388896942139, 0.9228237867355347, 0.9405824542045593, 0.8307462930679321, 0.9320471286773682, 0.9284308552742004, 0.8921729326248169, 0.9627602696418762]
[2025-05-22 00:16:40,676]: Mean: 0.92984891
[2025-05-22 00:16:40,676]: Min: 0.83036405
[2025-05-22 00:16:40,676]: Max: 1.03941345
[2025-05-22 00:16:40,677]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-22 00:16:40,678]: Sample Values (25 elements): [0.07301217317581177, -0.09734956920146942, -0.04867478460073471, 0.024337392300367355, 0.04867478460073471, 0.07301217317581177, 0.0, 0.04867478460073471, 0.0, 0.0, -0.04867478460073471, -0.09734956920146942, 0.0, -0.024337392300367355, 0.04867478460073471, 0.07301217317581177, -0.024337392300367355, -0.07301217317581177, -0.07301217317581177, -0.04867478460073471, 0.0, -0.024337392300367355, 0.04867478460073471, 0.0, -0.04867478460073471]
[2025-05-22 00:16:40,678]: Mean: 0.00010959
[2025-05-22 00:16:40,678]: Min: -0.17036174
[2025-05-22 00:16:40,678]: Max: 0.19469914
[2025-05-22 00:16:40,678]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,678]: Sample Values (25 elements): [0.8972722887992859, 0.9722763895988464, 0.8886533379554749, 0.8508360981941223, 0.8303636312484741, 1.0141628980636597, 1.0162570476531982, 0.9327332973480225, 0.9454212188720703, 1.0144134759902954, 0.9490002393722534, 1.0053411722183228, 0.9530500173568726, 0.8832240104675293, 0.9211996793746948, 0.9182659387588501, 0.9356759190559387, 0.8927897214889526, 0.9386569261550903, 0.881574273109436, 0.8434492349624634, 0.8891190886497498, 0.9778597950935364, 1.074188232421875, 0.9403738379478455]
[2025-05-22 00:16:40,678]: Mean: 0.92953444
[2025-05-22 00:16:40,679]: Min: 0.83036363
[2025-05-22 00:16:40,679]: Max: 1.07418823
[2025-05-22 00:16:40,680]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 00:16:40,680]: Sample Values (25 elements): [0.025770006701350212, -0.025770006701350212, 0.0, 0.025770006701350212, 0.025770006701350212, -0.051540013402700424, 0.025770006701350212, -0.051540013402700424, 0.025770006701350212, -0.025770006701350212, 0.025770006701350212, 0.051540013402700424, 0.051540013402700424, -0.025770006701350212, -0.025770006701350212, 0.025770006701350212, -0.025770006701350212, -0.025770006701350212, -0.051540013402700424, 0.025770006701350212, -0.025770006701350212, 0.0, 0.0, 0.025770006701350212, 0.0]
[2025-05-22 00:16:40,680]: Mean: -0.00233345
[2025-05-22 00:16:40,681]: Min: -0.18039005
[2025-05-22 00:16:40,681]: Max: 0.20616005
[2025-05-22 00:16:40,681]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,681]: Sample Values (25 elements): [0.9499397277832031, 0.9835891127586365, 1.0008574724197388, 0.9340378642082214, 1.0234742164611816, 0.9785106182098389, 0.9761450290679932, 0.9584054946899414, 0.9187626838684082, 0.9025920629501343, 0.9599400162696838, 0.9416331052780151, 1.0547401905059814, 0.9834983944892883, 1.0542197227478027, 0.9732057452201843, 1.020461082458496, 0.9245952367782593, 0.9980630874633789, 1.0091241598129272, 1.101242184638977, 1.0276119709014893, 0.9514813423156738, 0.9428396821022034, 1.0093845129013062]
[2025-05-22 00:16:40,681]: Mean: 0.97816718
[2025-05-22 00:16:40,681]: Min: 0.86981261
[2025-05-22 00:16:40,682]: Max: 1.10124218
[2025-05-22 00:16:40,683]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-22 00:16:40,683]: Sample Values (25 elements): [-0.07552619278430939, -0.18881547451019287, -0.07552619278430939, -0.07552619278430939, 0.11328928917646408, 0.07552619278430939, 0.07552619278430939, 0.037763096392154694, 0.11328928917646408, 0.037763096392154694, -0.11328928917646408, -0.11328928917646408, 0.037763096392154694, -0.11328928917646408, 0.0, -0.11328928917646408, -0.07552619278430939, -0.18881547451019287, 0.15105238556861877, 0.11328928917646408, 0.0, 0.07552619278430939, -0.22657857835292816, -0.037763096392154694, -0.15105238556861877]
[2025-05-22 00:16:40,683]: Mean: 0.00127229
[2025-05-22 00:16:40,683]: Min: -0.26434168
[2025-05-22 00:16:40,683]: Max: 0.30210477
[2025-05-22 00:16:40,683]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,684]: Sample Values (25 elements): [0.8384480476379395, 0.7997272610664368, 0.8019197583198547, 0.8357934951782227, 0.7240786552429199, 0.8676502108573914, 0.7984714508056641, 0.8680508136749268, 0.8260884284973145, 0.8010101318359375, 0.798060417175293, 0.8959623575210571, 0.799578845500946, 0.7578728199005127, 0.8050239682197571, 0.8577114939689636, 0.8009256720542908, 0.8373417258262634, 0.8350257277488708, 0.8119733929634094, 0.7170994281768799, 0.8232743740081787, 0.8205404877662659, 0.832820475101471, 0.7796050310134888]
[2025-05-22 00:16:40,684]: Mean: 0.80799478
[2025-05-22 00:16:40,684]: Min: 0.70741004
[2025-05-22 00:16:40,684]: Max: 0.89596236
[2025-05-22 00:16:40,685]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 00:16:40,686]: Sample Values (25 elements): [0.0, 0.04997376725077629, -0.04997376725077629, -0.024986883625388145, -0.024986883625388145, 0.0, 0.0, -0.024986883625388145, 0.024986883625388145, -0.024986883625388145, -0.04997376725077629, 0.024986883625388145, 0.0, -0.024986883625388145, 0.024986883625388145, 0.04997376725077629, 0.0, -0.09994753450155258, 0.07496064901351929, -0.04997376725077629, -0.024986883625388145, -0.024986883625388145, -0.024986883625388145, -0.04997376725077629, 0.07496064901351929]
[2025-05-22 00:16:40,686]: Mean: -0.00241911
[2025-05-22 00:16:40,686]: Min: -0.17490819
[2025-05-22 00:16:40,686]: Max: 0.19989507
[2025-05-22 00:16:40,686]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,686]: Sample Values (25 elements): [0.8932826519012451, 0.9901052713394165, 0.8506686091423035, 0.9156589508056641, 0.9006919860839844, 0.8719295263290405, 0.8842891454696655, 0.8896259665489197, 0.847576916217804, 0.9684203863143921, 0.8970217108726501, 0.8941261768341064, 0.9242717623710632, 0.9685361385345459, 0.898184597492218, 0.9149754643440247, 0.8117423057556152, 0.9388435482978821, 0.9205574989318848, 0.8882232308387756, 0.9098023772239685, 0.9511570334434509, 0.9673924446105957, 0.8084689974784851, 0.896885871887207]
[2025-05-22 00:16:40,687]: Mean: 0.91888165
[2025-05-22 00:16:40,687]: Min: 0.80846900
[2025-05-22 00:16:40,687]: Max: 1.01485384
[2025-05-22 00:16:40,688]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 00:16:40,688]: Sample Values (25 elements): [0.0, 0.018986644223332405, -0.056959934532642365, 0.018986644223332405, 0.018986644223332405, 0.03797328844666481, -0.03797328844666481, 0.0, -0.018986644223332405, -0.018986644223332405, 0.018986644223332405, -0.018986644223332405, -0.03797328844666481, 0.0, -0.018986644223332405, -0.03797328844666481, 0.03797328844666481, -0.056959934532642365, 0.03797328844666481, 0.018986644223332405, 0.03797328844666481, 0.018986644223332405, 0.018986644223332405, 0.03797328844666481, -0.03797328844666481]
[2025-05-22 00:16:40,689]: Mean: 0.00033117
[2025-05-22 00:16:40,689]: Min: -0.13290651
[2025-05-22 00:16:40,689]: Max: 0.15189315
[2025-05-22 00:16:40,689]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,689]: Sample Values (25 elements): [0.9709137678146362, 0.9360015988349915, 0.9408254623413086, 0.9759750366210938, 0.9548749923706055, 0.95789635181427, 0.9925774931907654, 0.9329079985618591, 1.0034006834030151, 0.984241783618927, 1.0387077331542969, 0.9718353152275085, 0.9945932626724243, 0.9946305751800537, 0.9632238149642944, 0.937950074672699, 0.9675394892692566, 0.9993988275527954, 0.9722620844841003, 0.9592654705047607, 1.0245157480239868, 0.9911597371101379, 1.029750108718872, 0.9908328652381897, 0.9462477564811707]
[2025-05-22 00:16:40,689]: Mean: 0.97668576
[2025-05-22 00:16:40,689]: Min: 0.90371054
[2025-05-22 00:16:40,690]: Max: 1.06609535
[2025-05-22 00:16:40,691]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 00:16:40,691]: Sample Values (25 elements): [0.05136076360940933, 0.03424051031470299, 0.03424051031470299, -0.03424051031470299, 0.06848102062940598, -0.03424051031470299, -0.017120255157351494, 0.03424051031470299, 0.0, 0.03424051031470299, -0.017120255157351494, 0.017120255157351494, 0.017120255157351494, 0.017120255157351494, 0.0, 0.06848102062940598, 0.03424051031470299, 0.03424051031470299, 0.017120255157351494, 0.05136076360940933, -0.06848102062940598, 0.017120255157351494, 0.03424051031470299, -0.03424051031470299, 0.017120255157351494]
[2025-05-22 00:16:40,691]: Mean: -0.00098642
[2025-05-22 00:16:40,691]: Min: -0.11984178
[2025-05-22 00:16:40,692]: Max: 0.13696204
[2025-05-22 00:16:40,692]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,692]: Sample Values (25 elements): [0.9156583547592163, 0.9089940786361694, 0.8931652903556824, 0.9231311678886414, 0.8791987299919128, 0.9231002926826477, 0.9206950664520264, 0.9248238205909729, 0.8955857157707214, 0.8996806144714355, 0.9445483684539795, 0.9361246228218079, 0.9114114046096802, 0.914116621017456, 0.9162444472312927, 0.9116383790969849, 0.8780683279037476, 0.9121474027633667, 0.9393951892852783, 0.8562062382698059, 0.9139136075973511, 0.868841826915741, 0.8845975995063782, 0.9169417023658752, 0.9260110259056091]
[2025-05-22 00:16:40,692]: Mean: 0.90955490
[2025-05-22 00:16:40,692]: Min: 0.85620624
[2025-05-22 00:16:40,692]: Max: 0.96111244
[2025-05-22 00:16:40,693]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-22 00:16:40,694]: Sample Values (25 elements): [0.0, 0.01259614061564207, 0.02519228123128414, 0.05038456246256828, 0.0, -0.03778842091560364, 0.01259614061564207, -0.02519228123128414, 0.02519228123128414, -0.01259614061564207, 0.01259614061564207, -0.01259614061564207, 0.0, 0.0, 0.0, 0.02519228123128414, -0.02519228123128414, 0.0, 0.02519228123128414, 0.02519228123128414, 0.0, -0.05038456246256828, 0.02519228123128414, -0.03778842091560364, -0.01259614061564207]
[2025-05-22 00:16:40,694]: Mean: 0.00066254
[2025-05-22 00:16:40,694]: Min: -0.08817299
[2025-05-22 00:16:40,694]: Max: 0.10076912
[2025-05-22 00:16:40,694]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-22 00:16:40,695]: Sample Values (25 elements): [1.066990852355957, 0.9999803304672241, 0.979091465473175, 1.0762078762054443, 1.0454446077346802, 1.0348029136657715, 1.0507816076278687, 1.0348328351974487, 1.0212186574935913, 1.0652306079864502, 1.110257863998413, 1.0294771194458008, 1.0700455904006958, 1.0281492471694946, 1.0011595487594604, 1.0272454023361206, 1.0649378299713135, 1.0862886905670166, 1.0302972793579102, 1.0325591564178467, 1.0210299491882324, 1.057205080986023, 1.048506259918213, 1.0322788953781128, 1.0508759021759033]
[2025-05-22 00:16:40,695]: Mean: 1.03808618
[2025-05-22 00:16:40,695]: Min: 0.97909147
[2025-05-22 00:16:40,695]: Max: 1.11025786
[2025-05-22 00:16:40,695]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-22 00:16:40,695]: Sample Values (25 elements): [0.15531602501869202, 0.6844966411590576, -0.230340838432312, 0.34882327914237976, -0.2415216863155365, -0.06000368669629097, -0.3450472950935364, -0.21407125890254974, -0.08792552351951599, -0.3593883514404297, 0.24987132847309113, -0.22323191165924072, -0.38082355260849, 0.040189921855926514, -0.12114106118679047, 0.27739217877388, -0.21125733852386475, -0.14999009668827057, -0.12720085680484772, 0.25403648614883423, -0.31385064125061035, 0.44406941533088684, 0.17847277224063873, -0.09193645417690277, -0.0003656826738733798]
[2025-05-22 00:16:40,696]: Mean: -0.00341582
[2025-05-22 00:16:40,696]: Min: -0.46272367
[2025-05-22 00:16:40,696]: Max: 0.68449664
