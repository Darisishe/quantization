[2025-05-05 07:11:41,212]: 
Training LeNet5 with relu
[2025-05-05 07:12:14,389]: [LeNet5_relu] Epoch: 001 Train Loss: 2.2995 Train Acc: 0.1039 Eval Loss: 2.2940 Eval Acc: 0.1273 (LR: 0.001000)
[2025-05-05 07:12:14,408]: [LeNet5_relu] Best Eval Accuracy: 0.1273
[2025-05-05 07:12:14,411]: 
Training of full-precision model finished!
[2025-05-05 07:12:14,411]: Model Architecture:
[2025-05-05 07:12:14,411]: LeNet5(
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): ReLU(inplace=True)
  )
  (fc2): Sequential(
    (0): Linear(in_features=120, out_features=84, bias=True)
    (1): ReLU(inplace=True)
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:12:14,411]: 
Model Parameters with Weights:
[2025-05-05 07:12:14,411]: 
Parameter: conv1.0.weight
[2025-05-05 07:12:14,411]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-05 07:12:14,433]: Sample Values (16 elements): [0.06947404891252518, -0.02629719115793705, -0.07548290491104126, 0.026297738775610924, 0.035239703953266144, 0.04373392090201378, -0.000490376609377563, 0.10057307034730911, -0.029403936117887497, 0.033644478768110275, 0.10771737992763519, -0.06292934715747833, -0.0314582884311676, 0.10473185032606125, 0.060349322855472565, -0.08346466720104218]
[2025-05-05 07:12:14,445]: Mean: 0.0008
[2025-05-05 07:12:14,457]: Min: -0.1133
[2025-05-05 07:12:14,458]: Max: 0.1132
[2025-05-05 07:12:14,458]: 
Parameter: conv1.0.bias
[2025-05-05 07:12:14,458]: Shape: torch.Size([6])
[2025-05-05 07:12:14,459]: Sample Values (6 elements): [-0.09233514219522476, 0.034118060022592545, -0.08082395792007446, 0.07548855245113373, -0.10102088749408722, 0.03585809841752052]
[2025-05-05 07:12:14,459]: Mean: -0.0215
[2025-05-05 07:12:14,460]: Min: -0.1010
[2025-05-05 07:12:14,460]: Max: 0.0755
[2025-05-05 07:12:14,460]: 
Parameter: conv2.0.weight
[2025-05-05 07:12:14,460]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-05 07:12:14,461]: Sample Values (16 elements): [-0.01880386844277382, 0.0012152832932770252, 0.07341255992650986, -0.055870797485113144, -0.019383493810892105, -0.05774068832397461, -0.027713611721992493, -0.0214917603880167, -0.021059582009911537, 0.0589807853102684, 0.034715063869953156, 0.05300593003630638, -0.009175390005111694, -0.030880959704518318, -0.0482243113219738, -0.04618966206908226]
[2025-05-05 07:12:14,461]: Mean: -0.0051
[2025-05-05 07:12:14,461]: Min: -0.0812
[2025-05-05 07:12:14,461]: Max: 0.0795
[2025-05-05 07:12:14,461]: 
Parameter: conv2.0.bias
[2025-05-05 07:12:14,461]: Shape: torch.Size([16])
[2025-05-05 07:12:14,462]: Sample Values (16 elements): [0.008912303484976292, 0.0006635385216213763, -0.007578214164823294, 0.03423852100968361, -0.05562780052423477, -0.006845213007181883, 0.06735050678253174, -0.011944535188376904, -0.029908286407589912, -0.027664827182888985, 0.0610063336789608, 0.02971441112458706, 0.03448915854096413, -0.029160229489207268, -0.07016687840223312, -0.05414628982543945]
[2025-05-05 07:12:14,462]: Mean: -0.0035
[2025-05-05 07:12:14,462]: Min: -0.0702
[2025-05-05 07:12:14,463]: Max: 0.0674
[2025-05-05 07:12:14,463]: 
Parameter: fc1.0.weight
[2025-05-05 07:12:14,463]: Shape: torch.Size([120, 400])
[2025-05-05 07:12:14,463]: Sample Values (16 elements): [0.023515479639172554, -0.03766195476055145, -0.021305739879608154, 0.011399375274777412, -0.04976799711585045, -0.03323942422866821, 0.03326733782887459, -0.030840106308460236, -0.04934864863753319, -0.01671622507274151, 0.019335057586431503, -0.03186002001166344, 0.027207359671592712, 0.017618682235479355, -0.013282503932714462, -0.009500395506620407]
[2025-05-05 07:12:14,464]: Mean: -0.0020
[2025-05-05 07:12:14,464]: Min: -0.0499
[2025-05-05 07:12:14,464]: Max: 0.0498
[2025-05-05 07:12:14,464]: 
Parameter: fc1.0.bias
[2025-05-05 07:12:14,464]: Shape: torch.Size([120])
[2025-05-05 07:12:14,465]: Sample Values (16 elements): [-0.016202639788389206, 0.02859973907470703, 0.041777078062295914, -0.023976348340511322, 0.014314022846519947, 0.040280427783727646, 0.03552486374974251, -0.03518039733171463, -0.04908169433474541, -0.01202334463596344, 0.017446007579565048, 0.022517438977956772, 0.004272692371159792, -0.01733561046421528, -0.043229978531599045, 0.014146402478218079]
[2025-05-05 07:12:14,465]: Mean: -0.0011
[2025-05-05 07:12:14,465]: Min: -0.0496
[2025-05-05 07:12:14,466]: Max: 0.0504
[2025-05-05 07:12:14,466]: 
Parameter: fc2.0.weight
[2025-05-05 07:12:14,466]: Shape: torch.Size([84, 120])
[2025-05-05 07:12:14,466]: Sample Values (16 elements): [0.031763698905706406, 0.06782741099596024, -0.020064769312739372, 0.06861472874879837, 0.06730508804321289, 0.04615067318081856, 0.0021426209714263678, 0.08567450195550919, 0.035130567848682404, -0.08078651875257492, 0.05773133412003517, -0.049597419798374176, -0.03136624023318291, 0.060275476425886154, 0.08948535472154617, -0.08994432538747787]
[2025-05-05 07:12:14,466]: Mean: 0.0056
[2025-05-05 07:12:14,466]: Min: -0.0902
[2025-05-05 07:12:14,467]: Max: 0.0895
[2025-05-05 07:12:14,467]: 
Parameter: fc2.0.bias
[2025-05-05 07:12:14,467]: Shape: torch.Size([84])
[2025-05-05 07:12:14,467]: Sample Values (16 elements): [-0.05155060440301895, -0.014713644050061703, 0.08121947199106216, 0.02609221078455448, -0.008398439735174179, -0.012229803018271923, -0.007104918826371431, -0.053741056472063065, 0.04036284610629082, -0.04349846765398979, 0.05535394698381424, -0.047956936061382294, 0.034820541739463806, 0.09058216959238052, 0.048613399267196655, 0.02571137808263302]
[2025-05-05 07:12:14,468]: Mean: -0.0026
[2025-05-05 07:12:14,468]: Min: -0.0906
[2025-05-05 07:12:14,468]: Max: 0.0949
[2025-05-05 07:12:14,468]: 
Parameter: fc3.weight
[2025-05-05 07:12:14,468]: Shape: torch.Size([10, 84])
[2025-05-05 07:12:14,469]: Sample Values (16 elements): [0.011958523653447628, -0.00037760924897156656, 0.09542782604694366, 0.06190257892012596, 0.043705739080905914, -0.09594236314296722, -0.035841573029756546, 0.08506208658218384, 0.08924807608127594, -0.06265229731798172, -0.09709769487380981, 0.06555657088756561, -0.10236934572458267, -0.03999911993741989, -0.0034649737644940615, -0.04335127770900726]
[2025-05-05 07:12:14,469]: Mean: 0.0057
[2025-05-05 07:12:14,469]: Min: -0.1083
[2025-05-05 07:12:14,469]: Max: 0.1065
[2025-05-05 07:12:14,469]: 
Parameter: fc3.bias
[2025-05-05 07:12:14,469]: Shape: torch.Size([10])
[2025-05-05 07:12:14,470]: Sample Values (10 elements): [-0.07537603378295898, 0.056909941136837006, -0.06720434129238129, -0.07532650232315063, 0.0698060393333435, 0.03726081922650337, 0.040663398802280426, -0.06896936893463135, 0.02008846029639244, -0.023547878488898277]
[2025-05-05 07:12:14,470]: Mean: -0.0086
[2025-05-05 07:12:14,470]: Min: -0.0754
[2025-05-05 07:12:14,470]: Max: 0.0698
[2025-05-05 07:12:14,470]: 


QAT of LeNet5 with relu down to 4 bits...
[2025-05-05 07:12:14,513]: [LeNet5_relu_quantized_4_bits] after configure_qat:
[2025-05-05 07:12:14,585]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:12:46,093]: [LeNet5_relu_quantized_4_bits] Epoch: 001 Train Loss: 2.2863 Train Acc: 0.1575 Eval Loss: 2.2693 Eval Acc: 0.1753 (LR: 0.001000)
[2025-05-05 07:12:46,104]: [LeNet5_relu_quantized_4_bits] Best Eval Accuracy: 0.1753
[2025-05-05 07:12:46,114]: 


Quantization of model down to 4 bits finished
[2025-05-05 07:12:46,114]: Model Architecture:
[2025-05-05 07:12:46,247]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2734], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.100526332855225)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0108, 0.0113, 0.0110, 0.0107, 0.0113, 0.0110, 0.0109, 0.0109, 0.0107,
                0.0109, 0.0108, 0.0109, 0.0110, 0.0109, 0.0107, 0.0108],
               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0813, -0.0798, -0.0793, -0.0806, -0.0830, -0.0796, -0.0818, -0.0811,
                  -0.0802, -0.0817, -0.0809, -0.0818, -0.0799, -0.0789, -0.0793, -0.0812],
                 device='cuda:0'), max_val=tensor([0.0794, 0.0844, 0.0825, 0.0804, 0.0848, 0.0824, 0.0805, 0.0821, 0.0806,
                  0.0821, 0.0784, 0.0815, 0.0825, 0.0817, 0.0805, 0.0810],
                 device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1502], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.253357410430908)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0067, 0.0068, 0.0066, 0.0067, 0.0067, 0.0066, 0.0066, 0.0068, 0.0067,
                0.0067, 0.0067, 0.0067, 0.0067, 0.0066, 0.0067, 0.0067, 0.0067, 0.0067,
                0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0070, 0.0066, 0.0067, 0.0067,
                0.0067, 0.0067, 0.0068, 0.0068, 0.0067, 0.0068, 0.0066, 0.0067, 0.0067,
                0.0067, 0.0067, 0.0067, 0.0066, 0.0068, 0.0066, 0.0066, 0.0067, 0.0067,
                0.0067, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0066, 0.0067, 0.0067,
                0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0066, 0.0067, 0.0069, 0.0067,
                0.0066, 0.0069, 0.0067, 0.0067, 0.0067, 0.0068, 0.0067, 0.0067, 0.0067,
                0.0067, 0.0066, 0.0067, 0.0067, 0.0067, 0.0067, 0.0068, 0.0066, 0.0067,
                0.0069, 0.0067, 0.0066, 0.0068, 0.0067, 0.0067, 0.0067, 0.0067, 0.0066,
                0.0067, 0.0067, 0.0067, 0.0066, 0.0067, 0.0068, 0.0067, 0.0067, 0.0067,
                0.0067, 0.0067, 0.0066, 0.0068, 0.0066, 0.0067, 0.0067, 0.0066, 0.0067,
                0.0067, 0.0066, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067,
                0.0067, 0.0065, 0.0068], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0499, -0.0492, -0.0498, -0.0504, -0.0497, -0.0481, -0.0497, -0.0510,
                  -0.0503, -0.0496, -0.0496, -0.0499, -0.0497, -0.0495, -0.0498, -0.0496,
                  -0.0500, -0.0501, -0.0500, -0.0494, -0.0497, -0.0498, -0.0499, -0.0524,
                  -0.0496, -0.0489, -0.0500, -0.0486, -0.0506, -0.0494, -0.0496, -0.0498,
                  -0.0493, -0.0497, -0.0502, -0.0501, -0.0502, -0.0495, -0.0502, -0.0494,
                  -0.0509, -0.0497, -0.0492, -0.0496, -0.0499, -0.0492, -0.0496, -0.0497,
                  -0.0492, -0.0499, -0.0499, -0.0498, -0.0506, -0.0496, -0.0501, -0.0501,
                  -0.0500, -0.0500, -0.0495, -0.0496, -0.0499, -0.0496, -0.0501, -0.0498,
                  -0.0502, -0.0500, -0.0504, -0.0506, -0.0497, -0.0501, -0.0502, -0.0502,
                  -0.0498, -0.0488, -0.0501, -0.0488, -0.0501, -0.0499, -0.0508, -0.0494,
                  -0.0506, -0.0475, -0.0500, -0.0493, -0.0506, -0.0495, -0.0499, -0.0489,
                  -0.0500, -0.0498, -0.0499, -0.0473, -0.0492, -0.0496, -0.0503, -0.0513,
                  -0.0492, -0.0487, -0.0499, -0.0498, -0.0503, -0.0495, -0.0499, -0.0498,
                  -0.0504, -0.0485, -0.0497, -0.0491, -0.0497, -0.0495, -0.0493, -0.0500,
                  -0.0499, -0.0504, -0.0502, -0.0501, -0.0500, -0.0503, -0.0489, -0.0495],
                 device='cuda:0'), max_val=tensor([0.0498, 0.0510, 0.0496, 0.0496, 0.0499, 0.0498, 0.0497, 0.0501, 0.0503,
                  0.0502, 0.0500, 0.0499, 0.0500, 0.0496, 0.0499, 0.0504, 0.0500, 0.0494,
                  0.0497, 0.0502, 0.0500, 0.0503, 0.0499, 0.0498, 0.0496, 0.0500, 0.0500,
                  0.0500, 0.0495, 0.0510, 0.0508, 0.0500, 0.0508, 0.0491, 0.0499, 0.0502,
                  0.0498, 0.0502, 0.0498, 0.0498, 0.0495, 0.0494, 0.0490, 0.0500, 0.0500,
                  0.0506, 0.0499, 0.0497, 0.0505, 0.0499, 0.0499, 0.0497, 0.0500, 0.0500,
                  0.0496, 0.0500, 0.0499, 0.0498, 0.0500, 0.0496, 0.0499, 0.0514, 0.0500,
                  0.0497, 0.0515, 0.0495, 0.0503, 0.0495, 0.0510, 0.0502, 0.0498, 0.0498,
                  0.0501, 0.0494, 0.0496, 0.0504, 0.0500, 0.0498, 0.0508, 0.0497, 0.0498,
                  0.0515, 0.0498, 0.0494, 0.0504, 0.0500, 0.0499, 0.0503, 0.0499, 0.0495,
                  0.0502, 0.0502, 0.0500, 0.0497, 0.0502, 0.0496, 0.0499, 0.0501, 0.0491,
                  0.0501, 0.0496, 0.0494, 0.0507, 0.0497, 0.0495, 0.0503, 0.0491, 0.0505,
                  0.0501, 0.0498, 0.0499, 0.0498, 0.0499, 0.0501, 0.0499, 0.0495, 0.0499,
                  0.0499, 0.0485, 0.0507], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0912], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.3683947324752808)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0121, 0.0121, 0.0119, 0.0122, 0.0121, 0.0121, 0.0122, 0.0121, 0.0120,
                0.0121, 0.0122, 0.0122, 0.0121, 0.0122, 0.0120, 0.0122, 0.0119, 0.0121,
                0.0120, 0.0121, 0.0122, 0.0121, 0.0121, 0.0121, 0.0121, 0.0119, 0.0120,
                0.0121, 0.0121, 0.0120, 0.0120, 0.0121, 0.0121, 0.0122, 0.0121, 0.0119,
                0.0120, 0.0121, 0.0121, 0.0119, 0.0117, 0.0121, 0.0123, 0.0121, 0.0122,
                0.0120, 0.0122, 0.0121, 0.0121, 0.0121, 0.0123, 0.0120, 0.0119, 0.0120,
                0.0119, 0.0122, 0.0122, 0.0120, 0.0121, 0.0121, 0.0119, 0.0121, 0.0127,
                0.0121, 0.0120, 0.0120, 0.0119, 0.0121, 0.0121, 0.0121, 0.0122, 0.0119,
                0.0122, 0.0121, 0.0122, 0.0121, 0.0121, 0.0119, 0.0120, 0.0122, 0.0118,
                0.0121, 0.0121, 0.0120], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0905, -0.0909, -0.0894, -0.0914, -0.0901, -0.0905, -0.0902, -0.0906,
                  -0.0895, -0.0903, -0.0916, -0.0909, -0.0906, -0.0913, -0.0902, -0.0887,
                  -0.0882, -0.0910, -0.0877, -0.0905, -0.0912, -0.0908, -0.0905, -0.0906,
                  -0.0883, -0.0883, -0.0857, -0.0910, -0.0878, -0.0875, -0.0876, -0.0892,
                  -0.0904, -0.0900, -0.0908, -0.0896, -0.0899, -0.0907, -0.0902, -0.0890,
                  -0.0880, -0.0907, -0.0878, -0.0906, -0.0879, -0.0878, -0.0912, -0.0907,
                  -0.0908, -0.0892, -0.0872, -0.0899, -0.0896, -0.0897, -0.0892, -0.0919,
                  -0.0883, -0.0882, -0.0911, -0.0874, -0.0889, -0.0909, -0.0935, -0.0910,
                  -0.0902, -0.0903, -0.0858, -0.0910, -0.0890, -0.0910, -0.0911, -0.0885,
                  -0.0913, -0.0902, -0.0911, -0.0905, -0.0901, -0.0891, -0.0903, -0.0890,
                  -0.0888, -0.0889, -0.0910, -0.0900], device='cuda:0'), max_val=tensor([0.0895, 0.0891, 0.0892, 0.0895, 0.0906, 0.0905, 0.0912, 0.0861, 0.0902,
                  0.0909, 0.0887, 0.0912, 0.0898, 0.0912, 0.0887, 0.0912, 0.0893, 0.0882,
                  0.0903, 0.0893, 0.0903, 0.0884, 0.0896, 0.0911, 0.0911, 0.0896, 0.0896,
                  0.0906, 0.0907, 0.0897, 0.0901, 0.0909, 0.0911, 0.0913, 0.0819, 0.0849,
                  0.0889, 0.0906, 0.0905, 0.0893, 0.0875, 0.0889, 0.0922, 0.0859, 0.0912,
                  0.0902, 0.0910, 0.0903, 0.0897, 0.0908, 0.0923, 0.0886, 0.0867, 0.0871,
                  0.0880, 0.0885, 0.0912, 0.0901, 0.0911, 0.0904, 0.0875, 0.0855, 0.0950,
                  0.0905, 0.0902, 0.0865, 0.0896, 0.0883, 0.0911, 0.0911, 0.0904, 0.0895,
                  0.0895, 0.0905, 0.0912, 0.0901, 0.0905, 0.0889, 0.0867, 0.0915, 0.0881,
                  0.0907, 0.0894, 0.0893], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0455], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=0.6825346350669861)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:12:46,247]: 
Model Parameters with Weights:
[2025-05-05 07:12:46,247]: 
Parameter: conv1.0.weight
[2025-05-05 07:12:46,247]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-05 07:12:46,252]: Sample Values (16 elements): [0.03879057988524437, 0.105770543217659, -0.05186229199171066, 0.06320443749427795, 0.07010140269994736, -0.009514777921140194, -0.05249828100204468, 0.040367648005485535, 0.060754772275686264, -0.09314767271280289, -0.008561588823795319, -0.08294897526502609, 0.04801299050450325, 0.018307527527213097, 0.03822378069162369, -0.053833700716495514]
[2025-05-05 07:12:46,252]: Mean: 0.0032
[2025-05-05 07:12:46,252]: Min: -0.1101
[2025-05-05 07:12:46,253]: Max: 0.1207
[2025-05-05 07:12:46,253]: 
Parameter: conv1.0.bias
[2025-05-05 07:12:46,253]: Shape: torch.Size([6])
[2025-05-05 07:12:46,253]: Sample Values (6 elements): [0.03679677098989487, -0.07648498564958572, 0.07704584300518036, 0.03914431482553482, -0.1019320860505104, -0.08788163959980011]
[2025-05-05 07:12:46,253]: Mean: -0.0189
[2025-05-05 07:12:46,254]: Min: -0.1019
[2025-05-05 07:12:46,254]: Max: 0.0770
[2025-05-05 07:12:46,255]: 
Parameter: conv2.0.weight
[2025-05-05 07:12:46,255]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-05 07:12:46,255]: Sample Values (16 elements): [0.05306055396795273, -0.05777805298566818, -0.07953400909900665, -0.03232590854167938, 0.06178324669599533, 0.0520191565155983, -0.05619977042078972, 0.038093362003564835, 0.07256856560707092, 0.015960751101374626, -0.011535546742379665, 0.020391441881656647, -0.058370694518089294, -0.04702365770936012, -0.06848713755607605, -0.07887198776006699]
[2025-05-05 07:12:46,255]: Mean: -0.0052
[2025-05-05 07:12:46,255]: Min: -0.0814
[2025-05-05 07:12:46,256]: Max: 0.0794
[2025-05-05 07:12:46,256]: 
Parameter: conv2.0.bias
[2025-05-05 07:12:46,256]: Shape: torch.Size([16])
[2025-05-05 07:12:46,256]: Sample Values (16 elements): [-0.0003032588865607977, 0.06332296133041382, 0.06662559509277344, 0.03741700202226639, -0.02970835752785206, -0.007884378544986248, -0.05583838000893593, -0.05429097265005112, 0.03283090144395828, 0.013208198361098766, -0.031724076718091965, -0.007125514093786478, -0.013800162822008133, -0.025540437549352646, 0.033720772713422775, -0.07039007544517517]
[2025-05-05 07:12:46,256]: Mean: -0.0031
[2025-05-05 07:12:46,257]: Min: -0.0704
[2025-05-05 07:12:46,257]: Max: 0.0666
[2025-05-05 07:12:46,257]: 
Parameter: fc1.0.weight
[2025-05-05 07:12:46,257]: Shape: torch.Size([120, 400])
[2025-05-05 07:12:46,257]: Sample Values (16 elements): [0.01459907740354538, -0.006286753807216883, -0.012110481970012188, -0.03133115544915199, -0.032811105251312256, -0.015282181091606617, -0.03490469977259636, 0.004073319956660271, -0.0450974777340889, -0.027276240289211273, -0.03629346564412117, 0.04222949966788292, -0.02808786928653717, -0.0094955675303936, 0.03937861695885658, -0.01937176287174225]
[2025-05-05 07:12:46,257]: Mean: -0.0020
[2025-05-05 07:12:46,258]: Min: -0.0499
[2025-05-05 07:12:46,258]: Max: 0.0498
[2025-05-05 07:12:46,258]: 
Parameter: fc1.0.bias
[2025-05-05 07:12:46,258]: Shape: torch.Size([120])
[2025-05-05 07:12:46,260]: Sample Values (16 elements): [0.01446892973035574, 0.005656014196574688, -0.0013503398513421416, 0.006530653685331345, -0.03961499035358429, -0.02221602573990822, 0.017700662836432457, -0.02193092741072178, -0.016612479463219643, -0.04352910816669464, 0.030198778957128525, 0.007847622036933899, -0.01592506282031536, -0.032924309372901917, 0.0002972533693537116, -0.039262525737285614]
[2025-05-05 07:12:46,260]: Mean: -0.0010
[2025-05-05 07:12:46,260]: Min: -0.0496
[2025-05-05 07:12:46,260]: Max: 0.0503
[2025-05-05 07:12:46,260]: 
Parameter: fc2.0.weight
[2025-05-05 07:12:46,260]: Shape: torch.Size([84, 120])
[2025-05-05 07:12:46,261]: Sample Values (16 elements): [-0.031259335577487946, 0.0632874071598053, -0.08082793653011322, -0.045005206018686295, 0.059393856674432755, 0.06677283346652985, 0.07837066799402237, 0.06417537480592728, -0.006201668176800013, -0.03311570733785629, -0.06792094558477402, -0.02066439390182495, -0.006351114250719547, 0.058764971792697906, 0.07253995537757874, -0.057385243475437164]
[2025-05-05 07:12:46,261]: Mean: 0.0052
[2025-05-05 07:12:46,261]: Min: -0.0909
[2025-05-05 07:12:46,262]: Max: 0.0895
[2025-05-05 07:12:46,262]: 
Parameter: fc2.0.bias
[2025-05-05 07:12:46,262]: Shape: torch.Size([84])
[2025-05-05 07:12:46,262]: Sample Values (16 elements): [-0.044908612966537476, 0.10157208889722824, 0.032165106385946274, -0.013640795834362507, 0.03173435106873512, 0.023962877690792084, -0.021108653396368027, -0.008889148011803627, -0.0035547760780900717, -0.012703646905720234, 0.022780485451221466, 0.06193740665912628, -0.08104541152715683, 0.039860259741544724, 0.049758415669202805, -0.024830462411046028]
[2025-05-05 07:12:46,262]: Mean: -0.0021
[2025-05-05 07:12:46,262]: Min: -0.0912
[2025-05-05 07:12:46,263]: Max: 0.1016
[2025-05-05 07:12:46,263]: 
Parameter: fc3.weight
[2025-05-05 07:12:46,263]: Shape: torch.Size([10, 84])
[2025-05-05 07:12:46,263]: Sample Values (16 elements): [0.04056257754564285, 0.059163376688957214, 0.0954095721244812, 0.06689154356718063, 0.07445874810218811, 0.10490821301937103, 0.09096726030111313, 0.10368061810731888, -0.03073936328291893, 0.015104062855243683, -0.040608420968055725, 0.08455683290958405, 0.08107210695743561, -0.02676551043987274, -0.01886088028550148, -0.008838036097586155]
[2025-05-05 07:12:46,263]: Mean: 0.0067
[2025-05-05 07:12:46,263]: Min: -0.1119
[2025-05-05 07:12:46,265]: Max: 0.1065
[2025-05-05 07:12:46,266]: 
Parameter: fc3.bias
[2025-05-05 07:12:46,266]: Shape: torch.Size([10])
[2025-05-05 07:12:46,266]: Sample Values (10 elements): [-0.08541972190141678, 0.03904558718204498, -0.059445835649967194, -0.060961611568927765, 0.04851873219013214, 0.04982587322592735, -0.009081616997718811, 0.06491293758153915, 0.010571036487817764, -0.08364482969045639]
[2025-05-05 07:12:46,266]: Mean: -0.0086
[2025-05-05 07:12:46,266]: Min: -0.0854
[2025-05-05 07:12:46,267]: Max: 0.0649
[2025-05-05 07:12:46,267]: 


QAT of LeNet5 with relu down to 3 bits...
[2025-05-05 07:12:46,294]: [LeNet5_relu_quantized_3_bits] after configure_qat:
[2025-05-05 07:12:46,312]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:13:18,204]: [LeNet5_relu_quantized_3_bits] Epoch: 001 Train Loss: 2.2885 Train Acc: 0.1455 Eval Loss: 2.2731 Eval Acc: 0.1652 (LR: 0.001000)
[2025-05-05 07:13:18,215]: [LeNet5_relu_quantized_3_bits] Best Eval Accuracy: 0.1652
[2025-05-05 07:13:18,224]: 


Quantization of model down to 3 bits finished
[2025-05-05 07:13:18,224]: Model Architecture:
[2025-05-05 07:13:18,314]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5772], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.040599822998047)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0232, 0.0241, 0.0236, 0.0230, 0.0242, 0.0235, 0.0234, 0.0234, 0.0230,
                0.0235, 0.0231, 0.0233, 0.0236, 0.0233, 0.0230, 0.0232],
               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0813, -0.0795, -0.0795, -0.0805, -0.0823, -0.0795, -0.0818, -0.0812,
                  -0.0802, -0.0817, -0.0809, -0.0817, -0.0800, -0.0789, -0.0792, -0.0811],
                 device='cuda:0'), max_val=tensor([0.0794, 0.0844, 0.0825, 0.0804, 0.0848, 0.0823, 0.0805, 0.0820, 0.0806,
                  0.0821, 0.0785, 0.0814, 0.0825, 0.0817, 0.0805, 0.0810],
                 device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3135], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.194241523742676)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0143, 0.0146, 0.0142, 0.0144, 0.0143, 0.0142, 0.0142, 0.0145, 0.0144,
                0.0144, 0.0143, 0.0143, 0.0143, 0.0142, 0.0143, 0.0144, 0.0144, 0.0143,
                0.0143, 0.0143, 0.0143, 0.0144, 0.0142, 0.0150, 0.0142, 0.0143, 0.0143,
                0.0143, 0.0144, 0.0146, 0.0145, 0.0143, 0.0145, 0.0142, 0.0143, 0.0144,
                0.0143, 0.0143, 0.0144, 0.0142, 0.0145, 0.0142, 0.0140, 0.0143, 0.0143,
                0.0145, 0.0142, 0.0142, 0.0144, 0.0143, 0.0143, 0.0142, 0.0144, 0.0143,
                0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0142, 0.0143, 0.0147, 0.0143,
                0.0142, 0.0147, 0.0143, 0.0144, 0.0144, 0.0146, 0.0143, 0.0144, 0.0144,
                0.0143, 0.0141, 0.0143, 0.0144, 0.0143, 0.0142, 0.0145, 0.0142, 0.0144,
                0.0147, 0.0143, 0.0141, 0.0145, 0.0143, 0.0143, 0.0144, 0.0143, 0.0142,
                0.0143, 0.0144, 0.0143, 0.0142, 0.0144, 0.0147, 0.0143, 0.0143, 0.0143,
                0.0143, 0.0144, 0.0141, 0.0145, 0.0142, 0.0144, 0.0144, 0.0142, 0.0144,
                0.0143, 0.0142, 0.0143, 0.0143, 0.0143, 0.0144, 0.0143, 0.0143, 0.0143,
                0.0144, 0.0140, 0.0145], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0499, -0.0492, -0.0496, -0.0503, -0.0497, -0.0481, -0.0497, -0.0508,
                  -0.0500, -0.0496, -0.0496, -0.0497, -0.0497, -0.0496, -0.0498, -0.0496,
                  -0.0503, -0.0501, -0.0500, -0.0493, -0.0497, -0.0499, -0.0498, -0.0526,
                  -0.0496, -0.0488, -0.0500, -0.0486, -0.0505, -0.0495, -0.0496, -0.0497,
                  -0.0495, -0.0497, -0.0501, -0.0504, -0.0502, -0.0495, -0.0503, -0.0494,
                  -0.0508, -0.0497, -0.0492, -0.0497, -0.0499, -0.0491, -0.0496, -0.0497,
                  -0.0493, -0.0499, -0.0499, -0.0497, -0.0505, -0.0496, -0.0500, -0.0500,
                  -0.0499, -0.0500, -0.0494, -0.0496, -0.0499, -0.0497, -0.0500, -0.0498,
                  -0.0501, -0.0500, -0.0502, -0.0505, -0.0497, -0.0501, -0.0503, -0.0504,
                  -0.0498, -0.0488, -0.0502, -0.0488, -0.0501, -0.0497, -0.0508, -0.0494,
                  -0.0504, -0.0475, -0.0500, -0.0492, -0.0508, -0.0497, -0.0499, -0.0488,
                  -0.0498, -0.0498, -0.0499, -0.0472, -0.0492, -0.0494, -0.0503, -0.0514,
                  -0.0492, -0.0487, -0.0499, -0.0498, -0.0503, -0.0495, -0.0499, -0.0498,
                  -0.0504, -0.0484, -0.0497, -0.0492, -0.0497, -0.0496, -0.0493, -0.0500,
                  -0.0499, -0.0504, -0.0502, -0.0502, -0.0499, -0.0503, -0.0489, -0.0495],
                 device='cuda:0'), max_val=tensor([0.0498, 0.0510, 0.0496, 0.0497, 0.0499, 0.0498, 0.0497, 0.0501, 0.0502,
                  0.0502, 0.0500, 0.0500, 0.0500, 0.0496, 0.0499, 0.0504, 0.0501, 0.0494,
                  0.0497, 0.0502, 0.0500, 0.0503, 0.0498, 0.0497, 0.0496, 0.0500, 0.0499,
                  0.0500, 0.0496, 0.0510, 0.0508, 0.0500, 0.0508, 0.0491, 0.0499, 0.0502,
                  0.0498, 0.0502, 0.0498, 0.0498, 0.0495, 0.0494, 0.0490, 0.0499, 0.0500,
                  0.0506, 0.0499, 0.0497, 0.0505, 0.0499, 0.0499, 0.0497, 0.0499, 0.0500,
                  0.0497, 0.0499, 0.0499, 0.0498, 0.0500, 0.0495, 0.0499, 0.0514, 0.0499,
                  0.0497, 0.0515, 0.0495, 0.0503, 0.0495, 0.0510, 0.0502, 0.0498, 0.0498,
                  0.0501, 0.0494, 0.0496, 0.0504, 0.0499, 0.0499, 0.0508, 0.0497, 0.0499,
                  0.0515, 0.0498, 0.0494, 0.0505, 0.0499, 0.0499, 0.0503, 0.0499, 0.0495,
                  0.0501, 0.0503, 0.0499, 0.0498, 0.0502, 0.0496, 0.0499, 0.0501, 0.0491,
                  0.0501, 0.0496, 0.0494, 0.0507, 0.0498, 0.0495, 0.0503, 0.0491, 0.0505,
                  0.0502, 0.0498, 0.0499, 0.0498, 0.0499, 0.0499, 0.0500, 0.0495, 0.0499,
                  0.0499, 0.0485, 0.0507], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1824], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.2765339612960815)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-4, quant_max=3, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0258, 0.0260, 0.0255, 0.0261, 0.0259, 0.0259, 0.0261, 0.0259, 0.0258,
                0.0260, 0.0261, 0.0260, 0.0259, 0.0261, 0.0258, 0.0260, 0.0255, 0.0260,
                0.0258, 0.0259, 0.0261, 0.0259, 0.0259, 0.0260, 0.0260, 0.0256, 0.0256,
                0.0260, 0.0259, 0.0256, 0.0257, 0.0260, 0.0260, 0.0261, 0.0259, 0.0256,
                0.0257, 0.0259, 0.0258, 0.0255, 0.0251, 0.0259, 0.0263, 0.0259, 0.0260,
                0.0258, 0.0261, 0.0259, 0.0259, 0.0259, 0.0264, 0.0257, 0.0256, 0.0256,
                0.0254, 0.0263, 0.0260, 0.0258, 0.0260, 0.0258, 0.0254, 0.0260, 0.0272,
                0.0260, 0.0258, 0.0258, 0.0256, 0.0260, 0.0260, 0.0260, 0.0260, 0.0256,
                0.0261, 0.0258, 0.0261, 0.0259, 0.0258, 0.0255, 0.0258, 0.0262, 0.0254,
                0.0259, 0.0260, 0.0257], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0904, -0.0909, -0.0894, -0.0913, -0.0901, -0.0905, -0.0902, -0.0906,
                  -0.0897, -0.0904, -0.0914, -0.0909, -0.0907, -0.0910, -0.0902, -0.0888,
                  -0.0882, -0.0909, -0.0877, -0.0905, -0.0912, -0.0907, -0.0905, -0.0907,
                  -0.0883, -0.0883, -0.0857, -0.0910, -0.0878, -0.0875, -0.0876, -0.0892,
                  -0.0904, -0.0900, -0.0908, -0.0895, -0.0899, -0.0907, -0.0902, -0.0890,
                  -0.0880, -0.0907, -0.0877, -0.0906, -0.0879, -0.0878, -0.0912, -0.0908,
                  -0.0908, -0.0892, -0.0872, -0.0899, -0.0897, -0.0896, -0.0889, -0.0920,
                  -0.0884, -0.0882, -0.0911, -0.0875, -0.0890, -0.0909, -0.0935, -0.0910,
                  -0.0902, -0.0903, -0.0858, -0.0910, -0.0890, -0.0909, -0.0911, -0.0886,
                  -0.0913, -0.0903, -0.0911, -0.0905, -0.0901, -0.0891, -0.0904, -0.0889,
                  -0.0888, -0.0888, -0.0910, -0.0899], device='cuda:0'), max_val=tensor([0.0895, 0.0890, 0.0892, 0.0895, 0.0906, 0.0905, 0.0912, 0.0861, 0.0902,
                  0.0909, 0.0886, 0.0912, 0.0895, 0.0912, 0.0887, 0.0912, 0.0893, 0.0882,
                  0.0903, 0.0893, 0.0903, 0.0884, 0.0896, 0.0911, 0.0911, 0.0896, 0.0896,
                  0.0906, 0.0907, 0.0897, 0.0901, 0.0909, 0.0910, 0.0913, 0.0819, 0.0850,
                  0.0889, 0.0906, 0.0905, 0.0893, 0.0875, 0.0890, 0.0922, 0.0859, 0.0912,
                  0.0902, 0.0910, 0.0903, 0.0897, 0.0907, 0.0923, 0.0885, 0.0866, 0.0871,
                  0.0880, 0.0885, 0.0912, 0.0902, 0.0911, 0.0904, 0.0875, 0.0855, 0.0950,
                  0.0905, 0.0901, 0.0865, 0.0896, 0.0882, 0.0911, 0.0911, 0.0904, 0.0895,
                  0.0895, 0.0905, 0.0912, 0.0901, 0.0905, 0.0889, 0.0868, 0.0915, 0.0881,
                  0.0907, 0.0894, 0.0894], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0978], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=0.6844348907470703)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:13:18,314]: 
Model Parameters with Weights:
[2025-05-05 07:13:18,314]: 
Parameter: conv1.0.weight
[2025-05-05 07:13:18,314]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-05 07:13:18,316]: Sample Values (16 elements): [0.05401788279414177, -0.0850287452340126, -0.0625058263540268, -0.08261118829250336, 0.04211686551570892, -0.032928112894296646, -0.06388842314481735, 0.07036417722702026, 0.04893657937645912, 0.0649520605802536, -0.03458793833851814, 0.051119640469551086, -0.09375762939453125, 0.06314346939325333, -0.008283733390271664, -0.10821768641471863]
[2025-05-05 07:13:18,317]: Mean: 0.0037
[2025-05-05 07:13:18,317]: Min: -0.1094
[2025-05-05 07:13:18,318]: Max: 0.1215
[2025-05-05 07:13:18,318]: 
Parameter: conv1.0.bias
[2025-05-05 07:13:18,318]: Shape: torch.Size([6])
[2025-05-05 07:13:18,318]: Sample Values (6 elements): [-0.08786880224943161, -0.07652296870946884, -0.10137487947940826, 0.03737522289156914, 0.039784759283065796, 0.07597191631793976]
[2025-05-05 07:13:18,321]: Mean: -0.0188
[2025-05-05 07:13:18,321]: Min: -0.1014
[2025-05-05 07:13:18,322]: Max: 0.0760
[2025-05-05 07:13:18,322]: 
Parameter: conv2.0.weight
[2025-05-05 07:13:18,322]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-05 07:13:18,322]: Sample Values (16 elements): [0.0725218653678894, -0.05990446358919144, -0.017179274931550026, -0.06840889155864716, 0.03241957351565361, -0.047250427305698395, -0.03226751089096069, -0.0535912811756134, -0.03654449060559273, -0.05679146945476532, -0.012504606507718563, -0.011279583908617496, 0.05897638201713562, -0.08118317276239395, 0.053470153361558914, -0.048497121781110764]
[2025-05-05 07:13:18,325]: Mean: -0.0052
[2025-05-05 07:13:18,325]: Min: -0.0814
[2025-05-05 07:13:18,326]: Max: 0.0794
[2025-05-05 07:13:18,326]: 
Parameter: conv2.0.bias
[2025-05-05 07:13:18,326]: Shape: torch.Size([16])
[2025-05-05 07:13:18,326]: Sample Values (16 elements): [-0.05592624843120575, 0.034396614879369736, 0.013269922696053982, -0.02610984817147255, -0.05429598316550255, 0.03682452067732811, 0.06877885013818741, -0.007149467710405588, -0.029548626393079758, 0.06436841934919357, -0.007785466965287924, -0.013466252014040947, -0.07079894840717316, -0.030248330906033516, 0.0002635104756336659, 0.03166060149669647]
[2025-05-05 07:13:18,329]: Mean: -0.0029
[2025-05-05 07:13:18,329]: Min: -0.0708
[2025-05-05 07:13:18,330]: Max: 0.0688
[2025-05-05 07:13:18,330]: 
Parameter: fc1.0.weight
[2025-05-05 07:13:18,330]: Shape: torch.Size([120, 400])
[2025-05-05 07:13:18,331]: Sample Values (16 elements): [0.0020839092321693897, 0.004256720654666424, -0.028085123747587204, 0.03591811656951904, 0.02789556048810482, 0.027204643934965134, 0.04223031550645828, 0.04903547838330269, -0.022959716618061066, -0.02872367575764656, 0.027772748842835426, -0.023701274767518044, 0.0056691477075219154, 0.001991053344681859, 0.0008148964261636138, 0.004759436007589102]
[2025-05-05 07:13:18,331]: Mean: -0.0020
[2025-05-05 07:13:18,332]: Min: -0.0499
[2025-05-05 07:13:18,332]: Max: 0.0498
[2025-05-05 07:13:18,332]: 
Parameter: fc1.0.bias
[2025-05-05 07:13:18,332]: Shape: torch.Size([120])
[2025-05-05 07:13:18,333]: Sample Values (16 elements): [0.02366097830235958, 0.015253832563757896, 0.005040612071752548, -0.006197079550474882, -0.03993919864296913, -0.03911452740430832, 0.036971233785152435, 0.03353908658027649, -0.02429601177573204, 0.017432672902941704, 0.02046101540327072, 0.004433433059602976, -0.006758134346455336, -0.016785893589258194, -0.043530065566301346, 0.026069780811667442]
[2025-05-05 07:13:18,333]: Mean: -0.0009
[2025-05-05 07:13:18,334]: Min: -0.0496
[2025-05-05 07:13:18,334]: Max: 0.0508
[2025-05-05 07:13:18,334]: 
Parameter: fc2.0.weight
[2025-05-05 07:13:18,334]: Shape: torch.Size([84, 120])
[2025-05-05 07:13:18,335]: Sample Values (16 elements): [0.08468617498874664, -0.030629927292466164, -0.07191171497106552, 0.022229887545108795, 0.06314871460199356, 0.005219792481511831, 0.07502955943346024, -0.005184034816920757, 0.05368354171514511, 0.05525822937488556, -0.013177154585719109, 0.05942881852388382, 0.06691084802150726, 0.027224045246839523, -0.018995674327015877, 0.01987316831946373]
[2025-05-05 07:13:18,336]: Mean: 0.0052
[2025-05-05 07:13:18,336]: Min: -0.0908
[2025-05-05 07:13:18,336]: Max: 0.0895
[2025-05-05 07:13:18,336]: 
Parameter: fc2.0.bias
[2025-05-05 07:13:18,336]: Shape: torch.Size([84])
[2025-05-05 07:13:18,337]: Sample Values (16 elements): [0.08436528593301773, -0.08574548363685608, -0.08118292689323425, -0.020924992859363556, 0.0005966684548184276, -0.05579512193799019, 0.050673823803663254, -0.06887156516313553, 0.027750544250011444, 0.0555831715464592, -0.03274131193757057, 0.060321222990751266, -0.08301645517349243, -0.021466311067342758, -0.020644119009375572, 0.042984139174222946]
[2025-05-05 07:13:18,338]: Mean: -0.0022
[2025-05-05 07:13:18,338]: Min: -0.0906
[2025-05-05 07:13:18,338]: Max: 0.1006
[2025-05-05 07:13:18,338]: 
Parameter: fc3.weight
[2025-05-05 07:13:18,338]: Shape: torch.Size([10, 84])
[2025-05-05 07:13:18,339]: Sample Values (16 elements): [0.09542535990476608, -0.004727852530777454, 0.10287699848413467, -0.09503450989723206, -0.08654700964689255, -0.0016872885171324015, -0.077977754175663, -0.027224795892834663, -0.03322169929742813, 0.05129021033644676, 0.04369537904858589, -0.08941183984279633, -0.027519728988409042, -0.04381223022937775, -0.001796973287127912, -0.029213763773441315]
[2025-05-05 07:13:18,340]: Mean: 0.0066
[2025-05-05 07:13:18,340]: Min: -0.1116
[2025-05-05 07:13:18,341]: Max: 0.1066
[2025-05-05 07:13:18,341]: 
Parameter: fc3.bias
[2025-05-05 07:13:18,341]: Shape: torch.Size([10])
[2025-05-05 07:13:18,341]: Sample Values (10 elements): [-0.06076192110776901, 0.048097364604473114, 0.03834386169910431, -0.08159537613391876, 0.06689370423555374, 0.012610235251486301, -0.06186997517943382, 0.04764915630221367, -0.08416441828012466, -0.010882054455578327]
[2025-05-05 07:13:18,342]: Mean: -0.0086
[2025-05-05 07:13:18,343]: Min: -0.0842
[2025-05-05 07:13:18,343]: Max: 0.0669
[2025-05-05 07:13:18,343]: 


QAT of LeNet5 with relu down to 2 bits...
[2025-05-05 07:13:18,383]: [LeNet5_relu_quantized_2_bits] after configure_qat:
[2025-05-05 07:13:18,408]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:13:50,388]: [LeNet5_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.2936 Train Acc: 0.1277 Eval Loss: 2.2854 Eval Acc: 0.1489 (LR: 0.001000)
[2025-05-05 07:13:50,399]: [LeNet5_relu_quantized_2_bits] Best Eval Accuracy: 0.1489
[2025-05-05 07:13:50,411]: 


Quantization of model down to 2 bits finished
[2025-05-05 07:13:50,411]: Model Architecture:
[2025-05-05 07:13:50,576]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3202], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.960728883743286)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0541, 0.0561, 0.0550, 0.0538, 0.0564, 0.0549, 0.0545, 0.0547, 0.0537,
                0.0547, 0.0539, 0.0542, 0.0550, 0.0544, 0.0537, 0.0540],
               device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0812, -0.0793, -0.0797, -0.0806, -0.0819, -0.0793, -0.0818, -0.0804,
                  -0.0800, -0.0811, -0.0809, -0.0814, -0.0805, -0.0790, -0.0795, -0.0810],
                 device='cuda:0'), max_val=tensor([0.0794, 0.0842, 0.0825, 0.0807, 0.0846, 0.0823, 0.0805, 0.0820, 0.0806,
                  0.0821, 0.0783, 0.0814, 0.0825, 0.0817, 0.0805, 0.0810],
                 device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6967], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.0899765491485596)
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0333, 0.0340, 0.0331, 0.0333, 0.0333, 0.0332, 0.0331, 0.0341, 0.0335,
                0.0335, 0.0333, 0.0334, 0.0333, 0.0331, 0.0333, 0.0336, 0.0334, 0.0335,
                0.0333, 0.0334, 0.0333, 0.0335, 0.0333, 0.0345, 0.0331, 0.0333, 0.0333,
                0.0334, 0.0336, 0.0340, 0.0338, 0.0333, 0.0339, 0.0331, 0.0334, 0.0337,
                0.0335, 0.0335, 0.0336, 0.0332, 0.0337, 0.0331, 0.0328, 0.0333, 0.0333,
                0.0337, 0.0332, 0.0331, 0.0337, 0.0333, 0.0333, 0.0332, 0.0339, 0.0333,
                0.0333, 0.0335, 0.0333, 0.0333, 0.0333, 0.0330, 0.0333, 0.0343, 0.0333,
                0.0332, 0.0343, 0.0334, 0.0335, 0.0336, 0.0340, 0.0335, 0.0334, 0.0337,
                0.0334, 0.0329, 0.0334, 0.0336, 0.0334, 0.0333, 0.0338, 0.0332, 0.0336,
                0.0344, 0.0333, 0.0330, 0.0337, 0.0333, 0.0333, 0.0335, 0.0333, 0.0332,
                0.0335, 0.0335, 0.0333, 0.0332, 0.0335, 0.0341, 0.0333, 0.0334, 0.0333,
                0.0334, 0.0332, 0.0330, 0.0338, 0.0332, 0.0336, 0.0335, 0.0331, 0.0336,
                0.0334, 0.0332, 0.0333, 0.0333, 0.0333, 0.0336, 0.0334, 0.0334, 0.0334,
                0.0336, 0.0326, 0.0337], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0499, -0.0491, -0.0496, -0.0500, -0.0498, -0.0482, -0.0496, -0.0512,
                  -0.0501, -0.0496, -0.0496, -0.0501, -0.0497, -0.0495, -0.0499, -0.0495,
                  -0.0501, -0.0502, -0.0499, -0.0494, -0.0497, -0.0497, -0.0497, -0.0517,
                  -0.0496, -0.0491, -0.0500, -0.0486, -0.0505, -0.0492, -0.0495, -0.0497,
                  -0.0495, -0.0497, -0.0502, -0.0505, -0.0502, -0.0493, -0.0504, -0.0494,
                  -0.0505, -0.0496, -0.0492, -0.0495, -0.0498, -0.0494, -0.0497, -0.0497,
                  -0.0491, -0.0499, -0.0499, -0.0497, -0.0509, -0.0496, -0.0499, -0.0502,
                  -0.0500, -0.0500, -0.0495, -0.0496, -0.0498, -0.0494, -0.0497, -0.0498,
                  -0.0502, -0.0501, -0.0499, -0.0503, -0.0493, -0.0501, -0.0501, -0.0505,
                  -0.0498, -0.0488, -0.0500, -0.0489, -0.0501, -0.0499, -0.0502, -0.0493,
                  -0.0504, -0.0473, -0.0500, -0.0494, -0.0505, -0.0495, -0.0499, -0.0487,
                  -0.0497, -0.0498, -0.0499, -0.0472, -0.0492, -0.0496, -0.0501, -0.0511,
                  -0.0494, -0.0487, -0.0499, -0.0497, -0.0498, -0.0495, -0.0498, -0.0498,
                  -0.0504, -0.0484, -0.0497, -0.0490, -0.0496, -0.0495, -0.0493, -0.0499,
                  -0.0499, -0.0504, -0.0501, -0.0501, -0.0501, -0.0505, -0.0489, -0.0495],
                 device='cuda:0'), max_val=tensor([0.0498, 0.0510, 0.0497, 0.0494, 0.0499, 0.0498, 0.0497, 0.0494, 0.0502,
                  0.0502, 0.0500, 0.0498, 0.0500, 0.0497, 0.0499, 0.0504, 0.0500, 0.0493,
                  0.0498, 0.0502, 0.0500, 0.0503, 0.0499, 0.0494, 0.0495, 0.0500, 0.0499,
                  0.0500, 0.0495, 0.0510, 0.0507, 0.0500, 0.0508, 0.0491, 0.0500, 0.0502,
                  0.0498, 0.0502, 0.0498, 0.0498, 0.0496, 0.0494, 0.0490, 0.0500, 0.0500,
                  0.0506, 0.0499, 0.0497, 0.0505, 0.0499, 0.0499, 0.0497, 0.0500, 0.0500,
                  0.0497, 0.0500, 0.0499, 0.0498, 0.0500, 0.0496, 0.0499, 0.0514, 0.0499,
                  0.0498, 0.0515, 0.0496, 0.0503, 0.0496, 0.0510, 0.0502, 0.0497, 0.0498,
                  0.0502, 0.0494, 0.0495, 0.0504, 0.0500, 0.0497, 0.0508, 0.0497, 0.0498,
                  0.0515, 0.0498, 0.0493, 0.0504, 0.0499, 0.0499, 0.0503, 0.0499, 0.0495,
                  0.0502, 0.0502, 0.0499, 0.0498, 0.0502, 0.0496, 0.0499, 0.0501, 0.0491,
                  0.0501, 0.0497, 0.0493, 0.0507, 0.0498, 0.0495, 0.0503, 0.0491, 0.0504,
                  0.0502, 0.0498, 0.0499, 0.0499, 0.0499, 0.0496, 0.0500, 0.0495, 0.0498,
                  0.0500, 0.0485, 0.0506], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3905], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.171441912651062)
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-2, quant_max=1, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([0.0606, 0.0606, 0.0596, 0.0607, 0.0604, 0.0603, 0.0608, 0.0604, 0.0602,
                0.0606, 0.0607, 0.0608, 0.0605, 0.0608, 0.0601, 0.0608, 0.0595, 0.0606,
                0.0601, 0.0603, 0.0608, 0.0605, 0.0604, 0.0607, 0.0607, 0.0597, 0.0598,
                0.0607, 0.0605, 0.0598, 0.0600, 0.0606, 0.0607, 0.0609, 0.0605, 0.0597,
                0.0600, 0.0605, 0.0603, 0.0596, 0.0587, 0.0605, 0.0615, 0.0604, 0.0607,
                0.0601, 0.0608, 0.0606, 0.0605, 0.0605, 0.0615, 0.0600, 0.0598, 0.0597,
                0.0594, 0.0609, 0.0608, 0.0601, 0.0607, 0.0603, 0.0593, 0.0606, 0.0634,
                0.0606, 0.0602, 0.0602, 0.0596, 0.0606, 0.0607, 0.0607, 0.0608, 0.0597,
                0.0610, 0.0603, 0.0608, 0.0604, 0.0603, 0.0593, 0.0602, 0.0610, 0.0592,
                0.0605, 0.0606, 0.0597], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',
               dtype=torch.int32)
        (activation_post_process): MovingAveragePerChannelMinMaxObserver(
          min_val=tensor([-0.0909, -0.0909, -0.0894, -0.0911, -0.0901, -0.0905, -0.0902, -0.0906,
                  -0.0894, -0.0903, -0.0910, -0.0909, -0.0907, -0.0912, -0.0902, -0.0887,
                  -0.0882, -0.0909, -0.0877, -0.0905, -0.0912, -0.0907, -0.0905, -0.0906,
                  -0.0883, -0.0883, -0.0858, -0.0911, -0.0877, -0.0875, -0.0877, -0.0892,
                  -0.0904, -0.0900, -0.0907, -0.0895, -0.0899, -0.0907, -0.0898, -0.0890,
                  -0.0880, -0.0907, -0.0879, -0.0906, -0.0880, -0.0878, -0.0912, -0.0909,
                  -0.0908, -0.0892, -0.0868, -0.0899, -0.0897, -0.0895, -0.0891, -0.0914,
                  -0.0880, -0.0879, -0.0911, -0.0873, -0.0889, -0.0910, -0.0929, -0.0910,
                  -0.0902, -0.0903, -0.0858, -0.0910, -0.0893, -0.0911, -0.0912, -0.0881,
                  -0.0915, -0.0899, -0.0911, -0.0905, -0.0903, -0.0889, -0.0903, -0.0895,
                  -0.0888, -0.0889, -0.0909, -0.0896], device='cuda:0'), max_val=tensor([0.0894, 0.0889, 0.0891, 0.0894, 0.0906, 0.0905, 0.0912, 0.0862, 0.0903,
                  0.0909, 0.0887, 0.0912, 0.0895, 0.0910, 0.0887, 0.0912, 0.0893, 0.0882,
                  0.0902, 0.0893, 0.0903, 0.0884, 0.0896, 0.0911, 0.0910, 0.0896, 0.0896,
                  0.0906, 0.0907, 0.0897, 0.0901, 0.0909, 0.0911, 0.0913, 0.0819, 0.0852,
                  0.0889, 0.0904, 0.0905, 0.0894, 0.0875, 0.0890, 0.0922, 0.0859, 0.0911,
                  0.0902, 0.0910, 0.0903, 0.0898, 0.0908, 0.0923, 0.0881, 0.0865, 0.0871,
                  0.0881, 0.0885, 0.0912, 0.0901, 0.0911, 0.0904, 0.0875, 0.0855, 0.0950,
                  0.0905, 0.0902, 0.0865, 0.0895, 0.0885, 0.0911, 0.0911, 0.0902, 0.0895,
                  0.0895, 0.0905, 0.0912, 0.0901, 0.0904, 0.0889, 0.0867, 0.0915, 0.0881,
                  0.0907, 0.0895, 0.0891], device='cuda:0')
        )
      )
      (activation_post_process): NoopObserver()
    )
    (1): Sequential(
      (0): ReLU(inplace=True)
      (1): QuantStub(
        (activation_post_process): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1911], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=0.5734342932701111)
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-05 07:13:50,577]: 
Model Parameters with Weights:
[2025-05-05 07:13:50,577]: 
Parameter: conv1.0.weight
[2025-05-05 07:13:50,577]: Shape: torch.Size([6, 3, 5, 5])
[2025-05-05 07:13:50,579]: Sample Values (16 elements): [0.10552340000867844, 0.0383683480322361, 0.028663791716098785, 0.06386865675449371, -0.0015239546773955226, 0.04105167090892792, 0.1183222308754921, 0.06221993640065193, 0.09361815452575684, -0.08624961227178574, -0.03455047681927681, 0.08901447802782059, -0.052558112889528275, -0.06247497349977493, 0.036747246980667114, -0.014599609188735485]
[2025-05-05 07:13:50,579]: Mean: 0.0025
[2025-05-05 07:13:50,580]: Min: -0.1120
[2025-05-05 07:13:50,580]: Max: 0.1183
[2025-05-05 07:13:50,580]: 
Parameter: conv1.0.bias
[2025-05-05 07:13:50,581]: Shape: torch.Size([6])
[2025-05-05 07:13:50,581]: Sample Values (6 elements): [-0.10016191005706787, 0.038012851029634476, 0.03924356773495674, -0.0914042517542839, 0.07615360617637634, -0.07807935029268265]
[2025-05-05 07:13:50,581]: Mean: -0.0194
[2025-05-05 07:13:50,582]: Min: -0.1002
[2025-05-05 07:13:50,582]: Max: 0.0762
[2025-05-05 07:13:50,582]: 
Parameter: conv2.0.weight
[2025-05-05 07:13:50,582]: Shape: torch.Size([16, 6, 5, 5])
[2025-05-05 07:13:50,586]: Sample Values (16 elements): [-0.0435112826526165, 0.005382217932492495, 0.058887917548418045, 0.03995143622159958, -0.04316946491599083, -0.037490032613277435, -0.055654123425483704, -0.03661486133933067, -0.06373986601829529, -0.023837203159928322, 0.020389484241604805, 0.016129592433571815, -0.058384623378515244, 0.03292049840092659, 0.07358281314373016, -0.024202095344662666]
[2025-05-05 07:13:50,586]: Mean: -0.0052
[2025-05-05 07:13:50,587]: Min: -0.0812
[2025-05-05 07:13:50,587]: Max: 0.0794
[2025-05-05 07:13:50,587]: 
Parameter: conv2.0.bias
[2025-05-05 07:13:50,587]: Shape: torch.Size([16])
[2025-05-05 07:13:50,587]: Sample Values (16 elements): [-0.008479701355099678, -0.007438028696924448, 0.035500556230545044, -0.009752989746630192, 0.010224110446870327, 0.06153780594468117, 0.07085748016834259, -0.07079548388719559, -0.028179461136460304, 0.03668392449617386, -0.025612501427531242, 0.0021410465706139803, -0.028079338371753693, -0.055394936352968216, 0.03102574497461319, -0.05348768085241318]
[2025-05-05 07:13:50,588]: Mean: -0.0025
[2025-05-05 07:13:50,588]: Min: -0.0708
[2025-05-05 07:13:50,589]: Max: 0.0709
[2025-05-05 07:13:50,589]: 
Parameter: fc1.0.weight
[2025-05-05 07:13:50,589]: Shape: torch.Size([120, 400])
[2025-05-05 07:13:50,589]: Sample Values (16 elements): [0.01436266116797924, 0.01005915179848671, -0.04232519492506981, 0.002309069037437439, -0.00520124938338995, -0.028003735467791557, 0.03771980479359627, -0.0009712501778267324, -0.02126850001513958, 0.01058700866997242, 0.0022037853486835957, 0.030137181282043457, -0.0012463846942409873, 0.01797792874276638, 0.017861245200037956, 0.0027973155956715345]
[2025-05-05 07:13:50,593]: Mean: -0.0019
[2025-05-05 07:13:50,593]: Min: -0.0499
[2025-05-05 07:13:50,594]: Max: 0.0498
[2025-05-05 07:13:50,594]: 
Parameter: fc1.0.bias
[2025-05-05 07:13:50,594]: Shape: torch.Size([120])
[2025-05-05 07:13:50,594]: Sample Values (16 elements): [-0.013201755471527576, 0.028771517798304558, 0.03982583060860634, -0.0026943283155560493, -0.03799957409501076, 0.03995383530855179, 0.05137988552451134, -0.0016271582571789622, -0.04305095970630646, 0.023706331849098206, -0.01853034645318985, 0.009274511598050594, 0.0037081767804920673, 0.020902549847960472, 0.020663393661379814, 0.036539189517498016]
[2025-05-05 07:13:50,595]: Mean: -0.0009
[2025-05-05 07:13:50,595]: Min: -0.0494
[2025-05-05 07:13:50,595]: Max: 0.0514
[2025-05-05 07:13:50,596]: 
Parameter: fc2.0.weight
[2025-05-05 07:13:50,596]: Shape: torch.Size([84, 120])
[2025-05-05 07:13:50,596]: Sample Values (16 elements): [-0.031457528471946716, -0.007160330656915903, 0.0640493631362915, -0.007099072448909283, 0.06806154549121857, -0.07193273305892944, 0.015870196744799614, -0.08802954852581024, 0.043223246932029724, 0.059641283005476, 0.055257007479667664, -0.09163043648004532, -0.08985200524330139, 0.05772304907441139, 0.02665940672159195, -0.02090184949338436]
[2025-05-05 07:13:50,597]: Mean: 0.0049
[2025-05-05 07:13:50,597]: Min: -0.0916
[2025-05-05 07:13:50,598]: Max: 0.0893
[2025-05-05 07:13:50,598]: 
Parameter: fc2.0.bias
[2025-05-05 07:13:50,598]: Shape: torch.Size([84])
[2025-05-05 07:13:50,600]: Sample Values (16 elements): [-0.03207375109195709, -0.0378449410200119, -0.054453134536743164, 0.026663832366466522, 0.04093685746192932, -0.08038730174303055, 0.023596758022904396, -0.009516295976936817, 0.08578583598136902, 0.03157268092036247, 0.03308430314064026, 0.03573581203818321, -0.06709146499633789, 0.0549447163939476, -0.015652049332857132, -0.04336171597242355]
[2025-05-05 07:13:50,600]: Mean: -0.0021
[2025-05-05 07:13:50,601]: Min: -0.0906
[2025-05-05 07:13:50,601]: Max: 0.1010
[2025-05-05 07:13:50,601]: 
Parameter: fc3.weight
[2025-05-05 07:13:50,601]: Shape: torch.Size([10, 84])
[2025-05-05 07:13:50,602]: Sample Values (16 elements): [0.06766557693481445, 0.10651819407939911, 0.037841785699129105, -0.017339861020445824, -0.03349965065717697, 0.05473010987043381, -0.016895180568099022, -0.0001248710323125124, -0.0626668855547905, 0.07569762319326401, 0.01935199648141861, -0.09002246707677841, 0.02036452665925026, 0.006540785543620586, -0.018946051597595215, 0.011873607523739338]
[2025-05-05 07:13:50,602]: Mean: 0.0063
[2025-05-05 07:13:50,603]: Min: -0.1080
[2025-05-05 07:13:50,603]: Max: 0.1065
[2025-05-05 07:13:50,603]: 
Parameter: fc3.bias
[2025-05-05 07:13:50,603]: Shape: torch.Size([10])
[2025-05-05 07:13:50,604]: Sample Values (10 elements): [-0.07920737564563751, 0.043169163167476654, -0.05854027718305588, -0.057736512273550034, 0.03687572106719017, -0.0792393833398819, -0.013252400793135166, 0.013411843217909336, 0.06292355060577393, 0.04591620713472366]
[2025-05-05 07:13:50,604]: Mean: -0.0086
[2025-05-05 07:13:50,607]: Min: -0.0792
[2025-05-05 07:13:50,607]: Max: 0.0629
