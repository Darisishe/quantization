[2025-05-12 05:26:54,267]: 
Training LeNet5 with relu
[2025-05-12 05:27:36,953]: [LeNet5_relu] Epoch: 001 Train Loss: 2.2788 Train Acc: 0.1352 Eval Loss: 2.2184 Eval Acc: 0.1734 (LR: 0.001000)
[2025-05-12 05:28:16,982]: [LeNet5_relu] Epoch: 002 Train Loss: 2.1341 Train Acc: 0.2202 Eval Loss: 2.0258 Eval Acc: 0.2529 (LR: 0.001000)
[2025-05-12 05:28:55,512]: [LeNet5_relu] Epoch: 003 Train Loss: 2.0056 Train Acc: 0.2638 Eval Loss: 1.9299 Eval Acc: 0.3056 (LR: 0.001000)
[2025-05-12 05:29:37,529]: [LeNet5_relu] Epoch: 004 Train Loss: 1.9311 Train Acc: 0.2926 Eval Loss: 1.8299 Eval Acc: 0.3405 (LR: 0.001000)
[2025-05-12 05:30:15,608]: [LeNet5_relu] Epoch: 005 Train Loss: 1.8282 Train Acc: 0.3282 Eval Loss: 1.7095 Eval Acc: 0.3749 (LR: 0.001000)
[2025-05-12 05:30:55,185]: [LeNet5_relu] Epoch: 006 Train Loss: 1.7492 Train Acc: 0.3517 Eval Loss: 1.6417 Eval Acc: 0.3959 (LR: 0.001000)
[2025-05-12 05:31:33,852]: [LeNet5_relu] Epoch: 007 Train Loss: 1.6944 Train Acc: 0.3726 Eval Loss: 1.5959 Eval Acc: 0.4166 (LR: 0.001000)
[2025-05-12 05:32:12,116]: [LeNet5_relu] Epoch: 008 Train Loss: 1.6527 Train Acc: 0.3898 Eval Loss: 1.5448 Eval Acc: 0.4339 (LR: 0.001000)
[2025-05-12 05:32:49,915]: [LeNet5_relu] Epoch: 009 Train Loss: 1.6194 Train Acc: 0.4047 Eval Loss: 1.5130 Eval Acc: 0.4495 (LR: 0.001000)
[2025-05-12 05:33:29,778]: [LeNet5_relu] Epoch: 010 Train Loss: 1.5876 Train Acc: 0.4173 Eval Loss: 1.4788 Eval Acc: 0.4618 (LR: 0.001000)
[2025-05-12 05:34:07,846]: [LeNet5_relu] Epoch: 011 Train Loss: 1.5620 Train Acc: 0.4264 Eval Loss: 1.4655 Eval Acc: 0.4737 (LR: 0.001000)
[2025-05-12 05:34:46,052]: [LeNet5_relu] Epoch: 012 Train Loss: 1.5350 Train Acc: 0.4390 Eval Loss: 1.4268 Eval Acc: 0.4880 (LR: 0.001000)
[2025-05-12 05:35:23,877]: [LeNet5_relu] Epoch: 013 Train Loss: 1.5182 Train Acc: 0.4479 Eval Loss: 1.4204 Eval Acc: 0.4900 (LR: 0.001000)
[2025-05-12 05:36:00,832]: [LeNet5_relu] Epoch: 014 Train Loss: 1.5024 Train Acc: 0.4514 Eval Loss: 1.3798 Eval Acc: 0.5011 (LR: 0.001000)
[2025-05-12 05:36:38,134]: [LeNet5_relu] Epoch: 015 Train Loss: 1.4858 Train Acc: 0.4570 Eval Loss: 1.3941 Eval Acc: 0.4979 (LR: 0.001000)
[2025-05-12 05:37:15,346]: [LeNet5_relu] Epoch: 016 Train Loss: 1.4640 Train Acc: 0.4660 Eval Loss: 1.3635 Eval Acc: 0.5099 (LR: 0.001000)
[2025-05-12 05:37:52,750]: [LeNet5_relu] Epoch: 017 Train Loss: 1.4517 Train Acc: 0.4704 Eval Loss: 1.3298 Eval Acc: 0.5214 (LR: 0.001000)
[2025-05-12 05:38:30,501]: [LeNet5_relu] Epoch: 018 Train Loss: 1.4350 Train Acc: 0.4748 Eval Loss: 1.3127 Eval Acc: 0.5268 (LR: 0.001000)
[2025-05-12 05:39:04,436]: [LeNet5_relu] Epoch: 019 Train Loss: 1.4240 Train Acc: 0.4834 Eval Loss: 1.3268 Eval Acc: 0.5194 (LR: 0.001000)
[2025-05-12 05:39:38,390]: [LeNet5_relu] Epoch: 020 Train Loss: 1.4063 Train Acc: 0.4906 Eval Loss: 1.2889 Eval Acc: 0.5407 (LR: 0.001000)
[2025-05-12 05:40:11,916]: [LeNet5_relu] Epoch: 021 Train Loss: 1.3950 Train Acc: 0.4925 Eval Loss: 1.2842 Eval Acc: 0.5361 (LR: 0.001000)
[2025-05-12 05:40:48,714]: [LeNet5_relu] Epoch: 022 Train Loss: 1.3797 Train Acc: 0.4995 Eval Loss: 1.3223 Eval Acc: 0.5281 (LR: 0.001000)
[2025-05-12 05:41:24,647]: [LeNet5_relu] Epoch: 023 Train Loss: 1.3659 Train Acc: 0.5065 Eval Loss: 1.2565 Eval Acc: 0.5490 (LR: 0.001000)
[2025-05-12 05:41:59,513]: [LeNet5_relu] Epoch: 024 Train Loss: 1.3581 Train Acc: 0.5079 Eval Loss: 1.2540 Eval Acc: 0.5505 (LR: 0.001000)
[2025-05-12 05:42:34,440]: [LeNet5_relu] Epoch: 025 Train Loss: 1.3520 Train Acc: 0.5144 Eval Loss: 1.2452 Eval Acc: 0.5512 (LR: 0.001000)
[2025-05-12 05:43:08,779]: [LeNet5_relu] Epoch: 026 Train Loss: 1.3350 Train Acc: 0.5185 Eval Loss: 1.2156 Eval Acc: 0.5682 (LR: 0.001000)
[2025-05-12 05:43:42,700]: [LeNet5_relu] Epoch: 027 Train Loss: 1.3230 Train Acc: 0.5239 Eval Loss: 1.2197 Eval Acc: 0.5629 (LR: 0.001000)
[2025-05-12 05:44:17,119]: [LeNet5_relu] Epoch: 028 Train Loss: 1.3206 Train Acc: 0.5250 Eval Loss: 1.2026 Eval Acc: 0.5692 (LR: 0.001000)
[2025-05-12 05:44:51,379]: [LeNet5_relu] Epoch: 029 Train Loss: 1.3129 Train Acc: 0.5292 Eval Loss: 1.2157 Eval Acc: 0.5648 (LR: 0.001000)
[2025-05-12 05:45:24,968]: [LeNet5_relu] Epoch: 030 Train Loss: 1.3005 Train Acc: 0.5331 Eval Loss: 1.1845 Eval Acc: 0.5772 (LR: 0.001000)
[2025-05-12 05:45:58,536]: [LeNet5_relu] Epoch: 031 Train Loss: 1.2932 Train Acc: 0.5373 Eval Loss: 1.1741 Eval Acc: 0.5805 (LR: 0.001000)
[2025-05-12 05:46:32,361]: [LeNet5_relu] Epoch: 032 Train Loss: 1.2870 Train Acc: 0.5393 Eval Loss: 1.1583 Eval Acc: 0.5851 (LR: 0.001000)
[2025-05-12 05:47:07,208]: [LeNet5_relu] Epoch: 033 Train Loss: 1.2749 Train Acc: 0.5461 Eval Loss: 1.1792 Eval Acc: 0.5783 (LR: 0.001000)
[2025-05-12 05:47:40,389]: [LeNet5_relu] Epoch: 034 Train Loss: 1.2712 Train Acc: 0.5428 Eval Loss: 1.1555 Eval Acc: 0.5893 (LR: 0.001000)
[2025-05-12 05:48:13,540]: [LeNet5_relu] Epoch: 035 Train Loss: 1.2576 Train Acc: 0.5479 Eval Loss: 1.1486 Eval Acc: 0.5907 (LR: 0.001000)
[2025-05-12 05:48:46,680]: [LeNet5_relu] Epoch: 036 Train Loss: 1.2530 Train Acc: 0.5500 Eval Loss: 1.1486 Eval Acc: 0.5881 (LR: 0.001000)
[2025-05-12 05:49:19,784]: [LeNet5_relu] Epoch: 037 Train Loss: 1.2520 Train Acc: 0.5515 Eval Loss: 1.1438 Eval Acc: 0.5941 (LR: 0.001000)
[2025-05-12 05:49:52,984]: [LeNet5_relu] Epoch: 038 Train Loss: 1.2435 Train Acc: 0.5562 Eval Loss: 1.1341 Eval Acc: 0.5956 (LR: 0.001000)
[2025-05-12 05:50:26,075]: [LeNet5_relu] Epoch: 039 Train Loss: 1.2314 Train Acc: 0.5612 Eval Loss: 1.1453 Eval Acc: 0.5915 (LR: 0.001000)
[2025-05-12 05:50:59,192]: [LeNet5_relu] Epoch: 040 Train Loss: 1.2329 Train Acc: 0.5617 Eval Loss: 1.1222 Eval Acc: 0.6029 (LR: 0.001000)
[2025-05-12 05:51:32,788]: [LeNet5_relu] Epoch: 041 Train Loss: 1.2224 Train Acc: 0.5620 Eval Loss: 1.1099 Eval Acc: 0.6050 (LR: 0.001000)
[2025-05-12 05:52:07,466]: [LeNet5_relu] Epoch: 042 Train Loss: 1.2116 Train Acc: 0.5667 Eval Loss: 1.1180 Eval Acc: 0.6008 (LR: 0.001000)
[2025-05-12 05:52:40,869]: [LeNet5_relu] Epoch: 043 Train Loss: 1.2088 Train Acc: 0.5700 Eval Loss: 1.0961 Eval Acc: 0.6112 (LR: 0.001000)
[2025-05-12 05:53:14,431]: [LeNet5_relu] Epoch: 044 Train Loss: 1.2057 Train Acc: 0.5687 Eval Loss: 1.1058 Eval Acc: 0.6074 (LR: 0.001000)
[2025-05-12 05:53:48,067]: [LeNet5_relu] Epoch: 045 Train Loss: 1.1982 Train Acc: 0.5731 Eval Loss: 1.1022 Eval Acc: 0.6080 (LR: 0.001000)
[2025-05-12 05:54:21,306]: [LeNet5_relu] Epoch: 046 Train Loss: 1.1988 Train Acc: 0.5743 Eval Loss: 1.0998 Eval Acc: 0.6076 (LR: 0.001000)
[2025-05-12 05:54:54,811]: [LeNet5_relu] Epoch: 047 Train Loss: 1.1878 Train Acc: 0.5783 Eval Loss: 1.0951 Eval Acc: 0.6080 (LR: 0.001000)
[2025-05-12 05:55:28,055]: [LeNet5_relu] Epoch: 048 Train Loss: 1.1878 Train Acc: 0.5791 Eval Loss: 1.0815 Eval Acc: 0.6117 (LR: 0.001000)
[2025-05-12 05:56:01,385]: [LeNet5_relu] Epoch: 049 Train Loss: 1.1810 Train Acc: 0.5809 Eval Loss: 1.0883 Eval Acc: 0.6104 (LR: 0.001000)
[2025-05-12 05:56:34,921]: [LeNet5_relu] Epoch: 050 Train Loss: 1.1719 Train Acc: 0.5822 Eval Loss: 1.0747 Eval Acc: 0.6131 (LR: 0.001000)
[2025-05-12 05:57:08,430]: [LeNet5_relu] Epoch: 051 Train Loss: 1.1689 Train Acc: 0.5865 Eval Loss: 1.0716 Eval Acc: 0.6153 (LR: 0.001000)
[2025-05-12 05:57:41,979]: [LeNet5_relu] Epoch: 052 Train Loss: 1.1607 Train Acc: 0.5891 Eval Loss: 1.0640 Eval Acc: 0.6221 (LR: 0.001000)
[2025-05-12 05:58:17,760]: [LeNet5_relu] Epoch: 053 Train Loss: 1.1616 Train Acc: 0.5894 Eval Loss: 1.0669 Eval Acc: 0.6164 (LR: 0.001000)
[2025-05-12 05:58:51,446]: [LeNet5_relu] Epoch: 054 Train Loss: 1.1569 Train Acc: 0.5906 Eval Loss: 1.0586 Eval Acc: 0.6204 (LR: 0.001000)
[2025-05-12 05:59:25,079]: [LeNet5_relu] Epoch: 055 Train Loss: 1.1571 Train Acc: 0.5906 Eval Loss: 1.0544 Eval Acc: 0.6266 (LR: 0.001000)
[2025-05-12 06:00:01,665]: [LeNet5_relu] Epoch: 056 Train Loss: 1.1503 Train Acc: 0.5917 Eval Loss: 1.0648 Eval Acc: 0.6212 (LR: 0.001000)
[2025-05-12 06:00:38,908]: [LeNet5_relu] Epoch: 057 Train Loss: 1.1443 Train Acc: 0.5969 Eval Loss: 1.0745 Eval Acc: 0.6129 (LR: 0.001000)
[2025-05-12 06:01:17,362]: [LeNet5_relu] Epoch: 058 Train Loss: 1.1382 Train Acc: 0.5951 Eval Loss: 1.0546 Eval Acc: 0.6282 (LR: 0.001000)
[2025-05-12 06:01:53,853]: [LeNet5_relu] Epoch: 059 Train Loss: 1.1371 Train Acc: 0.5974 Eval Loss: 1.0392 Eval Acc: 0.6306 (LR: 0.001000)
[2025-05-12 06:02:30,536]: [LeNet5_relu] Epoch: 060 Train Loss: 1.1360 Train Acc: 0.5967 Eval Loss: 1.0587 Eval Acc: 0.6229 (LR: 0.001000)
[2025-05-12 06:03:07,483]: [LeNet5_relu] Epoch: 061 Train Loss: 1.1296 Train Acc: 0.6001 Eval Loss: 1.0650 Eval Acc: 0.6190 (LR: 0.001000)
[2025-05-12 06:03:44,228]: [LeNet5_relu] Epoch: 062 Train Loss: 1.1320 Train Acc: 0.5985 Eval Loss: 1.0344 Eval Acc: 0.6330 (LR: 0.001000)
[2025-05-12 06:04:20,725]: [LeNet5_relu] Epoch: 063 Train Loss: 1.1257 Train Acc: 0.6026 Eval Loss: 1.0244 Eval Acc: 0.6366 (LR: 0.001000)
[2025-05-12 06:04:57,249]: [LeNet5_relu] Epoch: 064 Train Loss: 1.1183 Train Acc: 0.6053 Eval Loss: 1.0368 Eval Acc: 0.6321 (LR: 0.001000)
[2025-05-12 06:05:33,587]: [LeNet5_relu] Epoch: 065 Train Loss: 1.1147 Train Acc: 0.6038 Eval Loss: 1.0296 Eval Acc: 0.6319 (LR: 0.001000)
[2025-05-12 06:06:09,840]: [LeNet5_relu] Epoch: 066 Train Loss: 1.1105 Train Acc: 0.6086 Eval Loss: 1.0248 Eval Acc: 0.6344 (LR: 0.001000)
[2025-05-12 06:06:46,360]: [LeNet5_relu] Epoch: 067 Train Loss: 1.1096 Train Acc: 0.6071 Eval Loss: 1.0289 Eval Acc: 0.6335 (LR: 0.001000)
[2025-05-12 06:07:23,015]: [LeNet5_relu] Epoch: 068 Train Loss: 1.1063 Train Acc: 0.6067 Eval Loss: 1.0254 Eval Acc: 0.6387 (LR: 0.001000)
[2025-05-12 06:07:59,496]: [LeNet5_relu] Epoch: 069 Train Loss: 1.0987 Train Acc: 0.6105 Eval Loss: 1.0374 Eval Acc: 0.6313 (LR: 0.001000)
[2025-05-12 06:08:36,333]: [LeNet5_relu] Epoch: 070 Train Loss: 1.0967 Train Acc: 0.6116 Eval Loss: 1.0183 Eval Acc: 0.6411 (LR: 0.000100)
[2025-05-12 06:09:12,888]: [LeNet5_relu] Epoch: 071 Train Loss: 1.0736 Train Acc: 0.6217 Eval Loss: 0.9998 Eval Acc: 0.6438 (LR: 0.000100)
[2025-05-12 06:09:49,359]: [LeNet5_relu] Epoch: 072 Train Loss: 1.0656 Train Acc: 0.6248 Eval Loss: 0.9962 Eval Acc: 0.6443 (LR: 0.000100)
[2025-05-12 06:10:25,957]: [LeNet5_relu] Epoch: 073 Train Loss: 1.0682 Train Acc: 0.6242 Eval Loss: 0.9989 Eval Acc: 0.6452 (LR: 0.000100)
[2025-05-12 06:11:02,336]: [LeNet5_relu] Epoch: 074 Train Loss: 1.0665 Train Acc: 0.6211 Eval Loss: 1.0039 Eval Acc: 0.6433 (LR: 0.000100)
[2025-05-12 06:11:38,997]: [LeNet5_relu] Epoch: 075 Train Loss: 1.0649 Train Acc: 0.6240 Eval Loss: 0.9954 Eval Acc: 0.6462 (LR: 0.000100)
[2025-05-12 06:12:15,600]: [LeNet5_relu] Epoch: 076 Train Loss: 1.0642 Train Acc: 0.6259 Eval Loss: 1.0000 Eval Acc: 0.6462 (LR: 0.000100)
[2025-05-12 06:12:52,047]: [LeNet5_relu] Epoch: 077 Train Loss: 1.0658 Train Acc: 0.6240 Eval Loss: 0.9943 Eval Acc: 0.6459 (LR: 0.000100)
[2025-05-12 06:13:28,612]: [LeNet5_relu] Epoch: 078 Train Loss: 1.0659 Train Acc: 0.6227 Eval Loss: 0.9962 Eval Acc: 0.6457 (LR: 0.000100)
[2025-05-12 06:14:05,329]: [LeNet5_relu] Epoch: 079 Train Loss: 1.0641 Train Acc: 0.6248 Eval Loss: 0.9924 Eval Acc: 0.6463 (LR: 0.000100)
[2025-05-12 06:14:42,078]: [LeNet5_relu] Epoch: 080 Train Loss: 1.0630 Train Acc: 0.6246 Eval Loss: 0.9970 Eval Acc: 0.6467 (LR: 0.000100)
[2025-05-12 06:15:18,653]: [LeNet5_relu] Epoch: 081 Train Loss: 1.0648 Train Acc: 0.6241 Eval Loss: 0.9968 Eval Acc: 0.6471 (LR: 0.000100)
[2025-05-12 06:15:55,350]: [LeNet5_relu] Epoch: 082 Train Loss: 1.0606 Train Acc: 0.6255 Eval Loss: 0.9925 Eval Acc: 0.6482 (LR: 0.000100)
[2025-05-12 06:16:31,876]: [LeNet5_relu] Epoch: 083 Train Loss: 1.0623 Train Acc: 0.6238 Eval Loss: 0.9948 Eval Acc: 0.6442 (LR: 0.000100)
[2025-05-12 06:17:08,577]: [LeNet5_relu] Epoch: 084 Train Loss: 1.0595 Train Acc: 0.6275 Eval Loss: 0.9957 Eval Acc: 0.6457 (LR: 0.000100)
[2025-05-12 06:17:44,726]: [LeNet5_relu] Epoch: 085 Train Loss: 1.0566 Train Acc: 0.6271 Eval Loss: 0.9994 Eval Acc: 0.6460 (LR: 0.000100)
[2025-05-12 06:18:20,880]: [LeNet5_relu] Epoch: 086 Train Loss: 1.0624 Train Acc: 0.6266 Eval Loss: 1.0028 Eval Acc: 0.6445 (LR: 0.000100)
[2025-05-12 06:18:57,900]: [LeNet5_relu] Epoch: 087 Train Loss: 1.0596 Train Acc: 0.6259 Eval Loss: 0.9965 Eval Acc: 0.6477 (LR: 0.000100)
[2025-05-12 06:19:34,328]: [LeNet5_relu] Epoch: 088 Train Loss: 1.0578 Train Acc: 0.6267 Eval Loss: 0.9936 Eval Acc: 0.6465 (LR: 0.000100)
[2025-05-12 06:20:10,706]: [LeNet5_relu] Epoch: 089 Train Loss: 1.0520 Train Acc: 0.6297 Eval Loss: 0.9937 Eval Acc: 0.6455 (LR: 0.000100)
[2025-05-12 06:20:46,999]: [LeNet5_relu] Epoch: 090 Train Loss: 1.0582 Train Acc: 0.6252 Eval Loss: 0.9914 Eval Acc: 0.6460 (LR: 0.000100)
[2025-05-12 06:21:24,078]: [LeNet5_relu] Epoch: 091 Train Loss: 1.0519 Train Acc: 0.6291 Eval Loss: 0.9934 Eval Acc: 0.6481 (LR: 0.000100)
[2025-05-12 06:21:59,464]: [LeNet5_relu] Epoch: 092 Train Loss: 1.0589 Train Acc: 0.6244 Eval Loss: 0.9898 Eval Acc: 0.6492 (LR: 0.000100)
[2025-05-12 06:22:34,391]: [LeNet5_relu] Epoch: 093 Train Loss: 1.0575 Train Acc: 0.6262 Eval Loss: 0.9956 Eval Acc: 0.6488 (LR: 0.000100)
[2025-05-12 06:23:08,499]: [LeNet5_relu] Epoch: 094 Train Loss: 1.0561 Train Acc: 0.6269 Eval Loss: 0.9908 Eval Acc: 0.6503 (LR: 0.000100)
[2025-05-12 06:23:45,404]: [LeNet5_relu] Epoch: 095 Train Loss: 1.0550 Train Acc: 0.6273 Eval Loss: 0.9917 Eval Acc: 0.6478 (LR: 0.000100)
[2025-05-12 06:24:21,887]: [LeNet5_relu] Epoch: 096 Train Loss: 1.0558 Train Acc: 0.6275 Eval Loss: 0.9913 Eval Acc: 0.6484 (LR: 0.000100)
[2025-05-12 06:24:58,299]: [LeNet5_relu] Epoch: 097 Train Loss: 1.0575 Train Acc: 0.6253 Eval Loss: 0.9878 Eval Acc: 0.6500 (LR: 0.000100)
[2025-05-12 06:25:34,403]: [LeNet5_relu] Epoch: 098 Train Loss: 1.0582 Train Acc: 0.6262 Eval Loss: 0.9877 Eval Acc: 0.6498 (LR: 0.000100)
[2025-05-12 06:26:10,911]: [LeNet5_relu] Epoch: 099 Train Loss: 1.0502 Train Acc: 0.6276 Eval Loss: 0.9896 Eval Acc: 0.6502 (LR: 0.000100)
[2025-05-12 06:26:47,409]: [LeNet5_relu] Epoch: 100 Train Loss: 1.0518 Train Acc: 0.6294 Eval Loss: 0.9862 Eval Acc: 0.6507 (LR: 0.000010)
[2025-05-12 06:27:23,981]: [LeNet5_relu] Epoch: 101 Train Loss: 1.0442 Train Acc: 0.6294 Eval Loss: 0.9867 Eval Acc: 0.6504 (LR: 0.000010)
[2025-05-12 06:28:00,489]: [LeNet5_relu] Epoch: 102 Train Loss: 1.0498 Train Acc: 0.6289 Eval Loss: 0.9876 Eval Acc: 0.6506 (LR: 0.000010)
[2025-05-12 06:28:37,024]: [LeNet5_relu] Epoch: 103 Train Loss: 1.0489 Train Acc: 0.6288 Eval Loss: 0.9876 Eval Acc: 0.6503 (LR: 0.000010)
[2025-05-12 06:29:14,795]: [LeNet5_relu] Epoch: 104 Train Loss: 1.0499 Train Acc: 0.6288 Eval Loss: 0.9859 Eval Acc: 0.6517 (LR: 0.000010)
[2025-05-12 06:29:49,169]: [LeNet5_relu] Epoch: 105 Train Loss: 1.0516 Train Acc: 0.6290 Eval Loss: 0.9861 Eval Acc: 0.6509 (LR: 0.000010)
[2025-05-12 06:30:24,903]: [LeNet5_relu] Epoch: 106 Train Loss: 1.0450 Train Acc: 0.6312 Eval Loss: 0.9858 Eval Acc: 0.6510 (LR: 0.000010)
[2025-05-12 06:31:03,137]: [LeNet5_relu] Epoch: 107 Train Loss: 1.0516 Train Acc: 0.6283 Eval Loss: 0.9865 Eval Acc: 0.6511 (LR: 0.000010)
[2025-05-12 06:31:39,667]: [LeNet5_relu] Epoch: 108 Train Loss: 1.0518 Train Acc: 0.6281 Eval Loss: 0.9873 Eval Acc: 0.6507 (LR: 0.000010)
[2025-05-12 06:32:16,483]: [LeNet5_relu] Epoch: 109 Train Loss: 1.0503 Train Acc: 0.6288 Eval Loss: 0.9866 Eval Acc: 0.6503 (LR: 0.000010)
[2025-05-12 06:32:53,136]: [LeNet5_relu] Epoch: 110 Train Loss: 1.0502 Train Acc: 0.6293 Eval Loss: 0.9862 Eval Acc: 0.6517 (LR: 0.000010)
[2025-05-12 06:33:29,898]: [LeNet5_relu] Epoch: 111 Train Loss: 1.0575 Train Acc: 0.6281 Eval Loss: 0.9853 Eval Acc: 0.6512 (LR: 0.000010)
[2025-05-12 06:34:06,352]: [LeNet5_relu] Epoch: 112 Train Loss: 1.0513 Train Acc: 0.6291 Eval Loss: 0.9873 Eval Acc: 0.6499 (LR: 0.000010)
[2025-05-12 06:34:42,637]: [LeNet5_relu] Epoch: 113 Train Loss: 1.0510 Train Acc: 0.6300 Eval Loss: 0.9850 Eval Acc: 0.6509 (LR: 0.000010)
[2025-05-12 06:35:19,431]: [LeNet5_relu] Epoch: 114 Train Loss: 1.0455 Train Acc: 0.6312 Eval Loss: 0.9862 Eval Acc: 0.6511 (LR: 0.000010)
[2025-05-12 06:35:55,855]: [LeNet5_relu] Epoch: 115 Train Loss: 1.0504 Train Acc: 0.6274 Eval Loss: 0.9867 Eval Acc: 0.6513 (LR: 0.000010)
[2025-05-12 06:36:31,974]: [LeNet5_relu] Epoch: 116 Train Loss: 1.0463 Train Acc: 0.6294 Eval Loss: 0.9857 Eval Acc: 0.6527 (LR: 0.000010)
[2025-05-12 06:37:06,558]: [LeNet5_relu] Epoch: 117 Train Loss: 1.0488 Train Acc: 0.6299 Eval Loss: 0.9867 Eval Acc: 0.6510 (LR: 0.000010)
[2025-05-12 06:37:42,257]: [LeNet5_relu] Epoch: 118 Train Loss: 1.0446 Train Acc: 0.6301 Eval Loss: 0.9852 Eval Acc: 0.6522 (LR: 0.000010)
[2025-05-12 06:38:15,364]: [LeNet5_relu] Epoch: 119 Train Loss: 1.0501 Train Acc: 0.6282 Eval Loss: 0.9864 Eval Acc: 0.6509 (LR: 0.000010)
[2025-05-12 06:38:48,878]: [LeNet5_relu] Epoch: 120 Train Loss: 1.0450 Train Acc: 0.6326 Eval Loss: 0.9860 Eval Acc: 0.6514 (LR: 0.000010)
[2025-05-12 06:38:48,879]: [LeNet5_relu] Best Eval Accuracy: 0.6527
[2025-05-12 06:38:48,911]: 
Training of full-precision model finished!
[2025-05-12 06:38:48,911]: Model Architecture:
[2025-05-12 06:38:48,912]: LeNet5(
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(in_features=120, out_features=84, bias=True)
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 06:38:48,913]: 
Model Weights:
[2025-05-12 06:38:48,913]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-12 06:38:48,930]: Sample Values (25 elements): [-0.011546770110726357, 0.013186778873205185, 0.1705794483423233, 0.11655459553003311, 0.10181394219398499, 0.13128381967544556, -0.058845579624176025, 0.3239835500717163, -0.22999274730682373, -0.2678532600402832, -0.04209328815340996, 0.16100110113620758, -0.4546341300010681, -0.16176213324069977, 0.12554210424423218, -0.17920811474323273, 0.045001447200775146, -0.22548645734786987, -0.008026890456676483, 0.09581593424081802, -0.103852279484272, -0.2598017454147339, -0.2524881660938263, 0.05911542475223541, -0.17125321924686432]
[2025-05-12 06:38:48,937]: Mean: 0.00027796
[2025-05-12 06:38:48,946]: Min: -0.59632325
[2025-05-12 06:38:48,948]: Max: 0.67956424
[2025-05-12 06:38:48,948]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-12 06:38:48,949]: Sample Values (25 elements): [0.005334572866559029, -0.026965366676449776, 0.05325133353471756, -0.10003365576267242, 0.05337976664304733, -0.06163422018289566, 0.08599714189767838, 0.033855319023132324, 0.12286779284477234, -0.07834327220916748, -0.09427084773778915, -0.013265624642372131, -0.03373078256845474, 0.05062904581427574, -0.03415030986070633, 0.05099492147564888, 0.03624911978840828, -0.1492934226989746, 0.04217666760087013, -0.1666657030582428, 0.15104995667934418, 0.13434746861457825, 0.07147064059972763, 0.023738205432891846, -0.04108646139502525]
[2025-05-12 06:38:48,949]: Mean: 0.00553496
[2025-05-12 06:38:48,949]: Min: -0.36404818
[2025-05-12 06:38:48,950]: Max: 0.62406880
[2025-05-12 06:38:48,950]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-12 06:38:48,951]: Sample Values (25 elements): [0.027949145063757896, 0.015843192115426064, -0.017052454873919487, 0.04513321444392204, -0.023977180942893028, -0.01732778362929821, -0.09204254299402237, 0.009928002953529358, 0.009628638625144958, 0.019450567662715912, -0.01123888324946165, -0.017296258360147476, -0.026200102642178535, -0.06191176176071167, 0.010303138755261898, -0.01963518187403679, 0.029212357476353645, -0.00772953312844038, 0.06450177729129791, -0.08683790266513824, 0.018256140872836113, -0.00957398395985365, 0.014923805370926857, -0.07496939599514008, -0.0003964231291320175]
[2025-05-12 06:38:48,952]: Mean: -0.00003612
[2025-05-12 06:38:48,952]: Min: -0.19077975
[2025-05-12 06:38:48,953]: Max: 0.21691133
[2025-05-12 06:38:48,953]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-12 06:38:48,954]: Sample Values (25 elements): [-0.005377328023314476, -0.031862836331129074, -0.07817196100950241, -0.038040418177843094, -0.06198064237833023, 0.07105092704296112, 0.03909462317824364, 0.12507566809654236, -0.050980985164642334, 0.06562233716249466, 0.05078193172812462, 0.018074462190270424, -0.027717305347323418, 0.038533467799425125, 0.00847369059920311, -0.09209397435188293, 0.06185029447078705, -0.08358961343765259, 0.059201430529356, 0.03139021247625351, 0.03883523866534233, -0.04347629472613335, 0.08766202628612518, 0.06167116016149521, 0.0041343956254422665]
[2025-05-12 06:38:48,954]: Mean: 0.00201992
[2025-05-12 06:38:48,955]: Min: -0.24933007
[2025-05-12 06:38:48,955]: Max: 0.22141090
[2025-05-12 06:38:48,955]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-12 06:38:48,956]: Sample Values (25 elements): [0.21729710698127747, 0.23384256660938263, -0.294635534286499, -0.04561162367463112, 0.23812618851661682, 0.04971623793244362, -0.13082921504974365, -0.039288949221372604, -0.015611719340085983, -0.11882337927818298, 0.012404268607497215, 0.11725462228059769, -0.08814913034439087, -0.14885102212429047, 0.2034429907798767, -0.1677415370941162, 0.002662255894392729, 0.16347636282444, -0.11118220537900925, -0.11493241041898727, 0.035447537899017334, 0.18902386724948883, -0.25300589203834534, -0.05584375932812691, -0.1318228393793106]
[2025-05-12 06:38:48,956]: Mean: 0.00016283
[2025-05-12 06:38:48,957]: Min: -0.41422865
[2025-05-12 06:38:48,957]: Max: 0.40649536
[2025-05-12 06:38:48,957]: 


QAT of LeNet5 with relu down to 4 bits...
[2025-05-12 06:38:49,051]: [LeNet5_relu_quantized_4_bits] after configure_qat:
[2025-05-12 06:38:49,126]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 06:39:23,611]: [LeNet5_relu_quantized_4_bits] Epoch: 001 Train Loss: 1.1628 Train Acc: 0.5856 Eval Loss: 1.0806 Eval Acc: 0.6184 (LR: 0.001000)
[2025-05-12 06:40:00,499]: [LeNet5_relu_quantized_4_bits] Epoch: 002 Train Loss: 1.1639 Train Acc: 0.5874 Eval Loss: 1.0745 Eval Acc: 0.6211 (LR: 0.001000)
[2025-05-12 06:40:37,647]: [LeNet5_relu_quantized_4_bits] Epoch: 003 Train Loss: 1.1637 Train Acc: 0.5896 Eval Loss: 1.0716 Eval Acc: 0.6181 (LR: 0.001000)
[2025-05-12 06:41:17,462]: [LeNet5_relu_quantized_4_bits] Epoch: 004 Train Loss: 1.1594 Train Acc: 0.5880 Eval Loss: 1.0739 Eval Acc: 0.6200 (LR: 0.001000)
[2025-05-12 06:41:57,366]: [LeNet5_relu_quantized_4_bits] Epoch: 005 Train Loss: 1.1600 Train Acc: 0.5852 Eval Loss: 1.0837 Eval Acc: 0.6158 (LR: 0.001000)
[2025-05-12 06:42:37,214]: [LeNet5_relu_quantized_4_bits] Epoch: 006 Train Loss: 1.1565 Train Acc: 0.5902 Eval Loss: 1.0570 Eval Acc: 0.6264 (LR: 0.001000)
[2025-05-12 06:43:17,192]: [LeNet5_relu_quantized_4_bits] Epoch: 007 Train Loss: 1.1536 Train Acc: 0.5890 Eval Loss: 1.0717 Eval Acc: 0.6227 (LR: 0.001000)
[2025-05-12 06:43:57,073]: [LeNet5_relu_quantized_4_bits] Epoch: 008 Train Loss: 1.1558 Train Acc: 0.5863 Eval Loss: 1.0841 Eval Acc: 0.6136 (LR: 0.001000)
[2025-05-12 06:44:37,445]: [LeNet5_relu_quantized_4_bits] Epoch: 009 Train Loss: 1.1604 Train Acc: 0.5869 Eval Loss: 1.0760 Eval Acc: 0.6155 (LR: 0.001000)
[2025-05-12 06:45:17,670]: [LeNet5_relu_quantized_4_bits] Epoch: 010 Train Loss: 1.1539 Train Acc: 0.5875 Eval Loss: 1.0798 Eval Acc: 0.6162 (LR: 0.001000)
[2025-05-12 06:45:58,176]: [LeNet5_relu_quantized_4_bits] Epoch: 011 Train Loss: 1.1477 Train Acc: 0.5922 Eval Loss: 1.0848 Eval Acc: 0.6100 (LR: 0.001000)
[2025-05-12 06:46:38,942]: [LeNet5_relu_quantized_4_bits] Epoch: 012 Train Loss: 1.1560 Train Acc: 0.5910 Eval Loss: 1.0805 Eval Acc: 0.6168 (LR: 0.001000)
[2025-05-12 06:47:19,380]: [LeNet5_relu_quantized_4_bits] Epoch: 013 Train Loss: 1.1496 Train Acc: 0.5919 Eval Loss: 1.0842 Eval Acc: 0.6195 (LR: 0.001000)
[2025-05-12 06:47:59,784]: [LeNet5_relu_quantized_4_bits] Epoch: 014 Train Loss: 1.1491 Train Acc: 0.5911 Eval Loss: 1.0712 Eval Acc: 0.6171 (LR: 0.001000)
[2025-05-12 06:48:40,619]: [LeNet5_relu_quantized_4_bits] Epoch: 015 Train Loss: 1.1417 Train Acc: 0.5945 Eval Loss: 1.0462 Eval Acc: 0.6293 (LR: 0.001000)
[2025-05-12 06:49:20,953]: [LeNet5_relu_quantized_4_bits] Epoch: 016 Train Loss: 1.1384 Train Acc: 0.5945 Eval Loss: 1.0620 Eval Acc: 0.6221 (LR: 0.001000)
[2025-05-12 06:50:01,799]: [LeNet5_relu_quantized_4_bits] Epoch: 017 Train Loss: 1.1384 Train Acc: 0.5928 Eval Loss: 1.0784 Eval Acc: 0.6180 (LR: 0.001000)
[2025-05-12 06:50:41,989]: [LeNet5_relu_quantized_4_bits] Epoch: 018 Train Loss: 1.1405 Train Acc: 0.5949 Eval Loss: 1.0703 Eval Acc: 0.6192 (LR: 0.001000)
[2025-05-12 06:51:23,179]: [LeNet5_relu_quantized_4_bits] Epoch: 019 Train Loss: 1.1390 Train Acc: 0.5942 Eval Loss: 1.0630 Eval Acc: 0.6226 (LR: 0.001000)
[2025-05-12 06:52:03,891]: [LeNet5_relu_quantized_4_bits] Epoch: 020 Train Loss: 1.1352 Train Acc: 0.5986 Eval Loss: 1.0608 Eval Acc: 0.6243 (LR: 0.001000)
[2025-05-12 06:52:45,760]: [LeNet5_relu_quantized_4_bits] Epoch: 021 Train Loss: 1.1401 Train Acc: 0.5955 Eval Loss: 1.0622 Eval Acc: 0.6295 (LR: 0.001000)
[2025-05-12 06:53:25,532]: [LeNet5_relu_quantized_4_bits] Epoch: 022 Train Loss: 1.1290 Train Acc: 0.5988 Eval Loss: 1.0494 Eval Acc: 0.6313 (LR: 0.001000)
[2025-05-12 06:54:05,256]: [LeNet5_relu_quantized_4_bits] Epoch: 023 Train Loss: 1.1275 Train Acc: 0.6017 Eval Loss: 1.0548 Eval Acc: 0.6259 (LR: 0.001000)
[2025-05-12 06:54:45,906]: [LeNet5_relu_quantized_4_bits] Epoch: 024 Train Loss: 1.1302 Train Acc: 0.6006 Eval Loss: 1.0539 Eval Acc: 0.6202 (LR: 0.001000)
[2025-05-12 06:55:26,083]: [LeNet5_relu_quantized_4_bits] Epoch: 025 Train Loss: 1.1306 Train Acc: 0.5978 Eval Loss: 1.0419 Eval Acc: 0.6291 (LR: 0.001000)
[2025-05-12 06:56:04,497]: [LeNet5_relu_quantized_4_bits] Epoch: 026 Train Loss: 1.1245 Train Acc: 0.5998 Eval Loss: 1.0510 Eval Acc: 0.6303 (LR: 0.001000)
[2025-05-12 06:56:42,458]: [LeNet5_relu_quantized_4_bits] Epoch: 027 Train Loss: 1.1214 Train Acc: 0.6019 Eval Loss: 1.0538 Eval Acc: 0.6240 (LR: 0.001000)
[2025-05-12 06:57:22,354]: [LeNet5_relu_quantized_4_bits] Epoch: 028 Train Loss: 1.1244 Train Acc: 0.5989 Eval Loss: 1.0545 Eval Acc: 0.6192 (LR: 0.001000)
[2025-05-12 06:58:00,774]: [LeNet5_relu_quantized_4_bits] Epoch: 029 Train Loss: 1.1211 Train Acc: 0.6032 Eval Loss: 1.0344 Eval Acc: 0.6330 (LR: 0.001000)
[2025-05-12 06:58:38,886]: [LeNet5_relu_quantized_4_bits] Epoch: 030 Train Loss: 1.1161 Train Acc: 0.6029 Eval Loss: 1.0584 Eval Acc: 0.6252 (LR: 0.000250)
[2025-05-12 06:59:15,600]: [LeNet5_relu_quantized_4_bits] Epoch: 031 Train Loss: 1.0868 Train Acc: 0.6134 Eval Loss: 1.0076 Eval Acc: 0.6426 (LR: 0.000250)
[2025-05-12 06:59:52,772]: [LeNet5_relu_quantized_4_bits] Epoch: 032 Train Loss: 1.0810 Train Acc: 0.6151 Eval Loss: 1.0090 Eval Acc: 0.6427 (LR: 0.000250)
[2025-05-12 07:00:30,425]: [LeNet5_relu_quantized_4_bits] Epoch: 033 Train Loss: 1.0891 Train Acc: 0.6119 Eval Loss: 1.0244 Eval Acc: 0.6355 (LR: 0.000250)
[2025-05-12 07:01:10,425]: [LeNet5_relu_quantized_4_bits] Epoch: 034 Train Loss: 1.0846 Train Acc: 0.6151 Eval Loss: 1.0160 Eval Acc: 0.6368 (LR: 0.000250)
[2025-05-12 07:01:48,777]: [LeNet5_relu_quantized_4_bits] Epoch: 035 Train Loss: 1.0896 Train Acc: 0.6140 Eval Loss: 1.0204 Eval Acc: 0.6345 (LR: 0.000250)
[2025-05-12 07:02:26,192]: [LeNet5_relu_quantized_4_bits] Epoch: 036 Train Loss: 1.0792 Train Acc: 0.6173 Eval Loss: 1.0162 Eval Acc: 0.6365 (LR: 0.000250)
[2025-05-12 07:03:07,073]: [LeNet5_relu_quantized_4_bits] Epoch: 037 Train Loss: 1.0828 Train Acc: 0.6147 Eval Loss: 1.0135 Eval Acc: 0.6407 (LR: 0.000250)
[2025-05-12 07:03:46,117]: [LeNet5_relu_quantized_4_bits] Epoch: 038 Train Loss: 1.0863 Train Acc: 0.6149 Eval Loss: 1.0080 Eval Acc: 0.6368 (LR: 0.000250)
[2025-05-12 07:04:25,156]: [LeNet5_relu_quantized_4_bits] Epoch: 039 Train Loss: 1.0871 Train Acc: 0.6122 Eval Loss: 1.0257 Eval Acc: 0.6334 (LR: 0.000250)
[2025-05-12 07:05:01,949]: [LeNet5_relu_quantized_4_bits] Epoch: 040 Train Loss: 1.0896 Train Acc: 0.6123 Eval Loss: 1.0002 Eval Acc: 0.6446 (LR: 0.000250)
[2025-05-12 07:05:37,268]: [LeNet5_relu_quantized_4_bits] Epoch: 041 Train Loss: 1.0858 Train Acc: 0.6130 Eval Loss: 1.0225 Eval Acc: 0.6376 (LR: 0.000250)
[2025-05-12 07:06:12,483]: [LeNet5_relu_quantized_4_bits] Epoch: 042 Train Loss: 1.0827 Train Acc: 0.6156 Eval Loss: 1.0118 Eval Acc: 0.6370 (LR: 0.000250)
[2025-05-12 07:06:47,575]: [LeNet5_relu_quantized_4_bits] Epoch: 043 Train Loss: 1.0849 Train Acc: 0.6146 Eval Loss: 1.0254 Eval Acc: 0.6389 (LR: 0.000250)
[2025-05-12 07:07:22,830]: [LeNet5_relu_quantized_4_bits] Epoch: 044 Train Loss: 1.0835 Train Acc: 0.6172 Eval Loss: 1.0351 Eval Acc: 0.6327 (LR: 0.000250)
[2025-05-12 07:07:58,114]: [LeNet5_relu_quantized_4_bits] Epoch: 045 Train Loss: 1.0865 Train Acc: 0.6135 Eval Loss: 1.0229 Eval Acc: 0.6363 (LR: 0.000063)
[2025-05-12 07:08:33,417]: [LeNet5_relu_quantized_4_bits] Epoch: 046 Train Loss: 1.0667 Train Acc: 0.6200 Eval Loss: 1.0092 Eval Acc: 0.6399 (LR: 0.000063)
[2025-05-12 07:09:09,014]: [LeNet5_relu_quantized_4_bits] Epoch: 047 Train Loss: 1.0650 Train Acc: 0.6208 Eval Loss: 0.9942 Eval Acc: 0.6447 (LR: 0.000063)
[2025-05-12 07:09:47,274]: [LeNet5_relu_quantized_4_bits] Epoch: 048 Train Loss: 1.0672 Train Acc: 0.6204 Eval Loss: 1.0050 Eval Acc: 0.6467 (LR: 0.000063)
[2025-05-12 07:10:27,897]: [LeNet5_relu_quantized_4_bits] Epoch: 049 Train Loss: 1.0693 Train Acc: 0.6208 Eval Loss: 0.9996 Eval Acc: 0.6447 (LR: 0.000063)
[2025-05-12 07:11:07,734]: [LeNet5_relu_quantized_4_bits] Epoch: 050 Train Loss: 1.0660 Train Acc: 0.6217 Eval Loss: 1.0092 Eval Acc: 0.6389 (LR: 0.000063)
[2025-05-12 07:11:48,327]: [LeNet5_relu_quantized_4_bits] Epoch: 051 Train Loss: 1.0693 Train Acc: 0.6196 Eval Loss: 1.0067 Eval Acc: 0.6425 (LR: 0.000063)
[2025-05-12 07:12:28,427]: [LeNet5_relu_quantized_4_bits] Epoch: 052 Train Loss: 1.0647 Train Acc: 0.6208 Eval Loss: 0.9946 Eval Acc: 0.6454 (LR: 0.000063)
[2025-05-12 07:13:08,314]: [LeNet5_relu_quantized_4_bits] Epoch: 053 Train Loss: 1.0648 Train Acc: 0.6223 Eval Loss: 1.0139 Eval Acc: 0.6398 (LR: 0.000063)
[2025-05-12 07:13:48,194]: [LeNet5_relu_quantized_4_bits] Epoch: 054 Train Loss: 1.0669 Train Acc: 0.6230 Eval Loss: 1.0155 Eval Acc: 0.6381 (LR: 0.000063)
[2025-05-12 07:14:27,744]: [LeNet5_relu_quantized_4_bits] Epoch: 055 Train Loss: 1.0619 Train Acc: 0.6245 Eval Loss: 1.0222 Eval Acc: 0.6370 (LR: 0.000063)
[2025-05-12 07:15:07,350]: [LeNet5_relu_quantized_4_bits] Epoch: 056 Train Loss: 1.0731 Train Acc: 0.6182 Eval Loss: 1.0133 Eval Acc: 0.6391 (LR: 0.000063)
[2025-05-12 07:15:46,841]: [LeNet5_relu_quantized_4_bits] Epoch: 057 Train Loss: 1.0654 Train Acc: 0.6218 Eval Loss: 1.0014 Eval Acc: 0.6438 (LR: 0.000063)
[2025-05-12 07:16:26,080]: [LeNet5_relu_quantized_4_bits] Epoch: 058 Train Loss: 1.0690 Train Acc: 0.6229 Eval Loss: 1.0129 Eval Acc: 0.6371 (LR: 0.000063)
[2025-05-12 07:17:04,526]: [LeNet5_relu_quantized_4_bits] Epoch: 059 Train Loss: 1.0691 Train Acc: 0.6192 Eval Loss: 0.9950 Eval Acc: 0.6478 (LR: 0.000063)
[2025-05-12 07:17:43,093]: [LeNet5_relu_quantized_4_bits] Epoch: 060 Train Loss: 1.0721 Train Acc: 0.6184 Eval Loss: 1.0035 Eval Acc: 0.6413 (LR: 0.000063)
[2025-05-12 07:17:43,094]: [LeNet5_relu_quantized_4_bits] Best Eval Accuracy: 0.6478
[2025-05-12 07:17:43,118]: 


Quantization of model down to 4 bits finished
[2025-05-12 07:17:43,118]: Model Architecture:
[2025-05-12 07:17:43,155]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6725], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.0872745513916)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0832], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4577060341835022, max_val=0.7909167408943176)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3171], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=34.75596618652344)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0298], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21157445013523102, max_val=0.23474207520484924)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9047], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=28.57065773010254)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0312], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2503741681575775, max_val=0.21824336051940918)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0229], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.34362506866455)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 07:17:43,155]: 
Model Weights:
[2025-05-12 07:17:43,155]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-12 07:17:43,156]: Sample Values (25 elements): [0.16280074417591095, 0.08688687533140182, 0.14561985433101654, -0.09437260776758194, 0.2865743637084961, -0.02165982313454151, -0.8022021055221558, -0.2049316167831421, -0.06054339557886124, 0.06097853183746338, -0.04527655243873596, -0.032109878957271576, 0.15703384578227997, 0.006955984979867935, 0.273346483707428, 0.11047106236219406, 0.0634666159749031, 0.4992983937263489, 0.01939440704882145, 0.386120080947876, 0.2692859172821045, -0.4457501173019409, -0.19591543078422546, -0.09412292391061783, -0.009221200831234455]
[2025-05-12 07:17:43,156]: Mean: 0.00069502
[2025-05-12 07:17:43,157]: Min: -0.80220211
[2025-05-12 07:17:43,157]: Max: 0.88761693
[2025-05-12 07:17:43,160]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-12 07:17:43,161]: Sample Values (25 elements): [0.0, -0.08324162662029266, 0.0, 0.0, 0.0, 0.08324162662029266, 0.0, -0.08324162662029266, -0.08324162662029266, 0.0, 0.0, 0.16648325324058533, 0.249724879860878, 0.0, 0.16648325324058533, -0.08324162662029266, -0.08324162662029266, 0.0, -0.08324162662029266, -0.08324162662029266, 0.08324162662029266, -0.08324162662029266, 0.0, 0.08324162662029266, 0.16648325324058533]
[2025-05-12 07:17:43,162]: Mean: 0.00613907
[2025-05-12 07:17:43,162]: Min: -0.41620815
[2025-05-12 07:17:43,162]: Max: 0.83241630
[2025-05-12 07:17:43,164]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-12 07:17:43,166]: Sample Values (25 elements): [0.0, 0.02975434623658657, 0.0, 0.02975434623658657, 0.0, -0.02975434623658657, 0.05950869247317314, 0.02975434623658657, 0.05950869247317314, 0.05950869247317314, 0.0, 0.08926303684711456, 0.0, -0.02975434623658657, 0.02975434623658657, -0.05950869247317314, -0.02975434623658657, 0.0, -0.02975434623658657, 0.0, 0.0, 0.02975434623658657, 0.05950869247317314, 0.02975434623658657, -0.02975434623658657]
[2025-05-12 07:17:43,167]: Mean: -0.00012894
[2025-05-12 07:17:43,167]: Min: -0.20828043
[2025-05-12 07:17:43,167]: Max: 0.23803477
[2025-05-12 07:17:43,170]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-12 07:17:43,171]: Sample Values (25 elements): [0.0, -0.09372343122959137, 0.09372343122959137, -0.06248228996992111, -0.06248228996992111, 0.0, 0.0, -0.06248228996992111, -0.09372343122959137, -0.031241144984960556, -0.06248228996992111, 0.06248228996992111, 0.0, 0.09372343122959137, -0.06248228996992111, -0.06248228996992111, -0.031241144984960556, -0.031241144984960556, 0.06248228996992111, -0.031241144984960556, -0.06248228996992111, 0.0, 0.031241144984960556, 0.031241144984960556, 0.06248228996992111]
[2025-05-12 07:17:43,171]: Mean: 0.00058887
[2025-05-12 07:17:43,173]: Min: -0.24992916
[2025-05-12 07:17:43,174]: Max: 0.21868801
[2025-05-12 07:17:43,174]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-12 07:17:43,175]: Sample Values (25 elements): [0.06330979615449905, -0.20260588824748993, -0.09686599671840668, -0.013714340515434742, -0.027996938675642014, 0.09863214194774628, -0.154192715883255, -0.09610136598348618, 0.026882633566856384, 0.016412967815995216, 0.08796175569295883, 0.09474802017211914, 0.16789136826992035, 0.08776941895484924, -0.024750156328082085, 0.04327183961868286, -0.11337168514728546, 0.04612107574939728, -0.060000229626894, -0.12352019548416138, -0.0345831923186779, -0.05044018477201462, 0.1062745526432991, 0.00014677870785817504, 0.05701703950762749]
[2025-05-12 07:17:43,176]: Mean: 0.00016061
[2025-05-12 07:17:43,176]: Min: -0.26476678
[2025-05-12 07:17:43,176]: Max: 0.30726126
[2025-05-12 07:17:43,176]: 


QAT of LeNet5 with relu down to 3 bits...
[2025-05-12 07:17:43,227]: [LeNet5_relu_quantized_3_bits] after configure_qat:
[2025-05-12 07:17:43,248]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 07:18:22,166]: [LeNet5_relu_quantized_3_bits] Epoch: 001 Train Loss: 1.4132 Train Acc: 0.5049 Eval Loss: 1.2928 Eval Acc: 0.5414 (LR: 0.001000)
[2025-05-12 07:19:00,933]: [LeNet5_relu_quantized_3_bits] Epoch: 002 Train Loss: 1.3783 Train Acc: 0.5097 Eval Loss: 1.2714 Eval Acc: 0.5445 (LR: 0.001000)
[2025-05-12 07:19:39,747]: [LeNet5_relu_quantized_3_bits] Epoch: 003 Train Loss: 1.3609 Train Acc: 0.5134 Eval Loss: 1.2693 Eval Acc: 0.5434 (LR: 0.001000)
[2025-05-12 07:20:18,182]: [LeNet5_relu_quantized_3_bits] Epoch: 004 Train Loss: 1.3662 Train Acc: 0.5134 Eval Loss: 1.2514 Eval Acc: 0.5481 (LR: 0.001000)
[2025-05-12 07:20:54,412]: [LeNet5_relu_quantized_3_bits] Epoch: 005 Train Loss: 1.3640 Train Acc: 0.5140 Eval Loss: 1.2561 Eval Acc: 0.5512 (LR: 0.001000)
[2025-05-12 07:21:28,840]: [LeNet5_relu_quantized_3_bits] Epoch: 006 Train Loss: 1.3517 Train Acc: 0.5171 Eval Loss: 1.2572 Eval Acc: 0.5558 (LR: 0.001000)
[2025-05-12 07:22:03,663]: [LeNet5_relu_quantized_3_bits] Epoch: 007 Train Loss: 1.3454 Train Acc: 0.5194 Eval Loss: 1.2527 Eval Acc: 0.5565 (LR: 0.001000)
[2025-05-12 07:22:38,912]: [LeNet5_relu_quantized_3_bits] Epoch: 008 Train Loss: 1.3524 Train Acc: 0.5179 Eval Loss: 1.2874 Eval Acc: 0.5387 (LR: 0.001000)
[2025-05-12 07:23:13,574]: [LeNet5_relu_quantized_3_bits] Epoch: 009 Train Loss: 1.3449 Train Acc: 0.5166 Eval Loss: 1.2914 Eval Acc: 0.5382 (LR: 0.001000)
[2025-05-12 07:23:48,592]: [LeNet5_relu_quantized_3_bits] Epoch: 010 Train Loss: 1.3453 Train Acc: 0.5199 Eval Loss: 1.2609 Eval Acc: 0.5493 (LR: 0.001000)
[2025-05-12 07:24:23,510]: [LeNet5_relu_quantized_3_bits] Epoch: 011 Train Loss: 1.3385 Train Acc: 0.5181 Eval Loss: 1.2610 Eval Acc: 0.5454 (LR: 0.001000)
[2025-05-12 07:24:58,636]: [LeNet5_relu_quantized_3_bits] Epoch: 012 Train Loss: 1.3393 Train Acc: 0.5214 Eval Loss: 1.2463 Eval Acc: 0.5537 (LR: 0.001000)
[2025-05-12 07:25:33,243]: [LeNet5_relu_quantized_3_bits] Epoch: 013 Train Loss: 1.3476 Train Acc: 0.5174 Eval Loss: 1.2423 Eval Acc: 0.5538 (LR: 0.001000)
[2025-05-12 07:26:08,018]: [LeNet5_relu_quantized_3_bits] Epoch: 014 Train Loss: 1.3307 Train Acc: 0.5260 Eval Loss: 1.2775 Eval Acc: 0.5385 (LR: 0.001000)
[2025-05-12 07:26:42,457]: [LeNet5_relu_quantized_3_bits] Epoch: 015 Train Loss: 1.3312 Train Acc: 0.5241 Eval Loss: 1.2928 Eval Acc: 0.5294 (LR: 0.001000)
[2025-05-12 07:27:17,574]: [LeNet5_relu_quantized_3_bits] Epoch: 016 Train Loss: 1.3308 Train Acc: 0.5219 Eval Loss: 1.3058 Eval Acc: 0.5380 (LR: 0.001000)
[2025-05-12 07:27:52,247]: [LeNet5_relu_quantized_3_bits] Epoch: 017 Train Loss: 1.3246 Train Acc: 0.5242 Eval Loss: 1.2726 Eval Acc: 0.5485 (LR: 0.001000)
[2025-05-12 07:28:27,231]: [LeNet5_relu_quantized_3_bits] Epoch: 018 Train Loss: 1.3214 Train Acc: 0.5279 Eval Loss: 1.2396 Eval Acc: 0.5528 (LR: 0.001000)
[2025-05-12 07:29:02,892]: [LeNet5_relu_quantized_3_bits] Epoch: 019 Train Loss: 1.3255 Train Acc: 0.5254 Eval Loss: 1.2301 Eval Acc: 0.5635 (LR: 0.001000)
[2025-05-12 07:29:38,079]: [LeNet5_relu_quantized_3_bits] Epoch: 020 Train Loss: 1.3180 Train Acc: 0.5290 Eval Loss: 1.2538 Eval Acc: 0.5504 (LR: 0.001000)
[2025-05-12 07:30:13,083]: [LeNet5_relu_quantized_3_bits] Epoch: 021 Train Loss: 1.3195 Train Acc: 0.5266 Eval Loss: 1.2374 Eval Acc: 0.5558 (LR: 0.001000)
[2025-05-12 07:30:47,948]: [LeNet5_relu_quantized_3_bits] Epoch: 022 Train Loss: 1.3183 Train Acc: 0.5301 Eval Loss: 1.2530 Eval Acc: 0.5504 (LR: 0.001000)
[2025-05-12 07:31:22,931]: [LeNet5_relu_quantized_3_bits] Epoch: 023 Train Loss: 1.3164 Train Acc: 0.5285 Eval Loss: 1.2840 Eval Acc: 0.5358 (LR: 0.001000)
[2025-05-12 07:31:57,899]: [LeNet5_relu_quantized_3_bits] Epoch: 024 Train Loss: 1.3207 Train Acc: 0.5271 Eval Loss: 1.2422 Eval Acc: 0.5602 (LR: 0.001000)
[2025-05-12 07:32:32,792]: [LeNet5_relu_quantized_3_bits] Epoch: 025 Train Loss: 1.3213 Train Acc: 0.5264 Eval Loss: 1.2406 Eval Acc: 0.5544 (LR: 0.001000)
[2025-05-12 07:33:07,790]: [LeNet5_relu_quantized_3_bits] Epoch: 026 Train Loss: 1.3189 Train Acc: 0.5285 Eval Loss: 1.2200 Eval Acc: 0.5682 (LR: 0.001000)
[2025-05-12 07:33:42,831]: [LeNet5_relu_quantized_3_bits] Epoch: 027 Train Loss: 1.3164 Train Acc: 0.5293 Eval Loss: 1.2486 Eval Acc: 0.5556 (LR: 0.001000)
[2025-05-12 07:34:17,662]: [LeNet5_relu_quantized_3_bits] Epoch: 028 Train Loss: 1.3142 Train Acc: 0.5281 Eval Loss: 1.2302 Eval Acc: 0.5519 (LR: 0.001000)
[2025-05-12 07:34:52,790]: [LeNet5_relu_quantized_3_bits] Epoch: 029 Train Loss: 1.3207 Train Acc: 0.5261 Eval Loss: 1.2708 Eval Acc: 0.5467 (LR: 0.001000)
[2025-05-12 07:35:27,879]: [LeNet5_relu_quantized_3_bits] Epoch: 030 Train Loss: 1.3152 Train Acc: 0.5285 Eval Loss: 1.2615 Eval Acc: 0.5508 (LR: 0.000250)
[2025-05-12 07:36:03,192]: [LeNet5_relu_quantized_3_bits] Epoch: 031 Train Loss: 1.2731 Train Acc: 0.5447 Eval Loss: 1.1998 Eval Acc: 0.5732 (LR: 0.000250)
[2025-05-12 07:36:38,337]: [LeNet5_relu_quantized_3_bits] Epoch: 032 Train Loss: 1.2733 Train Acc: 0.5469 Eval Loss: 1.2152 Eval Acc: 0.5635 (LR: 0.000250)
[2025-05-12 07:37:13,410]: [LeNet5_relu_quantized_3_bits] Epoch: 033 Train Loss: 1.2740 Train Acc: 0.5403 Eval Loss: 1.2097 Eval Acc: 0.5670 (LR: 0.000250)
[2025-05-12 07:37:49,034]: [LeNet5_relu_quantized_3_bits] Epoch: 034 Train Loss: 1.2763 Train Acc: 0.5422 Eval Loss: 1.2211 Eval Acc: 0.5617 (LR: 0.000250)
[2025-05-12 07:38:24,036]: [LeNet5_relu_quantized_3_bits] Epoch: 035 Train Loss: 1.2796 Train Acc: 0.5405 Eval Loss: 1.2052 Eval Acc: 0.5647 (LR: 0.000250)
[2025-05-12 07:38:59,666]: [LeNet5_relu_quantized_3_bits] Epoch: 036 Train Loss: 1.2839 Train Acc: 0.5400 Eval Loss: 1.2087 Eval Acc: 0.5687 (LR: 0.000250)
[2025-05-12 07:39:35,429]: [LeNet5_relu_quantized_3_bits] Epoch: 037 Train Loss: 1.2845 Train Acc: 0.5375 Eval Loss: 1.2275 Eval Acc: 0.5658 (LR: 0.000250)
[2025-05-12 07:40:10,834]: [LeNet5_relu_quantized_3_bits] Epoch: 038 Train Loss: 1.2856 Train Acc: 0.5390 Eval Loss: 1.2041 Eval Acc: 0.5668 (LR: 0.000250)
[2025-05-12 07:40:46,351]: [LeNet5_relu_quantized_3_bits] Epoch: 039 Train Loss: 1.2878 Train Acc: 0.5398 Eval Loss: 1.2209 Eval Acc: 0.5630 (LR: 0.000250)
[2025-05-12 07:41:22,057]: [LeNet5_relu_quantized_3_bits] Epoch: 040 Train Loss: 1.2890 Train Acc: 0.5400 Eval Loss: 1.2193 Eval Acc: 0.5662 (LR: 0.000250)
[2025-05-12 07:41:57,676]: [LeNet5_relu_quantized_3_bits] Epoch: 041 Train Loss: 1.2940 Train Acc: 0.5379 Eval Loss: 1.2169 Eval Acc: 0.5665 (LR: 0.000250)
[2025-05-12 07:42:33,046]: [LeNet5_relu_quantized_3_bits] Epoch: 042 Train Loss: 1.2957 Train Acc: 0.5348 Eval Loss: 1.2206 Eval Acc: 0.5658 (LR: 0.000250)
[2025-05-12 07:43:08,785]: [LeNet5_relu_quantized_3_bits] Epoch: 043 Train Loss: 1.2918 Train Acc: 0.5352 Eval Loss: 1.2447 Eval Acc: 0.5560 (LR: 0.000250)
[2025-05-12 07:43:44,845]: [LeNet5_relu_quantized_3_bits] Epoch: 044 Train Loss: 1.2904 Train Acc: 0.5396 Eval Loss: 1.2163 Eval Acc: 0.5678 (LR: 0.000250)
[2025-05-12 07:44:20,908]: [LeNet5_relu_quantized_3_bits] Epoch: 045 Train Loss: 1.2992 Train Acc: 0.5355 Eval Loss: 1.2267 Eval Acc: 0.5670 (LR: 0.000063)
[2025-05-12 07:44:57,095]: [LeNet5_relu_quantized_3_bits] Epoch: 046 Train Loss: 1.2722 Train Acc: 0.5442 Eval Loss: 1.2017 Eval Acc: 0.5710 (LR: 0.000063)
[2025-05-12 07:45:32,640]: [LeNet5_relu_quantized_3_bits] Epoch: 047 Train Loss: 1.2732 Train Acc: 0.5453 Eval Loss: 1.1954 Eval Acc: 0.5778 (LR: 0.000063)
[2025-05-12 07:46:08,837]: [LeNet5_relu_quantized_3_bits] Epoch: 048 Train Loss: 1.2751 Train Acc: 0.5437 Eval Loss: 1.1982 Eval Acc: 0.5728 (LR: 0.000063)
[2025-05-12 07:46:44,946]: [LeNet5_relu_quantized_3_bits] Epoch: 049 Train Loss: 1.2784 Train Acc: 0.5428 Eval Loss: 1.1920 Eval Acc: 0.5703 (LR: 0.000063)
[2025-05-12 07:47:21,175]: [LeNet5_relu_quantized_3_bits] Epoch: 050 Train Loss: 1.2751 Train Acc: 0.5445 Eval Loss: 1.2173 Eval Acc: 0.5642 (LR: 0.000063)
[2025-05-12 07:47:57,037]: [LeNet5_relu_quantized_3_bits] Epoch: 051 Train Loss: 1.2744 Train Acc: 0.5433 Eval Loss: 1.2068 Eval Acc: 0.5660 (LR: 0.000063)
[2025-05-12 07:48:33,158]: [LeNet5_relu_quantized_3_bits] Epoch: 052 Train Loss: 1.2843 Train Acc: 0.5388 Eval Loss: 1.2301 Eval Acc: 0.5615 (LR: 0.000063)
[2025-05-12 07:49:09,224]: [LeNet5_relu_quantized_3_bits] Epoch: 053 Train Loss: 1.2802 Train Acc: 0.5392 Eval Loss: 1.2258 Eval Acc: 0.5595 (LR: 0.000063)
[2025-05-12 07:49:44,841]: [LeNet5_relu_quantized_3_bits] Epoch: 054 Train Loss: 1.2847 Train Acc: 0.5407 Eval Loss: 1.2266 Eval Acc: 0.5605 (LR: 0.000063)
[2025-05-12 07:50:20,486]: [LeNet5_relu_quantized_3_bits] Epoch: 055 Train Loss: 1.2853 Train Acc: 0.5391 Eval Loss: 1.2320 Eval Acc: 0.5657 (LR: 0.000063)
[2025-05-12 07:50:56,503]: [LeNet5_relu_quantized_3_bits] Epoch: 056 Train Loss: 1.2861 Train Acc: 0.5388 Eval Loss: 1.2139 Eval Acc: 0.5650 (LR: 0.000063)
[2025-05-12 07:51:32,938]: [LeNet5_relu_quantized_3_bits] Epoch: 057 Train Loss: 1.2844 Train Acc: 0.5407 Eval Loss: 1.2163 Eval Acc: 0.5643 (LR: 0.000063)
[2025-05-12 07:52:09,639]: [LeNet5_relu_quantized_3_bits] Epoch: 058 Train Loss: 1.2790 Train Acc: 0.5410 Eval Loss: 1.2131 Eval Acc: 0.5635 (LR: 0.000063)
[2025-05-12 07:52:46,386]: [LeNet5_relu_quantized_3_bits] Epoch: 059 Train Loss: 1.2808 Train Acc: 0.5416 Eval Loss: 1.2171 Eval Acc: 0.5636 (LR: 0.000063)
[2025-05-12 07:53:24,837]: [LeNet5_relu_quantized_3_bits] Epoch: 060 Train Loss: 1.2931 Train Acc: 0.5363 Eval Loss: 1.2121 Eval Acc: 0.5669 (LR: 0.000063)
[2025-05-12 07:53:24,838]: [LeNet5_relu_quantized_3_bits] Best Eval Accuracy: 0.5778
[2025-05-12 07:53:24,856]: 


Quantization of model down to 3 bits finished
[2025-05-12 07:53:24,856]: Model Architecture:
[2025-05-12 07:53:24,871]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.9001], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=41.300697326660156)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1684], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4209108054637909, max_val=0.7575581669807434)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([17.1843], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=120.29031372070312)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0671], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2184276431798935, max_val=0.25150883197784424)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([15.4880], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=108.4157943725586)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0637], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2248087227344513, max_val=0.22084924578666687)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.3687], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=58.5806884765625)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 07:53:24,871]: 
Model Weights:
[2025-05-12 07:53:24,871]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-12 07:53:24,871]: Sample Values (25 elements): [0.08383917808532715, -0.020901218056678772, -0.327935129404068, 0.28873389959335327, 0.022731294855475426, 0.05818969011306763, 0.04598987475037575, 0.09287914633750916, -0.13543321192264557, -0.3648499846458435, 0.4327223300933838, 0.578902006149292, -0.15992379188537598, -0.36438897252082825, 0.09308034181594849, -0.2829110622406006, -0.047960784286260605, -0.25445565581321716, 0.10915791988372803, 0.028830215334892273, -0.22756649553775787, -0.7565008997917175, -0.19047881662845612, -0.06734661012887955, -0.07022655755281448]
[2025-05-12 07:53:24,872]: Mean: 0.00227773
[2025-05-12 07:53:24,872]: Min: -0.94642591
[2025-05-12 07:53:24,872]: Max: 1.05460286
[2025-05-12 07:53:24,874]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-12 07:53:24,875]: Sample Values (25 elements): [0.0, 0.16835281252861023, 0.33670562505722046, 0.0, 0.16835281252861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16835281252861023, 0.0, 0.16835281252861023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16835281252861023, 0.16835281252861023, 0.16835281252861023, 0.0, -0.16835281252861023, -0.16835281252861023, 0.0]
[2025-05-12 07:53:24,875]: Mean: 0.01978146
[2025-05-12 07:53:24,875]: Min: -0.50505841
[2025-05-12 07:53:24,875]: Max: 0.67341125
[2025-05-12 07:53:24,877]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-12 07:53:24,878]: Sample Values (25 elements): [0.06713393330574036, 0.0, -0.06713393330574036, 0.0, 0.06713393330574036, 0.0, 0.0, 0.0, 0.06713393330574036, 0.06713393330574036, 0.0, -0.1342678666114807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06713393330574036, 0.0, 0.0, 0.06713393330574036]
[2025-05-12 07:53:24,878]: Mean: 0.00033147
[2025-05-12 07:53:24,878]: Min: -0.20140180
[2025-05-12 07:53:24,879]: Max: 0.26853573
[2025-05-12 07:53:24,881]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-12 07:53:24,882]: Sample Values (25 elements): [0.0, -0.063665471971035, -0.063665471971035, -0.063665471971035, 0.19099640846252441, 0.063665471971035, 0.063665471971035, 0.0, -0.063665471971035, -0.12733094394207, -0.12733094394207, 0.063665471971035, 0.0, -0.063665471971035, 0.0, 0.0, 0.0, 0.063665471971035, 0.063665471971035, 0.063665471971035, 0.0, 0.063665471971035, 0.0, 0.0, 0.0]
[2025-05-12 07:53:24,882]: Mean: -0.00020211
[2025-05-12 07:53:24,882]: Min: -0.25466189
[2025-05-12 07:53:24,882]: Max: 0.19099641
[2025-05-12 07:53:24,882]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-12 07:53:24,883]: Sample Values (25 elements): [-0.02102183923125267, 0.044796645641326904, 0.010505573824048042, -0.05680590122938156, 0.010485456325113773, 0.03665262460708618, -0.019389567896723747, 0.024042310193181038, 0.03186771646142006, 0.03925393521785736, 0.013568420894443989, -0.03524075821042061, -0.020121799781918526, 0.024517063051462173, 0.034540221095085144, 0.011291120201349258, 0.022083628922700882, -0.005770387127995491, -0.0009025200852192938, 0.029682589694857597, -0.0016413500998169184, -0.02792159654200077, 0.022907091304659843, -0.006652540527284145, 0.007332765497267246]
[2025-05-12 07:53:24,883]: Mean: 0.00016059
[2025-05-12 07:53:24,884]: Min: -0.13240689
[2025-05-12 07:53:24,884]: Max: 0.08215365
[2025-05-12 07:53:24,884]: 


QAT of LeNet5 with relu down to 2 bits...
[2025-05-12 07:53:24,914]: [LeNet5_relu_quantized_2_bits] after configure_qat:
[2025-05-12 07:53:24,929]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 07:54:02,928]: [LeNet5_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.1747 Train Acc: 0.2917 Eval Loss: 1.9150 Eval Acc: 0.3446 (LR: 0.001000)
[2025-05-12 07:54:40,240]: [LeNet5_relu_quantized_2_bits] Epoch: 002 Train Loss: 2.0099 Train Acc: 0.3194 Eval Loss: 1.8356 Eval Acc: 0.3544 (LR: 0.001000)
[2025-05-12 07:55:22,565]: [LeNet5_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.9143 Train Acc: 0.3383 Eval Loss: 1.8134 Eval Acc: 0.3745 (LR: 0.001000)
[2025-05-12 07:56:03,340]: [LeNet5_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.8621 Train Acc: 0.3459 Eval Loss: 1.7332 Eval Acc: 0.3847 (LR: 0.001000)
[2025-05-12 07:56:40,312]: [LeNet5_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.8333 Train Acc: 0.3480 Eval Loss: 1.7930 Eval Acc: 0.3657 (LR: 0.001000)
[2025-05-12 07:57:16,786]: [LeNet5_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.8000 Train Acc: 0.3548 Eval Loss: 1.7029 Eval Acc: 0.3880 (LR: 0.001000)
[2025-05-12 07:57:53,603]: [LeNet5_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.7884 Train Acc: 0.3501 Eval Loss: 1.6883 Eval Acc: 0.3906 (LR: 0.001000)
[2025-05-12 07:58:29,922]: [LeNet5_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.7733 Train Acc: 0.3561 Eval Loss: 1.7179 Eval Acc: 0.3803 (LR: 0.001000)
[2025-05-12 07:59:07,173]: [LeNet5_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.7896 Train Acc: 0.3509 Eval Loss: 1.7068 Eval Acc: 0.3844 (LR: 0.001000)
[2025-05-12 07:59:45,773]: [LeNet5_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.7818 Train Acc: 0.3531 Eval Loss: 1.7468 Eval Acc: 0.3706 (LR: 0.001000)
[2025-05-12 08:00:24,330]: [LeNet5_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.8014 Train Acc: 0.3450 Eval Loss: 1.7109 Eval Acc: 0.3849 (LR: 0.001000)
[2025-05-12 08:01:03,190]: [LeNet5_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.7920 Train Acc: 0.3475 Eval Loss: 1.7157 Eval Acc: 0.3829 (LR: 0.001000)
[2025-05-12 08:01:40,848]: [LeNet5_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.7972 Train Acc: 0.3466 Eval Loss: 1.7220 Eval Acc: 0.3857 (LR: 0.001000)
[2025-05-12 08:02:18,961]: [LeNet5_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.8156 Train Acc: 0.3462 Eval Loss: 1.7309 Eval Acc: 0.3661 (LR: 0.001000)
[2025-05-12 08:02:55,598]: [LeNet5_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.8243 Train Acc: 0.3364 Eval Loss: 1.7494 Eval Acc: 0.3724 (LR: 0.001000)
[2025-05-12 08:03:31,886]: [LeNet5_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.8110 Train Acc: 0.3466 Eval Loss: 1.6852 Eval Acc: 0.3941 (LR: 0.001000)
[2025-05-12 08:04:09,512]: [LeNet5_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.8111 Train Acc: 0.3421 Eval Loss: 1.7682 Eval Acc: 0.3551 (LR: 0.001000)
[2025-05-12 08:04:46,033]: [LeNet5_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.8026 Train Acc: 0.3470 Eval Loss: 1.6901 Eval Acc: 0.3905 (LR: 0.001000)
[2025-05-12 08:05:21,483]: [LeNet5_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.8038 Train Acc: 0.3456 Eval Loss: 1.7731 Eval Acc: 0.3673 (LR: 0.001000)
[2025-05-12 08:05:57,007]: [LeNet5_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.7844 Train Acc: 0.3542 Eval Loss: 1.7123 Eval Acc: 0.3952 (LR: 0.001000)
[2025-05-12 08:06:32,272]: [LeNet5_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.8006 Train Acc: 0.3487 Eval Loss: 1.7222 Eval Acc: 0.3747 (LR: 0.001000)
[2025-05-12 08:07:07,704]: [LeNet5_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.7916 Train Acc: 0.3505 Eval Loss: 1.7065 Eval Acc: 0.3770 (LR: 0.001000)
[2025-05-12 08:07:43,083]: [LeNet5_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.8029 Train Acc: 0.3424 Eval Loss: 1.7400 Eval Acc: 0.3627 (LR: 0.001000)
[2025-05-12 08:08:19,718]: [LeNet5_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.8228 Train Acc: 0.3388 Eval Loss: 1.7830 Eval Acc: 0.3478 (LR: 0.001000)
[2025-05-12 08:08:57,331]: [LeNet5_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.8468 Train Acc: 0.3320 Eval Loss: 1.8143 Eval Acc: 0.3429 (LR: 0.001000)
[2025-05-12 08:09:36,519]: [LeNet5_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.8626 Train Acc: 0.3251 Eval Loss: 1.7803 Eval Acc: 0.3605 (LR: 0.001000)
[2025-05-12 08:10:13,187]: [LeNet5_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.8685 Train Acc: 0.3280 Eval Loss: 1.7906 Eval Acc: 0.3574 (LR: 0.001000)
[2025-05-12 08:10:49,411]: [LeNet5_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.8665 Train Acc: 0.3285 Eval Loss: 1.7940 Eval Acc: 0.3610 (LR: 0.001000)
[2025-05-12 08:11:24,477]: [LeNet5_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.8802 Train Acc: 0.3267 Eval Loss: 1.8401 Eval Acc: 0.3331 (LR: 0.001000)
[2025-05-12 08:11:59,313]: [LeNet5_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.8808 Train Acc: 0.3259 Eval Loss: 1.8865 Eval Acc: 0.3316 (LR: 0.000250)
[2025-05-12 08:12:34,458]: [LeNet5_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.7779 Train Acc: 0.3495 Eval Loss: 1.7213 Eval Acc: 0.3719 (LR: 0.000250)
[2025-05-12 08:13:09,383]: [LeNet5_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.7875 Train Acc: 0.3488 Eval Loss: 1.7114 Eval Acc: 0.3815 (LR: 0.000250)
[2025-05-12 08:13:44,430]: [LeNet5_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.7917 Train Acc: 0.3453 Eval Loss: 1.7445 Eval Acc: 0.3642 (LR: 0.000250)
[2025-05-12 08:14:19,128]: [LeNet5_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.7963 Train Acc: 0.3470 Eval Loss: 1.7720 Eval Acc: 0.3619 (LR: 0.000250)
[2025-05-12 08:14:54,036]: [LeNet5_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.7959 Train Acc: 0.3432 Eval Loss: 1.7422 Eval Acc: 0.3653 (LR: 0.000250)
[2025-05-12 08:15:29,266]: [LeNet5_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.8001 Train Acc: 0.3452 Eval Loss: 1.7547 Eval Acc: 0.3806 (LR: 0.000250)
[2025-05-12 08:16:04,134]: [LeNet5_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.8013 Train Acc: 0.3449 Eval Loss: 1.7454 Eval Acc: 0.3690 (LR: 0.000250)
[2025-05-12 08:16:40,147]: [LeNet5_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.7984 Train Acc: 0.3429 Eval Loss: 1.7472 Eval Acc: 0.3700 (LR: 0.000250)
[2025-05-12 08:17:16,532]: [LeNet5_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.8072 Train Acc: 0.3418 Eval Loss: 1.7476 Eval Acc: 0.3647 (LR: 0.000250)
[2025-05-12 08:17:52,054]: [LeNet5_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.8104 Train Acc: 0.3421 Eval Loss: 1.7664 Eval Acc: 0.3556 (LR: 0.000250)
[2025-05-12 08:17:52,054]: Early stopping was triggered!
[2025-05-12 08:17:52,054]: [LeNet5_relu_quantized_2_bits] Best Eval Accuracy: 0.3952
[2025-05-12 08:17:52,081]: 


Quantization of model down to 2 bits finished
[2025-05-12 08:17:52,081]: Model Architecture:
[2025-05-12 08:17:52,094]: LeNet5(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([15.7704], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=47.31120300292969)
    )
  )
  (dequant): DeQuantStub()
  (conv1): Sequential(
    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv2): Sequential(
    (0): Conv2d(
      6, 16, kernel_size=(5, 5), stride=(1, 1)
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4134], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37805280089378357, max_val=0.8622273802757263)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([85.0500], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=255.14987182617188)
          )
        )
      )
    )
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(
      in_features=400, out_features=120, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1492], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20642629265785217, max_val=0.24112440645694733)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([132.2485], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=396.74560546875)
          )
        )
      )
    )
  )
  (fc2): Sequential(
    (0): Linear(
      in_features=120, out_features=84, bias=True
      (weight_fake_quant): FakeQuantize(
        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1378], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20658256113529205, max_val=0.20671792328357697)
      )
      (activation_post_process): NoopObserver()
    )
    (1): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([45.1445], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=135.43357849121094)
          )
        )
      )
    )
  )
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
[2025-05-12 08:17:52,094]: 
Model Weights:
[2025-05-12 08:17:52,094]: 
Layer: conv1.0
Layer Shape: torch.Size([6, 3, 5, 5])
[2025-05-12 08:17:52,094]: Sample Values (25 elements): [-0.8549668192863464, -0.3677819073200226, 0.3998298943042755, 0.1975744664669037, -0.07914990931749344, -0.056794121861457825, 0.5193559527397156, 0.01605098322033882, 0.006742652505636215, 0.015176753513514996, 0.37747636437416077, -0.12758827209472656, -0.12534523010253906, 0.002309763105586171, 0.1362304985523224, -0.08011061698198318, -0.1796528398990631, 0.1927684247493744, -0.33500736951828003, 0.236493781208992, 0.17517539858818054, -0.2824298143386841, -0.14456595480442047, 0.033976100385189056, 0.5339590907096863]
[2025-05-12 08:17:52,095]: Mean: 0.00237135
[2025-05-12 08:17:52,095]: Min: -1.12482679
[2025-05-12 08:17:52,095]: Max: 1.20727456
[2025-05-12 08:17:52,096]: 
Layer: conv2.0
Layer Shape: torch.Size([16, 6, 5, 5])
[2025-05-12 08:17:52,096]: Sample Values (25 elements): [0.41342827677726746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41342827677726746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41342827677726746, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 08:17:52,097]: Mean: 0.03376331
[2025-05-12 08:17:52,097]: Min: -0.41342828
[2025-05-12 08:17:52,097]: Max: 0.82685655
[2025-05-12 08:17:52,098]: 
Layer: fc1.0
Layer Shape: torch.Size([120, 400])
[2025-05-12 08:17:52,099]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 08:17:52,099]: Mean: 0.00132401
[2025-05-12 08:17:52,099]: Min: -0.14918455
[2025-05-12 08:17:52,099]: Max: 0.29836911
[2025-05-12 08:17:52,100]: 
Layer: fc2.0
Layer Shape: torch.Size([84, 120])
[2025-05-12 08:17:52,101]: Sample Values (25 elements): [-0.13776682317256927, -0.13776682317256927, 0.13776682317256927, -0.13776682317256927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13776682317256927, 0.0, 0.13776682317256927, -0.13776682317256927, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13776682317256927, 0.0, 0.0, -0.13776682317256927]
[2025-05-12 08:17:52,101]: Mean: -0.01175391
[2025-05-12 08:17:52,101]: Min: -0.13776682
[2025-05-12 08:17:52,101]: Max: 0.27553365
[2025-05-12 08:17:52,101]: 
Layer: fc3
Layer Shape: torch.Size([10, 84])
[2025-05-12 08:17:52,101]: Sample Values (25 elements): [0.005978303961455822, 0.0012798273237422109, -0.029795754700899124, 0.004045417997986078, 0.028369834646582603, 0.0108166029676795, 0.015404082834720612, -0.01878012716770172, -0.02340482920408249, 0.02172420173883438, 0.015336529351770878, 0.018499178811907768, 0.026721375063061714, -0.02941034361720085, 0.0010266060708090663, -0.027030454948544502, 0.027752798050642014, 0.015780290588736534, 0.034868113696575165, 0.007091693580150604, -0.03468889370560646, -0.016591882333159447, 0.011913593858480453, 0.003651568666100502, -0.0070606861263513565]
[2025-05-12 08:17:52,102]: Mean: 0.00016078
[2025-05-12 08:17:52,102]: Min: -0.10801791
[2025-05-12 08:17:52,102]: Max: 0.06503318
