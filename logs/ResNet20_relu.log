[2025-05-04 05:28:33,790]: 
Training ResNet20 with relu
[2025-05-04 05:29:44,640]: [ResNet20_relu] Epoch: 001 Train Loss: 1.8536 Train Acc: 0.3077 Eval Loss: 1.6071 Eval Acc: 0.3924 (LR: 0.001000)
[2025-05-04 05:30:52,265]: [ResNet20_relu] Epoch: 002 Train Loss: 1.5358 Train Acc: 0.4279 Eval Loss: 1.4098 Eval Acc: 0.4773 (LR: 0.001000)
[2025-05-04 05:32:01,831]: [ResNet20_relu] Epoch: 003 Train Loss: 1.3801 Train Acc: 0.4964 Eval Loss: 1.3202 Eval Acc: 0.5220 (LR: 0.001000)
[2025-05-04 05:33:07,030]: [ResNet20_relu] Epoch: 004 Train Loss: 1.2619 Train Acc: 0.5430 Eval Loss: 1.2845 Eval Acc: 0.5395 (LR: 0.001000)
[2025-05-04 05:34:10,947]: [ResNet20_relu] Epoch: 005 Train Loss: 1.1661 Train Acc: 0.5780 Eval Loss: 1.0844 Eval Acc: 0.6052 (LR: 0.001000)
[2025-05-04 05:35:15,584]: [ResNet20_relu] Epoch: 006 Train Loss: 1.0977 Train Acc: 0.6063 Eval Loss: 1.1231 Eval Acc: 0.6086 (LR: 0.001000)
[2025-05-04 05:36:21,175]: [ResNet20_relu] Epoch: 007 Train Loss: 1.0333 Train Acc: 0.6282 Eval Loss: 1.0709 Eval Acc: 0.6325 (LR: 0.001000)
[2025-05-04 05:37:27,294]: [ResNet20_relu] Epoch: 008 Train Loss: 0.9881 Train Acc: 0.6439 Eval Loss: 0.9628 Eval Acc: 0.6605 (LR: 0.001000)
[2025-05-04 05:38:32,406]: [ResNet20_relu] Epoch: 009 Train Loss: 0.9406 Train Acc: 0.6646 Eval Loss: 1.0028 Eval Acc: 0.6552 (LR: 0.001000)
[2025-05-04 05:39:37,359]: [ResNet20_relu] Epoch: 010 Train Loss: 0.9076 Train Acc: 0.6774 Eval Loss: 0.9061 Eval Acc: 0.6810 (LR: 0.001000)
[2025-05-04 05:40:43,950]: [ResNet20_relu] Epoch: 011 Train Loss: 0.8679 Train Acc: 0.6917 Eval Loss: 0.8812 Eval Acc: 0.6942 (LR: 0.001000)
[2025-05-04 05:41:49,448]: [ResNet20_relu] Epoch: 012 Train Loss: 0.8342 Train Acc: 0.7035 Eval Loss: 0.9065 Eval Acc: 0.6923 (LR: 0.001000)
[2025-05-04 05:42:56,181]: [ResNet20_relu] Epoch: 013 Train Loss: 0.8085 Train Acc: 0.7151 Eval Loss: 0.8374 Eval Acc: 0.7184 (LR: 0.001000)
[2025-05-04 05:44:01,782]: [ResNet20_relu] Epoch: 014 Train Loss: 0.7811 Train Acc: 0.7209 Eval Loss: 0.8740 Eval Acc: 0.7064 (LR: 0.001000)
[2025-05-04 05:45:06,100]: [ResNet20_relu] Epoch: 015 Train Loss: 0.7542 Train Acc: 0.7356 Eval Loss: 0.7676 Eval Acc: 0.7337 (LR: 0.001000)
[2025-05-04 05:46:10,421]: [ResNet20_relu] Epoch: 016 Train Loss: 0.7321 Train Acc: 0.7423 Eval Loss: 0.7913 Eval Acc: 0.7328 (LR: 0.001000)
[2025-05-04 05:47:15,919]: [ResNet20_relu] Epoch: 017 Train Loss: 0.7118 Train Acc: 0.7511 Eval Loss: 0.7722 Eval Acc: 0.7408 (LR: 0.001000)
[2025-05-04 05:48:21,964]: [ResNet20_relu] Epoch: 018 Train Loss: 0.6956 Train Acc: 0.7559 Eval Loss: 0.7406 Eval Acc: 0.7518 (LR: 0.001000)
[2025-05-04 05:49:28,552]: [ResNet20_relu] Epoch: 019 Train Loss: 0.6689 Train Acc: 0.7651 Eval Loss: 0.6909 Eval Acc: 0.7673 (LR: 0.001000)
[2025-05-04 05:50:35,875]: [ResNet20_relu] Epoch: 020 Train Loss: 0.6523 Train Acc: 0.7732 Eval Loss: 0.7418 Eval Acc: 0.7531 (LR: 0.001000)
[2025-05-04 05:51:43,896]: [ResNet20_relu] Epoch: 021 Train Loss: 0.6439 Train Acc: 0.7754 Eval Loss: 0.6473 Eval Acc: 0.7809 (LR: 0.001000)
[2025-05-04 05:52:53,159]: [ResNet20_relu] Epoch: 022 Train Loss: 0.6249 Train Acc: 0.7819 Eval Loss: 0.6360 Eval Acc: 0.7835 (LR: 0.001000)
[2025-05-04 05:54:00,137]: [ResNet20_relu] Epoch: 023 Train Loss: 0.6126 Train Acc: 0.7864 Eval Loss: 0.6123 Eval Acc: 0.7936 (LR: 0.001000)
[2025-05-04 05:55:08,185]: [ResNet20_relu] Epoch: 024 Train Loss: 0.5913 Train Acc: 0.7937 Eval Loss: 0.6021 Eval Acc: 0.7963 (LR: 0.001000)
[2025-05-04 05:56:15,931]: [ResNet20_relu] Epoch: 025 Train Loss: 0.5822 Train Acc: 0.7981 Eval Loss: 0.6150 Eval Acc: 0.7960 (LR: 0.001000)
[2025-05-04 05:57:22,070]: [ResNet20_relu] Epoch: 026 Train Loss: 0.5788 Train Acc: 0.7963 Eval Loss: 0.6007 Eval Acc: 0.7965 (LR: 0.001000)
[2025-05-04 05:58:30,663]: [ResNet20_relu] Epoch: 027 Train Loss: 0.5595 Train Acc: 0.8036 Eval Loss: 0.6799 Eval Acc: 0.7781 (LR: 0.001000)
[2025-05-04 05:59:39,052]: [ResNet20_relu] Epoch: 028 Train Loss: 0.5590 Train Acc: 0.8053 Eval Loss: 0.5863 Eval Acc: 0.7999 (LR: 0.001000)
[2025-05-04 06:00:45,101]: [ResNet20_relu] Epoch: 029 Train Loss: 0.5474 Train Acc: 0.8103 Eval Loss: 0.6081 Eval Acc: 0.7978 (LR: 0.001000)
[2025-05-04 06:01:50,011]: [ResNet20_relu] Epoch: 030 Train Loss: 0.5374 Train Acc: 0.8118 Eval Loss: 0.5703 Eval Acc: 0.8067 (LR: 0.001000)
[2025-05-04 06:02:55,968]: [ResNet20_relu] Epoch: 031 Train Loss: 0.5286 Train Acc: 0.8138 Eval Loss: 0.5629 Eval Acc: 0.8129 (LR: 0.001000)
[2025-05-04 06:03:58,572]: [ResNet20_relu] Epoch: 032 Train Loss: 0.5241 Train Acc: 0.8178 Eval Loss: 0.5588 Eval Acc: 0.8154 (LR: 0.001000)
[2025-05-04 06:04:59,537]: [ResNet20_relu] Epoch: 033 Train Loss: 0.5126 Train Acc: 0.8210 Eval Loss: 0.5412 Eval Acc: 0.8210 (LR: 0.001000)
[2025-05-04 06:05:58,207]: [ResNet20_relu] Epoch: 034 Train Loss: 0.5035 Train Acc: 0.8246 Eval Loss: 0.5808 Eval Acc: 0.8097 (LR: 0.001000)
[2025-05-04 06:06:57,174]: [ResNet20_relu] Epoch: 035 Train Loss: 0.4981 Train Acc: 0.8269 Eval Loss: 0.5407 Eval Acc: 0.8186 (LR: 0.001000)
[2025-05-04 06:08:03,719]: [ResNet20_relu] Epoch: 036 Train Loss: 0.4922 Train Acc: 0.8277 Eval Loss: 0.5273 Eval Acc: 0.8206 (LR: 0.001000)
[2025-05-04 06:09:07,120]: [ResNet20_relu] Epoch: 037 Train Loss: 0.4860 Train Acc: 0.8299 Eval Loss: 0.5306 Eval Acc: 0.8242 (LR: 0.001000)
[2025-05-04 06:10:10,553]: [ResNet20_relu] Epoch: 038 Train Loss: 0.4788 Train Acc: 0.8314 Eval Loss: 0.5790 Eval Acc: 0.8113 (LR: 0.001000)
[2025-05-04 06:11:07,661]: [ResNet20_relu] Epoch: 039 Train Loss: 0.4706 Train Acc: 0.8375 Eval Loss: 0.5445 Eval Acc: 0.8203 (LR: 0.001000)
[2025-05-04 06:12:05,352]: [ResNet20_relu] Epoch: 040 Train Loss: 0.4626 Train Acc: 0.8384 Eval Loss: 0.5652 Eval Acc: 0.8152 (LR: 0.001000)
[2025-05-04 06:13:03,281]: [ResNet20_relu] Epoch: 041 Train Loss: 0.4551 Train Acc: 0.8409 Eval Loss: 0.5002 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-04 06:14:01,072]: [ResNet20_relu] Epoch: 042 Train Loss: 0.4530 Train Acc: 0.8408 Eval Loss: 0.5333 Eval Acc: 0.8266 (LR: 0.001000)
[2025-05-04 06:14:58,332]: [ResNet20_relu] Epoch: 043 Train Loss: 0.4459 Train Acc: 0.8437 Eval Loss: 0.5279 Eval Acc: 0.8273 (LR: 0.001000)
[2025-05-04 06:15:56,217]: [ResNet20_relu] Epoch: 044 Train Loss: 0.4468 Train Acc: 0.8443 Eval Loss: 0.4906 Eval Acc: 0.8394 (LR: 0.001000)
[2025-05-04 06:16:57,556]: [ResNet20_relu] Epoch: 045 Train Loss: 0.4411 Train Acc: 0.8446 Eval Loss: 0.5150 Eval Acc: 0.8312 (LR: 0.001000)
[2025-05-04 06:17:59,997]: [ResNet20_relu] Epoch: 046 Train Loss: 0.4341 Train Acc: 0.8494 Eval Loss: 0.5906 Eval Acc: 0.8122 (LR: 0.001000)
[2025-05-04 06:19:02,522]: [ResNet20_relu] Epoch: 047 Train Loss: 0.4291 Train Acc: 0.8500 Eval Loss: 0.5758 Eval Acc: 0.8147 (LR: 0.001000)
[2025-05-04 06:20:04,930]: [ResNet20_relu] Epoch: 048 Train Loss: 0.4242 Train Acc: 0.8520 Eval Loss: 0.4925 Eval Acc: 0.8367 (LR: 0.001000)
[2025-05-04 06:21:03,063]: [ResNet20_relu] Epoch: 049 Train Loss: 0.4176 Train Acc: 0.8560 Eval Loss: 0.4897 Eval Acc: 0.8404 (LR: 0.001000)
[2025-05-04 06:22:01,452]: [ResNet20_relu] Epoch: 050 Train Loss: 0.4172 Train Acc: 0.8547 Eval Loss: 0.4875 Eval Acc: 0.8403 (LR: 0.001000)
[2025-05-04 06:23:08,355]: [ResNet20_relu] Epoch: 051 Train Loss: 0.4104 Train Acc: 0.8566 Eval Loss: 0.5123 Eval Acc: 0.8336 (LR: 0.001000)
[2025-05-04 06:24:10,062]: [ResNet20_relu] Epoch: 052 Train Loss: 0.4107 Train Acc: 0.8572 Eval Loss: 0.4846 Eval Acc: 0.8427 (LR: 0.001000)
[2025-05-04 06:25:08,968]: [ResNet20_relu] Epoch: 053 Train Loss: 0.4002 Train Acc: 0.8620 Eval Loss: 0.4717 Eval Acc: 0.8463 (LR: 0.001000)
[2025-05-04 06:26:08,659]: [ResNet20_relu] Epoch: 054 Train Loss: 0.3992 Train Acc: 0.8594 Eval Loss: 0.4729 Eval Acc: 0.8453 (LR: 0.001000)
[2025-05-04 06:27:06,804]: [ResNet20_relu] Epoch: 055 Train Loss: 0.3917 Train Acc: 0.8641 Eval Loss: 0.4864 Eval Acc: 0.8388 (LR: 0.001000)
[2025-05-04 06:28:07,119]: [ResNet20_relu] Epoch: 056 Train Loss: 0.3882 Train Acc: 0.8647 Eval Loss: 0.5158 Eval Acc: 0.8343 (LR: 0.001000)
[2025-05-04 06:29:13,186]: [ResNet20_relu] Epoch: 057 Train Loss: 0.3845 Train Acc: 0.8658 Eval Loss: 0.4821 Eval Acc: 0.8418 (LR: 0.001000)
[2025-05-04 06:30:23,377]: [ResNet20_relu] Epoch: 058 Train Loss: 0.3823 Train Acc: 0.8669 Eval Loss: 0.5174 Eval Acc: 0.8364 (LR: 0.001000)
[2025-05-04 06:31:32,786]: [ResNet20_relu] Epoch: 059 Train Loss: 0.3824 Train Acc: 0.8662 Eval Loss: 0.5098 Eval Acc: 0.8403 (LR: 0.001000)
[2025-05-04 06:32:43,242]: [ResNet20_relu] Epoch: 060 Train Loss: 0.3787 Train Acc: 0.8671 Eval Loss: 0.4748 Eval Acc: 0.8431 (LR: 0.001000)
[2025-05-04 06:33:52,777]: [ResNet20_relu] Epoch: 061 Train Loss: 0.3682 Train Acc: 0.8715 Eval Loss: 0.4573 Eval Acc: 0.8505 (LR: 0.001000)
[2025-05-04 06:35:02,963]: [ResNet20_relu] Epoch: 062 Train Loss: 0.3692 Train Acc: 0.8701 Eval Loss: 0.4638 Eval Acc: 0.8491 (LR: 0.001000)
[2025-05-04 06:36:12,981]: [ResNet20_relu] Epoch: 063 Train Loss: 0.3639 Train Acc: 0.8724 Eval Loss: 0.4527 Eval Acc: 0.8481 (LR: 0.001000)
[2025-05-04 06:37:22,776]: [ResNet20_relu] Epoch: 064 Train Loss: 0.3614 Train Acc: 0.8754 Eval Loss: 0.4685 Eval Acc: 0.8472 (LR: 0.001000)
[2025-05-04 06:38:33,672]: [ResNet20_relu] Epoch: 065 Train Loss: 0.3609 Train Acc: 0.8743 Eval Loss: 0.5161 Eval Acc: 0.8350 (LR: 0.001000)
[2025-05-04 06:39:44,054]: [ResNet20_relu] Epoch: 066 Train Loss: 0.3571 Train Acc: 0.8762 Eval Loss: 0.4772 Eval Acc: 0.8463 (LR: 0.001000)
[2025-05-04 06:40:54,268]: [ResNet20_relu] Epoch: 067 Train Loss: 0.3552 Train Acc: 0.8762 Eval Loss: 0.4493 Eval Acc: 0.8540 (LR: 0.001000)
[2025-05-04 06:42:03,847]: [ResNet20_relu] Epoch: 068 Train Loss: 0.3458 Train Acc: 0.8792 Eval Loss: 0.4983 Eval Acc: 0.8408 (LR: 0.001000)
[2025-05-04 06:43:14,287]: [ResNet20_relu] Epoch: 069 Train Loss: 0.3457 Train Acc: 0.8808 Eval Loss: 0.4475 Eval Acc: 0.8523 (LR: 0.001000)
[2025-05-04 06:44:24,895]: [ResNet20_relu] Epoch: 070 Train Loss: 0.3466 Train Acc: 0.8789 Eval Loss: 0.4542 Eval Acc: 0.8498 (LR: 0.000100)
[2025-05-04 06:45:34,827]: [ResNet20_relu] Epoch: 071 Train Loss: 0.3055 Train Acc: 0.8951 Eval Loss: 0.3964 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-04 06:46:53,425]: [ResNet20_relu] Epoch: 072 Train Loss: 0.2928 Train Acc: 0.8998 Eval Loss: 0.3974 Eval Acc: 0.8696 (LR: 0.000100)
[2025-05-04 06:48:04,637]: [ResNet20_relu] Epoch: 073 Train Loss: 0.2946 Train Acc: 0.8981 Eval Loss: 0.3945 Eval Acc: 0.8700 (LR: 0.000100)
[2025-05-04 06:49:15,208]: [ResNet20_relu] Epoch: 074 Train Loss: 0.2928 Train Acc: 0.8993 Eval Loss: 0.3918 Eval Acc: 0.8707 (LR: 0.000100)
[2025-05-04 06:50:25,552]: [ResNet20_relu] Epoch: 075 Train Loss: 0.2926 Train Acc: 0.8989 Eval Loss: 0.3999 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-04 06:51:35,310]: [ResNet20_relu] Epoch: 076 Train Loss: 0.2887 Train Acc: 0.9014 Eval Loss: 0.3973 Eval Acc: 0.8713 (LR: 0.000100)
[2025-05-04 06:52:45,564]: [ResNet20_relu] Epoch: 077 Train Loss: 0.2891 Train Acc: 0.9013 Eval Loss: 0.3958 Eval Acc: 0.8718 (LR: 0.000100)
[2025-05-04 06:53:55,010]: [ResNet20_relu] Epoch: 078 Train Loss: 0.2886 Train Acc: 0.8998 Eval Loss: 0.3981 Eval Acc: 0.8699 (LR: 0.000100)
[2025-05-04 06:55:05,130]: [ResNet20_relu] Epoch: 079 Train Loss: 0.2864 Train Acc: 0.9004 Eval Loss: 0.3977 Eval Acc: 0.8692 (LR: 0.000100)
[2025-05-04 06:56:14,981]: [ResNet20_relu] Epoch: 080 Train Loss: 0.2851 Train Acc: 0.9023 Eval Loss: 0.4001 Eval Acc: 0.8689 (LR: 0.000100)
[2025-05-04 06:57:16,434]: [ResNet20_relu] Epoch: 081 Train Loss: 0.2837 Train Acc: 0.9023 Eval Loss: 0.3954 Eval Acc: 0.8711 (LR: 0.000100)
[2025-05-04 06:58:25,169]: [ResNet20_relu] Epoch: 082 Train Loss: 0.2848 Train Acc: 0.9005 Eval Loss: 0.3937 Eval Acc: 0.8725 (LR: 0.000100)
[2025-05-04 06:59:34,865]: [ResNet20_relu] Epoch: 083 Train Loss: 0.2853 Train Acc: 0.9016 Eval Loss: 0.3936 Eval Acc: 0.8724 (LR: 0.000100)
[2025-05-04 07:00:44,769]: [ResNet20_relu] Epoch: 084 Train Loss: 0.2819 Train Acc: 0.9034 Eval Loss: 0.3951 Eval Acc: 0.8707 (LR: 0.000100)
[2025-05-04 07:01:54,220]: [ResNet20_relu] Epoch: 085 Train Loss: 0.2788 Train Acc: 0.9034 Eval Loss: 0.3983 Eval Acc: 0.8715 (LR: 0.000100)
[2025-05-04 07:03:04,379]: [ResNet20_relu] Epoch: 086 Train Loss: 0.2786 Train Acc: 0.9030 Eval Loss: 0.3979 Eval Acc: 0.8713 (LR: 0.000100)
[2025-05-04 07:04:15,084]: [ResNet20_relu] Epoch: 087 Train Loss: 0.2769 Train Acc: 0.9047 Eval Loss: 0.3950 Eval Acc: 0.8739 (LR: 0.000100)
[2025-05-04 07:05:14,732]: [ResNet20_relu] Epoch: 088 Train Loss: 0.2749 Train Acc: 0.9053 Eval Loss: 0.3970 Eval Acc: 0.8736 (LR: 0.000100)
[2025-05-04 07:06:14,529]: [ResNet20_relu] Epoch: 089 Train Loss: 0.2786 Train Acc: 0.9027 Eval Loss: 0.3963 Eval Acc: 0.8718 (LR: 0.000100)
[2025-05-04 07:07:24,981]: [ResNet20_relu] Epoch: 090 Train Loss: 0.2749 Train Acc: 0.9049 Eval Loss: 0.3978 Eval Acc: 0.8723 (LR: 0.000100)
[2025-05-04 07:08:34,937]: [ResNet20_relu] Epoch: 091 Train Loss: 0.2747 Train Acc: 0.9053 Eval Loss: 0.3963 Eval Acc: 0.8745 (LR: 0.000100)
[2025-05-04 07:09:44,601]: [ResNet20_relu] Epoch: 092 Train Loss: 0.2786 Train Acc: 0.9032 Eval Loss: 0.4009 Eval Acc: 0.8690 (LR: 0.000100)
[2025-05-04 07:10:53,755]: [ResNet20_relu] Epoch: 093 Train Loss: 0.2786 Train Acc: 0.9037 Eval Loss: 0.3965 Eval Acc: 0.8739 (LR: 0.000100)
[2025-05-04 07:12:03,385]: [ResNet20_relu] Epoch: 094 Train Loss: 0.2773 Train Acc: 0.9039 Eval Loss: 0.4012 Eval Acc: 0.8710 (LR: 0.000100)
[2025-05-04 07:13:14,026]: [ResNet20_relu] Epoch: 095 Train Loss: 0.2777 Train Acc: 0.9029 Eval Loss: 0.3901 Eval Acc: 0.8752 (LR: 0.000100)
[2025-05-04 07:14:23,878]: [ResNet20_relu] Epoch: 096 Train Loss: 0.2767 Train Acc: 0.9035 Eval Loss: 0.3933 Eval Acc: 0.8716 (LR: 0.000100)
[2025-05-04 07:15:34,738]: [ResNet20_relu] Epoch: 097 Train Loss: 0.2740 Train Acc: 0.9061 Eval Loss: 0.3953 Eval Acc: 0.8724 (LR: 0.000100)
[2025-05-04 07:16:44,741]: [ResNet20_relu] Epoch: 098 Train Loss: 0.2736 Train Acc: 0.9050 Eval Loss: 0.3956 Eval Acc: 0.8727 (LR: 0.000100)
[2025-05-04 07:17:46,810]: [ResNet20_relu] Epoch: 099 Train Loss: 0.2751 Train Acc: 0.9040 Eval Loss: 0.3976 Eval Acc: 0.8708 (LR: 0.000100)
[2025-05-04 07:18:47,973]: [ResNet20_relu] Epoch: 100 Train Loss: 0.2711 Train Acc: 0.9056 Eval Loss: 0.4034 Eval Acc: 0.8702 (LR: 0.000010)
[2025-05-04 07:19:49,291]: [ResNet20_relu] Epoch: 101 Train Loss: 0.2716 Train Acc: 0.9060 Eval Loss: 0.3970 Eval Acc: 0.8733 (LR: 0.000010)
[2025-05-04 07:20:50,113]: [ResNet20_relu] Epoch: 102 Train Loss: 0.2692 Train Acc: 0.9059 Eval Loss: 0.3956 Eval Acc: 0.8733 (LR: 0.000010)
[2025-05-04 07:21:50,143]: [ResNet20_relu] Epoch: 103 Train Loss: 0.2694 Train Acc: 0.9075 Eval Loss: 0.3948 Eval Acc: 0.8740 (LR: 0.000010)
[2025-05-04 07:22:50,375]: [ResNet20_relu] Epoch: 104 Train Loss: 0.2662 Train Acc: 0.9082 Eval Loss: 0.3969 Eval Acc: 0.8728 (LR: 0.000010)
[2025-05-04 07:23:50,955]: [ResNet20_relu] Epoch: 105 Train Loss: 0.2690 Train Acc: 0.9057 Eval Loss: 0.3984 Eval Acc: 0.8728 (LR: 0.000010)
[2025-05-04 07:24:51,997]: [ResNet20_relu] Epoch: 106 Train Loss: 0.2668 Train Acc: 0.9085 Eval Loss: 0.3963 Eval Acc: 0.8730 (LR: 0.000010)
[2025-05-04 07:25:52,242]: [ResNet20_relu] Epoch: 107 Train Loss: 0.2681 Train Acc: 0.9073 Eval Loss: 0.3984 Eval Acc: 0.8724 (LR: 0.000010)
[2025-05-04 07:26:54,864]: [ResNet20_relu] Epoch: 108 Train Loss: 0.2664 Train Acc: 0.9072 Eval Loss: 0.3964 Eval Acc: 0.8733 (LR: 0.000010)
[2025-05-04 07:27:53,300]: [ResNet20_relu] Epoch: 109 Train Loss: 0.2654 Train Acc: 0.9076 Eval Loss: 0.3945 Eval Acc: 0.8745 (LR: 0.000010)
[2025-05-04 07:28:52,067]: [ResNet20_relu] Epoch: 110 Train Loss: 0.2682 Train Acc: 0.9072 Eval Loss: 0.3948 Eval Acc: 0.8732 (LR: 0.000010)
[2025-05-04 07:28:52,071]: Early stopping was triggered!
[2025-05-04 07:28:52,126]: 
Training of full-precision model finished!
[2025-05-04 07:28:52,127]: Model Architecture:
[2025-05-04 07:28:52,128]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): ReLU(inplace=True)
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-04 07:28:52,128]: 
Model Parameters with Weights:
[2025-05-04 07:28:52,128]: 
Parameter: initial_layer.0.weight
[2025-05-04 07:28:52,128]: Shape: torch.Size([16, 3, 3, 3])
[2025-05-04 07:28:52,210]: Sample Values (16 elements): [-0.060221217572689056, 0.10561023652553558, -0.2620648145675659, 0.07041879743337631, 0.2508876323699951, 0.04123663902282715, 0.19119703769683838, -0.006668437272310257, -0.05923368036746979, 0.0741371139883995, 0.2563595175743103, -0.21283338963985443, -0.17676502466201782, 0.022263478487730026, 0.12237738817930222, 0.01603689417243004]
[2025-05-04 07:28:52,233]: Mean: -0.0040
[2025-05-04 07:28:52,239]: Std: 0.1626
[2025-05-04 07:28:52,266]: Min: -0.4956
[2025-05-04 07:28:52,271]: Max: 0.4231
[2025-05-04 07:28:52,271]: 
Parameter: initial_layer.1.weight
[2025-05-04 07:28:52,271]: Shape: torch.Size([16])
[2025-05-04 07:28:52,273]: Sample Values (16 elements): [0.9750244617462158, 0.888662576675415, 1.0735712051391602, 0.831102192401886, 1.0027366876602173, 0.9205467700958252, 0.9885244369506836, 1.0578994750976562, 0.9589855670928955, 0.9549590349197388, 0.9318088293075562, 0.9039490818977356, 0.8929247260093689, 1.0784614086151123, 0.8981047868728638, 1.01600182056427]
[2025-05-04 07:28:52,280]: Mean: 0.9608
[2025-05-04 07:28:52,282]: Std: 0.0721
[2025-05-04 07:28:52,284]: Min: 0.8311
[2025-05-04 07:28:52,285]: Max: 1.0785
[2025-05-04 07:28:52,285]: 
Parameter: initial_layer.1.bias
[2025-05-04 07:28:52,285]: Shape: torch.Size([16])
[2025-05-04 07:28:52,291]: Sample Values (16 elements): [0.4116869270801544, -0.009899938479065895, 0.1374088078737259, 0.1253206580877304, -0.006156062241643667, 0.35552752017974854, 0.08318597078323364, 0.1800725907087326, 0.1580582857131958, 0.21326936781406403, 0.14844480156898499, -0.04794030264019966, 0.003250895068049431, 0.19050045311450958, 0.0947185680270195, 0.03530401736497879]
[2025-05-04 07:28:52,292]: Mean: 0.1295
[2025-05-04 07:28:52,297]: Std: 0.1272
[2025-05-04 07:28:52,298]: Min: -0.0479
[2025-05-04 07:28:52,299]: Max: 0.4117
[2025-05-04 07:28:52,299]: 
Parameter: layer1.0.conv1.weight
[2025-05-04 07:28:52,299]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:28:52,301]: Sample Values (16 elements): [0.07723851501941681, 0.01263615395873785, -0.09842921048402786, 0.0331876166164875, 0.021536996588110924, -0.034533530473709106, 0.06494472175836563, -0.08697643131017685, -0.06717606633901596, -0.08468306064605713, 0.001455632853321731, -0.052219048142433167, 0.02387360669672489, 0.008522088639438152, -0.10728590935468674, 0.010663026943802834]
[2025-05-04 07:28:52,303]: Mean: -0.0045
[2025-05-04 07:28:52,305]: Std: 0.0626
[2025-05-04 07:28:52,308]: Min: -0.1982
[2025-05-04 07:28:52,311]: Max: 0.2166
[2025-05-04 07:28:52,311]: 
Parameter: layer1.0.bn1.weight
[2025-05-04 07:28:52,311]: Shape: torch.Size([16])
[2025-05-04 07:28:52,315]: Sample Values (16 elements): [0.9776031970977783, 0.9971187114715576, 0.9483090043067932, 0.9157418608665466, 0.8625640273094177, 0.9830723404884338, 1.019455075263977, 0.907629132270813, 0.9735813140869141, 0.8761440515518188, 1.0237125158309937, 0.9865725040435791, 1.0352518558502197, 0.9739982485771179, 1.0187177658081055, 0.9843343496322632]
[2025-05-04 07:28:52,321]: Mean: 0.9677
[2025-05-04 07:28:52,329]: Std: 0.0523
[2025-05-04 07:28:52,342]: Min: 0.8626
[2025-05-04 07:28:52,350]: Max: 1.0353
[2025-05-04 07:28:52,350]: 
Parameter: layer1.0.bn1.bias
[2025-05-04 07:28:52,350]: Shape: torch.Size([16])
[2025-05-04 07:28:52,353]: Sample Values (16 elements): [0.02494586817920208, 0.03866211324930191, 0.10015586018562317, 0.12410903722047806, -0.017338020727038383, 0.1300458312034607, 0.02548227459192276, 0.2231532484292984, 0.025237862020730972, -0.00399929191917181, -0.004947893787175417, -0.09770942479372025, 0.011084250174462795, 0.08427385985851288, 0.028340091928839684, -0.015496602281928062]
[2025-05-04 07:28:52,353]: Mean: 0.0422
[2025-05-04 07:28:52,363]: Std: 0.0753
[2025-05-04 07:28:52,364]: Min: -0.0977
[2025-05-04 07:28:52,369]: Max: 0.2232
[2025-05-04 07:28:52,369]: 
Parameter: layer1.0.conv2.weight
[2025-05-04 07:28:52,369]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:28:52,376]: Sample Values (16 elements): [-0.03191008418798447, 0.0017791504506021738, 0.06673099100589752, -0.06477874517440796, -0.05567891150712967, 0.024052131921052933, -0.02369825355708599, 0.03915470838546753, -0.03599701076745987, -0.09764048457145691, -0.09890270978212357, -0.004263328853994608, 0.03374451771378517, -0.1230301633477211, -0.027619577944278717, -0.04364069178700447]
[2025-05-04 07:28:52,378]: Mean: -0.0061
[2025-05-04 07:28:52,379]: Std: 0.0609
[2025-05-04 07:28:52,381]: Min: -0.2421
[2025-05-04 07:28:52,383]: Max: 0.1945
[2025-05-04 07:28:52,383]: 
Parameter: layer1.0.bn2.weight
[2025-05-04 07:28:52,383]: Shape: torch.Size([16])
[2025-05-04 07:28:52,390]: Sample Values (16 elements): [0.9646705389022827, 0.9122481942176819, 0.9858347773551941, 0.9801198840141296, 0.9945448040962219, 0.9981951713562012, 0.9615809321403503, 0.975561797618866, 1.0813658237457275, 1.1375173330307007, 0.9509404301643372, 0.9303629994392395, 0.9761902093887329, 0.9463570713996887, 1.0130724906921387, 0.9639266133308411]
[2025-05-04 07:28:52,391]: Mean: 0.9858
[2025-05-04 07:28:52,392]: Std: 0.0554
[2025-05-04 07:28:52,395]: Min: 0.9122
[2025-05-04 07:28:52,401]: Max: 1.1375
[2025-05-04 07:28:52,401]: 
Parameter: layer1.0.bn2.bias
[2025-05-04 07:28:52,401]: Shape: torch.Size([16])
[2025-05-04 07:28:52,424]: Sample Values (16 elements): [0.08213713765144348, 0.09557166695594788, 0.08131909370422363, -0.03600548952817917, 0.14870834350585938, 0.09972986578941345, 0.0063187056221067905, 0.1062748059630394, 0.046160463243722916, 0.07539121061563492, 0.09499774128198624, 0.13132698833942413, 0.0732446163892746, 0.2301769107580185, 0.16700296103954315, 0.0943683609366417]
[2025-05-04 07:28:52,425]: Mean: 0.0935
[2025-05-04 07:28:52,428]: Std: 0.0613
[2025-05-04 07:28:52,430]: Min: -0.0360
[2025-05-04 07:28:52,431]: Max: 0.2302
[2025-05-04 07:28:52,431]: 
Parameter: layer1.1.conv1.weight
[2025-05-04 07:28:52,431]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:28:52,438]: Sample Values (16 elements): [-0.0680026188492775, -0.04835982248187065, 0.02518605813384056, -0.06395074725151062, 0.031137928366661072, 0.028440754860639572, 0.016066404059529305, -0.03612392023205757, 0.027934065088629723, -0.05255706235766411, 0.13114623725414276, -0.07635566592216492, -0.1637757420539856, 0.0714913159608841, -0.07231484353542328, -0.06294533610343933]
[2025-05-04 07:28:52,440]: Mean: -0.0004
[2025-05-04 07:28:52,448]: Std: 0.0607
[2025-05-04 07:28:52,450]: Min: -0.2514
[2025-05-04 07:28:52,451]: Max: 0.2142
[2025-05-04 07:28:52,452]: 
Parameter: layer1.1.bn1.weight
[2025-05-04 07:28:52,452]: Shape: torch.Size([16])
[2025-05-04 07:28:52,459]: Sample Values (16 elements): [0.9450439214706421, 0.9258829951286316, 1.1481918096542358, 0.9379321932792664, 0.9562454223632812, 0.9456436634063721, 0.9103331565856934, 0.9602184295654297, 0.9567868709564209, 0.9345787763595581, 0.9724675416946411, 0.9595964550971985, 0.9430823922157288, 1.0814496278762817, 0.9206705093383789, 1.0161535739898682]
[2025-05-04 07:28:52,462]: Mean: 0.9696
[2025-05-04 07:28:52,463]: Std: 0.0627
[2025-05-04 07:28:52,464]: Min: 0.9103
[2025-05-04 07:28:52,471]: Max: 1.1482
[2025-05-04 07:28:52,471]: 
Parameter: layer1.1.bn1.bias
[2025-05-04 07:28:52,471]: Shape: torch.Size([16])
[2025-05-04 07:28:52,488]: Sample Values (16 elements): [-0.07063490897417068, 0.024175142869353294, -0.040870584547519684, -0.03316689282655716, -0.033790379762649536, -0.019841164350509644, 0.02240237407386303, 0.05854441225528717, 0.033024370670318604, 0.0004819263704121113, -0.06642749160528183, -0.031835783272981644, -0.012797322124242783, -0.06866724789142609, -0.07585874199867249, -0.04048258811235428]
[2025-05-04 07:28:52,497]: Mean: -0.0222
[2025-05-04 07:28:52,499]: Std: 0.0406
[2025-05-04 07:28:52,502]: Min: -0.0759
[2025-05-04 07:28:52,504]: Max: 0.0585
[2025-05-04 07:28:52,504]: 
Parameter: layer1.1.conv2.weight
[2025-05-04 07:28:52,504]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:28:52,506]: Sample Values (16 elements): [-0.00912608951330185, 0.04180577024817467, -0.06766925007104874, -0.12168014794588089, -0.0262034609913826, -0.004802830517292023, 0.07459983974695206, -0.007885954342782497, -0.004165329970419407, 0.08508571982383728, 0.15625105798244476, 0.012484087608754635, -0.03314199671149254, 0.022718949243426323, 0.00495438277721405, 0.0796235054731369]
[2025-05-04 07:28:52,506]: Mean: 0.0004
[2025-05-04 07:28:52,506]: Std: 0.0571
[2025-05-04 07:28:52,507]: Min: -0.1990
[2025-05-04 07:28:52,513]: Max: 0.2085
[2025-05-04 07:28:52,514]: 
Parameter: layer1.1.bn2.weight
[2025-05-04 07:28:52,514]: Shape: torch.Size([16])
[2025-05-04 07:28:52,515]: Sample Values (16 elements): [1.055182933807373, 0.9772905707359314, 0.8507317900657654, 0.9405966997146606, 0.9816832542419434, 0.9537612199783325, 0.9567265510559082, 0.9662117958068848, 0.929144024848938, 0.9668711423873901, 0.9283068180084229, 0.8927925229072571, 0.980369508266449, 0.9319437146186829, 0.9420239925384521, 0.9001894593238831]
[2025-05-04 07:28:52,525]: Mean: 0.9471
[2025-05-04 07:28:52,526]: Std: 0.0456
[2025-05-04 07:28:52,527]: Min: 0.8507
[2025-05-04 07:28:52,529]: Max: 1.0552
[2025-05-04 07:28:52,529]: 
Parameter: layer1.1.bn2.bias
[2025-05-04 07:28:52,529]: Shape: torch.Size([16])
[2025-05-04 07:28:52,532]: Sample Values (16 elements): [0.12208808958530426, 0.019438020884990692, 0.0038997649680823088, 0.049201276153326035, 0.07375591993331909, 0.04658518359065056, 0.038897108286619186, 0.03465665504336357, 0.03378107026219368, 0.10025718808174133, 0.09720607101917267, 0.06362282484769821, 0.0645548552274704, 0.05186944827437401, 0.10868942737579346, 0.09071624279022217]
[2025-05-04 07:28:52,535]: Mean: 0.0625
[2025-05-04 07:28:52,536]: Std: 0.0339
[2025-05-04 07:28:52,541]: Min: 0.0039
[2025-05-04 07:28:52,543]: Max: 0.1221
[2025-05-04 07:28:52,544]: 
Parameter: layer1.2.conv1.weight
[2025-05-04 07:28:52,544]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:28:52,562]: Sample Values (16 elements): [0.02469542995095253, 0.016640473157167435, 0.06247887760400772, 0.05735308304429054, 0.1320600062608719, -0.019510697573423386, -0.05162406712770462, -0.13349649310112, -0.04315311461687088, 0.011715298518538475, -0.00705528212711215, -0.01793661341071129, -0.022118227556347847, -0.002674667863175273, -0.050153475254774094, 0.03016945905983448]
[2025-05-04 07:28:52,564]: Mean: -0.0002
[2025-05-04 07:28:52,566]: Std: 0.0565
[2025-05-04 07:28:52,576]: Min: -0.1931
[2025-05-04 07:28:52,580]: Max: 0.1896
[2025-05-04 07:28:52,580]: 
Parameter: layer1.2.bn1.weight
[2025-05-04 07:28:52,580]: Shape: torch.Size([16])
[2025-05-04 07:28:52,581]: Sample Values (16 elements): [0.9318508505821228, 0.9506373405456543, 1.007720708847046, 0.9663122296333313, 0.928969144821167, 0.9448047280311584, 0.9758396744728088, 0.9648522138595581, 0.9655053019523621, 0.9856438040733337, 1.028220295906067, 0.9481040835380554, 0.9740848541259766, 1.0273253917694092, 0.9606713056564331, 0.9710632562637329]
[2025-05-04 07:28:52,581]: Mean: 0.9707
[2025-05-04 07:28:52,588]: Std: 0.0296
[2025-05-04 07:28:52,590]: Min: 0.9290
[2025-05-04 07:28:52,599]: Max: 1.0282
[2025-05-04 07:28:52,600]: 
Parameter: layer1.2.bn1.bias
[2025-05-04 07:28:52,600]: Shape: torch.Size([16])
[2025-05-04 07:28:52,600]: Sample Values (16 elements): [-0.03628349304199219, -0.03657984733581543, -0.10091597586870193, -0.03660227730870247, 0.016128648072481155, 0.03722161799669266, 0.0817616656422615, -0.04368819668889046, -0.008335436694324017, -0.019187433645129204, -0.0074165211990475655, -0.030306736007332802, -0.02532188408076763, 0.015494633466005325, 0.03562921658158302, 0.0839015319943428]
[2025-05-04 07:28:52,602]: Mean: -0.0047
[2025-05-04 07:28:52,604]: Std: 0.0481
[2025-05-04 07:28:52,605]: Min: -0.1009
[2025-05-04 07:28:52,613]: Max: 0.0839
[2025-05-04 07:28:52,613]: 
Parameter: layer1.2.conv2.weight
[2025-05-04 07:28:52,613]: Shape: torch.Size([16, 16, 3, 3])
[2025-05-04 07:28:52,617]: Sample Values (16 elements): [-0.05851173773407936, 0.04731263220310211, -0.0005601726588793099, -0.02990829013288021, -0.02643788419663906, 0.08706662058830261, 0.05823175236582756, 0.07483486831188202, -0.08109838515520096, -0.05957992747426033, 0.05634594336152077, 0.02315656654536724, -0.026210026815533638, 0.056303784251213074, -0.06291168928146362, 0.04716510698199272]
[2025-05-04 07:28:52,621]: Mean: -0.0032
[2025-05-04 07:28:52,626]: Std: 0.0533
[2025-05-04 07:28:52,629]: Min: -0.1648
[2025-05-04 07:28:52,630]: Max: 0.1938
[2025-05-04 07:28:52,630]: 
Parameter: layer1.2.bn2.weight
[2025-05-04 07:28:52,630]: Shape: torch.Size([16])
[2025-05-04 07:28:52,642]: Sample Values (16 elements): [0.9965828657150269, 0.9268184900283813, 0.9260815382003784, 0.9735695719718933, 1.0039023160934448, 0.9003973603248596, 0.9425029158592224, 0.9543477892875671, 0.9781835079193115, 0.9811615347862244, 1.0543789863586426, 0.9328447580337524, 0.970729649066925, 0.9710612893104553, 0.931997537612915, 0.9639257788658142]
[2025-05-04 07:28:52,651]: Mean: 0.9630
[2025-05-04 07:28:52,656]: Std: 0.0373
[2025-05-04 07:28:52,657]: Min: 0.9004
[2025-05-04 07:28:52,658]: Max: 1.0544
[2025-05-04 07:28:52,658]: 
Parameter: layer1.2.bn2.bias
[2025-05-04 07:28:52,658]: Shape: torch.Size([16])
[2025-05-04 07:28:52,665]: Sample Values (16 elements): [0.04997013881802559, 0.09414388239383698, 0.0884891003370285, 0.031127499416470528, 0.06749653816223145, 0.06207655742764473, 0.0799952819943428, 0.11806511878967285, -0.0006954693817533553, 0.018253836780786514, 0.06836067140102386, 0.06409531831741333, 0.0740474984049797, 0.02050131745636463, 0.062363434582948685, 0.029652900993824005]
[2025-05-04 07:28:52,667]: Mean: 0.0580
[2025-05-04 07:28:52,670]: Std: 0.0315
[2025-05-04 07:28:52,671]: Min: -0.0007
[2025-05-04 07:28:52,676]: Max: 0.1181
[2025-05-04 07:28:52,676]: 
Parameter: layer2.0.conv1.weight
[2025-05-04 07:28:52,676]: Shape: torch.Size([32, 16, 3, 3])
[2025-05-04 07:28:52,680]: Sample Values (16 elements): [-0.061742719262838364, 0.00899775605648756, 0.004237035289406776, 0.007918231189250946, 0.05541380122303963, 0.11567427963018417, -0.056387871503829956, -0.07656630128622055, 0.0029472364112734795, 0.07314786314964294, 0.03379106894135475, 0.00043517822632566094, 0.02835925482213497, -0.025966227054595947, 0.02320144884288311, 0.01333156879991293]
[2025-05-04 07:28:52,682]: Mean: -0.0011
[2025-05-04 07:28:52,683]: Std: 0.0521
[2025-05-04 07:28:52,685]: Min: -0.1767
[2025-05-04 07:28:52,686]: Max: 0.1899
[2025-05-04 07:28:52,686]: 
Parameter: layer2.0.bn1.weight
[2025-05-04 07:28:52,686]: Shape: torch.Size([32])
[2025-05-04 07:28:52,692]: Sample Values (16 elements): [0.9957635402679443, 1.0398638248443604, 0.9808967113494873, 0.9700847864151001, 0.980387806892395, 0.9607580900192261, 0.9289602637290955, 0.9813891649246216, 0.9888823628425598, 0.9560759663581848, 0.9798938035964966, 0.9613113403320312, 0.9786103367805481, 0.9899970889091492, 0.972831666469574, 0.9538716077804565]
[2025-05-04 07:28:52,694]: Mean: 0.9715
[2025-05-04 07:28:52,696]: Std: 0.0257
[2025-05-04 07:28:52,700]: Min: 0.9217
[2025-05-04 07:28:52,701]: Max: 1.0399
[2025-05-04 07:28:52,701]: 
Parameter: layer2.0.bn1.bias
[2025-05-04 07:28:52,701]: Shape: torch.Size([32])
[2025-05-04 07:28:52,705]: Sample Values (16 elements): [0.005182056687772274, 0.03708096593618393, 0.02941182069480419, -0.013221771456301212, -0.0011158192064613104, 7.990137237356976e-05, 0.04751679301261902, 0.043344851583242416, 0.034282926470041275, -0.019346769899129868, -0.011342245154082775, 0.002168143168091774, 0.02987448126077652, 0.0069969589821994305, 0.0017994085792452097, 0.026555245742201805]
[2025-05-04 07:28:52,710]: Mean: 0.0077
[2025-05-04 07:28:52,725]: Std: 0.0208
[2025-05-04 07:28:52,739]: Min: -0.0408
[2025-05-04 07:28:52,746]: Max: 0.0475
[2025-05-04 07:28:52,746]: 
Parameter: layer2.0.conv2.weight
[2025-05-04 07:28:52,746]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:28:52,747]: Sample Values (16 elements): [0.09637030959129333, 0.017253698781132698, -0.018947580829262733, 0.055100202560424805, 0.03312679007649422, 0.06533266603946686, 0.08158620446920395, -0.007003612816333771, -0.07493090629577637, -0.03520078957080841, 0.0016949251294136047, 0.019923260435461998, 0.041644856333732605, -0.02867298386991024, -0.03229445219039917, 0.044952768832445145]
[2025-05-04 07:28:52,749]: Mean: 0.0002
[2025-05-04 07:28:52,755]: Std: 0.0393
[2025-05-04 07:28:52,758]: Min: -0.1467
[2025-05-04 07:28:52,767]: Max: 0.1371
[2025-05-04 07:28:52,767]: 
Parameter: layer2.0.bn2.weight
[2025-05-04 07:28:52,767]: Shape: torch.Size([32])
[2025-05-04 07:28:52,770]: Sample Values (16 elements): [1.016003131866455, 0.9692388772964478, 0.9900100827217102, 0.9800739884376526, 0.9700939655303955, 0.9422808885574341, 0.972704291343689, 0.9793524742126465, 0.9403402805328369, 0.973520815372467, 0.9561362862586975, 1.0003966093063354, 0.9671737551689148, 0.9635635018348694, 0.9420164823532104, 0.973169207572937]
[2025-05-04 07:28:52,772]: Mean: 0.9750
[2025-05-04 07:28:52,774]: Std: 0.0213
[2025-05-04 07:28:52,778]: Min: 0.9257
[2025-05-04 07:28:52,782]: Max: 1.0160
[2025-05-04 07:28:52,782]: 
Parameter: layer2.0.bn2.bias
[2025-05-04 07:28:52,782]: Shape: torch.Size([32])
[2025-05-04 07:28:52,788]: Sample Values (16 elements): [-0.01943783089518547, 0.011499167419970036, -0.012811727821826935, 0.029836108908057213, 0.022763660177588463, 0.010742660611867905, -0.015139651484787464, 0.031429920345544815, 0.008626313880085945, 0.015271740965545177, 0.001836026436649263, -0.033289242535829544, 0.041170086711645126, 0.0317402258515358, 0.012751171365380287, 0.02131161279976368]
[2025-05-04 07:28:52,794]: Mean: 0.0163
[2025-05-04 07:28:52,796]: Std: 0.0213
[2025-05-04 07:28:52,806]: Min: -0.0333
[2025-05-04 07:28:52,819]: Max: 0.0643
[2025-05-04 07:28:52,819]: 
Parameter: layer2.0.downsample.0.weight
[2025-05-04 07:28:52,819]: Shape: torch.Size([32, 16, 1, 1])
[2025-05-04 07:28:52,820]: Sample Values (16 elements): [-0.1827438324689865, 0.19433368742465973, 0.1585778146982193, 0.12792101502418518, 0.14638067781925201, -0.2046380490064621, -0.15552745759487152, -0.08777357637882233, -0.22185219824314117, 0.029951369389891624, -0.10692965239286423, -0.048906922340393066, -0.238197460770607, -0.1497548520565033, -0.21705219149589539, -0.09631740301847458]
[2025-05-04 07:28:52,828]: Mean: -0.0016
[2025-05-04 07:28:52,831]: Std: 0.1455
[2025-05-04 07:28:52,832]: Min: -0.3239
[2025-05-04 07:28:52,833]: Max: 0.3594
[2025-05-04 07:28:52,834]: 
Parameter: layer2.0.downsample.1.weight
[2025-05-04 07:28:52,834]: Shape: torch.Size([32])
[2025-05-04 07:28:52,834]: Sample Values (16 elements): [0.9387626647949219, 0.912639856338501, 0.9056007862091064, 0.897438645362854, 0.8720446825027466, 0.9002469182014465, 0.8959287405014038, 0.919131875038147, 0.9125195741653442, 0.8847999572753906, 0.8960652947425842, 0.907063364982605, 0.9407098293304443, 0.8881024718284607, 0.9011362195014954, 0.8636335134506226]
[2025-05-04 07:28:52,835]: Mean: 0.9123
[2025-05-04 07:28:52,835]: Std: 0.0263
[2025-05-04 07:28:52,846]: Min: 0.8609
[2025-05-04 07:28:52,848]: Max: 0.9798
[2025-05-04 07:28:52,849]: 
Parameter: layer2.0.downsample.1.bias
[2025-05-04 07:28:52,849]: Shape: torch.Size([32])
[2025-05-04 07:28:52,851]: Sample Values (16 elements): [0.016957303509116173, 0.02131161279976368, 0.00517926923930645, 0.015271740965545177, 0.018330683931708336, 0.011485164985060692, 0.010742660611867905, -0.012811727821826935, 0.03048156201839447, 0.053713638335466385, 0.0317402258515358, 0.001836026436649263, 0.041170086711645126, -0.015139651484787464, 0.015602191910147667, 0.029836108908057213]
[2025-05-04 07:28:52,851]: Mean: 0.0163
[2025-05-04 07:28:52,853]: Std: 0.0213
[2025-05-04 07:28:52,855]: Min: -0.0333
[2025-05-04 07:28:52,856]: Max: 0.0643
[2025-05-04 07:28:52,856]: 
Parameter: layer2.1.conv1.weight
[2025-05-04 07:28:52,856]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:28:52,862]: Sample Values (16 elements): [-0.09873149544000626, 0.055456243455410004, -0.04001746326684952, -0.029721589758992195, -0.05971879884600639, -0.021656444296240807, 0.03995963931083679, 0.0371532067656517, 0.0070919133722782135, 0.030178850516676903, 0.037739984691143036, 0.02007553167641163, 0.027980780228972435, 0.013887631706893444, 0.05140147730708122, 0.05352133885025978]
[2025-05-04 07:28:52,867]: Mean: -0.0010
[2025-05-04 07:28:52,885]: Std: 0.0401
[2025-05-04 07:28:52,889]: Min: -0.1461
[2025-05-04 07:28:52,889]: Max: 0.1401
[2025-05-04 07:28:52,889]: 
Parameter: layer2.1.bn1.weight
[2025-05-04 07:28:52,889]: Shape: torch.Size([32])
[2025-05-04 07:28:52,890]: Sample Values (16 elements): [0.9521661400794983, 0.955240786075592, 1.0348314046859741, 0.9496150612831116, 0.985302746295929, 0.96934574842453, 0.94569331407547, 0.9636038541793823, 0.9753639101982117, 0.9709699749946594, 0.9554102420806885, 0.959139883518219, 0.9807677268981934, 0.996440589427948, 0.9631282091140747, 0.978917121887207]
[2025-05-04 07:28:52,891]: Mean: 0.9715
[2025-05-04 07:28:52,891]: Std: 0.0228
[2025-05-04 07:28:52,892]: Min: 0.9436
[2025-05-04 07:28:52,892]: Max: 1.0418
[2025-05-04 07:28:52,892]: 
Parameter: layer2.1.bn1.bias
[2025-05-04 07:28:52,892]: Shape: torch.Size([32])
[2025-05-04 07:28:52,893]: Sample Values (16 elements): [-0.009668370708823204, 0.0035593686625361443, -0.017166318371891975, -0.014658168889582157, -0.04419003054499626, -0.0001820193720050156, 0.02967163361608982, -0.006337882485240698, 0.007263967301696539, -0.01871187798678875, 0.013858040794730186, -0.008319266140460968, -0.011525732465088367, -0.027659490704536438, -0.05281592532992363, -0.019027139991521835]
[2025-05-04 07:28:52,894]: Mean: -0.0120
[2025-05-04 07:28:52,896]: Std: 0.0248
[2025-05-04 07:28:52,902]: Min: -0.0819
[2025-05-04 07:28:52,904]: Max: 0.0344
[2025-05-04 07:28:52,904]: 
Parameter: layer2.1.conv2.weight
[2025-05-04 07:28:52,905]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:28:52,907]: Sample Values (16 elements): [0.054877426475286484, -0.04369770362973213, 0.01437199767678976, -0.01084887981414795, -0.01447248924523592, 0.05332344397902489, 0.06945578008890152, 0.05585363879799843, 0.016814619302749634, -0.013867720030248165, -0.05180610343813896, -0.019516056403517723, -0.02689623273909092, -0.034355632960796356, -0.027041791006922722, 0.03169258311390877]
[2025-05-04 07:28:52,934]: Mean: -0.0006
[2025-05-04 07:28:52,944]: Std: 0.0394
[2025-05-04 07:28:52,945]: Min: -0.1285
[2025-05-04 07:28:52,945]: Max: 0.1317
[2025-05-04 07:28:52,945]: 
Parameter: layer2.1.bn2.weight
[2025-05-04 07:28:52,946]: Shape: torch.Size([32])
[2025-05-04 07:28:52,947]: Sample Values (16 elements): [0.9462482929229736, 0.9991577863693237, 0.978679358959198, 1.0095866918563843, 1.0191375017166138, 0.9643573760986328, 0.9782824516296387, 0.9986861348152161, 0.9594022035598755, 0.9800648093223572, 0.991279125213623, 1.0068367719650269, 0.9598734974861145, 0.9490267038345337, 0.9832624793052673, 1.0007860660552979]
[2025-05-04 07:28:52,947]: Mean: 0.9889
[2025-05-04 07:28:52,948]: Std: 0.0229
[2025-05-04 07:28:52,948]: Min: 0.9462
[2025-05-04 07:28:52,949]: Max: 1.0490
[2025-05-04 07:28:52,949]: 
Parameter: layer2.1.bn2.bias
[2025-05-04 07:28:52,949]: Shape: torch.Size([32])
[2025-05-04 07:28:52,950]: Sample Values (16 elements): [0.02222249284386635, 0.029057564213871956, 0.002564181573688984, -0.0016481783241033554, 0.03567801043391228, 0.01507195271551609, 0.012115967459976673, 0.00607638992369175, -0.006362796761095524, 0.03098886087536812, -0.006119306664913893, -0.017845911905169487, 0.03411369025707245, 0.021642588078975677, -0.01428151410073042, 0.017717823386192322]
[2025-05-04 07:28:52,951]: Mean: 0.0074
[2025-05-04 07:28:52,952]: Std: 0.0177
[2025-05-04 07:28:52,952]: Min: -0.0301
[2025-05-04 07:28:52,956]: Max: 0.0357
[2025-05-04 07:28:52,956]: 
Parameter: layer2.2.conv1.weight
[2025-05-04 07:28:52,956]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:28:52,963]: Sample Values (16 elements): [0.01745001971721649, 0.022134803235530853, -0.03466414287686348, 0.04701295122504234, 0.08895935118198395, 0.06920024752616882, 0.013406726531684399, -0.04190767928957939, 0.019695179536938667, -0.006830212194472551, -0.020418044179677963, -0.0044933767057955265, 0.02659486047923565, -0.008266430348157883, -0.006147939246147871, -0.0008295735460706055]
[2025-05-04 07:28:52,964]: Mean: -0.0014
[2025-05-04 07:28:52,966]: Std: 0.0388
[2025-05-04 07:28:52,967]: Min: -0.1311
[2025-05-04 07:28:52,969]: Max: 0.1185
[2025-05-04 07:28:52,969]: 
Parameter: layer2.2.bn1.weight
[2025-05-04 07:28:52,969]: Shape: torch.Size([32])
[2025-05-04 07:28:52,977]: Sample Values (16 elements): [0.9533961415290833, 0.9746063947677612, 0.9631466865539551, 0.9612643718719482, 0.9527871608734131, 1.0088592767715454, 0.9406939148902893, 0.959080696105957, 0.9614579677581787, 0.9526294469833374, 0.9814947843551636, 0.9681040644645691, 0.9997575283050537, 1.005666971206665, 0.9514005184173584, 0.9988602995872498]
[2025-05-04 07:28:52,988]: Mean: 0.9715
[2025-05-04 07:28:53,001]: Std: 0.0222
[2025-05-04 07:28:53,018]: Min: 0.9314
[2025-05-04 07:28:53,020]: Max: 1.0091
[2025-05-04 07:28:53,035]: 
Parameter: layer2.2.bn1.bias
[2025-05-04 07:28:53,036]: Shape: torch.Size([32])
[2025-05-04 07:28:53,051]: Sample Values (16 elements): [-0.04973922297358513, 0.01685386709868908, -0.012479567900300026, -0.039256501942873, -0.015123852528631687, -0.00817637238651514, 0.01255973894149065, -0.021191805601119995, -0.013911023736000061, 0.0053772348910570145, -0.013838603161275387, -0.006068878807127476, -0.019354702904820442, -0.043712224811315536, -0.04470568522810936, 0.011685452423989773]
[2025-05-04 07:28:53,064]: Mean: -0.0163
[2025-05-04 07:28:53,064]: Std: 0.0211
[2025-05-04 07:28:53,065]: Min: -0.0560
[2025-05-04 07:28:53,065]: Max: 0.0289
[2025-05-04 07:28:53,066]: 
Parameter: layer2.2.conv2.weight
[2025-05-04 07:28:53,066]: Shape: torch.Size([32, 32, 3, 3])
[2025-05-04 07:28:53,066]: Sample Values (16 elements): [-0.05822860822081566, 0.021398115903139114, -0.06892040371894836, 0.03348448872566223, -0.02848806045949459, 0.01943717524409294, -0.05463707447052002, -0.023261597380042076, -0.051698438823223114, -0.0032267714850604534, 0.019458187744021416, -0.011640775948762894, -0.058741990476846695, -0.049675047397613525, 0.038303669542074203, -0.013131493702530861]
[2025-05-04 07:28:53,067]: Mean: -0.0009
[2025-05-04 07:28:53,067]: Std: 0.0375
[2025-05-04 07:28:53,067]: Min: -0.1184
[2025-05-04 07:28:53,068]: Max: 0.1200
[2025-05-04 07:28:53,068]: 
Parameter: layer2.2.bn2.weight
[2025-05-04 07:28:53,068]: Shape: torch.Size([32])
[2025-05-04 07:28:53,068]: Sample Values (16 elements): [1.0065433979034424, 0.9905437231063843, 0.9886986613273621, 1.0245524644851685, 1.0146141052246094, 0.9920013546943665, 1.0127298831939697, 1.0684535503387451, 1.0145578384399414, 0.9865978360176086, 1.0105457305908203, 1.0103471279144287, 1.0006763935089111, 1.0501596927642822, 1.0104528665542603, 0.981451153755188]
[2025-05-04 07:28:53,068]: Mean: 1.0072
[2025-05-04 07:28:53,069]: Std: 0.0215
[2025-05-04 07:28:53,069]: Min: 0.9815
[2025-05-04 07:28:53,069]: Max: 1.0685
[2025-05-04 07:28:53,070]: 
Parameter: layer2.2.bn2.bias
[2025-05-04 07:28:53,070]: Shape: torch.Size([32])
[2025-05-04 07:28:53,070]: Sample Values (16 elements): [0.018998421728610992, -0.0017936915392056108, 0.02260139398276806, 0.007558335550129414, 0.013982628472149372, -0.005857911426573992, 0.007585277780890465, 0.023350747302174568, 0.02296280674636364, 0.030336998403072357, 0.013430530205368996, 0.008654862642288208, 0.017279833555221558, 0.029786206781864166, 0.01710553653538227, 0.039808761328458786]
[2025-05-04 07:28:53,070]: Mean: 0.0161
[2025-05-04 07:28:53,071]: Std: 0.0135
[2025-05-04 07:28:53,071]: Min: -0.0059
[2025-05-04 07:28:53,072]: Max: 0.0502
[2025-05-04 07:28:53,072]: 
Parameter: layer3.0.conv1.weight
[2025-05-04 07:28:53,072]: Shape: torch.Size([64, 32, 3, 3])
[2025-05-04 07:28:53,073]: Sample Values (16 elements): [-0.03829198330640793, 0.03770525008440018, 0.04538252577185631, -0.025713633745908737, 0.041563160717487335, -0.03355862572789192, -0.0265548974275589, -0.07825616002082825, -0.03357059136033058, 0.0017639402067288756, 0.054972805082798004, -0.08827769756317139, 0.043249279260635376, 0.02300107479095459, -0.052951738238334656, 0.007405896671116352]
[2025-05-04 07:28:53,075]: Mean: -0.0003
[2025-05-04 07:28:53,079]: Std: 0.0358
[2025-05-04 07:28:53,081]: Min: -0.1150
[2025-05-04 07:28:53,083]: Max: 0.1127
[2025-05-04 07:28:53,083]: 
Parameter: layer3.0.bn1.weight
[2025-05-04 07:28:53,083]: Shape: torch.Size([64])
[2025-05-04 07:28:53,086]: Sample Values (16 elements): [0.986807644367218, 0.9691768288612366, 0.9625672698020935, 0.9435590505599976, 0.9693762063980103, 0.9933635592460632, 0.9798287153244019, 0.9760797619819641, 0.996604323387146, 0.9740949273109436, 0.9590384364128113, 0.9709775447845459, 0.9799710512161255, 0.9578784108161926, 0.987443745136261, 0.9745597839355469]
[2025-05-04 07:28:53,087]: Mean: 0.9716
[2025-05-04 07:28:53,090]: Std: 0.0120
[2025-05-04 07:28:53,095]: Min: 0.9433
[2025-05-04 07:28:53,125]: Max: 0.9966
[2025-05-04 07:28:53,126]: 
Parameter: layer3.0.bn1.bias
[2025-05-04 07:28:53,126]: Shape: torch.Size([64])
[2025-05-04 07:28:53,127]: Sample Values (16 elements): [-0.003795371623709798, 0.0025200839154422283, -0.00744865695014596, -0.022139079868793488, -0.02167457714676857, -0.00931806955486536, -0.02778664045035839, -0.020117688924074173, -0.03242814913392067, 0.0025765516329556704, 0.0005898220697417855, -0.028371082618832588, -0.03887930512428284, -0.0057592992670834064, -0.0037426718045026064, -0.009583189152181149]
[2025-05-04 07:28:53,127]: Mean: -0.0178
[2025-05-04 07:28:53,129]: Std: 0.0140
[2025-05-04 07:28:53,129]: Min: -0.0570
[2025-05-04 07:28:53,129]: Max: 0.0197
[2025-05-04 07:28:53,130]: 
Parameter: layer3.0.conv2.weight
[2025-05-04 07:28:53,130]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:28:53,164]: Sample Values (16 elements): [0.017708031460642815, -0.013712402433156967, -0.02759929746389389, -0.039317164570093155, 0.006982552818953991, 0.0181611105799675, 0.017613502219319344, -0.0014547297032549977, -0.016861483454704285, -0.04198261722922325, -0.038035620003938675, 0.0036320751532912254, 0.008832097984850407, 0.007691855076700449, -0.00047665907186456025, -0.039974577724933624]
[2025-05-04 07:28:53,196]: Mean: -0.0005
[2025-05-04 07:28:53,206]: Std: 0.0269
[2025-05-04 07:28:53,220]: Min: -0.0959
[2025-05-04 07:28:53,221]: Max: 0.1194
[2025-05-04 07:28:53,221]: 
Parameter: layer3.0.bn2.weight
[2025-05-04 07:28:53,221]: Shape: torch.Size([64])
[2025-05-04 07:28:53,222]: Sample Values (16 elements): [1.0243842601776123, 1.0222903490066528, 0.9921661615371704, 1.015480875968933, 1.0143059492111206, 1.0120797157287598, 1.0059571266174316, 0.9949052929878235, 0.9957839250564575, 1.0207711458206177, 0.9960469007492065, 1.0091935396194458, 1.006641149520874, 1.0158785581588745, 1.0120224952697754, 0.9883807301521301]
[2025-05-04 07:28:53,223]: Mean: 1.0105
[2025-05-04 07:28:53,223]: Std: 0.0190
[2025-05-04 07:28:53,226]: Min: 0.9735
[2025-05-04 07:28:53,226]: Max: 1.0763
[2025-05-04 07:28:53,226]: 
Parameter: layer3.0.bn2.bias
[2025-05-04 07:28:53,226]: Shape: torch.Size([64])
[2025-05-04 07:28:53,227]: Sample Values (16 elements): [-0.03580556809902191, -0.010470333509147167, 0.00865872297435999, 0.015961341559886932, -0.02108040452003479, 0.02615213580429554, 0.02632649801671505, -0.015648705884814262, -0.005681980866938829, 0.0023034135811030865, -0.028029726818203926, -0.022358965128660202, -0.0008154921233654022, -0.0024829015601426363, 0.011688671074807644, -0.01834958605468273]
[2025-05-04 07:28:53,228]: Mean: -0.0059
[2025-05-04 07:28:53,229]: Std: 0.0136
[2025-05-04 07:28:53,229]: Min: -0.0358
[2025-05-04 07:28:53,232]: Max: 0.0263
[2025-05-04 07:28:53,233]: 
Parameter: layer3.0.downsample.0.weight
[2025-05-04 07:28:53,233]: Shape: torch.Size([64, 32, 1, 1])
[2025-05-04 07:28:53,233]: Sample Values (16 elements): [-0.018130576238036156, 0.1682540476322174, -0.1421148180961609, 0.059717871248722076, 0.02762896753847599, 0.07854799926280975, -0.06434188038110733, 0.15748780965805054, 0.056588202714920044, 0.11633365601301193, -0.03209548816084862, -0.09216836094856262, 0.010412396863102913, -0.05657854303717613, -0.08210385590791702, -0.05564773827791214]
[2025-05-04 07:28:53,234]: Mean: -0.0002
[2025-05-04 07:28:53,234]: Std: 0.1002
[2025-05-04 07:28:53,235]: Min: -0.1999
[2025-05-04 07:28:53,236]: Max: 0.2155
[2025-05-04 07:28:53,236]: 
Parameter: layer3.0.downsample.1.weight
[2025-05-04 07:28:53,236]: Shape: torch.Size([64])
[2025-05-04 07:28:53,237]: Sample Values (16 elements): [0.9323288202285767, 0.9400390982627869, 0.9402572512626648, 0.9251828193664551, 0.8901216983795166, 0.9222634434700012, 0.9409309029579163, 0.9178544282913208, 0.9518877267837524, 0.9600701928138733, 0.941697359085083, 0.9506599307060242, 0.9420543313026428, 0.9423475861549377, 0.9342911243438721, 0.9450466632843018]
[2025-05-04 07:28:53,237]: Mean: 0.9395
[2025-05-04 07:28:53,237]: Std: 0.0135
[2025-05-04 07:28:53,239]: Min: 0.8901
[2025-05-04 07:28:53,240]: Max: 0.9789
[2025-05-04 07:28:53,240]: 
Parameter: layer3.0.downsample.1.bias
[2025-05-04 07:28:53,240]: Shape: torch.Size([64])
[2025-05-04 07:28:53,241]: Sample Values (16 elements): [-0.016090646386146545, -0.02332315593957901, -0.00717071071267128, 0.006991631351411343, 0.007868543267250061, -0.020200321450829506, -0.013174540363252163, -0.028029726818203926, -0.0012658406049013138, -0.014952857047319412, -0.015648705884814262, -0.005824652034789324, 0.015961341559886932, 0.022612642496824265, 0.00865872297435999, -0.011942543089389801]
[2025-05-04 07:28:53,241]: Mean: -0.0059
[2025-05-04 07:28:53,242]: Std: 0.0136
[2025-05-04 07:28:53,244]: Min: -0.0358
[2025-05-04 07:28:53,250]: Max: 0.0263
[2025-05-04 07:28:53,250]: 
Parameter: layer3.1.conv1.weight
[2025-05-04 07:28:53,250]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:28:53,296]: Sample Values (16 elements): [0.028354378417134285, 0.004788707010447979, 0.012742619030177593, 0.01503643300384283, 0.019139142706990242, -0.03639772906899452, -0.013651631772518158, 0.018794814124703407, 0.0910571739077568, 0.029250262305140495, -0.017552152276039124, -0.02464427426457405, 0.013291757553815842, -0.06781764328479767, -0.020703565329313278, 0.028804579749703407]
[2025-05-04 07:28:53,313]: Mean: -0.0009
[2025-05-04 07:28:53,313]: Std: 0.0274
[2025-05-04 07:28:53,314]: Min: -0.0887
[2025-05-04 07:28:53,314]: Max: 0.0932
[2025-05-04 07:28:53,315]: 
Parameter: layer3.1.bn1.weight
[2025-05-04 07:28:53,315]: Shape: torch.Size([64])
[2025-05-04 07:28:53,316]: Sample Values (16 elements): [0.9594709873199463, 0.9794711470603943, 0.9979378581047058, 0.9854329824447632, 0.9608519077301025, 0.9710740447044373, 0.9742230772972107, 0.9634397029876709, 0.9700354337692261, 0.9848001599311829, 0.9635786414146423, 0.972377598285675, 0.9518006443977356, 0.9675741195678711, 0.9640045762062073, 0.984681248664856]
[2025-05-04 07:28:53,316]: Mean: 0.9712
[2025-05-04 07:28:53,317]: Std: 0.0108
[2025-05-04 07:28:53,319]: Min: 0.9429
[2025-05-04 07:28:53,319]: Max: 0.9979
[2025-05-04 07:28:53,319]: 
Parameter: layer3.1.bn1.bias
[2025-05-04 07:28:53,319]: Shape: torch.Size([64])
[2025-05-04 07:28:53,320]: Sample Values (16 elements): [-0.020397363230586052, -0.04107508435845375, -0.01807251200079918, -0.010948119685053825, -0.014346098527312279, -0.017825227230787277, -0.037708062678575516, -0.03832336887717247, -0.054843224585056305, -0.058345239609479904, -0.030711138620972633, -0.05809948965907097, -0.007149348966777325, -0.025110475718975067, -0.022342389449477196, -0.03659481555223465]
[2025-05-04 07:28:53,320]: Mean: -0.0319
[2025-05-04 07:28:53,325]: Std: 0.0197
[2025-05-04 07:28:53,330]: Min: -0.0778
[2025-05-04 07:28:53,334]: Max: 0.0146
[2025-05-04 07:28:53,334]: 
Parameter: layer3.1.conv2.weight
[2025-05-04 07:28:53,334]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:28:53,381]: Sample Values (16 elements): [0.004145355895161629, 0.028409883379936218, 0.008143946528434753, 0.03110053762793541, -0.0301695354282856, 0.04226088151335716, 0.046004414558410645, -0.03306369483470917, 0.025195950642228127, -0.0062514301389455795, -0.0035676793195307255, -0.012906944379210472, 0.004173270892351866, -0.0347919799387455, 0.018448220565915108, 0.01054270751774311]
[2025-05-04 07:28:53,382]: Mean: 0.0001
[2025-05-04 07:28:53,382]: Std: 0.0250
[2025-05-04 07:28:53,383]: Min: -0.0905
[2025-05-04 07:28:53,383]: Max: 0.0780
[2025-05-04 07:28:53,383]: 
Parameter: layer3.1.bn2.weight
[2025-05-04 07:28:53,384]: Shape: torch.Size([64])
[2025-05-04 07:28:53,384]: Sample Values (16 elements): [1.04081130027771, 1.0379681587219238, 1.0163882970809937, 1.0482616424560547, 1.0172408819198608, 1.034543752670288, 1.0557676553726196, 1.0519561767578125, 1.0030925273895264, 1.0319404602050781, 1.036750078201294, 1.0312862396240234, 1.0601574182510376, 1.0108811855316162, 1.0275381803512573, 1.0128412246704102]
[2025-05-04 07:28:53,385]: Mean: 1.0307
[2025-05-04 07:28:53,385]: Std: 0.0165
[2025-05-04 07:28:53,386]: Min: 0.9990
[2025-05-04 07:28:53,386]: Max: 1.0661
[2025-05-04 07:28:53,386]: 
Parameter: layer3.1.bn2.bias
[2025-05-04 07:28:53,387]: Shape: torch.Size([64])
[2025-05-04 07:28:53,387]: Sample Values (16 elements): [0.01278561633080244, 0.013118989765644073, 0.020053422078490257, -0.0003808540350291878, 0.006742103490978479, 0.00660890294238925, -0.0017498033121228218, -0.003546643303707242, 0.029447022825479507, 0.002366213593631983, 0.041554633527994156, 0.02073410339653492, 0.005271997302770615, 0.004951037932187319, 0.024124082177877426, -0.006927161011844873]
[2025-05-04 07:28:53,388]: Mean: 0.0131
[2025-05-04 07:28:53,388]: Std: 0.0133
[2025-05-04 07:28:53,389]: Min: -0.0178
[2025-05-04 07:28:53,404]: Max: 0.0472
[2025-05-04 07:28:53,405]: 
Parameter: layer3.2.conv1.weight
[2025-05-04 07:28:53,405]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:28:53,471]: Sample Values (16 elements): [0.02680063620209694, 0.0382564440369606, 0.023890936747193336, 0.01076248474419117, -0.017031706869602203, 0.00966525450348854, -0.027053052559494972, 0.007550450041890144, 0.03304070606827736, 0.011304320767521858, -0.038850631564855576, 0.008582917973399162, -0.02304164320230484, 0.03193619102239609, -0.01471697073429823, -0.0020503888372331858]
[2025-05-04 07:28:53,472]: Mean: -0.0000
[2025-05-04 07:28:53,483]: Std: 0.0245
[2025-05-04 07:28:53,487]: Min: -0.0730
[2025-05-04 07:28:53,498]: Max: 0.0747
[2025-05-04 07:28:53,499]: 
Parameter: layer3.2.bn1.weight
[2025-05-04 07:28:53,499]: Shape: torch.Size([64])
[2025-05-04 07:28:53,504]: Sample Values (16 elements): [0.9617085456848145, 0.9848954081535339, 0.9723238945007324, 0.9695115685462952, 0.9889410138130188, 0.9837931394577026, 0.9670029878616333, 0.9676336050033569, 0.9759762287139893, 0.970170259475708, 0.9916911125183105, 0.9613763689994812, 0.9663260579109192, 0.9610943794250488, 0.9816681742668152, 0.9599210023880005]
[2025-05-04 07:28:53,504]: Mean: 0.9717
[2025-05-04 07:28:53,505]: Std: 0.0097
[2025-05-04 07:28:53,505]: Min: 0.9530
[2025-05-04 07:28:53,506]: Max: 0.9965
[2025-05-04 07:28:53,506]: 
Parameter: layer3.2.bn1.bias
[2025-05-04 07:28:53,506]: Shape: torch.Size([64])
[2025-05-04 07:28:53,510]: Sample Values (16 elements): [-0.017317889258265495, -0.006574756931513548, -0.0020538095850497484, -0.022689608857035637, -0.020303795114159584, -0.011899048462510109, -0.027680547907948494, -0.017802035436034203, -0.017554424703121185, -0.007871764712035656, -0.00317443092353642, -0.009367060847580433, -0.005331755615770817, -0.0044640726409852505, -0.0364871472120285, -0.008776486851274967]
[2025-05-04 07:28:53,510]: Mean: -0.0136
[2025-05-04 07:28:53,511]: Std: 0.0088
[2025-05-04 07:28:53,511]: Min: -0.0365
[2025-05-04 07:28:53,512]: Max: 0.0039
[2025-05-04 07:28:53,512]: 
Parameter: layer3.2.conv2.weight
[2025-05-04 07:28:53,512]: Shape: torch.Size([64, 64, 3, 3])
[2025-05-04 07:28:53,527]: Sample Values (16 elements): [0.019109131768345833, -0.03130730241537094, -0.02067931741476059, -0.030893787741661072, 0.014547839760780334, -0.010689780116081238, -0.027025993913412094, 0.02346157655119896, 0.00323352194391191, 0.021365216001868248, -0.010681756772100925, 0.0016415002755820751, -0.023694761097431183, 0.008404988795518875, -0.02581021748483181, 0.03218638524413109]
[2025-05-04 07:28:53,541]: Mean: 0.0005
[2025-05-04 07:28:53,542]: Std: 0.0237
[2025-05-04 07:28:53,543]: Min: -0.0630
[2025-05-04 07:28:53,545]: Max: 0.0695
[2025-05-04 07:28:53,545]: 
Parameter: layer3.2.bn2.weight
[2025-05-04 07:28:53,545]: Shape: torch.Size([64])
[2025-05-04 07:28:53,560]: Sample Values (16 elements): [1.072892427444458, 1.0334774255752563, 1.053348183631897, 1.0657914876937866, 1.0714151859283447, 1.0628352165222168, 1.0898295640945435, 1.0990097522735596, 1.084469199180603, 1.0889678001403809, 1.0434596538543701, 1.0477170944213867, 1.090991735458374, 1.0385767221450806, 1.075171709060669, 1.0128498077392578]
[2025-05-04 07:28:53,561]: Mean: 1.0634
[2025-05-04 07:28:53,564]: Std: 0.0201
[2025-05-04 07:28:53,568]: Min: 1.0127
[2025-05-04 07:28:53,579]: Max: 1.1026
[2025-05-04 07:28:53,580]: 
Parameter: layer3.2.bn2.bias
[2025-05-04 07:28:53,580]: Shape: torch.Size([64])
[2025-05-04 07:28:53,600]: Sample Values (16 elements): [0.01925552450120449, 0.042534053325653076, 0.009967452846467495, 0.05121997371315956, 0.011311727575957775, 0.030895572155714035, 0.03526340797543526, 0.03931868448853493, 0.034021470695734024, 0.03505054861307144, 0.027779031544923782, 0.03237351030111313, 0.02303122729063034, 0.014265147969126701, 0.04306594282388687, 0.027797415852546692]
[2025-05-04 07:28:53,612]: Mean: 0.0310
[2025-05-04 07:28:53,617]: Std: 0.0115
[2025-05-04 07:28:53,627]: Min: 0.0088
[2025-05-04 07:28:53,628]: Max: 0.0704
[2025-05-04 07:28:53,628]: 
Parameter: fc.weight
[2025-05-04 07:28:53,628]: Shape: torch.Size([10, 64])
[2025-05-04 07:28:53,629]: Sample Values (16 elements): [0.14582563936710358, 0.10020185261964798, 0.020701583474874496, -0.16974516212940216, 0.03908576816320419, 0.13086622953414917, -0.17144988477230072, -0.03985181078314781, 0.26162776350975037, -0.25046393275260925, -0.06293538957834244, 0.05065420642495155, 0.3676173686981201, 0.1857609748840332, -0.04260854795575142, 0.09483496099710464]
[2025-05-04 07:28:53,629]: Mean: -0.0024
[2025-05-04 07:28:53,630]: Std: 0.1970
[2025-05-04 07:28:53,632]: Min: -0.3385
[2025-05-04 07:28:53,637]: Max: 0.5551
[2025-05-04 07:28:53,638]: 
Parameter: fc.bias
[2025-05-04 07:28:53,638]: Shape: torch.Size([10])
[2025-05-04 07:28:53,645]: Sample Values (10 elements): [0.04746422916650772, -0.027653755620121956, 0.07371151447296143, -0.04156799986958504, -0.008643736131489277, 0.13385330140590668, 0.10431845486164093, 0.01849302090704441, 0.10684812068939209, 0.05408765748143196]
[2025-05-04 07:28:53,648]: Mean: 0.0461
[2025-05-04 07:28:53,650]: Std: 0.0601
[2025-05-04 07:28:53,652]: Min: -0.0416
[2025-05-04 07:28:53,654]: Max: 0.1339
[2025-05-04 07:28:53,655]: 


QAT of ResNet20 with relu down to 4 bits...
[2025-05-04 07:28:55,505]: [ResNet20_relu_quantized_4_bits] after configure_qat:
[2025-05-04 07:28:56,294]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-8, quant_max=7, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): Sequential(
        (0): ReLU(inplace=True)
        (1): QuantStub(
          (activation_post_process): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-04 07:35:25,987]: [ResNet20_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.4450 Train Acc: 0.8419 Eval Loss: 0.5545 Eval Acc: 0.8261 (LR: 0.001000)
[2025-05-04 07:39:04,206]: [ResNet20_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.4365 Train Acc: 0.8463 Eval Loss: 0.5179 Eval Acc: 0.8277 (LR: 0.001000)
[2025-05-04 07:41:11,273]: [ResNet20_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.4360 Train Acc: 0.8479 Eval Loss: 0.6485 Eval Acc: 0.7928 (LR: 0.001000)
[2025-05-04 07:43:31,622]: [ResNet20_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.4324 Train Acc: 0.8491 Eval Loss: 0.5276 Eval Acc: 0.8243 (LR: 0.001000)
[2025-05-04 07:45:37,925]: [ResNet20_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.4248 Train Acc: 0.8492 Eval Loss: 0.5455 Eval Acc: 0.8185 (LR: 0.001000)
[2025-05-04 07:47:40,774]: [ResNet20_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.4210 Train Acc: 0.8514 Eval Loss: 0.5212 Eval Acc: 0.8296 (LR: 0.001000)
[2025-05-04 07:49:48,676]: [ResNet20_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.4237 Train Acc: 0.8496 Eval Loss: 0.5203 Eval Acc: 0.8308 (LR: 0.001000)
[2025-05-04 07:51:56,683]: [ResNet20_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.4169 Train Acc: 0.8532 Eval Loss: 0.6114 Eval Acc: 0.8061 (LR: 0.001000)
[2025-05-04 07:54:07,634]: [ResNet20_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.4163 Train Acc: 0.8540 Eval Loss: 0.5243 Eval Acc: 0.8302 (LR: 0.001000)
[2025-05-04 07:56:18,122]: [ResNet20_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.4117 Train Acc: 0.8543 Eval Loss: 0.5865 Eval Acc: 0.8140 (LR: 0.001000)
[2025-05-04 07:58:29,052]: [ResNet20_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.4122 Train Acc: 0.8543 Eval Loss: 0.4852 Eval Acc: 0.8404 (LR: 0.001000)
[2025-05-04 08:00:40,720]: [ResNet20_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.4107 Train Acc: 0.8547 Eval Loss: 0.5191 Eval Acc: 0.8312 (LR: 0.001000)
[2025-05-04 08:02:50,740]: [ResNet20_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.4051 Train Acc: 0.8588 Eval Loss: 0.4890 Eval Acc: 0.8353 (LR: 0.001000)
[2025-05-04 08:05:01,326]: [ResNet20_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.3980 Train Acc: 0.8602 Eval Loss: 0.5486 Eval Acc: 0.8203 (LR: 0.001000)
[2025-05-04 08:07:12,701]: [ResNet20_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.4012 Train Acc: 0.8605 Eval Loss: 0.5293 Eval Acc: 0.8314 (LR: 0.001000)
[2025-05-04 08:09:14,892]: [ResNet20_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.3972 Train Acc: 0.8595 Eval Loss: 0.5852 Eval Acc: 0.8177 (LR: 0.001000)
[2025-05-04 08:12:44,709]: [ResNet20_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.3985 Train Acc: 0.8594 Eval Loss: 0.5299 Eval Acc: 0.8241 (LR: 0.001000)
[2025-05-04 08:16:05,940]: [ResNet20_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.3964 Train Acc: 0.8594 Eval Loss: 0.5084 Eval Acc: 0.8323 (LR: 0.001000)
[2025-05-04 08:19:24,267]: [ResNet20_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.3907 Train Acc: 0.8615 Eval Loss: 0.5026 Eval Acc: 0.8367 (LR: 0.001000)
[2025-05-04 08:22:42,021]: [ResNet20_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.3976 Train Acc: 0.8601 Eval Loss: 0.4815 Eval Acc: 0.8437 (LR: 0.001000)
