[2025-05-12 08:36:47,098]: 
Training ResNet20 with relu
[2025-05-12 08:38:19,968]: [ResNet20_relu] Epoch: 001 Train Loss: 1.8671 Train Acc: 0.2962 Eval Loss: 1.6955 Eval Acc: 0.3683 (LR: 0.001000)
[2025-05-12 08:39:48,556]: [ResNet20_relu] Epoch: 002 Train Loss: 1.5458 Train Acc: 0.4184 Eval Loss: 1.4919 Eval Acc: 0.4396 (LR: 0.001000)
[2025-05-12 08:41:06,496]: [ResNet20_relu] Epoch: 003 Train Loss: 1.3907 Train Acc: 0.4871 Eval Loss: 1.2838 Eval Acc: 0.5213 (LR: 0.001000)
[2025-05-12 08:42:13,171]: [ResNet20_relu] Epoch: 004 Train Loss: 1.2802 Train Acc: 0.5347 Eval Loss: 1.2441 Eval Acc: 0.5469 (LR: 0.001000)
[2025-05-12 08:43:24,046]: [ResNet20_relu] Epoch: 005 Train Loss: 1.1810 Train Acc: 0.5755 Eval Loss: 1.1449 Eval Acc: 0.5791 (LR: 0.001000)
[2025-05-12 08:44:19,161]: [ResNet20_relu] Epoch: 006 Train Loss: 1.1077 Train Acc: 0.5986 Eval Loss: 1.1796 Eval Acc: 0.5829 (LR: 0.001000)
[2025-05-12 08:45:35,827]: [ResNet20_relu] Epoch: 007 Train Loss: 1.0444 Train Acc: 0.6260 Eval Loss: 1.0582 Eval Acc: 0.6282 (LR: 0.001000)
[2025-05-12 08:46:55,949]: [ResNet20_relu] Epoch: 008 Train Loss: 1.0002 Train Acc: 0.6392 Eval Loss: 1.0459 Eval Acc: 0.6348 (LR: 0.001000)
[2025-05-12 08:48:18,822]: [ResNet20_relu] Epoch: 009 Train Loss: 0.9663 Train Acc: 0.6557 Eval Loss: 0.9205 Eval Acc: 0.6774 (LR: 0.001000)
[2025-05-12 08:49:47,649]: [ResNet20_relu] Epoch: 010 Train Loss: 0.9296 Train Acc: 0.6685 Eval Loss: 0.9044 Eval Acc: 0.6802 (LR: 0.001000)
[2025-05-12 08:51:18,037]: [ResNet20_relu] Epoch: 011 Train Loss: 0.8959 Train Acc: 0.6814 Eval Loss: 0.9768 Eval Acc: 0.6633 (LR: 0.001000)
[2025-05-12 08:52:47,473]: [ResNet20_relu] Epoch: 012 Train Loss: 0.8641 Train Acc: 0.6934 Eval Loss: 0.8451 Eval Acc: 0.6999 (LR: 0.001000)
[2025-05-12 08:54:13,909]: [ResNet20_relu] Epoch: 013 Train Loss: 0.8369 Train Acc: 0.7030 Eval Loss: 0.8812 Eval Acc: 0.6962 (LR: 0.001000)
[2025-05-12 08:55:30,620]: [ResNet20_relu] Epoch: 014 Train Loss: 0.8105 Train Acc: 0.7123 Eval Loss: 0.7869 Eval Acc: 0.7263 (LR: 0.001000)
[2025-05-12 08:56:46,857]: [ResNet20_relu] Epoch: 015 Train Loss: 0.7875 Train Acc: 0.7234 Eval Loss: 0.8022 Eval Acc: 0.7193 (LR: 0.001000)
[2025-05-12 08:58:27,205]: [ResNet20_relu] Epoch: 016 Train Loss: 0.7681 Train Acc: 0.7297 Eval Loss: 0.8245 Eval Acc: 0.7206 (LR: 0.001000)
[2025-05-12 08:59:58,188]: [ResNet20_relu] Epoch: 017 Train Loss: 0.7467 Train Acc: 0.7366 Eval Loss: 0.8521 Eval Acc: 0.7122 (LR: 0.001000)
[2025-05-12 09:01:30,062]: [ResNet20_relu] Epoch: 018 Train Loss: 0.7265 Train Acc: 0.7453 Eval Loss: 0.7277 Eval Acc: 0.7514 (LR: 0.001000)
[2025-05-12 09:02:58,179]: [ResNet20_relu] Epoch: 019 Train Loss: 0.7061 Train Acc: 0.7522 Eval Loss: 0.7121 Eval Acc: 0.7586 (LR: 0.001000)
[2025-05-12 09:04:31,107]: [ResNet20_relu] Epoch: 020 Train Loss: 0.6820 Train Acc: 0.7608 Eval Loss: 0.7249 Eval Acc: 0.7539 (LR: 0.001000)
[2025-05-12 09:06:01,915]: [ResNet20_relu] Epoch: 021 Train Loss: 0.6684 Train Acc: 0.7666 Eval Loss: 0.6933 Eval Acc: 0.7657 (LR: 0.001000)
[2025-05-12 09:07:27,496]: [ResNet20_relu] Epoch: 022 Train Loss: 0.6492 Train Acc: 0.7715 Eval Loss: 0.6565 Eval Acc: 0.7754 (LR: 0.001000)
[2025-05-12 09:08:54,149]: [ResNet20_relu] Epoch: 023 Train Loss: 0.6441 Train Acc: 0.7740 Eval Loss: 0.6571 Eval Acc: 0.7764 (LR: 0.001000)
[2025-05-12 09:10:13,939]: [ResNet20_relu] Epoch: 024 Train Loss: 0.6228 Train Acc: 0.7818 Eval Loss: 0.6346 Eval Acc: 0.7850 (LR: 0.001000)
[2025-05-12 09:11:27,351]: [ResNet20_relu] Epoch: 025 Train Loss: 0.6127 Train Acc: 0.7869 Eval Loss: 0.6339 Eval Acc: 0.7889 (LR: 0.001000)
[2025-05-12 09:12:39,032]: [ResNet20_relu] Epoch: 026 Train Loss: 0.6017 Train Acc: 0.7894 Eval Loss: 0.6193 Eval Acc: 0.7924 (LR: 0.001000)
[2025-05-12 09:13:59,034]: [ResNet20_relu] Epoch: 027 Train Loss: 0.5910 Train Acc: 0.7933 Eval Loss: 0.6106 Eval Acc: 0.7906 (LR: 0.001000)
[2025-05-12 09:15:18,211]: [ResNet20_relu] Epoch: 028 Train Loss: 0.5804 Train Acc: 0.7969 Eval Loss: 0.6005 Eval Acc: 0.7976 (LR: 0.001000)
[2025-05-12 09:16:41,062]: [ResNet20_relu] Epoch: 029 Train Loss: 0.5658 Train Acc: 0.8025 Eval Loss: 0.6011 Eval Acc: 0.7975 (LR: 0.001000)
[2025-05-12 09:18:12,058]: [ResNet20_relu] Epoch: 030 Train Loss: 0.5554 Train Acc: 0.8061 Eval Loss: 0.5464 Eval Acc: 0.8163 (LR: 0.001000)
[2025-05-12 09:19:45,215]: [ResNet20_relu] Epoch: 031 Train Loss: 0.5495 Train Acc: 0.8091 Eval Loss: 0.5995 Eval Acc: 0.8024 (LR: 0.001000)
[2025-05-12 09:21:11,187]: [ResNet20_relu] Epoch: 032 Train Loss: 0.5382 Train Acc: 0.8125 Eval Loss: 0.5794 Eval Acc: 0.8042 (LR: 0.001000)
[2025-05-12 09:22:44,544]: [ResNet20_relu] Epoch: 033 Train Loss: 0.5274 Train Acc: 0.8153 Eval Loss: 0.6078 Eval Acc: 0.7918 (LR: 0.001000)
[2025-05-12 09:24:10,488]: [ResNet20_relu] Epoch: 034 Train Loss: 0.5222 Train Acc: 0.8167 Eval Loss: 0.5943 Eval Acc: 0.8000 (LR: 0.001000)
[2025-05-12 09:25:41,834]: [ResNet20_relu] Epoch: 035 Train Loss: 0.5122 Train Acc: 0.8197 Eval Loss: 0.6041 Eval Acc: 0.7961 (LR: 0.001000)
[2025-05-12 09:27:15,312]: [ResNet20_relu] Epoch: 036 Train Loss: 0.5050 Train Acc: 0.8236 Eval Loss: 0.5714 Eval Acc: 0.8141 (LR: 0.001000)
[2025-05-12 09:28:44,060]: [ResNet20_relu] Epoch: 037 Train Loss: 0.5007 Train Acc: 0.8253 Eval Loss: 0.5356 Eval Acc: 0.8206 (LR: 0.001000)
[2025-05-12 09:30:11,636]: [ResNet20_relu] Epoch: 038 Train Loss: 0.4942 Train Acc: 0.8274 Eval Loss: 0.5324 Eval Acc: 0.8212 (LR: 0.001000)
[2025-05-12 09:31:39,935]: [ResNet20_relu] Epoch: 039 Train Loss: 0.4861 Train Acc: 0.8312 Eval Loss: 0.5418 Eval Acc: 0.8202 (LR: 0.001000)
[2025-05-12 09:32:56,730]: [ResNet20_relu] Epoch: 040 Train Loss: 0.4810 Train Acc: 0.8332 Eval Loss: 0.5390 Eval Acc: 0.8218 (LR: 0.001000)
[2025-05-12 09:34:11,043]: [ResNet20_relu] Epoch: 041 Train Loss: 0.4754 Train Acc: 0.8329 Eval Loss: 0.5049 Eval Acc: 0.8303 (LR: 0.001000)
[2025-05-12 09:35:19,227]: [ResNet20_relu] Epoch: 042 Train Loss: 0.4698 Train Acc: 0.8361 Eval Loss: 0.5863 Eval Acc: 0.8068 (LR: 0.001000)
[2025-05-12 09:36:46,170]: [ResNet20_relu] Epoch: 043 Train Loss: 0.4602 Train Acc: 0.8408 Eval Loss: 0.5554 Eval Acc: 0.8137 (LR: 0.001000)
[2025-05-12 09:38:12,687]: [ResNet20_relu] Epoch: 044 Train Loss: 0.4570 Train Acc: 0.8412 Eval Loss: 0.5079 Eval Acc: 0.8303 (LR: 0.001000)
[2025-05-12 09:39:46,109]: [ResNet20_relu] Epoch: 045 Train Loss: 0.4531 Train Acc: 0.8400 Eval Loss: 0.6014 Eval Acc: 0.8057 (LR: 0.001000)
[2025-05-12 09:41:26,495]: [ResNet20_relu] Epoch: 046 Train Loss: 0.4465 Train Acc: 0.8443 Eval Loss: 0.5183 Eval Acc: 0.8267 (LR: 0.001000)
[2025-05-12 09:43:07,807]: [ResNet20_relu] Epoch: 047 Train Loss: 0.4421 Train Acc: 0.8453 Eval Loss: 0.4893 Eval Acc: 0.8327 (LR: 0.001000)
[2025-05-12 09:44:34,459]: [ResNet20_relu] Epoch: 048 Train Loss: 0.4331 Train Acc: 0.8470 Eval Loss: 0.5177 Eval Acc: 0.8273 (LR: 0.001000)
[2025-05-12 09:45:47,822]: [ResNet20_relu] Epoch: 049 Train Loss: 0.4338 Train Acc: 0.8481 Eval Loss: 0.4982 Eval Acc: 0.8358 (LR: 0.001000)
[2025-05-12 09:46:57,563]: [ResNet20_relu] Epoch: 050 Train Loss: 0.4263 Train Acc: 0.8503 Eval Loss: 0.5343 Eval Acc: 0.8243 (LR: 0.001000)
[2025-05-12 09:48:04,877]: [ResNet20_relu] Epoch: 051 Train Loss: 0.4174 Train Acc: 0.8542 Eval Loss: 0.5137 Eval Acc: 0.8289 (LR: 0.001000)
[2025-05-12 09:48:47,217]: [ResNet20_relu] Epoch: 052 Train Loss: 0.4226 Train Acc: 0.8517 Eval Loss: 0.5243 Eval Acc: 0.8298 (LR: 0.001000)
[2025-05-12 09:49:55,765]: [ResNet20_relu] Epoch: 053 Train Loss: 0.4118 Train Acc: 0.8562 Eval Loss: 0.5365 Eval Acc: 0.8254 (LR: 0.001000)
[2025-05-12 09:51:07,476]: [ResNet20_relu] Epoch: 054 Train Loss: 0.4081 Train Acc: 0.8569 Eval Loss: 0.5501 Eval Acc: 0.8198 (LR: 0.001000)
[2025-05-12 09:52:21,703]: [ResNet20_relu] Epoch: 055 Train Loss: 0.4028 Train Acc: 0.8581 Eval Loss: 0.4786 Eval Acc: 0.8384 (LR: 0.001000)
[2025-05-12 09:53:39,366]: [ResNet20_relu] Epoch: 056 Train Loss: 0.4006 Train Acc: 0.8605 Eval Loss: 0.5016 Eval Acc: 0.8341 (LR: 0.001000)
[2025-05-12 09:54:58,785]: [ResNet20_relu] Epoch: 057 Train Loss: 0.3920 Train Acc: 0.8635 Eval Loss: 0.4920 Eval Acc: 0.8412 (LR: 0.001000)
[2025-05-12 09:56:14,510]: [ResNet20_relu] Epoch: 058 Train Loss: 0.3937 Train Acc: 0.8620 Eval Loss: 0.4892 Eval Acc: 0.8348 (LR: 0.001000)
[2025-05-12 09:57:26,135]: [ResNet20_relu] Epoch: 059 Train Loss: 0.3888 Train Acc: 0.8637 Eval Loss: 0.5092 Eval Acc: 0.8280 (LR: 0.001000)
[2025-05-12 09:58:43,837]: [ResNet20_relu] Epoch: 060 Train Loss: 0.3821 Train Acc: 0.8647 Eval Loss: 0.4650 Eval Acc: 0.8493 (LR: 0.001000)
[2025-05-12 09:59:55,871]: [ResNet20_relu] Epoch: 061 Train Loss: 0.3775 Train Acc: 0.8691 Eval Loss: 0.4601 Eval Acc: 0.8467 (LR: 0.001000)
[2025-05-12 10:01:02,976]: [ResNet20_relu] Epoch: 062 Train Loss: 0.3790 Train Acc: 0.8669 Eval Loss: 0.5052 Eval Acc: 0.8353 (LR: 0.001000)
[2025-05-12 10:02:08,848]: [ResNet20_relu] Epoch: 063 Train Loss: 0.3718 Train Acc: 0.8694 Eval Loss: 0.5319 Eval Acc: 0.8287 (LR: 0.001000)
[2025-05-12 10:03:04,376]: [ResNet20_relu] Epoch: 064 Train Loss: 0.3665 Train Acc: 0.8704 Eval Loss: 0.4734 Eval Acc: 0.8446 (LR: 0.001000)
[2025-05-12 10:04:02,526]: [ResNet20_relu] Epoch: 065 Train Loss: 0.3654 Train Acc: 0.8724 Eval Loss: 0.4519 Eval Acc: 0.8524 (LR: 0.001000)
[2025-05-12 10:04:56,219]: [ResNet20_relu] Epoch: 066 Train Loss: 0.3640 Train Acc: 0.8729 Eval Loss: 0.4875 Eval Acc: 0.8430 (LR: 0.001000)
[2025-05-12 10:06:03,429]: [ResNet20_relu] Epoch: 067 Train Loss: 0.3623 Train Acc: 0.8745 Eval Loss: 0.4841 Eval Acc: 0.8434 (LR: 0.001000)
[2025-05-12 10:07:06,360]: [ResNet20_relu] Epoch: 068 Train Loss: 0.3533 Train Acc: 0.8751 Eval Loss: 0.4684 Eval Acc: 0.8498 (LR: 0.001000)
[2025-05-12 10:08:15,485]: [ResNet20_relu] Epoch: 069 Train Loss: 0.3494 Train Acc: 0.8764 Eval Loss: 0.5308 Eval Acc: 0.8288 (LR: 0.001000)
[2025-05-12 10:09:20,614]: [ResNet20_relu] Epoch: 070 Train Loss: 0.3474 Train Acc: 0.8791 Eval Loss: 0.5170 Eval Acc: 0.8334 (LR: 0.000100)
[2025-05-12 10:10:29,860]: [ResNet20_relu] Epoch: 071 Train Loss: 0.3122 Train Acc: 0.8913 Eval Loss: 0.4081 Eval Acc: 0.8644 (LR: 0.000100)
[2025-05-12 10:11:36,075]: [ResNet20_relu] Epoch: 072 Train Loss: 0.3029 Train Acc: 0.8944 Eval Loss: 0.4021 Eval Acc: 0.8659 (LR: 0.000100)
[2025-05-12 10:12:42,084]: [ResNet20_relu] Epoch: 073 Train Loss: 0.3009 Train Acc: 0.8958 Eval Loss: 0.4076 Eval Acc: 0.8633 (LR: 0.000100)
[2025-05-12 10:13:49,511]: [ResNet20_relu] Epoch: 074 Train Loss: 0.2949 Train Acc: 0.8972 Eval Loss: 0.4025 Eval Acc: 0.8659 (LR: 0.000100)
[2025-05-12 10:14:51,162]: [ResNet20_relu] Epoch: 075 Train Loss: 0.2948 Train Acc: 0.8979 Eval Loss: 0.4072 Eval Acc: 0.8628 (LR: 0.000100)
[2025-05-12 10:15:51,752]: [ResNet20_relu] Epoch: 076 Train Loss: 0.2938 Train Acc: 0.8984 Eval Loss: 0.4012 Eval Acc: 0.8666 (LR: 0.000100)
[2025-05-12 10:16:46,940]: [ResNet20_relu] Epoch: 077 Train Loss: 0.2904 Train Acc: 0.8992 Eval Loss: 0.4035 Eval Acc: 0.8654 (LR: 0.000100)
[2025-05-12 10:17:47,638]: [ResNet20_relu] Epoch: 078 Train Loss: 0.2917 Train Acc: 0.8982 Eval Loss: 0.3981 Eval Acc: 0.8653 (LR: 0.000100)
[2025-05-12 10:18:47,467]: [ResNet20_relu] Epoch: 079 Train Loss: 0.2900 Train Acc: 0.8984 Eval Loss: 0.4008 Eval Acc: 0.8661 (LR: 0.000100)
[2025-05-12 10:19:56,636]: [ResNet20_relu] Epoch: 080 Train Loss: 0.2925 Train Acc: 0.8984 Eval Loss: 0.4044 Eval Acc: 0.8616 (LR: 0.000100)
[2025-05-12 10:21:02,749]: [ResNet20_relu] Epoch: 081 Train Loss: 0.2886 Train Acc: 0.9004 Eval Loss: 0.4049 Eval Acc: 0.8645 (LR: 0.000100)
[2025-05-12 10:22:14,337]: [ResNet20_relu] Epoch: 082 Train Loss: 0.2882 Train Acc: 0.9006 Eval Loss: 0.4019 Eval Acc: 0.8672 (LR: 0.000100)
[2025-05-12 10:23:23,323]: [ResNet20_relu] Epoch: 083 Train Loss: 0.2872 Train Acc: 0.8996 Eval Loss: 0.4062 Eval Acc: 0.8640 (LR: 0.000100)
[2025-05-12 10:24:32,835]: [ResNet20_relu] Epoch: 084 Train Loss: 0.2865 Train Acc: 0.9005 Eval Loss: 0.4032 Eval Acc: 0.8646 (LR: 0.000100)
[2025-05-12 10:25:46,326]: [ResNet20_relu] Epoch: 085 Train Loss: 0.2850 Train Acc: 0.9015 Eval Loss: 0.4091 Eval Acc: 0.8633 (LR: 0.000100)
[2025-05-12 10:27:01,969]: [ResNet20_relu] Epoch: 086 Train Loss: 0.2841 Train Acc: 0.9016 Eval Loss: 0.4026 Eval Acc: 0.8637 (LR: 0.000100)
[2025-05-12 10:28:12,588]: [ResNet20_relu] Epoch: 087 Train Loss: 0.2848 Train Acc: 0.8999 Eval Loss: 0.3999 Eval Acc: 0.8666 (LR: 0.000100)
[2025-05-12 10:29:17,264]: [ResNet20_relu] Epoch: 088 Train Loss: 0.2849 Train Acc: 0.9006 Eval Loss: 0.4008 Eval Acc: 0.8637 (LR: 0.000100)
[2025-05-12 10:30:18,967]: [ResNet20_relu] Epoch: 089 Train Loss: 0.2815 Train Acc: 0.9013 Eval Loss: 0.4004 Eval Acc: 0.8674 (LR: 0.000100)
[2025-05-12 10:31:02,046]: [ResNet20_relu] Epoch: 090 Train Loss: 0.2818 Train Acc: 0.9019 Eval Loss: 0.4073 Eval Acc: 0.8643 (LR: 0.000100)
[2025-05-12 10:32:06,128]: [ResNet20_relu] Epoch: 091 Train Loss: 0.2825 Train Acc: 0.9020 Eval Loss: 0.4066 Eval Acc: 0.8640 (LR: 0.000100)
[2025-05-12 10:33:12,087]: [ResNet20_relu] Epoch: 092 Train Loss: 0.2821 Train Acc: 0.9018 Eval Loss: 0.4013 Eval Acc: 0.8654 (LR: 0.000100)
[2025-05-12 10:34:20,899]: [ResNet20_relu] Epoch: 093 Train Loss: 0.2790 Train Acc: 0.9029 Eval Loss: 0.4070 Eval Acc: 0.8637 (LR: 0.000100)
[2025-05-12 10:35:33,069]: [ResNet20_relu] Epoch: 094 Train Loss: 0.2861 Train Acc: 0.8999 Eval Loss: 0.4033 Eval Acc: 0.8654 (LR: 0.000100)
[2025-05-12 10:36:46,885]: [ResNet20_relu] Epoch: 095 Train Loss: 0.2801 Train Acc: 0.9026 Eval Loss: 0.4018 Eval Acc: 0.8665 (LR: 0.000100)
[2025-05-12 10:37:58,431]: [ResNet20_relu] Epoch: 096 Train Loss: 0.2772 Train Acc: 0.9039 Eval Loss: 0.4023 Eval Acc: 0.8670 (LR: 0.000100)
[2025-05-12 10:39:08,110]: [ResNet20_relu] Epoch: 097 Train Loss: 0.2810 Train Acc: 0.9017 Eval Loss: 0.4087 Eval Acc: 0.8629 (LR: 0.000100)
[2025-05-12 10:40:19,008]: [ResNet20_relu] Epoch: 098 Train Loss: 0.2763 Train Acc: 0.9042 Eval Loss: 0.4059 Eval Acc: 0.8654 (LR: 0.000100)
[2025-05-12 10:41:31,139]: [ResNet20_relu] Epoch: 099 Train Loss: 0.2744 Train Acc: 0.9045 Eval Loss: 0.4020 Eval Acc: 0.8661 (LR: 0.000100)
[2025-05-12 10:42:39,270]: [ResNet20_relu] Epoch: 100 Train Loss: 0.2789 Train Acc: 0.9025 Eval Loss: 0.4034 Eval Acc: 0.8661 (LR: 0.000010)
[2025-05-12 10:43:47,810]: [ResNet20_relu] Epoch: 101 Train Loss: 0.2724 Train Acc: 0.9051 Eval Loss: 0.4028 Eval Acc: 0.8661 (LR: 0.000010)
[2025-05-12 10:44:57,311]: [ResNet20_relu] Epoch: 102 Train Loss: 0.2700 Train Acc: 0.9058 Eval Loss: 0.3975 Eval Acc: 0.8684 (LR: 0.000010)
[2025-05-12 10:46:01,171]: [ResNet20_relu] Epoch: 103 Train Loss: 0.2722 Train Acc: 0.9050 Eval Loss: 0.4009 Eval Acc: 0.8659 (LR: 0.000010)
[2025-05-12 10:47:07,146]: [ResNet20_relu] Epoch: 104 Train Loss: 0.2740 Train Acc: 0.9043 Eval Loss: 0.4004 Eval Acc: 0.8675 (LR: 0.000010)
[2025-05-12 10:47:55,834]: [ResNet20_relu] Epoch: 105 Train Loss: 0.2682 Train Acc: 0.9074 Eval Loss: 0.3995 Eval Acc: 0.8665 (LR: 0.000010)
[2025-05-12 10:48:53,227]: [ResNet20_relu] Epoch: 106 Train Loss: 0.2718 Train Acc: 0.9046 Eval Loss: 0.3999 Eval Acc: 0.8669 (LR: 0.000010)
[2025-05-12 10:49:52,433]: [ResNet20_relu] Epoch: 107 Train Loss: 0.2708 Train Acc: 0.9061 Eval Loss: 0.4000 Eval Acc: 0.8666 (LR: 0.000010)
[2025-05-12 10:51:01,990]: [ResNet20_relu] Epoch: 108 Train Loss: 0.2716 Train Acc: 0.9052 Eval Loss: 0.4023 Eval Acc: 0.8655 (LR: 0.000010)
[2025-05-12 10:52:10,270]: [ResNet20_relu] Epoch: 109 Train Loss: 0.2730 Train Acc: 0.9045 Eval Loss: 0.4020 Eval Acc: 0.8645 (LR: 0.000010)
[2025-05-12 10:53:18,625]: [ResNet20_relu] Epoch: 110 Train Loss: 0.2709 Train Acc: 0.9052 Eval Loss: 0.4004 Eval Acc: 0.8666 (LR: 0.000010)
[2025-05-12 10:54:29,631]: [ResNet20_relu] Epoch: 111 Train Loss: 0.2722 Train Acc: 0.9059 Eval Loss: 0.4016 Eval Acc: 0.8665 (LR: 0.000010)
[2025-05-12 10:55:33,814]: [ResNet20_relu] Epoch: 112 Train Loss: 0.2710 Train Acc: 0.9062 Eval Loss: 0.4023 Eval Acc: 0.8658 (LR: 0.000010)
[2025-05-12 10:56:42,661]: [ResNet20_relu] Epoch: 113 Train Loss: 0.2713 Train Acc: 0.9061 Eval Loss: 0.3989 Eval Acc: 0.8658 (LR: 0.000010)
[2025-05-12 10:57:39,344]: [ResNet20_relu] Epoch: 114 Train Loss: 0.2692 Train Acc: 0.9053 Eval Loss: 0.4001 Eval Acc: 0.8665 (LR: 0.000010)
[2025-05-12 10:58:41,584]: [ResNet20_relu] Epoch: 115 Train Loss: 0.2697 Train Acc: 0.9060 Eval Loss: 0.4001 Eval Acc: 0.8664 (LR: 0.000010)
[2025-05-12 10:59:31,077]: [ResNet20_relu] Epoch: 116 Train Loss: 0.2702 Train Acc: 0.9057 Eval Loss: 0.3986 Eval Acc: 0.8659 (LR: 0.000010)
[2025-05-12 11:00:35,721]: [ResNet20_relu] Epoch: 117 Train Loss: 0.2672 Train Acc: 0.9068 Eval Loss: 0.4007 Eval Acc: 0.8656 (LR: 0.000010)
[2025-05-12 11:01:37,215]: [ResNet20_relu] Epoch: 118 Train Loss: 0.2714 Train Acc: 0.9056 Eval Loss: 0.4013 Eval Acc: 0.8667 (LR: 0.000010)
[2025-05-12 11:02:45,458]: [ResNet20_relu] Epoch: 119 Train Loss: 0.2731 Train Acc: 0.9051 Eval Loss: 0.3995 Eval Acc: 0.8664 (LR: 0.000010)
[2025-05-12 11:03:49,625]: [ResNet20_relu] Epoch: 120 Train Loss: 0.2741 Train Acc: 0.9045 Eval Loss: 0.4015 Eval Acc: 0.8657 (LR: 0.000010)
[2025-05-12 11:03:49,627]: [ResNet20_relu] Best Eval Accuracy: 0.8684
[2025-05-12 11:03:49,667]: 
Training of full-precision model finished!
[2025-05-12 11:03:49,667]: Model Architecture:
[2025-05-12 11:03:49,669]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:03:49,669]: 
Model Weights:
[2025-05-12 11:03:49,669]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 11:03:49,702]: Sample Values (25 elements): [0.1672813892364502, 0.025902707129716873, -0.10870786011219025, -0.008470811881124973, 0.12829986214637756, -0.18567603826522827, -0.0796230360865593, 0.10363553464412689, -0.1222049668431282, -0.14104744791984558, -0.1874222755432129, 0.13677513599395752, -0.015882298350334167, -0.01927156187593937, 0.017226865515112877, -0.22051352262496948, -0.11098002642393112, -0.18085923790931702, -0.043526824563741684, 0.0007161171524785459, -0.11275704950094223, 0.21342739462852478, -0.282463401556015, 0.10949412733316422, 0.08446481823921204]
[2025-05-12 11:03:49,717]: Mean: -0.00001662
[2025-05-12 11:03:49,736]: Min: -0.43581069
[2025-05-12 11:03:49,749]: Max: 0.52677101
[2025-05-12 11:03:49,749]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,755]: Sample Values (16 elements): [0.9192759394645691, 1.1424061059951782, 0.9473211765289307, 0.8104667067527771, 0.9799903035163879, 0.8378646373748779, 0.9747645854949951, 0.9167495965957642, 1.0496646165847778, 0.8646917939186096, 0.891884982585907, 0.9650465846061707, 0.9577247500419617, 0.9430510401725769, 0.9962366819381714, 0.9386895298957825]
[2025-05-12 11:03:49,755]: Mean: 0.94598931
[2025-05-12 11:03:49,756]: Min: 0.81046671
[2025-05-12 11:03:49,756]: Max: 1.14240611
[2025-05-12 11:03:49,756]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:03:49,757]: Sample Values (25 elements): [0.03466276451945305, 0.100969098508358, 0.007498604711145163, 0.035308580845594406, 0.04584395885467529, 0.0015859371051192284, -0.10119505971670151, -0.05877724289894104, -0.017044970765709877, -0.10016153007745743, 0.06048154458403587, -0.02131347917020321, 0.049199335277080536, -0.005226886831223965, -0.0529305674135685, -0.026868911460042, 0.07436998933553696, -0.031938664615154266, -0.15229417383670807, 0.0018515904666855931, -0.04943528771400452, 0.0035294517874717712, 0.07422824203968048, 0.0045716166496276855, -0.08873479813337326]
[2025-05-12 11:03:49,759]: Mean: -0.00495233
[2025-05-12 11:03:49,764]: Min: -0.29601938
[2025-05-12 11:03:49,766]: Max: 0.21633247
[2025-05-12 11:03:49,766]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,768]: Sample Values (16 elements): [0.9482596516609192, 0.9357519149780273, 0.9365490078926086, 0.9810147285461426, 0.9984458088874817, 0.9371829628944397, 0.9942519664764404, 0.9359910488128662, 0.8892720937728882, 1.0264835357666016, 0.9786602258682251, 0.9619690775871277, 0.9252657294273376, 0.9834513068199158, 1.0628933906555176, 1.0052356719970703]
[2025-05-12 11:03:49,769]: Mean: 0.96879238
[2025-05-12 11:03:49,770]: Min: 0.88927209
[2025-05-12 11:03:49,772]: Max: 1.06289339
[2025-05-12 11:03:49,772]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:03:49,777]: Sample Values (25 elements): [-0.034969937056303024, -0.033292531967163086, 0.08047577738761902, -0.054338257759809494, 0.03405827656388283, -0.060738325119018555, -0.0039601800963282585, 0.009646727703511715, -0.07430902123451233, -0.05854049697518349, -0.053080473095178604, -0.003988655749708414, 0.0612947940826416, -0.027751078829169273, -0.05896533280611038, -0.002727474318817258, 0.09398001432418823, -0.009683714248239994, 0.03730359673500061, -0.0050047715194523335, -0.06895333528518677, 0.0318608395755291, 0.030161118134856224, -0.06487996131181717, -0.06556186079978943]
[2025-05-12 11:03:49,777]: Mean: -0.00439626
[2025-05-12 11:03:49,779]: Min: -0.19966611
[2025-05-12 11:03:49,780]: Max: 0.18803242
[2025-05-12 11:03:49,780]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,787]: Sample Values (16 elements): [0.8978532552719116, 1.2265270948410034, 0.9891373515129089, 0.9526202082633972, 0.9311425685882568, 1.0413154363632202, 1.0181589126586914, 0.963767945766449, 0.9362215995788574, 1.0244061946868896, 0.9784306287765503, 0.9902480840682983, 1.0416855812072754, 1.0114600658416748, 1.0104297399520874, 0.9606925249099731]
[2025-05-12 11:03:49,797]: Mean: 0.99838102
[2025-05-12 11:03:49,800]: Min: 0.89785326
[2025-05-12 11:03:49,802]: Max: 1.22652709
[2025-05-12 11:03:49,802]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:03:49,817]: Sample Values (25 elements): [-0.05334723740816116, 0.1372271031141281, 0.035147614777088165, 0.05485198646783829, -0.026238683611154556, 0.05721268057823181, 0.011664270423352718, 0.04202983155846596, -0.005541977006942034, 0.014375672675669193, -0.10086651891469955, 0.06346599012613297, 0.035081762820482254, -0.020677994936704636, -0.09112302213907242, -0.1205899640917778, 0.04100654274225235, 0.025613008067011833, 0.033865321427583694, -0.07797176390886307, -0.02374291606247425, 0.09585373848676682, -0.01705710031092167, 0.022289510816335678, -0.06799237430095673]
[2025-05-12 11:03:49,822]: Mean: -0.00219210
[2025-05-12 11:03:49,829]: Min: -0.18936470
[2025-05-12 11:03:49,830]: Max: 0.18262576
[2025-05-12 11:03:49,830]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,849]: Sample Values (16 elements): [0.9426449537277222, 0.9477482438087463, 0.9472053647041321, 0.9712613224983215, 1.0578275918960571, 0.9714667201042175, 0.923881471157074, 0.9898893237113953, 0.9479033946990967, 0.9515884518623352, 0.9459415078163147, 0.948307991027832, 0.9768722057342529, 1.0083568096160889, 0.9984839558601379, 1.006091833114624]
[2025-05-12 11:03:49,849]: Mean: 0.97096694
[2025-05-12 11:03:49,851]: Min: 0.92388147
[2025-05-12 11:03:49,853]: Max: 1.05782759
[2025-05-12 11:03:49,853]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:03:49,855]: Sample Values (25 elements): [-0.07685644179582596, -0.007541479542851448, 0.018345003947615623, 0.05103365331888199, 0.022955074906349182, 0.05719788372516632, -0.051720958203077316, -0.05698704719543457, 0.0912492647767067, 0.02758532203733921, -0.05337848514318466, -0.03140091150999069, 0.03168901801109314, 0.050144195556640625, -0.03871393948793411, -0.09254015982151031, -0.03191791847348213, 0.0345667228102684, -0.048271723091602325, 0.09713124483823776, 0.04916907474398613, 0.0977880135178566, 0.07227785140275955, 0.0033762548118829727, 0.05426003038883209]
[2025-05-12 11:03:49,857]: Mean: -0.00192075
[2025-05-12 11:03:49,859]: Min: -0.19747603
[2025-05-12 11:03:49,860]: Max: 0.23784670
[2025-05-12 11:03:49,860]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,868]: Sample Values (16 elements): [0.877650260925293, 0.9141035079956055, 0.9218272566795349, 0.9814271926879883, 0.9774323105812073, 0.9620606899261475, 0.915681004524231, 0.9521343111991882, 0.9534263014793396, 0.9232856631278992, 0.9326984882354736, 0.8929886221885681, 0.9791973233222961, 0.9060589075088501, 0.912013828754425, 0.923081636428833]
[2025-05-12 11:03:49,875]: Mean: 0.93281668
[2025-05-12 11:03:49,883]: Min: 0.87765026
[2025-05-12 11:03:49,893]: Max: 0.98142719
[2025-05-12 11:03:49,893]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:03:49,894]: Sample Values (25 elements): [-0.09653005748987198, -0.02093571051955223, -0.0006598423351533711, 0.1336323320865631, -0.04125360772013664, 0.06292872130870819, 0.02599303051829338, 0.01270418893545866, 0.06086346507072449, -0.03360091894865036, 0.060033030807971954, -0.028258824720978737, -0.051253512501716614, 0.0062087057158350945, 0.12142433971166611, -0.07193664461374283, 0.09191771596670151, 0.06543511897325516, -0.06347541511058807, 0.016074949875473976, 0.008586689829826355, 0.09354154020547867, 0.03396610543131828, -0.016886932775378227, 0.03954551741480827]
[2025-05-12 11:03:49,895]: Mean: -0.00372597
[2025-05-12 11:03:49,895]: Min: -0.32211152
[2025-05-12 11:03:49,896]: Max: 0.21042004
[2025-05-12 11:03:49,896]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,901]: Sample Values (16 elements): [0.9737592339515686, 0.9371469020843506, 0.9832660555839539, 0.9803362488746643, 0.9583292603492737, 1.0014116764068604, 0.9664613604545593, 0.9405369162559509, 0.9549826383590698, 0.9739813804626465, 1.002218246459961, 0.9520848393440247, 1.0011298656463623, 0.9267450571060181, 1.0761793851852417, 0.9086737632751465]
[2025-05-12 11:03:49,903]: Mean: 0.97107768
[2025-05-12 11:03:49,905]: Min: 0.90867376
[2025-05-12 11:03:49,906]: Max: 1.07617939
[2025-05-12 11:03:49,906]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 11:03:49,908]: Sample Values (25 elements): [-0.015272737480700016, -0.04754924029111862, -0.024126123636960983, -0.005424798931926489, 0.03233763575553894, -0.00027226711972616613, 0.019901543855667114, -0.016303349286317825, 0.010433869436383247, -0.014121398329734802, -0.05650550127029419, -0.054303448647260666, -0.0932779535651207, 0.003385137999430299, 0.00820157490670681, -0.03694070503115654, -0.050365764647722244, 0.008003395050764084, -0.01127537339925766, -0.02342783659696579, -0.09986480325460434, 0.04933355748653412, -0.052238065749406815, 0.022978996858000755, 0.01815147139132023]
[2025-05-12 11:03:49,924]: Mean: 0.00128373
[2025-05-12 11:03:49,932]: Min: -0.14271773
[2025-05-12 11:03:49,934]: Max: 0.17940076
[2025-05-12 11:03:49,935]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 11:03:49,940]: Sample Values (16 elements): [0.9127805829048157, 1.008456826210022, 0.9598559737205505, 1.0289273262023926, 0.9949264526367188, 0.9966403841972351, 0.9690754413604736, 0.9732624292373657, 0.9011421203613281, 1.024119257926941, 0.9883057475090027, 0.9686153531074524, 0.9247317910194397, 0.8971155285835266, 1.044686198234558, 0.9900361895561218]
[2025-05-12 11:03:49,956]: Mean: 0.97391737
[2025-05-12 11:03:49,960]: Min: 0.89711553
[2025-05-12 11:03:49,968]: Max: 1.04468620
[2025-05-12 11:03:49,968]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 11:03:49,976]: Sample Values (25 elements): [-0.0904587134718895, -0.08567096292972565, 0.01513984426856041, 0.02835923247039318, 0.058657895773649216, 0.06456097960472107, 0.0007334231049753726, 0.010463039390742779, -0.026110006496310234, -0.07799951732158661, 0.004999415948987007, 0.05950312316417694, -0.03403232619166374, -0.018068689852952957, 0.04921119660139084, 0.06694867461919785, 0.008984951302409172, 0.05642412230372429, 0.09946493059396744, -0.03396746143698692, 0.0017487930599600077, 0.013314641080796719, -0.011406856589019299, 0.051544077694416046, -0.12547339498996735]
[2025-05-12 11:03:49,988]: Mean: -0.00365959
[2025-05-12 11:03:49,990]: Min: -0.16288182
[2025-05-12 11:03:49,992]: Max: 0.14325449
[2025-05-12 11:03:49,992]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:03:49,997]: Sample Values (25 elements): [1.0196932554244995, 0.9676250219345093, 0.998149573802948, 0.9537562131881714, 0.9830787777900696, 0.9513919353485107, 1.0254203081130981, 0.9939019083976746, 0.9602985978126526, 0.9344885349273682, 0.9569142460823059, 0.9276427626609802, 0.9995118975639343, 0.9640065431594849, 0.9938172101974487, 1.0074591636657715, 0.9425389766693115, 0.9550663232803345, 0.9796881079673767, 0.950227677822113, 0.917579710483551, 0.9845616817474365, 0.9984564185142517, 1.017296314239502, 0.9523324966430664]
[2025-05-12 11:03:50,002]: Mean: 0.97124082
[2025-05-12 11:03:50,005]: Min: 0.91757971
[2025-05-12 11:03:50,012]: Max: 1.02542031
[2025-05-12 11:03:50,012]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:03:50,030]: Sample Values (25 elements): [-0.016155876219272614, 0.02543194778263569, 0.047774553298950195, -0.011641948483884335, -0.012403211556375027, 0.03006022423505783, 0.04352477937936783, 0.026209399104118347, -0.010073862038552761, 0.03545435890555382, 0.013902583159506321, 0.01037344429641962, -0.02205091528594494, -0.0639321431517601, 0.01482625026255846, -0.024740274995565414, -0.006510415114462376, -0.0056440569460392, 0.052340928465127945, 0.021802036091685295, -0.02791069634258747, 0.03965073451399803, 0.003098689951002598, 0.04631660133600235, -0.011142436414957047]
[2025-05-12 11:03:50,031]: Mean: -0.00155900
[2025-05-12 11:03:50,032]: Min: -0.14331737
[2025-05-12 11:03:50,032]: Max: 0.14998610
[2025-05-12 11:03:50,032]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:03:50,035]: Sample Values (25 elements): [1.0139697790145874, 0.993686318397522, 1.0032137632369995, 1.0120820999145508, 0.9860694408416748, 1.0091490745544434, 1.016239881515503, 1.012188196182251, 0.9902689456939697, 0.9809809923171997, 1.0068111419677734, 0.9969932436943054, 1.0287412405014038, 0.9795038104057312, 0.9702333211898804, 0.9861143231391907, 0.9699983596801758, 0.9813241958618164, 0.9708111882209778, 1.0082145929336548, 0.9811649918556213, 1.0015405416488647, 0.9737055897712708, 0.9799878001213074, 0.991974949836731]
[2025-05-12 11:03:50,038]: Mean: 0.99446952
[2025-05-12 11:03:50,040]: Min: 0.95095176
[2025-05-12 11:03:50,042]: Max: 1.05667210
[2025-05-12 11:03:50,042]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 11:03:50,045]: Sample Values (25 elements): [0.15105153620243073, -0.0018537583528086543, -0.11098106950521469, -0.03787314146757126, 0.003792875912040472, -0.2552841901779175, -0.16393616795539856, 0.08478862792253494, 0.0037531026173382998, 0.13829828798770905, 0.04875388368964195, 0.2349005937576294, -0.025667883455753326, -0.08002639561891556, 0.1587604433298111, -0.1902533620595932, -0.23671558499336243, -0.22566856443881989, 0.27141350507736206, -0.15185797214508057, -0.023977164179086685, 0.14878979325294495, 0.008734512142837048, -0.195653036236763, 0.2017495036125183]
[2025-05-12 11:03:50,045]: Mean: -0.00295940
[2025-05-12 11:03:50,047]: Min: -0.32013550
[2025-05-12 11:03:50,048]: Max: 0.32192630
[2025-05-12 11:03:50,048]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 11:03:50,050]: Sample Values (25 elements): [0.9634402394294739, 0.8958108425140381, 0.8974193334579468, 0.8663589358329773, 0.8913792371749878, 0.8771443367004395, 0.8845626711845398, 0.9602941870689392, 0.9134500026702881, 0.9038518071174622, 0.8867499232292175, 0.9205361604690552, 0.8984596729278564, 0.9095310568809509, 0.8653610944747925, 0.88017737865448, 0.9248875379562378, 0.9176700711250305, 0.9182486534118652, 0.8689156174659729, 0.9098578691482544, 0.8994680047035217, 0.83033686876297, 0.879255473613739, 0.8978694081306458]
[2025-05-12 11:03:50,054]: Mean: 0.90102738
[2025-05-12 11:03:50,058]: Min: 0.83033687
[2025-05-12 11:03:50,059]: Max: 0.96344024
[2025-05-12 11:03:50,059]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:03:50,061]: Sample Values (25 elements): [0.07835667580366135, 0.037635475397109985, -0.04312611743807793, 0.030720602720975876, 0.020833730697631836, 0.010081151500344276, 0.0020176847465336323, -0.049875419586896896, 0.06962481886148453, 0.06700431555509567, -0.017958881333470345, -0.03972905874252319, -0.021027442067861557, -0.04994189366698265, -0.016271794214844704, -0.00576959690079093, -0.04595227912068367, -0.043999239802360535, 0.033319611102342606, -0.07812425494194031, 0.013238076120615005, 0.06063211336731911, -0.11619497090578079, -0.0014929318567737937, 0.028870418667793274]
[2025-05-12 11:03:50,063]: Mean: -0.00145656
[2025-05-12 11:03:50,065]: Min: -0.12524994
[2025-05-12 11:03:50,078]: Max: 0.12858683
[2025-05-12 11:03:50,079]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:03:50,083]: Sample Values (25 elements): [0.9851658940315247, 0.9943194389343262, 0.9783129096031189, 0.9726434350013733, 0.9479469656944275, 0.9632989168167114, 0.9672740697860718, 0.9298843145370483, 0.9603214263916016, 0.9580442309379578, 1.002356767654419, 0.9766529202461243, 0.9283661842346191, 0.9593846201896667, 0.9896669387817383, 0.9491021633148193, 0.935570478439331, 0.982918918132782, 0.9607099890708923, 1.0024174451828003, 0.9901496171951294, 0.977793276309967, 0.9743730425834656, 1.008023738861084, 0.9638784527778625]
[2025-05-12 11:03:50,093]: Mean: 0.97152996
[2025-05-12 11:03:50,096]: Min: 0.92836618
[2025-05-12 11:03:50,101]: Max: 1.00802374
[2025-05-12 11:03:50,101]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:03:50,123]: Sample Values (25 elements): [0.011036636307835579, 0.042066093534231186, -0.031005723401904106, 0.006364429369568825, 0.04408321529626846, 0.0012421573046594858, -0.015647178515791893, -0.020114434882998466, -0.049502499401569366, -0.06279169023036957, 0.004949553869664669, -0.026328235864639282, -0.02084539458155632, -0.014247914776206017, 0.0756322368979454, -0.04418089985847473, 0.014505651779472828, 0.017215508967638016, -0.05053959786891937, 0.06634718924760818, -0.027792278677225113, -0.05641942471265793, -0.025935065001249313, 0.05465110018849373, -0.0699276253581047]
[2025-05-12 11:03:50,127]: Mean: -0.00052599
[2025-05-12 11:03:50,129]: Min: -0.12124870
[2025-05-12 11:03:50,131]: Max: 0.13481706
[2025-05-12 11:03:50,131]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:03:50,133]: Sample Values (25 elements): [0.9440650939941406, 0.9777724742889404, 1.0362122058868408, 0.9920315146446228, 0.9965014457702637, 0.9583775401115417, 0.991655170917511, 1.0118656158447266, 0.9978442788124084, 0.9853314757347107, 0.9984430074691772, 0.9912518858909607, 0.9932669997215271, 0.9795293807983398, 1.0055813789367676, 1.0006190538406372, 0.9465985894203186, 0.9700725674629211, 0.9777950644493103, 0.9915192127227783, 0.9616401791572571, 0.9970536231994629, 0.9775446653366089, 0.9374150633811951, 1.0251715183258057]
[2025-05-12 11:03:50,135]: Mean: 0.98432624
[2025-05-12 11:03:50,140]: Min: 0.93741506
[2025-05-12 11:03:50,143]: Max: 1.03621221
[2025-05-12 11:03:50,143]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:03:50,158]: Sample Values (25 elements): [-0.021585697308182716, -0.029304400086402893, 0.021865900605916977, -0.019425539299845695, 0.016489392146468163, -0.05256786197423935, 0.05690859630703926, 0.013810593634843826, -0.030313584953546524, -0.06310724467039108, 0.03615735471248627, 0.02503957599401474, -0.03258712217211723, 0.011757428757846355, 0.06263743340969086, -0.043154820799827576, -0.01068039983510971, 0.04910770431160927, 0.06933488696813583, 0.0900958999991417, -0.005341073498129845, 0.02159036509692669, -0.016052620485424995, -0.007897578179836273, 0.03309084102511406]
[2025-05-12 11:03:50,168]: Mean: -0.00088501
[2025-05-12 11:03:50,169]: Min: -0.11815743
[2025-05-12 11:03:50,170]: Max: 0.13946357
[2025-05-12 11:03:50,170]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 11:03:50,171]: Sample Values (25 elements): [0.9701454043388367, 0.9703564643859863, 0.9720879793167114, 0.9631433486938477, 0.9877294301986694, 0.953454852104187, 0.9821905493736267, 0.9640756249427795, 0.9880685806274414, 0.9512388706207275, 0.9788587093353271, 0.9530773758888245, 0.9917093515396118, 0.9411704540252686, 0.9701668620109558, 0.984537661075592, 0.959732174873352, 0.960074245929718, 0.9800375699996948, 0.9695833325386047, 0.9885944128036499, 0.9717445969581604, 0.931171178817749, 0.9832630157470703, 0.9846714735031128]
[2025-05-12 11:03:50,172]: Mean: 0.97162533
[2025-05-12 11:03:50,172]: Min: 0.93117118
[2025-05-12 11:03:50,174]: Max: 1.00481117
[2025-05-12 11:03:50,174]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 11:03:50,180]: Sample Values (25 elements): [0.06827019155025482, -0.013257978484034538, -0.009200832806527615, 0.010638068430125713, -0.01799567975103855, 0.02603539079427719, 0.0259416364133358, 0.028571151196956635, -0.008516049012541771, -0.06635057181119919, 0.035267774015665054, -0.02507687918841839, -0.050526898354291916, 0.03789759799838066, -0.06932093948125839, 0.0014940776163712144, 0.029085403308272362, -0.03955644369125366, -0.011272942647337914, 0.04202907159924507, -0.03465302288532257, -0.042062535881996155, 0.05264697223901749, 0.04390142112970352, -0.03527793288230896]
[2025-05-12 11:03:50,181]: Mean: -0.00092382
[2025-05-12 11:03:50,184]: Min: -0.13201481
[2025-05-12 11:03:50,185]: Max: 0.11200815
[2025-05-12 11:03:50,185]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 11:03:50,187]: Sample Values (25 elements): [0.9794598817825317, 1.016182541847229, 0.9985769391059875, 0.9843173027038574, 1.0056860446929932, 0.9983715415000916, 1.003869652748108, 1.0096749067306519, 0.9810218811035156, 0.9875949621200562, 0.9893112182617188, 0.9937356114387512, 1.0150421857833862, 1.0395708084106445, 1.0054436922073364, 1.0139195919036865, 0.9818532466888428, 1.0125809907913208, 0.9958404898643494, 1.0020835399627686, 1.0139291286468506, 0.9731380939483643, 0.9831668734550476, 1.0262763500213623, 1.0015275478363037]
[2025-05-12 11:03:50,188]: Mean: 1.00177240
[2025-05-12 11:03:50,190]: Min: 0.97313809
[2025-05-12 11:03:50,192]: Max: 1.03957081
[2025-05-12 11:03:50,193]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 11:03:50,199]: Sample Values (25 elements): [-0.04003991186618805, -0.03633774816989899, 0.060916751623153687, -0.05648085102438927, -0.0502680242061615, 0.039211206138134, -0.004473826847970486, 0.042474955320358276, 0.049831874668598175, 0.05019364878535271, 0.05202401429414749, 0.009266081266105175, -0.03629625216126442, 0.00710572674870491, -0.049050260335206985, 0.034002985805273056, -0.004074095748364925, -0.0030621744226664305, -0.026824243366718292, -0.03890841826796532, -0.028154395520687103, -0.04543791338801384, -0.05930064991116524, 0.023982182145118713, -0.06695455312728882]
[2025-05-12 11:03:50,221]: Mean: -0.00085758
[2025-05-12 11:03:50,223]: Min: -0.10617865
[2025-05-12 11:03:50,226]: Max: 0.10946808
[2025-05-12 11:03:50,226]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,243]: Sample Values (25 elements): [0.9624292850494385, 1.0015511512756348, 0.9820522665977478, 0.9677140712738037, 0.9886677861213684, 0.9680310487747192, 0.9876984357833862, 0.9686988592147827, 0.9826555848121643, 0.9606838822364807, 0.9568247199058533, 0.9642177820205688, 0.960242509841919, 0.9721423983573914, 0.9668658971786499, 0.962526261806488, 0.9689764380455017, 0.9961236119270325, 0.9724368453025818, 0.9738100171089172, 0.9956586956977844, 0.9652827382087708, 0.9425327181816101, 0.9746000170707703, 0.9879104495048523]
[2025-05-12 11:03:50,250]: Mean: 0.97165519
[2025-05-12 11:03:50,251]: Min: 0.94253272
[2025-05-12 11:03:50,258]: Max: 1.00155115
[2025-05-12 11:03:50,258]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:03:50,266]: Sample Values (25 elements): [-0.020436309278011322, 0.011169192381203175, 0.02714184857904911, 0.000605151813942939, 0.0023323064669966698, -0.030093789100646973, -0.014214749448001385, -0.022461561486124992, 0.03083951026201248, -0.0027472295332700014, -0.01335036288946867, -0.0005325623205862939, -0.0008135351818054914, -0.0031565679237246513, -0.01656615547835827, 0.03530232235789299, 0.018723348155617714, -0.00245328969322145, 0.006862576585263014, -0.03297387808561325, 0.042242877185344696, 0.03601594269275665, 0.034599777311086655, 0.02615050971508026, -0.02295791544020176]
[2025-05-12 11:03:50,268]: Mean: -0.00078142
[2025-05-12 11:03:50,270]: Min: -0.09398541
[2025-05-12 11:03:50,271]: Max: 0.09977420
[2025-05-12 11:03:50,271]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,275]: Sample Values (25 elements): [1.006070852279663, 0.9951999187469482, 1.0115318298339844, 1.0246793031692505, 1.0270085334777832, 0.9989467859268188, 1.0087618827819824, 1.0023717880249023, 1.0122488737106323, 0.9904260039329529, 1.0096310377120972, 0.994793176651001, 0.997471034526825, 1.0123692750930786, 0.9933193325996399, 1.0389845371246338, 1.010448932647705, 1.0122908353805542, 0.9894307255744934, 1.0029534101486206, 1.0197639465332031, 1.0045146942138672, 0.9947528839111328, 0.997523307800293, 1.0102708339691162]
[2025-05-12 11:03:50,276]: Mean: 1.00684404
[2025-05-12 11:03:50,278]: Min: 0.98157483
[2025-05-12 11:03:50,281]: Max: 1.04340851
[2025-05-12 11:03:50,281]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 11:03:50,293]: Sample Values (25 elements): [0.14228256046772003, -0.10485401004552841, -0.07484588027000427, 0.10469122231006622, -0.15013302862644196, -0.05843444541096687, 0.08902536332607269, -0.10244195908308029, -0.010152922943234444, 0.1356792151927948, 0.10806687921285629, 0.018147043883800507, -0.04019533097743988, -0.13875459134578705, -0.05267767235636711, -0.16726361215114594, -0.02754713036119938, -0.029367700219154358, 0.023058660328388214, -0.10911675542593002, 0.04240301996469498, -0.18401595950126648, 0.05283910781145096, 0.055144406855106354, -0.010237505659461021]
[2025-05-12 11:03:50,301]: Mean: -0.00266095
[2025-05-12 11:03:50,310]: Min: -0.20776978
[2025-05-12 11:03:50,312]: Max: 0.21915103
[2025-05-12 11:03:50,312]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,313]: Sample Values (25 elements): [0.9173832535743713, 0.9611436724662781, 0.9342292547225952, 0.946590781211853, 0.9260498881340027, 0.9535607695579529, 0.9324732422828674, 0.9420568346977234, 0.9167965054512024, 0.9457023739814758, 0.9317286014556885, 0.9493987560272217, 0.9402898550033569, 0.9451048970222473, 0.944183349609375, 0.95549076795578, 0.9525930285453796, 0.9284798502922058, 0.9612875580787659, 0.9350706338882446, 0.9254215359687805, 0.9295117259025574, 0.9543507695198059, 0.9356652498245239, 0.93893963098526]
[2025-05-12 11:03:50,313]: Mean: 0.94130540
[2025-05-12 11:03:50,315]: Min: 0.91679651
[2025-05-12 11:03:50,320]: Max: 0.96647847
[2025-05-12 11:03:50,320]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:03:50,324]: Sample Values (25 elements): [-0.019294802099466324, -0.006351570598781109, 0.004349510185420513, -0.00022107137192506343, -0.025484897196292877, -0.013507718220353127, -0.012398863211274147, -0.0016015291912481189, -0.015062648802995682, -0.002632472664117813, -0.031836580485105515, 0.038607653230428696, 0.012869895435869694, 0.01924862340092659, -0.03103131242096424, 0.006825004704296589, 0.039080485701560974, -0.04495542123913765, 0.017952218651771545, -0.022785525768995285, -0.0254692230373621, -0.0077167474664747715, -0.029339050874114037, 0.03359028324484825, 0.05038316920399666]
[2025-05-12 11:03:50,325]: Mean: -0.00091544
[2025-05-12 11:03:50,326]: Min: -0.09303575
[2025-05-12 11:03:50,327]: Max: 0.09589800
[2025-05-12 11:03:50,327]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,329]: Sample Values (25 elements): [0.979803204536438, 0.9610269069671631, 0.9514263272285461, 0.9841746091842651, 0.9692386388778687, 0.9823084473609924, 0.966292679309845, 0.9855489730834961, 0.9613856673240662, 0.9799714684486389, 0.9650064706802368, 0.9553565979003906, 0.9704537391662598, 0.985319197177887, 0.9588789343833923, 0.9580414295196533, 0.9741474390029907, 0.9844514727592468, 0.965815007686615, 0.9744471311569214, 0.9762303829193115, 0.9691466689109802, 0.9442437887191772, 0.9583237767219543, 0.9723930358886719]
[2025-05-12 11:03:50,330]: Mean: 0.97108483
[2025-05-12 11:03:50,336]: Min: 0.94424379
[2025-05-12 11:03:50,337]: Max: 1.01316369
[2025-05-12 11:03:50,337]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:03:50,339]: Sample Values (25 elements): [0.015343786217272282, -0.06049186736345291, -0.01913207583129406, 0.008884403854608536, -0.005561722442507744, -0.04166141897439957, 0.015540920197963715, -0.03625025227665901, 0.02272781915962696, -0.016212904825806618, -0.0049395184032619, -0.01232717465609312, -0.025068249553442, 0.025145750492811203, -0.003546685678884387, -0.018779542297124863, -0.005561558995395899, -0.03443222865462303, 0.04562712460756302, 0.010669205337762833, 0.030024688690900803, 0.008257121779024601, -0.012287715449929237, -0.01588873751461506, 0.039356529712677]
[2025-05-12 11:03:50,342]: Mean: 0.00017694
[2025-05-12 11:03:50,349]: Min: -0.07817441
[2025-05-12 11:03:50,351]: Max: 0.07764912
[2025-05-12 11:03:50,352]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,371]: Sample Values (25 elements): [1.0438330173492432, 1.0316601991653442, 1.0280598402023315, 1.042309045791626, 1.0375261306762695, 1.0229692459106445, 1.0452625751495361, 1.0460628271102905, 1.0278215408325195, 1.0222746133804321, 1.031182885169983, 1.0281192064285278, 1.0616225004196167, 1.0549997091293335, 1.0317310094833374, 1.0265767574310303, 1.02204167842865, 1.01577889919281, 1.0596495866775513, 1.0387450456619263, 1.0348727703094482, 1.0069154500961304, 1.025854229927063, 1.0201375484466553, 1.0473783016204834]
[2025-05-12 11:03:50,373]: Mean: 1.03210163
[2025-05-12 11:03:50,378]: Min: 0.99264568
[2025-05-12 11:03:50,379]: Max: 1.09606481
[2025-05-12 11:03:50,379]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:03:50,387]: Sample Values (25 elements): [0.011942158453166485, 0.026250522583723068, 0.03442370519042015, -0.03916251286864281, -0.026246484369039536, -0.017947176471352577, 0.023335417732596397, 0.025515366345643997, 0.01742691360414028, -0.018667995929718018, 0.006711374036967754, -0.03709997236728668, -0.015185086987912655, 0.008222612552344799, -0.007380822207778692, 0.007043089251965284, -0.017598748207092285, -0.00012871887884102762, 0.03083285130560398, 0.008564244024455547, 0.028352735564112663, -0.04151173308491707, 0.029146617278456688, 0.02168179862201214, -0.01658247597515583]
[2025-05-12 11:03:50,392]: Mean: -0.00004096
[2025-05-12 11:03:50,395]: Min: -0.07044271
[2025-05-12 11:03:50,402]: Max: 0.07615206
[2025-05-12 11:03:50,402]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,405]: Sample Values (25 elements): [0.9765008687973022, 0.9777682423591614, 0.9633938670158386, 0.9648516774177551, 0.9879859685897827, 0.9719984531402588, 0.9747776985168457, 0.96881502866745, 0.9702355861663818, 0.9664382934570312, 0.9703865647315979, 0.9813969731330872, 0.9814760684967041, 0.9519979953765869, 0.9576407074928284, 0.9700034856796265, 0.9723357558250427, 0.9475072622299194, 0.973258912563324, 0.9772493243217468, 0.9584487080574036, 0.9748741984367371, 0.9667342305183411, 0.9755675196647644, 0.9635916948318481]
[2025-05-12 11:03:50,406]: Mean: 0.97163218
[2025-05-12 11:03:50,408]: Min: 0.94750726
[2025-05-12 11:03:50,413]: Max: 0.99198067
[2025-05-12 11:03:50,413]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 11:03:50,421]: Sample Values (25 elements): [0.018330417573451996, 0.04253944009542465, -0.033035289496183395, 0.01196304988116026, 0.02719856984913349, -0.014258049428462982, 0.029810471460223198, 0.02294067293405533, -0.023954136297106743, -0.03078179992735386, 0.007876746356487274, 0.015719899907708168, 0.015466608107089996, -0.012682704254984856, 0.006974837277084589, 0.02475850097835064, 0.007613477762788534, -0.03683030232787132, -0.037111129611730576, -0.04222354292869568, 0.0015833782963454723, -0.01979207620024681, 0.049967631697654724, 0.007430777419358492, 0.0601092129945755]
[2025-05-12 11:03:50,435]: Mean: 0.00041713
[2025-05-12 11:03:50,444]: Min: -0.06815454
[2025-05-12 11:03:50,445]: Max: 0.06680452
[2025-05-12 11:03:50,445]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 11:03:50,446]: Sample Values (25 elements): [1.0609170198440552, 1.0789350271224976, 1.0199217796325684, 1.0837095975875854, 1.101609706878662, 1.0819982290267944, 1.0496453046798706, 1.0762680768966675, 1.0463714599609375, 1.0551788806915283, 1.0700581073760986, 1.0438098907470703, 1.0549664497375488, 1.0928330421447754, 1.0414310693740845, 1.0785309076309204, 1.0693854093551636, 1.057364821434021, 1.0620263814926147, 1.0474334955215454, 1.0288788080215454, 1.058453917503357, 1.0623329877853394, 1.0762872695922852, 1.1070555448532104]
[2025-05-12 11:03:50,447]: Mean: 1.06573272
[2025-05-12 11:03:50,450]: Min: 1.01992178
[2025-05-12 11:03:50,454]: Max: 1.11095655
[2025-05-12 11:03:50,455]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 11:03:50,458]: Sample Values (25 elements): [0.3271053731441498, -0.2273828685283661, 0.3428242802619934, -0.1544448286294937, 0.17823253571987152, -0.03349708020687103, -0.1666431576013565, -0.013338825665414333, -0.04759060963988304, 0.0860123261809349, 0.32674428820610046, 0.006826287135481834, -0.11399649828672409, 0.0713239535689354, -0.006829565856605768, -0.24352793395519257, -0.2626648545265198, -0.1998087763786316, 0.01999659277498722, -0.13108469545841217, -0.14767752587795258, 0.23004572093486786, -0.29181385040283203, 0.03400988504290581, 0.034320782870054245]
[2025-05-12 11:03:50,460]: Mean: -0.00654610
[2025-05-12 11:03:50,461]: Min: -0.34146941
[2025-05-12 11:03:50,462]: Max: 0.56097835
[2025-05-12 11:03:50,462]: 


QAT of ResNet20 with relu down to 4 bits...
[2025-05-12 11:03:52,137]: [ResNet20_relu_quantized_4_bits] after configure_qat:
[2025-05-12 11:03:52,855]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 11:07:37,110]: [ResNet20_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.4674 Train Acc: 0.8356 Eval Loss: 0.6114 Eval Acc: 0.8056 (LR: 0.001000)
[2025-05-12 11:09:50,378]: [ResNet20_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.4606 Train Acc: 0.8392 Eval Loss: 0.5643 Eval Acc: 0.8182 (LR: 0.001000)
[2025-05-12 11:12:12,183]: [ResNet20_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.4502 Train Acc: 0.8432 Eval Loss: 0.5389 Eval Acc: 0.8217 (LR: 0.001000)
[2025-05-12 11:14:28,810]: [ResNet20_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.4427 Train Acc: 0.8431 Eval Loss: 0.6088 Eval Acc: 0.8074 (LR: 0.001000)
[2025-05-12 11:16:43,448]: [ResNet20_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.4339 Train Acc: 0.8458 Eval Loss: 0.5303 Eval Acc: 0.8257 (LR: 0.001000)
[2025-05-12 11:19:04,585]: [ResNet20_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.4392 Train Acc: 0.8451 Eval Loss: 0.5327 Eval Acc: 0.8240 (LR: 0.001000)
[2025-05-12 11:21:17,383]: [ResNet20_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.4323 Train Acc: 0.8494 Eval Loss: 0.5259 Eval Acc: 0.8278 (LR: 0.001000)
[2025-05-12 11:23:32,212]: [ResNet20_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.4274 Train Acc: 0.8504 Eval Loss: 0.5298 Eval Acc: 0.8317 (LR: 0.001000)
[2025-05-12 11:25:49,365]: [ResNet20_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.4198 Train Acc: 0.8526 Eval Loss: 0.5351 Eval Acc: 0.8216 (LR: 0.001000)
[2025-05-12 11:27:59,074]: [ResNet20_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.4251 Train Acc: 0.8496 Eval Loss: 0.5669 Eval Acc: 0.8153 (LR: 0.001000)
[2025-05-12 11:30:19,522]: [ResNet20_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.4236 Train Acc: 0.8510 Eval Loss: 0.5225 Eval Acc: 0.8278 (LR: 0.001000)
[2025-05-12 11:32:28,876]: [ResNet20_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.4171 Train Acc: 0.8533 Eval Loss: 0.5402 Eval Acc: 0.8232 (LR: 0.001000)
[2025-05-12 11:34:47,953]: [ResNet20_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.4162 Train Acc: 0.8540 Eval Loss: 0.4825 Eval Acc: 0.8389 (LR: 0.001000)
[2025-05-12 11:37:04,329]: [ResNet20_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.4090 Train Acc: 0.8557 Eval Loss: 0.4781 Eval Acc: 0.8402 (LR: 0.001000)
[2025-05-12 11:39:17,969]: [ResNet20_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.4125 Train Acc: 0.8539 Eval Loss: 0.5547 Eval Acc: 0.8242 (LR: 0.001000)
[2025-05-12 11:41:38,581]: [ResNet20_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.4050 Train Acc: 0.8574 Eval Loss: 0.4809 Eval Acc: 0.8402 (LR: 0.001000)
[2025-05-12 11:43:21,808]: [ResNet20_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.4065 Train Acc: 0.8558 Eval Loss: 0.5270 Eval Acc: 0.8258 (LR: 0.001000)
[2025-05-12 11:45:00,898]: [ResNet20_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.4028 Train Acc: 0.8588 Eval Loss: 0.5488 Eval Acc: 0.8211 (LR: 0.001000)
[2025-05-12 11:46:38,903]: [ResNet20_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.4059 Train Acc: 0.8589 Eval Loss: 0.5309 Eval Acc: 0.8234 (LR: 0.001000)
[2025-05-12 11:48:22,093]: [ResNet20_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.3985 Train Acc: 0.8621 Eval Loss: 0.4982 Eval Acc: 0.8352 (LR: 0.001000)
[2025-05-12 11:50:05,759]: [ResNet20_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.3980 Train Acc: 0.8601 Eval Loss: 0.5322 Eval Acc: 0.8291 (LR: 0.001000)
[2025-05-12 11:51:52,334]: [ResNet20_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.3920 Train Acc: 0.8619 Eval Loss: 0.5076 Eval Acc: 0.8337 (LR: 0.001000)
[2025-05-12 11:53:41,667]: [ResNet20_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.3927 Train Acc: 0.8620 Eval Loss: 0.5110 Eval Acc: 0.8327 (LR: 0.001000)
[2025-05-12 11:55:31,003]: [ResNet20_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.3906 Train Acc: 0.8625 Eval Loss: 0.5038 Eval Acc: 0.8365 (LR: 0.001000)
[2025-05-12 11:57:20,406]: [ResNet20_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.3872 Train Acc: 0.8645 Eval Loss: 0.5492 Eval Acc: 0.8256 (LR: 0.001000)
[2025-05-12 11:59:08,516]: [ResNet20_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.3857 Train Acc: 0.8644 Eval Loss: 0.4895 Eval Acc: 0.8400 (LR: 0.001000)
[2025-05-12 12:00:52,341]: [ResNet20_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.3864 Train Acc: 0.8628 Eval Loss: 0.4962 Eval Acc: 0.8370 (LR: 0.001000)
[2025-05-12 12:02:36,447]: [ResNet20_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.3857 Train Acc: 0.8625 Eval Loss: 0.5058 Eval Acc: 0.8346 (LR: 0.001000)
[2025-05-12 12:04:14,501]: [ResNet20_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.3817 Train Acc: 0.8652 Eval Loss: 0.5337 Eval Acc: 0.8208 (LR: 0.001000)
[2025-05-12 12:05:55,732]: [ResNet20_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.3748 Train Acc: 0.8668 Eval Loss: 0.4972 Eval Acc: 0.8369 (LR: 0.000250)
[2025-05-12 12:07:39,930]: [ResNet20_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.3473 Train Acc: 0.8791 Eval Loss: 0.4245 Eval Acc: 0.8610 (LR: 0.000250)
[2025-05-12 12:09:30,050]: [ResNet20_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.3384 Train Acc: 0.8807 Eval Loss: 0.4272 Eval Acc: 0.8565 (LR: 0.000250)
[2025-05-12 12:11:20,770]: [ResNet20_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.3367 Train Acc: 0.8826 Eval Loss: 0.4413 Eval Acc: 0.8520 (LR: 0.000250)
[2025-05-12 12:13:10,929]: [ResNet20_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.3345 Train Acc: 0.8821 Eval Loss: 0.4236 Eval Acc: 0.8575 (LR: 0.000250)
[2025-05-12 12:15:01,055]: [ResNet20_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.3351 Train Acc: 0.8834 Eval Loss: 0.4237 Eval Acc: 0.8613 (LR: 0.000250)
[2025-05-12 12:16:48,581]: [ResNet20_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.3323 Train Acc: 0.8832 Eval Loss: 0.4320 Eval Acc: 0.8591 (LR: 0.000250)
[2025-05-12 12:18:35,141]: [ResNet20_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.3276 Train Acc: 0.8844 Eval Loss: 0.4224 Eval Acc: 0.8642 (LR: 0.000250)
[2025-05-12 12:20:19,946]: [ResNet20_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.3313 Train Acc: 0.8834 Eval Loss: 0.4350 Eval Acc: 0.8589 (LR: 0.000250)
[2025-05-12 12:22:06,275]: [ResNet20_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.3281 Train Acc: 0.8845 Eval Loss: 0.4246 Eval Acc: 0.8608 (LR: 0.000250)
[2025-05-12 12:23:53,381]: [ResNet20_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.3293 Train Acc: 0.8855 Eval Loss: 0.4268 Eval Acc: 0.8607 (LR: 0.000250)
[2025-05-12 12:25:41,011]: [ResNet20_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.3279 Train Acc: 0.8851 Eval Loss: 0.4139 Eval Acc: 0.8616 (LR: 0.000250)
[2025-05-12 12:27:28,868]: [ResNet20_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.3296 Train Acc: 0.8843 Eval Loss: 0.4596 Eval Acc: 0.8510 (LR: 0.000250)
[2025-05-12 12:29:17,084]: [ResNet20_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.3273 Train Acc: 0.8849 Eval Loss: 0.4439 Eval Acc: 0.8523 (LR: 0.000250)
[2025-05-12 12:31:07,761]: [ResNet20_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.3288 Train Acc: 0.8837 Eval Loss: 0.4370 Eval Acc: 0.8567 (LR: 0.000250)
[2025-05-12 12:32:59,925]: [ResNet20_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.3230 Train Acc: 0.8858 Eval Loss: 0.4367 Eval Acc: 0.8589 (LR: 0.000063)
[2025-05-12 12:34:52,691]: [ResNet20_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.3153 Train Acc: 0.8888 Eval Loss: 0.4174 Eval Acc: 0.8605 (LR: 0.000063)
[2025-05-12 12:36:47,272]: [ResNet20_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.3098 Train Acc: 0.8909 Eval Loss: 0.4173 Eval Acc: 0.8640 (LR: 0.000063)
[2025-05-12 12:38:42,157]: [ResNet20_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.3081 Train Acc: 0.8920 Eval Loss: 0.4134 Eval Acc: 0.8649 (LR: 0.000063)
[2025-05-12 12:40:36,996]: [ResNet20_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.3106 Train Acc: 0.8901 Eval Loss: 0.4196 Eval Acc: 0.8624 (LR: 0.000063)
[2025-05-12 12:42:32,264]: [ResNet20_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.3105 Train Acc: 0.8921 Eval Loss: 0.4132 Eval Acc: 0.8632 (LR: 0.000063)
[2025-05-12 12:44:27,415]: [ResNet20_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.3058 Train Acc: 0.8947 Eval Loss: 0.4047 Eval Acc: 0.8658 (LR: 0.000063)
[2025-05-12 12:46:23,016]: [ResNet20_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.3074 Train Acc: 0.8911 Eval Loss: 0.4165 Eval Acc: 0.8620 (LR: 0.000063)
[2025-05-12 12:48:19,405]: [ResNet20_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.3075 Train Acc: 0.8916 Eval Loss: 0.4182 Eval Acc: 0.8618 (LR: 0.000063)
[2025-05-12 12:50:16,115]: [ResNet20_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.3119 Train Acc: 0.8895 Eval Loss: 0.4097 Eval Acc: 0.8642 (LR: 0.000063)
[2025-05-12 12:52:12,933]: [ResNet20_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.3095 Train Acc: 0.8913 Eval Loss: 0.4156 Eval Acc: 0.8619 (LR: 0.000063)
[2025-05-12 12:54:09,032]: [ResNet20_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.3071 Train Acc: 0.8931 Eval Loss: 0.4184 Eval Acc: 0.8632 (LR: 0.000063)
[2025-05-12 12:56:03,776]: [ResNet20_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.3059 Train Acc: 0.8934 Eval Loss: 0.4124 Eval Acc: 0.8617 (LR: 0.000063)
[2025-05-12 12:57:58,092]: [ResNet20_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.3060 Train Acc: 0.8915 Eval Loss: 0.4217 Eval Acc: 0.8567 (LR: 0.000063)
[2025-05-12 12:59:51,334]: [ResNet20_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.3045 Train Acc: 0.8918 Eval Loss: 0.4126 Eval Acc: 0.8653 (LR: 0.000063)
[2025-05-12 13:01:43,609]: [ResNet20_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.3058 Train Acc: 0.8916 Eval Loss: 0.4152 Eval Acc: 0.8631 (LR: 0.000063)
[2025-05-12 13:01:43,610]: [ResNet20_relu_quantized_4_bits] Best Eval Accuracy: 0.8658
[2025-05-12 13:01:43,696]: 


Quantization of model down to 4 bits finished
[2025-05-12 13:01:43,696]: Model Architecture:
[2025-05-12 13:01:43,885]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8862], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.292924880981445)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3298064172267914, max_val=0.24782682955265045)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5729], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.592756271362305)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0291], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21662470698356628, max_val=0.2199368178844452)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.49994468688965)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0281], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21089543402194977, max_val=0.21088983118534088)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5833], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.75003433227539)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0315], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2031475007534027, max_val=0.26988452672958374)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2392], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=18.588075637817383)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0393], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33517682552337646, max_val=0.2550686001777649)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4745], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.117580890655518)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14831459522247314, max_val=0.18802092969417572)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3497], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=20.24534034729004)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0230], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18694885075092316, max_val=0.15842416882514954)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4240], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.36058235168457)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0221], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16604748368263245, max_val=0.16596971452236176)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33129218220710754, max_val=0.34327515959739685)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6668], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.00259017944336)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0191], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1361239105463028, max_val=0.15034690499305725)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4018], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.026902675628662)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0189], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12783923745155334, max_val=0.15505017340183258)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8969], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.453340530395508)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0199], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13207504153251648, max_val=0.16622646152973175)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4077], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.116142749786377)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0178], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14064036309719086, max_val=0.12670430541038513)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0082], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.122383117675781)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0162], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12019088119268417, max_val=0.12348412722349167)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.169620990753174)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0150], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1121850535273552, max_val=0.1122068539261818)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0301], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22546100616455078, max_val=0.22647559642791748)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6415], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.62228775024414)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0145], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10980383306741714, max_val=0.10740020871162415)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4297], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.4458770751953125)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09072808921337128, max_val=0.09110545367002487)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8418], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.627283096313477)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0110], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07640675455331802, max_val=0.08890794962644577)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4373], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.559393405914307)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0097], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07323462516069412, max_val=0.07214252650737762)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1717], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.575241088867188)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:01:43,885]: 
Model Weights:
[2025-05-12 13:01:43,885]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 13:01:43,886]: Sample Values (25 elements): [0.1779603660106659, 0.3137345314025879, 0.08110567182302475, -0.13108794391155243, -0.04274345189332962, 0.31308847665786743, 0.07640057057142258, -0.02480553276836872, -0.17628172039985657, -0.06864836066961288, -0.18412774801254272, 0.261562705039978, -0.45816463232040405, -0.009363771416246891, 0.14583706855773926, 0.06644227355718613, -0.23993194103240967, 0.15463869273662567, 0.12628915905952454, -0.2778134346008301, -0.12483040988445282, 0.049682989716529846, -0.011015851981937885, -0.33774974942207336, -0.11157853901386261]
[2025-05-12 13:01:43,886]: Mean: -0.00078227
[2025-05-12 13:01:43,887]: Min: -0.46308875
[2025-05-12 13:01:43,887]: Max: 0.59433949
[2025-05-12 13:01:43,887]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,887]: Sample Values (16 elements): [1.0652340650558472, 1.283524751663208, 1.2175592184066772, 1.0257062911987305, 1.2204856872558594, 1.2142695188522339, 1.1244746446609497, 1.2987492084503174, 0.9213247299194336, 1.247900128364563, 1.3945542573928833, 1.1511517763137817, 1.133547306060791, 1.1344724893569946, 0.9498534202575684, 0.903104305267334]
[2025-05-12 13:01:43,888]: Mean: 1.14286947
[2025-05-12 13:01:43,888]: Min: 0.90310431
[2025-05-12 13:01:43,888]: Max: 1.39455426
[2025-05-12 13:01:43,895]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:01:43,899]: Sample Values (25 elements): [-0.03850884363055229, 0.0, -0.03850884363055229, -0.07701768726110458, -0.11552652716636658, 0.03850884363055229, -0.03850884363055229, 0.0, 0.07701768726110458, 0.0, 0.0, 0.07701768726110458, 0.03850884363055229, 0.03850884363055229, 0.0, 0.07701768726110458, 0.07701768726110458, 0.0, -0.07701768726110458, 0.07701768726110458, -0.07701768726110458, -0.07701768726110458, 0.0, -0.07701768726110458, -0.15403537452220917]
[2025-05-12 13:01:43,901]: Mean: -0.00870795
[2025-05-12 13:01:43,904]: Min: -0.34657958
[2025-05-12 13:01:43,906]: Max: 0.23105305
[2025-05-12 13:01:43,906]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,912]: Sample Values (16 elements): [1.1889888048171997, 0.9558530449867249, 0.9533165097236633, 0.9146770238876343, 1.1075865030288696, 1.0447802543640137, 1.1198111772537231, 1.0126997232437134, 0.9843062162399292, 0.9796006679534912, 1.0057281255722046, 1.0336402654647827, 1.0550724267959595, 0.9635355472564697, 1.0069962739944458, 0.9664729237556458]
[2025-05-12 13:01:43,920]: Mean: 1.01831663
[2025-05-12 13:01:43,925]: Min: 0.91467702
[2025-05-12 13:01:43,932]: Max: 1.18898880
[2025-05-12 13:01:43,935]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:01:43,937]: Sample Values (25 elements): [-0.08731237053871155, 0.08731237053871155, 0.0, 0.08731237053871155, 0.029104124754667282, 0.029104124754667282, 0.1455206274986267, -0.029104124754667282, -0.029104124754667282, -0.08731237053871155, 0.08731237053871155, 0.058208249509334564, 0.08731237053871155, -0.029104124754667282, 0.058208249509334564, 0.029104124754667282, -0.08731237053871155, 0.029104124754667282, 0.029104124754667282, -0.08731237053871155, -0.058208249509334564, 0.058208249509334564, -0.029104124754667282, -0.058208249509334564, -0.058208249509334564]
[2025-05-12 13:01:43,937]: Mean: -0.00447173
[2025-05-12 13:01:43,937]: Min: -0.20372887
[2025-05-12 13:01:43,938]: Max: 0.23283300
[2025-05-12 13:01:43,938]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,938]: Sample Values (16 elements): [1.0120280981063843, 1.0997077226638794, 0.9937470555305481, 1.1404709815979004, 1.059185266494751, 1.037708044052124, 1.4284923076629639, 0.9232892990112305, 1.1010574102401733, 1.028011679649353, 1.1037253141403198, 1.1479651927947998, 1.013526439666748, 1.0831295251846313, 1.1678009033203125, 1.0609118938446045]
[2025-05-12 13:01:43,939]: Mean: 1.08754730
[2025-05-12 13:01:43,939]: Min: 0.92328930
[2025-05-12 13:01:43,939]: Max: 1.42849231
[2025-05-12 13:01:43,942]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:01:43,943]: Sample Values (25 elements): [-0.028119036927819252, 0.0843571126461029, 0.11247614771127701, 0.0, -0.028119036927819252, -0.0843571126461029, -0.0843571126461029, 0.056238073855638504, 0.028119036927819252, 0.0843571126461029, 0.028119036927819252, -0.028119036927819252, -0.056238073855638504, -0.056238073855638504, -0.0843571126461029, 0.056238073855638504, 0.0, -0.028119036927819252, 0.0, 0.0, 0.056238073855638504, 0.028119036927819252, -0.028119036927819252, -0.028119036927819252, 0.0]
[2025-05-12 13:01:43,943]: Mean: -0.00186728
[2025-05-12 13:01:43,943]: Min: -0.22495230
[2025-05-12 13:01:43,944]: Max: 0.19683325
[2025-05-12 13:01:43,944]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,944]: Sample Values (16 elements): [0.9378708004951477, 0.9505913853645325, 0.9449594020843506, 0.9194071292877197, 0.9471077919006348, 0.9857826232910156, 1.020627498626709, 1.0050395727157593, 1.0182379484176636, 0.9585202932357788, 1.1470715999603271, 0.969411313533783, 1.018395185470581, 1.0989857912063599, 1.0226387977600098, 0.975904107093811]
[2025-05-12 13:01:43,944]: Mean: 0.99503446
[2025-05-12 13:01:43,945]: Min: 0.91940713
[2025-05-12 13:01:43,945]: Max: 1.14707160
[2025-05-12 13:01:43,948]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:01:43,949]: Sample Values (25 elements): [-0.06307102739810944, 0.06307102739810944, 0.12614205479621887, 0.0, 0.03153551369905472, 0.0, 0.03153551369905472, 0.0, 0.06307102739810944, -0.06307102739810944, -0.09460654109716415, -0.09460654109716415, -0.03153551369905472, -0.03153551369905472, -0.09460654109716415, 0.06307102739810944, 0.0, 0.06307102739810944, 0.06307102739810944, 0.03153551369905472, 0.09460654109716415, 0.0, 0.06307102739810944, 0.03153551369905472, 0.03153551369905472]
[2025-05-12 13:01:43,949]: Mean: -0.00054749
[2025-05-12 13:01:43,950]: Min: -0.18921308
[2025-05-12 13:01:43,950]: Max: 0.28381962
[2025-05-12 13:01:43,950]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,951]: Sample Values (16 elements): [1.0663361549377441, 0.9803583025932312, 0.9162723422050476, 1.0138002634048462, 1.0483862161636353, 1.013647437095642, 0.9602365493774414, 0.9551472663879395, 0.936753511428833, 1.0667247772216797, 1.0012437105178833, 0.9341815114021301, 0.9325891733169556, 0.9513058066368103, 0.9749252796173096, 1.0299181938171387]
[2025-05-12 13:01:43,951]: Mean: 0.98636413
[2025-05-12 13:01:43,951]: Min: 0.91627234
[2025-05-12 13:01:43,952]: Max: 1.06672478
[2025-05-12 13:01:43,954]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:01:43,955]: Sample Values (25 elements): [-0.11804908514022827, -0.07869938760995865, -0.039349693804979324, 0.07869938760995865, -0.039349693804979324, 0.1573987752199173, 0.039349693804979324, 0.11804908514022827, 0.039349693804979324, 0.07869938760995865, 0.0, -0.039349693804979324, 0.039349693804979324, 0.039349693804979324, 0.0, 0.039349693804979324, 0.039349693804979324, 0.0, 0.039349693804979324, -0.11804908514022827, 0.0, 0.0, -0.039349693804979324, 0.0, -0.07869938760995865]
[2025-05-12 13:01:43,955]: Mean: -0.00375735
[2025-05-12 13:01:43,955]: Min: -0.35414726
[2025-05-12 13:01:43,956]: Max: 0.23609817
[2025-05-12 13:01:43,956]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,957]: Sample Values (16 elements): [0.9987978935241699, 0.9308573007583618, 0.9801590442657471, 0.9951680898666382, 0.8886505365371704, 1.0367298126220703, 0.9586447477340698, 0.9937445521354675, 0.9287727475166321, 0.9797009229660034, 0.9476305842399597, 0.978042483329773, 0.9625102281570435, 1.0656943321228027, 1.1529768705368042, 0.9223945736885071]
[2025-05-12 13:01:43,957]: Mean: 0.98252970
[2025-05-12 13:01:43,958]: Min: 0.88865054
[2025-05-12 13:01:43,958]: Max: 1.15297687
[2025-05-12 13:01:43,960]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 13:01:43,962]: Sample Values (25 elements): [0.044844746589660645, 0.044844746589660645, 0.044844746589660645, 0.0, 0.08968949317932129, 0.0, 0.06726711988449097, -0.022422373294830322, -0.08968949317932129, 0.044844746589660645, 0.022422373294830322, -0.08968949317932129, -0.08968949317932129, -0.08968949317932129, -0.11211186647415161, 0.044844746589660645, 0.022422373294830322, 0.022422373294830322, -0.044844746589660645, 0.11211186647415161, 0.022422373294830322, 0.044844746589660645, -0.06726711988449097, -0.022422373294830322, 0.022422373294830322]
[2025-05-12 13:01:43,962]: Mean: 0.00259843
[2025-05-12 13:01:43,962]: Min: -0.15695661
[2025-05-12 13:01:43,963]: Max: 0.17937899
[2025-05-12 13:01:43,963]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 13:01:43,963]: Sample Values (16 elements): [1.0633502006530762, 1.1189606189727783, 0.9206472635269165, 1.023868203163147, 1.035159945487976, 1.0715497732162476, 1.0408116579055786, 1.1167298555374146, 1.1366519927978516, 0.9312971830368042, 1.035710096359253, 1.0521275997161865, 0.9746648073196411, 0.9981884956359863, 1.03490149974823, 0.95712810754776]
[2025-05-12 13:01:43,964]: Mean: 1.03198421
[2025-05-12 13:01:43,964]: Min: 0.92064726
[2025-05-12 13:01:43,964]: Max: 1.13665199
[2025-05-12 13:01:43,967]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 13:01:43,967]: Sample Values (25 elements): [0.0, -0.02302488684654236, 0.0, 0.02302488684654236, -0.06907466053962708, -0.09209954738616943, 0.06907466053962708, -0.09209954738616943, -0.04604977369308472, -0.06907466053962708, -0.04604977369308472, -0.02302488684654236, -0.06907466053962708, -0.13814932107925415, 0.04604977369308472, -0.04604977369308472, -0.04604977369308472, -0.02302488684654236, -0.09209954738616943, -0.11512443423271179, -0.09209954738616943, 0.09209954738616943, 0.04604977369308472, -0.04604977369308472, -0.06907466053962708]
[2025-05-12 13:01:43,968]: Mean: -0.00431217
[2025-05-12 13:01:43,968]: Min: -0.18419909
[2025-05-12 13:01:43,968]: Max: 0.16117421
[2025-05-12 13:01:43,968]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:01:43,969]: Sample Values (25 elements): [0.9447677731513977, 1.038572907447815, 0.9162998795509338, 0.9938852787017822, 0.9533606767654419, 0.9587603211402893, 0.9409836530685425, 0.9854326248168945, 1.0417449474334717, 1.0138055086135864, 0.9978773593902588, 0.943636417388916, 1.0346518754959106, 0.9267851710319519, 0.9049603939056396, 0.9269770383834839, 0.9665802121162415, 1.0040448904037476, 0.9977636337280273, 1.0071696043014526, 0.985196590423584, 0.950858473777771, 0.9766501784324646, 0.9686633348464966, 0.9778785705566406]
[2025-05-12 13:01:43,969]: Mean: 0.97614455
[2025-05-12 13:01:43,969]: Min: 0.90496039
[2025-05-12 13:01:43,970]: Max: 1.04174495
[2025-05-12 13:01:43,973]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:01:43,973]: Sample Values (25 elements): [0.022134507074952126, 0.0, -0.022134507074952126, 0.022134507074952126, -0.06640352308750153, 0.022134507074952126, -0.06640352308750153, -0.06640352308750153, -0.04426901414990425, -0.04426901414990425, 0.022134507074952126, -0.06640352308750153, 0.022134507074952126, -0.04426901414990425, 0.0, 0.022134507074952126, -0.06640352308750153, 0.0, -0.04426901414990425, 0.022134507074952126, 0.022134507074952126, -0.04426901414990425, -0.022134507074952126, 0.0885380282998085, -0.022134507074952126]
[2025-05-12 13:01:43,974]: Mean: -0.00202467
[2025-05-12 13:01:43,974]: Min: -0.17707606
[2025-05-12 13:01:43,974]: Max: 0.15494154
[2025-05-12 13:01:43,974]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:01:43,975]: Sample Values (25 elements): [1.0252217054367065, 0.9863607883453369, 1.0130248069763184, 1.0055885314941406, 1.0178325176239014, 1.0770752429962158, 1.038474202156067, 1.056368112564087, 1.0496182441711426, 1.0578346252441406, 1.0291552543640137, 1.038296103477478, 1.0456255674362183, 1.0395817756652832, 1.0512405633926392, 1.0262231826782227, 1.027907371520996, 1.0099138021469116, 1.0360963344573975, 1.000231385231018, 0.9985203146934509, 1.0992939472198486, 0.988772451877594, 1.0226824283599854, 1.0009697675704956]
[2025-05-12 13:01:43,976]: Mean: 1.02607274
[2025-05-12 13:01:43,976]: Min: 0.97023034
[2025-05-12 13:01:43,976]: Max: 1.09929395
[2025-05-12 13:01:43,978]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 13:01:43,979]: Sample Values (25 elements): [0.044971123337745667, 0.134913370013237, 0.044971123337745667, -0.22485561668872833, 0.22485561668872833, -0.044971123337745667, 0.08994224667549133, 0.134913370013237, 0.08994224667549133, -0.22485561668872833, -0.134913370013237, 0.0, 0.22485561668872833, 0.044971123337745667, -0.044971123337745667, -0.08994224667549133, 0.134913370013237, 0.08994224667549133, -0.08994224667549133, -0.269826740026474, -0.17988449335098267, 0.0, -0.044971123337745667, 0.17988449335098267, 0.0]
[2025-05-12 13:01:43,979]: Mean: -0.00184452
[2025-05-12 13:01:43,980]: Min: -0.31479788
[2025-05-12 13:01:43,980]: Max: 0.35976899
[2025-05-12 13:01:43,980]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 13:01:43,980]: Sample Values (25 elements): [0.844292938709259, 0.8855969905853271, 0.9116925597190857, 0.8130070567131042, 0.8195216059684753, 0.848078727722168, 0.8762620091438293, 0.845397412776947, 0.7538495659828186, 0.8760657906532288, 0.8906389474868774, 0.816533088684082, 0.8513832092285156, 0.8535832166671753, 0.8956993222236633, 0.8661989569664001, 0.8563969135284424, 0.8383719325065613, 0.875053882598877, 0.8537793159484863, 0.9333953261375427, 0.8240659236907959, 0.8469221591949463, 0.8304890990257263, 0.8870760202407837]
[2025-05-12 13:01:43,981]: Mean: 0.86167443
[2025-05-12 13:01:43,981]: Min: 0.75384957
[2025-05-12 13:01:43,982]: Max: 0.95039868
[2025-05-12 13:01:43,984]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:01:43,984]: Sample Values (25 elements): [-0.05729416012763977, 0.0, -0.07639221101999283, 0.0, -0.019098052754998207, 0.05729416012763977, -0.019098052754998207, -0.05729416012763977, 0.038196105509996414, 0.0, 0.0, 0.0, 0.019098052754998207, 0.019098052754998207, -0.019098052754998207, 0.019098052754998207, -0.038196105509996414, 0.038196105509996414, 0.0, 0.038196105509996414, -0.038196105509996414, 0.038196105509996414, 0.0, 0.07639221101999283, 0.038196105509996414]
[2025-05-12 13:01:43,984]: Mean: -0.00201010
[2025-05-12 13:01:43,985]: Min: -0.13368636
[2025-05-12 13:01:43,985]: Max: 0.15278442
[2025-05-12 13:01:43,985]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:01:43,986]: Sample Values (25 elements): [0.9399510025978088, 0.9439247846603394, 0.9527815580368042, 1.011489987373352, 0.944364607334137, 0.9977070689201355, 1.0122783184051514, 0.9735769629478455, 0.9849244952201843, 1.0175303220748901, 1.0055468082427979, 0.9368342757225037, 0.9577603936195374, 0.9163990616798401, 0.9664997458457947, 0.9517131447792053, 0.9493035078048706, 0.9517571926116943, 0.9845003485679626, 1.0074431896209717, 0.9599614143371582, 0.9945867657661438, 0.9074073433876038, 0.9723713994026184, 0.9962616562843323]
[2025-05-12 13:01:43,986]: Mean: 0.96994209
[2025-05-12 13:01:43,987]: Min: 0.90740734
[2025-05-12 13:01:43,987]: Max: 1.01753032
[2025-05-12 13:01:43,988]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:01:43,989]: Sample Values (25 elements): [0.0, -0.03771854564547539, 0.09429636597633362, 0.0, -0.05657781660556793, -0.07543709129095078, 0.05657781660556793, 0.018859272822737694, 0.018859272822737694, -0.018859272822737694, 0.018859272822737694, -0.03771854564547539, 0.03771854564547539, 0.018859272822737694, -0.018859272822737694, 0.018859272822737694, 0.0, -0.07543709129095078, -0.03771854564547539, -0.018859272822737694, 0.018859272822737694, -0.03771854564547539, -0.03771854564547539, 0.05657781660556793, -0.07543709129095078]
[2025-05-12 13:01:43,989]: Mean: -0.00027217
[2025-05-12 13:01:43,990]: Min: -0.13201492
[2025-05-12 13:01:43,990]: Max: 0.15087418
[2025-05-12 13:01:43,990]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:01:43,990]: Sample Values (25 elements): [1.0263949632644653, 0.966792106628418, 1.0577239990234375, 0.9603248834609985, 0.9936883449554443, 1.0187655687332153, 1.0141555070877075, 1.0414646863937378, 0.9813715815544128, 1.072595477104187, 0.9718995690345764, 1.0416566133499146, 1.0204803943634033, 0.9613975286483765, 1.0156604051589966, 1.0166360139846802, 1.0326796770095825, 1.00627601146698, 1.0259770154953003, 0.9985580444335938, 1.031692624092102, 0.9953521490097046, 1.0418375730514526, 0.985938310623169, 1.0152690410614014]
[2025-05-12 13:01:43,990]: Mean: 1.00823164
[2025-05-12 13:01:43,991]: Min: 0.95470923
[2025-05-12 13:01:43,991]: Max: 1.07259548
[2025-05-12 13:01:43,993]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:01:43,994]: Sample Values (25 elements): [-0.01988682895898819, -0.03977365791797638, 0.0, -0.03977365791797638, 0.07954731583595276, 0.05966048687696457, 0.01988682895898819, -0.05966048687696457, 0.05966048687696457, 0.05966048687696457, -0.03977365791797638, 0.0, 0.03977365791797638, -0.03977365791797638, 0.03977365791797638, 0.03977365791797638, -0.01988682895898819, 0.01988682895898819, -0.01988682895898819, -0.01988682895898819, -0.01988682895898819, 0.01988682895898819, 0.01988682895898819, -0.05966048687696457, -0.07954731583595276]
[2025-05-12 13:01:43,994]: Mean: -0.00122782
[2025-05-12 13:01:43,994]: Min: -0.13920781
[2025-05-12 13:01:43,994]: Max: 0.15909463
[2025-05-12 13:01:43,994]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 13:01:43,995]: Sample Values (25 elements): [0.954045832157135, 0.9445139169692993, 0.9529708027839661, 0.91828852891922, 0.9770355820655823, 0.980379045009613, 0.9513975381851196, 0.9921888709068298, 0.9803094863891602, 0.9829954504966736, 0.9212296009063721, 0.9672026634216309, 0.9749102592468262, 1.0148440599441528, 0.9741469025611877, 0.956017255783081, 0.9605254530906677, 0.9496749639511108, 0.9715607166290283, 0.9511619210243225, 0.9562109112739563, 0.9456781148910522, 0.9881691932678223, 0.9711503386497498, 0.9860961437225342]
[2025-05-12 13:01:43,995]: Mean: 0.96646476
[2025-05-12 13:01:43,996]: Min: 0.91828853
[2025-05-12 13:01:43,996]: Max: 1.01484406
[2025-05-12 13:01:43,998]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 13:01:43,998]: Sample Values (25 elements): [-0.07129168510437012, -0.01782292127609253, 0.0, 0.01782292127609253, 0.0, 0.03564584255218506, 0.0, -0.07129168510437012, -0.01782292127609253, 0.03564584255218506, 0.0, 0.01782292127609253, -0.01782292127609253, 0.01782292127609253, -0.01782292127609253, 0.08911460638046265, -0.03564584255218506, -0.03564584255218506, -0.01782292127609253, 0.03564584255218506, 0.05346876382827759, -0.05346876382827759, 0.03564584255218506, 0.01782292127609253, 0.03564584255218506]
[2025-05-12 13:01:43,998]: Mean: -0.00081224
[2025-05-12 13:01:43,999]: Min: -0.14258337
[2025-05-12 13:01:43,999]: Max: 0.12476045
[2025-05-12 13:01:43,999]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 13:01:44,000]: Sample Values (25 elements): [0.9914242625236511, 0.9785066843032837, 1.072908878326416, 1.0009669065475464, 1.0220566987991333, 0.9747563004493713, 1.0152126550674438, 1.0088021755218506, 1.0182791948318481, 1.0318316221237183, 1.020450234413147, 1.0148266553878784, 1.052832007408142, 1.0390413999557495, 1.0054181814193726, 1.0148457288742065, 1.0258355140686035, 0.9948778748512268, 1.0110589265823364, 0.990913450717926, 1.0400888919830322, 1.0466948747634888, 1.0445879697799683, 1.0521045923233032, 0.988078773021698]
[2025-05-12 13:01:44,000]: Mean: 1.02099097
[2025-05-12 13:01:44,000]: Min: 0.97475630
[2025-05-12 13:01:44,000]: Max: 1.07290888
[2025-05-12 13:01:44,002]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 13:01:44,003]: Sample Values (25 elements): [-0.03249005228281021, 0.06498010456562042, 0.016245026141405106, 0.016245026141405106, -0.016245026141405106, -0.04873507842421532, 0.03249005228281021, 0.03249005228281021, -0.016245026141405106, -0.03249005228281021, -0.04873507842421532, 0.0, 0.03249005228281021, -0.016245026141405106, 0.04873507842421532, 0.016245026141405106, -0.04873507842421532, 0.06498010456562042, -0.03249005228281021, 0.04873507842421532, -0.04873507842421532, -0.06498010456562042, -0.03249005228281021, -0.016245026141405106, 0.03249005228281021]
[2025-05-12 13:01:44,003]: Mean: -0.00085667
[2025-05-12 13:01:44,003]: Min: -0.11371519
[2025-05-12 13:01:44,004]: Max: 0.12996021
[2025-05-12 13:01:44,004]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,004]: Sample Values (25 elements): [0.960584819316864, 0.9593790769577026, 0.9788966178894043, 0.9725609421730042, 0.9747862219810486, 0.956320583820343, 0.9732444882392883, 0.9783793687820435, 0.9624840021133423, 0.963762640953064, 0.9722541570663452, 0.9567506313323975, 0.9580068588256836, 0.9587775468826294, 0.9975182414054871, 0.9772115349769592, 0.9985764622688293, 0.947858989238739, 0.9645901322364807, 0.9934780597686768, 0.9795843362808228, 0.9569727182388306, 0.9516335129737854, 0.9836738109588623, 0.9776362180709839]
[2025-05-12 13:01:44,005]: Mean: 0.96774483
[2025-05-12 13:01:44,012]: Min: 0.93006462
[2025-05-12 13:01:44,016]: Max: 1.00956035
[2025-05-12 13:01:44,041]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:01:44,043]: Sample Values (25 elements): [0.014959488064050674, 0.014959488064050674, -0.014959488064050674, 0.014959488064050674, -0.02991897612810135, 0.014959488064050674, -0.04487846419215202, 0.014959488064050674, -0.04487846419215202, 0.04487846419215202, 0.04487846419215202, -0.02991897612810135, -0.07479743659496307, 0.0, -0.014959488064050674, -0.0598379522562027, 0.014959488064050674, -0.014959488064050674, 0.02991897612810135, -0.014959488064050674, 0.02991897612810135, -0.04487846419215202, -0.02991897612810135, -0.014959488064050674, -0.02991897612810135]
[2025-05-12 13:01:44,043]: Mean: -0.00082621
[2025-05-12 13:01:44,043]: Min: -0.10471642
[2025-05-12 13:01:44,043]: Max: 0.11967590
[2025-05-12 13:01:44,044]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,044]: Sample Values (25 elements): [0.9861823320388794, 1.0250815153121948, 1.0220998525619507, 1.0300874710083008, 1.0172368288040161, 1.0078709125518799, 1.0163979530334473, 1.032458782196045, 1.007515788078308, 0.9903854727745056, 1.0146228075027466, 0.9829147458076477, 1.0028804540634155, 1.0018187761306763, 1.0318900346755981, 1.0379526615142822, 1.0032674074172974, 1.040146827697754, 1.021283507347107, 1.0226927995681763, 1.0307224988937378, 1.0135852098464966, 1.047183632850647, 0.9947992563247681, 1.0335103273391724]
[2025-05-12 13:01:44,044]: Mean: 1.02007508
[2025-05-12 13:01:44,045]: Min: 0.98291475
[2025-05-12 13:01:44,045]: Max: 1.06790984
[2025-05-12 13:01:44,048]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 13:01:44,048]: Sample Values (25 elements): [0.06025826930999756, 0.1506456732749939, -0.03012913465499878, 0.06025826930999756, 0.09038740396499634, -0.21090394258499146, 0.0, 0.18077480792999268, 0.0, 0.0, -0.12051653861999512, 0.06025826930999756, -0.06025826930999756, 0.12051653861999512, 0.12051653861999512, 0.09038740396499634, -0.03012913465499878, -0.18077480792999268, 0.06025826930999756, 0.03012913465499878, 0.06025826930999756, 0.21090394258499146, 0.06025826930999756, 0.06025826930999756, -0.09038740396499634]
[2025-05-12 13:01:44,049]: Mean: -0.00254509
[2025-05-12 13:01:44,049]: Min: -0.21090394
[2025-05-12 13:01:44,049]: Max: 0.24103308
[2025-05-12 13:01:44,050]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,051]: Sample Values (25 elements): [0.929046094417572, 0.9285621643066406, 0.9319599270820618, 0.8987075090408325, 0.9374711513519287, 0.9060986638069153, 0.914950430393219, 0.9101834893226624, 0.9324756860733032, 0.9269128441810608, 0.9052491188049316, 0.8932417035102844, 0.9110754132270813, 0.9306460618972778, 0.9410049915313721, 0.8999956250190735, 0.941617488861084, 0.893121600151062, 0.8847101926803589, 0.9427247643470764, 0.8909386992454529, 0.9272876977920532, 0.8993840217590332, 0.9152769446372986, 0.9222325682640076]
[2025-05-12 13:01:44,052]: Mean: 0.91027868
[2025-05-12 13:01:44,052]: Min: 0.87643528
[2025-05-12 13:01:44,052]: Max: 0.94272476
[2025-05-12 13:01:44,056]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:01:44,058]: Sample Values (25 elements): [0.0, 0.01448024157434702, 0.0, 0.01448024157434702, 0.01448024157434702, 0.02896048314869404, 0.01448024157434702, -0.05792096629738808, 0.02896048314869404, -0.04344072565436363, -0.02896048314869404, 0.04344072565436363, 0.01448024157434702, -0.04344072565436363, -0.01448024157434702, -0.05792096629738808, 0.01448024157434702, 0.02896048314869404, -0.01448024157434702, 0.0, -0.05792096629738808, 0.0, -0.04344072565436363, 0.07240121066570282, 0.0]
[2025-05-12 13:01:44,059]: Mean: -0.00088302
[2025-05-12 13:01:44,061]: Min: -0.11584193
[2025-05-12 13:01:44,064]: Max: 0.10136169
[2025-05-12 13:01:44,064]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,068]: Sample Values (25 elements): [0.994759202003479, 0.9442027807235718, 0.9841028451919556, 0.9587142467498779, 0.965021014213562, 0.9580368399620056, 0.9570540189743042, 0.9507445096969604, 0.9522728323936462, 0.9734280705451965, 1.0018149614334106, 0.9718172550201416, 0.9739644527435303, 0.9905126690864563, 0.9522886872291565, 0.9566594362258911, 0.9644768834114075, 0.9422222971916199, 0.9638046026229858, 0.9648146033287048, 0.9570666551589966, 0.9506391286849976, 0.9578540325164795, 0.9534377455711365, 0.9706233143806458]
[2025-05-12 13:01:44,070]: Mean: 0.96554303
[2025-05-12 13:01:44,073]: Min: 0.93134034
[2025-05-12 13:01:44,081]: Max: 1.02100277
[2025-05-12 13:01:44,099]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:01:44,100]: Sample Values (25 elements): [0.06061138957738876, 0.012122278101742268, 0.0, 0.012122278101742268, 0.012122278101742268, 0.0, 0.024244556203484535, -0.024244556203484535, 0.0, 0.03636683523654938, -0.024244556203484535, -0.012122278101742268, -0.012122278101742268, 0.04848911240696907, -0.024244556203484535, -0.024244556203484535, -0.04848911240696907, -0.03636683523654938, -0.03636683523654938, -0.012122278101742268, -0.024244556203484535, -0.024244556203484535, -0.03636683523654938, 0.0, 0.012122278101742268]
[2025-05-12 13:01:44,101]: Mean: 0.00013055
[2025-05-12 13:01:44,101]: Min: -0.08485594
[2025-05-12 13:01:44,101]: Max: 0.09697822
[2025-05-12 13:01:44,101]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,102]: Sample Values (25 elements): [1.0498967170715332, 1.039792537689209, 1.0496826171875, 1.0657174587249756, 1.0525914430618286, 1.109054446220398, 1.0585700273513794, 1.0278041362762451, 1.0507532358169556, 1.024215817451477, 1.0632662773132324, 1.0218700170516968, 1.050271987915039, 1.046270489692688, 1.0561699867248535, 1.031099796295166, 1.0696563720703125, 1.0268582105636597, 1.025638222694397, 1.0265651941299438, 1.0343018770217896, 1.0278996229171753, 1.0533769130706787, 1.0523388385772705, 1.0446367263793945]
[2025-05-12 13:01:44,103]: Mean: 1.03632045
[2025-05-12 13:01:44,103]: Min: 0.99142426
[2025-05-12 13:01:44,103]: Max: 1.10905445
[2025-05-12 13:01:44,105]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:01:44,107]: Sample Values (25 elements): [-0.011020963080227375, -0.0440838523209095, 0.02204192616045475, -0.0440838523209095, 0.011020963080227375, 0.0330628901720047, 0.011020963080227375, 0.0440838523209095, 0.0, 0.0440838523209095, -0.02204192616045475, -0.0330628901720047, 0.0, -0.0330628901720047, 0.011020963080227375, -0.0330628901720047, 0.02204192616045475, -0.0440838523209095, -0.0330628901720047, -0.011020963080227375, 0.0440838523209095, 0.011020963080227375, -0.02204192616045475, 0.0330628901720047, 0.0330628901720047]
[2025-05-12 13:01:44,107]: Mean: -0.00005770
[2025-05-12 13:01:44,107]: Min: -0.07714674
[2025-05-12 13:01:44,108]: Max: 0.08816770
[2025-05-12 13:01:44,108]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,108]: Sample Values (25 elements): [0.9707767367362976, 0.9566524028778076, 0.9649087190628052, 0.9705066084861755, 0.9828901886940002, 0.9427260160446167, 0.9538882374763489, 0.9501375555992126, 0.9544131755828857, 0.9480059146881104, 0.9304508566856384, 0.9479330778121948, 0.9552777409553528, 0.9313377141952515, 0.9590291380882263, 0.9708251357078552, 0.9569306373596191, 0.9702567458152771, 0.9660751223564148, 0.9538394212722778, 0.9604274034500122, 0.9482157230377197, 0.9746301770210266, 0.9510748982429504, 0.9505950808525085]
[2025-05-12 13:01:44,109]: Mean: 0.95983601
[2025-05-12 13:01:44,109]: Min: 0.93045086
[2025-05-12 13:01:44,109]: Max: 0.99116457
[2025-05-12 13:01:44,112]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 13:01:44,112]: Sample Values (25 elements): [0.009691821411252022, 0.019383642822504044, 0.0, 0.029075464233756065, -0.029075464233756065, 0.009691821411252022, 0.019383642822504044, -0.03876728564500809, 0.03876728564500809, 0.03876728564500809, 0.03876728564500809, 0.0, -0.009691821411252022, 0.019383642822504044, -0.03876728564500809, -0.009691821411252022, 0.009691821411252022, 0.0, 0.019383642822504044, -0.03876728564500809, 0.009691821411252022, -0.009691821411252022, 0.0, -0.029075464233756065, -0.03876728564500809]
[2025-05-12 13:01:44,113]: Mean: 0.00051398
[2025-05-12 13:01:44,113]: Min: -0.07753457
[2025-05-12 13:01:44,113]: Max: 0.06784275
[2025-05-12 13:01:44,113]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 13:01:44,114]: Sample Values (25 elements): [1.0502294301986694, 1.0949287414550781, 1.0729721784591675, 1.0912500619888306, 1.0963339805603027, 1.0761111974716187, 1.0652037858963013, 1.056439995765686, 1.0642822980880737, 1.0878815650939941, 1.0554760694503784, 1.0487008094787598, 1.0847601890563965, 1.0922842025756836, 1.069972276687622, 1.0780470371246338, 1.0900852680206299, 1.0488592386245728, 1.033008098602295, 1.0406417846679688, 1.078540563583374, 1.0711548328399658, 1.0626921653747559, 1.0940872430801392, 1.0805341005325317]
[2025-05-12 13:01:44,114]: Mean: 1.07492924
[2025-05-12 13:01:44,115]: Min: 1.02049708
[2025-05-12 13:01:44,116]: Max: 1.12553060
[2025-05-12 13:01:44,116]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 13:01:44,116]: Sample Values (25 elements): [-0.18845967948436737, -0.2596597671508789, -0.2823259234428406, -0.1719379723072052, 0.07498279958963394, 0.08844398707151413, 0.0861193835735321, -0.22265048325061798, -0.18568181991577148, -0.04973223805427551, -0.03410683572292328, -0.1809694766998291, 0.38642460107803345, 0.6011031270027161, -0.1226954460144043, -0.24253177642822266, -0.2200997918844223, -0.03065033070743084, 0.1302044838666916, 0.14614923298358917, 0.08303660154342651, 0.2989251911640167, -0.10362839698791504, -0.11892995238304138, 0.0493813194334507]
[2025-05-12 13:01:44,117]: Mean: -0.00645802
[2025-05-12 13:01:44,117]: Min: -0.36848602
[2025-05-12 13:01:44,117]: Max: 0.60110313
[2025-05-12 13:01:44,117]: 


QAT of ResNet20 with relu down to 3 bits...
[2025-05-12 13:01:44,423]: [ResNet20_relu_quantized_3_bits] after configure_qat:
[2025-05-12 13:01:44,533]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 13:03:35,943]: [ResNet20_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.8277 Train Acc: 0.7135 Eval Loss: 0.8186 Eval Acc: 0.7218 (LR: 0.001000)
[2025-05-12 13:05:26,255]: [ResNet20_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.7374 Train Acc: 0.7407 Eval Loss: 0.8201 Eval Acc: 0.7300 (LR: 0.001000)
[2025-05-12 13:07:16,301]: [ResNet20_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.7087 Train Acc: 0.7517 Eval Loss: 0.7768 Eval Acc: 0.7410 (LR: 0.001000)
[2025-05-12 13:09:04,892]: [ResNet20_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.7027 Train Acc: 0.7542 Eval Loss: 0.8851 Eval Acc: 0.7045 (LR: 0.001000)
[2025-05-12 13:10:54,001]: [ResNet20_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.6943 Train Acc: 0.7563 Eval Loss: 0.7489 Eval Acc: 0.7491 (LR: 0.001000)
[2025-05-12 13:12:42,560]: [ResNet20_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.6883 Train Acc: 0.7589 Eval Loss: 0.7339 Eval Acc: 0.7529 (LR: 0.001000)
[2025-05-12 13:14:32,488]: [ResNet20_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.6801 Train Acc: 0.7598 Eval Loss: 0.7061 Eval Acc: 0.7573 (LR: 0.001000)
[2025-05-12 13:16:22,007]: [ResNet20_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.6741 Train Acc: 0.7626 Eval Loss: 0.7062 Eval Acc: 0.7611 (LR: 0.001000)
[2025-05-12 13:18:11,749]: [ResNet20_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.6707 Train Acc: 0.7625 Eval Loss: 0.7744 Eval Acc: 0.7454 (LR: 0.001000)
[2025-05-12 13:20:00,329]: [ResNet20_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.6499 Train Acc: 0.7704 Eval Loss: 0.7294 Eval Acc: 0.7535 (LR: 0.001000)
[2025-05-12 13:21:48,683]: [ResNet20_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.6545 Train Acc: 0.7710 Eval Loss: 0.6612 Eval Acc: 0.7745 (LR: 0.001000)
[2025-05-12 13:23:37,369]: [ResNet20_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.6492 Train Acc: 0.7705 Eval Loss: 0.6924 Eval Acc: 0.7602 (LR: 0.001000)
[2025-05-12 13:25:26,092]: [ResNet20_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.6420 Train Acc: 0.7756 Eval Loss: 0.7166 Eval Acc: 0.7544 (LR: 0.001000)
[2025-05-12 13:27:14,632]: [ResNet20_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.6406 Train Acc: 0.7756 Eval Loss: 0.6541 Eval Acc: 0.7762 (LR: 0.001000)
[2025-05-12 13:29:03,799]: [ResNet20_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.6396 Train Acc: 0.7759 Eval Loss: 0.6431 Eval Acc: 0.7799 (LR: 0.001000)
[2025-05-12 13:30:52,032]: [ResNet20_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.6410 Train Acc: 0.7750 Eval Loss: 0.6574 Eval Acc: 0.7779 (LR: 0.001000)
[2025-05-12 13:32:39,401]: [ResNet20_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.6315 Train Acc: 0.7804 Eval Loss: 0.6815 Eval Acc: 0.7753 (LR: 0.001000)
[2025-05-12 13:34:24,675]: [ResNet20_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.6260 Train Acc: 0.7798 Eval Loss: 0.6849 Eval Acc: 0.7719 (LR: 0.001000)
[2025-05-12 13:36:09,834]: [ResNet20_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.6296 Train Acc: 0.7797 Eval Loss: 0.6839 Eval Acc: 0.7672 (LR: 0.001000)
[2025-05-12 13:37:55,480]: [ResNet20_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.6238 Train Acc: 0.7792 Eval Loss: 0.6365 Eval Acc: 0.7839 (LR: 0.001000)
[2025-05-12 13:39:43,104]: [ResNet20_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.6162 Train Acc: 0.7838 Eval Loss: 0.6278 Eval Acc: 0.7878 (LR: 0.001000)
[2025-05-12 13:41:29,649]: [ResNet20_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.6171 Train Acc: 0.7835 Eval Loss: 0.6654 Eval Acc: 0.7755 (LR: 0.001000)
[2025-05-12 13:43:16,150]: [ResNet20_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.6188 Train Acc: 0.7826 Eval Loss: 0.6503 Eval Acc: 0.7746 (LR: 0.001000)
[2025-05-12 13:44:58,411]: [ResNet20_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.6140 Train Acc: 0.7850 Eval Loss: 0.6770 Eval Acc: 0.7750 (LR: 0.001000)
[2025-05-12 13:46:36,849]: [ResNet20_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.6044 Train Acc: 0.7886 Eval Loss: 0.6382 Eval Acc: 0.7859 (LR: 0.001000)
[2025-05-12 13:48:16,074]: [ResNet20_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.6062 Train Acc: 0.7881 Eval Loss: 0.6289 Eval Acc: 0.7873 (LR: 0.001000)
[2025-05-12 13:49:51,854]: [ResNet20_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.6015 Train Acc: 0.7900 Eval Loss: 0.6514 Eval Acc: 0.7784 (LR: 0.001000)
[2025-05-12 13:51:39,880]: [ResNet20_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.5954 Train Acc: 0.7909 Eval Loss: 0.6708 Eval Acc: 0.7782 (LR: 0.001000)
[2025-05-12 13:53:32,449]: [ResNet20_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.5970 Train Acc: 0.7901 Eval Loss: 0.6390 Eval Acc: 0.7834 (LR: 0.001000)
[2025-05-12 13:55:25,197]: [ResNet20_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.5946 Train Acc: 0.7924 Eval Loss: 0.6519 Eval Acc: 0.7766 (LR: 0.000250)
[2025-05-12 13:57:15,475]: [ResNet20_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.5620 Train Acc: 0.8027 Eval Loss: 0.5645 Eval Acc: 0.8063 (LR: 0.000250)
[2025-05-12 13:59:04,827]: [ResNet20_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.5571 Train Acc: 0.8041 Eval Loss: 0.5921 Eval Acc: 0.7988 (LR: 0.000250)
[2025-05-12 14:00:52,726]: [ResNet20_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.5570 Train Acc: 0.8044 Eval Loss: 0.5686 Eval Acc: 0.8063 (LR: 0.000250)
[2025-05-12 14:02:38,859]: [ResNet20_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.5538 Train Acc: 0.8076 Eval Loss: 0.5853 Eval Acc: 0.7969 (LR: 0.000250)
[2025-05-12 14:04:26,956]: [ResNet20_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.5519 Train Acc: 0.8070 Eval Loss: 0.5759 Eval Acc: 0.8034 (LR: 0.000250)
[2025-05-12 14:06:14,992]: [ResNet20_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.5479 Train Acc: 0.8101 Eval Loss: 0.5830 Eval Acc: 0.7997 (LR: 0.000250)
[2025-05-12 14:08:03,311]: [ResNet20_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.5516 Train Acc: 0.8068 Eval Loss: 0.5732 Eval Acc: 0.8032 (LR: 0.000250)
[2025-05-12 14:09:53,708]: [ResNet20_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.5515 Train Acc: 0.8073 Eval Loss: 0.5900 Eval Acc: 0.7966 (LR: 0.000250)
[2025-05-12 14:11:43,526]: [ResNet20_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.5481 Train Acc: 0.8099 Eval Loss: 0.5757 Eval Acc: 0.7995 (LR: 0.000250)
[2025-05-12 14:13:33,801]: [ResNet20_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.5495 Train Acc: 0.8088 Eval Loss: 0.5934 Eval Acc: 0.7971 (LR: 0.000250)
[2025-05-12 14:15:20,261]: [ResNet20_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.5477 Train Acc: 0.8078 Eval Loss: 0.5656 Eval Acc: 0.8073 (LR: 0.000250)
[2025-05-12 14:17:08,328]: [ResNet20_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.5506 Train Acc: 0.8076 Eval Loss: 0.5932 Eval Acc: 0.8017 (LR: 0.000250)
[2025-05-12 14:18:58,017]: [ResNet20_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.5527 Train Acc: 0.8051 Eval Loss: 0.5652 Eval Acc: 0.8093 (LR: 0.000250)
[2025-05-12 14:20:50,497]: [ResNet20_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.5474 Train Acc: 0.8086 Eval Loss: 0.6107 Eval Acc: 0.7936 (LR: 0.000250)
[2025-05-12 14:22:44,851]: [ResNet20_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.5494 Train Acc: 0.8102 Eval Loss: 0.5729 Eval Acc: 0.8051 (LR: 0.000063)
[2025-05-12 14:24:52,948]: [ResNet20_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.5285 Train Acc: 0.8157 Eval Loss: 0.5430 Eval Acc: 0.8159 (LR: 0.000063)
[2025-05-12 14:26:48,479]: [ResNet20_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.5290 Train Acc: 0.8153 Eval Loss: 0.5417 Eval Acc: 0.8098 (LR: 0.000063)
[2025-05-12 14:28:45,976]: [ResNet20_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.5337 Train Acc: 0.8127 Eval Loss: 0.5452 Eval Acc: 0.8102 (LR: 0.000063)
[2025-05-12 14:30:50,747]: [ResNet20_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.5284 Train Acc: 0.8131 Eval Loss: 0.5440 Eval Acc: 0.8128 (LR: 0.000063)
[2025-05-12 14:32:50,213]: [ResNet20_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.5262 Train Acc: 0.8146 Eval Loss: 0.5491 Eval Acc: 0.8095 (LR: 0.000063)
[2025-05-12 14:34:44,200]: [ResNet20_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.5284 Train Acc: 0.8152 Eval Loss: 0.5462 Eval Acc: 0.8114 (LR: 0.000063)
[2025-05-12 14:36:37,211]: [ResNet20_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.5262 Train Acc: 0.8168 Eval Loss: 0.5324 Eval Acc: 0.8167 (LR: 0.000063)
[2025-05-12 14:38:34,435]: [ResNet20_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.5270 Train Acc: 0.8151 Eval Loss: 0.5297 Eval Acc: 0.8180 (LR: 0.000063)
[2025-05-12 14:40:40,238]: [ResNet20_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.5232 Train Acc: 0.8164 Eval Loss: 0.5427 Eval Acc: 0.8173 (LR: 0.000063)
[2025-05-12 14:42:51,868]: [ResNet20_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.5240 Train Acc: 0.8164 Eval Loss: 0.5360 Eval Acc: 0.8163 (LR: 0.000063)
[2025-05-12 14:45:05,605]: [ResNet20_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.5297 Train Acc: 0.8131 Eval Loss: 0.5413 Eval Acc: 0.8173 (LR: 0.000063)
[2025-05-12 14:47:19,140]: [ResNet20_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.5312 Train Acc: 0.8136 Eval Loss: 0.5479 Eval Acc: 0.8095 (LR: 0.000063)
[2025-05-12 14:49:35,222]: [ResNet20_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.5295 Train Acc: 0.8155 Eval Loss: 0.5435 Eval Acc: 0.8104 (LR: 0.000063)
[2025-05-12 14:51:51,721]: [ResNet20_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.5233 Train Acc: 0.8171 Eval Loss: 0.5665 Eval Acc: 0.8097 (LR: 0.000063)
[2025-05-12 14:54:10,354]: [ResNet20_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.5288 Train Acc: 0.8147 Eval Loss: 0.5493 Eval Acc: 0.8106 (LR: 0.000063)
[2025-05-12 14:54:10,355]: [ResNet20_relu_quantized_3_bits] Best Eval Accuracy: 0.8180
[2025-05-12 14:54:10,416]: 


Quantization of model down to 3 bits finished
[2025-05-12 14:54:10,416]: Model Architecture:
[2025-05-12 14:54:10,615]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3529], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.4703426361084)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0801], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2872118353843689, max_val=0.27341124415397644)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1594], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.115562438964844)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19283902645111084, max_val=0.19304697215557098)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.9862], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=20.903247833251953)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20209969580173492, max_val=0.20304517447948456)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3124], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.186777114868164)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0634], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20458967983722687, max_val=0.23947200179100037)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.3379], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=23.365028381347656)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0817], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.350866436958313, max_val=0.22113972902297974)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9746], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.822288513183594)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0543], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1811704933643341, max_val=0.19906078279018402)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.6440], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.508304595947266)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0510], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18979589641094208, max_val=0.16697539389133453)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.0302], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.211146831512451)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15588441491127014, max_val=0.1557677984237671)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1017], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3460233211517334, max_val=0.3659762740135193)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.7273], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.091424942016602)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0406], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14215463399887085, max_val=0.14217136800289154)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9645], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.751328468322754)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0453], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13052181899547577, max_val=0.18660199642181396)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5078], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.55450439453125)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0385], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12927493453025818, max_val=0.1400783360004425)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8875], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.212435722351074)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0391], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1370161920785904, max_val=0.1368047446012497)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7307], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=19.115137100219727)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0341], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11330760270357132, max_val=0.12518879771232605)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9332], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.53272008895874)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0320], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1047491505742073, max_val=0.11943929642438889)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0683], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23915527760982513, max_val=0.23914074897766113)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.4761], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.332555770874023)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0320], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10719337314367294, max_val=0.11663675308227539)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9844], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.8905134201049805)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09220197796821594, max_val=0.09361878782510757)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0132], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.092724800109863)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0230], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.076456218957901, max_val=0.08442066609859467)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9641], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.748420715332031)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06800289452075958, max_val=0.06750304251909256)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.7350], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=19.14520263671875)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 14:54:10,615]: 
Model Weights:
[2025-05-12 14:54:10,615]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 14:54:10,618]: Sample Values (25 elements): [-0.24746239185333252, -0.011473486199975014, 0.09604065120220184, 0.4024070203304291, 0.16912129521369934, -0.0817067101597786, -0.14523279666900635, -0.2356462925672531, 0.10368859767913818, -0.17585626244544983, 0.05485072359442711, 0.04546860232949257, -0.047165922820568085, -0.0536385253071785, -0.19440412521362305, -0.06693418323993683, 0.08182770013809204, 0.1433923840522766, 0.06097634509205818, -0.1319352686405182, 0.19996137917041779, 0.2492087334394455, -0.07330348342657089, 0.0731184184551239, -0.28873416781425476]
[2025-05-12 14:54:10,620]: Mean: -0.00065810
[2025-05-12 14:54:10,624]: Min: -0.43540755
[2025-05-12 14:54:10,629]: Max: 0.53864133
[2025-05-12 14:54:10,629]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,652]: Sample Values (16 elements): [1.205045223236084, 1.6708687543869019, 1.0218607187271118, 1.2211617231369019, 1.575015664100647, 1.4257142543792725, 1.5360697507858276, 1.0300447940826416, 1.3044307231903076, 1.4434703588485718, 1.2904860973358154, 1.0496455430984497, 1.4062689542770386, 1.1896275281906128, 1.338791847229004, 1.5114026069641113]
[2025-05-12 14:54:10,654]: Mean: 1.32624400
[2025-05-12 14:54:10,655]: Min: 1.02186072
[2025-05-12 14:54:10,655]: Max: 1.67086875
[2025-05-12 14:54:10,657]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:54:10,658]: Sample Values (25 elements): [0.0, 0.0, -0.0800890102982521, 0.0, 0.0, 0.0800890102982521, -0.0800890102982521, -0.0800890102982521, 0.0, -0.0800890102982521, 0.0, 0.0, 0.0, 0.1601780205965042, 0.0800890102982521, -0.0800890102982521, 0.0800890102982521, 0.0, 0.0800890102982521, -0.1601780205965042, -0.0800890102982521, -0.0800890102982521, -0.0800890102982521, 0.0, 0.0]
[2025-05-12 14:54:10,659]: Mean: -0.00764739
[2025-05-12 14:54:10,659]: Min: -0.32035604
[2025-05-12 14:54:10,659]: Max: 0.24026704
[2025-05-12 14:54:10,659]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,660]: Sample Values (16 elements): [1.052065134048462, 1.0571351051330566, 1.0578901767730713, 1.0467013120651245, 1.197553277015686, 1.117910623550415, 1.184775948524475, 1.0014454126358032, 0.9816200137138367, 1.0940046310424805, 1.0094491243362427, 1.2487677335739136, 1.0318471193313599, 1.0020604133605957, 0.9973217844963074, 1.0970852375030518]
[2025-05-12 14:54:10,660]: Mean: 1.07360208
[2025-05-12 14:54:10,661]: Min: 0.98162001
[2025-05-12 14:54:10,661]: Max: 1.24876773
[2025-05-12 14:54:10,665]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:54:10,665]: Sample Values (25 elements): [-0.11025317013263702, 0.0, -0.05512658506631851, -0.05512658506631851, 0.05512658506631851, 0.0, 0.0, 0.0, 0.0, -0.05512658506631851, 0.0, -0.05512658506631851, 0.0, 0.05512658506631851, 0.05512658506631851, 0.16537976264953613, 0.0, 0.0, 0.0, 0.11025317013263702, 0.0, 0.05512658506631851, 0.05512658506631851, 0.05512658506631851, 0.0]
[2025-05-12 14:54:10,666]: Mean: -0.00581413
[2025-05-12 14:54:10,666]: Min: -0.16537976
[2025-05-12 14:54:10,667]: Max: 0.22050634
[2025-05-12 14:54:10,667]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,668]: Sample Values (16 elements): [1.1696592569351196, 1.144927978515625, 1.1157276630401611, 1.213741421699524, 1.5262702703475952, 1.2375608682632446, 1.3197555541992188, 1.0873762369155884, 1.215701699256897, 1.1074652671813965, 1.2590584754943848, 1.1567144393920898, 1.2530460357666016, 1.1988595724105835, 0.9904108047485352, 1.2350531816482544]
[2025-05-12 14:54:10,668]: Mean: 1.20195806
[2025-05-12 14:54:10,669]: Min: 0.99041080
[2025-05-12 14:54:10,670]: Max: 1.52627027
[2025-05-12 14:54:10,672]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:54:10,673]: Sample Values (25 elements): [0.0, 0.0, -0.05787791684269905, 0.0, -0.1157558336853981, 0.0, -0.05787791684269905, 0.0, 0.05787791684269905, 0.05787791684269905, 0.0, 0.0, 0.1157558336853981, -0.17363375425338745, -0.1157558336853981, 0.0, 0.0, 0.0, 0.0, -0.1157558336853981, -0.1157558336853981, 0.0, 0.0, 0.0, 0.05787791684269905]
[2025-05-12 14:54:10,673]: Mean: -0.00231110
[2025-05-12 14:54:10,674]: Min: -0.17363375
[2025-05-12 14:54:10,675]: Max: 0.23151167
[2025-05-12 14:54:10,675]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,675]: Sample Values (16 elements): [1.013056993484497, 0.9464122653007507, 1.0166674852371216, 1.2127985954284668, 1.066771149635315, 1.0580854415893555, 0.9891201853752136, 1.2395912408828735, 0.9838207364082336, 0.9706987142562866, 1.0055440664291382, 1.0666818618774414, 1.0452618598937988, 1.067903757095337, 1.0394268035888672, 0.9471409320831299]
[2025-05-12 14:54:10,676]: Mean: 1.04181135
[2025-05-12 14:54:10,676]: Min: 0.94641227
[2025-05-12 14:54:10,676]: Max: 1.23959124
[2025-05-12 14:54:10,678]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:54:10,680]: Sample Values (25 elements): [-0.06343740224838257, 0.0, 0.06343740224838257, 0.06343740224838257, 0.0, 0.06343740224838257, 0.06343740224838257, -0.06343740224838257, 0.12687480449676514, -0.12687480449676514, 0.0, -0.06343740224838257, -0.06343740224838257, -0.12687480449676514, -0.12687480449676514, 0.0, 0.0, 0.0, -0.06343740224838257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 14:54:10,680]: Mean: 0.00096368
[2025-05-12 14:54:10,680]: Min: -0.19031221
[2025-05-12 14:54:10,681]: Max: 0.25374961
[2025-05-12 14:54:10,681]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,681]: Sample Values (16 elements): [1.0385446548461914, 1.2154747247695923, 1.0458906888961792, 1.1541074514389038, 1.1316648721694946, 1.0577671527862549, 1.2253912687301636, 1.1709191799163818, 1.1484133005142212, 1.0867880582809448, 1.1871310472488403, 1.0163825750350952, 1.080223798751831, 1.0432531833648682, 1.2089836597442627, 1.2152901887893677]
[2025-05-12 14:54:10,682]: Mean: 1.12663913
[2025-05-12 14:54:10,682]: Min: 1.01638258
[2025-05-12 14:54:10,682]: Max: 1.22539127
[2025-05-12 14:54:10,685]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:54:10,686]: Sample Values (25 elements): [-0.08171524107456207, -0.08171524107456207, 0.08171524107456207, 0.0, 0.0, 0.08171524107456207, 0.0, 0.0, -0.08171524107456207, 0.0, 0.08171524107456207, 0.16343048214912415, 0.08171524107456207, 0.0, 0.08171524107456207, 0.0, -0.08171524107456207, -0.16343048214912415, 0.0, 0.0, -0.08171524107456207, 0.0, 0.08171524107456207, 0.0, 0.0]
[2025-05-12 14:54:10,687]: Mean: -0.00532000
[2025-05-12 14:54:10,687]: Min: -0.32686096
[2025-05-12 14:54:10,687]: Max: 0.24514572
[2025-05-12 14:54:10,687]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,687]: Sample Values (16 elements): [0.9602105617523193, 0.9541516304016113, 1.1089601516723633, 1.0004043579101562, 1.0163666009902954, 0.9597713351249695, 1.2749974727630615, 0.9724924564361572, 1.025403618812561, 0.9086963534355164, 1.0026514530181885, 0.9577653408050537, 0.9307705163955688, 1.0761866569519043, 1.1645138263702393, 0.997019350528717]
[2025-05-12 14:54:10,688]: Mean: 1.01939762
[2025-05-12 14:54:10,689]: Min: 0.90869635
[2025-05-12 14:54:10,689]: Max: 1.27499747
[2025-05-12 14:54:10,691]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 14:54:10,691]: Sample Values (25 elements): [-0.054318707436323166, -0.054318707436323166, -0.054318707436323166, 0.054318707436323166, 0.054318707436323166, 0.0, -0.054318707436323166, 0.0, 0.054318707436323166, 0.10863741487264633, 0.054318707436323166, 0.054318707436323166, -0.054318707436323166, 0.10863741487264633, 0.10863741487264633, -0.054318707436323166, -0.054318707436323166, 0.054318707436323166, 0.0, -0.054318707436323166, -0.054318707436323166, 0.0, 0.0, 0.054318707436323166, 0.10863741487264633]
[2025-05-12 14:54:10,692]: Mean: 0.00256977
[2025-05-12 14:54:10,692]: Min: -0.16295612
[2025-05-12 14:54:10,693]: Max: 0.21727483
[2025-05-12 14:54:10,693]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 14:54:10,693]: Sample Values (16 elements): [1.0064141750335693, 1.2489279508590698, 1.0818955898284912, 1.105881929397583, 1.1818749904632568, 1.2296918630599976, 1.1660503149032593, 1.1549272537231445, 1.1237260103225708, 1.1574535369873047, 1.198746919631958, 1.2653988599777222, 1.3272558450698853, 1.0754456520080566, 1.1499478816986084, 1.1731202602386475]
[2025-05-12 14:54:10,694]: Mean: 1.16542244
[2025-05-12 14:54:10,694]: Min: 1.00641418
[2025-05-12 14:54:10,695]: Max: 1.32725585
[2025-05-12 14:54:10,697]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 14:54:10,698]: Sample Values (25 elements): [0.0, 0.050967294722795486, -0.10193458944559097, -0.10193458944559097, 0.050967294722795486, 0.050967294722795486, 0.0, -0.050967294722795486, -0.050967294722795486, 0.050967294722795486, 0.0, 0.0, -0.10193458944559097, 0.0, 0.0, -0.050967294722795486, -0.050967294722795486, -0.050967294722795486, -0.050967294722795486, 0.0, -0.10193458944559097, 0.0, 0.0, -0.10193458944559097, 0.0]
[2025-05-12 14:54:10,698]: Mean: -0.00391546
[2025-05-12 14:54:10,698]: Min: -0.20386918
[2025-05-12 14:54:10,699]: Max: 0.15290189
[2025-05-12 14:54:10,699]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,700]: Sample Values (25 elements): [1.025395154953003, 0.9963625073432922, 1.038756251335144, 0.9853249192237854, 1.0799353122711182, 1.0281704664230347, 1.0616962909698486, 1.0168637037277222, 1.029085636138916, 0.9894447326660156, 0.9911822080612183, 0.9686205983161926, 0.9492759704589844, 1.0650721788406372, 1.015738606452942, 1.097596526145935, 1.053572177886963, 1.0360902547836304, 1.0437887907028198, 1.0087820291519165, 0.9877026081085205, 0.9408478736877441, 0.9440916776657104, 1.0943028926849365, 1.0113509893417358]
[2025-05-12 14:54:10,701]: Mean: 1.01733613
[2025-05-12 14:54:10,701]: Min: 0.94084787
[2025-05-12 14:54:10,702]: Max: 1.09759653
[2025-05-12 14:54:10,705]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:54:10,705]: Sample Values (25 elements): [-0.044521577656269073, -0.044521577656269073, 0.0, 0.0, 0.0, -0.044521577656269073, -0.044521577656269073, -0.044521577656269073, 0.0, -0.044521577656269073, 0.044521577656269073, 0.044521577656269073, 0.0, 0.0, 0.044521577656269073, -0.044521577656269073, 0.044521577656269073, 0.0, 0.0, -0.13356474041938782, 0.0, 0.0, 0.0, -0.044521577656269073, 0.0]
[2025-05-12 14:54:10,706]: Mean: -0.00211593
[2025-05-12 14:54:10,706]: Min: -0.17808631
[2025-05-12 14:54:10,707]: Max: 0.13356474
[2025-05-12 14:54:10,707]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,707]: Sample Values (25 elements): [1.183464527130127, 1.1317269802093506, 1.1503283977508545, 1.1620450019836426, 1.1759384870529175, 1.0733602046966553, 1.0963181257247925, 1.1194008588790894, 1.1230876445770264, 1.107374668121338, 1.0470073223114014, 1.13367760181427, 1.0787253379821777, 1.146822214126587, 1.0699095726013184, 1.1638894081115723, 1.0619515180587769, 1.0602296590805054, 1.1259572505950928, 1.112818956375122, 1.1449463367462158, 1.0358866453170776, 1.0853137969970703, 1.1195334196090698, 1.1401863098144531]
[2025-05-12 14:54:10,708]: Mean: 1.11234760
[2025-05-12 14:54:10,709]: Min: 1.03588665
[2025-05-12 14:54:10,709]: Max: 1.18346453
[2025-05-12 14:54:10,712]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 14:54:10,713]: Sample Values (25 elements): [-0.10171422362327576, -0.10171422362327576, 0.0, 0.10171422362327576, -0.10171422362327576, 0.0, -0.30514267086982727, 0.10171422362327576, 0.2034284472465515, 0.10171422362327576, 0.10171422362327576, 0.10171422362327576, 0.10171422362327576, -0.10171422362327576, 0.10171422362327576, 0.0, 0.10171422362327576, 0.2034284472465515, -0.10171422362327576, 0.0, -0.2034284472465515, 0.10171422362327576, 0.30514267086982727, -0.10171422362327576, 0.0]
[2025-05-12 14:54:10,714]: Mean: 0.00158928
[2025-05-12 14:54:10,715]: Min: -0.30514267
[2025-05-12 14:54:10,715]: Max: 0.40685689
[2025-05-12 14:54:10,715]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,716]: Sample Values (25 elements): [0.9376580715179443, 0.822643518447876, 0.7450752258300781, 0.7914412617683411, 0.8364173769950867, 0.8720300197601318, 0.9044598937034607, 0.8584420084953308, 0.8862492442131042, 0.8809195756912231, 0.8934495449066162, 0.8639676570892334, 0.8207313418388367, 0.8724058866500854, 0.8642913103103638, 0.8804721236228943, 0.8618031740188599, 0.8534903526306152, 0.8404174447059631, 0.8658171892166138, 0.8501089215278625, 0.9166274070739746, 0.8620153665542603, 0.8438907265663147, 0.9851253628730774]
[2025-05-12 14:54:10,716]: Mean: 0.87033510
[2025-05-12 14:54:10,716]: Min: 0.74507523
[2025-05-12 14:54:10,717]: Max: 0.98512536
[2025-05-12 14:54:10,719]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:54:10,720]: Sample Values (25 elements): [0.04061796888709068, 0.0, 0.08123593777418137, 0.0, 0.0, 0.04061796888709068, -0.04061796888709068, 0.04061796888709068, -0.04061796888709068, -0.04061796888709068, -0.04061796888709068, 0.04061796888709068, 0.08123593777418137, 0.0, -0.04061796888709068, 0.08123593777418137, 0.04061796888709068, 0.0, 0.0, -0.04061796888709068, 0.04061796888709068, -0.04061796888709068, 0.0, 0.04061796888709068, 0.08123593777418137]
[2025-05-12 14:54:10,720]: Mean: -0.00246811
[2025-05-12 14:54:10,720]: Min: -0.12185390
[2025-05-12 14:54:10,721]: Max: 0.16247188
[2025-05-12 14:54:10,721]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,721]: Sample Values (25 elements): [0.9470813274383545, 0.9451327919960022, 0.9630141258239746, 1.0728013515472412, 1.029602289199829, 0.932463526725769, 1.029500126838684, 0.9971402883529663, 1.0077259540557861, 0.9941743016242981, 1.0038564205169678, 0.9709056615829468, 0.9707973003387451, 0.9647728800773621, 1.0516785383224487, 1.054394245147705, 1.0008230209350586, 1.033859372138977, 0.9638017416000366, 0.9823300838470459, 0.9715341925621033, 0.9796605110168457, 0.9955733418464661, 1.0067858695983887, 0.9663681387901306]
[2025-05-12 14:54:10,722]: Mean: 0.99976242
[2025-05-12 14:54:10,722]: Min: 0.93246353
[2025-05-12 14:54:10,722]: Max: 1.07448912
[2025-05-12 14:54:10,724]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:54:10,725]: Sample Values (25 elements): [-0.045303381979465485, -0.045303381979465485, 0.0, 0.045303381979465485, 0.0, -0.045303381979465485, 0.045303381979465485, -0.045303381979465485, 0.0, -0.045303381979465485, 0.0, 0.045303381979465485, 0.0, 0.0, -0.045303381979465485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045303381979465485, 0.0, -0.045303381979465485, -0.045303381979465485, -0.045303381979465485]
[2025-05-12 14:54:10,725]: Mean: 0.00031461
[2025-05-12 14:54:10,726]: Min: -0.13591015
[2025-05-12 14:54:10,726]: Max: 0.18121353
[2025-05-12 14:54:10,726]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,727]: Sample Values (25 elements): [1.0777610540390015, 1.1257576942443848, 1.0752581357955933, 1.0296611785888672, 1.0767886638641357, 1.0893985033035278, 1.02752685546875, 1.0600910186767578, 1.0983998775482178, 1.0496082305908203, 1.061423897743225, 1.0472825765609741, 1.0644317865371704, 1.0131248235702515, 1.0919439792633057, 1.164093017578125, 1.0375878810882568, 1.1090030670166016, 1.1098768711090088, 1.007265329360962, 1.0649595260620117, 1.0757874250411987, 1.0994267463684082, 1.1498456001281738, 1.1335945129394531]
[2025-05-12 14:54:10,727]: Mean: 1.08061528
[2025-05-12 14:54:10,727]: Min: 1.00726533
[2025-05-12 14:54:10,728]: Max: 1.16409302
[2025-05-12 14:54:10,730]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:54:10,731]: Sample Values (25 elements): [0.0, -0.03847905993461609, 0.0, 0.03847905993461609, 0.03847905993461609, 0.0, -0.03847905993461609, -0.03847905993461609, 0.0, 0.0, 0.03847905993461609, 0.03847905993461609, -0.03847905993461609, 0.0, -0.03847905993461609, 0.03847905993461609, 0.0, 0.0, 0.03847905993461609, -0.03847905993461609, -0.03847905993461609, -0.03847905993461609, 0.0, -0.03847905993461609, 0.03847905993461609]
[2025-05-12 14:54:10,731]: Mean: -0.00177865
[2025-05-12 14:54:10,731]: Min: -0.11543718
[2025-05-12 14:54:10,732]: Max: 0.15391624
[2025-05-12 14:54:10,732]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,733]: Sample Values (25 elements): [0.9814916253089905, 0.9849913120269775, 1.0071961879730225, 0.9656312465667725, 1.0292937755584717, 0.9536857604980469, 0.9714660048484802, 0.9951907992362976, 0.9517948031425476, 1.0221883058547974, 0.9686405062675476, 0.9646265506744385, 1.0120794773101807, 0.9848430156707764, 0.987129271030426, 1.002347469329834, 0.9617560505867004, 1.0071409940719604, 0.9711391925811768, 0.9881954789161682, 1.0005152225494385, 0.9622144103050232, 0.9147548675537109, 0.9546743035316467, 0.9657013416290283]
[2025-05-12 14:54:10,733]: Mean: 0.98395586
[2025-05-12 14:54:10,733]: Min: 0.91475487
[2025-05-12 14:54:10,733]: Max: 1.04344571
[2025-05-12 14:54:10,736]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 14:54:10,736]: Sample Values (25 elements): [0.0, 0.07823451608419418, -0.07823451608419418, 0.03911725804209709, 0.0, 0.11735177040100098, -0.03911725804209709, 0.03911725804209709, 0.0, -0.03911725804209709, -0.03911725804209709, -0.03911725804209709, 0.03911725804209709, 0.03911725804209709, 0.03911725804209709, 0.0, -0.03911725804209709, 0.03911725804209709, 0.0, 0.0, 0.03911725804209709, 0.03911725804209709, 0.0, 0.0, 0.03911725804209709]
[2025-05-12 14:54:10,737]: Mean: -0.00095077
[2025-05-12 14:54:10,739]: Min: -0.15646903
[2025-05-12 14:54:10,743]: Max: 0.11735177
[2025-05-12 14:54:10,743]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 14:54:10,755]: Sample Values (25 elements): [1.081626296043396, 1.0932351350784302, 1.1005711555480957, 1.1166300773620605, 1.0688135623931885, 1.0927366018295288, 1.066371202468872, 1.140476107597351, 1.0500152111053467, 1.1150975227355957, 1.0942683219909668, 1.0958666801452637, 1.0563225746154785, 1.0410267114639282, 1.0353151559829712, 1.0632904767990112, 1.0520316362380981, 1.1069892644882202, 1.0676343441009521, 1.1027473211288452, 1.1002845764160156, 1.1022874116897583, 1.1108542680740356, 1.0544273853302002, 1.081420660018921]
[2025-05-12 14:54:10,769]: Mean: 1.07782233
[2025-05-12 14:54:10,774]: Min: 1.01464355
[2025-05-12 14:54:10,774]: Max: 1.14047611
[2025-05-12 14:54:10,778]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 14:54:10,779]: Sample Values (25 elements): [-0.03407086431980133, 0.03407086431980133, 0.03407086431980133, -0.03407086431980133, 0.03407086431980133, -0.03407086431980133, 0.0, -0.03407086431980133, 0.0, 0.0, -0.03407086431980133, -0.03407086431980133, 0.0, -0.06814172863960266, 0.0, 0.0, 0.06814172863960266, 0.03407086431980133, 0.03407086431980133, 0.0, 0.0, 0.0, 0.03407086431980133, 0.0, 0.0]
[2025-05-12 14:54:10,780]: Mean: -0.00087432
[2025-05-12 14:54:10,780]: Min: -0.10221259
[2025-05-12 14:54:10,780]: Max: 0.13628346
[2025-05-12 14:54:10,780]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,781]: Sample Values (25 elements): [0.9925057888031006, 1.0105338096618652, 0.9955222010612488, 0.9654619693756104, 0.9914652109146118, 1.003865122795105, 1.0070031881332397, 1.0123037099838257, 0.9763315320014954, 0.9841390252113342, 0.9292369484901428, 1.0140039920806885, 0.9912171363830566, 0.9819105863571167, 1.0282684564590454, 0.9747057557106018, 0.989695131778717, 0.9942821264266968, 0.9568648338317871, 0.9863292574882507, 0.9726024270057678, 0.9838343858718872, 1.0312671661376953, 0.9897199869155884, 0.9712295532226562]
[2025-05-12 14:54:10,781]: Mean: 0.98945534
[2025-05-12 14:54:10,781]: Min: 0.92923695
[2025-05-12 14:54:10,782]: Max: 1.03730285
[2025-05-12 14:54:10,795]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:54:10,819]: Sample Values (25 elements): [0.03202683478593826, 0.0, 0.03202683478593826, -0.06405366957187653, 0.03202683478593826, 0.03202683478593826, -0.03202683478593826, 0.0, -0.03202683478593826, -0.03202683478593826, 0.09608050435781479, -0.06405366957187653, 0.03202683478593826, 0.0, 0.0, 0.03202683478593826, 0.03202683478593826, -0.03202683478593826, 0.03202683478593826, -0.03202683478593826, 0.03202683478593826, -0.03202683478593826, 0.06405366957187653, -0.03202683478593826, 0.0]
[2025-05-12 14:54:10,820]: Mean: -0.00048131
[2025-05-12 14:54:10,821]: Min: -0.09608050
[2025-05-12 14:54:10,821]: Max: 0.12810734
[2025-05-12 14:54:10,821]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,822]: Sample Values (25 elements): [1.0267845392227173, 1.0785750150680542, 1.0749621391296387, 1.0440152883529663, 1.0470083951950073, 1.0319972038269043, 1.0175362825393677, 1.0264979600906372, 1.0501904487609863, 1.0726414918899536, 1.0711323022842407, 1.0313583612442017, 1.0772258043289185, 1.0181653499603271, 1.1038436889648438, 1.0790677070617676, 1.0608317852020264, 1.0554368495941162, 1.0321180820465088, 1.052782416343689, 1.0922818183898926, 1.0241913795471191, 1.0436352491378784, 1.058262586593628, 1.0478074550628662]
[2025-05-12 14:54:10,822]: Mean: 1.04976630
[2025-05-12 14:54:10,823]: Min: 1.01168585
[2025-05-12 14:54:10,824]: Max: 1.10980952
[2025-05-12 14:54:10,831]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 14:54:10,833]: Sample Values (25 elements): [0.1366560161113739, 0.0, 0.06832800805568695, 0.0, -0.1366560161113739, 0.0, -0.1366560161113739, 0.06832800805568695, 0.0, -0.06832800805568695, -0.1366560161113739, -0.06832800805568695, -0.06832800805568695, 0.06832800805568695, 0.20498402416706085, 0.06832800805568695, 0.0, 0.0, 0.1366560161113739, 0.06832800805568695, 0.1366560161113739, -0.06832800805568695, 0.20498402416706085, -0.06832800805568695, 0.0]
[2025-05-12 14:54:10,834]: Mean: -0.00133453
[2025-05-12 14:54:10,834]: Min: -0.27331203
[2025-05-12 14:54:10,835]: Max: 0.20498402
[2025-05-12 14:54:10,835]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,836]: Sample Values (25 elements): [0.8912996649742126, 0.8960233330726624, 0.8799365162849426, 0.9040555953979492, 0.8872373700141907, 0.933472752571106, 0.8922553062438965, 0.9137486219406128, 0.9102353453636169, 0.8653031587600708, 0.8974131345748901, 0.9101790189743042, 0.9333204030990601, 0.9373958110809326, 0.9251857399940491, 0.8856685161590576, 0.9133808016777039, 0.8828204870223999, 0.8945743441581726, 0.9463464021682739, 0.9067574739456177, 0.8995410799980164, 0.9115999341011047, 0.9304631352424622, 0.9441658854484558]
[2025-05-12 14:54:10,836]: Mean: 0.91038382
[2025-05-12 14:54:10,837]: Min: 0.86530316
[2025-05-12 14:54:10,838]: Max: 0.95232499
[2025-05-12 14:54:10,841]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:54:10,842]: Sample Values (25 elements): [0.0, 0.031975775957107544, 0.031975775957107544, 0.09592732787132263, -0.06395155191421509, -0.031975775957107544, 0.031975775957107544, 0.0, 0.031975775957107544, 0.0, 0.0, 0.031975775957107544, -0.031975775957107544, 0.0, 0.0, -0.031975775957107544, 0.0, 0.0, 0.031975775957107544, -0.031975775957107544, -0.031975775957107544, 0.06395155191421509, 0.0, -0.06395155191421509, 0.031975775957107544]
[2025-05-12 14:54:10,843]: Mean: -0.00059504
[2025-05-12 14:54:10,844]: Min: -0.09592733
[2025-05-12 14:54:10,844]: Max: 0.12790310
[2025-05-12 14:54:10,844]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,845]: Sample Values (25 elements): [0.9579479694366455, 0.9888214468955994, 1.026261806488037, 1.017200231552124, 0.9732669591903687, 0.9941390156745911, 0.9865486025810242, 1.0001215934753418, 0.9776594042778015, 0.985276460647583, 1.0126657485961914, 0.9913569092750549, 0.9722135663032532, 0.9731848239898682, 0.9843237400054932, 1.0478264093399048, 0.9658128619194031, 0.9997343420982361, 0.9366578459739685, 0.9685648083686829, 0.9903463125228882, 0.9485633373260498, 0.9688898921012878, 1.0100445747375488, 0.9992915391921997]
[2025-05-12 14:54:10,845]: Mean: 0.98468471
[2025-05-12 14:54:10,847]: Min: 0.93665785
[2025-05-12 14:54:10,847]: Max: 1.05944300
[2025-05-12 14:54:10,850]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:54:10,851]: Sample Values (25 elements): [0.02654586173593998, 0.02654586173593998, 0.0, 0.0, -0.02654586173593998, -0.02654586173593998, -0.02654586173593998, 0.0, 0.0, 0.0, -0.02654586173593998, -0.02654586173593998, 0.0, 0.0, -0.02654586173593998, -0.05309172347187996, -0.02654586173593998, -0.02654586173593998, -0.02654586173593998, 0.0, -0.02654586173593998, 0.02654586173593998, 0.0, 0.0, 0.02654586173593998]
[2025-05-12 14:54:10,852]: Mean: 0.00018579
[2025-05-12 14:54:10,853]: Min: -0.07963759
[2025-05-12 14:54:10,854]: Max: 0.10618345
[2025-05-12 14:54:10,854]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,854]: Sample Values (25 elements): [1.065664529800415, 1.031970739364624, 1.0303821563720703, 1.0542742013931274, 1.0700299739837646, 1.0322394371032715, 1.0694239139556885, 1.0565184354782104, 1.0051255226135254, 1.0860446691513062, 1.0634750127792358, 1.0559287071228027, 1.0184205770492554, 1.0129879713058472, 1.0832018852233887, 1.0383087396621704, 1.0321159362792969, 1.0457600355148315, 1.0622596740722656, 1.0247735977172852, 1.012589693069458, 1.034109354019165, 1.0566110610961914, 1.0368578433990479, 1.0532673597335815]
[2025-05-12 14:54:10,855]: Mean: 1.04307771
[2025-05-12 14:54:10,855]: Min: 0.99487436
[2025-05-12 14:54:10,855]: Max: 1.13719428
[2025-05-12 14:54:10,858]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:54:10,859]: Sample Values (25 elements): [0.022982431575655937, 0.022982431575655937, 0.0, 0.022982431575655937, 0.0, 0.022982431575655937, -0.045964863151311874, 0.045964863151311874, -0.022982431575655937, 0.0, 0.022982431575655937, 0.022982431575655937, 0.022982431575655937, 0.022982431575655937, -0.022982431575655937, -0.022982431575655937, 0.0, 0.0, 0.022982431575655937, 0.0, 0.022982431575655937, 0.0, 0.0, 0.0, 0.045964863151311874]
[2025-05-12 14:54:10,860]: Mean: -0.00003429
[2025-05-12 14:54:10,860]: Min: -0.06894729
[2025-05-12 14:54:10,861]: Max: 0.09192973
[2025-05-12 14:54:10,861]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,863]: Sample Values (25 elements): [0.9522961378097534, 0.9612480998039246, 0.9747955203056335, 0.9718638062477112, 0.9990853667259216, 0.9556539058685303, 0.9619942307472229, 0.9659624695777893, 0.9665095210075378, 0.9732623100280762, 0.961704671382904, 0.9676791429519653, 0.974198579788208, 0.9710930585861206, 0.9578717350959778, 0.9595112204551697, 0.955946683883667, 0.984311580657959, 0.9553772807121277, 0.9648999571800232, 0.9347506761550903, 0.9574018120765686, 0.9506681561470032, 0.9682577252388, 0.9704270362854004]
[2025-05-12 14:54:10,863]: Mean: 0.96398294
[2025-05-12 14:54:10,864]: Min: 0.93210530
[2025-05-12 14:54:10,864]: Max: 1.00106025
[2025-05-12 14:54:10,866]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 14:54:10,867]: Sample Values (25 elements): [-0.01935797929763794, 0.0, 0.01935797929763794, 0.01935797929763794, -0.03871595859527588, -0.03871595859527588, 0.01935797929763794, 0.01935797929763794, -0.01935797929763794, -0.01935797929763794, 0.01935797929763794, -0.01935797929763794, 0.0, -0.05807393789291382, -0.01935797929763794, -0.01935797929763794, 0.01935797929763794, 0.0, 0.03871595859527588, 0.03871595859527588, 0.0, 0.03871595859527588, -0.03871595859527588, 0.0, 0.01935797929763794]
[2025-05-12 14:54:10,868]: Mean: 0.00068003
[2025-05-12 14:54:10,868]: Min: -0.07743192
[2025-05-12 14:54:10,868]: Max: 0.05807394
[2025-05-12 14:54:10,868]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 14:54:10,869]: Sample Values (25 elements): [1.0874555110931396, 1.0504814386367798, 1.1065583229064941, 1.0418598651885986, 1.0669479370117188, 1.053881049156189, 1.0795140266418457, 1.0628950595855713, 1.0664669275283813, 1.0619958639144897, 1.0929795503616333, 1.075709581375122, 1.053719162940979, 1.0545430183410645, 1.0798295736312866, 1.0337378978729248, 1.0512664318084717, 1.1045057773590088, 1.0497416257858276, 1.0328266620635986, 1.058559775352478, 1.0788911581039429, 1.0655903816223145, 1.078967809677124, 1.074533462524414]
[2025-05-12 14:54:10,869]: Mean: 1.06292820
[2025-05-12 14:54:10,869]: Min: 1.01167715
[2025-05-12 14:54:10,870]: Max: 1.10821342
[2025-05-12 14:54:10,870]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 14:54:10,870]: Sample Values (25 elements): [0.03315221145749092, -0.1328880935907364, 0.2885635197162628, -0.08837903290987015, 0.12014617025852203, 0.0996471419930458, -0.11964953690767288, 0.11721284687519073, -0.2678682506084442, -0.2014734447002411, 0.33144015073776245, 0.21853579580783844, 0.3031632602214813, 0.03300248831510544, 0.2601688504219055, -0.23575034737586975, 0.12897378206253052, -0.155499666929245, 0.14413245022296906, -0.15834331512451172, -0.041760772466659546, -0.21623685956001282, -0.25809282064437866, -0.14852085709571838, 0.09303431957960129]
[2025-05-12 14:54:10,870]: Mean: -0.00645804
[2025-05-12 14:54:10,871]: Min: -0.34481686
[2025-05-12 14:54:10,871]: Max: 0.55554652
[2025-05-12 14:54:10,871]: 


QAT of ResNet20 with relu down to 2 bits...
[2025-05-12 14:54:11,237]: [ResNet20_relu_quantized_2_bits] after configure_qat:
[2025-05-12 14:54:11,418]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 14:56:06,035]: [ResNet20_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.0666 Train Acc: 0.2358 Eval Loss: 1.9865 Eval Acc: 0.2727 (LR: 0.001000)
[2025-05-12 14:57:55,585]: [ResNet20_relu_quantized_2_bits] Epoch: 002 Train Loss: 1.8356 Train Acc: 0.3119 Eval Loss: 1.7345 Eval Acc: 0.3492 (LR: 0.001000)
[2025-05-12 14:59:46,157]: [ResNet20_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.7556 Train Acc: 0.3509 Eval Loss: 1.6694 Eval Acc: 0.3813 (LR: 0.001000)
[2025-05-12 15:01:36,324]: [ResNet20_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.6786 Train Acc: 0.3770 Eval Loss: 1.6053 Eval Acc: 0.4077 (LR: 0.001000)
[2025-05-12 15:03:27,728]: [ResNet20_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.6385 Train Acc: 0.3928 Eval Loss: 1.6076 Eval Acc: 0.4033 (LR: 0.001000)
[2025-05-12 15:05:20,670]: [ResNet20_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.6180 Train Acc: 0.4030 Eval Loss: 1.6031 Eval Acc: 0.4114 (LR: 0.001000)
[2025-05-12 15:07:14,411]: [ResNet20_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.5863 Train Acc: 0.4125 Eval Loss: 1.5827 Eval Acc: 0.4197 (LR: 0.001000)
[2025-05-12 15:09:07,335]: [ResNet20_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.5717 Train Acc: 0.4216 Eval Loss: 1.6815 Eval Acc: 0.3955 (LR: 0.001000)
[2025-05-12 15:10:59,986]: [ResNet20_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.5903 Train Acc: 0.4133 Eval Loss: 1.5879 Eval Acc: 0.4219 (LR: 0.001000)
[2025-05-12 15:12:52,450]: [ResNet20_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.5688 Train Acc: 0.4230 Eval Loss: 1.6154 Eval Acc: 0.4157 (LR: 0.001000)
[2025-05-12 15:14:42,328]: [ResNet20_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.5798 Train Acc: 0.4184 Eval Loss: 1.5862 Eval Acc: 0.4238 (LR: 0.001000)
[2025-05-12 15:16:32,310]: [ResNet20_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.5515 Train Acc: 0.4315 Eval Loss: 1.5122 Eval Acc: 0.4419 (LR: 0.001000)
[2025-05-12 15:18:26,244]: [ResNet20_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.5851 Train Acc: 0.4124 Eval Loss: 1.5689 Eval Acc: 0.4338 (LR: 0.001000)
[2025-05-12 15:20:20,107]: [ResNet20_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.5757 Train Acc: 0.4209 Eval Loss: 1.6817 Eval Acc: 0.3954 (LR: 0.001000)
[2025-05-12 15:22:12,890]: [ResNet20_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.6286 Train Acc: 0.4025 Eval Loss: 1.6678 Eval Acc: 0.3930 (LR: 0.001000)
[2025-05-12 15:24:04,915]: [ResNet20_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.6294 Train Acc: 0.4049 Eval Loss: 1.6972 Eval Acc: 0.3812 (LR: 0.001000)
[2025-05-12 15:25:56,571]: [ResNet20_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.6129 Train Acc: 0.4047 Eval Loss: 1.6206 Eval Acc: 0.4072 (LR: 0.001000)
[2025-05-12 15:27:47,334]: [ResNet20_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.6144 Train Acc: 0.4051 Eval Loss: 1.5403 Eval Acc: 0.4373 (LR: 0.001000)
[2025-05-12 15:29:35,642]: [ResNet20_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.6214 Train Acc: 0.4019 Eval Loss: 1.5464 Eval Acc: 0.4292 (LR: 0.001000)
[2025-05-12 15:31:25,249]: [ResNet20_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.5722 Train Acc: 0.4195 Eval Loss: 1.5793 Eval Acc: 0.4294 (LR: 0.001000)
[2025-05-12 15:33:12,812]: [ResNet20_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.5763 Train Acc: 0.4180 Eval Loss: 1.5309 Eval Acc: 0.4353 (LR: 0.001000)
[2025-05-12 15:35:00,909]: [ResNet20_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.5714 Train Acc: 0.4178 Eval Loss: 1.5486 Eval Acc: 0.4201 (LR: 0.001000)
[2025-05-12 15:36:49,148]: [ResNet20_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.5505 Train Acc: 0.4288 Eval Loss: 1.5693 Eval Acc: 0.4226 (LR: 0.001000)
[2025-05-12 15:38:37,538]: [ResNet20_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.5523 Train Acc: 0.4251 Eval Loss: 1.5958 Eval Acc: 0.4202 (LR: 0.001000)
[2025-05-12 15:40:25,880]: [ResNet20_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.5328 Train Acc: 0.4317 Eval Loss: 1.4958 Eval Acc: 0.4463 (LR: 0.001000)
[2025-05-12 15:42:13,381]: [ResNet20_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.5158 Train Acc: 0.4395 Eval Loss: 1.5944 Eval Acc: 0.4357 (LR: 0.001000)
[2025-05-12 15:44:00,605]: [ResNet20_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.4974 Train Acc: 0.4496 Eval Loss: 1.5232 Eval Acc: 0.4400 (LR: 0.001000)
[2025-05-12 15:45:46,821]: [ResNet20_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.4837 Train Acc: 0.4543 Eval Loss: 1.4999 Eval Acc: 0.4490 (LR: 0.001000)
[2025-05-12 15:47:35,076]: [ResNet20_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.4786 Train Acc: 0.4567 Eval Loss: 1.4699 Eval Acc: 0.4570 (LR: 0.001000)
[2025-05-12 15:49:27,771]: [ResNet20_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.4671 Train Acc: 0.4625 Eval Loss: 1.5393 Eval Acc: 0.4404 (LR: 0.000250)
[2025-05-12 15:51:24,281]: [ResNet20_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.4662 Train Acc: 0.4599 Eval Loss: 1.4338 Eval Acc: 0.4684 (LR: 0.000250)
[2025-05-12 15:53:23,914]: [ResNet20_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.5230 Train Acc: 0.4374 Eval Loss: 1.4471 Eval Acc: 0.4683 (LR: 0.000250)
[2025-05-12 15:55:22,559]: [ResNet20_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.5008 Train Acc: 0.4496 Eval Loss: 1.4395 Eval Acc: 0.4702 (LR: 0.000250)
[2025-05-12 15:57:17,863]: [ResNet20_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.4747 Train Acc: 0.4608 Eval Loss: 1.6319 Eval Acc: 0.4077 (LR: 0.000250)
[2025-05-12 15:59:02,788]: [ResNet20_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.4747 Train Acc: 0.4602 Eval Loss: 1.5056 Eval Acc: 0.4587 (LR: 0.000250)
[2025-05-12 16:00:39,535]: [ResNet20_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.5145 Train Acc: 0.4472 Eval Loss: 1.4902 Eval Acc: 0.4565 (LR: 0.000250)
[2025-05-12 16:02:24,565]: [ResNet20_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.5002 Train Acc: 0.4514 Eval Loss: 1.4542 Eval Acc: 0.4731 (LR: 0.000250)
[2025-05-12 16:04:11,624]: [ResNet20_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.4735 Train Acc: 0.4608 Eval Loss: 1.5383 Eval Acc: 0.4426 (LR: 0.000250)
[2025-05-12 16:06:04,278]: [ResNet20_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.4930 Train Acc: 0.4512 Eval Loss: 1.6185 Eval Acc: 0.4319 (LR: 0.000250)
[2025-05-12 16:07:59,562]: [ResNet20_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.4921 Train Acc: 0.4543 Eval Loss: 1.5029 Eval Acc: 0.4570 (LR: 0.000250)
[2025-05-12 16:09:54,803]: [ResNet20_relu_quantized_2_bits] Epoch: 041 Train Loss: 1.4794 Train Acc: 0.4619 Eval Loss: 1.5315 Eval Acc: 0.4465 (LR: 0.000250)
[2025-05-12 16:11:48,960]: [ResNet20_relu_quantized_2_bits] Epoch: 042 Train Loss: 1.4669 Train Acc: 0.4666 Eval Loss: 1.4806 Eval Acc: 0.4635 (LR: 0.000250)
[2025-05-12 16:13:40,910]: [ResNet20_relu_quantized_2_bits] Epoch: 043 Train Loss: 1.4694 Train Acc: 0.4633 Eval Loss: 1.4525 Eval Acc: 0.4686 (LR: 0.000250)
[2025-05-12 16:15:32,786]: [ResNet20_relu_quantized_2_bits] Epoch: 044 Train Loss: 1.4716 Train Acc: 0.4621 Eval Loss: 1.5185 Eval Acc: 0.4449 (LR: 0.000250)
[2025-05-12 16:17:21,965]: [ResNet20_relu_quantized_2_bits] Epoch: 045 Train Loss: 1.4738 Train Acc: 0.4630 Eval Loss: 1.4912 Eval Acc: 0.4533 (LR: 0.000063)
[2025-05-12 16:19:07,772]: [ResNet20_relu_quantized_2_bits] Epoch: 046 Train Loss: 1.5658 Train Acc: 0.4254 Eval Loss: 1.5803 Eval Acc: 0.4306 (LR: 0.000063)
[2025-05-12 16:20:53,910]: [ResNet20_relu_quantized_2_bits] Epoch: 047 Train Loss: 1.6175 Train Acc: 0.4057 Eval Loss: 1.5875 Eval Acc: 0.4209 (LR: 0.000063)
[2025-05-12 16:22:39,687]: [ResNet20_relu_quantized_2_bits] Epoch: 048 Train Loss: 1.6257 Train Acc: 0.4067 Eval Loss: 1.6550 Eval Acc: 0.4006 (LR: 0.000063)
[2025-05-12 16:24:26,192]: [ResNet20_relu_quantized_2_bits] Epoch: 049 Train Loss: 1.6273 Train Acc: 0.4051 Eval Loss: 1.5734 Eval Acc: 0.4266 (LR: 0.000063)
[2025-05-12 16:26:15,852]: [ResNet20_relu_quantized_2_bits] Epoch: 050 Train Loss: 1.6089 Train Acc: 0.4098 Eval Loss: 1.6044 Eval Acc: 0.4121 (LR: 0.000063)
[2025-05-12 16:28:08,119]: [ResNet20_relu_quantized_2_bits] Epoch: 051 Train Loss: 1.6042 Train Acc: 0.4105 Eval Loss: 1.5479 Eval Acc: 0.4322 (LR: 0.000063)
[2025-05-12 16:30:01,226]: [ResNet20_relu_quantized_2_bits] Epoch: 052 Train Loss: 1.5962 Train Acc: 0.4147 Eval Loss: 1.6199 Eval Acc: 0.4126 (LR: 0.000063)
[2025-05-12 16:31:54,102]: [ResNet20_relu_quantized_2_bits] Epoch: 053 Train Loss: 1.5950 Train Acc: 0.4159 Eval Loss: 1.5621 Eval Acc: 0.4169 (LR: 0.000063)
[2025-05-12 16:33:47,625]: [ResNet20_relu_quantized_2_bits] Epoch: 054 Train Loss: 1.5708 Train Acc: 0.4235 Eval Loss: 1.6114 Eval Acc: 0.4073 (LR: 0.000063)
[2025-05-12 16:35:43,064]: [ResNet20_relu_quantized_2_bits] Epoch: 055 Train Loss: 1.5640 Train Acc: 0.4274 Eval Loss: 1.5207 Eval Acc: 0.4432 (LR: 0.000063)
[2025-05-12 16:37:38,227]: [ResNet20_relu_quantized_2_bits] Epoch: 056 Train Loss: 1.5299 Train Acc: 0.4401 Eval Loss: 1.5043 Eval Acc: 0.4502 (LR: 0.000063)
[2025-05-12 16:39:32,075]: [ResNet20_relu_quantized_2_bits] Epoch: 057 Train Loss: 1.5434 Train Acc: 0.4346 Eval Loss: 1.5500 Eval Acc: 0.4303 (LR: 0.000063)
[2025-05-12 16:39:32,076]: Early stopping was triggered!
[2025-05-12 16:39:32,076]: [ResNet20_relu_quantized_2_bits] Best Eval Accuracy: 0.4731
[2025-05-12 16:39:32,141]: 


Quantization of model down to 2 bits finished
[2025-05-12 16:39:32,141]: Model Architecture:
[2025-05-12 16:39:32,314]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.1110], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=21.332910537719727)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2072], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37516751885414124, max_val=0.24634124338626862)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5836], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.750744819641113)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1840], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2668440639972687, max_val=0.285247266292572)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.0982], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=24.294740676879883)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2229], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.339605450630188, max_val=0.3291918933391571)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5783], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.734955310821533)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1315], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19689059257507324, max_val=0.19757428765296936)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.6968], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.090473175048828)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2883], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3793187737464905, max_val=0.4854806065559387)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8983], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.69487476348877)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1542], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24029530584812164, max_val=0.22231525182724)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([9.2315], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=27.69452667236328)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1358], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19202399253845215, max_val=0.21546882390975952)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.9059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=11.717671394348145)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1090], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15720121562480927, max_val=0.16968181729316711)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2680], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40202730894088745, max_val=0.40194815397262573)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.9998], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.999390602111816)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1090], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.177484393119812, max_val=0.1495995670557022)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1327], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.398085117340088)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0985], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11966151744127274, max_val=0.1758430004119873)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.1999], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=18.599550247192383)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0852], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13264930248260498, max_val=0.12293024361133575)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2689], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.806763648986816)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0954], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.13052508234977722, max_val=0.15560902655124664)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.6057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=22.816986083984375)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0851], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12045065313577652, max_val=0.1347043514251709)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.6149], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.844578742980957)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0677], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09288142621517181, max_val=0.11021459102630615)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1595], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24512024223804474, max_val=0.2333417683839798)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.6274], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.882163047790527)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0885], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12060346454381943, max_val=0.14479602873325348)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1028], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.308437824249268)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0696], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10418429970741272, max_val=0.10458491742610931)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.4030], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.209066390991211)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0986718013882637, max_val=0.10708460956811905)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.011529922485352)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0558], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08377181738615036, max_val=0.0837639570236206)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.1528], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.45849895477295)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-12 16:39:32,314]: 
Model Weights:
[2025-05-12 16:39:32,314]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-12 16:39:32,315]: Sample Values (25 elements): [-0.1614980250597, 0.18757393956184387, -0.24063092470169067, -0.23554833233356476, 0.0515478141605854, 0.029163667932152748, -0.18616175651550293, -0.08162938058376312, -0.07253244519233704, 0.19552534818649292, -0.04087146744132042, 0.23342137038707733, 0.07666755467653275, -0.15982039272785187, 0.3861222267150879, -0.337919145822525, -0.01406110916286707, -0.09600237011909485, 0.17978982627391815, 0.2116982489824295, -0.013831233605742455, 0.2120586782693863, -0.030445467680692673, -0.21205970644950867, 0.13659925758838654]
[2025-05-12 16:39:32,315]: Mean: -0.01385887
[2025-05-12 16:39:32,315]: Min: -0.82611513
[2025-05-12 16:39:32,316]: Max: 0.61617386
[2025-05-12 16:39:32,316]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,317]: Sample Values (16 elements): [2.085876941680908, 1.9580199718475342, 1.7911278009414673, 2.689025640487671, 1.3064379692077637, 1.1062037944793701, 1.4107168912887573, 1.696332335472107, 1.3108129501342773, 1.0417475700378418, 3.104790687561035, 1.9666188955307007, 2.3502540588378906, 1.1203914880752563, 3.142038345336914, 1.4160139560699463]
[2025-05-12 16:39:32,317]: Mean: 1.84352565
[2025-05-12 16:39:32,318]: Min: 1.04174757
[2025-05-12 16:39:32,319]: Max: 3.14203835
[2025-05-12 16:39:32,320]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:39:32,321]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20717200636863708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20717200636863708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20717200636863708]
[2025-05-12 16:39:32,321]: Mean: -0.00440600
[2025-05-12 16:39:32,322]: Min: -0.41434401
[2025-05-12 16:39:32,322]: Max: 0.20717201
[2025-05-12 16:39:32,322]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,323]: Sample Values (16 elements): [1.4182243347167969, 0.9856216907501221, 1.1990694999694824, 1.2106549739837646, 0.9760050773620605, 1.2490209341049194, 1.0009098052978516, 1.2524645328521729, 1.1760663986206055, 1.110205054283142, 1.0483359098434448, 1.10051429271698, 1.0835886001586914, 1.059478998184204, 0.9745236039161682, 1.0749114751815796]
[2025-05-12 16:39:32,323]: Mean: 1.11997473
[2025-05-12 16:39:32,323]: Min: 0.97452360
[2025-05-12 16:39:32,324]: Max: 1.41822433
[2025-05-12 16:39:32,326]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:39:32,327]: Sample Values (25 elements): [0.0, 0.0, 0.18402987718582153, 0.18402987718582153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18402987718582153, 0.0, 0.0, 0.18402987718582153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18402987718582153, 0.0, 0.0]
[2025-05-12 16:39:32,327]: Mean: 0.00535156
[2025-05-12 16:39:32,328]: Min: -0.18402988
[2025-05-12 16:39:32,328]: Max: 0.36805975
[2025-05-12 16:39:32,328]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,329]: Sample Values (16 elements): [1.1958425045013428, 1.1598961353302002, 1.280735731124878, 1.3352776765823364, 2.0619959831237793, 1.1181296110153198, 1.2389311790466309, 1.2897809743881226, 1.8558834791183472, 1.0105353593826294, 1.4137728214263916, 1.2055879831314087, 1.1941397190093994, 1.7195954322814941, 1.3558461666107178, 1.1558849811553955]
[2025-05-12 16:39:32,329]: Mean: 1.34948981
[2025-05-12 16:39:32,329]: Min: 1.01053536
[2025-05-12 16:39:32,330]: Max: 2.06199598
[2025-05-12 16:39:32,332]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:39:32,333]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2229340523481369, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2229340523481369, 0.0, 0.0, 0.0]
[2025-05-12 16:39:32,333]: Mean: -0.00483798
[2025-05-12 16:39:32,333]: Min: -0.44586810
[2025-05-12 16:39:32,333]: Max: 0.22293405
[2025-05-12 16:39:32,333]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,334]: Sample Values (16 elements): [0.9789327383041382, 1.080971360206604, 0.9128629565238953, 0.892448902130127, 1.0960891246795654, 1.1714270114898682, 0.9381607174873352, 0.9686964750289917, 1.1533246040344238, 0.9777641296386719, 0.9779403209686279, 1.1598634719848633, 1.0330678224563599, 0.954733669757843, 1.1963179111480713, 1.2598010301589966]
[2025-05-12 16:39:32,335]: Mean: 1.04702520
[2025-05-12 16:39:32,335]: Min: 0.89244890
[2025-05-12 16:39:32,335]: Max: 1.25980103
[2025-05-12 16:39:32,337]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:39:32,338]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.13148705661296844, 0.13148705661296844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.13148705661296844, 0.0, -0.13148705661296844, 0.0, 0.13148705661296844, 0.0, 0.0, 0.13148705661296844, 0.0, 0.13148705661296844, 0.0, -0.13148705661296844, 0.13148705661296844, 0.0]
[2025-05-12 16:39:32,338]: Mean: -0.00194035
[2025-05-12 16:39:32,339]: Min: -0.13148706
[2025-05-12 16:39:32,339]: Max: 0.26297411
[2025-05-12 16:39:32,339]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,340]: Sample Values (16 elements): [1.6702793836593628, 1.1081111431121826, 1.0095160007476807, 1.125739574432373, 1.1333301067352295, 1.0793739557266235, 1.1455985307693481, 1.6793029308319092, 1.2397311925888062, 0.9333884119987488, 1.1583905220031738, 1.9016529321670532, 1.1403214931488037, 1.15897798538208, 1.1297937631607056, 1.15121328830719]
[2025-05-12 16:39:32,340]: Mean: 1.23529518
[2025-05-12 16:39:32,340]: Min: 0.93338841
[2025-05-12 16:39:32,341]: Max: 1.90165293
[2025-05-12 16:39:32,342]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:39:32,343]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:39:32,344]: Mean: -0.00025023
[2025-05-12 16:39:32,344]: Min: -0.28826708
[2025-05-12 16:39:32,344]: Max: 0.57653415
[2025-05-12 16:39:32,344]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,345]: Sample Values (16 elements): [0.9256888031959534, 1.123852252960205, 0.9781515002250671, 1.3964486122131348, 0.9427797198295593, 1.1222572326660156, 1.1044903993606567, 0.9749782681465149, 1.0523467063903809, 1.0035834312438965, 1.0047740936279297, 1.0289816856384277, 0.9958274364471436, 1.1392072439193726, 0.9863218665122986, 1.0428820848464966]
[2025-05-12 16:39:32,345]: Mean: 1.05141079
[2025-05-12 16:39:32,345]: Min: 0.92568880
[2025-05-12 16:39:32,346]: Max: 1.39644861
[2025-05-12 16:39:32,347]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-12 16:39:32,348]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.15420342981815338, 0.0, 0.0, 0.15420342981815338, 0.0, -0.15420342981815338, 0.0, 0.0, 0.0, 0.0, 0.15420342981815338, 0.0, 0.0, 0.0, 0.0, 0.15420342981815338, 0.0, 0.0, 0.0, -0.15420342981815338]
[2025-05-12 16:39:32,349]: Mean: 0.00348029
[2025-05-12 16:39:32,349]: Min: -0.30840686
[2025-05-12 16:39:32,349]: Max: 0.15420343
[2025-05-12 16:39:32,349]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-12 16:39:32,350]: Sample Values (16 elements): [0.9583470225334167, 1.7733745574951172, 0.9703233242034912, 1.1298515796661377, 1.152780294418335, 1.1428340673446655, 1.0733349323272705, 1.5432943105697632, 1.0738519430160522, 0.988852322101593, 1.3844050168991089, 1.1232020854949951, 1.1332528591156006, 1.1533210277557373, 1.169398546218872, 1.1307196617126465]
[2025-05-12 16:39:32,351]: Mean: 1.18132138
[2025-05-12 16:39:32,351]: Min: 0.95834702
[2025-05-12 16:39:32,351]: Max: 1.77337456
[2025-05-12 16:39:32,354]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-12 16:39:32,354]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358310431241989, 0.0, -0.1358310431241989, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1358310431241989, 0.1358310431241989, 0.0, 0.0, 0.0, 0.0, 0.1358310431241989, 0.0]
[2025-05-12 16:39:32,354]: Mean: -0.00763460
[2025-05-12 16:39:32,356]: Min: -0.13583104
[2025-05-12 16:39:32,356]: Max: 0.27166209
[2025-05-12 16:39:32,356]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,358]: Sample Values (25 elements): [1.0861283540725708, 1.0604883432388306, 1.1514859199523926, 1.0746546983718872, 1.0709879398345947, 1.2159956693649292, 1.0536930561065674, 1.2897238731384277, 1.1313731670379639, 1.0935338735580444, 1.0378507375717163, 1.2602726221084595, 1.0417841672897339, 0.9627347588539124, 1.102860689163208, 1.0971063375473022, 1.1447923183441162, 1.155364751815796, 1.210245966911316, 1.0148144960403442, 1.0534592866897583, 1.2262756824493408, 1.096736192703247, 1.1679246425628662, 1.3042563199996948]
[2025-05-12 16:39:32,358]: Mean: 1.12974048
[2025-05-12 16:39:32,358]: Min: 0.96273476
[2025-05-12 16:39:32,359]: Max: 1.30425632
[2025-05-12 16:39:32,361]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:39:32,362]: Sample Values (25 elements): [-0.10896049439907074, -0.10896049439907074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10896049439907074, 0.0, -0.10896049439907074, -0.10896049439907074]
[2025-05-12 16:39:32,362]: Mean: -0.00361783
[2025-05-12 16:39:32,363]: Min: -0.10896049
[2025-05-12 16:39:32,363]: Max: 0.21792099
[2025-05-12 16:39:32,363]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,363]: Sample Values (25 elements): [1.1579487323760986, 1.0756170749664307, 1.114137887954712, 1.0944225788116455, 1.2877565622329712, 1.094111442565918, 1.0228865146636963, 1.174957036972046, 1.0365463495254517, 1.1294357776641846, 1.1105917692184448, 1.2834808826446533, 1.0971977710723877, 1.4089266061782837, 1.0807101726531982, 1.087106704711914, 1.2023929357528687, 1.278245449066162, 1.2120604515075684, 1.1224182844161987, 1.1356619596481323, 1.075099229812622, 1.1515644788742065, 1.0518996715545654, 1.1494680643081665]
[2025-05-12 16:39:32,364]: Mean: 1.14638340
[2025-05-12 16:39:32,364]: Min: 1.02288651
[2025-05-12 16:39:32,365]: Max: 1.40892661
[2025-05-12 16:39:32,366]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-12 16:39:32,367]: Sample Values (25 elements): [0.2679916024208069, 0.0, 0.2679916024208069, 0.2679916024208069, 0.0, 0.0, -0.2679916024208069, 0.2679916024208069, 0.0, 0.0, -0.2679916024208069, -0.2679916024208069, -0.2679916024208069, 0.2679916024208069, 0.0, 0.0, 0.0, 0.0, -0.2679916024208069, -0.2679916024208069, 0.0, -0.2679916024208069, 0.0, -0.2679916024208069, 0.0]
[2025-05-12 16:39:32,367]: Mean: -0.01308553
[2025-05-12 16:39:32,368]: Min: -0.53598320
[2025-05-12 16:39:32,368]: Max: 0.26799160
[2025-05-12 16:39:32,368]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,369]: Sample Values (25 elements): [0.9119137525558472, 0.8825262784957886, 0.9088401794433594, 0.8928341269493103, 1.0336374044418335, 0.980035126209259, 1.0559180974960327, 1.071927785873413, 0.8890393376350403, 0.9024382829666138, 0.9079936146736145, 0.9411046504974365, 0.9502336382865906, 0.871999204158783, 0.9819647073745728, 0.9194945096969604, 1.1210746765136719, 0.8768569827079773, 0.9819336533546448, 0.9176455736160278, 0.9978297352790833, 0.9785894155502319, 0.9180892109870911, 0.917055606842041, 0.8686561584472656]
[2025-05-12 16:39:32,369]: Mean: 0.94793677
[2025-05-12 16:39:32,370]: Min: 0.85831845
[2025-05-12 16:39:32,370]: Max: 1.12107468
[2025-05-12 16:39:32,372]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:39:32,373]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10902945697307587, 0.0, 0.0, 0.10902945697307587, 0.0, -0.10902945697307587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:39:32,373]: Mean: -0.00592706
[2025-05-12 16:39:32,373]: Min: -0.21805891
[2025-05-12 16:39:32,374]: Max: 0.10902946
[2025-05-12 16:39:32,374]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,374]: Sample Values (25 elements): [1.2438958883285522, 0.935371458530426, 0.99898761510849, 1.0644638538360596, 0.9792872667312622, 0.942980170249939, 1.0059868097305298, 1.03866708278656, 1.0343492031097412, 1.0046073198318481, 0.9673612713813782, 1.0135328769683838, 0.9916239380836487, 0.9591735601425171, 1.0300590991973877, 0.9810432195663452, 0.9825067520141602, 1.0043519735336304, 0.9654203057289124, 1.0165801048278809, 1.1494888067245483, 1.043793797492981, 1.0042147636413574, 0.9938964247703552, 1.0629535913467407]
[2025-05-12 16:39:32,374]: Mean: 1.01516366
[2025-05-12 16:39:32,375]: Min: 0.93537146
[2025-05-12 16:39:32,375]: Max: 1.24389589
[2025-05-12 16:39:32,377]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:39:32,378]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09850174188613892, 0.0, -0.09850174188613892, 0.0, 0.0, 0.0, -0.09850174188613892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09850174188613892, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:39:32,378]: Mean: 0.00086574
[2025-05-12 16:39:32,378]: Min: -0.09850174
[2025-05-12 16:39:32,378]: Max: 0.19700348
[2025-05-12 16:39:32,378]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,379]: Sample Values (25 elements): [1.0640259981155396, 1.0659794807434082, 1.144012212753296, 1.1165190935134888, 1.4702190160751343, 1.3062293529510498, 1.0999855995178223, 1.1299809217453003, 1.067905068397522, 1.2565093040466309, 1.18936288356781, 1.0806270837783813, 1.0658241510391235, 1.092777132987976, 1.3014190196990967, 1.1864615678787231, 1.1808671951293945, 1.1446329355239868, 1.1288132667541504, 1.1364794969558716, 1.1116498708724976, 1.2134106159210205, 1.2669976949691772, 1.0571868419647217, 1.099043607711792]
[2025-05-12 16:39:32,379]: Mean: 1.15893257
[2025-05-12 16:39:32,379]: Min: 1.04052436
[2025-05-12 16:39:32,380]: Max: 1.47021902
[2025-05-12 16:39:32,382]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:39:32,382]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08519299328327179, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08519299328327179, -0.08519299328327179, -0.08519299328327179, 0.0, 0.0, -0.08519299328327179, 0.08519299328327179, -0.08519299328327179, 0.08519299328327179, 0.0, 0.0, 0.0, -0.08519299328327179]
[2025-05-12 16:39:32,383]: Mean: -0.00507497
[2025-05-12 16:39:32,383]: Min: -0.17038599
[2025-05-12 16:39:32,383]: Max: 0.08519299
[2025-05-12 16:39:32,384]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,385]: Sample Values (25 elements): [1.0848373174667358, 0.924446702003479, 0.9499424695968628, 0.9943149089813232, 0.9642515778541565, 1.004218578338623, 0.9755203127861023, 0.934637188911438, 1.008034586906433, 0.9700864553451538, 0.9756544232368469, 1.0060224533081055, 1.014829158782959, 1.0025463104248047, 1.015514850616455, 1.030815601348877, 0.9548181891441345, 1.0079271793365479, 1.0219753980636597, 1.012332558631897, 0.9775974154472351, 0.9812862277030945, 1.0219734907150269, 0.9951228499412537, 1.0296010971069336]
[2025-05-12 16:39:32,385]: Mean: 0.99686027
[2025-05-12 16:39:32,386]: Min: 0.92444670
[2025-05-12 16:39:32,386]: Max: 1.08483732
[2025-05-12 16:39:32,389]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-12 16:39:32,390]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09537830203771591, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09537830203771591, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:39:32,390]: Mean: -0.00056921
[2025-05-12 16:39:32,390]: Min: -0.09537830
[2025-05-12 16:39:32,390]: Max: 0.19075660
[2025-05-12 16:39:32,390]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-12 16:39:32,391]: Sample Values (25 elements): [1.0675660371780396, 1.0354164838790894, 1.2357341051101685, 1.2663112878799438, 1.0700703859329224, 1.0189430713653564, 1.1250126361846924, 1.057715654373169, 1.2407406568527222, 1.1569043397903442, 1.1573923826217651, 1.0979384183883667, 1.116021990776062, 1.0424607992172241, 1.086917757987976, 1.0998938083648682, 1.121503472328186, 1.1371532678604126, 1.2185320854187012, 1.123828411102295, 1.1494709253311157, 1.1313011646270752, 1.1880489587783813, 1.164126992225647, 1.1466679573059082]
[2025-05-12 16:39:32,392]: Mean: 1.13706100
[2025-05-12 16:39:32,392]: Min: 1.01894307
[2025-05-12 16:39:32,392]: Max: 1.29857194
[2025-05-12 16:39:32,395]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-12 16:39:32,396]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.08505202829837799, 0.0, 0.0, 0.08505202829837799, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08505202829837799, 0.0, 0.0, 0.0, -0.08505202829837799, 0.0, -0.08505202829837799, 0.0, 0.0, 0.0, 0.0, 0.08505202829837799]
[2025-05-12 16:39:32,397]: Mean: -0.00292090
[2025-05-12 16:39:32,397]: Min: -0.08505203
[2025-05-12 16:39:32,397]: Max: 0.17010406
[2025-05-12 16:39:32,397]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,398]: Sample Values (25 elements): [1.0846747159957886, 1.0352137088775635, 1.0204249620437622, 1.00617253780365, 1.0285061597824097, 1.0017555952072144, 1.0332263708114624, 1.053367257118225, 1.0445013046264648, 1.0885504484176636, 0.993046224117279, 0.9972562193870544, 1.0210212469100952, 0.9997180104255676, 1.0356709957122803, 1.010964274406433, 0.9959909915924072, 1.0699212551116943, 0.9880321025848389, 1.0931391716003418, 1.0091171264648438, 1.0482450723648071, 0.9866638779640198, 1.0177665948867798, 1.066014051437378]
[2025-05-12 16:39:32,398]: Mean: 1.02195966
[2025-05-12 16:39:32,398]: Min: 0.94212145
[2025-05-12 16:39:32,399]: Max: 1.09313917
[2025-05-12 16:39:32,401]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:39:32,402]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.06769877672195435, -0.06769877672195435, 0.0, 0.0, 0.06769877672195435, 0.0, 0.0, 0.0, 0.06769877672195435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06769877672195435, 0.06769877672195435, -0.06769877672195435, 0.0, 0.0, -0.06769877672195435]
[2025-05-12 16:39:32,402]: Mean: -0.00104127
[2025-05-12 16:39:32,403]: Min: -0.06769878
[2025-05-12 16:39:32,403]: Max: 0.13539755
[2025-05-12 16:39:32,403]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,403]: Sample Values (25 elements): [1.0534775257110596, 1.0517621040344238, 1.035291314125061, 1.0508886575698853, 1.0903809070587158, 1.130202054977417, 1.0443115234375, 1.065536618232727, 1.147393822669983, 1.0932925939559937, 1.0733318328857422, 1.0792335271835327, 1.045064926147461, 1.053400993347168, 1.0322502851486206, 1.0124905109405518, 1.1010152101516724, 1.116754412651062, 1.0499926805496216, 1.0807965993881226, 1.089011549949646, 1.0684088468551636, 1.0920687913894653, 1.0438494682312012, 1.0806059837341309]
[2025-05-12 16:39:32,403]: Mean: 1.07306647
[2025-05-12 16:39:32,404]: Min: 1.01249051
[2025-05-12 16:39:32,404]: Max: 1.15500998
[2025-05-12 16:39:32,406]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-12 16:39:32,407]: Sample Values (25 elements): [-0.15948742628097534, -0.15948742628097534, 0.15948742628097534, 0.15948742628097534, 0.15948742628097534, -0.15948742628097534, 0.0, 0.0, 0.0, -0.15948742628097534, 0.15948742628097534, 0.0, 0.0, 0.0, 0.15948742628097534, -0.15948742628097534, 0.0, 0.0, 0.0, 0.0, -0.15948742628097534, 0.0, -0.15948742628097534, 0.15948742628097534, -0.15948742628097534]
[2025-05-12 16:39:32,407]: Mean: -0.01043521
[2025-05-12 16:39:32,408]: Min: -0.31897485
[2025-05-12 16:39:32,408]: Max: 0.15948743
[2025-05-12 16:39:32,408]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,408]: Sample Values (25 elements): [0.898185133934021, 0.9280974268913269, 0.9262424111366272, 0.8937942385673523, 0.9381072521209717, 0.8948348164558411, 0.9136751294136047, 0.9255150556564331, 0.9272705912590027, 0.9481380581855774, 0.9576357007026672, 0.9160952568054199, 0.9468269944190979, 0.9195472002029419, 0.8941110372543335, 0.9161386489868164, 0.9366018772125244, 0.9264630079269409, 0.8848773837089539, 0.9081009030342102, 0.9349818825721741, 0.9008429050445557, 0.9143203496932983, 0.9531750679016113, 0.9449741840362549]
[2025-05-12 16:39:32,409]: Mean: 0.92470682
[2025-05-12 16:39:32,409]: Min: 0.88195437
[2025-05-12 16:39:32,409]: Max: 0.96253598
[2025-05-12 16:39:32,412]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:39:32,415]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.08846642076969147, 0.0, 0.0, -0.08846642076969147, 0.08846642076969147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.08846642076969147, 0.0]
[2025-05-12 16:39:32,420]: Mean: -0.00095512
[2025-05-12 16:39:32,424]: Min: -0.08846642
[2025-05-12 16:39:32,436]: Max: 0.17693284
[2025-05-12 16:39:32,436]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,448]: Sample Values (25 elements): [1.0216830968856812, 0.9888746738433838, 1.0322140455245972, 1.0171796083450317, 0.9840139150619507, 1.0526373386383057, 0.9777212142944336, 0.9792529940605164, 0.9715197682380676, 1.0042873620986938, 1.0244605541229248, 0.9828683733940125, 1.0325747728347778, 1.0156383514404297, 0.9685155749320984, 0.9696623682975769, 0.9805166721343994, 0.9942089915275574, 0.9880505800247192, 0.9673654437065125, 0.9778516888618469, 0.9810067415237427, 0.9853156805038452, 0.9531752467155457, 0.9933100938796997]
[2025-05-12 16:39:32,448]: Mean: 0.99817801
[2025-05-12 16:39:32,448]: Min: 0.95040011
[2025-05-12 16:39:32,449]: Max: 1.07248533
[2025-05-12 16:39:32,450]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:39:32,452]: Sample Values (25 elements): [0.0, 0.0, 0.06958962976932526, 0.0, 0.0, -0.06958962976932526, 0.0, 0.0, 0.0, -0.06958962976932526, 0.06958962976932526, 0.0, 0.0, 0.0, 0.0, 0.06958962976932526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-12 16:39:32,452]: Mean: 0.00058331
[2025-05-12 16:39:32,452]: Min: -0.06958963
[2025-05-12 16:39:32,452]: Max: 0.13917926
[2025-05-12 16:39:32,452]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,453]: Sample Values (25 elements): [1.0711628198623657, 1.1212502717971802, 1.1743179559707642, 1.209953784942627, 1.0974072217941284, 1.108823537826538, 1.1498550176620483, 1.121781826019287, 1.1133849620819092, 1.1488208770751953, 1.109824776649475, 1.1460384130477905, 1.1350929737091064, 1.1857028007507324, 1.125869870185852, 1.1710563898086548, 1.147998571395874, 1.1242032051086426, 1.0862295627593994, 1.0868054628372192, 1.1600561141967773, 1.151188850402832, 1.0978931188583374, 1.1278282403945923, 1.0902574062347412]
[2025-05-12 16:39:32,454]: Mean: 1.11808801
[2025-05-12 16:39:32,454]: Min: 1.05916655
[2025-05-12 16:39:32,455]: Max: 1.20995378
[2025-05-12 16:39:32,457]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:39:32,457]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.06858569383621216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06858569383621216, 0.0, 0.0, -0.06858569383621216, 0.0, 0.0, -0.06858569383621216, 0.0, 0.06858569383621216, 0.0]
[2025-05-12 16:39:32,458]: Mean: -0.00196283
[2025-05-12 16:39:32,459]: Min: -0.06858569
[2025-05-12 16:39:32,460]: Max: 0.13717139
[2025-05-12 16:39:32,460]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,461]: Sample Values (25 elements): [0.9855369925498962, 0.9837647080421448, 0.9848775267601013, 0.9936160445213318, 1.0329089164733887, 1.0017879009246826, 1.0036540031433105, 0.9884785413742065, 1.0057380199432373, 1.013918399810791, 1.0095555782318115, 0.9984437823295593, 1.0061802864074707, 0.9703155159950256, 0.9890365600585938, 1.0012274980545044, 0.987073540687561, 0.9758842587471008, 0.9786335825920105, 0.933627188205719, 0.9831069707870483, 0.9998798966407776, 1.0261900424957275, 0.9965307712554932, 0.9839960932731628]
[2025-05-12 16:39:32,461]: Mean: 0.99215472
[2025-05-12 16:39:32,461]: Min: 0.93362719
[2025-05-12 16:39:32,462]: Max: 1.10462785
[2025-05-12 16:39:32,466]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-12 16:39:32,468]: Sample Values (25 elements): [0.0, 0.0, 0.0558452308177948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0558452308177948, 0.0, 0.0, -0.0558452308177948, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0558452308177948, -0.0558452308177948, 0.0, -0.0558452308177948, -0.0558452308177948, 0.0558452308177948, 0.0, 0.0]
[2025-05-12 16:39:32,468]: Mean: -0.00173001
[2025-05-12 16:39:32,469]: Min: -0.05584523
[2025-05-12 16:39:32,470]: Max: 0.05584523
[2025-05-12 16:39:32,470]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-12 16:39:32,471]: Sample Values (25 elements): [1.2086137533187866, 1.2000364065170288, 1.111828088760376, 1.1296511888504028, 1.1798807382583618, 1.0995644330978394, 1.1121832132339478, 1.1810698509216309, 1.1443268060684204, 1.174504041671753, 1.1395879983901978, 1.1892038583755493, 1.1446949243545532, 1.124741792678833, 1.1148258447647095, 1.1836106777191162, 1.1027119159698486, 1.1537158489227295, 1.1564772129058838, 1.164901614189148, 1.2248659133911133, 1.173049807548523, 1.1302719116210938, 1.1740968227386475, 1.1271674633026123]
[2025-05-12 16:39:32,472]: Mean: 1.16330290
[2025-05-12 16:39:32,472]: Min: 1.08066356
[2025-05-12 16:39:32,473]: Max: 1.25466847
[2025-05-12 16:39:32,473]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-12 16:39:32,473]: Sample Values (25 elements): [0.20231015980243683, -0.17265938222408295, -0.16528940200805664, 0.06745663285255432, -0.06645269691944122, -0.08605863898992538, -0.2188064008951187, -0.049948800355196, -0.10127175599336624, -0.08357521146535873, -0.07338011264801025, 0.04008689522743225, -0.1630963832139969, 0.19686134159564972, 0.17105518281459808, -0.21092483401298523, -0.2566099762916565, 0.2209668904542923, -0.0026787109673023224, -0.09105122089385986, 0.0068750022910535336, -0.09613741934299469, 0.09956270456314087, -0.22837573289871216, -0.22542940080165863]
[2025-05-12 16:39:32,474]: Mean: -0.00645841
[2025-05-12 16:39:32,474]: Min: -0.32263592
[2025-05-12 16:39:32,474]: Max: 0.50749373
