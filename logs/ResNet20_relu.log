[2025-05-26 15:25:52,208]: 
Training ResNet20 with relu
[2025-05-26 15:26:35,874]: [ResNet20_relu] Epoch: 001 Train Loss: 1.4813 Train Acc: 0.4531 Eval Loss: 1.2484 Eval Acc: 0.5475 (LR: 0.00100000)
[2025-05-26 15:27:37,345]: [ResNet20_relu] Epoch: 002 Train Loss: 1.1089 Train Acc: 0.6015 Eval Loss: 1.0866 Eval Acc: 0.6178 (LR: 0.00100000)
[2025-05-26 15:28:23,493]: [ResNet20_relu] Epoch: 003 Train Loss: 0.9511 Train Acc: 0.6616 Eval Loss: 0.9259 Eval Acc: 0.6736 (LR: 0.00100000)
[2025-05-26 15:29:24,375]: [ResNet20_relu] Epoch: 004 Train Loss: 0.8407 Train Acc: 0.7042 Eval Loss: 0.7991 Eval Acc: 0.7235 (LR: 0.00100000)
[2025-05-26 15:30:11,936]: [ResNet20_relu] Epoch: 005 Train Loss: 0.7530 Train Acc: 0.7374 Eval Loss: 0.8791 Eval Acc: 0.7139 (LR: 0.00100000)
[2025-05-26 15:31:10,997]: [ResNet20_relu] Epoch: 006 Train Loss: 0.7009 Train Acc: 0.7544 Eval Loss: 0.7589 Eval Acc: 0.7452 (LR: 0.00100000)
[2025-05-26 15:31:59,277]: [ResNet20_relu] Epoch: 007 Train Loss: 0.6504 Train Acc: 0.7740 Eval Loss: 0.6451 Eval Acc: 0.7782 (LR: 0.00100000)
[2025-05-26 15:32:58,513]: [ResNet20_relu] Epoch: 008 Train Loss: 0.6161 Train Acc: 0.7851 Eval Loss: 0.7516 Eval Acc: 0.7689 (LR: 0.00100000)
[2025-05-26 15:33:48,584]: [ResNet20_relu] Epoch: 009 Train Loss: 0.5907 Train Acc: 0.7934 Eval Loss: 0.7112 Eval Acc: 0.7690 (LR: 0.00100000)
[2025-05-26 15:34:47,839]: [ResNet20_relu] Epoch: 010 Train Loss: 0.5613 Train Acc: 0.8050 Eval Loss: 0.5720 Eval Acc: 0.8044 (LR: 0.00100000)
[2025-05-26 15:35:43,467]: [ResNet20_relu] Epoch: 011 Train Loss: 0.5481 Train Acc: 0.8093 Eval Loss: 0.6018 Eval Acc: 0.7944 (LR: 0.00100000)
[2025-05-26 15:36:39,947]: [ResNet20_relu] Epoch: 012 Train Loss: 0.5240 Train Acc: 0.8183 Eval Loss: 0.5323 Eval Acc: 0.8165 (LR: 0.00100000)
[2025-05-26 15:37:41,736]: [ResNet20_relu] Epoch: 013 Train Loss: 0.5093 Train Acc: 0.8249 Eval Loss: 0.5375 Eval Acc: 0.8225 (LR: 0.00100000)
[2025-05-26 15:38:30,505]: [ResNet20_relu] Epoch: 014 Train Loss: 0.4894 Train Acc: 0.8302 Eval Loss: 0.4814 Eval Acc: 0.8366 (LR: 0.00100000)
[2025-05-26 15:39:31,881]: [ResNet20_relu] Epoch: 015 Train Loss: 0.4764 Train Acc: 0.8346 Eval Loss: 0.4623 Eval Acc: 0.8423 (LR: 0.00100000)
[2025-05-26 15:40:19,409]: [ResNet20_relu] Epoch: 016 Train Loss: 0.4570 Train Acc: 0.8435 Eval Loss: 0.5181 Eval Acc: 0.8315 (LR: 0.00100000)
[2025-05-26 15:41:20,647]: [ResNet20_relu] Epoch: 017 Train Loss: 0.4510 Train Acc: 0.8438 Eval Loss: 0.5686 Eval Acc: 0.8154 (LR: 0.00100000)
[2025-05-26 15:42:08,526]: [ResNet20_relu] Epoch: 018 Train Loss: 0.4411 Train Acc: 0.8490 Eval Loss: 0.4953 Eval Acc: 0.8363 (LR: 0.00100000)
[2025-05-26 15:43:09,886]: [ResNet20_relu] Epoch: 019 Train Loss: 0.4332 Train Acc: 0.8496 Eval Loss: 0.4859 Eval Acc: 0.8403 (LR: 0.00100000)
[2025-05-26 15:43:55,277]: [ResNet20_relu] Epoch: 020 Train Loss: 0.4176 Train Acc: 0.8542 Eval Loss: 0.4571 Eval Acc: 0.8488 (LR: 0.00100000)
[2025-05-26 15:44:56,813]: [ResNet20_relu] Epoch: 021 Train Loss: 0.4119 Train Acc: 0.8569 Eval Loss: 0.4591 Eval Acc: 0.8483 (LR: 0.00100000)
[2025-05-26 15:45:39,492]: [ResNet20_relu] Epoch: 022 Train Loss: 0.4012 Train Acc: 0.8616 Eval Loss: 0.4796 Eval Acc: 0.8382 (LR: 0.00100000)
[2025-05-26 15:46:40,871]: [ResNet20_relu] Epoch: 023 Train Loss: 0.3947 Train Acc: 0.8627 Eval Loss: 0.4547 Eval Acc: 0.8466 (LR: 0.00100000)
[2025-05-26 15:47:22,219]: [ResNet20_relu] Epoch: 024 Train Loss: 0.3869 Train Acc: 0.8639 Eval Loss: 0.4360 Eval Acc: 0.8528 (LR: 0.00100000)
[2025-05-26 15:48:23,440]: [ResNet20_relu] Epoch: 025 Train Loss: 0.3799 Train Acc: 0.8694 Eval Loss: 0.3998 Eval Acc: 0.8639 (LR: 0.00100000)
[2025-05-26 15:49:04,191]: [ResNet20_relu] Epoch: 026 Train Loss: 0.3721 Train Acc: 0.8717 Eval Loss: 0.4627 Eval Acc: 0.8446 (LR: 0.00100000)
[2025-05-26 15:50:05,630]: [ResNet20_relu] Epoch: 027 Train Loss: 0.3631 Train Acc: 0.8733 Eval Loss: 0.4853 Eval Acc: 0.8382 (LR: 0.00100000)
[2025-05-26 15:50:47,819]: [ResNet20_relu] Epoch: 028 Train Loss: 0.3639 Train Acc: 0.8736 Eval Loss: 0.4438 Eval Acc: 0.8531 (LR: 0.00100000)
[2025-05-26 15:51:49,197]: [ResNet20_relu] Epoch: 029 Train Loss: 0.3525 Train Acc: 0.8765 Eval Loss: 0.4387 Eval Acc: 0.8565 (LR: 0.00100000)
[2025-05-26 15:52:30,140]: [ResNet20_relu] Epoch: 030 Train Loss: 0.3463 Train Acc: 0.8790 Eval Loss: 0.3948 Eval Acc: 0.8685 (LR: 0.00100000)
[2025-05-26 15:53:31,576]: [ResNet20_relu] Epoch: 031 Train Loss: 0.3414 Train Acc: 0.8801 Eval Loss: 0.4235 Eval Acc: 0.8574 (LR: 0.00100000)
[2025-05-26 15:54:11,817]: [ResNet20_relu] Epoch: 032 Train Loss: 0.3407 Train Acc: 0.8803 Eval Loss: 0.4245 Eval Acc: 0.8584 (LR: 0.00100000)
[2025-05-26 15:55:12,972]: [ResNet20_relu] Epoch: 033 Train Loss: 0.3372 Train Acc: 0.8825 Eval Loss: 0.3884 Eval Acc: 0.8700 (LR: 0.00100000)
[2025-05-26 15:55:53,056]: [ResNet20_relu] Epoch: 034 Train Loss: 0.3294 Train Acc: 0.8858 Eval Loss: 0.4088 Eval Acc: 0.8657 (LR: 0.00100000)
[2025-05-26 15:56:54,429]: [ResNet20_relu] Epoch: 035 Train Loss: 0.3307 Train Acc: 0.8843 Eval Loss: 0.4675 Eval Acc: 0.8516 (LR: 0.00100000)
[2025-05-26 15:57:34,089]: [ResNet20_relu] Epoch: 036 Train Loss: 0.3216 Train Acc: 0.8883 Eval Loss: 0.3837 Eval Acc: 0.8736 (LR: 0.00100000)
[2025-05-26 15:58:35,436]: [ResNet20_relu] Epoch: 037 Train Loss: 0.3149 Train Acc: 0.8897 Eval Loss: 0.4006 Eval Acc: 0.8685 (LR: 0.00100000)
[2025-05-26 15:59:15,124]: [ResNet20_relu] Epoch: 038 Train Loss: 0.3148 Train Acc: 0.8907 Eval Loss: 0.3991 Eval Acc: 0.8736 (LR: 0.00100000)
[2025-05-26 16:00:16,365]: [ResNet20_relu] Epoch: 039 Train Loss: 0.3126 Train Acc: 0.8908 Eval Loss: 0.4196 Eval Acc: 0.8630 (LR: 0.00100000)
[2025-05-26 16:00:56,556]: [ResNet20_relu] Epoch: 040 Train Loss: 0.3094 Train Acc: 0.8919 Eval Loss: 0.3848 Eval Acc: 0.8748 (LR: 0.00100000)
[2025-05-26 16:01:58,047]: [ResNet20_relu] Epoch: 041 Train Loss: 0.3040 Train Acc: 0.8936 Eval Loss: 0.4723 Eval Acc: 0.8502 (LR: 0.00100000)
[2025-05-26 16:02:37,458]: [ResNet20_relu] Epoch: 042 Train Loss: 0.2974 Train Acc: 0.8960 Eval Loss: 0.3709 Eval Acc: 0.8767 (LR: 0.00100000)
[2025-05-26 16:03:39,019]: [ResNet20_relu] Epoch: 043 Train Loss: 0.2962 Train Acc: 0.8971 Eval Loss: 0.3893 Eval Acc: 0.8725 (LR: 0.00100000)
[2025-05-26 16:04:18,405]: [ResNet20_relu] Epoch: 044 Train Loss: 0.2954 Train Acc: 0.8966 Eval Loss: 0.3813 Eval Acc: 0.8788 (LR: 0.00100000)
[2025-05-26 16:05:19,814]: [ResNet20_relu] Epoch: 045 Train Loss: 0.2949 Train Acc: 0.8957 Eval Loss: 0.4388 Eval Acc: 0.8580 (LR: 0.00100000)
[2025-05-26 16:05:59,218]: [ResNet20_relu] Epoch: 046 Train Loss: 0.2862 Train Acc: 0.9011 Eval Loss: 0.3812 Eval Acc: 0.8769 (LR: 0.00100000)
[2025-05-26 16:07:00,627]: [ResNet20_relu] Epoch: 047 Train Loss: 0.2822 Train Acc: 0.9002 Eval Loss: 0.3805 Eval Acc: 0.8775 (LR: 0.00100000)
[2025-05-26 16:07:39,868]: [ResNet20_relu] Epoch: 048 Train Loss: 0.2831 Train Acc: 0.9019 Eval Loss: 0.3755 Eval Acc: 0.8744 (LR: 0.00010000)
[2025-05-26 16:08:41,255]: [ResNet20_relu] Epoch: 049 Train Loss: 0.2265 Train Acc: 0.9206 Eval Loss: 0.2998 Eval Acc: 0.8999 (LR: 0.00010000)
[2025-05-26 16:09:20,600]: [ResNet20_relu] Epoch: 050 Train Loss: 0.2059 Train Acc: 0.9286 Eval Loss: 0.2990 Eval Acc: 0.9022 (LR: 0.00010000)
[2025-05-26 16:10:21,810]: [ResNet20_relu] Epoch: 051 Train Loss: 0.1965 Train Acc: 0.9313 Eval Loss: 0.2997 Eval Acc: 0.9013 (LR: 0.00010000)
[2025-05-26 16:11:01,270]: [ResNet20_relu] Epoch: 052 Train Loss: 0.1892 Train Acc: 0.9351 Eval Loss: 0.2965 Eval Acc: 0.9025 (LR: 0.00010000)
[2025-05-26 16:12:02,646]: [ResNet20_relu] Epoch: 053 Train Loss: 0.1870 Train Acc: 0.9353 Eval Loss: 0.2994 Eval Acc: 0.9030 (LR: 0.00010000)
[2025-05-26 16:12:41,889]: [ResNet20_relu] Epoch: 054 Train Loss: 0.1847 Train Acc: 0.9355 Eval Loss: 0.2992 Eval Acc: 0.9035 (LR: 0.00010000)
[2025-05-26 16:13:43,302]: [ResNet20_relu] Epoch: 055 Train Loss: 0.1808 Train Acc: 0.9365 Eval Loss: 0.2989 Eval Acc: 0.9048 (LR: 0.00010000)
[2025-05-26 16:14:22,510]: [ResNet20_relu] Epoch: 056 Train Loss: 0.1742 Train Acc: 0.9399 Eval Loss: 0.2962 Eval Acc: 0.9041 (LR: 0.00010000)
[2025-05-26 16:15:23,874]: [ResNet20_relu] Epoch: 057 Train Loss: 0.1748 Train Acc: 0.9392 Eval Loss: 0.3003 Eval Acc: 0.9049 (LR: 0.00010000)
[2025-05-26 16:16:03,153]: [ResNet20_relu] Epoch: 058 Train Loss: 0.1712 Train Acc: 0.9401 Eval Loss: 0.3008 Eval Acc: 0.9035 (LR: 0.00010000)
[2025-05-26 16:17:04,330]: [ResNet20_relu] Epoch: 059 Train Loss: 0.1687 Train Acc: 0.9409 Eval Loss: 0.3039 Eval Acc: 0.9044 (LR: 0.00010000)
[2025-05-26 16:17:43,973]: [ResNet20_relu] Epoch: 060 Train Loss: 0.1706 Train Acc: 0.9404 Eval Loss: 0.2984 Eval Acc: 0.9056 (LR: 0.00010000)
[2025-05-26 16:18:45,490]: [ResNet20_relu] Epoch: 061 Train Loss: 0.1670 Train Acc: 0.9419 Eval Loss: 0.3016 Eval Acc: 0.9041 (LR: 0.00010000)
[2025-05-26 16:19:25,011]: [ResNet20_relu] Epoch: 062 Train Loss: 0.1667 Train Acc: 0.9423 Eval Loss: 0.3061 Eval Acc: 0.9033 (LR: 0.00001000)
[2025-05-26 16:20:26,758]: [ResNet20_relu] Epoch: 063 Train Loss: 0.1612 Train Acc: 0.9435 Eval Loss: 0.3026 Eval Acc: 0.9055 (LR: 0.00001000)
[2025-05-26 16:21:06,453]: [ResNet20_relu] Epoch: 064 Train Loss: 0.1578 Train Acc: 0.9449 Eval Loss: 0.3021 Eval Acc: 0.9038 (LR: 0.00001000)
[2025-05-26 16:22:07,895]: [ResNet20_relu] Epoch: 065 Train Loss: 0.1557 Train Acc: 0.9457 Eval Loss: 0.2999 Eval Acc: 0.9049 (LR: 0.00001000)
[2025-05-26 16:22:47,583]: [ResNet20_relu] Epoch: 066 Train Loss: 0.1597 Train Acc: 0.9447 Eval Loss: 0.2995 Eval Acc: 0.9068 (LR: 0.00001000)
[2025-05-26 16:23:49,146]: [ResNet20_relu] Epoch: 067 Train Loss: 0.1552 Train Acc: 0.9467 Eval Loss: 0.2993 Eval Acc: 0.9056 (LR: 0.00001000)
[2025-05-26 16:24:29,495]: [ResNet20_relu] Epoch: 068 Train Loss: 0.1580 Train Acc: 0.9449 Eval Loss: 0.2990 Eval Acc: 0.9055 (LR: 0.00000100)
[2025-05-26 16:25:30,844]: [ResNet20_relu] Epoch: 069 Train Loss: 0.1551 Train Acc: 0.9459 Eval Loss: 0.2991 Eval Acc: 0.9062 (LR: 0.00000100)
[2025-05-26 16:26:12,085]: [ResNet20_relu] Epoch: 070 Train Loss: 0.1548 Train Acc: 0.9462 Eval Loss: 0.2994 Eval Acc: 0.9065 (LR: 0.00000100)
[2025-05-26 16:27:13,486]: [ResNet20_relu] Epoch: 071 Train Loss: 0.1546 Train Acc: 0.9459 Eval Loss: 0.2960 Eval Acc: 0.9061 (LR: 0.00000100)
[2025-05-26 16:27:56,103]: [ResNet20_relu] Epoch: 072 Train Loss: 0.1532 Train Acc: 0.9472 Eval Loss: 0.2974 Eval Acc: 0.9063 (LR: 0.00000100)
[2025-05-26 16:28:57,595]: [ResNet20_relu] Epoch: 073 Train Loss: 0.1541 Train Acc: 0.9466 Eval Loss: 0.2984 Eval Acc: 0.9054 (LR: 0.00000100)
[2025-05-26 16:29:42,790]: [ResNet20_relu] Epoch: 074 Train Loss: 0.1558 Train Acc: 0.9473 Eval Loss: 0.3042 Eval Acc: 0.9054 (LR: 0.00000100)
[2025-05-26 16:30:44,188]: [ResNet20_relu] Epoch: 075 Train Loss: 0.1545 Train Acc: 0.9472 Eval Loss: 0.3012 Eval Acc: 0.9057 (LR: 0.00000100)
[2025-05-26 16:31:31,758]: [ResNet20_relu] Epoch: 076 Train Loss: 0.1536 Train Acc: 0.9473 Eval Loss: 0.3013 Eval Acc: 0.9069 (LR: 0.00000100)
[2025-05-26 16:32:30,863]: [ResNet20_relu] Epoch: 077 Train Loss: 0.1539 Train Acc: 0.9459 Eval Loss: 0.2976 Eval Acc: 0.9064 (LR: 0.00000010)
[2025-05-26 16:33:18,639]: [ResNet20_relu] Epoch: 078 Train Loss: 0.1547 Train Acc: 0.9454 Eval Loss: 0.2975 Eval Acc: 0.9057 (LR: 0.00000010)
[2025-05-26 16:34:17,728]: [ResNet20_relu] Epoch: 079 Train Loss: 0.1529 Train Acc: 0.9472 Eval Loss: 0.2986 Eval Acc: 0.9059 (LR: 0.00000010)
[2025-05-26 16:35:06,166]: [ResNet20_relu] Epoch: 080 Train Loss: 0.1509 Train Acc: 0.9474 Eval Loss: 0.3002 Eval Acc: 0.9074 (LR: 0.00000010)
[2025-05-26 16:36:05,670]: [ResNet20_relu] Epoch: 081 Train Loss: 0.1548 Train Acc: 0.9463 Eval Loss: 0.2992 Eval Acc: 0.9060 (LR: 0.00000010)
[2025-05-26 16:36:58,628]: [ResNet20_relu] Epoch: 082 Train Loss: 0.1542 Train Acc: 0.9465 Eval Loss: 0.2998 Eval Acc: 0.9062 (LR: 0.00000010)
[2025-05-26 16:37:58,469]: [ResNet20_relu] Epoch: 083 Train Loss: 0.1542 Train Acc: 0.9464 Eval Loss: 0.2968 Eval Acc: 0.9063 (LR: 0.00000010)
[2025-05-26 16:38:58,478]: [ResNet20_relu] Epoch: 084 Train Loss: 0.1549 Train Acc: 0.9464 Eval Loss: 0.3010 Eval Acc: 0.9067 (LR: 0.00000010)
[2025-05-26 16:39:49,136]: [ResNet20_relu] Epoch: 085 Train Loss: 0.1546 Train Acc: 0.9460 Eval Loss: 0.3000 Eval Acc: 0.9059 (LR: 0.00000010)
[2025-05-26 16:40:50,531]: [ResNet20_relu] Epoch: 086 Train Loss: 0.1519 Train Acc: 0.9485 Eval Loss: 0.2995 Eval Acc: 0.9063 (LR: 0.00000010)
[2025-05-26 16:41:38,847]: [ResNet20_relu] Epoch: 087 Train Loss: 0.1558 Train Acc: 0.9460 Eval Loss: 0.3010 Eval Acc: 0.9058 (LR: 0.00000010)
[2025-05-26 16:42:39,946]: [ResNet20_relu] Epoch: 088 Train Loss: 0.1530 Train Acc: 0.9466 Eval Loss: 0.2984 Eval Acc: 0.9059 (LR: 0.00000010)
[2025-05-26 16:43:27,507]: [ResNet20_relu] Epoch: 089 Train Loss: 0.1514 Train Acc: 0.9480 Eval Loss: 0.2998 Eval Acc: 0.9059 (LR: 0.00000010)
[2025-05-26 16:44:28,685]: [ResNet20_relu] Epoch: 090 Train Loss: 0.1518 Train Acc: 0.9470 Eval Loss: 0.3009 Eval Acc: 0.9068 (LR: 0.00000010)
[2025-05-26 16:45:16,329]: [ResNet20_relu] Epoch: 091 Train Loss: 0.1572 Train Acc: 0.9457 Eval Loss: 0.3011 Eval Acc: 0.9060 (LR: 0.00000010)
[2025-05-26 16:46:17,618]: [ResNet20_relu] Epoch: 092 Train Loss: 0.1548 Train Acc: 0.9462 Eval Loss: 0.3015 Eval Acc: 0.9052 (LR: 0.00000010)
[2025-05-26 16:47:01,723]: [ResNet20_relu] Epoch: 093 Train Loss: 0.1555 Train Acc: 0.9466 Eval Loss: 0.2966 Eval Acc: 0.9053 (LR: 0.00000010)
[2025-05-26 16:48:03,000]: [ResNet20_relu] Epoch: 094 Train Loss: 0.1519 Train Acc: 0.9471 Eval Loss: 0.3001 Eval Acc: 0.9054 (LR: 0.00000010)
[2025-05-26 16:48:45,323]: [ResNet20_relu] Epoch: 095 Train Loss: 0.1538 Train Acc: 0.9463 Eval Loss: 0.3006 Eval Acc: 0.9052 (LR: 0.00000010)
[2025-05-26 16:49:46,693]: [ResNet20_relu] Epoch: 096 Train Loss: 0.1528 Train Acc: 0.9465 Eval Loss: 0.3007 Eval Acc: 0.9068 (LR: 0.00000010)
[2025-05-26 16:50:28,019]: [ResNet20_relu] Epoch: 097 Train Loss: 0.1544 Train Acc: 0.9463 Eval Loss: 0.2985 Eval Acc: 0.9074 (LR: 0.00000010)
[2025-05-26 16:51:29,514]: [ResNet20_relu] Epoch: 098 Train Loss: 0.1527 Train Acc: 0.9473 Eval Loss: 0.3003 Eval Acc: 0.9063 (LR: 0.00000010)
[2025-05-26 16:52:09,600]: [ResNet20_relu] Epoch: 099 Train Loss: 0.1557 Train Acc: 0.9463 Eval Loss: 0.2994 Eval Acc: 0.9065 (LR: 0.00000010)
[2025-05-26 16:53:11,256]: [ResNet20_relu] Epoch: 100 Train Loss: 0.1508 Train Acc: 0.9471 Eval Loss: 0.2998 Eval Acc: 0.9061 (LR: 0.00000010)
[2025-05-26 16:53:11,267]: Early stopping was triggered!
[2025-05-26 16:53:11,267]: [ResNet20_relu] Best Eval Accuracy: 0.9074
[2025-05-26 16:53:11,290]: 
Training of full-precision model finished!
[2025-05-26 16:53:11,290]: Model Architecture:
[2025-05-26 16:53:11,291]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-26 16:53:11,291]: 
Model Weights:
[2025-05-26 16:53:11,291]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-26 16:53:11,305]: Sample Values (25 elements): [-0.06944714486598969, 0.01693590357899666, 0.012442945502698421, -0.23484785854816437, 0.20252196490764618, -0.3223825991153717, -0.052084699273109436, -0.01920592598617077, 0.004430427681654692, -0.044035837054252625, -0.005793697200715542, -0.03316017612814903, -0.3932247757911682, 0.19542467594146729, -0.2012404352426529, -0.2141382098197937, -0.02374950423836708, -0.21705323457717896, -0.0314396433532238, 0.03005351312458515, 0.08901578187942505, 0.2188182920217514, -0.2102162390947342, -0.14879074692726135, 0.1558685600757599]
[2025-05-26 16:53:11,323]: Mean: -0.00177801
[2025-05-26 16:53:11,337]: Min: -0.57571203
[2025-05-26 16:53:11,349]: Max: 0.58451843
[2025-05-26 16:53:11,349]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,373]: Sample Values (16 elements): [0.9460241794586182, 1.0902910232543945, 0.9487730860710144, 0.647705614566803, 0.8931306004524231, 0.817290723323822, 0.82699054479599, 0.880601167678833, 0.6483545899391174, 0.7867387533187866, 0.8722672462463379, 1.051156997680664, 0.7469402551651001, 0.9022521376609802, 1.0815681219100952, 0.7487413883209229]
[2025-05-26 16:53:11,383]: Mean: 0.86805165
[2025-05-26 16:53:11,386]: Min: 0.64770561
[2025-05-26 16:53:11,386]: Max: 1.09029102
[2025-05-26 16:53:11,386]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 16:53:11,386]: Sample Values (25 elements): [0.13861630856990814, -0.1459234058856964, -0.03224564343690872, 0.10038119554519653, -0.015287197194993496, -0.08479459583759308, 0.097347691655159, -0.002846639370545745, -0.013887189328670502, 0.012767543084919453, 0.017912037670612335, 0.14453132450580597, 0.18515489995479584, -0.04289975017309189, -0.1487862467765808, -0.03339209035038948, 0.05526698753237724, -0.007043728604912758, -0.02939729392528534, 0.07184689491987228, -0.06641659140586853, 0.22428761422634125, 0.1608692854642868, -0.04011990502476692, 0.08260270953178406]
[2025-05-26 16:53:11,387]: Mean: -0.00498166
[2025-05-26 16:53:11,387]: Min: -0.45633760
[2025-05-26 16:53:11,387]: Max: 0.41873100
[2025-05-26 16:53:11,387]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,387]: Sample Values (16 elements): [0.7459178566932678, 0.9617465138435364, 0.9220549464225769, 0.9535410404205322, 0.6993470788002014, 0.8161779642105103, 0.7764644026756287, 0.8782574534416199, 0.6965503096580505, 0.74235999584198, 0.8056981563568115, 0.7163147926330566, 1.0149229764938354, 0.850075900554657, 0.8721858263015747, 0.8277813792228699]
[2025-05-26 16:53:11,395]: Mean: 0.82996225
[2025-05-26 16:53:11,406]: Min: 0.69655031
[2025-05-26 16:53:11,416]: Max: 1.01492298
[2025-05-26 16:53:11,416]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 16:53:11,435]: Sample Values (25 elements): [-0.0732649639248848, 0.004258514381945133, -0.09860289096832275, 0.019753167405724525, 0.15700235962867737, -0.02345147542655468, 0.04691650718450546, 0.20855523645877838, 0.013233473524451256, 0.24437789618968964, -0.1783992052078247, -0.12935248017311096, -0.0004549308796413243, 0.07624455541372299, 0.2601768374443054, 0.07713283598423004, -0.11953319609165192, -0.06058814749121666, -0.009290504269301891, 0.07870803028345108, -0.08486230671405792, -0.00150099687743932, 0.0519738644361496, 0.04824395850300789, 0.12374909222126007]
[2025-05-26 16:53:11,440]: Mean: -0.00169621
[2025-05-26 16:53:11,454]: Min: -0.47460803
[2025-05-26 16:53:11,461]: Max: 0.40775049
[2025-05-26 16:53:11,461]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,493]: Sample Values (16 elements): [0.8472059369087219, 0.7804713845252991, 0.8616793155670166, 0.8785893321037292, 0.6777443289756775, 0.7654775977134705, 0.7603170275688171, 0.695167601108551, 0.8397377133369446, 0.7644745707511902, 0.6517124176025391, 0.9309258460998535, 0.6881105303764343, 0.9784388542175293, 0.9988359212875366, 0.7127717733383179]
[2025-05-26 16:53:11,505]: Mean: 0.80197871
[2025-05-26 16:53:11,517]: Min: 0.65171242
[2025-05-26 16:53:11,528]: Max: 0.99883592
[2025-05-26 16:53:11,528]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 16:53:11,541]: Sample Values (25 elements): [0.16370169818401337, 0.19578400254249573, 0.13696183264255524, -0.020825183019042015, -0.011191858910024166, -0.17752711474895477, 0.07534755021333694, -0.21794432401657104, 0.24101239442825317, 0.004260378424078226, -0.05540063604712486, -0.06420291215181351, -0.06790059804916382, 0.20995517075061798, 0.22605600953102112, -0.1652389019727707, -0.011693603359162807, 0.02599412389099598, -0.18501661717891693, 0.03353926166892052, -0.013672268018126488, -0.025531604886054993, -0.41808757185935974, 0.051302459090948105, -0.0048002395778894424]
[2025-05-26 16:53:11,541]: Mean: -0.00424482
[2025-05-26 16:53:11,542]: Min: -0.53378308
[2025-05-26 16:53:11,542]: Max: 0.42405516
[2025-05-26 16:53:11,542]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,542]: Sample Values (16 elements): [0.7447129487991333, 0.8095925450325012, 0.8459277749061584, 0.6279643177986145, 0.7780296206474304, 0.8763099908828735, 0.783664882183075, 0.8106929659843445, 0.8974590301513672, 0.8211206793785095, 0.9069122076034546, 0.722558856010437, 0.9735313653945923, 0.846290111541748, 0.6901304721832275, 0.6786120533943176]
[2025-05-26 16:53:11,542]: Mean: 0.80084431
[2025-05-26 16:53:11,542]: Min: 0.62796432
[2025-05-26 16:53:11,543]: Max: 0.97353137
[2025-05-26 16:53:11,543]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 16:53:11,565]: Sample Values (25 elements): [-0.3543925881385803, -0.006224294658750296, -0.09613805264234543, -0.13119883835315704, 0.03184759244322777, 0.08072759211063385, -0.008935942314565182, -0.010895186103880405, 0.11123660206794739, 0.07691066712141037, -0.1267538070678711, -0.13240152597427368, -0.11667029559612274, 0.06771978735923767, 0.03672102838754654, -0.18882186710834503, -0.050019510090351105, -0.1842014044523239, 0.05017329752445221, 0.05446397140622139, 0.000715492176823318, -0.06405510753393173, 0.13981817662715912, 0.06739357858896255, -0.17014260590076447]
[2025-05-26 16:53:11,571]: Mean: -0.01398175
[2025-05-26 16:53:11,581]: Min: -0.41187143
[2025-05-26 16:53:11,590]: Max: 0.41352466
[2025-05-26 16:53:11,590]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,611]: Sample Values (16 elements): [0.8620423078536987, 0.6270956993103027, 0.67787104845047, 0.8758558630943298, 0.6322587132453918, 0.8831912875175476, 0.9228582978248596, 1.1441751718521118, 0.913115382194519, 0.841996967792511, 0.7739399671554565, 0.9293538928031921, 0.8003678917884827, 0.6182281374931335, 0.7399687170982361, 0.6993900537490845]
[2025-05-26 16:53:11,622]: Mean: 0.80885684
[2025-05-26 16:53:11,634]: Min: 0.61822814
[2025-05-26 16:53:11,648]: Max: 1.14417517
[2025-05-26 16:53:11,648]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 16:53:11,672]: Sample Values (25 elements): [0.12672671675682068, -0.12955455482006073, -0.03351099044084549, 0.23936311900615692, 0.14957135915756226, 0.032808568328619, -0.3080935776233673, 0.0269828662276268, -0.004885889124125242, 0.009878894314169884, 0.04662502929568291, 0.004382006358355284, -0.056480374187231064, 0.0839230939745903, -0.16854070127010345, 0.08428841829299927, 0.03429914638400078, 0.09680414944887161, 0.2978598177433014, 0.014165384694933891, -0.01363650243729353, -0.039773520082235336, -0.14731505513191223, 0.3654027283191681, -0.039751727133989334]
[2025-05-26 16:53:11,683]: Mean: -0.00859109
[2025-05-26 16:53:11,694]: Min: -0.66520172
[2025-05-26 16:53:11,696]: Max: 0.42141110
[2025-05-26 16:53:11,696]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,697]: Sample Values (16 elements): [0.7207762002944946, 0.7632079124450684, 0.6462885141372681, 1.0057532787322998, 0.6211404800415039, 0.8716825246810913, 0.5401238203048706, 0.8378646373748779, 0.9048200845718384, 0.8351160883903503, 1.0829689502716064, 0.6872288584709167, 0.804189920425415, 0.7976284027099609, 0.752657413482666, 0.9126542806625366]
[2025-05-26 16:53:11,697]: Mean: 0.79900634
[2025-05-26 16:53:11,697]: Min: 0.54012382
[2025-05-26 16:53:11,697]: Max: 1.08296895
[2025-05-26 16:53:11,697]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-26 16:53:11,698]: Sample Values (25 elements): [0.15174910426139832, -0.1046748012304306, 0.13228940963745117, -0.011564918793737888, -0.11359853297472, 0.057838499546051025, 0.14421042799949646, -0.10011269152164459, 0.1503930687904358, 0.039575960487127304, 0.1317225843667984, 0.08237399905920029, -0.25094330310821533, -0.013212908990681171, -0.08659971505403519, -0.05297258496284485, -0.15117523074150085, 0.03585031256079674, 0.3382071852684021, -0.12645837664604187, -0.04266631230711937, 0.09316375106573105, 0.11069812625646591, 0.11344534158706665, 0.07147049158811569]
[2025-05-26 16:53:11,698]: Mean: -0.00247157
[2025-05-26 16:53:11,705]: Min: -0.55931365
[2025-05-26 16:53:11,721]: Max: 0.38763449
[2025-05-26 16:53:11,721]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-26 16:53:11,741]: Sample Values (16 elements): [0.7515314221382141, 0.7680696845054626, 0.6488227248191833, 1.0817548036575317, 0.6645779609680176, 0.7578600645065308, 0.7202102541923523, 0.8682772517204285, 0.812370777130127, 0.4893953502178192, 0.6060091853141785, 0.693393886089325, 0.8377862572669983, 0.7787551283836365, 0.9537220597267151, 0.6507810354232788]
[2025-05-26 16:53:11,748]: Mean: 0.75520730
[2025-05-26 16:53:11,755]: Min: 0.48939535
[2025-05-26 16:53:11,766]: Max: 1.08175480
[2025-05-26 16:53:11,766]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-26 16:53:11,788]: Sample Values (25 elements): [0.07661689817905426, 0.011308395303785801, -0.035485174506902695, -0.26278963685035706, 0.1198875680565834, -0.09995722025632858, 0.03622779995203018, -0.3767147362232208, 0.006195142399519682, 0.00871744193136692, 0.10767523944377899, 0.02942577935755253, 0.02355971559882164, 0.07128452509641647, -0.061601076275110245, -0.11723547428846359, 0.12462332844734192, -0.037062570452690125, 0.018672918900847435, -0.10969753563404083, 0.07773986458778381, -0.18582771718502045, -0.0019529839046299458, -0.14168457686901093, 0.1050439327955246]
[2025-05-26 16:53:11,796]: Mean: -0.00399798
[2025-05-26 16:53:11,815]: Min: -0.46719182
[2025-05-26 16:53:11,827]: Max: 0.45064783
[2025-05-26 16:53:11,827]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-26 16:53:11,849]: Sample Values (25 elements): [0.8443235754966736, 0.9011112451553345, 0.801745593547821, 0.842883288860321, 0.7436912059783936, 0.8613316416740417, 0.656184732913971, 0.7570188641548157, 0.7671007513999939, 0.8092347979545593, 0.7998128533363342, 0.7837128639221191, 0.7752468585968018, 0.8435607552528381, 0.9134564995765686, 0.7511869668960571, 0.8111700415611267, 0.7383199334144592, 0.8756116032600403, 0.7332378625869751, 0.9179977178573608, 0.8552464842796326, 0.8468815088272095, 0.7875655889511108, 0.9279040098190308]
[2025-05-26 16:53:11,851]: Mean: 0.82265574
[2025-05-26 16:53:11,852]: Min: 0.65618473
[2025-05-26 16:53:11,852]: Max: 0.92790401
[2025-05-26 16:53:11,852]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 16:53:11,852]: Sample Values (25 elements): [0.06012611836194992, -0.1386764943599701, -0.16481144726276398, -0.045695919543504715, 0.1183539479970932, -0.035699017345905304, 0.0311045590788126, 0.014457251876592636, 0.23897789418697357, 0.08290960639715195, -0.058278888463974, -0.1595405489206314, -0.0936378538608551, 0.04857466742396355, -0.11527856439352036, 0.028018373996019363, -0.06632307916879654, 0.013041009195148945, 0.2001669853925705, 0.026521973311901093, -0.08367377519607544, 0.1286150962114334, -0.12657523155212402, -0.1218782588839531, 0.013488693162798882]
[2025-05-26 16:53:11,852]: Mean: -0.00567010
[2025-05-26 16:53:11,853]: Min: -0.66825604
[2025-05-26 16:53:11,853]: Max: 0.39378053
[2025-05-26 16:53:11,853]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-26 16:53:11,876]: Sample Values (25 elements): [0.7602068185806274, 0.985835611820221, 0.832670271396637, 0.7351142764091492, 0.8984037041664124, 0.9171175360679626, 0.9247773289680481, 0.8079590797424316, 0.923621654510498, 0.8434529900550842, 0.8155636787414551, 1.118540644645691, 0.8629985451698303, 0.7466905117034912, 0.6810113787651062, 0.8079708218574524, 0.8165906667709351, 0.8597387671470642, 0.8284758925437927, 0.9709491729736328, 0.9268723130226135, 0.8225439190864563, 0.9225975871086121, 0.8999439477920532, 1.0173875093460083]
[2025-05-26 16:53:11,887]: Mean: 0.88356256
[2025-05-26 16:53:11,896]: Min: 0.68101138
[2025-05-26 16:53:11,903]: Max: 1.11854064
[2025-05-26 16:53:11,903]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-26 16:53:11,921]: Sample Values (25 elements): [-0.058304086327552795, 0.0435079000890255, -0.07053783535957336, 0.15502628684043884, 0.08875135332345963, 0.16174322366714478, 0.16689056158065796, -0.06953567266464233, 0.08967962861061096, -0.124976746737957, -0.04491918906569481, 0.11883296817541122, 0.05675830692052841, -0.38997069001197815, 0.10227639973163605, 0.04303683340549469, 0.016909128054976463, -0.2590327262878418, 0.17101605236530304, -0.14378860592842102, -0.05530410632491112, -0.16524791717529297, -0.28748297691345215, -0.11636253446340561, 0.23599427938461304]
[2025-05-26 16:53:11,932]: Mean: -0.00364566
[2025-05-26 16:53:11,944]: Min: -0.48883379
[2025-05-26 16:53:11,958]: Max: 0.63699985
[2025-05-26 16:53:11,958]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-26 16:53:11,982]: Sample Values (25 elements): [0.7981453537940979, 0.7008032202720642, 0.6448080539703369, 0.6264615654945374, 0.6610890030860901, 0.6560819149017334, 0.7186558246612549, 0.5967638492584229, 0.6877283453941345, 0.46726563572883606, 0.672865629196167, 0.6565126776695251, 0.6389659643173218, 0.571175217628479, 0.7374787330627441, 0.833473265171051, 0.8600149750709534, 0.7775911688804626, 0.558518648147583, 0.5321992039680481, 0.9003522396087646, 0.5625523924827576, 0.6877959370613098, 0.8400024175643921, 0.6714797616004944]
[2025-05-26 16:53:11,994]: Mean: 0.69022131
[2025-05-26 16:53:12,004]: Min: 0.46726564
[2025-05-26 16:53:12,007]: Max: 0.98205429
[2025-05-26 16:53:12,007]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 16:53:12,007]: Sample Values (25 elements): [0.06824856996536255, -0.054819829761981964, 0.05415919050574303, 0.10089383274316788, 0.008380129002034664, 0.03110094740986824, 0.015182019211351871, -0.19329173862934113, -0.05580852925777435, -0.08781013637781143, -0.0845571979880333, -0.08806253224611282, 0.10465787351131439, -0.2456788569688797, 0.08045203238725662, -0.05096591264009476, 0.014024192467331886, 0.03606154024600983, -0.12271521985530853, 0.05464886128902435, 0.139396071434021, -0.036990828812122345, -0.1346651315689087, 0.10406123846769333, -0.02848484180867672]
[2025-05-26 16:53:12,007]: Mean: -0.00978051
[2025-05-26 16:53:12,008]: Min: -0.37350786
[2025-05-26 16:53:12,008]: Max: 0.45045120
[2025-05-26 16:53:12,008]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-26 16:53:12,008]: Sample Values (25 elements): [0.840735912322998, 0.7513941526412964, 0.8053902983665466, 0.7206823229789734, 0.8934112787246704, 0.8586472868919373, 0.762128472328186, 0.8172141909599304, 0.8362900614738464, 0.8795727491378784, 0.7659280896186829, 0.7304213643074036, 0.7642751932144165, 0.895588755607605, 0.724711537361145, 0.8174523711204529, 0.8503872156143188, 0.8631331324577332, 0.7141754627227783, 0.796227216720581, 0.7302740216255188, 0.8847280144691467, 0.7700735926628113, 0.8869671821594238, 0.689315915107727]
[2025-05-26 16:53:12,015]: Mean: 0.79251301
[2025-05-26 16:53:12,027]: Min: 0.68931592
[2025-05-26 16:53:12,036]: Max: 0.89558876
[2025-05-26 16:53:12,036]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 16:53:12,055]: Sample Values (25 elements): [-0.10961180180311203, -0.2550380229949951, -0.003637320827692747, -0.1284789890050888, -0.05873672664165497, -0.09223604202270508, 0.0007404748466797173, 0.16931451857089996, 0.09271684288978577, -0.05965740233659744, 0.07172853499650955, -0.09750057011842728, 0.0008370130672119558, 0.1322927623987198, -0.02571096271276474, 0.10903449356555939, 0.023522425442934036, 0.01137610524892807, -0.14343535900115967, -0.13045483827590942, -0.06903897970914841, 0.12319362163543701, 0.023246990516781807, 0.013753642328083515, -0.029882926493883133]
[2025-05-26 16:53:12,060]: Mean: -0.00583536
[2025-05-26 16:53:12,075]: Min: -0.35618222
[2025-05-26 16:53:12,081]: Max: 0.36016962
[2025-05-26 16:53:12,081]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-26 16:53:12,113]: Sample Values (25 elements): [0.9571712017059326, 0.8278999924659729, 0.6206609010696411, 0.9085280299186707, 0.6816739439964294, 0.7912167310714722, 0.7704409956932068, 1.0248769521713257, 0.7934874296188354, 0.6702910661697388, 0.7689835429191589, 0.8350542187690735, 0.5567546486854553, 0.6617209315299988, 0.842019259929657, 0.6220454573631287, 0.7625006437301636, 0.8959271311759949, 0.72831130027771, 0.8489130139350891, 0.6625744104385376, 1.150860071182251, 0.6782158613204956, 0.8506066799163818, 0.7849595546722412]
[2025-05-26 16:53:12,126]: Mean: 0.79617125
[2025-05-26 16:53:12,138]: Min: 0.55675465
[2025-05-26 16:53:12,149]: Max: 1.15086007
[2025-05-26 16:53:12,149]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 16:53:12,162]: Sample Values (25 elements): [0.22678665816783905, 0.03129781782627106, 0.05593399703502655, 0.1129811555147171, 0.19578468799591064, -0.11090227216482162, -0.018001612275838852, 0.015268794260919094, -0.09416160732507706, 0.007619454059749842, -0.09125443547964096, -0.060295190662145615, 0.014603105373680592, 0.027589237317442894, -0.022397268563508987, 0.041200190782547, 0.024131163954734802, 0.04574607312679291, 0.010666392743587494, -0.13820120692253113, -0.11751915514469147, 0.010662490501999855, 0.11299978941679001, -0.04654860869050026, 0.003093512263149023]
[2025-05-26 16:53:12,162]: Mean: -0.01388461
[2025-05-26 16:53:12,162]: Min: -0.40923324
[2025-05-26 16:53:12,163]: Max: 0.37162068
[2025-05-26 16:53:12,163]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-26 16:53:12,163]: Sample Values (25 elements): [0.820612370967865, 0.6190972328186035, 0.7527449727058411, 0.7300381064414978, 0.8704968690872192, 0.6379262804985046, 0.7706120610237122, 0.9360001087188721, 0.7127590179443359, 0.6873871088027954, 0.6216400265693665, 0.6973882913589478, 0.8145365118980408, 0.7177590131759644, 0.7782217860221863, 0.7017971277236938, 0.8441545367240906, 0.8590484857559204, 0.7076143622398376, 0.7064675092697144, 0.7546895146369934, 0.8683516979217529, 0.593264639377594, 0.7663225531578064, 0.5666245818138123]
[2025-05-26 16:53:12,163]: Mean: 0.75324911
[2025-05-26 16:53:12,163]: Min: 0.56662458
[2025-05-26 16:53:12,171]: Max: 0.93600011
[2025-05-26 16:53:12,171]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-26 16:53:12,192]: Sample Values (25 elements): [-0.08935567736625671, -0.12619420886039734, 0.07872703671455383, 0.15410515666007996, 0.16513395309448242, -0.018047930672764778, 0.09387100487947464, 0.028678057715296745, -0.026851492002606392, 0.004754767287522554, 0.07735073566436768, -0.058783579617738724, -0.030957821756601334, -0.07697146385908127, 0.033282358199357986, -0.16683438420295715, 0.11930907517671585, -0.18511377274990082, -0.02728198654949665, -0.007784117944538593, -0.059157051146030426, -0.026233233511447906, -0.13355939090251923, 0.007613527588546276, -0.25247395038604736]
[2025-05-26 16:53:12,201]: Mean: -0.00355304
[2025-05-26 16:53:12,211]: Min: -0.38490656
[2025-05-26 16:53:12,216]: Max: 0.34348264
[2025-05-26 16:53:12,216]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-26 16:53:12,242]: Sample Values (25 elements): [0.7750083208084106, 0.8781557679176331, 0.7393203973770142, 0.7551126480102539, 0.7546843886375427, 0.9018710255622864, 0.7294759154319763, 0.7390684485435486, 0.6763002872467041, 0.8496683239936829, 0.7104405164718628, 0.8014360666275024, 0.6946730017662048, 0.7355484366416931, 0.6993733048439026, 0.7064106464385986, 0.7068424224853516, 0.780725359916687, 0.6735877394676208, 0.9753650426864624, 0.6907347440719604, 0.6481263041496277, 1.0347964763641357, 0.6834702491760254, 0.7226753830909729]
[2025-05-26 16:53:12,255]: Mean: 0.77229637
[2025-05-26 16:53:12,268]: Min: 0.64194864
[2025-05-26 16:53:12,280]: Max: 1.03479648
[2025-05-26 16:53:12,280]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-26 16:53:12,304]: Sample Values (25 elements): [-0.08179504424333572, -0.03329871594905853, -0.024351658299565315, 0.0042261527851223946, -0.22050191462039948, 0.0772784873843193, 0.048141490668058395, 0.005763476714491844, -0.11124734580516815, -0.11051695793867111, -0.1188952773809433, -0.07209242880344391, -0.006887875031679869, 0.08344200998544693, -0.02956434339284897, 0.08426646888256073, 0.11555669456720352, -0.015471403487026691, -0.04063037037849426, 0.046281445771455765, 0.060021501034498215, 0.17384856939315796, -0.1053822711110115, 0.01047764252871275, -0.10419771075248718]
[2025-05-26 16:53:12,315]: Mean: -0.00231145
[2025-05-26 16:53:12,317]: Min: -0.36017370
[2025-05-26 16:53:12,317]: Max: 0.40461463
[2025-05-26 16:53:12,317]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,318]: Sample Values (25 elements): [0.6730953454971313, 0.7798324227333069, 0.7046952843666077, 0.706540048122406, 0.6978319883346558, 0.7496323585510254, 0.7194908857345581, 0.806928277015686, 0.720485508441925, 0.6941741108894348, 0.9266091585159302, 0.6765279173851013, 0.761631965637207, 0.8774563074111938, 0.7511789202690125, 0.6716228723526001, 0.7557079792022705, 0.8090895414352417, 0.6851906180381775, 0.7647442817687988, 0.7604175209999084, 0.6949316263198853, 0.7261044979095459, 0.8692240118980408, 0.7280266880989075]
[2025-05-26 16:53:12,318]: Mean: 0.73329341
[2025-05-26 16:53:12,318]: Min: 0.58073509
[2025-05-26 16:53:12,318]: Max: 0.92660916
[2025-05-26 16:53:12,318]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 16:53:12,342]: Sample Values (25 elements): [-0.012587066739797592, -0.04275526851415634, -0.04250766709446907, 0.03684854134917259, 0.017835207283496857, 0.034178100526332855, 0.045657433569431305, 0.15062740445137024, -0.041720833629369736, -0.026767950505018234, -0.0092335669323802, 0.07579405605792999, -0.1641925424337387, -0.06843414157629013, -0.01711389608681202, -0.15245157480239868, 0.04366684705018997, -0.07784822583198547, 0.16116879880428314, 0.03602295368909836, 0.02241101674735546, -0.012246735394001007, 0.03538964316248894, -0.05294738709926605, 0.005930660758167505]
[2025-05-26 16:53:12,348]: Mean: -0.00511866
[2025-05-26 16:53:12,357]: Min: -0.36893761
[2025-05-26 16:53:12,366]: Max: 0.39880630
[2025-05-26 16:53:12,366]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,387]: Sample Values (25 elements): [0.8958367109298706, 0.87779301404953, 0.8814616203308105, 0.9978500008583069, 0.9604738354682922, 0.936271071434021, 0.7557615637779236, 0.797596275806427, 0.7415459156036377, 1.0818990468978882, 0.9924464225769043, 1.1530402898788452, 0.9821513295173645, 0.9524419903755188, 0.9268239736557007, 0.9274445176124573, 0.9280033707618713, 0.9135081768035889, 0.7948647141456604, 0.7895546555519104, 0.9640163779258728, 0.8527306318283081, 1.0287327766418457, 0.7794219851493835, 0.7589333653450012]
[2025-05-26 16:53:12,398]: Mean: 0.93178588
[2025-05-26 16:53:12,410]: Min: 0.74154592
[2025-05-26 16:53:12,423]: Max: 1.21059465
[2025-05-26 16:53:12,423]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-26 16:53:12,449]: Sample Values (25 elements): [-0.2895949184894562, -0.08208316564559937, 0.0006794660002924502, -0.07214553654193878, 0.0001590346946613863, 0.07049407809972763, 0.10721155256032944, 0.021947752684354782, -0.20962561666965485, -0.26424744725227356, -0.0368666835129261, 0.04015372693538666, 0.25116270780563354, -0.083070769906044, 0.013907285407185555, -0.018357453867793083, 0.07520436495542526, -0.019183848053216934, 0.05910300463438034, 0.15023395419120789, 0.04698885232210159, 0.03559176251292229, 0.14786197245121002, -0.14164456725120544, -0.15394845604896545]
[2025-05-26 16:53:12,460]: Mean: -0.00349480
[2025-05-26 16:53:12,470]: Min: -0.39916086
[2025-05-26 16:53:12,472]: Max: 0.42943841
[2025-05-26 16:53:12,472]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,473]: Sample Values (25 elements): [0.5221688747406006, 0.6813521385192871, 0.4562074840068817, 0.5670632719993591, 0.6287616491317749, 0.5546311736106873, 0.4694168269634247, 0.6672533750534058, 0.44314292073249817, 0.48695775866508484, 0.4090699851512909, 0.5531575679779053, 0.5897173881530762, 0.6343590021133423, 0.5323798656463623, 0.6125568747520447, 0.6968932747840881, 0.6180809140205383, 0.5602192878723145, 0.5078282356262207, 0.5702153444290161, 0.4498786926269531, 0.5321868658065796, 0.47853314876556396, 0.49962204694747925]
[2025-05-26 16:53:12,473]: Mean: 0.55357111
[2025-05-26 16:53:12,473]: Min: 0.35498214
[2025-05-26 16:53:12,473]: Max: 0.75238854
[2025-05-26 16:53:12,473]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 16:53:12,481]: Sample Values (25 elements): [0.043369002640247345, -0.026484832167625427, 0.04382437467575073, 0.16371244192123413, 0.08678896725177765, -0.047188326716423035, -0.03550836443901062, -0.05278823897242546, 0.0625072568655014, -0.08394843339920044, -0.20010779798030853, 0.0799616128206253, -0.002718108706176281, -0.016853779554367065, -0.032053377479314804, -0.040462054312229156, -0.039269573986530304, -0.014538292773067951, -0.12877535820007324, -0.05008666589856148, 0.03428525850176811, -0.04078962281346321, 0.01685001328587532, -0.02620592713356018, 0.08455610275268555]
[2025-05-26 16:53:12,497]: Mean: -0.00803639
[2025-05-26 16:53:12,509]: Min: -0.37433928
[2025-05-26 16:53:12,517]: Max: 0.35072187
[2025-05-26 16:53:12,517]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,531]: Sample Values (25 elements): [0.6904122829437256, 0.6937932968139648, 0.7198683023452759, 0.7210726737976074, 0.6687721610069275, 0.7670519351959229, 0.70497065782547, 0.7289857864379883, 0.7777217626571655, 0.6916759014129639, 0.6070521473884583, 0.6071315407752991, 0.5835806727409363, 0.656188428401947, 0.6921910643577576, 0.8289439082145691, 0.6820906400680542, 0.7346174716949463, 0.7315410375595093, 0.5413722395896912, 0.5626987218856812, 0.7199510335922241, 0.7876186370849609, 0.7207620739936829, 0.6163601875305176]
[2025-05-26 16:53:12,542]: Mean: 0.69555080
[2025-05-26 16:53:12,553]: Min: 0.49381819
[2025-05-26 16:53:12,565]: Max: 0.84434229
[2025-05-26 16:53:12,565]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 16:53:12,592]: Sample Values (25 elements): [0.03590000793337822, 0.03378761559724808, -0.06789170205593109, -0.07590797543525696, 0.1238940954208374, -0.012792134657502174, -0.05645877495408058, -0.023390479385852814, 0.0034333751536905766, -0.03070446290075779, -0.030230065807700157, -0.024960944429039955, -0.06239740550518036, -0.030047470703721046, 0.14670833945274353, 0.0313851535320282, -0.001861853408627212, -0.045312799513339996, -0.06161915883421898, 0.04922796040773392, -0.07196828722953796, -0.05926535278558731, 0.008666148409247398, 0.0014730362454429269, -0.07877135276794434]
[2025-05-26 16:53:12,603]: Mean: -0.00275900
[2025-05-26 16:53:12,614]: Min: -0.27079698
[2025-05-26 16:53:12,623]: Max: 0.22653627
[2025-05-26 16:53:12,623]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,628]: Sample Values (25 elements): [1.0511245727539062, 1.0069977045059204, 1.043241262435913, 0.9955659508705139, 1.0031507015228271, 1.0433279275894165, 1.005831241607666, 1.2390729188919067, 0.9533618092536926, 1.0736970901489258, 1.0782771110534668, 0.9618154168128967, 1.0392122268676758, 1.1941174268722534, 0.8617061972618103, 1.0830793380737305, 1.034920334815979, 0.9617919325828552, 0.9474215507507324, 0.7376288771629333, 1.1005642414093018, 1.003705620765686, 1.1277037858963013, 1.0459141731262207, 1.0810823440551758]
[2025-05-26 16:53:12,628]: Mean: 1.01547432
[2025-05-26 16:53:12,628]: Min: 0.66933763
[2025-05-26 16:53:12,628]: Max: 1.25252390
[2025-05-26 16:53:12,628]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 16:53:12,629]: Sample Values (25 elements): [-0.020832091569900513, 0.03463929519057274, 0.04405860975384712, -5.899023737737154e-31, 0.04359741508960724, -0.0824674442410469, 4.907207092219077e-41, 0.0024717673659324646, 0.08379099518060684, 0.005219268146902323, -0.013705258257687092, 0.012121500447392464, 0.2169148325920105, -0.001302638091146946, 0.03439938649535179, 0.0006646006950177252, 0.08151455968618393, -0.0018150507239624858, 3.1825512989096903e-19, 0.08054809272289276, 0.051033176481723785, -0.06701146066188812, -0.041205551475286484, -0.03001570701599121, -0.03445830196142197]
[2025-05-26 16:53:12,636]: Mean: -0.00248995
[2025-05-26 16:53:12,652]: Min: -0.24713053
[2025-05-26 16:53:12,664]: Max: 0.28318891
[2025-05-26 16:53:12,664]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,679]: Sample Values (25 elements): [0.5662978291511536, 0.7850766777992249, 0.3416999578475952, 0.6806345582008362, 0.7053264379501343, 0.6939213871955872, 0.6465002298355103, 3.5270957596367225e-05, 0.6875334978103638, 0.7555617094039917, 0.5850499272346497, 0.7276888489723206, 0.696631133556366, -8.39380663819611e-06, 0.0003626550314947963, 0.7091860771179199, 0.06214506924152374, 0.3467317223548889, 0.43578940629959106, 0.6651847958564758, 0.406518816947937, 0.8391880393028259, 0.05663496255874634, 0.7151074409484863, 0.650722861289978]
[2025-05-26 16:53:12,687]: Mean: 0.50055462
[2025-05-26 16:53:12,698]: Min: -0.00000839
[2025-05-26 16:53:12,708]: Max: 0.83918804
[2025-05-26 16:53:12,708]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-26 16:53:12,734]: Sample Values (25 elements): [-0.047366734594106674, 0.039303120225667953, -0.047449175268411636, -0.054539915174245834, -0.06852434575557709, 0.015203753486275673, -0.03856494650244713, -0.04870796203613281, 0.09296019375324249, -0.022833997383713722, -0.00696447491645813, -0.08334333449602127, -0.03167237713932991, 0.10238085687160492, -0.003849009983241558, -0.0016186714638024569, -0.06940177083015442, 0.035048943012952805, -0.0012130241375416517, -0.051223453134298325, -5.5351289340830274e-42, 0.06766773015260696, -0.014572658576071262, 0.025495661422610283, 0.01848178170621395]
[2025-05-26 16:53:12,747]: Mean: 0.00129906
[2025-05-26 16:53:12,759]: Min: -0.16644956
[2025-05-26 16:53:12,770]: Max: 0.19482549
[2025-05-26 16:53:12,770]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-26 16:53:12,783]: Sample Values (25 elements): [1.01060950756073, 1.1721752882003784, 1.0000007152557373, 1.1917186975479126, 0.8541190028190613, 1.1689001321792603, 0.8894569873809814, 1.0373907089233398, 1.0464935302734375, 1.1627253293991089, 1.0223174095153809, 0.9632009267807007, 1.061997890472412, 1.0193893909454346, 1.0095371007919312, 1.0313478708267212, 0.9954600930213928, 1.254279375076294, 1.0375205278396606, 1.0846703052520752, 0.9681646227836609, 1.1033577919006348, 0.942283034324646, 1.1599856615066528, 1.2239336967468262]
[2025-05-26 16:53:12,783]: Mean: 1.06532025
[2025-05-26 16:53:12,783]: Min: 0.83497131
[2025-05-26 16:53:12,784]: Max: 1.31986094
[2025-05-26 16:53:12,784]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-26 16:53:12,784]: Sample Values (25 elements): [0.45572325587272644, 0.32220539450645447, 0.35204508900642395, 0.30900877714157104, 0.17463664710521698, 0.19700032472610474, 0.3632606565952301, -0.010930471122264862, -0.41186490654945374, -0.19462549686431885, -0.3175378441810608, 0.07363548129796982, 0.06410504877567291, 0.33130329847335815, -0.42565324902534485, 0.26832735538482666, 0.31489884853363037, -0.20848292112350464, 0.2636623680591583, -0.2946213185787201, -0.23903560638427734, 0.37485992908477783, 0.32020625472068787, -0.2625986933708191, 0.31129422783851624]
[2025-05-26 16:53:12,784]: Mean: -0.06884580
[2025-05-26 16:53:12,784]: Min: -0.74809176
[2025-05-26 16:53:12,792]: Max: 0.47487757
[2025-05-26 16:53:12,792]: Checkpoint of model at path [checkpoint/ResNet20_relu.ckpt] will be used for QAT
[2025-05-27 03:32:54,468]: Checkpoint of model at path [checkpoint/ResNet20_relu.ckpt] will be used for QAT
[2025-05-27 03:32:54,478]: 


QAT of ResNet20 with relu down to 4 bits...
[2025-05-27 03:32:54,701]: [ResNet20_relu_quantized_4_bits] after configure_qat:
[2025-05-27 03:32:54,802]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 03:34:42,621]: [ResNet20_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.3805 Train Acc: 0.8663 Eval Loss: 0.4814 Eval Acc: 0.8492 (LR: 0.00100000)
[2025-05-27 03:36:23,490]: [ResNet20_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.3799 Train Acc: 0.8666 Eval Loss: 0.4817 Eval Acc: 0.8465 (LR: 0.00100000)
[2025-05-27 03:38:04,382]: [ResNet20_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.3809 Train Acc: 0.8666 Eval Loss: 0.4773 Eval Acc: 0.8443 (LR: 0.00100000)
[2025-05-27 03:39:45,519]: [ResNet20_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.3785 Train Acc: 0.8682 Eval Loss: 0.5260 Eval Acc: 0.8315 (LR: 0.00100000)
[2025-05-27 03:41:26,875]: [ResNet20_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.3821 Train Acc: 0.8638 Eval Loss: 0.5254 Eval Acc: 0.8305 (LR: 0.00100000)
[2025-05-27 03:43:08,685]: [ResNet20_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.3780 Train Acc: 0.8677 Eval Loss: 0.4905 Eval Acc: 0.8416 (LR: 0.00100000)
[2025-05-27 03:44:57,886]: [ResNet20_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.3806 Train Acc: 0.8656 Eval Loss: 0.5001 Eval Acc: 0.8377 (LR: 0.00100000)
[2025-05-27 03:46:49,982]: [ResNet20_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.3780 Train Acc: 0.8676 Eval Loss: 0.4997 Eval Acc: 0.8359 (LR: 0.00100000)
[2025-05-27 03:48:42,858]: [ResNet20_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.3798 Train Acc: 0.8693 Eval Loss: 0.5041 Eval Acc: 0.8393 (LR: 0.00010000)
[2025-05-27 03:50:35,096]: [ResNet20_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.3161 Train Acc: 0.8910 Eval Loss: 0.3568 Eval Acc: 0.8810 (LR: 0.00010000)
[2025-05-27 03:52:26,768]: [ResNet20_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.2895 Train Acc: 0.8983 Eval Loss: 0.3573 Eval Acc: 0.8800 (LR: 0.00010000)
[2025-05-27 03:54:20,521]: [ResNet20_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.2832 Train Acc: 0.9007 Eval Loss: 0.3444 Eval Acc: 0.8822 (LR: 0.00010000)
[2025-05-27 03:56:16,290]: [ResNet20_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.2802 Train Acc: 0.9023 Eval Loss: 0.3610 Eval Acc: 0.8791 (LR: 0.00010000)
[2025-05-27 03:58:14,242]: [ResNet20_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.2769 Train Acc: 0.9040 Eval Loss: 0.3453 Eval Acc: 0.8868 (LR: 0.00010000)
[2025-05-27 04:00:10,929]: [ResNet20_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.2769 Train Acc: 0.9031 Eval Loss: 0.3419 Eval Acc: 0.8903 (LR: 0.00010000)
[2025-05-27 04:02:07,324]: [ResNet20_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.2730 Train Acc: 0.9046 Eval Loss: 0.3498 Eval Acc: 0.8826 (LR: 0.00010000)
[2025-05-27 04:04:03,146]: [ResNet20_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.2727 Train Acc: 0.9045 Eval Loss: 0.3497 Eval Acc: 0.8840 (LR: 0.00010000)
[2025-05-27 04:05:59,302]: [ResNet20_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.2682 Train Acc: 0.9069 Eval Loss: 0.3454 Eval Acc: 0.8861 (LR: 0.00010000)
[2025-05-27 04:07:55,124]: [ResNet20_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.2663 Train Acc: 0.9061 Eval Loss: 0.3516 Eval Acc: 0.8839 (LR: 0.00010000)
[2025-05-27 04:09:53,072]: [ResNet20_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.2652 Train Acc: 0.9069 Eval Loss: 0.3518 Eval Acc: 0.8844 (LR: 0.00010000)
[2025-05-27 04:11:54,014]: [ResNet20_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.2648 Train Acc: 0.9076 Eval Loss: 0.3478 Eval Acc: 0.8866 (LR: 0.00001000)
[2025-05-27 04:13:50,830]: [ResNet20_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.2488 Train Acc: 0.9120 Eval Loss: 0.3401 Eval Acc: 0.8874 (LR: 0.00001000)
[2025-05-27 04:15:47,150]: [ResNet20_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.2529 Train Acc: 0.9118 Eval Loss: 0.3394 Eval Acc: 0.8885 (LR: 0.00001000)
[2025-05-27 04:17:43,327]: [ResNet20_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.2489 Train Acc: 0.9120 Eval Loss: 0.3383 Eval Acc: 0.8914 (LR: 0.00001000)
[2025-05-27 04:19:38,035]: [ResNet20_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.2468 Train Acc: 0.9112 Eval Loss: 0.3345 Eval Acc: 0.8912 (LR: 0.00001000)
[2025-05-27 04:21:31,667]: [ResNet20_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.2489 Train Acc: 0.9125 Eval Loss: 0.3374 Eval Acc: 0.8900 (LR: 0.00001000)
[2025-05-27 04:23:22,625]: [ResNet20_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.2510 Train Acc: 0.9104 Eval Loss: 0.3321 Eval Acc: 0.8895 (LR: 0.00001000)
[2025-05-27 04:25:07,595]: [ResNet20_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.2440 Train Acc: 0.9138 Eval Loss: 0.3369 Eval Acc: 0.8915 (LR: 0.00001000)
[2025-05-27 04:26:46,332]: [ResNet20_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.2470 Train Acc: 0.9132 Eval Loss: 0.3352 Eval Acc: 0.8884 (LR: 0.00001000)
[2025-05-27 04:28:25,609]: [ResNet20_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.2470 Train Acc: 0.9140 Eval Loss: 0.3326 Eval Acc: 0.8913 (LR: 0.00001000)
[2025-05-27 04:30:04,971]: [ResNet20_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.2456 Train Acc: 0.9138 Eval Loss: 0.3354 Eval Acc: 0.8907 (LR: 0.00001000)
[2025-05-27 04:31:47,668]: [ResNet20_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.2481 Train Acc: 0.9123 Eval Loss: 0.3349 Eval Acc: 0.8898 (LR: 0.00001000)
[2025-05-27 04:33:38,684]: [ResNet20_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.2424 Train Acc: 0.9131 Eval Loss: 0.3313 Eval Acc: 0.8905 (LR: 0.00001000)
[2025-05-27 04:35:27,628]: [ResNet20_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.2443 Train Acc: 0.9143 Eval Loss: 0.3376 Eval Acc: 0.8882 (LR: 0.00001000)
[2025-05-27 04:37:21,217]: [ResNet20_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.2463 Train Acc: 0.9127 Eval Loss: 0.3367 Eval Acc: 0.8918 (LR: 0.00001000)
[2025-05-27 04:39:20,753]: [ResNet20_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.2440 Train Acc: 0.9163 Eval Loss: 0.3315 Eval Acc: 0.8906 (LR: 0.00001000)
[2025-05-27 04:41:24,002]: [ResNet20_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.2470 Train Acc: 0.9123 Eval Loss: 0.3369 Eval Acc: 0.8911 (LR: 0.00001000)
[2025-05-27 04:43:28,423]: [ResNet20_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.2445 Train Acc: 0.9149 Eval Loss: 0.3360 Eval Acc: 0.8901 (LR: 0.00001000)
[2025-05-27 04:45:30,860]: [ResNet20_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.2426 Train Acc: 0.9141 Eval Loss: 0.3349 Eval Acc: 0.8908 (LR: 0.00000100)
[2025-05-27 04:47:33,528]: [ResNet20_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.2384 Train Acc: 0.9165 Eval Loss: 0.3302 Eval Acc: 0.8909 (LR: 0.00000100)
[2025-05-27 04:49:35,362]: [ResNet20_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.2387 Train Acc: 0.9153 Eval Loss: 0.3301 Eval Acc: 0.8902 (LR: 0.00000100)
[2025-05-27 04:51:37,243]: [ResNet20_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.2410 Train Acc: 0.9155 Eval Loss: 0.3370 Eval Acc: 0.8895 (LR: 0.00000100)
[2025-05-27 04:53:37,972]: [ResNet20_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.2404 Train Acc: 0.9166 Eval Loss: 0.3316 Eval Acc: 0.8893 (LR: 0.00000100)
[2025-05-27 04:55:38,072]: [ResNet20_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.2425 Train Acc: 0.9147 Eval Loss: 0.3302 Eval Acc: 0.8916 (LR: 0.00000100)
[2025-05-27 04:57:37,242]: [ResNet20_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.2401 Train Acc: 0.9157 Eval Loss: 0.3290 Eval Acc: 0.8924 (LR: 0.00000100)
[2025-05-27 04:59:33,499]: [ResNet20_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.2419 Train Acc: 0.9139 Eval Loss: 0.3343 Eval Acc: 0.8914 (LR: 0.00000100)
[2025-05-27 05:01:29,723]: [ResNet20_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.2402 Train Acc: 0.9156 Eval Loss: 0.3339 Eval Acc: 0.8940 (LR: 0.00000100)
[2025-05-27 05:03:25,137]: [ResNet20_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.2423 Train Acc: 0.9144 Eval Loss: 0.3382 Eval Acc: 0.8928 (LR: 0.00000100)
[2025-05-27 05:05:20,345]: [ResNet20_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.2393 Train Acc: 0.9159 Eval Loss: 0.3338 Eval Acc: 0.8905 (LR: 0.00000100)
[2025-05-27 05:07:15,085]: [ResNet20_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.2398 Train Acc: 0.9157 Eval Loss: 0.3384 Eval Acc: 0.8908 (LR: 0.00000100)
[2025-05-27 05:09:09,105]: [ResNet20_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.2451 Train Acc: 0.9141 Eval Loss: 0.3285 Eval Acc: 0.8916 (LR: 0.00000100)
[2025-05-27 05:11:02,496]: [ResNet20_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.2416 Train Acc: 0.9151 Eval Loss: 0.3338 Eval Acc: 0.8906 (LR: 0.00000100)
[2025-05-27 05:12:55,929]: [ResNet20_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.2422 Train Acc: 0.9163 Eval Loss: 0.3300 Eval Acc: 0.8931 (LR: 0.00000100)
[2025-05-27 05:14:49,990]: [ResNet20_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.2400 Train Acc: 0.9154 Eval Loss: 0.3250 Eval Acc: 0.8961 (LR: 0.00000100)
[2025-05-27 05:16:44,331]: [ResNet20_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.2375 Train Acc: 0.9164 Eval Loss: 0.3340 Eval Acc: 0.8913 (LR: 0.00000100)
[2025-05-27 05:18:39,722]: [ResNet20_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.2367 Train Acc: 0.9177 Eval Loss: 0.3302 Eval Acc: 0.8921 (LR: 0.00000100)
[2025-05-27 05:20:34,820]: [ResNet20_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.2404 Train Acc: 0.9139 Eval Loss: 0.3319 Eval Acc: 0.8939 (LR: 0.00000100)
[2025-05-27 05:22:29,082]: [ResNet20_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.2404 Train Acc: 0.9155 Eval Loss: 0.3297 Eval Acc: 0.8932 (LR: 0.00000100)
[2025-05-27 05:24:22,566]: [ResNet20_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.2405 Train Acc: 0.9153 Eval Loss: 0.3310 Eval Acc: 0.8911 (LR: 0.00000100)
[2025-05-27 05:26:16,316]: [ResNet20_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.2391 Train Acc: 0.9162 Eval Loss: 0.3418 Eval Acc: 0.8881 (LR: 0.00000010)
[2025-05-27 05:26:16,317]: [ResNet20_relu_quantized_4_bits] Best Eval Accuracy: 0.8961
[2025-05-27 05:26:16,377]: 


Quantization of model down to 4 bits finished
[2025-05-27 05:26:16,377]: Model Architecture:
[2025-05-27 05:26:16,440]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8869], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=28.30329704284668)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0658], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5088806748390198, max_val=0.4783143997192383)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7126], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.68844985961914)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0621], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.500265896320343, max_val=0.4318336546421051)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.2256], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=33.38378143310547)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0689], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5829007625579834, max_val=0.4512575566768646)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3785], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.677862644195557)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0575], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4130178391933441, max_val=0.4498920142650604)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1520], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=32.28015899658203)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0829], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7731339931488037, max_val=0.4699954390525818)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4088], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.132444381713867)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0703], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6145794987678528, max_val=0.440247118473053)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1363], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=32.04502487182617)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0622], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48300695419311523, max_val=0.44971996545791626)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3931], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.89592170715332)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0707], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6175777316093445, max_val=0.44235309958457947)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0774], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48102128505706787, max_val=0.6793330907821655)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.181897163391113)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0617], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.43325862288475037, max_val=0.4918038547039032)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3262], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.893041133880615)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0477], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3592226803302765, max_val=0.3558197021484375)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7953], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=11.930160522460938)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4466869533061981, max_val=0.44871196150779724)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3258], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.887462139129639)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0477], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3528899848461151, max_val=0.3630574345588684)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1673], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.508920669555664)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0582], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4144996106624603, max_val=0.45793813467025757)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3307], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.9608869552612305)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0502], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3617393374443054, max_val=0.39061465859413147)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0571], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4043245017528534, max_val=0.4517560303211212)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5641], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.461897850036621)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0515], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37444570660591125, max_val=0.3979191780090332)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3204], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.806471824645996)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0356], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2742336392402649, max_val=0.2594652771949768)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8858], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.286980628967285)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0394], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26376423239707947, max_val=0.3273586630821228)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.059025764465332)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0251], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16905485093593597, max_val=0.20669865608215332)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3624], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=20.43643569946289)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 05:26:16,440]: 
Model Weights:
[2025-05-27 05:26:16,440]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 05:26:16,441]: Sample Values (25 elements): [-0.0018763008993119001, -0.3807123005390167, -0.03215677663683891, 0.24513137340545654, -0.037687089294195175, 0.07145547866821289, -0.04732140526175499, -0.11947806179523468, 0.2019577920436859, -0.14862105250358582, -0.23354941606521606, -0.1371016800403595, -0.28626564145088196, 0.12749263644218445, -0.17700344324111938, 0.12023947387933731, -0.08134771138429642, 0.045620955526828766, 0.3167307674884796, 0.07837704569101334, 0.1349998414516449, -0.028764694929122925, 0.09044185280799866, 0.21904057264328003, 0.29257285594940186]
[2025-05-27 05:26:16,441]: Mean: -0.00165119
[2025-05-27 05:26:16,441]: Min: -0.59323597
[2025-05-27 05:26:16,442]: Max: 0.52994639
[2025-05-27 05:26:16,442]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,442]: Sample Values (16 elements): [1.5554310083389282, 1.7766395807266235, 1.7007322311401367, 1.2745062112808228, 1.4836146831512451, 1.761122703552246, 2.0399160385131836, 1.6429369449615479, 1.4233851432800293, 1.5606873035430908, 1.0963271856307983, 2.551367998123169, 1.1155668497085571, 1.5570851564407349, 1.1968756914138794, 1.6599775552749634]
[2025-05-27 05:26:16,442]: Mean: 1.58726072
[2025-05-27 05:26:16,442]: Min: 1.09632719
[2025-05-27 05:26:16,442]: Max: 2.55136800
[2025-05-27 05:26:16,443]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:16,444]: Sample Values (25 elements): [-0.13162599503993988, 0.06581299751996994, -0.06581299751996994, -0.13162599503993988, 0.0, 0.19743898510932922, -0.06581299751996994, 0.19743898510932922, 0.0, 0.0, -0.06581299751996994, 0.06581299751996994, 0.0, 0.0, -0.06581299751996994, -0.06581299751996994, 0.13162599503993988, 0.13162599503993988, 0.06581299751996994, -0.06581299751996994, 0.0, -0.06581299751996994, -0.26325199007987976, 0.06581299751996994, -0.26325199007987976]
[2025-05-27 05:26:16,444]: Mean: -0.01162582
[2025-05-27 05:26:16,444]: Min: -0.52650398
[2025-05-27 05:26:16,444]: Max: 0.46069098
[2025-05-27 05:26:16,444]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,445]: Sample Values (16 elements): [0.8260698318481445, 0.8736459612846375, 1.0160439014434814, 0.8767611980438232, 0.8967683911323547, 0.8968003988265991, 0.9567254781723022, 0.7924883365631104, 0.9105619788169861, 1.2312347888946533, 1.1324971914291382, 1.0608516931533813, 0.9559628963470459, 1.0812935829162598, 1.0277775526046753, 0.8455960750579834]
[2025-05-27 05:26:16,445]: Mean: 0.96131742
[2025-05-27 05:26:16,445]: Min: 0.79248834
[2025-05-27 05:26:16,445]: Max: 1.23123479
[2025-05-27 05:26:16,446]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:16,446]: Sample Values (25 elements): [-0.24855990707874298, 0.062139976769685745, 0.0, 0.0, 0.0, -0.062139976769685745, 0.12427995353937149, -0.062139976769685745, -0.062139976769685745, -0.18641993403434753, 0.0, 0.18641993403434753, 0.0, 0.24855990707874298, 0.12427995353937149, -0.12427995353937149, 0.12427995353937149, 0.0, 0.12427995353937149, 0.18641993403434753, -0.062139976769685745, 0.0, 0.18641993403434753, 0.062139976769685745, 0.12427995353937149]
[2025-05-27 05:26:16,447]: Mean: 0.00016182
[2025-05-27 05:26:16,447]: Min: -0.49711981
[2025-05-27 05:26:16,447]: Max: 0.43497983
[2025-05-27 05:26:16,447]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,447]: Sample Values (16 elements): [0.9392869472503662, 0.9536256790161133, 1.2226617336273193, 0.9317836761474609, 1.1872464418411255, 1.1113600730895996, 1.2141382694244385, 1.3734558820724487, 1.1844775676727295, 1.1347482204437256, 1.1401596069335938, 1.3379679918289185, 1.5020684003829956, 1.2278445959091187, 1.2013033628463745, 1.0822334289550781]
[2025-05-27 05:26:16,447]: Mean: 1.17152262
[2025-05-27 05:26:16,447]: Min: 0.93178368
[2025-05-27 05:26:16,448]: Max: 1.50206840
[2025-05-27 05:26:16,449]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:16,449]: Sample Values (25 elements): [-0.13788776099681854, 0.06894388049840927, -0.06894388049840927, 0.0, 0.13788776099681854, -0.06894388049840927, -0.06894388049840927, -0.06894388049840927, -0.20683163404464722, -0.20683163404464722, -0.13788776099681854, -0.06894388049840927, 0.06894388049840927, 0.06894388049840927, 0.06894388049840927, 0.0, 0.06894388049840927, -0.06894388049840927, 0.0, 0.13788776099681854, -0.06894388049840927, -0.06894388049840927, 0.06894388049840927, 0.13788776099681854, 0.0]
[2025-05-27 05:26:16,449]: Mean: -0.00490746
[2025-05-27 05:26:16,449]: Min: -0.55155104
[2025-05-27 05:26:16,449]: Max: 0.48260716
[2025-05-27 05:26:16,449]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,450]: Sample Values (16 elements): [0.7477967739105225, 1.0618056058883667, 0.6857782602310181, 0.7203242778778076, 0.8865112662315369, 0.8279520273208618, 0.6543563008308411, 0.7389107942581177, 0.7777455449104309, 0.8273877501487732, 0.8703564405441284, 0.9023614525794983, 0.8045986294746399, 0.9632033705711365, 0.7392615675926208, 0.570832371711731]
[2025-05-27 05:26:16,450]: Mean: 0.79869890
[2025-05-27 05:26:16,450]: Min: 0.57083237
[2025-05-27 05:26:16,450]: Max: 1.06180561
[2025-05-27 05:26:16,451]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:16,451]: Sample Values (25 elements): [0.2876366376876831, 0.0575273260474205, 0.0575273260474205, -0.1725819706916809, 0.115054652094841, -0.115054652094841, -0.0575273260474205, 0.0575273260474205, 0.0, -0.0575273260474205, -0.115054652094841, 0.0, 0.0575273260474205, -0.230109304189682, -0.0575273260474205, 0.115054652094841, -0.115054652094841, 0.115054652094841, -0.1725819706916809, 0.0, 0.0, -0.115054652094841, -0.115054652094841, 0.115054652094841, -0.230109304189682]
[2025-05-27 05:26:16,451]: Mean: -0.00736570
[2025-05-27 05:26:16,452]: Min: -0.40269127
[2025-05-27 05:26:16,452]: Max: 0.46021861
[2025-05-27 05:26:16,452]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,452]: Sample Values (16 elements): [0.7401292324066162, 0.9913113713264465, 1.3040105104446411, 0.8423236012458801, 1.3769840002059937, 0.9620773196220398, 1.325438380241394, 0.9687533974647522, 0.9032412171363831, 0.8513059616088867, 0.9625669717788696, 1.331451416015625, 1.1259973049163818, 0.8275450468063354, 1.0587513446807861, 1.0655087232589722]
[2025-05-27 05:26:16,452]: Mean: 1.03983724
[2025-05-27 05:26:16,452]: Min: 0.74012923
[2025-05-27 05:26:16,452]: Max: 1.37698400
[2025-05-27 05:26:16,453]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:16,454]: Sample Values (25 elements): [0.0, -0.08287529647350311, -0.16575059294700623, 0.24862588942050934, 0.0, 0.0, -0.16575059294700623, -0.16575059294700623, 0.0, 0.0, 0.08287529647350311, -0.16575059294700623, 0.08287529647350311, 0.0, 0.08287529647350311, 0.16575059294700623, 0.0, 0.08287529647350311, 0.0, 0.0, -0.16575059294700623, -0.16575059294700623, -0.08287529647350311, 0.0, -0.24862588942050934]
[2025-05-27 05:26:16,454]: Mean: -0.00787747
[2025-05-27 05:26:16,454]: Min: -0.74587768
[2025-05-27 05:26:16,454]: Max: 0.49725178
[2025-05-27 05:26:16,454]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,454]: Sample Values (16 elements): [0.8872601985931396, 0.6594031453132629, 0.6630328893661499, 1.1899694204330444, 0.6966403722763062, 0.9619609117507935, 0.7117047309875488, 0.8395732045173645, 0.9635941386222839, 1.0052484273910522, 0.6143069267272949, 0.8857104778289795, 0.5344753265380859, 0.7652504444122314, 0.9080021381378174, 0.7438702583312988]
[2025-05-27 05:26:16,455]: Mean: 0.81437516
[2025-05-27 05:26:16,455]: Min: 0.53447533
[2025-05-27 05:26:16,455]: Max: 1.18996942
[2025-05-27 05:26:16,456]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 05:26:16,456]: Sample Values (25 elements): [-0.07032177597284317, 0.0, -0.2109653353691101, 0.0, 0.07032177597284317, 0.0, -0.07032177597284317, 0.0, -0.07032177597284317, -0.14064355194568634, 0.0, 0.35160887241363525, 0.07032177597284317, 0.2812871038913727, 0.07032177597284317, 0.0, 0.07032177597284317, 0.2109653353691101, -0.14064355194568634, 0.07032177597284317, -0.14064355194568634, 0.14064355194568634, 0.07032177597284317, -0.2812871038913727, 0.14064355194568634]
[2025-05-27 05:26:16,456]: Mean: 0.00070200
[2025-05-27 05:26:16,457]: Min: -0.63289601
[2025-05-27 05:26:16,457]: Max: 0.42193067
[2025-05-27 05:26:16,457]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 05:26:16,457]: Sample Values (16 elements): [0.687262773513794, 1.1008754968643188, 0.6814866662025452, 1.1592973470687866, 0.7348318696022034, 1.021740436553955, 0.864702045917511, 0.9763734340667725, 0.9388512969017029, 1.1995354890823364, 1.0281237363815308, 1.373054027557373, 1.025088906288147, 0.8993326425552368, 1.0206893682479858, 0.9644260406494141]
[2025-05-27 05:26:16,458]: Mean: 0.97972947
[2025-05-27 05:26:16,458]: Min: 0.68148667
[2025-05-27 05:26:16,458]: Max: 1.37305403
[2025-05-27 05:26:16,459]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 05:26:16,460]: Sample Values (25 elements): [0.24872717261314392, 0.0, 0.0, -0.06218179315328598, -0.12436358630657196, -0.06218179315328598, 0.18654537200927734, 0.0, 0.0, 0.0, -0.24872717261314392, 0.06218179315328598, 0.06218179315328598, -0.06218179315328598, 0.24872717261314392, 0.06218179315328598, -0.18654537200927734, 0.0, -0.06218179315328598, 0.18654537200927734, -0.06218179315328598, 0.3109089732170105, 0.06218179315328598, 0.12436358630657196, 0.06218179315328598]
[2025-05-27 05:26:16,460]: Mean: -0.00322514
[2025-05-27 05:26:16,460]: Min: -0.49745435
[2025-05-27 05:26:16,460]: Max: 0.43527254
[2025-05-27 05:26:16,460]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,461]: Sample Values (25 elements): [0.9351106882095337, 0.9723537564277649, 0.7878751158714294, 0.8587557673454285, 0.7680861949920654, 0.9248673915863037, 0.8412823677062988, 0.7690494656562805, 0.9402000904083252, 0.8914430737495422, 0.8599736094474792, 0.8440822958946228, 0.8719978332519531, 0.807434618473053, 0.8240671753883362, 0.7207522392272949, 0.8130977153778076, 0.9200387001037598, 0.8062081933021545, 0.858046293258667, 0.8322944641113281, 0.7744460701942444, 0.8106794953346252, 0.8134504556655884, 0.786760151386261]
[2025-05-27 05:26:16,461]: Mean: 0.85397649
[2025-05-27 05:26:16,461]: Min: 0.72075224
[2025-05-27 05:26:16,461]: Max: 0.97235376
[2025-05-27 05:26:16,462]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:16,463]: Sample Values (25 elements): [-0.07066205888986588, 0.21198618412017822, -0.07066205888986588, 0.14132411777973175, -0.21198618412017822, 0.0, 0.07066205888986588, 0.0, -0.14132411777973175, -0.2826482355594635, 0.0, 0.07066205888986588, 0.0, 0.07066205888986588, 0.0, 0.14132411777973175, 0.0, -0.07066205888986588, -0.07066205888986588, -0.07066205888986588, -0.21198618412017822, -0.14132411777973175, -0.14132411777973175, 0.14132411777973175, -0.07066205888986588]
[2025-05-27 05:26:16,463]: Mean: -0.00558948
[2025-05-27 05:26:16,463]: Min: -0.63595855
[2025-05-27 05:26:16,463]: Max: 0.42397237
[2025-05-27 05:26:16,463]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,463]: Sample Values (25 elements): [0.9862255454063416, 0.8682729601860046, 0.9099076986312866, 0.9156409502029419, 0.8309506177902222, 0.9092922806739807, 1.0215104818344116, 0.8938323855400085, 0.985920250415802, 1.007964849472046, 1.0517383813858032, 1.1348158121109009, 0.9356981515884399, 1.0153696537017822, 0.9684394598007202, 1.0903092622756958, 0.8114354014396667, 0.9058803915977478, 0.9272339344024658, 0.8537784814834595, 0.9300906658172607, 1.2178308963775635, 0.7501512765884399, 1.0103141069412231, 0.9281563758850098]
[2025-05-27 05:26:16,464]: Mean: 0.97118282
[2025-05-27 05:26:16,464]: Min: 0.75015128
[2025-05-27 05:26:16,464]: Max: 1.21783090
[2025-05-27 05:26:16,465]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 05:26:16,465]: Sample Values (25 elements): [0.3094278573989868, 0.3094278573989868, -0.1547139286994934, 0.0773569643497467, 0.0773569643497467, 0.0773569643497467, -0.1547139286994934, -0.3094278573989868, -0.2320708930492401, -0.1547139286994934, 0.0, 0.3094278573989868, -0.3094278573989868, -0.0773569643497467, 0.0773569643497467, -0.1547139286994934, -0.1547139286994934, 0.0, 0.1547139286994934, 0.0773569643497467, 0.0773569643497467, 0.3094278573989868, -0.0773569643497467, -0.0773569643497467, -0.2320708930492401]
[2025-05-27 05:26:16,465]: Mean: -0.00483481
[2025-05-27 05:26:16,466]: Min: -0.46414179
[2025-05-27 05:26:16,466]: Max: 0.69621265
[2025-05-27 05:26:16,466]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,466]: Sample Values (25 elements): [0.6412044763565063, 0.47848251461982727, 0.6534222364425659, 0.6650254130363464, 0.6269640922546387, 0.8076821565628052, 0.5124984979629517, 0.6712913513183594, 0.8826454281806946, 0.7163665890693665, 0.6297849416732788, 0.814909040927887, 0.6308820843696594, 0.635793149471283, 0.5372694134712219, 0.9295583963394165, 0.601459801197052, 0.5693748593330383, 0.5906149744987488, 0.6813332438468933, 0.6191619038581848, 0.4711168110370636, 0.5688627362251282, 0.5214037895202637, 0.7660369277000427]
[2025-05-27 05:26:16,466]: Mean: 0.64502680
[2025-05-27 05:26:16,466]: Min: 0.43436334
[2025-05-27 05:26:16,467]: Max: 0.92955840
[2025-05-27 05:26:16,468]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:16,468]: Sample Values (25 elements): [0.12334166467189789, -0.061670832335948944, -0.12334166467189789, 0.061670832335948944, 0.0, 0.061670832335948944, -0.061670832335948944, 0.061670832335948944, 0.0, 0.0, 0.061670832335948944, -0.12334166467189789, 0.0, -0.061670832335948944, 0.0, 0.061670832335948944, 0.18501248955726624, 0.061670832335948944, 0.0, -0.12334166467189789, -0.061670832335948944, -0.18501248955726624, -0.061670832335948944, -0.061670832335948944, 0.061670832335948944]
[2025-05-27 05:26:16,468]: Mean: -0.01008441
[2025-05-27 05:26:16,468]: Min: -0.43169582
[2025-05-27 05:26:16,469]: Max: 0.49336666
[2025-05-27 05:26:16,469]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,469]: Sample Values (25 elements): [0.8055627942085266, 0.7023056149482727, 0.7672120928764343, 0.8504537343978882, 0.7734507322311401, 0.7167214155197144, 0.8096725940704346, 0.7666179537773132, 0.7302287817001343, 0.7271838784217834, 0.902301013469696, 0.7398828268051147, 0.7777643203735352, 0.6874236464500427, 0.7090091109275818, 0.7800440192222595, 0.7093732357025146, 0.8852855563163757, 0.7266669273376465, 0.784845769405365, 0.8847329020500183, 0.8835607171058655, 0.7762126922607422, 0.8651466369628906, 0.8993320465087891]
[2025-05-27 05:26:16,469]: Mean: 0.79268914
[2025-05-27 05:26:16,469]: Min: 0.68742365
[2025-05-27 05:26:16,469]: Max: 0.90230101
[2025-05-27 05:26:16,470]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:16,471]: Sample Values (25 elements): [-0.04766949266195297, -0.09533898532390594, 0.0, 0.0, -0.09533898532390594, 0.04766949266195297, 0.0, 0.04766949266195297, -0.1906779706478119, -0.09533898532390594, 0.0, 0.09533898532390594, 0.04766949266195297, -0.14300847053527832, -0.04766949266195297, 0.0, -0.09533898532390594, -0.09533898532390594, -0.14300847053527832, -0.14300847053527832, 0.0, 0.09533898532390594, -0.14300847053527832, -0.14300847053527832, 0.04766949266195297]
[2025-05-27 05:26:16,471]: Mean: -0.00578799
[2025-05-27 05:26:16,471]: Min: -0.38135594
[2025-05-27 05:26:16,471]: Max: 0.33368644
[2025-05-27 05:26:16,471]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,472]: Sample Values (25 elements): [1.0187865495681763, 0.9923623204231262, 1.2331229448318481, 0.7790356874465942, 0.862896203994751, 0.8289573788642883, 0.9073932766914368, 0.6804647445678711, 0.8853087425231934, 0.7939902544021606, 0.8056319952011108, 0.8299070596694946, 0.9498481154441833, 0.6457101702690125, 0.6947453618049622, 0.9768707752227783, 0.752676784992218, 0.9392383694648743, 0.9571231603622437, 0.771568238735199, 1.0845931768417358, 0.6793797016143799, 0.7436124086380005, 0.8239420652389526, 0.728136420249939]
[2025-05-27 05:26:16,472]: Mean: 0.85008818
[2025-05-27 05:26:16,472]: Min: 0.62179381
[2025-05-27 05:26:16,472]: Max: 1.23312294
[2025-05-27 05:26:16,473]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:16,474]: Sample Values (25 elements): [-0.11938653886318207, 0.0, -0.11938653886318207, -0.11938653886318207, -0.059693269431591034, -0.1790798008441925, 0.11938653886318207, -0.059693269431591034, -0.059693269431591034, -0.059693269431591034, -0.059693269431591034, 0.0, 0.0, 0.11938653886318207, -0.1790798008441925, 0.059693269431591034, 0.11938653886318207, 0.059693269431591034, 0.0, 0.11938653886318207, -0.1790798008441925, -0.059693269431591034, 0.0, 0.11938653886318207, 0.11938653886318207]
[2025-05-27 05:26:16,474]: Mean: -0.01316154
[2025-05-27 05:26:16,474]: Min: -0.41785288
[2025-05-27 05:26:16,474]: Max: 0.47754616
[2025-05-27 05:26:16,474]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,474]: Sample Values (25 elements): [0.6254153251647949, 0.7111150622367859, 0.684829831123352, 0.7708958983421326, 0.5776843428611755, 0.6215521097183228, 0.7052792906761169, 0.7234541773796082, 0.8353045582771301, 0.755149245262146, 0.5888225436210632, 0.919474720954895, 0.6874138712882996, 0.6454456448554993, 0.8205017447471619, 0.8710289597511292, 0.6926483511924744, 0.6336484551429749, 0.7580639719963074, 0.700836718082428, 0.8387473821640015, 0.7466762661933899, 0.8271549940109253, 0.8509740829467773, 0.8379255533218384]
[2025-05-27 05:26:16,475]: Mean: 0.74968612
[2025-05-27 05:26:16,475]: Min: 0.57768434
[2025-05-27 05:26:16,475]: Max: 0.91947472
[2025-05-27 05:26:16,476]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 05:26:16,476]: Sample Values (25 elements): [-0.1431894600391388, 0.09545964002609253, 0.0, -0.047729820013046265, -0.047729820013046265, -0.047729820013046265, -0.047729820013046265, 0.047729820013046265, 0.047729820013046265, -0.047729820013046265, 0.09545964002609253, 0.1431894600391388, 0.1431894600391388, -0.1431894600391388, 0.047729820013046265, -0.1431894600391388, -0.19091928005218506, -0.33410874009132385, 0.09545964002609253, -0.047729820013046265, -0.09545964002609253, 0.1431894600391388, 0.0, -0.047729820013046265, 0.09545964002609253]
[2025-05-27 05:26:16,476]: Mean: -0.00356316
[2025-05-27 05:26:16,477]: Min: -0.33410874
[2025-05-27 05:26:16,477]: Max: 0.38183856
[2025-05-27 05:26:16,477]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 05:26:16,477]: Sample Values (25 elements): [0.7630771994590759, 0.7349613904953003, 0.8525053858757019, 0.7073272466659546, 0.9598782062530518, 0.7404336333274841, 0.8661065101623535, 1.131969928741455, 0.7802253365516663, 0.9323336482048035, 0.6669682860374451, 0.7977158427238464, 1.100412368774414, 0.7826049327850342, 0.7258070111274719, 0.7848547101020813, 0.7214903235435486, 0.9733802080154419, 1.0620754957199097, 0.7884774208068848, 0.8902040123939514, 0.9048319458961487, 0.7085641026496887, 1.0277355909347534, 0.8255025148391724]
[2025-05-27 05:26:16,477]: Mean: 0.83090127
[2025-05-27 05:26:16,477]: Min: 0.66696829
[2025-05-27 05:26:16,478]: Max: 1.13196993
[2025-05-27 05:26:16,479]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 05:26:16,479]: Sample Values (25 elements): [0.05816251039505005, 0.0, 0.05816251039505005, 0.1163250207901001, 0.0, -0.17448753118515015, -0.1163250207901001, 0.0, 0.1163250207901001, 0.0, 0.0, 0.1163250207901001, 0.17448753118515015, 0.0, 0.17448753118515015, 0.0, 0.0, -0.05816251039505005, -0.05816251039505005, -0.05816251039505005, 0.0, 0.0, 0.2326500415802002, 0.0, -0.05816251039505005]
[2025-05-27 05:26:16,479]: Mean: -0.00209211
[2025-05-27 05:26:16,480]: Min: -0.40713757
[2025-05-27 05:26:16,480]: Max: 0.46530008
[2025-05-27 05:26:16,480]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,480]: Sample Values (25 elements): [0.7755650877952576, 0.7221183776855469, 0.7832357287406921, 0.6631098389625549, 0.681982696056366, 0.7500975131988525, 0.7898572087287903, 0.7393988966941833, 0.7745267152786255, 0.7027575373649597, 0.7014841437339783, 0.6881691813468933, 0.8717262744903564, 0.7392802834510803, 0.7054364085197449, 0.713001549243927, 0.7265307307243347, 0.7796159982681274, 0.815160870552063, 0.6982731819152832, 0.6619959473609924, 0.6269720792770386, 0.7743362784385681, 0.6887611150741577, 0.6407794952392578]
[2025-05-27 05:26:16,480]: Mean: 0.75027126
[2025-05-27 05:26:16,480]: Min: 0.62697208
[2025-05-27 05:26:16,481]: Max: 0.95751339
[2025-05-27 05:26:16,482]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:16,482]: Sample Values (25 elements): [0.050156932324171066, -0.050156932324171066, -0.1504707932472229, 0.0, -0.050156932324171066, -0.050156932324171066, -0.050156932324171066, -0.1504707932472229, -0.050156932324171066, 0.050156932324171066, -0.10031386464834213, 0.050156932324171066, -0.050156932324171066, -0.10031386464834213, -0.1504707932472229, 0.0, 0.0, 0.050156932324171066, -0.10031386464834213, -0.050156932324171066, 0.0, 0.0, 0.0, -0.050156932324171066, -0.10031386464834213]
[2025-05-27 05:26:16,482]: Mean: -0.00445731
[2025-05-27 05:26:16,483]: Min: -0.35109854
[2025-05-27 05:26:16,483]: Max: 0.40125546
[2025-05-27 05:26:16,483]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,483]: Sample Values (25 elements): [0.7886272668838501, 1.0803840160369873, 0.9120619297027588, 0.8533389568328857, 0.8707494735717773, 0.7448299527168274, 1.0285884141921997, 1.0451370477676392, 0.9102141261100769, 0.9599177241325378, 0.9941104054450989, 0.8324413895606995, 0.8187065124511719, 0.9635018706321716, 1.1260132789611816, 0.9743577837944031, 0.9334464073181152, 0.7808241248130798, 0.9824281334877014, 1.0079580545425415, 0.9505922198295593, 1.072288155555725, 0.8940231800079346, 0.9217848181724548, 1.1162638664245605]
[2025-05-27 05:26:16,483]: Mean: 0.95559847
[2025-05-27 05:26:16,483]: Min: 0.74482995
[2025-05-27 05:26:16,484]: Max: 1.25624752
[2025-05-27 05:26:16,485]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 05:26:16,485]: Sample Values (25 elements): [0.05707202106714249, -0.28536009788513184, -0.17121607065200806, 0.0, -0.11414404213428497, 0.05707202106714249, -0.05707202106714249, -0.17121607065200806, 0.11414404213428497, -0.22828808426856995, 0.17121607065200806, 0.05707202106714249, 0.17121607065200806, -0.05707202106714249, -0.22828808426856995, 0.0, -0.05707202106714249, -0.05707202106714249, -0.22828808426856995, -0.05707202106714249, 0.22828808426856995, 0.22828808426856995, -0.17121607065200806, -0.17121607065200806, -0.17121607065200806]
[2025-05-27 05:26:16,485]: Mean: -0.00261952
[2025-05-27 05:26:16,485]: Min: -0.39950415
[2025-05-27 05:26:16,485]: Max: 0.45657617
[2025-05-27 05:26:16,485]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,486]: Sample Values (25 elements): [0.504533588886261, 0.5269421935081482, 0.5395678281784058, 0.5663886070251465, 0.41088375449180603, 0.6326958537101746, 0.4475032687187195, 0.538033127784729, 0.5403592586517334, 0.4751061797142029, 0.5878909826278687, 0.4573836922645569, 0.49841004610061646, 0.43647927045822144, 0.5903540849685669, 0.7226887941360474, 0.5499216914176941, 0.590609610080719, 0.5053953528404236, 0.5210022926330566, 0.6924837231636047, 0.5811144709587097, 0.6394954919815063, 0.58616703748703, 0.5221741199493408]
[2025-05-27 05:26:16,486]: Mean: 0.51940817
[2025-05-27 05:26:16,486]: Min: 0.33615038
[2025-05-27 05:26:16,486]: Max: 0.72268879
[2025-05-27 05:26:16,487]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:16,488]: Sample Values (25 elements): [0.0, 0.15447299182415009, 0.10298199206590652, -0.05149099603295326, 0.05149099603295326, -0.05149099603295326, 0.15447299182415009, 0.0, 0.0, 0.15447299182415009, 0.10298199206590652, 0.05149099603295326, 0.0, 0.0, 0.05149099603295326, 0.05149099603295326, 0.15447299182415009, -0.15447299182415009, 0.10298199206590652, 0.05149099603295326, 0.15447299182415009, -0.15447299182415009, 0.10298199206590652, 0.0, -0.05149099603295326]
[2025-05-27 05:26:16,488]: Mean: -0.00697972
[2025-05-27 05:26:16,488]: Min: -0.36043698
[2025-05-27 05:26:16,488]: Max: 0.41192797
[2025-05-27 05:26:16,488]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,489]: Sample Values (25 elements): [0.7624701857566833, 0.5532273650169373, 0.6975963711738586, 0.8230755925178528, 0.7274640202522278, 0.7976370453834534, 0.7448565363883972, 0.516974687576294, 0.7355560660362244, 0.612883448600769, 0.5861701369285583, 0.6989860534667969, 0.659014880657196, 0.7118922472000122, 0.6509074568748474, 0.6638368964195251, 0.7222928404808044, 0.6988422870635986, 0.7033506631851196, 0.6246058344841003, 0.7050671577453613, 0.7253011465072632, 0.782342255115509, 0.6979551911354065, 0.7592736482620239]
[2025-05-27 05:26:16,489]: Mean: 0.69888341
[2025-05-27 05:26:16,489]: Min: 0.51697469
[2025-05-27 05:26:16,489]: Max: 0.84363180
[2025-05-27 05:26:16,490]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:16,491]: Sample Values (25 elements): [-0.10673975944519043, 0.0, -0.03557991981506348, -0.03557991981506348, -0.07115983963012695, -0.1423196792602539, 0.07115983963012695, 0.07115983963012695, 0.0, -0.03557991981506348, 0.10673975944519043, -0.03557991981506348, -0.10673975944519043, 0.03557991981506348, 0.03557991981506348, 0.07115983963012695, 0.0, 0.10673975944519043, 0.0, -0.07115983963012695, 0.07115983963012695, 0.07115983963012695, 0.03557991981506348, 0.03557991981506348, 0.10673975944519043]
[2025-05-27 05:26:16,491]: Mean: -0.00304607
[2025-05-27 05:26:16,491]: Min: -0.28463936
[2025-05-27 05:26:16,492]: Max: 0.24905944
[2025-05-27 05:26:16,492]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,492]: Sample Values (25 elements): [1.1536918878555298, 1.0636374950408936, 0.930938720703125, 1.0500702857971191, 1.053652286529541, 1.0304853916168213, 0.990043580532074, 0.685102105140686, 0.9563798308372498, 0.9551333785057068, 1.0558727979660034, 1.1303820610046387, 0.990804135799408, 0.9459378719329834, 0.9531289935112, 1.0357460975646973, 1.036894679069519, 0.7905057668685913, 1.1671165227890015, 0.9328924417495728, 1.0003546476364136, 1.0136719942092896, 0.8672624230384827, 0.9479706883430481, 1.027087926864624]
[2025-05-27 05:26:16,492]: Mean: 0.99213529
[2025-05-27 05:26:16,492]: Min: 0.61938763
[2025-05-27 05:26:16,492]: Max: 1.25331044
[2025-05-27 05:26:16,493]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:16,494]: Sample Values (25 elements): [0.03940820321440697, -0.03940820321440697, 0.15763281285762787, 0.0, 0.15763281285762787, 0.0, 0.1182246059179306, 0.0, 0.0, 0.0, 0.07881640642881393, 0.07881640642881393, -0.03940820321440697, 0.0, 0.0, 0.0, -0.03940820321440697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03940820321440697, 0.03940820321440697, 0.0]
[2025-05-27 05:26:16,494]: Mean: -0.00161421
[2025-05-27 05:26:16,494]: Min: -0.27585742
[2025-05-27 05:26:16,494]: Max: 0.31526563
[2025-05-27 05:26:16,495]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,495]: Sample Values (25 elements): [0.6947941184043884, 0.6137576103210449, 0.8246535658836365, 0.7495390176773071, 0.5665889382362366, 0.6751211285591125, 0.6075332164764404, 0.5935471057891846, -5.520555430054049e-41, 0.7659944891929626, -5.599308403749104e-41, 0.5419619679450989, 0.5272542834281921, 0.7175789475440979, 0.2522088289260864, 0.40282556414604187, 0.06299290060997009, -5.78147720411133e-41, 0.6890550255775452, 0.6494778990745544, 0.5071576237678528, 0.7241491675376892, 0.7000654339790344, 0.49437206983566284, 0.517979085445404]
[2025-05-27 05:26:16,495]: Mean: 0.46210390
[2025-05-27 05:26:16,495]: Min: -0.00000000
[2025-05-27 05:26:16,495]: Max: 0.82465357
[2025-05-27 05:26:16,496]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 05:26:16,497]: Sample Values (25 elements): [-0.025050239637494087, -0.025050239637494087, -0.050100479274988174, 0.12525120377540588, -0.07515072077512741, -0.025050239637494087, 0.050100479274988174, 0.07515072077512741, 0.0, 0.025050239637494087, 0.0, 0.025050239637494087, 0.0, 0.0, -0.025050239637494087, 0.0, -0.025050239637494087, 0.050100479274988174, -0.025050239637494087, -0.025050239637494087, -0.025050239637494087, 0.0, 0.0, 0.0, 0.050100479274988174]
[2025-05-27 05:26:16,497]: Mean: 0.00220780
[2025-05-27 05:26:16,497]: Min: -0.17535168
[2025-05-27 05:26:16,497]: Max: 0.20040192
[2025-05-27 05:26:16,497]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 05:26:16,498]: Sample Values (25 elements): [0.9346516728401184, 0.8550020456314087, 1.0789276361465454, 0.9651936292648315, 0.9252064824104309, 0.880818247795105, 0.9891934990882874, 0.905049204826355, 1.042008399963379, 0.9485155344009399, 0.849452793598175, 0.9494503736495972, 1.1037548780441284, 1.2235896587371826, 0.9629021883010864, 0.9819945693016052, 1.0036596059799194, 1.0203646421432495, 0.9494932889938354, 1.0377449989318848, 1.045357346534729, 0.7604392766952515, 0.9770050644874573, 0.7580283880233765, 0.8639510869979858]
[2025-05-27 05:26:16,498]: Mean: 0.99165499
[2025-05-27 05:26:16,498]: Min: 0.74979973
[2025-05-27 05:26:16,498]: Max: 1.24316263
[2025-05-27 05:26:16,498]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 05:26:16,498]: Sample Values (25 elements): [0.26398909091949463, -0.18479034304618835, -0.21583040058612823, -0.12905597686767578, -0.17386366426944733, -0.45709314942359924, -0.15165898203849792, 0.2211569994688034, 0.25348928570747375, 0.15622901916503906, 0.24510560929775238, 0.32965052127838135, -0.37976422905921936, 0.18579787015914917, -0.30278652906417847, -0.30496102571487427, -0.3278322219848633, -0.02195228450000286, -0.3429412543773651, 0.13309790194034576, 0.056623514741659164, 0.25571009516716003, -0.0906435176730156, 0.012325051240622997, -0.5048403143882751]
[2025-05-27 05:26:16,499]: Mean: -0.06862374
[2025-05-27 05:26:16,499]: Min: -0.76736170
[2025-05-27 05:26:16,499]: Max: 0.49914578
[2025-05-27 05:26:16,499]: 


QAT of ResNet20 with relu down to 3 bits...
[2025-05-27 05:26:16,636]: [ResNet20_relu_quantized_3_bits] after configure_qat:
[2025-05-27 05:26:16,691]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 05:28:10,822]: [ResNet20_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.6559 Train Acc: 0.7730 Eval Loss: 0.6450 Eval Acc: 0.7888 (LR: 0.00100000)
[2025-05-27 05:30:04,449]: [ResNet20_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.5970 Train Acc: 0.7923 Eval Loss: 0.7315 Eval Acc: 0.7581 (LR: 0.00100000)
[2025-05-27 05:31:55,687]: [ResNet20_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.5920 Train Acc: 0.7946 Eval Loss: 0.6771 Eval Acc: 0.7766 (LR: 0.00100000)
[2025-05-27 05:33:42,130]: [ResNet20_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.5777 Train Acc: 0.7980 Eval Loss: 0.6053 Eval Acc: 0.7981 (LR: 0.00100000)
[2025-05-27 05:35:23,303]: [ResNet20_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.5815 Train Acc: 0.7974 Eval Loss: 0.6131 Eval Acc: 0.7947 (LR: 0.00100000)
[2025-05-27 05:37:03,814]: [ResNet20_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.5818 Train Acc: 0.7983 Eval Loss: 0.6738 Eval Acc: 0.7788 (LR: 0.00100000)
[2025-05-27 05:38:44,457]: [ResNet20_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.5867 Train Acc: 0.7943 Eval Loss: 0.6597 Eval Acc: 0.7768 (LR: 0.00100000)
[2025-05-27 05:40:24,644]: [ResNet20_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.5751 Train Acc: 0.7992 Eval Loss: 0.7151 Eval Acc: 0.7614 (LR: 0.00100000)
[2025-05-27 05:42:10,386]: [ResNet20_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.5782 Train Acc: 0.7973 Eval Loss: 0.5842 Eval Acc: 0.8018 (LR: 0.00100000)
[2025-05-27 05:44:01,995]: [ResNet20_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.5708 Train Acc: 0.7999 Eval Loss: 0.6072 Eval Acc: 0.7918 (LR: 0.00100000)
[2025-05-27 05:45:51,940]: [ResNet20_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.5712 Train Acc: 0.7998 Eval Loss: 0.6928 Eval Acc: 0.7668 (LR: 0.00100000)
[2025-05-27 05:47:45,950]: [ResNet20_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.5644 Train Acc: 0.8036 Eval Loss: 0.6950 Eval Acc: 0.7664 (LR: 0.00100000)
[2025-05-27 05:49:43,195]: [ResNet20_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.5653 Train Acc: 0.8015 Eval Loss: 0.6007 Eval Acc: 0.7941 (LR: 0.00100000)
[2025-05-27 05:51:41,238]: [ResNet20_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.5583 Train Acc: 0.8055 Eval Loss: 0.6769 Eval Acc: 0.7770 (LR: 0.00100000)
[2025-05-27 05:53:40,386]: [ResNet20_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.5583 Train Acc: 0.8035 Eval Loss: 0.6407 Eval Acc: 0.7843 (LR: 0.00010000)
[2025-05-27 05:55:37,524]: [ResNet20_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.4833 Train Acc: 0.8314 Eval Loss: 0.4798 Eval Acc: 0.8344 (LR: 0.00010000)
[2025-05-27 05:57:32,728]: [ResNet20_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.4657 Train Acc: 0.8364 Eval Loss: 0.4718 Eval Acc: 0.8380 (LR: 0.00010000)
[2025-05-27 05:59:28,225]: [ResNet20_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.4579 Train Acc: 0.8396 Eval Loss: 0.4615 Eval Acc: 0.8436 (LR: 0.00010000)
[2025-05-27 06:01:23,604]: [ResNet20_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.4505 Train Acc: 0.8424 Eval Loss: 0.4633 Eval Acc: 0.8452 (LR: 0.00010000)
[2025-05-27 06:03:19,815]: [ResNet20_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.4481 Train Acc: 0.8450 Eval Loss: 0.4652 Eval Acc: 0.8408 (LR: 0.00010000)
[2025-05-27 06:05:15,532]: [ResNet20_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.4548 Train Acc: 0.8410 Eval Loss: 0.4542 Eval Acc: 0.8429 (LR: 0.00010000)
[2025-05-27 06:07:11,194]: [ResNet20_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.4477 Train Acc: 0.8433 Eval Loss: 0.4829 Eval Acc: 0.8384 (LR: 0.00010000)
[2025-05-27 06:09:07,849]: [ResNet20_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.4429 Train Acc: 0.8441 Eval Loss: 0.4634 Eval Acc: 0.8421 (LR: 0.00010000)
[2025-05-27 06:11:00,810]: [ResNet20_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.4501 Train Acc: 0.8421 Eval Loss: 0.4687 Eval Acc: 0.8435 (LR: 0.00010000)
[2025-05-27 06:12:55,658]: [ResNet20_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.4391 Train Acc: 0.8458 Eval Loss: 0.4689 Eval Acc: 0.8441 (LR: 0.00010000)
[2025-05-27 06:14:49,649]: [ResNet20_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.4409 Train Acc: 0.8456 Eval Loss: 0.4725 Eval Acc: 0.8390 (LR: 0.00010000)
[2025-05-27 06:16:43,335]: [ResNet20_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.4403 Train Acc: 0.8449 Eval Loss: 0.4898 Eval Acc: 0.8367 (LR: 0.00001000)
[2025-05-27 06:18:36,194]: [ResNet20_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.4207 Train Acc: 0.8532 Eval Loss: 0.4523 Eval Acc: 0.8453 (LR: 0.00001000)
[2025-05-27 06:20:26,729]: [ResNet20_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.4184 Train Acc: 0.8548 Eval Loss: 0.4442 Eval Acc: 0.8499 (LR: 0.00001000)
[2025-05-27 06:22:08,683]: [ResNet20_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.4168 Train Acc: 0.8547 Eval Loss: 0.4411 Eval Acc: 0.8509 (LR: 0.00001000)
[2025-05-27 06:23:49,281]: [ResNet20_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.4171 Train Acc: 0.8535 Eval Loss: 0.4494 Eval Acc: 0.8498 (LR: 0.00001000)
[2025-05-27 06:25:30,061]: [ResNet20_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.4193 Train Acc: 0.8526 Eval Loss: 0.4432 Eval Acc: 0.8483 (LR: 0.00001000)
[2025-05-27 06:27:10,716]: [ResNet20_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.4138 Train Acc: 0.8564 Eval Loss: 0.4312 Eval Acc: 0.8519 (LR: 0.00001000)
[2025-05-27 06:28:52,930]: [ResNet20_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.4130 Train Acc: 0.8555 Eval Loss: 0.4466 Eval Acc: 0.8510 (LR: 0.00001000)
[2025-05-27 06:30:42,679]: [ResNet20_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.4160 Train Acc: 0.8544 Eval Loss: 0.4352 Eval Acc: 0.8515 (LR: 0.00001000)
[2025-05-27 06:32:35,177]: [ResNet20_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.4135 Train Acc: 0.8539 Eval Loss: 0.4414 Eval Acc: 0.8509 (LR: 0.00001000)
[2025-05-27 06:34:28,084]: [ResNet20_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.4158 Train Acc: 0.8541 Eval Loss: 0.4374 Eval Acc: 0.8526 (LR: 0.00001000)
[2025-05-27 06:36:21,775]: [ResNet20_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.4124 Train Acc: 0.8545 Eval Loss: 0.4390 Eval Acc: 0.8502 (LR: 0.00001000)
[2025-05-27 06:38:12,979]: [ResNet20_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.4149 Train Acc: 0.8558 Eval Loss: 0.4282 Eval Acc: 0.8584 (LR: 0.00001000)
[2025-05-27 06:40:05,920]: [ResNet20_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.4170 Train Acc: 0.8544 Eval Loss: 0.4421 Eval Acc: 0.8506 (LR: 0.00001000)
[2025-05-27 06:42:00,820]: [ResNet20_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.4192 Train Acc: 0.8537 Eval Loss: 0.4594 Eval Acc: 0.8488 (LR: 0.00001000)
[2025-05-27 06:43:57,863]: [ResNet20_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.4104 Train Acc: 0.8560 Eval Loss: 0.4418 Eval Acc: 0.8497 (LR: 0.00001000)
[2025-05-27 06:45:54,899]: [ResNet20_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.4186 Train Acc: 0.8546 Eval Loss: 0.4543 Eval Acc: 0.8468 (LR: 0.00001000)
[2025-05-27 06:47:51,207]: [ResNet20_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.4171 Train Acc: 0.8527 Eval Loss: 0.4472 Eval Acc: 0.8477 (LR: 0.00001000)
[2025-05-27 06:49:47,283]: [ResNet20_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.4116 Train Acc: 0.8565 Eval Loss: 0.4565 Eval Acc: 0.8436 (LR: 0.00000100)
[2025-05-27 06:51:43,581]: [ResNet20_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.4121 Train Acc: 0.8562 Eval Loss: 0.4335 Eval Acc: 0.8566 (LR: 0.00000100)
[2025-05-27 06:53:39,864]: [ResNet20_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.4088 Train Acc: 0.8577 Eval Loss: 0.4370 Eval Acc: 0.8514 (LR: 0.00000100)
[2025-05-27 06:55:35,392]: [ResNet20_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.4120 Train Acc: 0.8555 Eval Loss: 0.4330 Eval Acc: 0.8514 (LR: 0.00000100)
[2025-05-27 06:57:31,935]: [ResNet20_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.4069 Train Acc: 0.8559 Eval Loss: 0.4310 Eval Acc: 0.8559 (LR: 0.00000100)
[2025-05-27 06:59:28,472]: [ResNet20_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.4077 Train Acc: 0.8573 Eval Loss: 0.4401 Eval Acc: 0.8526 (LR: 0.00000100)
[2025-05-27 07:01:24,350]: [ResNet20_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.4060 Train Acc: 0.8590 Eval Loss: 0.4307 Eval Acc: 0.8516 (LR: 0.00000010)
[2025-05-27 07:03:20,132]: [ResNet20_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.4085 Train Acc: 0.8567 Eval Loss: 0.4348 Eval Acc: 0.8536 (LR: 0.00000010)
[2025-05-27 07:05:13,812]: [ResNet20_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.4043 Train Acc: 0.8590 Eval Loss: 0.4419 Eval Acc: 0.8525 (LR: 0.00000010)
[2025-05-27 07:07:05,984]: [ResNet20_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.4062 Train Acc: 0.8582 Eval Loss: 0.4295 Eval Acc: 0.8575 (LR: 0.00000010)
[2025-05-27 07:08:50,216]: [ResNet20_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.4063 Train Acc: 0.8568 Eval Loss: 0.4297 Eval Acc: 0.8579 (LR: 0.00000010)
[2025-05-27 07:10:13,294]: [ResNet20_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.4065 Train Acc: 0.8585 Eval Loss: 0.4274 Eval Acc: 0.8573 (LR: 0.00000010)
[2025-05-27 07:11:31,994]: [ResNet20_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.4021 Train Acc: 0.8589 Eval Loss: 0.4321 Eval Acc: 0.8544 (LR: 0.00000010)
[2025-05-27 07:12:50,646]: [ResNet20_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.4089 Train Acc: 0.8574 Eval Loss: 0.4319 Eval Acc: 0.8535 (LR: 0.00000010)
[2025-05-27 07:14:09,585]: [ResNet20_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.4015 Train Acc: 0.8602 Eval Loss: 0.4383 Eval Acc: 0.8547 (LR: 0.00000010)
[2025-05-27 07:15:35,702]: [ResNet20_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.4039 Train Acc: 0.8596 Eval Loss: 0.4400 Eval Acc: 0.8551 (LR: 0.00000010)
[2025-05-27 07:15:35,702]: [ResNet20_relu_quantized_3_bits] Best Eval Accuracy: 0.8584
[2025-05-27 07:15:35,761]: 


Quantization of model down to 3 bits finished
[2025-05-27 07:15:35,761]: Model Architecture:
[2025-05-27 07:15:35,922]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.0725], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=42.50728988647461)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1370], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5069156885147095, max_val=0.4522843360900879)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.7702], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.391313552856445)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1495], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5288907289505005, max_val=0.5177872180938721)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.1503], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=50.05208969116211)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1837], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.826680064201355, max_val=0.4595617651939392)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9432], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.602543830871582)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1473], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4590533375740051, max_val=0.5718809366226196)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.5460], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=52.82231521606445)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1993], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.907672643661499, max_val=0.48723506927490234)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.3847856521606445)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1490], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.521492600440979, max_val=0.5216543674468994)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([7.8292], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=54.804412841796875)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1611], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5640069246292114, max_val=0.5636841058731079)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2718], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.902373313903809)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1422], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5398049354553223, max_val=0.4556952714920044)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2295], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6466010808944702, max_val=0.9599952697753906)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9518], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.662322998046875)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1453], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47275447845458984, max_val=0.5442368984222412)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8259], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.781387805938721)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1243], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39989590644836426, max_val=0.47000470757484436)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.9100], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=20.369997024536133)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1548], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5242791175842285, max_val=0.5590047836303711)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8446], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.912018775939941)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1066], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36679601669311523, max_val=0.3791191577911377)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.1548], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=29.083541870117188)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1180], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3662760257720947, max_val=0.4598223567008972)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9163], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.414131164550781)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1207], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39421290159225464, max_val=0.4505354166030884)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1441], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4722122550010681, max_val=0.5366953611373901)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=10.631930351257324)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1208], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38639068603515625, max_val=0.4589238166809082)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8137], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.696021556854248)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0927], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33208656311035156, max_val=0.3169158697128296)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3608], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.525848388671875)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0865], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28812623023986816, max_val=0.31732481718063354)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8561], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.992973804473877)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0715], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.247701495885849, max_val=0.2528493404388428)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.2509], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=22.756576538085938)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 07:15:35,922]: 
Model Weights:
[2025-05-27 07:15:35,922]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 07:15:35,923]: Sample Values (25 elements): [0.16097381711006165, -0.0696251168847084, 0.09583132714033127, -0.21589155495166779, -0.004175408277660608, 0.13175827264785767, 0.18906110525131226, -0.1415502429008484, 0.12179002165794373, -0.048017147928476334, -0.021168004721403122, 0.1223987564444542, 0.03682776167988777, 0.011395005509257317, -0.17430877685546875, 0.20469044148921967, -0.001385662704706192, 0.2165142297744751, -0.04877079650759697, 0.25784340500831604, -0.5307818651199341, -0.07459180802106857, 0.02345326915383339, 0.18470387160778046, -0.15412554144859314]
[2025-05-27 07:15:35,923]: Mean: -0.00050665
[2025-05-27 07:15:35,923]: Min: -0.65706015
[2025-05-27 07:15:35,923]: Max: 0.57726020
[2025-05-27 07:15:35,924]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,924]: Sample Values (16 elements): [2.435058355331421, 2.8410792350769043, 2.0227112770080566, 2.742746353149414, 2.1139743328094482, 2.9122862815856934, 2.458811044692993, 1.9121698141098022, 2.3963534832000732, 2.9229488372802734, 2.4743244647979736, 3.0209543704986572, 3.1387743949890137, 2.475688934326172, 2.4682178497314453, 4.015613079071045]
[2025-05-27 07:15:35,924]: Mean: 2.64698195
[2025-05-27 07:15:35,924]: Min: 1.91216981
[2025-05-27 07:15:35,925]: Max: 4.01561308
[2025-05-27 07:15:35,927]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:15:35,927]: Sample Values (25 elements): [-0.13702857494354248, 0.0, -0.13702857494354248, 0.0, 0.13702857494354248, 0.0, 0.13702857494354248, -0.13702857494354248, 0.13702857494354248, 0.0, 0.27405714988708496, 0.13702857494354248, 0.0, 0.13702857494354248, 0.0, 0.13702857494354248, -0.27405714988708496, 0.0, 0.0, 0.0, 0.0, 0.27405714988708496, -0.27405714988708496, -0.27405714988708496, 0.0]
[2025-05-27 07:15:35,928]: Mean: -0.00838586
[2025-05-27 07:15:35,928]: Min: -0.54811430
[2025-05-27 07:15:35,928]: Max: 0.41108572
[2025-05-27 07:15:35,928]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,929]: Sample Values (16 elements): [1.3816957473754883, 1.1130834817886353, 1.4568372964859009, 1.032423734664917, 1.2377474308013916, 1.0808335542678833, 1.1233329772949219, 1.2654637098312378, 1.3314495086669922, 1.395113468170166, 1.234304666519165, 1.350075364112854, 1.3494607210159302, 1.6958285570144653, 1.059969425201416, 1.165979027748108]
[2025-05-27 07:15:35,929]: Mean: 1.26709986
[2025-05-27 07:15:35,929]: Min: 1.03242373
[2025-05-27 07:15:35,930]: Max: 1.69582856
[2025-05-27 07:15:35,932]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:15:35,933]: Sample Values (25 elements): [0.0, 0.0, -0.14952543377876282, 0.14952543377876282, -0.14952543377876282, 0.14952543377876282, 0.0, -0.14952543377876282, 0.0, 0.0, 0.0, -0.14952543377876282, 0.0, 0.0, 0.0, 0.0, 0.29905086755752563, -0.14952543377876282, 0.14952543377876282, 0.0, 0.0, 0.0, 0.0, 0.14952543377876282, 0.0]
[2025-05-27 07:15:35,933]: Mean: -0.00700900
[2025-05-27 07:15:35,933]: Min: -0.59810174
[2025-05-27 07:15:35,934]: Max: 0.44857630
[2025-05-27 07:15:35,934]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,935]: Sample Values (16 elements): [1.9179943799972534, 2.543471097946167, 2.054408311843872, 2.1446197032928467, 1.8804986476898193, 2.1056649684906006, 1.7893743515014648, 2.1720645427703857, 1.7725671529769897, 1.815966010093689, 1.9790068864822388, 2.1448540687561035, 1.6056182384490967, 1.5868439674377441, 1.9441289901733398, 2.0632336139678955]
[2025-05-27 07:15:35,935]: Mean: 1.97001970
[2025-05-27 07:15:35,935]: Min: 1.58684397
[2025-05-27 07:15:35,936]: Max: 2.54347110
[2025-05-27 07:15:35,938]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:15:35,939]: Sample Values (25 elements): [0.0, 0.0, 0.36749765276908875, -0.18374882638454437, 0.0, -0.36749765276908875, 0.0, 0.36749765276908875, -0.18374882638454437, 0.0, 0.0, 0.18374882638454437, 0.18374882638454437, 0.0, -0.18374882638454437, 0.0, 0.0, 0.18374882638454437, -0.18374882638454437, 0.0, 0.0, 0.18374882638454437, 0.0, 0.0, 0.0]
[2025-05-27 07:15:35,939]: Mean: -0.00949050
[2025-05-27 07:15:35,939]: Min: -0.73499531
[2025-05-27 07:15:35,940]: Max: 0.55124646
[2025-05-27 07:15:35,940]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,940]: Sample Values (16 elements): [0.9131274819374084, 0.9015269875526428, 0.9640936851501465, 0.7993338108062744, 1.2092039585113525, 0.9432476758956909, 1.090929627418518, 1.0149823427200317, 0.7601678371429443, 0.7767808437347412, 0.9735886454582214, 0.8084639310836792, 0.9796827435493469, 1.373666763305664, 0.7781286239624023, 1.1265350580215454]
[2025-05-27 07:15:35,941]: Mean: 0.96334124
[2025-05-27 07:15:35,941]: Min: 0.76016784
[2025-05-27 07:15:35,941]: Max: 1.37366676
[2025-05-27 07:15:35,943]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:15:35,943]: Sample Values (25 elements): [0.14727634191513062, 0.0, 0.14727634191513062, 0.0, 0.14727634191513062, 0.14727634191513062, -0.14727634191513062, 0.29455268383026123, 0.14727634191513062, 0.14727634191513062, 0.0, 0.0, -0.14727634191513062, 0.14727634191513062, 0.14727634191513062, -0.14727634191513062, 0.0, -0.14727634191513062, 0.0, -0.14727634191513062, 0.0, 0.0, -0.14727634191513062, 0.0, -0.14727634191513062]
[2025-05-27 07:15:35,944]: Mean: -0.00127844
[2025-05-27 07:15:35,944]: Min: -0.44182903
[2025-05-27 07:15:35,944]: Max: 0.58910537
[2025-05-27 07:15:35,945]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,945]: Sample Values (16 elements): [1.7572643756866455, 2.127157688140869, 2.172846555709839, 1.7794201374053955, 1.8695374727249146, 1.468858242034912, 2.025683879852295, 1.8577439785003662, 1.730705976486206, 1.9623477458953857, 2.178345203399658, 1.4852304458618164, 1.528331995010376, 2.171736001968384, 1.9823654890060425, 1.802977442741394]
[2025-05-27 07:15:35,945]: Mean: 1.86878443
[2025-05-27 07:15:35,946]: Min: 1.46885824
[2025-05-27 07:15:35,946]: Max: 2.17834520
[2025-05-27 07:15:35,948]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:15:35,953]: Sample Values (25 elements): [-0.1992725431919098, -0.7970901727676392, 0.0, -0.3985450863838196, 0.0, 0.0, 0.0, 0.0, -0.1992725431919098, 0.0, -0.1992725431919098, 0.0, 0.0, -0.1992725431919098, -0.3985450863838196, 0.0, 0.1992725431919098, -0.1992725431919098, -0.1992725431919098, 0.0, 0.0, -0.1992725431919098, -0.1992725431919098, 0.0, 0.1992725431919098]
[2025-05-27 07:15:35,956]: Mean: -0.01392486
[2025-05-27 07:15:35,960]: Min: -0.99636269
[2025-05-27 07:15:35,972]: Max: 0.39854509
[2025-05-27 07:15:35,972]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,984]: Sample Values (16 elements): [0.647379994392395, 1.257717251777649, 1.0655710697174072, 1.1587729454040527, 1.1426950693130493, 1.1745991706848145, 0.9560751914978027, 0.7304286956787109, 0.7962132692337036, 0.6432583928108215, 1.3275434970855713, 0.7333601713180542, 1.178443431854248, 0.70698082447052, 0.803769052028656, 1.1275731325149536]
[2025-05-27 07:15:35,984]: Mean: 0.96564883
[2025-05-27 07:15:35,984]: Min: 0.64325839
[2025-05-27 07:15:35,985]: Max: 1.32754350
[2025-05-27 07:15:35,987]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 07:15:35,987]: Sample Values (25 elements): [-0.1490209996700287, -0.44706299901008606, -0.1490209996700287, -0.1490209996700287, 0.1490209996700287, 0.0, 0.1490209996700287, -0.1490209996700287, 0.0, 0.0, -0.1490209996700287, 0.0, 0.1490209996700287, -0.1490209996700287, 0.0, 0.1490209996700287, -0.2980419993400574, 0.0, 0.2980419993400574, -0.1490209996700287, 0.0, 0.0, 0.0, -0.2980419993400574, 0.1490209996700287]
[2025-05-27 07:15:35,987]: Mean: 0.01054272
[2025-05-27 07:15:35,988]: Min: -0.44706300
[2025-05-27 07:15:35,988]: Max: 0.59608400
[2025-05-27 07:15:35,988]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 07:15:35,988]: Sample Values (16 elements): [1.7797983884811401, 1.6446576118469238, 1.837546467781067, 1.6363931894302368, 1.8075474500656128, 1.758486032485962, 1.9638807773590088, 1.4354934692382812, 1.3797402381896973, 2.0519821643829346, 2.00740385055542, 1.5987108945846558, 2.2350268363952637, 2.034165620803833, 1.3348592519760132, 2.0847606658935547]
[2025-05-27 07:15:35,989]: Mean: 1.78690338
[2025-05-27 07:15:35,989]: Min: 1.33485925
[2025-05-27 07:15:35,989]: Max: 2.23502684
[2025-05-27 07:15:35,992]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 07:15:35,993]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.16109871864318848, -0.16109871864318848, 0.0, 0.16109871864318848, 0.0, 0.0, 0.0, -0.16109871864318848, 0.0, -0.16109871864318848, 0.0, -0.16109871864318848, 0.0, -0.16109871864318848, 0.0, 0.0, 0.0, 0.0, 0.16109871864318848, 0.16109871864318848, 0.16109871864318848]
[2025-05-27 07:15:35,993]: Mean: -0.00412536
[2025-05-27 07:15:35,994]: Min: -0.64439487
[2025-05-27 07:15:35,995]: Max: 0.48329616
[2025-05-27 07:15:35,995]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 07:15:35,996]: Sample Values (25 elements): [1.0835819244384766, 1.1983890533447266, 1.1204105615615845, 1.1794263124465942, 1.1765824556350708, 1.0938254594802856, 1.1532492637634277, 1.2752846479415894, 1.1422972679138184, 1.1166889667510986, 1.1411876678466797, 1.2480766773223877, 0.9634074568748474, 1.1264466047286987, 1.191049575805664, 1.2844417095184326, 1.0306917428970337, 1.1661136150360107, 1.1535617113113403, 1.0937210321426392, 1.0750889778137207, 1.0064904689788818, 0.9574950933456421, 1.2820310592651367, 1.2020516395568848]
[2025-05-27 07:15:35,996]: Mean: 1.14159966
[2025-05-27 07:15:35,996]: Min: 0.95749509
[2025-05-27 07:15:35,997]: Max: 1.43509328
[2025-05-27 07:15:35,998]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:15:36,000]: Sample Values (25 elements): [0.0, -0.1422143280506134, 0.0, -0.1422143280506134, -0.1422143280506134, 0.1422143280506134, -0.1422143280506134, 0.1422143280506134, 0.0, 0.0, 0.0, 0.2844286561012268, 0.1422143280506134, 0.1422143280506134, 0.0, 0.0, -0.1422143280506134, 0.0, -0.1422143280506134, 0.0, 0.0, -0.1422143280506134, -0.1422143280506134, 0.0, -0.1422143280506134]
[2025-05-27 07:15:36,000]: Mean: -0.00373436
[2025-05-27 07:15:36,000]: Min: -0.56885731
[2025-05-27 07:15:36,001]: Max: 0.42664298
[2025-05-27 07:15:36,001]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 07:15:36,001]: Sample Values (25 elements): [1.1956157684326172, 1.246720790863037, 1.5069785118103027, 1.29398775100708, 1.0041511058807373, 1.4641350507736206, 1.3433295488357544, 1.1753567457199097, 1.3459023237228394, 1.4696269035339355, 1.3352446556091309, 1.4425092935562134, 1.3122001886367798, 1.2313969135284424, 1.3785496950149536, 1.0951509475708008, 1.375649333000183, 1.3756307363510132, 1.4933661222457886, 1.802690029144287, 1.170151710510254, 1.328911304473877, 1.2838637828826904, 1.4247393608093262, 1.45473313331604]
[2025-05-27 07:15:36,001]: Mean: 1.34132504
[2025-05-27 07:15:36,002]: Min: 1.00415111
[2025-05-27 07:15:36,002]: Max: 1.80269003
[2025-05-27 07:15:36,004]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 07:15:36,005]: Sample Values (25 elements): [0.22951377928256989, -0.22951377928256989, 0.0, 0.22951377928256989, 0.0, -0.22951377928256989, 0.45902755856513977, 0.22951377928256989, 0.22951377928256989, 0.45902755856513977, 0.22951377928256989, 0.0, -0.22951377928256989, 0.0, 0.0, -0.22951377928256989, 0.0, 0.45902755856513977, -0.45902755856513977, -0.45902755856513977, 0.22951377928256989, -0.22951377928256989, 0.22951377928256989, -0.22951377928256989, 0.0]
[2025-05-27 07:15:36,005]: Mean: -0.01075846
[2025-05-27 07:15:36,006]: Min: -0.68854135
[2025-05-27 07:15:36,006]: Max: 0.91805512
[2025-05-27 07:15:36,006]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 07:15:36,006]: Sample Values (25 elements): [0.6394063234329224, 0.86222904920578, 0.7194942235946655, 0.6569644808769226, 0.8232568502426147, 0.5503290891647339, 0.6741688847541809, 0.7445949912071228, 0.6109869480133057, 0.6405059695243835, 0.4665544331073761, 0.8362502455711365, 0.8008004426956177, 1.058655858039856, 0.7344072461128235, 0.6479613184928894, 0.7034575343132019, 0.6922734379768372, 0.5265979170799255, 0.8199753761291504, 1.018762469291687, 0.9747713804244995, 0.5754927396774292, 0.7512170076370239, 0.7732784748077393]
[2025-05-27 07:15:36,007]: Mean: 0.73825037
[2025-05-27 07:15:36,007]: Min: 0.46655443
[2025-05-27 07:15:36,008]: Max: 1.05865586
[2025-05-27 07:15:36,010]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:15:36,010]: Sample Values (25 elements): [-0.1452844887971878, 0.0, 0.1452844887971878, 0.0, 0.0, 0.1452844887971878, 0.1452844887971878, 0.0, -0.2905689775943756, 0.2905689775943756, 0.0, 0.0, 0.0, 0.0, -0.1452844887971878, 0.0, -0.1452844887971878, -0.1452844887971878, 0.1452844887971878, 0.0, 0.0, 0.1452844887971878, 0.0, 0.0, 0.0]
[2025-05-27 07:15:36,010]: Mean: -0.01023108
[2025-05-27 07:15:36,011]: Min: -0.43585348
[2025-05-27 07:15:36,011]: Max: 0.58113796
[2025-05-27 07:15:36,011]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 07:15:36,012]: Sample Values (25 elements): [0.9338476657867432, 0.928825855255127, 0.8209467530250549, 0.8840905427932739, 0.9021282196044922, 0.886313259601593, 0.9080771207809448, 1.0067967176437378, 0.8659090995788574, 0.7692322731018066, 0.9497517347335815, 0.8540148735046387, 0.9253793358802795, 0.8435689210891724, 0.866520881652832, 0.8478108644485474, 1.056201696395874, 0.8231194019317627, 0.8878528475761414, 0.7451441884040833, 0.9055526256561279, 1.0631985664367676, 0.9789706468582153, 0.8652992844581604, 0.8478580117225647]
[2025-05-27 07:15:36,013]: Mean: 0.90384686
[2025-05-27 07:15:36,013]: Min: 0.74514419
[2025-05-27 07:15:36,013]: Max: 1.06319857
[2025-05-27 07:15:36,017]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:15:36,018]: Sample Values (25 elements): [0.0, -0.24854303896427155, 0.12427151948213577, -0.12427151948213577, 0.0, -0.12427151948213577, 0.0, -0.12427151948213577, 0.12427151948213577, -0.12427151948213577, -0.12427151948213577, -0.12427151948213577, 0.12427151948213577, 0.0, 0.0, 0.0, 0.0, -0.12427151948213577, 0.0, 0.0, -0.12427151948213577, -0.12427151948213577, 0.0, -0.12427151948213577, 0.0]
[2025-05-27 07:15:36,019]: Mean: -0.00308791
[2025-05-27 07:15:36,019]: Min: -0.37281457
[2025-05-27 07:15:36,019]: Max: 0.49708608
[2025-05-27 07:15:36,019]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 07:15:36,020]: Sample Values (25 elements): [0.9807963967323303, 1.2815179824829102, 1.1079727411270142, 0.8803476691246033, 1.0391088724136353, 1.0857596397399902, 1.4073415994644165, 1.1308192014694214, 1.3729828596115112, 1.1130701303482056, 1.2041205167770386, 1.5362062454223633, 1.2023080587387085, 0.9709218144416809, 1.1957387924194336, 1.1455410718917847, 1.3531185388565063, 1.4696484804153442, 1.0036789178848267, 1.064395546913147, 1.1872457265853882, 1.1906992197036743, 1.2111985683441162, 1.260905385017395, 1.0611107349395752]
[2025-05-27 07:15:36,020]: Mean: 1.20407152
[2025-05-27 07:15:36,021]: Min: 0.88034767
[2025-05-27 07:15:36,021]: Max: 1.67081916
[2025-05-27 07:15:36,023]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:15:36,023]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.1547548472881317, 0.1547548472881317, 0.1547548472881317, 0.0, -0.1547548472881317, -0.1547548472881317, 0.1547548472881317, -0.1547548472881317, 0.1547548472881317, 0.0, -0.1547548472881317, 0.0, 0.0, -0.1547548472881317, 0.1547548472881317, 0.1547548472881317, -0.1547548472881317, -0.1547548472881317, 0.1547548472881317, 0.0, -0.1547548472881317, 0.0]
[2025-05-27 07:15:36,023]: Mean: -0.01081403
[2025-05-27 07:15:36,024]: Min: -0.46426454
[2025-05-27 07:15:36,024]: Max: 0.61901939
[2025-05-27 07:15:36,024]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 07:15:36,025]: Sample Values (25 elements): [0.8108454942703247, 0.7294759750366211, 0.7703276872634888, 0.7934429049491882, 0.6543795466423035, 0.8527259230613708, 0.882999062538147, 0.7703483700752258, 0.6720572710037231, 0.8565340042114258, 0.6765245199203491, 0.8917167782783508, 0.9206578135490417, 0.9580408930778503, 0.927677571773529, 1.0097343921661377, 0.7862927913665771, 0.8775352835655212, 0.9162731170654297, 0.7115616202354431, 0.8995886445045471, 0.7657910585403442, 0.8659218549728394, 0.826752781867981, 0.7282772064208984]
[2025-05-27 07:15:36,025]: Mean: 0.81679440
[2025-05-27 07:15:36,026]: Min: 0.65437955
[2025-05-27 07:15:36,026]: Max: 1.00973439
[2025-05-27 07:15:36,027]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 07:15:36,030]: Sample Values (25 elements): [0.0, 0.21311862766742706, 0.0, 0.0, -0.10655931383371353, -0.10655931383371353, 0.0, -0.10655931383371353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10655931383371353, 0.0, -0.10655931383371353, 0.10655931383371353, -0.10655931383371353, 0.0, 0.0, 0.10655931383371353, 0.3196779489517212, 0.0, -0.10655931383371353]
[2025-05-27 07:15:36,033]: Mean: -0.00139905
[2025-05-27 07:15:36,038]: Min: -0.31967795
[2025-05-27 07:15:36,045]: Max: 0.42623726
[2025-05-27 07:15:36,045]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 07:15:36,063]: Sample Values (25 elements): [1.2303935289382935, 1.1049144268035889, 1.0785973072052002, 1.2678176164627075, 1.5142842531204224, 1.2251781225204468, 1.1615699529647827, 1.2633483409881592, 1.1176323890686035, 1.226003885269165, 1.0753464698791504, 1.1080684661865234, 1.2864913940429688, 1.0974253416061401, 1.5405826568603516, 1.0042548179626465, 1.148277759552002, 1.053687334060669, 1.357351541519165, 1.191214680671692, 1.1462206840515137, 1.0681027173995972, 1.4178341627120972, 1.1434260606765747, 1.2127172946929932]
[2025-05-27 07:15:36,064]: Mean: 1.21962595
[2025-05-27 07:15:36,064]: Min: 1.00425482
[2025-05-27 07:15:36,064]: Max: 1.54058266
[2025-05-27 07:15:36,066]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 07:15:36,067]: Sample Values (25 elements): [0.11801405251026154, 0.11801405251026154, 0.0, -0.11801405251026154, -0.11801405251026154, 0.23602810502052307, -0.23602810502052307, 0.0, 0.11801405251026154, -0.11801405251026154, -0.11801405251026154, 0.0, 0.11801405251026154, 0.23602810502052307, -0.11801405251026154, 0.11801405251026154, 0.23602810502052307, 0.11801405251026154, 0.0, 0.0, 0.11801405251026154, -0.11801405251026154, 0.11801405251026154, -0.11801405251026154, -0.23602810502052307]
[2025-05-27 07:15:36,067]: Mean: -0.00137657
[2025-05-27 07:15:36,067]: Min: -0.35404217
[2025-05-27 07:15:36,068]: Max: 0.47205621
[2025-05-27 07:15:36,068]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,068]: Sample Values (25 elements): [0.7899636626243591, 0.9620748162269592, 1.1241554021835327, 0.8382276892662048, 0.9717289805412292, 0.961769700050354, 0.8514682054519653, 0.8943784236907959, 0.8370060324668884, 0.8152366280555725, 1.0510773658752441, 0.9928044080734253, 0.938670814037323, 0.7878972887992859, 0.8488085269927979, 0.864404022693634, 0.9588850736618042, 0.8910079598426819, 0.8856936097145081, 0.9507971405982971, 0.9605093598365784, 0.8586421012878418, 1.082067847251892, 0.9238836169242859, 0.8693869113922119]
[2025-05-27 07:15:36,068]: Mean: 0.91195345
[2025-05-27 07:15:36,069]: Min: 0.78221321
[2025-05-27 07:15:36,069]: Max: 1.15961540
[2025-05-27 07:15:36,071]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:15:36,072]: Sample Values (25 elements): [-0.12067833542823792, -0.24135667085647583, 0.0, -0.12067833542823792, 0.0, 0.0, 0.12067833542823792, 0.0, -0.12067833542823792, -0.12067833542823792, 0.12067833542823792, -0.12067833542823792, -0.24135667085647583, 0.0, 0.12067833542823792, 0.12067833542823792, 0.24135667085647583, -0.12067833542823792, -0.24135667085647583, -0.12067833542823792, 0.0, -0.12067833542823792, 0.0, 0.0, 0.12067833542823792]
[2025-05-27 07:15:36,072]: Mean: -0.00262871
[2025-05-27 07:15:36,073]: Min: -0.36203501
[2025-05-27 07:15:36,073]: Max: 0.48271334
[2025-05-27 07:15:36,073]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,073]: Sample Values (25 elements): [0.9672293663024902, 1.0965298414230347, 1.3402360677719116, 1.4432108402252197, 1.0477405786514282, 1.184032917022705, 0.9377866983413696, 0.9444624185562134, 1.3459078073501587, 1.2121672630310059, 0.9215587973594666, 0.913879930973053, 1.1087734699249268, 1.1021502017974854, 1.0780314207077026, 1.1526741981506348, 1.0993136167526245, 0.9275673627853394, 1.0315786600112915, 0.9704923033714294, 0.9302533268928528, 1.091065764427185, 0.7985748052597046, 1.1404248476028442, 1.109978199005127]
[2025-05-27 07:15:36,074]: Mean: 1.12072313
[2025-05-27 07:15:36,074]: Min: 0.79857481
[2025-05-27 07:15:36,074]: Max: 1.48612046
[2025-05-27 07:15:36,077]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 07:15:36,077]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.1441296637058258, 0.0, 0.1441296637058258, -0.1441296637058258, 0.0, -0.1441296637058258, 0.1441296637058258, 0.0, 0.1441296637058258, -0.1441296637058258, -0.1441296637058258, 0.0, 0.0, 0.0, 0.0, 0.2882593274116516, -0.1441296637058258, 0.0, -0.1441296637058258, 0.1441296637058258, 0.0]
[2025-05-27 07:15:36,077]: Mean: 0.00253353
[2025-05-27 07:15:36,077]: Min: -0.43238899
[2025-05-27 07:15:36,078]: Max: 0.57651865
[2025-05-27 07:15:36,078]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,078]: Sample Values (25 elements): [0.48041093349456787, 0.4884456396102905, 0.3573363423347473, 0.5547584295272827, 0.7187296748161316, 0.6978788375854492, 0.43440842628479004, 0.4002763032913208, 0.6614392399787903, 0.4941028654575348, 0.5792334079742432, 0.446407288312912, 0.6031374335289001, 0.5854642987251282, 0.6845786571502686, 0.45680972933769226, 0.32385921478271484, 0.5757673382759094, 0.5540850758552551, 0.4904946982860565, 0.43425002694129944, 0.42674553394317627, 0.5513048768043518, 0.6580781936645508, 0.453722208738327]
[2025-05-27 07:15:36,079]: Mean: 0.53431869
[2025-05-27 07:15:36,080]: Min: 0.32385921
[2025-05-27 07:15:36,080]: Max: 0.75493550
[2025-05-27 07:15:36,081]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:15:36,082]: Sample Values (25 elements): [-0.12075921893119812, -0.12075921893119812, -0.12075921893119812, 0.0, 0.12075921893119812, 0.12075921893119812, 0.0, 0.0, 0.12075921893119812, -0.12075921893119812, 0.0, 0.0, 0.12075921893119812, 0.0, 0.0, -0.12075921893119812, -0.12075921893119812, -0.12075921893119812, -0.12075921893119812, 0.0, 0.0, 0.0, -0.12075921893119812, 0.12075921893119812, 0.0]
[2025-05-27 07:15:36,082]: Mean: -0.00540508
[2025-05-27 07:15:36,082]: Min: -0.36227766
[2025-05-27 07:15:36,084]: Max: 0.48303688
[2025-05-27 07:15:36,084]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,084]: Sample Values (25 elements): [0.9561063647270203, 0.8000378608703613, 0.6101197004318237, 0.8284878730773926, 0.8317394256591797, 0.7868368625640869, 0.8653397560119629, 0.6487374901771545, 0.878305196762085, 0.9084796905517578, 0.729414165019989, 0.7713661789894104, 0.7995254397392273, 0.704192042350769, 0.7584863305091858, 0.8012555837631226, 0.7805125713348389, 0.8141941428184509, 0.7780018448829651, 0.6312413215637207, 0.748769223690033, 0.8819984197616577, 0.8012353777885437, 0.732896625995636, 0.7209169268608093]
[2025-05-27 07:15:36,084]: Mean: 0.79118276
[2025-05-27 07:15:36,085]: Min: 0.60795563
[2025-05-27 07:15:36,085]: Max: 1.02330577
[2025-05-27 07:15:36,087]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:15:36,088]: Sample Values (25 elements): [0.0, 0.09271463751792908, 0.0, 0.0, -0.09271463751792908, -0.09271463751792908, -0.09271463751792908, -0.09271463751792908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09271463751792908, -0.09271463751792908, -0.09271463751792908, 0.09271463751792908, 0.09271463751792908, 0.09271463751792908, -0.09271463751792908, 0.0, 0.0, 0.0, -0.09271463751792908]
[2025-05-27 07:15:36,088]: Mean: -0.00274140
[2025-05-27 07:15:36,089]: Min: -0.37085855
[2025-05-27 07:15:36,089]: Max: 0.27814391
[2025-05-27 07:15:36,089]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,090]: Sample Values (25 elements): [1.0781062841415405, 1.0783218145370483, 1.0823042392730713, 1.013117790222168, 0.9229233860969543, 1.1500829458236694, 1.2004505395889282, 1.0463868379592896, 1.14679753780365, 0.9093230962753296, 1.0321637392044067, 1.3096904754638672, 1.0842561721801758, 1.148768424987793, 1.0292688608169556, 1.3003036975860596, 1.0818287134170532, 1.024526834487915, 1.1272623538970947, 1.0268181562423706, 1.0385305881500244, 1.0778851509094238, 1.2575483322143555, 1.0922406911849976, 1.0978052616119385]
[2025-05-27 07:15:36,090]: Mean: 1.06487668
[2025-05-27 07:15:36,090]: Min: 0.69290543
[2025-05-27 07:15:36,090]: Max: 1.30969048
[2025-05-27 07:15:36,093]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:15:36,094]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.08649300783872604, -0.08649300783872604, 0.0, 0.0, 0.08649300783872604, 0.08649300783872604, 0.0, 0.0, 0.08649300783872604, 0.0, 0.0, -0.1729860156774521, -0.08649300783872604, 0.0, -0.1729860156774521, 0.0, 0.0, -0.08649300783872604, 0.0, 0.08649300783872604, 0.0, 0.0]
[2025-05-27 07:15:36,094]: Mean: -0.00178082
[2025-05-27 07:15:36,095]: Min: -0.25947902
[2025-05-27 07:15:36,095]: Max: 0.34597203
[2025-05-27 07:15:36,095]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,096]: Sample Values (25 elements): [0.004420135170221329, 0.8831406235694885, 6.292951143589889e-41, 0.7689107656478882, 0.6118592023849487, 5.527281662682808e-41, 0.8447735905647278, -4.951067734152444e-41, 0.4894265830516815, 0.5432893633842468, -5.546759711336923e-41, 0.7401177883148193, 0.43516862392425537, 0.5156547427177429, 0.5997170209884644, 0.6163148880004883, 0.6495941281318665, 0.6490941643714905, 0.5528597235679626, 0.7489596009254456, 0.035493094474077225, 0.6492523550987244, 0.5400040745735168, 0.5309022068977356, 0.7140586972236633]
[2025-05-27 07:15:36,096]: Mean: 0.47536439
[2025-05-27 07:15:36,096]: Min: -0.00000000
[2025-05-27 07:15:36,097]: Max: 0.95903021
[2025-05-27 07:15:36,099]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 07:15:36,100]: Sample Values (25 elements): [-0.07150726765394211, 0.0, 0.0, 0.0, -0.07150726765394211, 0.14301453530788422, -0.07150726765394211, 0.0, 0.0, 0.0, -0.07150726765394211, 0.0, 0.07150726765394211, -0.07150726765394211, 0.0, 0.07150726765394211, 0.07150726765394211, -0.07150726765394211, 0.0, 0.0, -0.07150726765394211, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 07:15:36,100]: Mean: -0.00154987
[2025-05-27 07:15:36,101]: Min: -0.21452180
[2025-05-27 07:15:36,101]: Max: 0.28602907
[2025-05-27 07:15:36,101]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 07:15:36,109]: Sample Values (25 elements): [0.9842191338539124, 0.8171303868293762, 1.0213056802749634, 0.959720253944397, 0.9970577955245972, 0.9709404110908508, 0.9518126845359802, 1.0070056915283203, 1.0534082651138306, 0.7423893809318542, 1.2708035707473755, 0.9573733806610107, 0.9734529256820679, 0.9101592302322388, 1.0087831020355225, 1.0490344762802124, 1.083200454711914, 1.03829026222229, 1.1300369501113892, 0.8979287147521973, 1.1257704496383667, 1.0305041074752808, 1.1670081615447998, 0.907253623008728, 1.0613402128219604]
[2025-05-27 07:15:36,113]: Mean: 1.01096678
[2025-05-27 07:15:36,126]: Min: 0.74238938
[2025-05-27 07:15:36,137]: Max: 1.27080357
[2025-05-27 07:15:36,137]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 07:15:36,138]: Sample Values (25 elements): [0.06415850669145584, -0.231081023812294, 0.09186436980962753, 0.02011956088244915, -0.42460110783576965, 0.289856493473053, 0.10792037844657898, -0.14896570146083832, 0.17580214142799377, 0.003818514058366418, 0.3090502917766571, 0.016206711530685425, 0.12689360976219177, 0.17772646248340607, -0.05283393710851669, 0.24445392191410065, -0.3034355640411377, 0.1453707367181778, -0.4455946087837219, 0.1180902048945427, -0.0672505721449852, -0.29173147678375244, 0.2675352394580841, -0.5977761149406433, -0.21204106509685516]
[2025-05-27 07:15:36,138]: Mean: -0.06241198
[2025-05-27 07:15:36,138]: Min: -0.74844295
[2025-05-27 07:15:36,138]: Max: 0.46091706
[2025-05-27 07:15:36,139]: 


QAT of ResNet20 with relu down to 2 bits...
[2025-05-27 07:15:36,463]: [ResNet20_relu_quantized_2_bits] after configure_qat:
[2025-05-27 07:15:36,620]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 07:17:04,346]: [ResNet20_relu_quantized_2_bits] Epoch: 001 Train Loss: 2.0676 Train Acc: 0.2398 Eval Loss: 1.7425 Eval Acc: 0.3544 (LR: 0.00100000)
[2025-05-27 07:18:39,546]: [ResNet20_relu_quantized_2_bits] Epoch: 002 Train Loss: 1.6461 Train Acc: 0.3876 Eval Loss: 1.5798 Eval Acc: 0.4221 (LR: 0.00100000)
[2025-05-27 07:20:16,133]: [ResNet20_relu_quantized_2_bits] Epoch: 003 Train Loss: 1.5399 Train Acc: 0.4311 Eval Loss: 1.5589 Eval Acc: 0.4341 (LR: 0.00100000)
[2025-05-27 07:21:57,303]: [ResNet20_relu_quantized_2_bits] Epoch: 004 Train Loss: 1.5154 Train Acc: 0.4457 Eval Loss: 1.4604 Eval Acc: 0.4620 (LR: 0.00100000)
[2025-05-27 07:23:40,117]: [ResNet20_relu_quantized_2_bits] Epoch: 005 Train Loss: 1.5046 Train Acc: 0.4497 Eval Loss: 1.4581 Eval Acc: 0.4634 (LR: 0.00100000)
[2025-05-27 07:25:22,833]: [ResNet20_relu_quantized_2_bits] Epoch: 006 Train Loss: 1.4737 Train Acc: 0.4606 Eval Loss: 1.4798 Eval Acc: 0.4586 (LR: 0.00100000)
[2025-05-27 07:27:04,036]: [ResNet20_relu_quantized_2_bits] Epoch: 007 Train Loss: 1.6019 Train Acc: 0.4127 Eval Loss: 1.6072 Eval Acc: 0.4161 (LR: 0.00100000)
[2025-05-27 07:28:45,243]: [ResNet20_relu_quantized_2_bits] Epoch: 008 Train Loss: 1.6481 Train Acc: 0.3955 Eval Loss: 1.5556 Eval Acc: 0.4323 (LR: 0.00100000)
[2025-05-27 07:30:30,267]: [ResNet20_relu_quantized_2_bits] Epoch: 009 Train Loss: 1.6175 Train Acc: 0.4069 Eval Loss: 1.8326 Eval Acc: 0.3756 (LR: 0.00100000)
[2025-05-27 07:32:09,538]: [ResNet20_relu_quantized_2_bits] Epoch: 010 Train Loss: 1.5785 Train Acc: 0.4201 Eval Loss: 1.5963 Eval Acc: 0.4310 (LR: 0.00100000)
[2025-05-27 07:33:45,647]: [ResNet20_relu_quantized_2_bits] Epoch: 011 Train Loss: 1.5636 Train Acc: 0.4295 Eval Loss: 1.6196 Eval Acc: 0.4065 (LR: 0.00010000)
[2025-05-27 07:35:17,108]: [ResNet20_relu_quantized_2_bits] Epoch: 012 Train Loss: 1.5216 Train Acc: 0.4471 Eval Loss: 1.5588 Eval Acc: 0.4450 (LR: 0.00010000)
[2025-05-27 07:36:43,425]: [ResNet20_relu_quantized_2_bits] Epoch: 013 Train Loss: 1.5019 Train Acc: 0.4522 Eval Loss: 1.4854 Eval Acc: 0.4553 (LR: 0.00010000)
[2025-05-27 07:38:05,807]: [ResNet20_relu_quantized_2_bits] Epoch: 014 Train Loss: 1.5016 Train Acc: 0.4527 Eval Loss: 1.4548 Eval Acc: 0.4670 (LR: 0.00010000)
[2025-05-27 07:39:26,887]: [ResNet20_relu_quantized_2_bits] Epoch: 015 Train Loss: 1.4794 Train Acc: 0.4621 Eval Loss: 1.4832 Eval Acc: 0.4598 (LR: 0.00010000)
[2025-05-27 07:40:54,241]: [ResNet20_relu_quantized_2_bits] Epoch: 016 Train Loss: 1.4761 Train Acc: 0.4655 Eval Loss: 1.4550 Eval Acc: 0.4727 (LR: 0.00010000)
[2025-05-27 07:42:27,040]: [ResNet20_relu_quantized_2_bits] Epoch: 017 Train Loss: 1.4666 Train Acc: 0.4656 Eval Loss: 1.3990 Eval Acc: 0.4956 (LR: 0.00010000)
[2025-05-27 07:43:59,730]: [ResNet20_relu_quantized_2_bits] Epoch: 018 Train Loss: 1.4556 Train Acc: 0.4719 Eval Loss: 1.4509 Eval Acc: 0.4722 (LR: 0.00010000)
[2025-05-27 07:45:34,734]: [ResNet20_relu_quantized_2_bits] Epoch: 019 Train Loss: 1.4613 Train Acc: 0.4722 Eval Loss: 1.4356 Eval Acc: 0.4747 (LR: 0.00010000)
[2025-05-27 07:47:13,964]: [ResNet20_relu_quantized_2_bits] Epoch: 020 Train Loss: 1.4687 Train Acc: 0.4658 Eval Loss: 1.4400 Eval Acc: 0.4775 (LR: 0.00010000)
[2025-05-27 07:49:00,008]: [ResNet20_relu_quantized_2_bits] Epoch: 021 Train Loss: 1.4618 Train Acc: 0.4699 Eval Loss: 1.4438 Eval Acc: 0.4807 (LR: 0.00010000)
[2025-05-27 07:50:43,313]: [ResNet20_relu_quantized_2_bits] Epoch: 022 Train Loss: 1.4555 Train Acc: 0.4739 Eval Loss: 1.5068 Eval Acc: 0.4542 (LR: 0.00010000)
[2025-05-27 07:52:27,445]: [ResNet20_relu_quantized_2_bits] Epoch: 023 Train Loss: 1.4534 Train Acc: 0.4724 Eval Loss: 1.4469 Eval Acc: 0.4821 (LR: 0.00001000)
[2025-05-27 07:54:08,341]: [ResNet20_relu_quantized_2_bits] Epoch: 024 Train Loss: 1.4488 Train Acc: 0.4770 Eval Loss: 1.4121 Eval Acc: 0.4909 (LR: 0.00001000)
[2025-05-27 07:55:48,830]: [ResNet20_relu_quantized_2_bits] Epoch: 025 Train Loss: 1.4371 Train Acc: 0.4781 Eval Loss: 1.4133 Eval Acc: 0.4926 (LR: 0.00001000)
[2025-05-27 07:57:30,194]: [ResNet20_relu_quantized_2_bits] Epoch: 026 Train Loss: 1.4340 Train Acc: 0.4807 Eval Loss: 1.4197 Eval Acc: 0.4867 (LR: 0.00001000)
[2025-05-27 07:59:06,668]: [ResNet20_relu_quantized_2_bits] Epoch: 027 Train Loss: 1.4392 Train Acc: 0.4773 Eval Loss: 1.3856 Eval Acc: 0.4974 (LR: 0.00001000)
[2025-05-27 08:00:41,207]: [ResNet20_relu_quantized_2_bits] Epoch: 028 Train Loss: 1.4297 Train Acc: 0.4808 Eval Loss: 1.3904 Eval Acc: 0.4968 (LR: 0.00001000)
[2025-05-27 08:02:12,685]: [ResNet20_relu_quantized_2_bits] Epoch: 029 Train Loss: 1.4280 Train Acc: 0.4834 Eval Loss: 1.4344 Eval Acc: 0.4807 (LR: 0.00001000)
[2025-05-27 08:03:38,859]: [ResNet20_relu_quantized_2_bits] Epoch: 030 Train Loss: 1.4320 Train Acc: 0.4821 Eval Loss: 1.4393 Eval Acc: 0.4814 (LR: 0.00001000)
[2025-05-27 08:05:01,591]: [ResNet20_relu_quantized_2_bits] Epoch: 031 Train Loss: 1.4430 Train Acc: 0.4767 Eval Loss: 1.4287 Eval Acc: 0.4817 (LR: 0.00001000)
[2025-05-27 08:06:24,550]: [ResNet20_relu_quantized_2_bits] Epoch: 032 Train Loss: 1.4315 Train Acc: 0.4811 Eval Loss: 1.4391 Eval Acc: 0.4802 (LR: 0.00001000)
[2025-05-27 08:07:47,511]: [ResNet20_relu_quantized_2_bits] Epoch: 033 Train Loss: 1.4286 Train Acc: 0.4853 Eval Loss: 1.3934 Eval Acc: 0.4936 (LR: 0.00000100)
[2025-05-27 08:09:19,093]: [ResNet20_relu_quantized_2_bits] Epoch: 034 Train Loss: 1.4300 Train Acc: 0.4799 Eval Loss: 1.4485 Eval Acc: 0.4819 (LR: 0.00000100)
[2025-05-27 08:10:50,084]: [ResNet20_relu_quantized_2_bits] Epoch: 035 Train Loss: 1.4300 Train Acc: 0.4839 Eval Loss: 1.4050 Eval Acc: 0.4953 (LR: 0.00000100)
[2025-05-27 08:12:25,477]: [ResNet20_relu_quantized_2_bits] Epoch: 036 Train Loss: 1.4264 Train Acc: 0.4824 Eval Loss: 1.3860 Eval Acc: 0.4969 (LR: 0.00000100)
[2025-05-27 08:14:05,049]: [ResNet20_relu_quantized_2_bits] Epoch: 037 Train Loss: 1.4300 Train Acc: 0.4824 Eval Loss: 1.3739 Eval Acc: 0.5058 (LR: 0.00000100)
[2025-05-27 08:15:49,771]: [ResNet20_relu_quantized_2_bits] Epoch: 038 Train Loss: 1.4280 Train Acc: 0.4834 Eval Loss: 1.3916 Eval Acc: 0.4965 (LR: 0.00000100)
[2025-05-27 08:17:34,393]: [ResNet20_relu_quantized_2_bits] Epoch: 039 Train Loss: 1.4319 Train Acc: 0.4834 Eval Loss: 1.4169 Eval Acc: 0.4900 (LR: 0.00000100)
[2025-05-27 08:19:18,052]: [ResNet20_relu_quantized_2_bits] Epoch: 040 Train Loss: 1.4266 Train Acc: 0.4836 Eval Loss: 1.3859 Eval Acc: 0.5005 (LR: 0.00000100)
[2025-05-27 08:20:59,953]: [ResNet20_relu_quantized_2_bits] Epoch: 041 Train Loss: 1.4253 Train Acc: 0.4836 Eval Loss: 1.3892 Eval Acc: 0.4992 (LR: 0.00000100)
[2025-05-27 08:22:39,116]: [ResNet20_relu_quantized_2_bits] Epoch: 042 Train Loss: 1.4282 Train Acc: 0.4828 Eval Loss: 1.5698 Eval Acc: 0.4482 (LR: 0.00000100)
[2025-05-27 08:24:18,221]: [ResNet20_relu_quantized_2_bits] Epoch: 043 Train Loss: 1.4282 Train Acc: 0.4812 Eval Loss: 1.3959 Eval Acc: 0.4976 (LR: 0.00000010)
[2025-05-27 08:25:54,398]: [ResNet20_relu_quantized_2_bits] Epoch: 044 Train Loss: 1.4304 Train Acc: 0.4802 Eval Loss: 1.4328 Eval Acc: 0.4819 (LR: 0.00000010)
[2025-05-27 08:27:27,631]: [ResNet20_relu_quantized_2_bits] Epoch: 045 Train Loss: 1.4274 Train Acc: 0.4841 Eval Loss: 1.4286 Eval Acc: 0.4866 (LR: 0.00000010)
[2025-05-27 08:28:57,081]: [ResNet20_relu_quantized_2_bits] Epoch: 046 Train Loss: 1.4277 Train Acc: 0.4838 Eval Loss: 1.4586 Eval Acc: 0.4804 (LR: 0.00000010)
[2025-05-27 08:30:18,240]: [ResNet20_relu_quantized_2_bits] Epoch: 047 Train Loss: 1.4313 Train Acc: 0.4795 Eval Loss: 1.4152 Eval Acc: 0.4918 (LR: 0.00000010)
[2025-05-27 08:31:40,470]: [ResNet20_relu_quantized_2_bits] Epoch: 048 Train Loss: 1.4335 Train Acc: 0.4809 Eval Loss: 1.4489 Eval Acc: 0.4799 (LR: 0.00000010)
[2025-05-27 08:33:01,257]: [ResNet20_relu_quantized_2_bits] Epoch: 049 Train Loss: 1.4283 Train Acc: 0.4844 Eval Loss: 1.4043 Eval Acc: 0.4988 (LR: 0.00000010)
[2025-05-27 08:34:29,272]: [ResNet20_relu_quantized_2_bits] Epoch: 050 Train Loss: 1.4292 Train Acc: 0.4838 Eval Loss: 1.4186 Eval Acc: 0.4882 (LR: 0.00000010)
[2025-05-27 08:35:58,344]: [ResNet20_relu_quantized_2_bits] Epoch: 051 Train Loss: 1.4256 Train Acc: 0.4823 Eval Loss: 1.4263 Eval Acc: 0.4843 (LR: 0.00000010)
[2025-05-27 08:37:32,497]: [ResNet20_relu_quantized_2_bits] Epoch: 052 Train Loss: 1.4315 Train Acc: 0.4791 Eval Loss: 1.4494 Eval Acc: 0.4825 (LR: 0.00000010)
[2025-05-27 08:39:17,117]: [ResNet20_relu_quantized_2_bits] Epoch: 053 Train Loss: 1.4269 Train Acc: 0.4823 Eval Loss: 1.4063 Eval Acc: 0.4904 (LR: 0.00000010)
[2025-05-27 08:41:04,183]: [ResNet20_relu_quantized_2_bits] Epoch: 054 Train Loss: 1.4235 Train Acc: 0.4820 Eval Loss: 1.4211 Eval Acc: 0.4895 (LR: 0.00000010)
[2025-05-27 08:42:46,323]: [ResNet20_relu_quantized_2_bits] Epoch: 055 Train Loss: 1.4263 Train Acc: 0.4851 Eval Loss: 1.4270 Eval Acc: 0.4909 (LR: 0.00000010)
[2025-05-27 08:44:33,101]: [ResNet20_relu_quantized_2_bits] Epoch: 056 Train Loss: 1.4285 Train Acc: 0.4819 Eval Loss: 1.4291 Eval Acc: 0.4894 (LR: 0.00000010)
[2025-05-27 08:46:19,841]: [ResNet20_relu_quantized_2_bits] Epoch: 057 Train Loss: 1.4344 Train Acc: 0.4801 Eval Loss: 1.4529 Eval Acc: 0.4817 (LR: 0.00000010)
[2025-05-27 08:48:06,274]: [ResNet20_relu_quantized_2_bits] Epoch: 058 Train Loss: 1.4226 Train Acc: 0.4827 Eval Loss: 1.4063 Eval Acc: 0.4893 (LR: 0.00000010)
[2025-05-27 08:49:50,106]: [ResNet20_relu_quantized_2_bits] Epoch: 059 Train Loss: 1.4316 Train Acc: 0.4797 Eval Loss: 1.4035 Eval Acc: 0.4924 (LR: 0.00000010)
[2025-05-27 08:51:33,671]: [ResNet20_relu_quantized_2_bits] Epoch: 060 Train Loss: 1.4277 Train Acc: 0.4841 Eval Loss: 1.4069 Eval Acc: 0.4927 (LR: 0.00000010)
[2025-05-27 08:51:33,671]: [ResNet20_relu_quantized_2_bits] Best Eval Accuracy: 0.5058
[2025-05-27 08:51:33,871]: 


Quantization of model down to 2 bits finished
[2025-05-27 08:51:33,871]: Model Architecture:
[2025-05-27 08:51:34,098]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.9319], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.795780181884766)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3617], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5719009637832642, max_val=0.5133465528488159)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.6122], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.836489677429199)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3850], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5849717855453491, max_val=0.5699330568313599)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([11.5118], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=34.53543472290039)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4350], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7578802704811096, max_val=0.5472612380981445)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.8830], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.6490478515625)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3271], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4813860058784485, max_val=0.4998156428337097)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([13.5486], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=40.64585876464844)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4805], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8667351007461548, max_val=0.5748084783554077)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.4934], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.480057239532471)
            )
          )
        )
      )
      (conv2): Conv2d(
        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3179], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42014506459236145, max_val=0.5336059331893921)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([15.0973], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=45.291786193847656)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3491], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5239335298538208, max_val=0.5233540534973145)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.1292], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=12.387516021728516)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3168], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5072197914123535, max_val=0.443229615688324)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4577], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6001137495040894, max_val=0.7728757858276367)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.8987], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.695982933044434)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3222], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4725514054298401, max_val=0.49395251274108887)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.3597], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=7.079011917114258)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3160], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4066212475299835, max_val=0.5414042472839355)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([6.6139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=19.84161949157715)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3257], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5998282432556152, max_val=0.3772202134132385)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.7091], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.127424716949463)
            )
          )
        )
      )
      (conv2): Conv2d(
        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2381], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3018900752067566, max_val=0.41244834661483765)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.4502], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.350690841674805)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2932], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4340296983718872, max_val=0.4454503059387207)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.1978], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.593386173248291)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2787], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.417704701423645, max_val=0.4184946119785309)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3249], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4925891160964966, max_val=0.4820833206176758)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.0737], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.221146583557129)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2439], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3646676540374756, max_val=0.367160826921463)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5867], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.75999116897583)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2278], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3458058834075928, max_val=0.33755791187286377)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([4.8754], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.626245498657227)
            )
          )
        )
      )
    )
    (2): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2190], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3360363841056824, max_val=0.32105714082717896)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6066], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.819733142852783)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2240], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3424234986305237, max_val=0.3295338749885559)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ReLU(inplace=True)
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([5.7579], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.27367401123047)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[2025-05-27 08:51:34,098]: 
Model Weights:
[2025-05-27 08:51:34,098]: 
Layer: initial_layer.0
Layer Shape: torch.Size([16, 3, 3, 3])
[2025-05-27 08:51:34,099]: Sample Values (25 elements): [-0.13660135865211487, -0.15230901539325714, 0.015371568500995636, 0.10277068614959717, 0.09208953380584717, -0.352379709482193, 0.14740735292434692, 0.29083681106567383, -0.027623556554317474, 0.19635827839374542, -0.052441563457250595, 0.05841536819934845, -0.12577854096889496, 0.2077338546514511, -0.09702129662036896, -0.29172927141189575, 0.02934824302792549, 0.016090990975499153, 0.34799492359161377, -0.10449782013893127, -0.270037978887558, -0.12639939785003662, -0.09885728359222412, -0.5962240695953369, 0.27586331963539124]
[2025-05-27 08:51:34,100]: Mean: -0.00874997
[2025-05-27 08:51:34,100]: Min: -0.59622407
[2025-05-27 08:51:34,100]: Max: 0.52934831
[2025-05-27 08:51:34,100]: 
Layer: initial_layer.1
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,101]: Sample Values (16 elements): [1.2966450452804565, 0.7469789981842041, 1.8206373453140259, 1.8783296346664429, 1.61722731590271, 1.6569743156433105, 1.6444247961044312, 1.7484029531478882, 2.668760299682617, 1.6225377321243286, 3.371993064880371, 2.2526168823242188, 1.3116296529769897, 2.3802597522735596, 2.1941967010498047, 1.9024173021316528]
[2025-05-27 08:51:34,101]: Mean: 1.88212693
[2025-05-27 08:51:34,101]: Min: 0.74697900
[2025-05-27 08:51:34,102]: Max: 3.37199306
[2025-05-27 08:51:34,104]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:51:34,105]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.36174917221069336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.36174917221069336, 0.0, -0.36174917221069336, 0.36174917221069336, 0.0, 0.36174917221069336, 0.0, 0.0, 0.0, -0.36174917221069336, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,105]: Mean: -0.01099064
[2025-05-27 08:51:34,106]: Min: -0.72349834
[2025-05-27 08:51:34,106]: Max: 0.36174917
[2025-05-27 08:51:34,106]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,106]: Sample Values (16 elements): [0.892693817615509, 1.0596442222595215, 1.0901786088943481, 1.1338926553726196, 1.4960917234420776, 0.7936791777610779, 0.9028023481369019, 0.8997269868850708, 1.2309492826461792, 1.037357211112976, 1.101661205291748, 1.2780598402023315, 1.0813164710998535, 1.3121843338012695, 1.103551983833313, 1.274864673614502]
[2025-05-27 08:51:34,107]: Mean: 1.10554099
[2025-05-27 08:51:34,107]: Min: 0.79367918
[2025-05-27 08:51:34,108]: Max: 1.49609172
[2025-05-27 08:51:34,109]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:51:34,110]: Sample Values (25 elements): [0.0, 0.38496828079223633, 0.0, 0.0, 0.0, 0.0, 0.38496828079223633, 0.38496828079223633, 0.0, 0.0, 0.0, 0.0, 0.0, -0.38496828079223633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38496828079223633, 0.0, -0.38496828079223633, 0.0, 0.0]
[2025-05-27 08:51:34,111]: Mean: 0.00634930
[2025-05-27 08:51:34,112]: Min: -0.76993656
[2025-05-27 08:51:34,112]: Max: 0.38496828
[2025-05-27 08:51:34,112]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,114]: Sample Values (16 elements): [1.8426932096481323, 2.14681077003479, 1.420096516609192, 1.549134612083435, 1.2753169536590576, 1.364473581314087, 1.666437029838562, 1.6918988227844238, 2.1376538276672363, 1.556345820426941, 1.603980541229248, 1.573403239250183, 1.6224321126937866, 1.755384922027588, 2.7064311504364014, 1.4136614799499512]
[2025-05-27 08:51:34,115]: Mean: 1.70788467
[2025-05-27 08:51:34,115]: Min: 1.27531695
[2025-05-27 08:51:34,116]: Max: 2.70643115
[2025-05-27 08:51:34,119]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:51:34,121]: Sample Values (25 elements): [0.4350472092628479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4350472092628479, -0.4350472092628479, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,122]: Mean: -0.00226587
[2025-05-27 08:51:34,122]: Min: -0.87009442
[2025-05-27 08:51:34,123]: Max: 0.43504721
[2025-05-27 08:51:34,123]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,125]: Sample Values (16 elements): [0.8515535593032837, 1.0618833303451538, 1.1209819316864014, 0.9963713884353638, 1.3104344606399536, 1.170971155166626, 0.8678433895111084, 0.6565730571746826, 1.2317785024642944, 1.0216152667999268, 1.5353187322616577, 0.6910530924797058, 0.9976584911346436, 1.2439286708831787, 1.1106985807418823, 0.8331518769264221]
[2025-05-27 08:51:34,125]: Mean: 1.04386353
[2025-05-27 08:51:34,126]: Min: 0.65657306
[2025-05-27 08:51:34,126]: Max: 1.53531873
[2025-05-27 08:51:34,131]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:51:34,132]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.32706722617149353, 0.0, 0.0, 0.0, 0.0, 0.32706722617149353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32706722617149353, 0.0, 0.0, 0.0, 0.0, -0.32706722617149353, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,132]: Mean: -0.00212934
[2025-05-27 08:51:34,133]: Min: -0.32706723
[2025-05-27 08:51:34,133]: Max: 0.65413445
[2025-05-27 08:51:34,134]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,134]: Sample Values (16 elements): [1.6406744718551636, 1.9661883115768433, 2.44970440864563, 2.181187391281128, 1.5834029912948608, 1.783469557762146, 1.221047043800354, 1.4709786176681519, 1.4225126504898071, 1.7567545175552368, 1.649990439414978, 1.4387646913528442, 1.5043601989746094, 1.5715305805206299, 1.9452143907546997, 1.8681308031082153]
[2025-05-27 08:51:34,135]: Mean: 1.71586943
[2025-05-27 08:51:34,135]: Min: 1.22104704
[2025-05-27 08:51:34,135]: Max: 2.44970441
[2025-05-27 08:51:34,138]: 
Layer: layer1.2.conv1
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:51:34,139]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.4805145263671875, 0.4805145263671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4805145263671875, 0.0, 0.0]
[2025-05-27 08:51:34,139]: Mean: -0.01397330
[2025-05-27 08:51:34,139]: Min: -0.96102905
[2025-05-27 08:51:34,140]: Max: 0.48051453
[2025-05-27 08:51:34,140]: 
Layer: layer1.2.bn1
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,141]: Sample Values (16 elements): [1.047345757484436, 1.1139956712722778, 0.9453778862953186, 1.084513783454895, 0.7613599896430969, 1.050179123878479, 0.8239870071411133, 1.4296504259109497, 0.7845520973205566, 1.1891511678695679, 1.049321174621582, 1.086899757385254, 0.7705623507499695, 0.9225634336471558, 1.1422420740127563, 1.1115034818649292]
[2025-05-27 08:51:34,142]: Mean: 1.01957536
[2025-05-27 08:51:34,142]: Min: 0.76135999
[2025-05-27 08:51:34,142]: Max: 1.42965043
[2025-05-27 08:51:34,145]: 
Layer: layer1.2.conv2
Layer Shape: torch.Size([16, 16, 3, 3])
[2025-05-27 08:51:34,146]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31791698932647705, 0.0, 0.31791698932647705, 0.0, -0.31791698932647705, 0.0]
[2025-05-27 08:51:34,146]: Mean: 0.01379848
[2025-05-27 08:51:34,147]: Min: -0.31791699
[2025-05-27 08:51:34,147]: Max: 0.63583398
[2025-05-27 08:51:34,147]: 
Layer: layer1.2.bn2
Layer Shape: torch.Size([16])
[2025-05-27 08:51:34,148]: Sample Values (16 elements): [1.4941920042037964, 2.2495498657226562, 1.2595458030700684, 1.0378369092941284, 2.1408557891845703, 1.1471083164215088, 1.9028033018112183, 1.3686187267303467, 1.464804768562317, 1.9104084968566895, 2.2483327388763428, 1.7054263353347778, 1.5452616214752197, 1.4704653024673462, 1.288580060005188, 1.8833848237991333]
[2025-05-27 08:51:34,148]: Mean: 1.63232350
[2025-05-27 08:51:34,148]: Min: 1.03783691
[2025-05-27 08:51:34,149]: Max: 2.24954987
[2025-05-27 08:51:34,151]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([32, 16, 3, 3])
[2025-05-27 08:51:34,151]: Sample Values (25 elements): [0.0, -0.34909588098526, 0.0, -0.34909588098526, 0.0, 0.0, 0.0, 0.0, 0.0, -0.34909588098526, 0.0, -0.34909588098526, 0.0, 0.34909588098526, 0.0, 0.0, 0.0, 0.0, -0.34909588098526, 0.0, 0.0, 0.0, 0.34909588098526, 0.0, 0.0]
[2025-05-27 08:51:34,152]: Mean: -0.00484855
[2025-05-27 08:51:34,152]: Min: -0.69819176
[2025-05-27 08:51:34,152]: Max: 0.34909588
[2025-05-27 08:51:34,152]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,153]: Sample Values (25 elements): [1.2576935291290283, 1.5989023447036743, 0.9623385667800903, 1.094519019126892, 1.5234172344207764, 1.0475172996520996, 1.419628381729126, 1.3807638883590698, 1.22359037399292, 1.4189428091049194, 1.052767276763916, 1.2484210729599, 1.6012600660324097, 1.2072397470474243, 1.3813005685806274, 1.1677311658859253, 1.3012033700942993, 1.1581976413726807, 1.506489872932434, 1.1367928981781006, 1.5336424112319946, 1.2617825269699097, 1.110836148262024, 1.5527422428131104, 1.3865183591842651]
[2025-05-27 08:51:34,154]: Mean: 1.27463162
[2025-05-27 08:51:34,154]: Min: 0.96233857
[2025-05-27 08:51:34,154]: Max: 1.60126007
[2025-05-27 08:51:34,157]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:51:34,157]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.3168164789676666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3168164789676666, 0.0, 0.0, 0.0, 0.3168164789676666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,158]: Mean: -0.01364759
[2025-05-27 08:51:34,158]: Min: -0.63363296
[2025-05-27 08:51:34,159]: Max: 0.31681648
[2025-05-27 08:51:34,159]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,160]: Sample Values (25 elements): [1.1176574230194092, 1.3049460649490356, 1.3466647863388062, 1.3067331314086914, 1.2902953624725342, 1.4306923151016235, 1.055723786354065, 1.2980897426605225, 1.2129490375518799, 1.063665509223938, 1.4203381538391113, 1.6019716262817383, 1.3551334142684937, 1.3636833429336548, 1.3302350044250488, 1.3074367046356201, 1.1459178924560547, 1.3744804859161377, 1.3667789697647095, 1.1829344034194946, 1.2404049634933472, 1.2357995510101318, 1.3417882919311523, 1.2670146226882935, 1.1429246664047241]
[2025-05-27 08:51:34,160]: Mean: 1.26329803
[2025-05-27 08:51:34,161]: Min: 1.01531756
[2025-05-27 08:51:34,162]: Max: 1.60197163
[2025-05-27 08:51:34,165]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([32, 16, 1, 1])
[2025-05-27 08:51:34,168]: Sample Values (25 elements): [0.0, 0.0, -0.4576631784439087, 0.0, -0.4576631784439087, 0.0, 0.4576631784439087, 0.4576631784439087, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4576631784439087, 0.0, 0.4576631784439087, 0.0, 0.0, -0.4576631784439087, 0.0, -0.4576631784439087, 0.0, -0.4576631784439087, 0.9153263568878174, 0.0]
[2025-05-27 08:51:34,168]: Mean: -0.02681620
[2025-05-27 08:51:34,169]: Min: -0.45766318
[2025-05-27 08:51:34,169]: Max: 0.91532636
[2025-05-27 08:51:34,169]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,171]: Sample Values (25 elements): [0.6891056895256042, 0.938225507736206, 0.8743909597396851, 0.9967257380485535, 0.5928672552108765, 0.8311384320259094, 1.4080193042755127, 0.9311783909797668, 0.9051388502120972, 0.5490207076072693, 0.7425633668899536, 0.8592081069946289, 0.9601735472679138, 0.9887177348136902, 1.111471176147461, 0.8260650634765625, 1.1297308206558228, 0.603819727897644, 0.8240777850151062, 0.851024329662323, 0.7884504795074463, 0.9560332894325256, 0.8324794173240662, 0.8140243291854858, 0.9486819505691528]
[2025-05-27 08:51:34,171]: Mean: 0.87576050
[2025-05-27 08:51:34,172]: Min: 0.54902071
[2025-05-27 08:51:34,172]: Max: 1.40801930
[2025-05-27 08:51:34,176]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:51:34,177]: Sample Values (25 elements): [0.0, -0.3221679925918579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3221679925918579, 0.0, -0.3221679925918579, 0.3221679925918579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,178]: Mean: -0.01178067
[2025-05-27 08:51:34,178]: Min: -0.32216799
[2025-05-27 08:51:34,178]: Max: 0.64433599
[2025-05-27 08:51:34,178]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,180]: Sample Values (25 elements): [0.8713138699531555, 1.091134786605835, 0.9642282724380493, 0.9655290246009827, 0.7321969866752625, 1.0489709377288818, 1.2394652366638184, 0.9320520758628845, 0.7937036156654358, 0.8775915503501892, 0.8953762650489807, 1.1317386627197266, 0.8529337048530579, 0.9613252282142639, 1.0336600542068481, 1.0050445795059204, 1.04119873046875, 0.8580569624900818, 1.079815149307251, 1.0291945934295654, 1.372902750968933, 0.8284212350845337, 1.0774861574172974, 0.7794919610023499, 0.9715679883956909]
[2025-05-27 08:51:34,181]: Mean: 0.98472011
[2025-05-27 08:51:34,181]: Min: 0.73219699
[2025-05-27 08:51:34,181]: Max: 1.37290275
[2025-05-27 08:51:34,184]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:51:34,185]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,186]: Mean: -0.00157730
[2025-05-27 08:51:34,186]: Min: -0.31600851
[2025-05-27 08:51:34,187]: Max: 0.63201702
[2025-05-27 08:51:34,187]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,188]: Sample Values (25 elements): [1.2167006731033325, 1.1824464797973633, 1.3726812601089478, 1.3520606756210327, 1.2293885946273804, 1.2616289854049683, 1.3870083093643188, 1.6482268571853638, 1.1019865274429321, 1.5349915027618408, 1.560733675956726, 1.268206000328064, 1.3377058506011963, 1.3131654262542725, 1.249074101448059, 1.2106447219848633, 1.213017463684082, 1.7996777296066284, 1.4337753057479858, 1.6328458786010742, 1.4537361860275269, 1.360865592956543, 1.332029104232788, 1.378327488899231, 1.1954225301742554]
[2025-05-27 08:51:34,188]: Mean: 1.35942030
[2025-05-27 08:51:34,189]: Min: 1.10198653
[2025-05-27 08:51:34,189]: Max: 1.79967773
[2025-05-27 08:51:34,192]: 
Layer: layer2.2.conv1
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:51:34,193]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3256828188896179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3256828188896179, 0.0, 0.3256828188896179, 0.0, 0.0, -0.3256828188896179, -0.3256828188896179, 0.0, 0.0]
[2025-05-27 08:51:34,193]: Mean: -0.01381749
[2025-05-27 08:51:34,194]: Min: -0.65136564
[2025-05-27 08:51:34,194]: Max: 0.32568282
[2025-05-27 08:51:34,194]: 
Layer: layer2.2.bn1
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,195]: Sample Values (25 elements): [0.866916835308075, 0.9241882562637329, 0.9849653840065002, 0.6037111878395081, 0.9922603964805603, 0.88533616065979, 0.9351658225059509, 0.9621856808662415, 0.8706221580505371, 0.9958706498146057, 0.8687388300895691, 0.8483074903488159, 0.8213722109794617, 0.910535454750061, 0.8959470391273499, 0.8701407313346863, 0.7079368233680725, 0.7903052568435669, 0.8794592618942261, 0.7559719085693359, 0.8642928600311279, 0.8330085277557373, 0.9881890416145325, 0.9721860885620117, 0.9656240940093994]
[2025-05-27 08:51:34,195]: Mean: 0.88580155
[2025-05-27 08:51:34,195]: Min: 0.60371119
[2025-05-27 08:51:34,196]: Max: 1.07654583
[2025-05-27 08:51:34,198]: 
Layer: layer2.2.conv2
Layer Shape: torch.Size([32, 32, 3, 3])
[2025-05-27 08:51:34,199]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23811280727386475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23811280727386475, 0.0, 0.0, 0.0, 0.23811280727386475, 0.23811280727386475]
[2025-05-27 08:51:34,199]: Mean: 0.00147270
[2025-05-27 08:51:34,199]: Min: -0.23811281
[2025-05-27 08:51:34,200]: Max: 0.47622561
[2025-05-27 08:51:34,200]: 
Layer: layer2.2.bn2
Layer Shape: torch.Size([32])
[2025-05-27 08:51:34,201]: Sample Values (25 elements): [1.1465786695480347, 1.4108675718307495, 1.958315372467041, 1.5916634798049927, 1.4136624336242676, 1.5700682401657104, 1.7643409967422485, 1.3459749221801758, 1.4122912883758545, 1.5012255907058716, 1.75312340259552, 1.151014804840088, 1.6830955743789673, 1.6641098260879517, 1.4351178407669067, 1.6230634450912476, 1.6832377910614014, 1.4022537469863892, 1.5460456609725952, 1.630370020866394, 1.609537959098816, 1.5278068780899048, 1.5932906866073608, 1.4050384759902954, 1.357767939567566]
[2025-05-27 08:51:34,201]: Mean: 1.50293028
[2025-05-27 08:51:34,201]: Min: 1.08399129
[2025-05-27 08:51:34,202]: Max: 1.95831537
[2025-05-27 08:51:34,204]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([64, 32, 3, 3])
[2025-05-27 08:51:34,205]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.29316002130508423, 0.0, -0.29316002130508423, -0.29316002130508423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.29316002130508423, -0.29316002130508423, 0.0, 0.0, 0.0, 0.0, 0.29316002130508423]
[2025-05-27 08:51:34,205]: Mean: -0.00314918
[2025-05-27 08:51:34,206]: Min: -0.29316002
[2025-05-27 08:51:34,206]: Max: 0.58632004
[2025-05-27 08:51:34,206]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,208]: Sample Values (25 elements): [1.0356791019439697, 1.099007487297058, 1.0745172500610352, 0.827829897403717, 1.0298058986663818, 1.090416669845581, 1.1188209056854248, 0.982122540473938, 1.110321044921875, 1.096660852432251, 0.9548546671867371, 1.136053442955017, 0.879379153251648, 0.8592213988304138, 1.0765881538391113, 0.9970574975013733, 0.848466694355011, 1.0768252611160278, 0.963323712348938, 1.003644347190857, 0.9822298288345337, 1.1445003747940063, 1.1786727905273438, 0.8650901317596436, 1.035698652267456]
[2025-05-27 08:51:34,208]: Mean: 0.99414134
[2025-05-27 08:51:34,209]: Min: 0.72403020
[2025-05-27 08:51:34,209]: Max: 1.25372195
[2025-05-27 08:51:34,214]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:51:34,215]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, -0.27873310446739197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,216]: Mean: -0.00409057
[2025-05-27 08:51:34,217]: Min: -0.27873310
[2025-05-27 08:51:34,217]: Max: 0.55746621
[2025-05-27 08:51:34,217]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,218]: Sample Values (25 elements): [1.1018341779708862, 0.9714630842208862, 1.084145188331604, 1.3546772003173828, 1.155411958694458, 1.2221291065216064, 1.1800047159194946, 1.266109824180603, 1.2143537998199463, 1.0306901931762695, 1.0715850591659546, 1.1153254508972168, 1.1144773960113525, 1.3600974082946777, 1.2220743894577026, 1.1230018138885498, 1.1723562479019165, 1.1490200757980347, 1.3597590923309326, 0.8883776068687439, 1.2262145280838013, 1.0788373947143555, 1.4289424419403076, 0.877905011177063, 1.1356937885284424]
[2025-05-27 08:51:34,218]: Mean: 1.15283096
[2025-05-27 08:51:34,220]: Min: 0.87790501
[2025-05-27 08:51:34,221]: Max: 1.42940688
[2025-05-27 08:51:34,224]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([64, 32, 1, 1])
[2025-05-27 08:51:34,226]: Sample Values (25 elements): [0.3248908221721649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3248908221721649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3248908221721649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,226]: Mean: -0.01427743
[2025-05-27 08:51:34,227]: Min: -0.64978164
[2025-05-27 08:51:34,228]: Max: 0.32489082
[2025-05-27 08:51:34,228]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,229]: Sample Values (25 elements): [0.49553006887435913, 0.5778050422668457, 0.44774192571640015, 0.606814980506897, 0.5915735960006714, 0.5377099514007568, 0.6434844136238098, 0.5669308304786682, 0.7263931632041931, 0.6993606686592102, 0.5821811556816101, 0.6790806651115417, 0.5622408986091614, 0.5343338847160339, 0.6114047765731812, 0.4903244376182556, 0.6310164928436279, 0.7969903945922852, 0.8017643094062805, 0.9035954475402832, 0.6079666614532471, 0.7294086813926697, 0.4531780779361725, 0.6196271181106567, 0.5140363574028015]
[2025-05-27 08:51:34,229]: Mean: 0.60695028
[2025-05-27 08:51:34,229]: Min: 0.36027062
[2025-05-27 08:51:34,230]: Max: 0.90359545
[2025-05-27 08:51:34,233]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:51:34,234]: Sample Values (25 elements): [0.0, 0.0, 0.24394284188747406, -0.24394284188747406, -0.24394284188747406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24394284188747406, 0.0, 0.0, -0.24394284188747406, 0.0, 0.0, 0.24394284188747406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,234]: Mean: -0.00339471
[2025-05-27 08:51:34,234]: Min: -0.24394284
[2025-05-27 08:51:34,235]: Max: 0.48788568
[2025-05-27 08:51:34,235]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,236]: Sample Values (25 elements): [0.7832959294319153, 0.843449592590332, 0.6885928511619568, 0.8175369501113892, 0.7787136435508728, 0.7538794279098511, 0.9397122859954834, 0.7287106513977051, 0.924595057964325, 0.8906005620956421, 0.9024872183799744, 0.8263813257217407, 0.9009799361228943, 0.9072285294532776, 0.9351407289505005, 0.6671801209449768, 0.788548469543457, 0.8658512830734253, 0.7349948287010193, 1.0111680030822754, 0.8636189103126526, 0.6981391310691833, 0.772737979888916, 0.709068775177002, 0.7386866807937622]
[2025-05-27 08:51:34,237]: Mean: 0.84976393
[2025-05-27 08:51:34,237]: Min: 0.66583979
[2025-05-27 08:51:34,238]: Max: 1.20413840
[2025-05-27 08:51:34,240]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:51:34,241]: Sample Values (25 elements): [0.0, 0.0, -0.22778794169425964, -0.22778794169425964, 0.0, 0.0, 0.0, 0.0, -0.22778794169425964, 0.22778794169425964, 0.0, -0.22778794169425964, 0.22778794169425964, 0.0, 0.0, -0.22778794169425964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,241]: Mean: -0.00694536
[2025-05-27 08:51:34,241]: Min: -0.45557588
[2025-05-27 08:51:34,242]: Max: 0.22778794
[2025-05-27 08:51:34,242]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,242]: Sample Values (25 elements): [1.0716946125030518, 1.153281331062317, 1.4253225326538086, 1.2473666667938232, 1.1466894149780273, 1.3007396459579468, 1.0863529443740845, 1.273923397064209, 1.3944652080535889, 1.2569411993026733, 1.3283913135528564, 1.1784390211105347, 1.2339088916778564, 1.2290916442871094, 0.9781002998352051, 1.3864681720733643, 1.2122191190719604, 1.4466502666473389, 1.2304688692092896, 1.4578503370285034, 1.279282569885254, 1.2647967338562012, 1.2450721263885498, 1.1992850303649902, 1.0835248231887817]
[2025-05-27 08:51:34,243]: Mean: 1.26324725
[2025-05-27 08:51:34,243]: Min: 0.95973802
[2025-05-27 08:51:34,244]: Max: 1.52730894
[2025-05-27 08:51:34,246]: 
Layer: layer3.2.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:51:34,247]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2190311849117279, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2190311849117279, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-27 08:51:34,247]: Mean: -0.00341048
[2025-05-27 08:51:34,247]: Min: -0.43806237
[2025-05-27 08:51:34,248]: Max: 0.21903118
[2025-05-27 08:51:34,248]: 
Layer: layer3.2.bn1
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,249]: Sample Values (25 elements): [0.6698536276817322, 0.8674089312553406, -5.53246646700081e-41, 0.7052496671676636, 0.6769497394561768, -5.78147720411133e-41, 0.855780303478241, 0.9404714703559875, 4.929908127341139e-41, -4.975029937892398e-41, 0.6056498289108276, 4.975590457278128e-41, 0.7263860702514648, 0.8437291979789734, -4.951067734152444e-41, 1.0169175863265991, 0.8481079339981079, 0.6873120069503784, 0.5959380865097046, -4.907066962372644e-41, 0.635941743850708, 6.168095450418547e-41, 0.7597402334213257, 0.7756133675575256, 4.968303705263639e-41]
[2025-05-27 08:51:34,249]: Mean: 0.56676501
[2025-05-27 08:51:34,249]: Min: -0.00000000
[2025-05-27 08:51:34,250]: Max: 1.09747052
[2025-05-27 08:51:34,252]: 
Layer: layer3.2.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-27 08:51:34,253]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.22398579120635986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.22398579120635986, 0.22398579120635986, 0.0, 0.0, 0.0, -0.22398579120635986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.22398579120635986]
[2025-05-27 08:51:34,254]: Mean: -0.01029275
[2025-05-27 08:51:34,255]: Min: -0.44797158
[2025-05-27 08:51:34,255]: Max: 0.22398579
[2025-05-27 08:51:34,255]: 
Layer: layer3.2.bn2
Layer Shape: torch.Size([64])
[2025-05-27 08:51:34,256]: Sample Values (25 elements): [1.0790300369262695, 1.4921820163726807, 1.4528106451034546, 1.2225311994552612, 1.4903161525726318, 1.3171097040176392, 1.243693470954895, 1.392282485961914, 1.4875472784042358, 1.3376449346542358, 1.1047332286834717, 1.2911732196807861, 1.2739959955215454, 1.051902174949646, 1.2718299627304077, 1.374464750289917, 1.3246668577194214, 1.426919937133789, 1.2110894918441772, 1.4437464475631714, 1.3173035383224487, 1.1992624998092651, 1.4837946891784668, 1.457823634147644, 1.4510560035705566]
[2025-05-27 08:51:34,256]: Mean: 1.30552077
[2025-05-27 08:51:34,257]: Min: 0.92245519
[2025-05-27 08:51:34,258]: Max: 1.57083189
[2025-05-27 08:51:34,258]: 
Layer: fc
Layer Shape: torch.Size([10, 64])
[2025-05-27 08:51:34,259]: Sample Values (25 elements): [-0.11378603428602219, -0.10376893728971481, 8.25981333036907e-05, -0.09884285926818848, -0.3000539541244507, 0.01715059205889702, 0.0053615933284163475, -0.06171030551195145, -0.12324057519435883, -0.09732937812805176, -0.0330599881708622, 0.14388710260391235, 0.10223077982664108, -0.25179430842399597, -0.2640410363674164, -0.2102648764848709, -0.084243543446064, -0.08448003977537155, -0.2436191588640213, -0.008821563795208931, -0.027557898312807083, 0.11886727064847946, -0.4173152446746826, 0.003315168200060725, 0.13374601304531097]
[2025-05-27 08:51:34,260]: Mean: -0.05661405
[2025-05-27 08:51:34,260]: Min: -0.53324652
[2025-05-27 08:51:34,260]: Max: 0.31963590
