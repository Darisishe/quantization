[2025-05-21 07:52:20,648]: Checkpoint of model at path [checkpoint/ResNet18_relu6.ckpt] will be used for QAT
[2025-05-21 07:52:20,648]: 


QAT of ResNet18 with parametrized_relu down to 2 bits...
[2025-05-21 07:52:21,025]: [ResNet18_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-05-21 07:52:21,185]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 07:54:12,704]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 0.9784 Train Acc: 0.6632 Eval Loss: 0.7913 Eval Acc: 0.7285 (LR: 0.010000)
[2025-05-21 07:56:13,405]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 0.7000 Train Acc: 0.7566 Eval Loss: 0.6840 Eval Acc: 0.7689 (LR: 0.010000)
[2025-05-21 07:58:05,822]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 0.6174 Train Acc: 0.7863 Eval Loss: 0.6005 Eval Acc: 0.7981 (LR: 0.010000)
[2025-05-21 07:59:49,314]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 0.5706 Train Acc: 0.8020 Eval Loss: 0.7963 Eval Acc: 0.7530 (LR: 0.010000)
[2025-05-21 08:01:37,021]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 0.5417 Train Acc: 0.8125 Eval Loss: 0.5238 Eval Acc: 0.8211 (LR: 0.010000)
[2025-05-21 08:03:25,636]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 0.5055 Train Acc: 0.8247 Eval Loss: 0.5360 Eval Acc: 0.8242 (LR: 0.010000)
[2025-05-21 08:05:12,988]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 0.4877 Train Acc: 0.8313 Eval Loss: 0.7266 Eval Acc: 0.7724 (LR: 0.010000)
[2025-05-21 08:07:01,888]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 0.4692 Train Acc: 0.8364 Eval Loss: 0.5759 Eval Acc: 0.8159 (LR: 0.010000)
[2025-05-21 08:08:50,533]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 0.4530 Train Acc: 0.8438 Eval Loss: 0.5664 Eval Acc: 0.8103 (LR: 0.010000)
[2025-05-21 08:10:37,761]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 0.4411 Train Acc: 0.8468 Eval Loss: 0.4953 Eval Acc: 0.8362 (LR: 0.010000)
[2025-05-21 08:12:24,879]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 0.4228 Train Acc: 0.8516 Eval Loss: 0.5345 Eval Acc: 0.8274 (LR: 0.010000)
[2025-05-21 08:14:11,854]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 0.4128 Train Acc: 0.8555 Eval Loss: 0.4784 Eval Acc: 0.8429 (LR: 0.010000)
[2025-05-21 08:15:59,067]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 0.4039 Train Acc: 0.8597 Eval Loss: 0.4889 Eval Acc: 0.8388 (LR: 0.010000)
[2025-05-21 08:17:47,140]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 0.3951 Train Acc: 0.8634 Eval Loss: 0.5750 Eval Acc: 0.8152 (LR: 0.010000)
[2025-05-21 08:19:51,639]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 0.3849 Train Acc: 0.8652 Eval Loss: 0.5355 Eval Acc: 0.8285 (LR: 0.001000)
[2025-05-21 08:21:56,087]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 0.2937 Train Acc: 0.8978 Eval Loss: 0.3415 Eval Acc: 0.8839 (LR: 0.001000)
[2025-05-21 08:24:00,412]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 0.2666 Train Acc: 0.9062 Eval Loss: 0.3394 Eval Acc: 0.8879 (LR: 0.001000)
[2025-05-21 08:26:04,719]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 0.2614 Train Acc: 0.9078 Eval Loss: 0.3305 Eval Acc: 0.8924 (LR: 0.001000)
[2025-05-21 08:28:08,976]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 0.2533 Train Acc: 0.9120 Eval Loss: 0.3329 Eval Acc: 0.8883 (LR: 0.001000)
[2025-05-21 08:30:13,979]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 0.2518 Train Acc: 0.9109 Eval Loss: 0.3301 Eval Acc: 0.8899 (LR: 0.001000)
[2025-05-21 08:32:18,620]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 0.2473 Train Acc: 0.9133 Eval Loss: 0.3244 Eval Acc: 0.8929 (LR: 0.001000)
[2025-05-21 08:34:23,513]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 0.2453 Train Acc: 0.9141 Eval Loss: 0.3324 Eval Acc: 0.8883 (LR: 0.001000)
[2025-05-21 08:36:27,764]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 0.2434 Train Acc: 0.9155 Eval Loss: 0.3352 Eval Acc: 0.8915 (LR: 0.001000)
[2025-05-21 08:38:32,141]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 0.2344 Train Acc: 0.9179 Eval Loss: 0.3297 Eval Acc: 0.8947 (LR: 0.001000)
[2025-05-21 08:40:36,581]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 0.2387 Train Acc: 0.9165 Eval Loss: 0.3337 Eval Acc: 0.8924 (LR: 0.001000)
[2025-05-21 08:42:41,067]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 0.2319 Train Acc: 0.9190 Eval Loss: 0.3357 Eval Acc: 0.8891 (LR: 0.001000)
[2025-05-21 08:44:45,353]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 0.2322 Train Acc: 0.9186 Eval Loss: 0.3349 Eval Acc: 0.8921 (LR: 0.001000)
[2025-05-21 08:46:49,631]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 0.2285 Train Acc: 0.9201 Eval Loss: 0.3305 Eval Acc: 0.8921 (LR: 0.001000)
[2025-05-21 08:48:53,854]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 0.2293 Train Acc: 0.9196 Eval Loss: 0.3308 Eval Acc: 0.8946 (LR: 0.001000)
[2025-05-21 08:50:57,775]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 0.2257 Train Acc: 0.9219 Eval Loss: 0.3211 Eval Acc: 0.8955 (LR: 0.000100)
[2025-05-21 08:53:01,880]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 0.2129 Train Acc: 0.9248 Eval Loss: 0.3170 Eval Acc: 0.8952 (LR: 0.000100)
[2025-05-21 08:55:06,144]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 0.2109 Train Acc: 0.9251 Eval Loss: 0.3181 Eval Acc: 0.8958 (LR: 0.000100)
[2025-05-21 08:57:10,259]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 0.2109 Train Acc: 0.9244 Eval Loss: 0.3107 Eval Acc: 0.8999 (LR: 0.000100)
[2025-05-21 08:59:14,343]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 0.2087 Train Acc: 0.9274 Eval Loss: 0.3128 Eval Acc: 0.9001 (LR: 0.000100)
[2025-05-21 09:01:18,637]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 0.2075 Train Acc: 0.9262 Eval Loss: 0.3202 Eval Acc: 0.8945 (LR: 0.000100)
[2025-05-21 09:03:22,566]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 0.2073 Train Acc: 0.9254 Eval Loss: 0.3134 Eval Acc: 0.8964 (LR: 0.000100)
[2025-05-21 09:05:26,796]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 0.2053 Train Acc: 0.9270 Eval Loss: 0.3171 Eval Acc: 0.8972 (LR: 0.000100)
[2025-05-21 09:07:30,803]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 0.2028 Train Acc: 0.9277 Eval Loss: 0.3203 Eval Acc: 0.8989 (LR: 0.000100)
[2025-05-21 09:09:35,022]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 0.2067 Train Acc: 0.9256 Eval Loss: 0.3122 Eval Acc: 0.8956 (LR: 0.000100)
[2025-05-21 09:11:39,782]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 0.2027 Train Acc: 0.9284 Eval Loss: 0.3196 Eval Acc: 0.8987 (LR: 0.000100)
[2025-05-21 09:13:44,018]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 0.2040 Train Acc: 0.9283 Eval Loss: 0.3117 Eval Acc: 0.8986 (LR: 0.000100)
[2025-05-21 09:15:48,276]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 0.2031 Train Acc: 0.9283 Eval Loss: 0.3120 Eval Acc: 0.8982 (LR: 0.000100)
[2025-05-21 09:17:52,538]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 0.1999 Train Acc: 0.9305 Eval Loss: 0.3146 Eval Acc: 0.8977 (LR: 0.000100)
[2025-05-21 09:19:56,810]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 0.2054 Train Acc: 0.9280 Eval Loss: 0.3195 Eval Acc: 0.8963 (LR: 0.000100)
[2025-05-21 09:22:00,935]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 0.2022 Train Acc: 0.9275 Eval Loss: 0.3226 Eval Acc: 0.8988 (LR: 0.000010)
[2025-05-21 09:24:05,210]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 0.1980 Train Acc: 0.9300 Eval Loss: 0.3229 Eval Acc: 0.8937 (LR: 0.000010)
[2025-05-21 09:26:09,537]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 0.1993 Train Acc: 0.9296 Eval Loss: 0.3155 Eval Acc: 0.8979 (LR: 0.000010)
[2025-05-21 09:28:13,763]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 0.2018 Train Acc: 0.9289 Eval Loss: 0.3155 Eval Acc: 0.8976 (LR: 0.000010)
[2025-05-21 09:30:17,862]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 0.1963 Train Acc: 0.9307 Eval Loss: 0.3108 Eval Acc: 0.8982 (LR: 0.000010)
[2025-05-21 09:32:21,775]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 0.1966 Train Acc: 0.9310 Eval Loss: 0.3201 Eval Acc: 0.8959 (LR: 0.000010)
[2025-05-21 09:34:25,820]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 0.1973 Train Acc: 0.9303 Eval Loss: 0.3124 Eval Acc: 0.8976 (LR: 0.000010)
[2025-05-21 09:36:29,960]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 0.1989 Train Acc: 0.9299 Eval Loss: 0.3169 Eval Acc: 0.8953 (LR: 0.000010)
[2025-05-21 09:38:34,208]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 0.1970 Train Acc: 0.9303 Eval Loss: 0.3168 Eval Acc: 0.8958 (LR: 0.000010)
[2025-05-21 09:40:38,318]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 0.1991 Train Acc: 0.9308 Eval Loss: 0.3144 Eval Acc: 0.8982 (LR: 0.000010)
[2025-05-21 09:42:42,585]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 0.1987 Train Acc: 0.9297 Eval Loss: 0.3103 Eval Acc: 0.9003 (LR: 0.000010)
[2025-05-21 09:44:46,686]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 0.1967 Train Acc: 0.9294 Eval Loss: 0.3169 Eval Acc: 0.8997 (LR: 0.000010)
[2025-05-21 09:46:50,995]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 0.1988 Train Acc: 0.9300 Eval Loss: 0.3098 Eval Acc: 0.8981 (LR: 0.000010)
[2025-05-21 09:48:55,163]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 0.1975 Train Acc: 0.9302 Eval Loss: 0.3161 Eval Acc: 0.8993 (LR: 0.000010)
[2025-05-21 09:50:59,758]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 0.1987 Train Acc: 0.9306 Eval Loss: 0.3081 Eval Acc: 0.8985 (LR: 0.000010)
[2025-05-21 09:53:04,739]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 0.2013 Train Acc: 0.9287 Eval Loss: 0.3196 Eval Acc: 0.8968 (LR: 0.000010)
[2025-05-21 09:53:04,739]: [ResNet18_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.9003
[2025-05-21 09:53:04,842]: 


Quantization of model down to 2 bits finished
[2025-05-21 09:53:04,842]: Model Architecture:
[2025-05-21 09:53:04,889]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1897], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.273287296295166, max_val=0.29590457677841187)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8623], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1185], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16754606366157532, max_val=0.18784409761428833)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8532], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1049], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15740129351615906, max_val=0.1573997437953949)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8744], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0710], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09729670733213425, max_val=0.11576886475086212)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9417], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0665], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09618400782346725, max_val=0.10326196253299713)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8743], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0656], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09677257388830185, max_val=0.10016341507434845)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1283], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1789555847644806, max_val=0.20595037937164307)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8871], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0637], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08290419727563858, max_val=0.10827523469924927)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8750], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0608], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09024494141340256, max_val=0.09201506525278091)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0132], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0478], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06757086515426636, max_val=0.07595067471265793)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8753], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0436], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06303969770669937, max_val=0.06773767620325089)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0902], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1250540167093277, max_val=0.14562486112117767)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9106], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0466], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07030759006738663, max_val=0.06956730782985687)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8745], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0381], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05620536953210831, max_val=0.05811635032296181)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.0062], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0348], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04811261221766472, max_val=0.056182585656642914)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8750], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0268], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03720901161432266, max_val=0.04309150204062462)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0568], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0854579508304596, max_val=0.08489043265581131)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8958], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0236], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.033458441495895386, max_val=0.03743219003081322)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8756], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0174], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.025347117334604263, max_val=0.026791896671056747)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8677], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 09:53:04,889]: 
Model Weights:
[2025-05-21 09:53:04,889]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-21 09:53:04,890]: Sample Values (25 elements): [-0.17047077417373657, 0.020500926300883293, -0.07352086901664734, -0.1258397251367569, -0.1164834052324295, -0.10569003969430923, 0.15200230479240417, -0.02809816598892212, 0.09088248759508133, 0.2003205418586731, -0.10514254868030548, 0.12337268888950348, -0.09233751147985458, 0.07798144221305847, -0.07247068732976913, -0.1335810124874115, 0.05510968342423439, -0.10686563700437546, 0.17785121500492096, 0.007115726359188557, 0.003038539784029126, -0.0405142679810524, -0.18903887271881104, 0.0788784846663475, 0.01526040118187666]
[2025-05-21 09:53:04,894]: Mean: -0.00061756
[2025-05-21 09:53:04,895]: Min: -0.51043123
[2025-05-21 09:53:04,895]: Max: 0.52500236
[2025-05-21 09:53:04,895]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-21 09:53:04,895]: Sample Values (25 elements): [1.0387951135635376, 1.2323625087738037, 1.0464040040969849, 1.0948529243469238, 1.0506287813186646, 1.3713350296020508, 1.0482165813446045, 0.9928508996963501, 1.2233850955963135, 1.0567699670791626, 1.1396293640136719, 1.000967264175415, 1.2433973550796509, 1.036789059638977, 1.0081026554107666, 1.2424105405807495, 1.1694996356964111, 0.9761919379234314, 1.3928910493850708, 1.0746911764144897, 1.1236077547073364, 1.2470371723175049, 1.0094839334487915, 0.92853182554245, 1.321514368057251]
[2025-05-21 09:53:04,895]: Mean: 1.13955665
[2025-05-21 09:53:04,896]: Min: 0.86260682
[2025-05-21 09:53:04,896]: Max: 1.54495251
[2025-05-21 09:53:04,897]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 09:53:04,897]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18973062932491302, 0.18973062932491302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,898]: Mean: -0.00083378
[2025-05-21 09:53:04,898]: Min: -0.18973063
[2025-05-21 09:53:04,898]: Max: 0.37946126
[2025-05-21 09:53:04,898]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 09:53:04,898]: Sample Values (25 elements): [0.9112492799758911, 0.9838358163833618, 1.0232166051864624, 1.0434027910232544, 0.9693155288696289, 0.8896819949150085, 0.8652400374412537, 0.9875229001045227, 1.0134927034378052, 0.9338547587394714, 1.052209734916687, 0.9512513875961304, 0.9526947140693665, 0.9153549671173096, 0.9067719578742981, 0.9215185642242432, 1.0217499732971191, 1.0585856437683105, 0.9952210783958435, 1.4168332815170288, 1.0037999153137207, 0.9982215762138367, 1.1144694089889526, 1.089657187461853, 0.9512190818786621]
[2025-05-21 09:53:04,898]: Mean: 1.01126659
[2025-05-21 09:53:04,898]: Min: 0.86524004
[2025-05-21 09:53:04,899]: Max: 1.43481898
[2025-05-21 09:53:04,900]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 09:53:04,900]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.11846339702606201, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11846339702606201, 0.0]
[2025-05-21 09:53:04,900]: Mean: -0.00174816
[2025-05-21 09:53:04,900]: Min: -0.11846340
[2025-05-21 09:53:04,900]: Max: 0.23692679
[2025-05-21 09:53:04,901]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 09:53:04,901]: Sample Values (25 elements): [0.9207321405410767, 0.9852884411811829, 1.0422070026397705, 0.930905282497406, 0.899837851524353, 1.1268190145492554, 0.8905504941940308, 1.0200408697128296, 0.9384427070617676, 0.9467166066169739, 1.0131597518920898, 0.9975100159645081, 0.9391977787017822, 0.9508832097053528, 0.9173916578292847, 0.9569171667098999, 0.9701942205429077, 0.8858552575111389, 0.8872184753417969, 0.9264631867408752, 1.2944005727767944, 0.9531309604644775, 0.9282041192054749, 1.088539958000183, 0.9649986624717712]
[2025-05-21 09:53:04,901]: Mean: 0.97105241
[2025-05-21 09:53:04,901]: Min: 0.88585526
[2025-05-21 09:53:04,901]: Max: 1.29440057
[2025-05-21 09:53:04,902]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 09:53:04,903]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,903]: Mean: -0.00149157
[2025-05-21 09:53:04,903]: Min: -0.20986736
[2025-05-21 09:53:04,903]: Max: 0.10493368
[2025-05-21 09:53:04,903]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 09:53:04,903]: Sample Values (25 elements): [1.0194367170333862, 0.9536504745483398, 0.9580169320106506, 0.9475794434547424, 0.9100620746612549, 0.9260675311088562, 0.9488074779510498, 0.9513981342315674, 0.940071702003479, 0.9471292495727539, 1.0119889974594116, 0.9466598033905029, 0.9646154642105103, 0.9655335545539856, 0.9845815896987915, 0.9467709064483643, 0.9818847179412842, 0.9763144850730896, 0.998811662197113, 0.975887656211853, 0.953700840473175, 0.9355878233909607, 0.9736315608024597, 0.9735903739929199, 0.944262683391571]
[2025-05-21 09:53:04,904]: Mean: 0.96942127
[2025-05-21 09:53:04,904]: Min: 0.89881152
[2025-05-21 09:53:04,904]: Max: 1.10731554
[2025-05-21 09:53:04,905]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 09:53:04,905]: Sample Values (25 elements): [0.0, -0.07102185487747192, 0.0, 0.0, 0.07102185487747192, 0.0, 0.0, 0.0, 0.07102185487747192, 0.0, -0.07102185487747192, -0.07102185487747192, 0.0, 0.0, -0.07102185487747192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.07102185487747192, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,906]: Mean: -0.00054137
[2025-05-21 09:53:04,906]: Min: -0.07102185
[2025-05-21 09:53:04,906]: Max: 0.14204371
[2025-05-21 09:53:04,906]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 09:53:04,906]: Sample Values (25 elements): [0.9820699095726013, 0.9945641160011292, 0.9649034738540649, 1.0161292552947998, 1.0154393911361694, 1.0439785718917847, 0.9783185124397278, 0.9469854831695557, 0.969061553478241, 1.0183733701705933, 1.0022947788238525, 0.9772897362709045, 0.9885573983192444, 0.9834561944007874, 1.0641926527023315, 0.9888766407966614, 1.0009198188781738, 0.9655094742774963, 0.9889665246009827, 0.9967166185379028, 1.0149093866348267, 0.975545346736908, 0.9755406975746155, 0.9546028971672058, 0.9354798793792725]
[2025-05-21 09:53:04,906]: Mean: 0.99167359
[2025-05-21 09:53:04,907]: Min: 0.91496944
[2025-05-21 09:53:04,907]: Max: 1.06419265
[2025-05-21 09:53:04,908]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-21 09:53:04,909]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06648200005292892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06648200005292892, 0.0, 0.0, 0.0, 0.0, 0.06648200005292892]
[2025-05-21 09:53:04,909]: Mean: -0.00066457
[2025-05-21 09:53:04,909]: Min: -0.06648200
[2025-05-21 09:53:04,909]: Max: 0.13296400
[2025-05-21 09:53:04,909]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-21 09:53:04,909]: Sample Values (25 elements): [0.9552182555198669, 0.9797950983047485, 0.9538843631744385, 0.932425856590271, 0.9866938591003418, 0.9180346727371216, 0.9580414295196533, 0.9845839142799377, 0.9699935913085938, 0.9570545554161072, 0.9329226016998291, 0.9621632099151611, 0.9622611403465271, 0.9842939376831055, 0.9635143280029297, 0.936072826385498, 0.9607522487640381, 0.9761887192726135, 0.9541689157485962, 0.9261467456817627, 0.9170365333557129, 0.9533313512802124, 0.9663288593292236, 0.9131971597671509, 0.9494431018829346]
[2025-05-21 09:53:04,910]: Mean: 0.96092415
[2025-05-21 09:53:04,910]: Min: 0.91319716
[2025-05-21 09:53:04,910]: Max: 1.04502964
[2025-05-21 09:53:04,911]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 09:53:04,912]: Sample Values (25 elements): [0.0, -0.06564532965421677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06564532965421677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,912]: Mean: -0.00041847
[2025-05-21 09:53:04,912]: Min: -0.06564533
[2025-05-21 09:53:04,913]: Max: 0.13129066
[2025-05-21 09:53:04,913]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-21 09:53:04,913]: Sample Values (25 elements): [0.9914118051528931, 0.9899818897247314, 0.9608743786811829, 0.9380544424057007, 0.9817392826080322, 0.9949953556060791, 0.9441027641296387, 0.9182202219963074, 0.9717559814453125, 0.9736462235450745, 0.9525113701820374, 0.9833433628082275, 0.9611882567405701, 0.9551135897636414, 0.9427253007888794, 0.9375417232513428, 0.9419044852256775, 0.9379152059555054, 0.9739911556243896, 0.9976033568382263, 1.0051100254058838, 0.9283488392829895, 0.9783252477645874, 0.9604556560516357, 0.976058304309845]
[2025-05-21 09:53:04,913]: Mean: 0.96404499
[2025-05-21 09:53:04,913]: Min: 0.91599041
[2025-05-21 09:53:04,913]: Max: 1.03617597
[2025-05-21 09:53:04,914]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-21 09:53:04,915]: Sample Values (25 elements): [0.0, 0.12830199301242828, 0.12830199301242828, 0.0, 0.0, 0.12830199301242828, -0.12830199301242828, 0.0, 0.12830199301242828, 0.0, -0.12830199301242828, 0.0, -0.12830199301242828, 0.0, -0.12830199301242828, 0.0, 0.0, -0.12830199301242828, -0.12830199301242828, 0.12830199301242828, -0.12830199301242828, 0.0, -0.12830199301242828, 0.12830199301242828, 0.0]
[2025-05-21 09:53:04,915]: Mean: -0.00023493
[2025-05-21 09:53:04,915]: Min: -0.12830199
[2025-05-21 09:53:04,915]: Max: 0.25660399
[2025-05-21 09:53:04,915]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-21 09:53:04,915]: Sample Values (25 elements): [0.8382226824760437, 0.8600462079048157, 0.8430116772651672, 0.9112133383750916, 0.8644056916236877, 0.8732824325561523, 0.9018015265464783, 0.8662632703781128, 0.8565137982368469, 0.9002566337585449, 0.8836097121238708, 0.8836174607276917, 0.8889727592468262, 0.8519848585128784, 0.8927773237228394, 0.8749983310699463, 0.8617115616798401, 0.9102345108985901, 0.8987232446670532, 0.8586868047714233, 0.8893989324569702, 0.8778713941574097, 0.9065665602684021, 0.8611506223678589, 0.8684125542640686]
[2025-05-21 09:53:04,916]: Mean: 0.88314533
[2025-05-21 09:53:04,916]: Min: 0.79027718
[2025-05-21 09:53:04,916]: Max: 0.93674237
[2025-05-21 09:53:04,917]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 09:53:04,918]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, -0.06372648477554321, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06372648477554321, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06372648477554321, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,918]: Mean: -0.00068672
[2025-05-21 09:53:04,918]: Min: -0.06372648
[2025-05-21 09:53:04,919]: Max: 0.12745297
[2025-05-21 09:53:04,919]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-21 09:53:04,919]: Sample Values (25 elements): [0.9287553429603577, 0.9036204218864441, 0.9379391670227051, 0.9251236915588379, 0.9823745489120483, 0.9146226048469543, 0.9025348424911499, 0.9391393065452576, 0.9055574536323547, 0.9723844528198242, 0.9581477642059326, 0.9420614242553711, 0.941354513168335, 1.0001505613327026, 0.9575314521789551, 0.9035288691520691, 0.9338772892951965, 0.91229248046875, 0.9414256811141968, 0.9238083362579346, 0.92030930519104, 0.9202120304107666, 0.9482994675636292, 1.0028297901153564, 0.9396848678588867]
[2025-05-21 09:53:04,919]: Mean: 0.94035226
[2025-05-21 09:53:04,919]: Min: 0.87630969
[2025-05-21 09:53:04,919]: Max: 1.03890705
[2025-05-21 09:53:04,920]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 09:53:04,922]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.060753341764211655, 0.0, 0.0, 0.0, 0.060753341764211655, 0.0, 0.060753341764211655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.060753341764211655, -0.060753341764211655]
[2025-05-21 09:53:04,922]: Mean: -0.00019776
[2025-05-21 09:53:04,922]: Min: -0.06075334
[2025-05-21 09:53:04,922]: Max: 0.12150668
[2025-05-21 09:53:04,922]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-21 09:53:04,923]: Sample Values (25 elements): [0.964173436164856, 0.9905405044555664, 0.9764841794967651, 0.9595732688903809, 0.9637075066566467, 0.9507209658622742, 0.9647225141525269, 0.9709970355033875, 0.9739426374435425, 0.9517888426780701, 1.0121607780456543, 0.9883390069007874, 0.9161850214004517, 0.9507790803909302, 0.951164186000824, 0.9718889594078064, 0.9385290741920471, 0.9560403823852539, 0.9387298226356506, 0.9541052579879761, 0.955250084400177, 0.9524816274642944, 0.9676886796951294, 0.9587432742118835, 1.0341581106185913]
[2025-05-21 09:53:04,923]: Mean: 0.96325386
[2025-05-21 09:53:04,923]: Min: 0.91618502
[2025-05-21 09:53:04,923]: Max: 1.03415811
[2025-05-21 09:53:04,924]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-21 09:53:04,927]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04784051701426506, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04784051701426506, 0.0, 0.0, 0.0, -0.04784051701426506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,927]: Mean: -0.00032817
[2025-05-21 09:53:04,927]: Min: -0.04784052
[2025-05-21 09:53:04,927]: Max: 0.09568103
[2025-05-21 09:53:04,927]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-21 09:53:04,927]: Sample Values (25 elements): [0.933449923992157, 0.9329474568367004, 0.9387277364730835, 0.9229095578193665, 0.9499834775924683, 0.9191752672195435, 0.9213292598724365, 0.9682413935661316, 0.922176718711853, 0.9398001432418823, 0.9292007684707642, 0.9367153644561768, 0.9398494958877563, 0.9264102578163147, 0.9312587976455688, 0.9253995418548584, 0.9721747636795044, 0.9486176371574402, 0.9355233907699585, 0.9463062882423401, 0.9324294924736023, 0.9222320914268494, 0.946179211139679, 0.9123854637145996, 0.9584581851959229]
[2025-05-21 09:53:04,928]: Mean: 0.94008696
[2025-05-21 09:53:04,928]: Min: 0.91056687
[2025-05-21 09:53:04,928]: Max: 1.00384486
[2025-05-21 09:53:04,929]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 09:53:04,935]: Sample Values (25 elements): [0.0, 0.0, 0.04359246417880058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04359246417880058, 0.0, 0.0, 0.04359246417880058, 0.04359246417880058, 0.0, 0.0, 0.0, 0.04359246417880058, 0.0, -0.04359246417880058, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,935]: Mean: -0.00045254
[2025-05-21 09:53:04,935]: Min: -0.04359246
[2025-05-21 09:53:04,935]: Max: 0.08718493
[2025-05-21 09:53:04,935]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-21 09:53:04,935]: Sample Values (25 elements): [0.9432328939437866, 0.9574370384216309, 0.9462274312973022, 0.9470822811126709, 0.9396383166313171, 0.9329042434692383, 0.9874146580696106, 0.97867351770401, 0.9497529864311218, 0.9339510798454285, 0.951062023639679, 0.9345542788505554, 0.9631004929542542, 1.0010676383972168, 0.9423344731330872, 0.9667187333106995, 0.9368693828582764, 0.9354978203773499, 0.9191420674324036, 0.9340800642967224, 0.9253574013710022, 0.9773210287094116, 0.9380025863647461, 0.9866238832473755, 0.9303502440452576]
[2025-05-21 09:53:04,936]: Mean: 0.94521075
[2025-05-21 09:53:04,936]: Min: 0.90118825
[2025-05-21 09:53:04,936]: Max: 1.00106764
[2025-05-21 09:53:04,937]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-21 09:53:04,937]: Sample Values (25 elements): [0.0, 0.09022629261016846, 0.0, 0.0, -0.09022629261016846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09022629261016846, 0.0, 0.09022629261016846, 0.0, 0.0, -0.09022629261016846, 0.0, 0.09022629261016846, 0.0, -0.09022629261016846, 0.09022629261016846, -0.09022629261016846, 0.09022629261016846, 0.0]
[2025-05-21 09:53:04,938]: Mean: -0.00079851
[2025-05-21 09:53:04,938]: Min: -0.09022629
[2025-05-21 09:53:04,938]: Max: 0.18045259
[2025-05-21 09:53:04,938]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-21 09:53:04,938]: Sample Values (25 elements): [0.8779891133308411, 0.9003198146820068, 0.8876402974128723, 0.8864390254020691, 0.890224039554596, 0.8782435059547424, 0.8721352219581604, 0.8906840085983276, 0.8976917862892151, 0.8937418460845947, 0.8804493546485901, 0.9017007946968079, 0.8843561410903931, 0.8815138339996338, 0.8877284526824951, 0.9007890224456787, 0.8947693109512329, 0.891676664352417, 0.887307345867157, 0.8697766065597534, 0.9060672521591187, 0.8810409903526306, 0.8922542929649353, 0.8892675638198853, 0.8546599745750427]
[2025-05-21 09:53:04,938]: Mean: 0.88381934
[2025-05-21 09:53:04,938]: Min: 0.85104191
[2025-05-21 09:53:04,939]: Max: 0.92858112
[2025-05-21 09:53:04,940]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 09:53:04,945]: Sample Values (25 elements): [0.0, -0.04662496596574783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:04,946]: Mean: -0.00045184
[2025-05-21 09:53:04,946]: Min: -0.09324993
[2025-05-21 09:53:04,946]: Max: 0.04662497
[2025-05-21 09:53:04,946]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-21 09:53:04,946]: Sample Values (25 elements): [0.9308672547340393, 0.922522783279419, 0.9292066097259521, 0.9191457629203796, 0.9201661348342896, 0.9128354787826538, 0.9197330474853516, 0.9217333793640137, 0.9277913570404053, 0.9057314991950989, 0.9257979989051819, 0.9368085861206055, 0.9124966263771057, 0.9107134342193604, 0.9238981008529663, 0.9321010112762451, 0.9487971067428589, 0.9363874197006226, 0.9501187205314636, 0.9379254579544067, 0.9298670291900635, 0.9261366724967957, 0.931590735912323, 0.9216363430023193, 0.9354305863380432]
[2025-05-21 09:53:04,946]: Mean: 0.92697465
[2025-05-21 09:53:04,947]: Min: 0.89238018
[2025-05-21 09:53:04,947]: Max: 0.96824056
[2025-05-21 09:53:04,948]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 09:53:04,954]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0381072461605072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0381072461605072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0381072461605072, 0.0]
[2025-05-21 09:53:04,954]: Mean: -0.00014588
[2025-05-21 09:53:04,954]: Min: -0.03810725
[2025-05-21 09:53:04,954]: Max: 0.07621449
[2025-05-21 09:53:04,954]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-21 09:53:04,954]: Sample Values (25 elements): [0.9469144940376282, 0.9457201361656189, 0.9431493878364563, 0.960746169090271, 0.9422861337661743, 0.9304240942001343, 0.9565240740776062, 0.9488164186477661, 0.9530826210975647, 0.9272260069847107, 0.9498218297958374, 0.9569193124771118, 0.9326414465904236, 0.941605269908905, 0.9295166730880737, 0.931760847568512, 0.9355027675628662, 0.9647660255432129, 0.9417703747749329, 0.9398527145385742, 0.946652352809906, 0.9346596598625183, 0.9343791007995605, 0.9448398351669312, 0.9318700432777405]
[2025-05-21 09:53:04,955]: Mean: 0.93984640
[2025-05-21 09:53:04,955]: Min: 0.91279966
[2025-05-21 09:53:04,955]: Max: 0.98685616
[2025-05-21 09:53:04,956]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-21 09:53:04,972]: Sample Values (25 elements): [-0.03476506844162941, 0.03476506844162941, 0.0, 0.0, 0.03476506844162941, 0.0, 0.0, 0.0, 0.0, 0.03476506844162941, 0.0, 0.0, 0.03476506844162941, 0.0, -0.03476506844162941, 0.0, 0.0, 0.0, 0.0, -0.03476506844162941, 0.0, 0.0, 0.0, -0.03476506844162941, 0.0]
[2025-05-21 09:53:04,972]: Mean: -0.00004727
[2025-05-21 09:53:04,972]: Min: -0.03476507
[2025-05-21 09:53:04,972]: Max: 0.06953014
[2025-05-21 09:53:04,973]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-21 09:53:04,973]: Sample Values (25 elements): [0.9136476516723633, 0.9151526093482971, 0.9100289344787598, 0.935189962387085, 0.92229163646698, 0.9119138121604919, 0.9373988509178162, 0.9261909127235413, 0.9104695320129395, 0.923594057559967, 0.9112187623977661, 0.9257429242134094, 0.9189518690109253, 0.9288938045501709, 0.9185888767242432, 0.9218547940254211, 0.9267699718475342, 0.9197288751602173, 0.9202158451080322, 0.9106994867324829, 0.9140624403953552, 0.9174636602401733, 0.9194097518920898, 0.9128559827804565, 0.9178427457809448]
[2025-05-21 09:53:04,973]: Mean: 0.91926444
[2025-05-21 09:53:04,973]: Min: 0.90270221
[2025-05-21 09:53:04,973]: Max: 0.94356889
[2025-05-21 09:53:04,974]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 09:53:05,018]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.026766836643218994, -0.026766836643218994, 0.0, 0.026766836643218994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026766836643218994, 0.0, 0.0, 0.0]
[2025-05-21 09:53:05,018]: Mean: -0.00013234
[2025-05-21 09:53:05,018]: Min: -0.02676684
[2025-05-21 09:53:05,018]: Max: 0.05353367
[2025-05-21 09:53:05,018]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-21 09:53:05,019]: Sample Values (25 elements): [0.9231996536254883, 0.9145742058753967, 0.9048997163772583, 0.9107804894447327, 0.913121223449707, 0.919964075088501, 0.9047989249229431, 0.9135136604309082, 0.9134877920150757, 0.9056071043014526, 0.9027677178382874, 0.9192626476287842, 0.9119547009468079, 0.9219262003898621, 0.9067906737327576, 0.9212075471878052, 0.9114195704460144, 0.9099450707435608, 0.9171454906463623, 0.9207634925842285, 0.9070157408714294, 0.9029092788696289, 0.9072418212890625, 0.9148887991905212, 0.9053768515586853]
[2025-05-21 09:53:05,019]: Mean: 0.91086179
[2025-05-21 09:53:05,019]: Min: 0.89569783
[2025-05-21 09:53:05,019]: Max: 0.94159788
[2025-05-21 09:53:05,020]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-21 09:53:05,021]: Sample Values (25 elements): [-0.0567827969789505, 0.0, 0.0567827969789505, -0.0567827969789505, -0.0567827969789505, 0.0, 0.0567827969789505, 0.0, -0.0567827969789505, -0.0567827969789505, 0.0567827969789505, -0.0567827969789505, 0.0, 0.0567827969789505, 0.0567827969789505, 0.0, 0.0567827969789505, 0.0567827969789505, 0.0, 0.0, -0.0567827969789505, 0.0567827969789505, -0.0567827969789505, 0.0567827969789505, 0.0567827969789505]
[2025-05-21 09:53:05,022]: Mean: 0.00011480
[2025-05-21 09:53:05,022]: Min: -0.11356559
[2025-05-21 09:53:05,022]: Max: 0.05678280
[2025-05-21 09:53:05,022]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-21 09:53:05,022]: Sample Values (25 elements): [0.8954266905784607, 0.8978976011276245, 0.8987624645233154, 0.9059068560600281, 0.9018681645393372, 0.8921129703521729, 0.9016037583351135, 0.9015463590621948, 0.8977312445640564, 0.8948124647140503, 0.8984836935997009, 0.8988330364227295, 0.8999108076095581, 0.8995265364646912, 0.8998245000839233, 0.8916189074516296, 0.8977435827255249, 0.8980001211166382, 0.895798921585083, 0.8996838927268982, 0.8951830267906189, 0.8930124044418335, 0.8988070487976074, 0.8912109732627869, 0.9034005403518677]
[2025-05-21 09:53:05,022]: Mean: 0.89814776
[2025-05-21 09:53:05,023]: Min: 0.88334692
[2025-05-21 09:53:05,023]: Max: 0.91480088
[2025-05-21 09:53:05,024]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 09:53:05,065]: Sample Values (25 elements): [0.0, 0.0, 0.023630212992429733, 0.0, 0.0, 0.023630212992429733, 0.0, 0.0, 0.0, 0.023630212992429733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023630212992429733, 0.0, 0.0, 0.0, -0.023630212992429733, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 09:53:05,066]: Mean: -0.00020384
[2025-05-21 09:53:05,066]: Min: -0.02363021
[2025-05-21 09:53:05,066]: Max: 0.04726043
[2025-05-21 09:53:05,066]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-21 09:53:05,068]: Sample Values (25 elements): [0.916970431804657, 0.9086601734161377, 0.9129436612129211, 0.9104785323143005, 0.9102740287780762, 0.9056256413459778, 0.910973072052002, 0.9093778133392334, 0.9100724458694458, 0.9123024940490723, 0.9105307459831238, 0.9066420793533325, 0.910540759563446, 0.9088019728660583, 0.9136946201324463, 0.909612238407135, 0.9075076580047607, 0.908077597618103, 0.9100937247276306, 0.9166607856750488, 0.9114087224006653, 0.910279393196106, 0.9152706265449524, 0.9076555967330933, 0.9094077944755554]
[2025-05-21 09:53:05,068]: Mean: 0.91096222
[2025-05-21 09:53:05,068]: Min: 0.90070957
[2025-05-21 09:53:05,068]: Max: 0.93078721
[2025-05-21 09:53:05,069]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 09:53:05,113]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017379671335220337, 0.0, 0.0, -0.017379671335220337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017379671335220337, 0.0, 0.0, -0.017379671335220337, 0.0, 0.0]
[2025-05-21 09:53:05,113]: Mean: 0.00023651
[2025-05-21 09:53:05,113]: Min: -0.01737967
[2025-05-21 09:53:05,113]: Max: 0.03475934
[2025-05-21 09:53:05,114]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-21 09:53:05,114]: Sample Values (25 elements): [0.9093745946884155, 0.9044812321662903, 0.8975663781166077, 0.9062303304672241, 0.9020258188247681, 0.9042056202888489, 0.9092519283294678, 0.9032215476036072, 0.9013873338699341, 0.9033217430114746, 0.90252286195755, 0.907963216304779, 0.9061102271080017, 0.9001280069351196, 0.9054875373840332, 0.907721996307373, 0.9084053635597229, 0.9060903787612915, 0.906116247177124, 0.9066802859306335, 0.903357744216919, 0.9067742824554443, 0.8992501497268677, 0.9053389430046082, 0.9072965979576111]
[2025-05-21 09:53:05,114]: Mean: 0.90477246
[2025-05-21 09:53:05,114]: Min: 0.89591032
[2025-05-21 09:53:05,114]: Max: 0.91719449
[2025-05-21 09:53:05,114]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-21 09:53:05,115]: Sample Values (25 elements): [0.04390380531549454, -0.001089638564735651, 0.0916535034775734, -0.06429646164178848, -0.011279743164777756, -0.06069115921854973, -0.0213130172342062, -0.01221719104796648, -0.052537184208631516, 0.017365945503115654, 0.07228095084428787, -0.04096948355436325, -0.11160174012184143, -0.07190502434968948, -0.0817883089184761, 0.03191402181982994, -0.007690886501222849, 0.012575090862810612, -0.07587827742099762, 0.03747185692191124, 0.01824861951172352, 0.002000259468331933, 0.06109383702278137, 0.13220907747745514, -0.08475076407194138]
[2025-05-21 09:53:05,115]: Mean: -0.00024700
[2025-05-21 09:53:05,115]: Min: -0.18303907
[2025-05-21 09:53:05,115]: Max: 0.17098573
[2025-05-21 09:53:05,115]: 


QAT of ResNet18 with parametrized_relu down to 3 bits...
[2025-05-21 09:53:05,308]: [ResNet18_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-05-21 09:53:05,334]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 09:55:09,932]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.7814 Train Acc: 0.7382 Eval Loss: 0.7551 Eval Acc: 0.7557 (LR: 0.010000)
[2025-05-21 09:57:14,457]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.5075 Train Acc: 0.8228 Eval Loss: 0.5822 Eval Acc: 0.8118 (LR: 0.010000)
[2025-05-21 09:59:19,324]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.4379 Train Acc: 0.8476 Eval Loss: 0.6414 Eval Acc: 0.8088 (LR: 0.010000)
[2025-05-21 10:01:23,814]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.4012 Train Acc: 0.8597 Eval Loss: 0.5763 Eval Acc: 0.8217 (LR: 0.010000)
[2025-05-21 10:03:28,468]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.3707 Train Acc: 0.8721 Eval Loss: 0.4792 Eval Acc: 0.8502 (LR: 0.010000)
[2025-05-21 10:05:33,099]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.3471 Train Acc: 0.8770 Eval Loss: 0.4643 Eval Acc: 0.8502 (LR: 0.010000)
[2025-05-21 10:07:37,416]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.3220 Train Acc: 0.8873 Eval Loss: 0.4385 Eval Acc: 0.8612 (LR: 0.010000)
[2025-05-21 10:09:41,709]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.3093 Train Acc: 0.8915 Eval Loss: 0.4727 Eval Acc: 0.8531 (LR: 0.010000)
[2025-05-21 10:11:46,005]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.2953 Train Acc: 0.8971 Eval Loss: 0.4025 Eval Acc: 0.8745 (LR: 0.010000)
[2025-05-21 10:13:50,093]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.2811 Train Acc: 0.9017 Eval Loss: 0.5302 Eval Acc: 0.8409 (LR: 0.010000)
[2025-05-21 10:15:54,358]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.2694 Train Acc: 0.9055 Eval Loss: 0.4382 Eval Acc: 0.8645 (LR: 0.010000)
[2025-05-21 10:17:58,484]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.2518 Train Acc: 0.9109 Eval Loss: 0.4520 Eval Acc: 0.8642 (LR: 0.010000)
[2025-05-21 10:20:02,611]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.2491 Train Acc: 0.9115 Eval Loss: 0.5932 Eval Acc: 0.8382 (LR: 0.010000)
[2025-05-21 10:22:06,671]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.2378 Train Acc: 0.9150 Eval Loss: 0.4783 Eval Acc: 0.8553 (LR: 0.010000)
[2025-05-21 10:24:10,936]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.2297 Train Acc: 0.9191 Eval Loss: 0.3878 Eval Acc: 0.8828 (LR: 0.001000)
[2025-05-21 10:26:15,047]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.1472 Train Acc: 0.9490 Eval Loss: 0.2738 Eval Acc: 0.9134 (LR: 0.001000)
[2025-05-21 10:28:19,349]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.1216 Train Acc: 0.9579 Eval Loss: 0.2777 Eval Acc: 0.9163 (LR: 0.001000)
[2025-05-21 10:30:23,934]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.1150 Train Acc: 0.9603 Eval Loss: 0.2696 Eval Acc: 0.9184 (LR: 0.001000)
[2025-05-21 10:32:28,720]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.1094 Train Acc: 0.9611 Eval Loss: 0.2742 Eval Acc: 0.9181 (LR: 0.001000)
[2025-05-21 10:34:33,968]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.1069 Train Acc: 0.9633 Eval Loss: 0.2693 Eval Acc: 0.9187 (LR: 0.001000)
[2025-05-21 10:36:38,370]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.1004 Train Acc: 0.9651 Eval Loss: 0.2742 Eval Acc: 0.9181 (LR: 0.001000)
[2025-05-21 10:38:42,831]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.0988 Train Acc: 0.9653 Eval Loss: 0.2771 Eval Acc: 0.9180 (LR: 0.001000)
[2025-05-21 10:40:46,886]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.0956 Train Acc: 0.9665 Eval Loss: 0.2779 Eval Acc: 0.9178 (LR: 0.001000)
[2025-05-21 10:42:51,179]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.0905 Train Acc: 0.9683 Eval Loss: 0.2790 Eval Acc: 0.9178 (LR: 0.001000)
[2025-05-21 10:44:40,715]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.0895 Train Acc: 0.9688 Eval Loss: 0.2901 Eval Acc: 0.9186 (LR: 0.001000)
[2025-05-21 10:46:28,765]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.0871 Train Acc: 0.9693 Eval Loss: 0.2925 Eval Acc: 0.9150 (LR: 0.001000)
[2025-05-21 10:48:17,937]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.0852 Train Acc: 0.9707 Eval Loss: 0.2837 Eval Acc: 0.9189 (LR: 0.001000)
[2025-05-21 10:50:07,266]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.0825 Train Acc: 0.9704 Eval Loss: 0.2930 Eval Acc: 0.9174 (LR: 0.001000)
[2025-05-21 10:51:56,245]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.0842 Train Acc: 0.9705 Eval Loss: 0.2967 Eval Acc: 0.9173 (LR: 0.001000)
[2025-05-21 10:53:45,389]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.0785 Train Acc: 0.9723 Eval Loss: 0.2999 Eval Acc: 0.9176 (LR: 0.000100)
[2025-05-21 10:55:31,456]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.0731 Train Acc: 0.9748 Eval Loss: 0.2871 Eval Acc: 0.9208 (LR: 0.000100)
[2025-05-21 10:57:14,908]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.0711 Train Acc: 0.9751 Eval Loss: 0.2895 Eval Acc: 0.9199 (LR: 0.000100)
[2025-05-21 10:58:58,489]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.0733 Train Acc: 0.9739 Eval Loss: 0.2929 Eval Acc: 0.9179 (LR: 0.000100)
[2025-05-21 11:00:44,436]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.0710 Train Acc: 0.9755 Eval Loss: 0.2894 Eval Acc: 0.9204 (LR: 0.000100)
[2025-05-21 11:02:31,778]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.0683 Train Acc: 0.9767 Eval Loss: 0.2916 Eval Acc: 0.9200 (LR: 0.000100)
[2025-05-21 11:04:32,606]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.0712 Train Acc: 0.9755 Eval Loss: 0.2885 Eval Acc: 0.9191 (LR: 0.000100)
[2025-05-21 11:06:38,300]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.0686 Train Acc: 0.9767 Eval Loss: 0.2851 Eval Acc: 0.9217 (LR: 0.000100)
[2025-05-21 11:08:44,291]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.0668 Train Acc: 0.9771 Eval Loss: 0.2875 Eval Acc: 0.9208 (LR: 0.000100)
[2025-05-21 11:10:49,971]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.0678 Train Acc: 0.9775 Eval Loss: 0.2865 Eval Acc: 0.9206 (LR: 0.000100)
[2025-05-21 11:12:54,511]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.0656 Train Acc: 0.9776 Eval Loss: 0.2882 Eval Acc: 0.9200 (LR: 0.000100)
[2025-05-21 11:14:47,837]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.0670 Train Acc: 0.9775 Eval Loss: 0.2904 Eval Acc: 0.9177 (LR: 0.000100)
[2025-05-21 11:16:35,285]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.0659 Train Acc: 0.9774 Eval Loss: 0.2864 Eval Acc: 0.9192 (LR: 0.000100)
[2025-05-21 11:18:22,776]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.0672 Train Acc: 0.9766 Eval Loss: 0.2907 Eval Acc: 0.9213 (LR: 0.000100)
[2025-05-21 11:20:10,298]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.0655 Train Acc: 0.9775 Eval Loss: 0.2908 Eval Acc: 0.9212 (LR: 0.000100)
[2025-05-21 11:21:57,475]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.0668 Train Acc: 0.9768 Eval Loss: 0.2887 Eval Acc: 0.9190 (LR: 0.000010)
[2025-05-21 11:23:44,847]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.0657 Train Acc: 0.9778 Eval Loss: 0.2910 Eval Acc: 0.9205 (LR: 0.000010)
[2025-05-21 11:25:32,419]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.0662 Train Acc: 0.9769 Eval Loss: 0.2866 Eval Acc: 0.9213 (LR: 0.000010)
[2025-05-21 11:27:19,789]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.0666 Train Acc: 0.9770 Eval Loss: 0.2905 Eval Acc: 0.9210 (LR: 0.000010)
[2025-05-21 11:29:07,373]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.0658 Train Acc: 0.9776 Eval Loss: 0.2911 Eval Acc: 0.9203 (LR: 0.000010)
[2025-05-21 11:30:49,469]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.0650 Train Acc: 0.9775 Eval Loss: 0.2932 Eval Acc: 0.9192 (LR: 0.000010)
[2025-05-21 11:32:31,405]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.0627 Train Acc: 0.9781 Eval Loss: 0.2919 Eval Acc: 0.9191 (LR: 0.000010)
[2025-05-21 11:34:13,366]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.0639 Train Acc: 0.9776 Eval Loss: 0.2901 Eval Acc: 0.9201 (LR: 0.000010)
[2025-05-21 11:35:55,314]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.0653 Train Acc: 0.9771 Eval Loss: 0.2895 Eval Acc: 0.9201 (LR: 0.000010)
[2025-05-21 11:37:37,108]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.0659 Train Acc: 0.9774 Eval Loss: 0.2945 Eval Acc: 0.9215 (LR: 0.000010)
[2025-05-21 11:39:19,065]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.0648 Train Acc: 0.9783 Eval Loss: 0.2882 Eval Acc: 0.9197 (LR: 0.000010)
[2025-05-21 11:41:00,831]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.0645 Train Acc: 0.9782 Eval Loss: 0.2880 Eval Acc: 0.9204 (LR: 0.000010)
[2025-05-21 11:42:42,772]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.0641 Train Acc: 0.9781 Eval Loss: 0.2882 Eval Acc: 0.9207 (LR: 0.000010)
[2025-05-21 11:44:24,538]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.0636 Train Acc: 0.9782 Eval Loss: 0.2895 Eval Acc: 0.9197 (LR: 0.000010)
[2025-05-21 11:46:06,297]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.0647 Train Acc: 0.9776 Eval Loss: 0.2884 Eval Acc: 0.9214 (LR: 0.000010)
[2025-05-21 11:47:49,067]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.0636 Train Acc: 0.9783 Eval Loss: 0.2903 Eval Acc: 0.9202 (LR: 0.000010)
[2025-05-21 11:47:49,068]: [ResNet18_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.9217
[2025-05-21 11:47:49,165]: 


Quantization of model down to 3 bits finished
[2025-05-21 11:47:49,165]: Model Architecture:
[2025-05-21 11:47:49,213]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8571], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0754], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28605586290359497, max_val=0.24188734591007233)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7970], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0589], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18112583458423615, max_val=0.23100629448890686)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7828], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0429], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14503011107444763, max_val=0.15510550141334534)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0347], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10502047836780548, max_val=0.13770565390586853)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0305], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10032506287097931, max_val=0.11284598708152771)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0293], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09845146536827087, max_val=0.10688448697328568)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0555], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1937432438135147, max_val=0.19488157331943512)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8060], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0264], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0907362625002861, max_val=0.09427022933959961)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09288817644119263, max_val=0.09230244159698486)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8325], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0212], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07181280851364136, max_val=0.07684791833162308)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0201], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07039020210504532, max_val=0.07045295834541321)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0379], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.12533628940582275, max_val=0.13966965675354004)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8098], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0191], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06375997513532639, max_val=0.07003048807382584)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0164], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05704368278384209, max_val=0.05785484239459038)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8306], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0142], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.049468882381916046, max_val=0.05023171007633209)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0125], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04195323958992958, max_val=0.04530194401741028)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0252], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0869038924574852, max_val=0.08928995579481125)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8074], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03750878572463989, max_val=0.04197043552994728)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02372284233570099, max_val=0.03004385344684124)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.7975], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 11:47:49,213]: 
Model Weights:
[2025-05-21 11:47:49,213]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-21 11:47:49,214]: Sample Values (25 elements): [-0.009129605256021023, -0.2690781056880951, -0.15584740042686462, 0.5417826175689697, 0.07350362837314606, 0.12037219852209091, 0.15987662971019745, -0.011797288432717323, -0.2683435380458832, -0.18800236284732819, -0.28637272119522095, -0.0945192351937294, -0.06073295325040817, 0.12846368551254272, -0.07927000522613525, -0.08788609504699707, -0.18210667371749878, -0.14263732731342316, -0.04229247197508812, 0.19816964864730835, 0.01860210858285427, -0.008268877863883972, -0.15148431062698364, 0.09606398642063141, -0.03411442041397095]
[2025-05-21 11:47:49,214]: Mean: -0.00059108
[2025-05-21 11:47:49,214]: Min: -0.52750665
[2025-05-21 11:47:49,214]: Max: 0.54606605
[2025-05-21 11:47:49,214]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-21 11:47:49,215]: Sample Values (25 elements): [1.1170586347579956, 1.1803454160690308, 0.7153321504592896, 0.9138625860214233, 1.1216548681259155, 1.0946933031082153, 0.989075779914856, 0.9959461688995361, 0.9805676341056824, 0.9485265612602234, 0.9633600115776062, 1.0004826784133911, 0.9612876772880554, 0.8932506442070007, 0.9747354984283447, 0.9279447793960571, 0.898157000541687, 1.043015718460083, 0.912294864654541, 0.9208234548568726, 1.2050304412841797, 0.9837521910667419, 0.7972850799560547, 0.9471243619918823, 1.0576881170272827]
[2025-05-21 11:47:49,215]: Mean: 0.96231627
[2025-05-21 11:47:49,215]: Min: 0.71533215
[2025-05-21 11:47:49,215]: Max: 1.27438283
[2025-05-21 11:47:49,217]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 11:47:49,217]: Sample Values (25 elements): [0.07542046159505844, 0.0, 0.0, 0.0, -0.07542046159505844, 0.0, 0.0, -0.07542046159505844, 0.0, 0.07542046159505844, -0.07542046159505844, 0.0, 0.07542046159505844, 0.0, 0.0, 0.07542046159505844, -0.07542046159505844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07542046159505844, 0.0, -0.07542046159505844]
[2025-05-21 11:47:49,218]: Mean: -0.00304022
[2025-05-21 11:47:49,218]: Min: -0.30168185
[2025-05-21 11:47:49,218]: Max: 0.22626138
[2025-05-21 11:47:49,218]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 11:47:49,218]: Sample Values (25 elements): [0.8846004009246826, 0.9493608474731445, 0.958641529083252, 0.9318755865097046, 0.9509586691856384, 0.8729940056800842, 0.9533836841583252, 0.8813065886497498, 0.9578279852867126, 0.8738268613815308, 0.8771860599517822, 0.9730671048164368, 0.8889416456222534, 1.285045862197876, 0.9841081500053406, 0.9845784306526184, 0.9553810358047485, 0.8723169565200806, 0.962975263595581, 0.90376877784729, 0.9647111296653748, 0.9266944527626038, 0.8691582679748535, 1.2010135650634766, 0.9838032722473145]
[2025-05-21 11:47:49,218]: Mean: 0.96451652
[2025-05-21 11:47:49,218]: Min: 0.85985452
[2025-05-21 11:47:49,219]: Max: 1.35431600
[2025-05-21 11:47:49,220]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 11:47:49,220]: Sample Values (25 elements): [-0.05887601524591446, 0.0, 0.0, 0.0, 0.05887601524591446, 0.05887601524591446, 0.0, -0.05887601524591446, -0.05887601524591446, 0.0, 0.05887601524591446, -0.05887601524591446, 0.0, 0.0, 0.0, 0.0, -0.05887601524591446, 0.05887601524591446, 0.05887601524591446, 0.0, 0.0, 0.0, 0.05887601524591446, 0.0, 0.0]
[2025-05-21 11:47:49,220]: Mean: -0.00195167
[2025-05-21 11:47:49,220]: Min: -0.17662805
[2025-05-21 11:47:49,221]: Max: 0.23550406
[2025-05-21 11:47:49,221]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 11:47:49,221]: Sample Values (25 elements): [0.936025083065033, 0.9409789443016052, 0.9622969031333923, 1.0151666402816772, 1.016858458518982, 0.9782741665840149, 0.945364236831665, 0.9213912487030029, 0.9943850636482239, 1.0068385601043701, 0.930172860622406, 0.9255578517913818, 0.9834958910942078, 0.9627159237861633, 1.2696422338485718, 1.3699734210968018, 1.1843924522399902, 0.9652737379074097, 0.9294394254684448, 0.9720179438591003, 0.8945565819740295, 0.9942882657051086, 0.917605459690094, 0.9613284468650818, 0.9493691921234131]
[2025-05-21 11:47:49,221]: Mean: 0.98725599
[2025-05-21 11:47:49,221]: Min: 0.88730997
[2025-05-21 11:47:49,222]: Max: 1.36997342
[2025-05-21 11:47:49,223]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 11:47:49,223]: Sample Values (25 elements): [-0.04287651926279068, 0.0, 0.04287651926279068, 0.04287651926279068, 0.04287651926279068, 0.0, -0.04287651926279068, 0.0, -0.04287651926279068, 0.0, -0.04287651926279068, 0.0, 0.08575303852558136, -0.04287651926279068, -0.04287651926279068, 0.04287651926279068, -0.04287651926279068, 0.08575303852558136, -0.04287651926279068, 0.0, -0.08575303852558136, 0.0, -0.08575303852558136, -0.04287651926279068, 0.04287651926279068]
[2025-05-21 11:47:49,223]: Mean: -0.00208544
[2025-05-21 11:47:49,224]: Min: -0.12862957
[2025-05-21 11:47:49,224]: Max: 0.17150608
[2025-05-21 11:47:49,224]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 11:47:49,224]: Sample Values (25 elements): [0.9059741497039795, 0.9173922538757324, 0.9187849164009094, 0.9178152084350586, 0.9806854724884033, 0.9102907180786133, 0.9366525411605835, 0.9986419081687927, 0.947547435760498, 0.8929510116577148, 0.9460458755493164, 0.939814567565918, 0.9844844937324524, 0.9316073060035706, 0.9544093012809753, 0.9950889945030212, 1.0005316734313965, 0.9648520946502686, 0.93895423412323, 0.9175640940666199, 0.9200419783592224, 0.9309778809547424, 0.858633279800415, 0.9114070534706116, 0.90731281042099]
[2025-05-21 11:47:49,224]: Mean: 0.93395847
[2025-05-21 11:47:49,225]: Min: 0.85759610
[2025-05-21 11:47:49,225]: Max: 1.03400838
[2025-05-21 11:47:49,226]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 11:47:49,226]: Sample Values (25 elements): [-0.03467516228556633, 0.0, 0.0, 0.03467516228556633, 0.03467516228556633, 0.0, 0.0, 0.03467516228556633, -0.03467516228556633, 0.03467516228556633, 0.03467516228556633, 0.03467516228556633, 0.03467516228556633, -0.06935032457113266, 0.03467516228556633, 0.06935032457113266, 0.0, 0.06935032457113266, 0.03467516228556633, 0.03467516228556633, 0.0, 0.03467516228556633, -0.03467516228556633, -0.03467516228556633, 0.0]
[2025-05-21 11:47:49,227]: Mean: -0.00010911
[2025-05-21 11:47:49,227]: Min: -0.10402548
[2025-05-21 11:47:49,227]: Max: 0.13870065
[2025-05-21 11:47:49,227]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 11:47:49,227]: Sample Values (25 elements): [1.0034633874893188, 0.9014456868171692, 0.9613624215126038, 0.9461347460746765, 0.9338681697845459, 0.9614432454109192, 0.9915653467178345, 0.9412935972213745, 1.0036885738372803, 0.9608588814735413, 0.9308322072029114, 1.0071443319320679, 1.0239614248275757, 0.9872332215309143, 0.9696184396743774, 0.9629247784614563, 0.9666878581047058, 0.9404167532920837, 0.9405997395515442, 0.9506077170372009, 0.9688330292701721, 0.9637293815612793, 0.930538535118103, 0.9927718639373779, 0.9158463478088379]
[2025-05-21 11:47:49,227]: Mean: 0.95744842
[2025-05-21 11:47:49,228]: Min: 0.89525563
[2025-05-21 11:47:49,228]: Max: 1.03072584
[2025-05-21 11:47:49,229]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-21 11:47:49,230]: Sample Values (25 elements): [0.0, -0.03045300766825676, 0.0, 0.03045300766825676, -0.06090601533651352, -0.03045300766825676, -0.03045300766825676, -0.03045300766825676, 0.06090601533651352, -0.03045300766825676, 0.03045300766825676, 0.0, 0.0, 0.0, -0.03045300766825676, 0.03045300766825676, 0.0, -0.03045300766825676, 0.0, 0.0, -0.03045300766825676, 0.06090601533651352, -0.03045300766825676, 0.03045300766825676, -0.03045300766825676]
[2025-05-21 11:47:49,231]: Mean: -0.00111192
[2025-05-21 11:47:49,231]: Min: -0.09135902
[2025-05-21 11:47:49,231]: Max: 0.12181203
[2025-05-21 11:47:49,231]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-21 11:47:49,231]: Sample Values (25 elements): [0.9250005483627319, 0.9227945804595947, 0.9240087866783142, 0.9161249399185181, 0.9455378651618958, 0.9292899370193481, 0.9489155411720276, 1.0249954462051392, 0.9084427952766418, 0.9143410325050354, 0.9319014549255371, 0.9482823610305786, 0.9377223253250122, 0.9471758604049683, 0.8953103423118591, 0.9490197896957397, 0.9310033917427063, 0.9452970623970032, 0.923987627029419, 0.9161640405654907, 0.9379461407661438, 0.9207593202590942, 0.9175025224685669, 0.9114662408828735, 0.9266840219497681]
[2025-05-21 11:47:49,232]: Mean: 0.92632645
[2025-05-21 11:47:49,232]: Min: 0.88512254
[2025-05-21 11:47:49,232]: Max: 1.02499545
[2025-05-21 11:47:49,233]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 11:47:49,235]: Sample Values (25 elements): [0.029333706945180893, -0.029333706945180893, -0.029333706945180893, 0.029333706945180893, 0.0, -0.029333706945180893, 0.0, 0.0, 0.058667413890361786, 0.0, 0.029333706945180893, 0.0, -0.029333706945180893, 0.0, 0.0, 0.0, 0.029333706945180893, 0.0, -0.029333706945180893, 0.029333706945180893, -0.029333706945180893, 0.0, 0.0, 0.0, 0.0]
[2025-05-21 11:47:49,235]: Mean: -0.00109452
[2025-05-21 11:47:49,235]: Min: -0.08800112
[2025-05-21 11:47:49,235]: Max: 0.11733483
[2025-05-21 11:47:49,235]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-21 11:47:49,235]: Sample Values (25 elements): [0.9103977680206299, 0.9072431325912476, 0.9263145923614502, 0.9299525022506714, 0.9223167896270752, 0.9259888529777527, 0.9431630969047546, 0.9187763333320618, 0.9453244209289551, 0.895674467086792, 0.937341034412384, 0.9138476848602295, 0.9261621832847595, 0.9359368681907654, 0.9455885291099548, 0.9044093489646912, 0.903174638748169, 0.9102374315261841, 0.9338324069976807, 0.9820104837417603, 0.9390577077865601, 0.9569947719573975, 0.9457191824913025, 0.9167042374610901, 0.9478505253791809]
[2025-05-21 11:47:49,236]: Mean: 0.93001676
[2025-05-21 11:47:49,236]: Min: 0.88879436
[2025-05-21 11:47:49,236]: Max: 0.98681837
[2025-05-21 11:47:49,237]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-21 11:47:49,237]: Sample Values (25 elements): [0.11103566735982895, -0.11103566735982895, -0.055517833679914474, 0.055517833679914474, -0.055517833679914474, -0.11103566735982895, -0.11103566735982895, -0.055517833679914474, 0.0, 0.11103566735982895, 0.11103566735982895, 0.055517833679914474, -0.11103566735982895, -0.16655349731445312, 0.11103566735982895, 0.055517833679914474, 0.0, -0.055517833679914474, 0.11103566735982895, -0.055517833679914474, -0.055517833679914474, -0.11103566735982895, 0.0, -0.055517833679914474, 0.055517833679914474]
[2025-05-21 11:47:49,237]: Mean: 0.00079970
[2025-05-21 11:47:49,238]: Min: -0.16655350
[2025-05-21 11:47:49,238]: Max: 0.22207133
[2025-05-21 11:47:49,238]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-21 11:47:49,238]: Sample Values (25 elements): [0.8510162234306335, 0.8764646053314209, 0.8976345062255859, 0.8698246479034424, 0.8883383870124817, 0.8917086720466614, 0.870118260383606, 0.8650036454200745, 0.8705400228500366, 0.872823178768158, 0.8979535698890686, 0.8738656640052795, 0.8714454770088196, 0.8902373909950256, 0.8756413459777832, 0.8760738968849182, 0.8619992733001709, 0.8883090615272522, 0.8723923563957214, 0.8797968626022339, 0.8865596055984497, 0.8726353645324707, 0.852340042591095, 0.8645466566085815, 0.8996767401695251]
[2025-05-21 11:47:49,238]: Mean: 0.87201291
[2025-05-21 11:47:49,238]: Min: 0.81400090
[2025-05-21 11:47:49,239]: Max: 0.91563958
[2025-05-21 11:47:49,239]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 11:47:49,241]: Sample Values (25 elements): [0.026429500430822372, -0.026429500430822372, 0.052859000861644745, 0.0, 0.0, -0.026429500430822372, 0.0, -0.026429500430822372, -0.052859000861644745, -0.026429500430822372, 0.0, 0.026429500430822372, 0.026429500430822372, -0.026429500430822372, 0.0, -0.026429500430822372, -0.026429500430822372, 0.026429500430822372, -0.026429500430822372, 0.0, 0.0, 0.026429500430822372, -0.026429500430822372, 0.0, 0.0]
[2025-05-21 11:47:49,241]: Mean: -0.00113618
[2025-05-21 11:47:49,241]: Min: -0.07928850
[2025-05-21 11:47:49,241]: Max: 0.10571800
[2025-05-21 11:47:49,241]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-21 11:47:49,242]: Sample Values (25 elements): [0.963050901889801, 0.8903034925460815, 0.9215757846832275, 0.8940532207489014, 0.8917491436004639, 0.9104323983192444, 0.8923346400260925, 0.9134508967399597, 0.9621837735176086, 0.9189854264259338, 0.9244450330734253, 0.9143733382225037, 0.8996451497077942, 0.9110788702964783, 0.9069945216178894, 0.911528468132019, 0.8547803163528442, 0.8816781640052795, 0.921380877494812, 0.9144587516784668, 0.9056100249290466, 0.9300307035446167, 0.9956315755844116, 0.906649112701416, 0.9154390692710876]
[2025-05-21 11:47:49,242]: Mean: 0.91987514
[2025-05-21 11:47:49,242]: Min: 0.85478032
[2025-05-21 11:47:49,242]: Max: 1.00179327
[2025-05-21 11:47:49,243]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 11:47:49,245]: Sample Values (25 elements): [0.0, 0.0, -0.026455802842974663, 0.0, -0.026455802842974663, 0.0, -0.026455802842974663, -0.026455802842974663, -0.026455802842974663, -0.026455802842974663, 0.0, 0.026455802842974663, -0.026455802842974663, -0.026455802842974663, 0.0, 0.026455802842974663, 0.0, -0.026455802842974663, -0.026455802842974663, 0.026455802842974663, 0.0, 0.026455802842974663, 0.052911605685949326, -0.026455802842974663, 0.0]
[2025-05-21 11:47:49,245]: Mean: -0.00033102
[2025-05-21 11:47:49,245]: Min: -0.10582321
[2025-05-21 11:47:49,245]: Max: 0.07936741
[2025-05-21 11:47:49,245]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-21 11:47:49,245]: Sample Values (25 elements): [0.9156566262245178, 0.9839135408401489, 0.9454075694084167, 0.9402663707733154, 0.9399619698524475, 0.921501636505127, 0.9574881196022034, 0.9594963192939758, 0.9558643102645874, 0.9010211229324341, 0.9701822996139526, 0.952468752861023, 0.9428035616874695, 0.946755051612854, 0.9079201221466064, 0.9341826438903809, 0.9553617835044861, 0.9946044683456421, 0.9528552293777466, 0.9292787909507751, 0.9388313293457031, 0.9394471049308777, 0.9163768291473389, 0.9504889249801636, 0.9401359558105469]
[2025-05-21 11:47:49,246]: Mean: 0.94222534
[2025-05-21 11:47:49,246]: Min: 0.89961690
[2025-05-21 11:47:49,246]: Max: 1.03342271
[2025-05-21 11:47:49,247]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-21 11:47:49,250]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.021237248554825783, 0.0, -0.021237248554825783, 0.0, 0.0, -0.021237248554825783, 0.042474497109651566, 0.0, 0.0, 0.0, -0.021237248554825783, 0.0, 0.0, 0.0, 0.021237248554825783, 0.021237248554825783, 0.0, 0.021237248554825783, -0.021237248554825783, 0.0, 0.021237248554825783, 0.021237248554825783]
[2025-05-21 11:47:49,250]: Mean: -0.00038771
[2025-05-21 11:47:49,250]: Min: -0.06371175
[2025-05-21 11:47:49,250]: Max: 0.08494899
[2025-05-21 11:47:49,250]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-21 11:47:49,251]: Sample Values (25 elements): [0.9146684408187866, 0.9240276217460632, 0.9085254669189453, 0.9199861288070679, 0.9197166562080383, 0.9260053634643555, 0.9097945690155029, 0.908896267414093, 0.9141558408737183, 0.9159297347068787, 0.9253305792808533, 0.9328389763832092, 0.9282447099685669, 0.9103256464004517, 0.9248051643371582, 0.9144904017448425, 0.8952813148498535, 0.912108838558197, 0.933786928653717, 0.9410973787307739, 0.929715633392334, 0.9149981141090393, 0.9128811359405518, 0.9361631274223328, 0.9126055836677551]
[2025-05-21 11:47:49,251]: Mean: 0.91840976
[2025-05-21 11:47:49,251]: Min: 0.86649877
[2025-05-21 11:47:49,251]: Max: 0.96128315
[2025-05-21 11:47:49,252]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 11:47:49,259]: Sample Values (25 elements): [-0.020120451226830482, 0.0, 0.020120451226830482, 0.020120451226830482, 0.040240902453660965, -0.020120451226830482, 0.0, 0.0, 0.0, 0.020120451226830482, 0.020120451226830482, 0.020120451226830482, 0.0, 0.020120451226830482, 0.020120451226830482, 0.020120451226830482, -0.040240902453660965, 0.0, -0.020120451226830482, -0.020120451226830482, 0.020120451226830482, 0.0, 0.0, 0.020120451226830482, -0.020120451226830482]
[2025-05-21 11:47:49,259]: Mean: -0.00078169
[2025-05-21 11:47:49,259]: Min: -0.06036136
[2025-05-21 11:47:49,260]: Max: 0.08048180
[2025-05-21 11:47:49,260]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-21 11:47:49,260]: Sample Values (25 elements): [0.9474019408226013, 0.9463202953338623, 0.9405823349952698, 0.9325758218765259, 0.915889322757721, 0.9477443099021912, 0.9368370175361633, 0.9234859943389893, 0.9262896180152893, 0.9270278811454773, 0.9387620091438293, 0.9161856770515442, 0.9319134950637817, 0.9493711590766907, 0.9354137182235718, 0.9263780117034912, 0.9211306571960449, 0.9172990322113037, 0.9220678210258484, 0.9102300405502319, 0.9172208309173584, 0.9195075631141663, 0.9549035429954529, 0.9425888061523438, 0.9374993443489075]
[2025-05-21 11:47:49,260]: Mean: 0.92674136
[2025-05-21 11:47:49,261]: Min: 0.89818043
[2025-05-21 11:47:49,261]: Max: 0.96269995
[2025-05-21 11:47:49,262]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-21 11:47:49,262]: Sample Values (25 elements): [-0.07571598887443542, 0.03785799443721771, 0.0, 0.07571598887443542, -0.03785799443721771, -0.03785799443721771, -0.03785799443721771, 0.03785799443721771, 0.0, -0.03785799443721771, 0.03785799443721771, 0.03785799443721771, -0.03785799443721771, -0.03785799443721771, 0.03785799443721771, -0.07571598887443542, -0.03785799443721771, 0.0, 0.03785799443721771, 0.07571598887443542, -0.07571598887443542, 0.07571598887443542, -0.03785799443721771, -0.07571598887443542, 0.0]
[2025-05-21 11:47:49,263]: Mean: -0.00017099
[2025-05-21 11:47:49,263]: Min: -0.11357398
[2025-05-21 11:47:49,263]: Max: 0.15143198
[2025-05-21 11:47:49,263]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-21 11:47:49,263]: Sample Values (25 elements): [0.8577198386192322, 0.8719576597213745, 0.8700241446495056, 0.882538914680481, 0.8749406933784485, 0.8725218772888184, 0.8835950493812561, 0.8639272451400757, 0.8672723770141602, 0.8527804613113403, 0.8631914258003235, 0.8637109398841858, 0.8903747797012329, 0.8795052766799927, 0.8602832555770874, 0.8787064552307129, 0.8930253386497498, 0.878856360912323, 0.8709086775779724, 0.8854485750198364, 0.8855503797531128, 0.8926213979721069, 0.8625676035881042, 0.8742431402206421, 0.8711483478546143]
[2025-05-21 11:47:49,263]: Mean: 0.87763143
[2025-05-21 11:47:49,264]: Min: 0.84705985
[2025-05-21 11:47:49,264]: Max: 0.90526557
[2025-05-21 11:47:49,265]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 11:47:49,271]: Sample Values (25 elements): [-0.019112924113869667, 0.0, -0.019112924113869667, 0.019112924113869667, -0.019112924113869667, 0.0, -0.019112924113869667, 0.0, 0.0, 0.019112924113869667, -0.019112924113869667, 0.0, 0.019112924113869667, -0.019112924113869667, 0.019112924113869667, -0.019112924113869667, 0.038225848227739334, 0.0, -0.019112924113869667, 0.019112924113869667, 0.038225848227739334, 0.0, 0.0, -0.019112924113869667, 0.0]
[2025-05-21 11:47:49,271]: Mean: -0.00080321
[2025-05-21 11:47:49,271]: Min: -0.05733877
[2025-05-21 11:47:49,272]: Max: 0.07645170
[2025-05-21 11:47:49,272]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-21 11:47:49,272]: Sample Values (25 elements): [0.8992239832878113, 0.8875671625137329, 0.9088607430458069, 0.9295692443847656, 0.9230184555053711, 0.9261161088943481, 0.9252468943595886, 0.9404025673866272, 0.9061974287033081, 0.9184625744819641, 0.9219189286231995, 0.9138838052749634, 0.9179957509040833, 0.9068861603736877, 0.9014714360237122, 0.9091125726699829, 0.9023154973983765, 0.9057716727256775, 0.9130537509918213, 0.9163243174552917, 0.9161351919174194, 0.9015980958938599, 0.9221284985542297, 0.926771342754364, 0.8973281383514404]
[2025-05-21 11:47:49,272]: Mean: 0.91525805
[2025-05-21 11:47:49,272]: Min: 0.87514406
[2025-05-21 11:47:49,272]: Max: 0.95226055
[2025-05-21 11:47:49,274]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 11:47:49,282]: Sample Values (25 elements): [0.0, -0.01641407422721386, 0.01641407422721386, 0.0, 0.01641407422721386, -0.01641407422721386, 0.0, 0.0, 0.01641407422721386, -0.01641407422721386, 0.0, 0.0, -0.01641407422721386, 0.01641407422721386, -0.01641407422721386, 0.0, -0.01641407422721386, 0.0, 0.0, -0.01641407422721386, -0.01641407422721386, 0.01641407422721386, 0.01641407422721386, 0.0, -0.01641407422721386]
[2025-05-21 11:47:49,282]: Mean: -0.00016681
[2025-05-21 11:47:49,282]: Min: -0.04924222
[2025-05-21 11:47:49,282]: Max: 0.06565630
[2025-05-21 11:47:49,282]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-21 11:47:49,282]: Sample Values (25 elements): [0.9424475431442261, 0.9042977094650269, 0.9445492625236511, 0.9253405332565308, 0.9320552945137024, 0.9397553205490112, 0.9136155843734741, 0.9222189784049988, 0.9627196788787842, 0.9464195370674133, 0.9391174912452698, 0.9382209181785583, 0.9234533309936523, 0.9385337233543396, 0.9183467030525208, 0.9185360670089722, 0.9304128885269165, 0.9169468283653259, 0.9123423099517822, 0.93446284532547, 0.9193836450576782, 0.9306633472442627, 0.9299972653388977, 0.9163879156112671, 0.94597989320755]
[2025-05-21 11:47:49,283]: Mean: 0.93156505
[2025-05-21 11:47:49,283]: Min: 0.90429771
[2025-05-21 11:47:49,283]: Max: 0.97350776
[2025-05-21 11:47:49,284]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-21 11:47:49,300]: Sample Values (25 elements): [-0.01424294151365757, 0.0, 0.01424294151365757, -0.01424294151365757, 0.01424294151365757, -0.01424294151365757, 0.0, -0.01424294151365757, 0.0, 0.0, 0.01424294151365757, 0.0, -0.02848588302731514, 0.0, 0.0, -0.01424294151365757, -0.01424294151365757, 0.01424294151365757, -0.01424294151365757, -0.01424294151365757, 0.0, -0.01424294151365757, -0.02848588302731514, 0.01424294151365757, -0.01424294151365757]
[2025-05-21 11:47:49,300]: Mean: -0.00003928
[2025-05-21 11:47:49,300]: Min: -0.04272883
[2025-05-21 11:47:49,300]: Max: 0.05697177
[2025-05-21 11:47:49,300]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-21 11:47:49,301]: Sample Values (25 elements): [0.911666750907898, 0.9147310256958008, 0.9131852984428406, 0.9102291464805603, 0.9105352163314819, 0.9182243943214417, 0.9072837829589844, 0.9132081866264343, 0.9110498428344727, 0.9108532071113586, 0.9077770113945007, 0.9069245457649231, 0.9074150323867798, 0.9071596264839172, 0.9061383008956909, 0.9120391607284546, 0.8940278887748718, 0.9063605666160583, 0.91530841588974, 0.9184292554855347, 0.9077801704406738, 0.9077543020248413, 0.9099569320678711, 0.9179967045783997, 0.9265668988227844]
[2025-05-21 11:47:49,301]: Mean: 0.91283691
[2025-05-21 11:47:49,301]: Min: 0.89402789
[2025-05-21 11:47:49,301]: Max: 0.93236297
[2025-05-21 11:47:49,302]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 11:47:49,351]: Sample Values (25 elements): [0.0, 0.0, -0.01246502622961998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01246502622961998, 0.01246502622961998, 0.01246502622961998, 0.0, 0.0, 0.0, -0.01246502622961998, 0.0, 0.0, -0.01246502622961998, -0.01246502622961998, 0.01246502622961998, 0.0, -0.01246502622961998, 0.01246502622961998]
[2025-05-21 11:47:49,351]: Mean: -0.00011676
[2025-05-21 11:47:49,352]: Min: -0.03739508
[2025-05-21 11:47:49,352]: Max: 0.04986010
[2025-05-21 11:47:49,352]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-21 11:47:49,354]: Sample Values (25 elements): [0.9160383343696594, 0.9187090396881104, 0.9205561876296997, 0.9052410125732422, 0.9104499220848083, 0.9126753211021423, 0.9052917957305908, 0.9128332734107971, 0.9028545618057251, 0.8954831957817078, 0.9132553935050964, 0.9044837951660156, 0.9133607745170593, 0.9084745049476624, 0.9069140553474426, 0.9069890379905701, 0.9102054238319397, 0.9143675565719604, 0.9057588577270508, 0.9118284583091736, 0.9124953746795654, 0.9049868583679199, 0.9059672951698303, 0.9138780236244202, 0.9115409255027771]
[2025-05-21 11:47:49,354]: Mean: 0.91011369
[2025-05-21 11:47:49,354]: Min: 0.89548320
[2025-05-21 11:47:49,354]: Max: 0.93229628
[2025-05-21 11:47:49,356]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-21 11:47:49,357]: Sample Values (25 elements): [0.0, 0.025170547887682915, 0.0, 0.0, -0.025170547887682915, -0.05034109577536583, -0.05034109577536583, 0.0, 0.0, 0.0, 0.05034109577536583, 0.025170547887682915, 0.0, 0.0, -0.025170547887682915, 0.025170547887682915, -0.025170547887682915, -0.025170547887682915, -0.025170547887682915, 0.05034109577536583, 0.025170547887682915, 0.05034109577536583, 0.0, -0.025170547887682915, 0.05034109577536583]
[2025-05-21 11:47:49,357]: Mean: 0.00007720
[2025-05-21 11:47:49,357]: Min: -0.07551164
[2025-05-21 11:47:49,357]: Max: 0.10068219
[2025-05-21 11:47:49,358]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-21 11:47:49,358]: Sample Values (25 elements): [0.9000354409217834, 0.8916875720024109, 0.9038116335868835, 0.8981804847717285, 0.9003669619560242, 0.8996928930282593, 0.8990151286125183, 0.9008640050888062, 0.9008418321609497, 0.9021589756011963, 0.9046570062637329, 0.9016998410224915, 0.904212474822998, 0.9007055163383484, 0.9037052989006042, 0.8938449025154114, 0.9004610776901245, 0.8969454169273376, 0.9057043790817261, 0.9030036926269531, 0.8980873823165894, 0.8971441388130188, 0.8979892134666443, 0.8932549953460693, 0.878696084022522]
[2025-05-21 11:47:49,358]: Mean: 0.89960533
[2025-05-21 11:47:49,358]: Min: 0.87869608
[2025-05-21 11:47:49,358]: Max: 0.91349095
[2025-05-21 11:47:49,359]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 11:47:49,397]: Sample Values (25 elements): [0.011354174464941025, -0.011354174464941025, -0.011354174464941025, 0.0, 0.0, 0.0, 0.011354174464941025, 0.0, 0.011354174464941025, -0.011354174464941025, -0.011354174464941025, 0.0, 0.0, 0.011354174464941025, -0.011354174464941025, -0.02270834892988205, -0.011354174464941025, 0.011354174464941025, 0.0, -0.02270834892988205, 0.0, 0.0, -0.02270834892988205, 0.0, 0.0]
[2025-05-21 11:47:49,397]: Mean: -0.00010743
[2025-05-21 11:47:49,397]: Min: -0.03406252
[2025-05-21 11:47:49,397]: Max: 0.04541670
[2025-05-21 11:47:49,397]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-21 11:47:49,398]: Sample Values (25 elements): [0.9099920392036438, 0.9061694145202637, 0.9144093990325928, 0.9168781638145447, 0.9081827402114868, 0.9139633178710938, 0.9108017086982727, 0.9118742346763611, 0.9083225727081299, 0.9264470934867859, 0.9136680364608765, 0.9089171886444092, 0.9116305708885193, 0.91618412733078, 0.9058244824409485, 0.9123400449752808, 0.9143385291099548, 0.9100936651229858, 0.9104384183883667, 0.912812352180481, 0.9115518927574158, 0.9073248505592346, 0.9071211814880371, 0.9171817898750305, 0.908047616481781]
[2025-05-21 11:47:49,398]: Mean: 0.91087544
[2025-05-21 11:47:49,398]: Min: 0.90166092
[2025-05-21 11:47:49,398]: Max: 0.94176388
[2025-05-21 11:47:49,399]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 11:47:49,434]: Sample Values (25 elements): [0.007680957205593586, -0.007680957205593586, 0.007680957205593586, -0.015361914411187172, 0.0, 0.015361914411187172, 0.007680957205593586, -0.007680957205593586, -0.007680957205593586, -0.007680957205593586, 0.0, 0.0, 0.015361914411187172, 0.0, 0.0, 0.007680957205593586, 0.007680957205593586, 0.0, 0.0, 0.0, 0.007680957205593586, -0.007680957205593586, -0.007680957205593586, 0.015361914411187172, 0.015361914411187172]
[2025-05-21 11:47:49,434]: Mean: 0.00038302
[2025-05-21 11:47:49,435]: Min: -0.02304287
[2025-05-21 11:47:49,435]: Max: 0.03072383
[2025-05-21 11:47:49,435]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-21 11:47:49,435]: Sample Values (25 elements): [0.9144116044044495, 0.9097623229026794, 0.910911500453949, 0.9048416018486023, 0.9142929911613464, 0.9018548727035522, 0.912310004234314, 0.9139206409454346, 0.9125393629074097, 0.9133322238922119, 0.9099081158638, 0.9144836068153381, 0.9142900109291077, 0.907231330871582, 0.9121906161308289, 0.9051209688186646, 0.9131644368171692, 0.9087713360786438, 0.9095951318740845, 0.9094926118850708, 0.9146138429641724, 0.9140662550926208, 0.9076703786849976, 0.9097434282302856, 0.910544753074646]
[2025-05-21 11:47:49,435]: Mean: 0.91120625
[2025-05-21 11:47:49,436]: Min: 0.89778322
[2025-05-21 11:47:49,436]: Max: 0.92462575
[2025-05-21 11:47:49,436]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-21 11:47:49,436]: Sample Values (25 elements): [-0.05650463327765465, 0.012138261459767818, -0.06829236447811127, 0.09806820750236511, 0.08659744262695312, 0.0955299362540245, 0.07010428607463837, -0.08646631985902786, -0.06956279277801514, -0.06297473609447479, 0.004137865267693996, 0.0375199131667614, -0.07845502346754074, -0.08613929897546768, 0.07143327593803406, 0.11256758868694305, 0.09060569852590561, 0.024824324995279312, 0.011684500612318516, -0.0607588104903698, -0.06843641400337219, -0.05737020820379257, 0.08625325560569763, -0.01305739488452673, -0.06344563513994217]
[2025-05-21 11:47:49,436]: Mean: -0.00024706
[2025-05-21 11:47:49,436]: Min: -0.17844841
[2025-05-21 11:47:49,437]: Max: 0.18903847
[2025-05-21 11:47:49,437]: 


QAT of ResNet18 with parametrized_relu down to 4 bits...
[2025-05-21 11:47:49,633]: [ResNet18_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-05-21 11:47:49,657]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 11:49:31,778]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.8148 Train Acc: 0.7279 Eval Loss: 0.6676 Eval Acc: 0.7828 (LR: 0.010000)
[2025-05-21 11:51:13,862]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.4959 Train Acc: 0.8278 Eval Loss: 0.4952 Eval Acc: 0.8410 (LR: 0.010000)
[2025-05-21 11:52:55,999]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.4096 Train Acc: 0.8579 Eval Loss: 0.4461 Eval Acc: 0.8538 (LR: 0.010000)
[2025-05-21 11:54:38,170]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.3633 Train Acc: 0.8729 Eval Loss: 0.4490 Eval Acc: 0.8554 (LR: 0.010000)
[2025-05-21 11:56:20,109]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.3349 Train Acc: 0.8826 Eval Loss: 0.4414 Eval Acc: 0.8627 (LR: 0.010000)
[2025-05-21 11:58:02,119]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.3090 Train Acc: 0.8922 Eval Loss: 0.4231 Eval Acc: 0.8687 (LR: 0.010000)
[2025-05-21 11:59:44,296]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.2857 Train Acc: 0.9005 Eval Loss: 0.3949 Eval Acc: 0.8779 (LR: 0.010000)
[2025-05-21 12:01:26,702]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.2659 Train Acc: 0.9079 Eval Loss: 0.3737 Eval Acc: 0.8831 (LR: 0.010000)
[2025-05-21 12:03:08,790]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.2517 Train Acc: 0.9113 Eval Loss: 0.4541 Eval Acc: 0.8679 (LR: 0.010000)
[2025-05-21 12:04:50,384]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.2364 Train Acc: 0.9185 Eval Loss: 0.3766 Eval Acc: 0.8844 (LR: 0.010000)
[2025-05-21 12:06:31,751]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.2268 Train Acc: 0.9206 Eval Loss: 0.3730 Eval Acc: 0.8903 (LR: 0.010000)
[2025-05-21 12:08:13,334]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.2148 Train Acc: 0.9239 Eval Loss: 0.4292 Eval Acc: 0.8756 (LR: 0.010000)
[2025-05-21 12:09:54,748]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.2063 Train Acc: 0.9262 Eval Loss: 0.3351 Eval Acc: 0.8978 (LR: 0.010000)
[2025-05-21 12:11:36,341]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.1989 Train Acc: 0.9298 Eval Loss: 0.3368 Eval Acc: 0.8981 (LR: 0.010000)
[2025-05-21 12:13:18,047]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.1888 Train Acc: 0.9327 Eval Loss: 0.4274 Eval Acc: 0.8814 (LR: 0.001000)
[2025-05-21 12:14:59,410]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.1107 Train Acc: 0.9613 Eval Loss: 0.2547 Eval Acc: 0.9229 (LR: 0.001000)
[2025-05-21 12:16:41,007]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.0875 Train Acc: 0.9701 Eval Loss: 0.2528 Eval Acc: 0.9235 (LR: 0.001000)
[2025-05-21 12:18:22,808]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.0780 Train Acc: 0.9736 Eval Loss: 0.2562 Eval Acc: 0.9277 (LR: 0.001000)
[2025-05-21 12:20:04,643]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.0766 Train Acc: 0.9735 Eval Loss: 0.2583 Eval Acc: 0.9265 (LR: 0.001000)
[2025-05-21 12:21:47,462]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.0708 Train Acc: 0.9762 Eval Loss: 0.2548 Eval Acc: 0.9264 (LR: 0.001000)
[2025-05-21 12:23:29,118]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.0676 Train Acc: 0.9768 Eval Loss: 0.2627 Eval Acc: 0.9271 (LR: 0.001000)
[2025-05-21 12:25:10,882]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.0650 Train Acc: 0.9778 Eval Loss: 0.2603 Eval Acc: 0.9266 (LR: 0.001000)
[2025-05-21 12:26:52,600]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.0618 Train Acc: 0.9790 Eval Loss: 0.2625 Eval Acc: 0.9249 (LR: 0.001000)
[2025-05-21 12:28:33,992]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.0604 Train Acc: 0.9791 Eval Loss: 0.2691 Eval Acc: 0.9257 (LR: 0.001000)
[2025-05-21 12:30:15,361]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.0573 Train Acc: 0.9800 Eval Loss: 0.2688 Eval Acc: 0.9264 (LR: 0.001000)
[2025-05-21 12:31:56,527]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.0553 Train Acc: 0.9814 Eval Loss: 0.2729 Eval Acc: 0.9250 (LR: 0.001000)
[2025-05-21 12:33:38,110]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.0545 Train Acc: 0.9810 Eval Loss: 0.2828 Eval Acc: 0.9240 (LR: 0.001000)
[2025-05-21 12:35:19,672]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.0509 Train Acc: 0.9827 Eval Loss: 0.2787 Eval Acc: 0.9249 (LR: 0.001000)
[2025-05-21 12:37:00,825]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.0521 Train Acc: 0.9822 Eval Loss: 0.2823 Eval Acc: 0.9227 (LR: 0.001000)
[2025-05-21 12:38:42,211]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.0492 Train Acc: 0.9835 Eval Loss: 0.2872 Eval Acc: 0.9242 (LR: 0.000100)
[2025-05-21 12:40:23,578]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.0445 Train Acc: 0.9844 Eval Loss: 0.2751 Eval Acc: 0.9267 (LR: 0.000100)
[2025-05-21 12:42:04,758]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.0444 Train Acc: 0.9855 Eval Loss: 0.2780 Eval Acc: 0.9263 (LR: 0.000100)
[2025-05-21 12:43:45,953]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.0428 Train Acc: 0.9853 Eval Loss: 0.2776 Eval Acc: 0.9246 (LR: 0.000100)
[2025-05-21 12:45:27,523]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.0423 Train Acc: 0.9853 Eval Loss: 0.2773 Eval Acc: 0.9266 (LR: 0.000100)
[2025-05-21 12:47:08,870]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.0423 Train Acc: 0.9861 Eval Loss: 0.2788 Eval Acc: 0.9260 (LR: 0.000100)
[2025-05-21 12:49:03,654]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.0436 Train Acc: 0.9855 Eval Loss: 0.2752 Eval Acc: 0.9270 (LR: 0.000100)
[2025-05-21 12:50:45,085]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.0408 Train Acc: 0.9863 Eval Loss: 0.2750 Eval Acc: 0.9281 (LR: 0.000100)
[2025-05-21 12:52:26,338]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.0414 Train Acc: 0.9867 Eval Loss: 0.2800 Eval Acc: 0.9283 (LR: 0.000100)
[2025-05-21 12:54:08,027]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.0413 Train Acc: 0.9860 Eval Loss: 0.2751 Eval Acc: 0.9275 (LR: 0.000100)
[2025-05-21 12:55:50,560]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.0410 Train Acc: 0.9864 Eval Loss: 0.2797 Eval Acc: 0.9270 (LR: 0.000100)
[2025-05-21 12:57:32,088]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.0393 Train Acc: 0.9871 Eval Loss: 0.2787 Eval Acc: 0.9261 (LR: 0.000100)
[2025-05-21 12:59:13,548]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.0399 Train Acc: 0.9870 Eval Loss: 0.2754 Eval Acc: 0.9263 (LR: 0.000100)
[2025-05-21 13:00:54,966]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.0401 Train Acc: 0.9867 Eval Loss: 0.2791 Eval Acc: 0.9276 (LR: 0.000100)
[2025-05-21 13:02:36,236]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.0396 Train Acc: 0.9865 Eval Loss: 0.2754 Eval Acc: 0.9267 (LR: 0.000100)
[2025-05-21 13:04:17,683]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.0393 Train Acc: 0.9872 Eval Loss: 0.2815 Eval Acc: 0.9249 (LR: 0.000010)
[2025-05-21 13:05:59,096]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.0384 Train Acc: 0.9878 Eval Loss: 0.2797 Eval Acc: 0.9256 (LR: 0.000010)
[2025-05-21 13:07:40,331]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.0377 Train Acc: 0.9885 Eval Loss: 0.2808 Eval Acc: 0.9267 (LR: 0.000010)
[2025-05-21 13:09:21,900]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.0387 Train Acc: 0.9874 Eval Loss: 0.2788 Eval Acc: 0.9264 (LR: 0.000010)
[2025-05-21 13:11:03,448]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.0388 Train Acc: 0.9871 Eval Loss: 0.2804 Eval Acc: 0.9275 (LR: 0.000010)
[2025-05-21 13:12:44,903]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.0385 Train Acc: 0.9876 Eval Loss: 0.2815 Eval Acc: 0.9270 (LR: 0.000010)
[2025-05-21 13:14:26,262]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.0380 Train Acc: 0.9875 Eval Loss: 0.2736 Eval Acc: 0.9286 (LR: 0.000010)
[2025-05-21 13:16:07,507]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.0392 Train Acc: 0.9875 Eval Loss: 0.2765 Eval Acc: 0.9271 (LR: 0.000010)
[2025-05-21 13:17:48,778]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.0394 Train Acc: 0.9870 Eval Loss: 0.2788 Eval Acc: 0.9273 (LR: 0.000010)
[2025-05-21 13:19:30,218]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.0380 Train Acc: 0.9880 Eval Loss: 0.2790 Eval Acc: 0.9273 (LR: 0.000010)
[2025-05-21 13:21:11,641]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.0395 Train Acc: 0.9866 Eval Loss: 0.2792 Eval Acc: 0.9266 (LR: 0.000010)
[2025-05-21 13:22:53,058]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.0385 Train Acc: 0.9878 Eval Loss: 0.2821 Eval Acc: 0.9282 (LR: 0.000010)
[2025-05-21 13:24:34,514]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.0392 Train Acc: 0.9875 Eval Loss: 0.2746 Eval Acc: 0.9267 (LR: 0.000010)
[2025-05-21 13:26:16,137]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.0376 Train Acc: 0.9875 Eval Loss: 0.2746 Eval Acc: 0.9268 (LR: 0.000010)
[2025-05-21 13:27:57,416]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.0384 Train Acc: 0.9869 Eval Loss: 0.2770 Eval Acc: 0.9274 (LR: 0.000010)
[2025-05-21 13:29:39,836]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.0392 Train Acc: 0.9868 Eval Loss: 0.2799 Eval Acc: 0.9263 (LR: 0.000010)
[2025-05-21 13:29:39,836]: [ResNet18_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.9286
[2025-05-21 13:29:39,925]: 


Quantization of model down to 4 bits finished
[2025-05-21 13:29:39,925]: Model Architecture:
[2025-05-21 13:29:39,970]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4000], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.0)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ReLU6(inplace=True)
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0337], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780543565750122, max_val=0.22773385047912598)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3728], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0270], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17485252022743225, max_val=0.23083055019378662)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3672], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0220], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15529820322990417, max_val=0.17489302158355713)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3748], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0160], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11671315878629684, max_val=0.12275800108909607)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3703], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0142], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10496937483549118, max_val=0.10840211808681488)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3749], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0132], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09484361112117767, max_val=0.1029515266418457)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0253], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1842447966337204, max_val=0.19587284326553345)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3760], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09008406847715378, max_val=0.11755891144275665)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3749], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0123], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08763958513736725, max_val=0.09736831486225128)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3838], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0105], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07728535681962967, max_val=0.0803452655673027)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3749], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0095], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06531170010566711, max_val=0.07761809229850769)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0183], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1314220428466797, max_val=0.14343488216400146)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3773], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0100], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07140800356864929, max_val=0.07792145013809204)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3748], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0082], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05654261261224747, max_val=0.06678974628448486)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3836], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04805989935994148, max_val=0.05103953555226326)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3749], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03418618068099022, max_val=0.03947249427437782)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0126], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.09507568925619125, max_val=0.09411199390888214)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3769], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03991042077541351, max_val=0.03898737207055092)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3753], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0036], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.025692736729979515, max_val=0.027576608583331108)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU(
            (quant): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3718], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): FixedQParamsObserver()
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-21 13:29:39,970]: 
Model Weights:
[2025-05-21 13:29:39,970]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-21 13:29:39,971]: Sample Values (25 elements): [-0.22763875126838684, 0.09414517879486084, 0.04244931787252426, -0.034227002412080765, -0.049249496310949326, -0.08695102483034134, -0.09200992435216904, 0.2875960171222687, -0.039062004536390305, -0.23160618543624878, 0.24012453854084015, -0.0033960165455937386, -0.05540953949093819, 0.07010362297296524, 0.0781634971499443, -0.14268134534358978, 0.03525092452764511, -0.17011797428131104, -0.006882882211357355, -0.222499281167984, 0.23737135529518127, -0.14466984570026398, 0.019605863839387894, 0.11003167927265167, -0.20269091427326202]
[2025-05-21 13:29:39,971]: Mean: -0.00083645
[2025-05-21 13:29:39,971]: Min: -0.54348689
[2025-05-21 13:29:39,971]: Max: 0.56449580
[2025-05-21 13:29:39,971]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-21 13:29:39,971]: Sample Values (25 elements): [0.9674368500709534, 0.8561035394668579, 1.175438642501831, 0.776294469833374, 0.8612893223762512, 0.8688899278640747, 0.8397567272186279, 0.9037700891494751, 1.0959542989730835, 0.7501369714736938, 0.968624472618103, 0.7533897757530212, 0.8718996047973633, 0.8826198577880859, 0.9444541335105896, 0.9380996823310852, 0.8444308042526245, 0.887247622013092, 0.9215601086616516, 0.8627291917800903, 0.8885374069213867, 0.767607569694519, 0.7964156866073608, 0.6737474799156189, 0.703390896320343]
[2025-05-21 13:29:39,971]: Mean: 0.87924188
[2025-05-21 13:29:39,972]: Min: 0.67374748
[2025-05-21 13:29:39,972]: Max: 1.17543864
[2025-05-21 13:29:39,973]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 13:29:39,973]: Sample Values (25 elements): [0.0, -0.06743843108415604, 0.03371921554207802, 0.06743843108415604, 0.0, 0.0, -0.03371921554207802, 0.03371921554207802, -0.06743843108415604, -0.03371921554207802, 0.0, -0.06743843108415604, -0.03371921554207802, -0.03371921554207802, -0.03371921554207802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03371921554207802, -0.03371921554207802, 0.0, 0.0]
[2025-05-21 13:29:39,973]: Mean: -0.00311636
[2025-05-21 13:29:39,974]: Min: -0.26975372
[2025-05-21 13:29:39,974]: Max: 0.23603451
[2025-05-21 13:29:39,974]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-21 13:29:39,974]: Sample Values (25 elements): [0.9213797450065613, 0.8775344491004944, 0.9569025635719299, 0.8194476366043091, 0.9137681722640991, 0.9691661596298218, 0.9356713891029358, 0.9172059893608093, 0.8647715449333191, 0.9353378415107727, 0.9348738193511963, 0.9021087884902954, 0.8992855548858643, 0.9650481343269348, 0.9473089575767517, 0.8632461428642273, 0.8926068544387817, 1.2433414459228516, 0.9712291359901428, 0.8108702898025513, 0.9147998690605164, 1.1306869983673096, 0.8292745351791382, 0.9355196356773376, 0.8136394023895264]
[2025-05-21 13:29:39,974]: Mean: 0.92425525
[2025-05-21 13:29:39,974]: Min: 0.79725164
[2025-05-21 13:29:39,974]: Max: 1.29146039
[2025-05-21 13:29:39,975]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 13:29:39,976]: Sample Values (25 elements): [0.027045536786317825, 0.027045536786317825, 0.0, 0.0, 0.027045536786317825, 0.0, 0.027045536786317825, -0.05409107357263565, 0.027045536786317825, 0.027045536786317825, -0.027045536786317825, 0.027045536786317825, 0.027045536786317825, 0.027045536786317825, 0.0, 0.05409107357263565, 0.0, 0.027045536786317825, 0.0, 0.027045536786317825, -0.05409107357263565, 0.0, 0.0, -0.05409107357263565, -0.027045536786317825]
[2025-05-21 13:29:39,976]: Mean: -0.00147245
[2025-05-21 13:29:39,976]: Min: -0.16227323
[2025-05-21 13:29:39,976]: Max: 0.24340983
[2025-05-21 13:29:39,977]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-21 13:29:39,977]: Sample Values (25 elements): [1.0106990337371826, 1.0000460147857666, 0.906585156917572, 0.8707987070083618, 0.9204919934272766, 0.9911380410194397, 0.8541838526725769, 0.9582455158233643, 0.9130963683128357, 0.9563595652580261, 0.901394248008728, 0.9808138012886047, 0.9325976967811584, 0.9785311818122864, 0.8968096375465393, 0.8971938490867615, 0.944843053817749, 0.9323313236236572, 0.9524742960929871, 0.9779222011566162, 0.9028725028038025, 0.950370192527771, 1.0931204557418823, 0.9649848341941833, 1.319529414176941]
[2025-05-21 13:29:39,977]: Mean: 0.96587545
[2025-05-21 13:29:39,977]: Min: 0.85418385
[2025-05-21 13:29:39,977]: Max: 1.31952941
[2025-05-21 13:29:39,978]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 13:29:39,979]: Sample Values (25 elements): [-0.022012747824192047, 0.06603824347257614, -0.044025495648384094, -0.044025495648384094, 0.0, 0.0, 0.022012747824192047, 0.0, -0.022012747824192047, -0.022012747824192047, -0.022012747824192047, -0.022012747824192047, 0.0, 0.0, 0.022012747824192047, -0.044025495648384094, -0.06603824347257614, 0.022012747824192047, -0.06603824347257614, 0.0, 0.022012747824192047, 0.0, 0.0, -0.022012747824192047, 0.0]
[2025-05-21 13:29:39,979]: Mean: -0.00242317
[2025-05-21 13:29:39,979]: Min: -0.15408924
[2025-05-21 13:29:39,979]: Max: 0.17610198
[2025-05-21 13:29:39,979]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-21 13:29:39,979]: Sample Values (25 elements): [0.9308218359947205, 0.9192585349082947, 0.8804548978805542, 0.8578741550445557, 0.9126226305961609, 0.9380513429641724, 0.9024996757507324, 0.910404622554779, 0.9657032489776611, 0.9192052483558655, 0.9416497349739075, 0.9466041326522827, 0.9383102059364319, 0.9659022092819214, 0.9273630380630493, 0.8925204277038574, 0.8971468210220337, 0.8994920253753662, 0.9990124106407166, 0.9718987345695496, 0.8898150324821472, 0.9019480347633362, 0.9082114100456238, 0.8728436827659607, 0.8834772706031799]
[2025-05-21 13:29:39,980]: Mean: 0.91437167
[2025-05-21 13:29:39,980]: Min: 0.82497960
[2025-05-21 13:29:39,980]: Max: 0.99901241
[2025-05-21 13:29:39,981]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-21 13:29:39,981]: Sample Values (25 elements): [-0.031929485499858856, 0.015964742749929428, -0.015964742749929428, 0.0, 0.0, -0.06385897099971771, 0.031929485499858856, 0.0, -0.015964742749929428, -0.015964742749929428, 0.0, -0.031929485499858856, -0.031929485499858856, 0.0, -0.015964742749929428, 0.031929485499858856, 0.015964742749929428, 0.015964742749929428, 0.015964742749929428, -0.015964742749929428, 0.0, -0.031929485499858856, -0.015964742749929428, 0.031929485499858856, -0.015964742749929428]
[2025-05-21 13:29:39,982]: Mean: -0.00041185
[2025-05-21 13:29:39,982]: Min: -0.11175320
[2025-05-21 13:29:39,982]: Max: 0.12771794
[2025-05-21 13:29:39,982]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-21 13:29:39,982]: Sample Values (25 elements): [0.9677634835243225, 0.8991288542747498, 0.961180567741394, 0.9028773307800293, 0.8859080672264099, 0.9224691390991211, 0.9932049512863159, 0.9141337275505066, 0.9239001274108887, 0.9569078683853149, 0.8981044292449951, 0.978131890296936, 0.958797037601471, 0.9027849435806274, 0.9219642877578735, 0.9468633532524109, 0.8983893990516663, 0.9056270718574524, 0.9235410094261169, 0.8986556529998779, 0.9053406119346619, 0.9703455567359924, 0.8879585266113281, 0.9746748805046082, 0.9060187935829163]
[2025-05-21 13:29:39,982]: Mean: 0.93689823
[2025-05-21 13:29:39,982]: Min: 0.88590807
[2025-05-21 13:29:39,983]: Max: 1.03127348
[2025-05-21 13:29:39,984]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-21 13:29:39,984]: Sample Values (25 elements): [-0.014224766753613949, 0.056899067014455795, 0.014224766753613949, 0.028449533507227898, -0.04267429932951927, -0.04267429932951927, -0.028449533507227898, 0.014224766753613949, -0.028449533507227898, 0.0, -0.014224766753613949, -0.014224766753613949, 0.028449533507227898, 0.014224766753613949, 0.014224766753613949, -0.056899067014455795, 0.014224766753613949, 0.014224766753613949, -0.028449533507227898, -0.028449533507227898, 0.028449533507227898, -0.04267429932951927, 0.0, 0.0, -0.014224766753613949]
[2025-05-21 13:29:39,985]: Mean: -0.00109935
[2025-05-21 13:29:39,985]: Min: -0.09957337
[2025-05-21 13:29:39,985]: Max: 0.11379813
[2025-05-21 13:29:39,985]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-21 13:29:39,985]: Sample Values (25 elements): [0.893958330154419, 0.9178135991096497, 0.9310488104820251, 0.9304925203323364, 0.9027941226959229, 0.9178317189216614, 0.9027314782142639, 0.9170438051223755, 0.8989855647087097, 0.9216007590293884, 0.8993211984634399, 0.8937622308731079, 0.9280742406845093, 0.9352840185165405, 0.8957063555717468, 0.9015509486198425, 0.9353160858154297, 1.011051893234253, 0.8863441348075867, 0.9199643731117249, 0.9015638828277588, 0.9133970737457275, 0.9306586384773254, 0.9192756414413452, 0.9047279357910156]
[2025-05-21 13:29:39,985]: Mean: 0.91345567
[2025-05-21 13:29:39,985]: Min: 0.86091197
[2025-05-21 13:29:39,986]: Max: 1.01105189
[2025-05-21 13:29:39,986]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 13:29:39,988]: Sample Values (25 elements): [0.0, 0.02637268602848053, -0.039559029042720795, 0.039559029042720795, -0.039559029042720795, 0.02637268602848053, -0.02637268602848053, -0.039559029042720795, 0.0, -0.02637268602848053, -0.039559029042720795, 0.02637268602848053, -0.013186343014240265, 0.013186343014240265, 0.0, 0.013186343014240265, 0.02637268602848053, 0.02637268602848053, 0.0, 0.013186343014240265, 0.013186343014240265, -0.013186343014240265, -0.02637268602848053, -0.013186343014240265, -0.013186343014240265]
[2025-05-21 13:29:39,988]: Mean: -0.00118784
[2025-05-21 13:29:39,988]: Min: -0.09230440
[2025-05-21 13:29:39,988]: Max: 0.10549074
[2025-05-21 13:29:39,988]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-21 13:29:39,989]: Sample Values (25 elements): [0.907875657081604, 0.937876284122467, 0.9381583333015442, 0.9026888012886047, 0.9419324994087219, 0.9034819602966309, 0.9164698123931885, 0.894101083278656, 0.9401882290840149, 0.9171409606933594, 0.9314880967140198, 0.9212740659713745, 0.9016198515892029, 0.9545013904571533, 0.9223889708518982, 0.9026186466217041, 0.9063641428947449, 0.8917431235313416, 0.8923729658126831, 0.903962254524231, 0.8988565802574158, 0.9029956459999084, 0.8892769813537598, 0.9014783501625061, 0.9218465685844421]
[2025-05-21 13:29:39,989]: Mean: 0.91760570
[2025-05-21 13:29:39,989]: Min: 0.87503874
[2025-05-21 13:29:39,989]: Max: 0.98740393
[2025-05-21 13:29:39,990]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-21 13:29:39,990]: Sample Values (25 elements): [0.05068235844373703, -0.05068235844373703, 0.025341179221868515, -0.07602353394031525, -0.10136471688747406, 0.0, -0.10136471688747406, 0.0, 0.0, 0.07602353394031525, 0.0, -0.10136471688747406, 0.0, 0.12670589983463287, -0.025341179221868515, -0.025341179221868515, -0.025341179221868515, 0.0, -0.025341179221868515, -0.07602353394031525, 0.07602353394031525, 0.025341179221868515, -0.07602353394031525, 0.0, 0.05068235844373703]
[2025-05-21 13:29:39,990]: Mean: 0.00126520
[2025-05-21 13:29:39,991]: Min: -0.17738825
[2025-05-21 13:29:39,991]: Max: 0.20272943
[2025-05-21 13:29:39,991]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-21 13:29:39,991]: Sample Values (25 elements): [0.8633059859275818, 0.8705459237098694, 0.8819682598114014, 0.880768358707428, 0.887445330619812, 0.8915205597877502, 0.8688379526138306, 0.8752140998840332, 0.8755509257316589, 0.8917839527130127, 0.883173406124115, 0.8651528358459473, 0.8945571184158325, 0.8720406293869019, 0.8977318406105042, 0.8805442452430725, 0.8735195398330688, 0.8750222325325012, 0.8537513613700867, 0.8745789527893066, 0.8641741871833801, 0.8801267147064209, 0.8752405643463135, 0.8392696380615234, 0.8969641327857971]
[2025-05-21 13:29:39,991]: Mean: 0.87433314
[2025-05-21 13:29:39,991]: Min: 0.83018684
[2025-05-21 13:29:39,991]: Max: 0.91271746
[2025-05-21 13:29:39,992]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 13:29:39,994]: Sample Values (25 elements): [0.013842865824699402, 0.027685731649398804, 0.0, 0.013842865824699402, -0.013842865824699402, 0.027685731649398804, 0.041528597474098206, 0.013842865824699402, -0.013842865824699402, 0.0, -0.013842865824699402, 0.013842865824699402, 0.027685731649398804, -0.013842865824699402, 0.027685731649398804, -0.027685731649398804, 0.0, -0.041528597474098206, 0.027685731649398804, -0.013842865824699402, 0.027685731649398804, 0.0, 0.013842865824699402, -0.05537146329879761, 0.041528597474098206]
[2025-05-21 13:29:39,994]: Mean: -0.00119441
[2025-05-21 13:29:39,994]: Min: -0.09690006
[2025-05-21 13:29:39,994]: Max: 0.11074293
[2025-05-21 13:29:39,994]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-21 13:29:39,995]: Sample Values (25 elements): [0.8778015375137329, 0.8900525569915771, 0.8929076194763184, 0.8990382552146912, 0.8952775597572327, 0.9480524659156799, 0.9495762586593628, 0.9102339744567871, 0.8911212086677551, 0.8852334022521973, 0.8813142776489258, 0.9064278602600098, 0.8810194134712219, 0.9116446375846863, 0.9087648987770081, 0.9000458121299744, 0.8912028074264526, 0.9317088723182678, 0.8936391472816467, 0.9169061183929443, 0.8828989863395691, 0.9047747850418091, 0.919357180595398, 0.8812731504440308, 0.9339627623558044]
[2025-05-21 13:29:39,995]: Mean: 0.91093731
[2025-05-21 13:29:39,995]: Min: 0.85628241
[2025-05-21 13:29:39,995]: Max: 0.98328096
[2025-05-21 13:29:39,996]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-21 13:29:39,997]: Sample Values (25 elements): [0.012333860620856285, -0.037001579999923706, -0.012333860620856285, 0.0, 0.012333860620856285, 0.02466772124171257, 0.0, 0.012333860620856285, -0.012333860620856285, 0.0, 0.0, 0.012333860620856285, -0.012333860620856285, -0.02466772124171257, 0.012333860620856285, 0.0, 0.012333860620856285, 0.02466772124171257, 0.037001579999923706, 0.0, 0.012333860620856285, 0.012333860620856285, -0.02466772124171257, 0.02466772124171257, -0.012333860620856285]
[2025-05-21 13:29:39,997]: Mean: -0.00038928
[2025-05-21 13:29:39,998]: Min: -0.08633702
[2025-05-21 13:29:39,998]: Max: 0.09867088
[2025-05-21 13:29:39,998]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-21 13:29:39,998]: Sample Values (25 elements): [0.9400633573532104, 0.9164445996284485, 0.9237176775932312, 0.9540034532546997, 0.9612292647361755, 0.9376615285873413, 0.9333910942077637, 0.9351307153701782, 0.924490213394165, 0.9324588179588318, 0.9250622391700745, 0.9363433718681335, 0.9264453053474426, 0.9339309334754944, 0.9096041321754456, 0.9189934730529785, 0.9347119927406311, 0.9293826222419739, 0.9148820042610168, 0.9995843768119812, 0.9294690489768982, 0.9283599257469177, 0.9284798502922058, 0.933053731918335, 0.9243463277816772]
[2025-05-21 13:29:39,998]: Mean: 0.93358433
[2025-05-21 13:29:39,998]: Min: 0.88780695
[2025-05-21 13:29:39,999]: Max: 0.99958438
[2025-05-21 13:29:39,999]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-21 13:29:40,002]: Sample Values (25 elements): [0.021017413586378098, -0.031526118516922, 0.0, 0.021017413586378098, 0.010508706793189049, -0.010508706793189049, -0.010508706793189049, 0.0, 0.010508706793189049, -0.021017413586378098, -0.010508706793189049, 0.010508706793189049, 0.0, -0.042034827172756195, 0.042034827172756195, 0.021017413586378098, -0.042034827172756195, -0.031526118516922, 0.010508706793189049, -0.031526118516922, 0.031526118516922, 0.021017413586378098, 0.010508706793189049, -0.042034827172756195, -0.021017413586378098]
[2025-05-21 13:29:40,002]: Mean: -0.00048009
[2025-05-21 13:29:40,002]: Min: -0.07356095
[2025-05-21 13:29:40,003]: Max: 0.08406965
[2025-05-21 13:29:40,003]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-21 13:29:40,003]: Sample Values (25 elements): [0.9043563008308411, 0.914556622505188, 0.9116227626800537, 0.9126363396644592, 0.907966136932373, 0.9137798547744751, 0.904813826084137, 0.9114706516265869, 0.9261718988418579, 0.9052014350891113, 0.9012162685394287, 0.8863118290901184, 0.910010039806366, 0.9076464772224426, 0.8957051038742065, 0.9053146839141846, 0.9084624648094177, 0.9130125641822815, 0.8955723643302917, 0.914316713809967, 0.912577748298645, 0.9084854125976562, 0.9315450191497803, 0.9164807200431824, 0.9171868562698364]
[2025-05-21 13:29:40,003]: Mean: 0.91156614
[2025-05-21 13:29:40,003]: Min: 0.86563665
[2025-05-21 13:29:40,003]: Max: 0.94969654
[2025-05-21 13:29:40,004]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 13:29:40,011]: Sample Values (25 elements): [0.0, -0.03811461478471756, 0.0, -0.00952865369617939, -0.00952865369617939, 0.0, -0.00952865369617939, 0.0, -0.00952865369617939, 0.0, -0.00952865369617939, -0.02858596108853817, -0.02858596108853817, 0.00952865369617939, -0.00952865369617939, 0.00952865369617939, 0.0, 0.01905730739235878, -0.00952865369617939, 0.00952865369617939, -0.01905730739235878, -0.00952865369617939, -0.01905730739235878, -0.00952865369617939, 0.01905730739235878]
[2025-05-21 13:29:40,011]: Mean: -0.00083714
[2025-05-21 13:29:40,011]: Min: -0.06670058
[2025-05-21 13:29:40,011]: Max: 0.07622923
[2025-05-21 13:29:40,011]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-21 13:29:40,011]: Sample Values (25 elements): [0.9194547533988953, 0.9190693497657776, 0.9235365390777588, 0.9342793226242065, 0.92852783203125, 0.9074423909187317, 0.9179083704948425, 0.9589528441429138, 0.9232438206672668, 0.9140920042991638, 0.936389684677124, 0.9328481554985046, 0.9233114123344421, 0.8933417797088623, 0.9339208602905273, 0.9187383651733398, 0.9198698997497559, 0.9053086042404175, 0.9246357679367065, 0.9547582864761353, 0.9312867522239685, 0.9368848204612732, 0.9266110062599182, 0.909123957157135, 0.9092375040054321]
[2025-05-21 13:29:40,012]: Mean: 0.91988993
[2025-05-21 13:29:40,012]: Min: 0.87562829
[2025-05-21 13:29:40,012]: Max: 0.96368301
[2025-05-21 13:29:40,013]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-21 13:29:40,013]: Sample Values (25 elements): [0.03664759173989296, 0.10994277894496918, 0.0, 0.0, 0.03664759173989296, -0.07329518347978592, 0.07329518347978592, 0.0, 0.05497138947248459, 0.0, -0.10994277894496918, -0.07329518347978592, 0.09161897748708725, 0.10994277894496918, -0.05497138947248459, 0.09161897748708725, -0.01832379586994648, 0.03664759173989296, -0.01832379586994648, -0.05497138947248459, -0.01832379586994648, -0.01832379586994648, -0.05497138947248459, 0.05497138947248459, -0.07329518347978592]
[2025-05-21 13:29:40,014]: Mean: -0.00028519
[2025-05-21 13:29:40,014]: Min: -0.12826657
[2025-05-21 13:29:40,014]: Max: 0.14659037
[2025-05-21 13:29:40,014]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-21 13:29:40,014]: Sample Values (25 elements): [0.8916305303573608, 0.8682196140289307, 0.8627299666404724, 0.8897613286972046, 0.8660784959793091, 0.8914963006973267, 0.8778802156448364, 0.8608482480049133, 0.9075626730918884, 0.8978592753410339, 0.85875004529953, 0.8790230751037598, 0.8718013167381287, 0.8780105113983154, 0.8915677070617676, 0.8789308071136475, 0.8864918947219849, 0.8790299296379089, 0.8822582960128784, 0.8689097166061401, 0.8954112529754639, 0.8736116886138916, 0.8843456506729126, 0.8761186599731445, 0.8753635883331299]
[2025-05-21 13:29:40,014]: Mean: 0.87858683
[2025-05-21 13:29:40,014]: Min: 0.84859425
[2025-05-21 13:29:40,015]: Max: 0.91069311
[2025-05-21 13:29:40,015]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 13:29:40,022]: Sample Values (25 elements): [0.009955297224223614, -0.019910594448447227, 0.009955297224223614, 0.019910594448447227, -0.009955297224223614, -0.009955297224223614, -0.009955297224223614, 0.0, 0.0, 0.0, 0.0, -0.009955297224223614, -0.009955297224223614, 0.029865890741348267, 0.019910594448447227, -0.009955297224223614, 0.0, 0.0, 0.0, 0.0, 0.009955297224223614, -0.029865890741348267, 0.0, 0.019910594448447227, -0.019910594448447227]
[2025-05-21 13:29:40,022]: Mean: -0.00082076
[2025-05-21 13:29:40,022]: Min: -0.06968708
[2025-05-21 13:29:40,022]: Max: 0.07964238
[2025-05-21 13:29:40,022]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-21 13:29:40,022]: Sample Values (25 elements): [0.9177908301353455, 0.8868259191513062, 0.9244818687438965, 0.9105514287948608, 0.9185630083084106, 0.9071831703186035, 0.9131472706794739, 0.8909773230552673, 0.8977761268615723, 0.9127418994903564, 0.9082437753677368, 0.8991873264312744, 0.9129520058631897, 0.938485860824585, 0.9145804643630981, 0.9017988443374634, 0.9181227087974548, 0.9064991474151611, 0.8978442549705505, 0.9048154354095459, 0.9233179092407227, 0.9114751815795898, 0.9047815799713135, 0.9082158803939819, 0.9147735238075256]
[2025-05-21 13:29:40,023]: Mean: 0.91059095
[2025-05-21 13:29:40,023]: Min: 0.85781032
[2025-05-21 13:29:40,023]: Max: 0.95336753
[2025-05-21 13:29:40,024]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-21 13:29:40,030]: Sample Values (25 elements): [-0.008222158998250961, 0.016444317996501923, 0.016444317996501923, 0.0, 0.008222158998250961, -0.016444317996501923, 0.0, -0.016444317996501923, -0.008222158998250961, -0.008222158998250961, -0.008222158998250961, -0.008222158998250961, -0.008222158998250961, -0.008222158998250961, 0.008222158998250961, -0.016444317996501923, 0.008222158998250961, 0.008222158998250961, 0.0, 0.008222158998250961, -0.032888635993003845, 0.016444317996501923, -0.016444317996501923, -0.016444317996501923, 0.008222158998250961]
[2025-05-21 13:29:40,030]: Mean: -0.00019409
[2025-05-21 13:29:40,030]: Min: -0.05755511
[2025-05-21 13:29:40,030]: Max: 0.06577727
[2025-05-21 13:29:40,030]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-21 13:29:40,031]: Sample Values (25 elements): [0.9280214905738831, 0.9107094407081604, 0.9259819984436035, 0.9279278516769409, 0.919981062412262, 0.9086909294128418, 0.9207723140716553, 0.9456781148910522, 0.9219620823860168, 0.9296138882637024, 0.944621741771698, 0.9159756302833557, 0.9207569360733032, 0.911160945892334, 0.9258255362510681, 0.9234817624092102, 0.9223896861076355, 0.9210052490234375, 0.9277925491333008, 0.9576782584190369, 0.9240064024925232, 0.9356252551078796, 0.9226992130279541, 0.9359297752380371, 0.9259049296379089]
[2025-05-21 13:29:40,031]: Mean: 0.92871368
[2025-05-21 13:29:40,031]: Min: 0.89817166
[2025-05-21 13:29:40,031]: Max: 0.96993458
[2025-05-21 13:29:40,032]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-21 13:29:40,045]: Sample Values (25 elements): [-0.013213260099291801, 0.006606630049645901, 0.019819889217615128, -0.019819889217615128, -0.006606630049645901, -0.006606630049645901, 0.006606630049645901, 0.019819889217615128, 0.026426520198583603, 0.0, -0.006606630049645901, 0.019819889217615128, 0.019819889217615128, -0.026426520198583603, 0.0, 0.0, 0.0, 0.019819889217615128, 0.006606630049645901, 0.0, 0.019819889217615128, 0.0, 0.013213260099291801, 0.019819889217615128, -0.006606630049645901]
[2025-05-21 13:29:40,046]: Mean: -0.00005397
[2025-05-21 13:29:40,046]: Min: -0.04624641
[2025-05-21 13:29:40,046]: Max: 0.05285304
[2025-05-21 13:29:40,046]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-21 13:29:40,046]: Sample Values (25 elements): [0.916204035282135, 0.9130301475524902, 0.9090136289596558, 0.9108341932296753, 0.909687876701355, 0.9196211099624634, 0.9104350209236145, 0.912284255027771, 0.9119349122047424, 0.9090878963470459, 0.9207376837730408, 0.9089946150779724, 0.9069527387619019, 0.9113975763320923, 0.91660475730896, 0.8938624262809753, 0.9061106443405151, 0.916772186756134, 0.912257969379425, 0.9031275510787964, 0.913682758808136, 0.9027668833732605, 0.9135385751724243, 0.9017044305801392, 0.9112066030502319]
[2025-05-21 13:29:40,046]: Mean: 0.91100168
[2025-05-21 13:29:40,047]: Min: 0.89386243
[2025-05-21 13:29:40,047]: Max: 0.92898548
[2025-05-21 13:29:40,048]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 13:29:40,090]: Sample Values (25 elements): [0.014731734991073608, -0.009821156971156597, 0.0, -0.004910578485578299, 0.014731734991073608, -0.019642313942313194, 0.009821156971156597, -0.014731734991073608, 0.009821156971156597, 0.009821156971156597, -0.004910578485578299, -0.009821156971156597, -0.004910578485578299, 0.0, 0.009821156971156597, 0.02455289289355278, 0.004910578485578299, 0.0, -0.014731734991073608, 0.0, 0.009821156971156597, -0.004910578485578299, 0.0, 0.004910578485578299, 0.009821156971156597]
[2025-05-21 13:29:40,090]: Mean: -0.00010070
[2025-05-21 13:29:40,090]: Min: -0.03437405
[2025-05-21 13:29:40,091]: Max: 0.03928463
[2025-05-21 13:29:40,091]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-21 13:29:40,091]: Sample Values (25 elements): [0.9126542806625366, 0.9025781750679016, 0.9119554758071899, 0.9122456908226013, 0.9078745245933533, 0.9102082252502441, 0.9123228192329407, 0.9221956729888916, 0.9011383056640625, 0.904062032699585, 0.9106814861297607, 0.9056621789932251, 0.9051421284675598, 0.9098851084709167, 0.9031241536140442, 0.9192437529563904, 0.9143267273902893, 0.9128578305244446, 0.9009894728660583, 0.9128054976463318, 0.9104205965995789, 0.9004676938056946, 0.9152766466140747, 0.9122666120529175, 0.9059142470359802]
[2025-05-21 13:29:40,091]: Mean: 0.90978634
[2025-05-21 13:29:40,091]: Min: 0.89841348
[2025-05-21 13:29:40,091]: Max: 0.93152970
[2025-05-21 13:29:40,093]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-21 13:29:40,094]: Sample Values (25 elements): [0.0, 0.02522502839565277, 0.02522502839565277, 0.02522502839565277, 0.02522502839565277, 0.037837542593479156, -0.02522502839565277, -0.06306257098913193, -0.05045005679130554, 0.02522502839565277, -0.037837542593479156, -0.05045005679130554, 0.06306257098913193, 0.02522502839565277, 0.05045005679130554, 0.0, -0.037837542593479156, -0.02522502839565277, -0.05045005679130554, 0.012612514197826385, -0.012612514197826385, 0.02522502839565277, 0.012612514197826385, 0.02522502839565277, 0.037837542593479156]
[2025-05-21 13:29:40,094]: Mean: -0.00006486
[2025-05-21 13:29:40,094]: Min: -0.10090011
[2025-05-21 13:29:40,094]: Max: 0.08828760
[2025-05-21 13:29:40,094]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-21 13:29:40,095]: Sample Values (25 elements): [0.9022462964057922, 0.903369128704071, 0.9001501798629761, 0.896523654460907, 0.90045166015625, 0.9018222093582153, 0.9076122045516968, 0.8974813222885132, 0.8998637795448303, 0.9011403322219849, 0.895436704158783, 0.8965440988540649, 0.9025551080703735, 0.8980106115341187, 0.9037394523620605, 0.9006513953208923, 0.8979843854904175, 0.8982061743736267, 0.8964195847511292, 0.8877997994422913, 0.9007182717323303, 0.8992745280265808, 0.9072443246841431, 0.9059985280036926, 0.896866500377655]
[2025-05-21 13:29:40,095]: Mean: 0.90099788
[2025-05-21 13:29:40,095]: Min: 0.88725936
[2025-05-21 13:29:40,095]: Max: 0.91431135
[2025-05-21 13:29:40,096]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 13:29:40,136]: Sample Values (25 elements): [0.010519705712795258, -0.010519705712795258, 0.010519705712795258, -0.010519705712795258, 0.0, -0.010519705712795258, 0.010519705712795258, 0.005259852856397629, 0.005259852856397629, 0.010519705712795258, 0.010519705712795258, 0.010519705712795258, 0.0, 0.0, -0.005259852856397629, -0.015779558569192886, -0.005259852856397629, 0.010519705712795258, 0.015779558569192886, 0.005259852856397629, -0.010519705712795258, 0.0, 0.0, 0.0, -0.005259852856397629]
[2025-05-21 13:29:40,137]: Mean: -0.00009800
[2025-05-21 13:29:40,137]: Min: -0.04207882
[2025-05-21 13:29:40,137]: Max: 0.03681897
[2025-05-21 13:29:40,137]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-21 13:29:40,138]: Sample Values (25 elements): [0.911643922328949, 0.9073137640953064, 0.9101015329360962, 0.9083269834518433, 0.9109693765640259, 0.9151103496551514, 0.907072126865387, 0.9153127670288086, 0.9040876030921936, 0.9103147983551025, 0.9095884561538696, 0.9141993522644043, 0.9164196848869324, 0.9088220596313477, 0.9081568717956543, 0.9123372435569763, 0.9087203741073608, 0.9075882434844971, 0.9067670106887817, 0.9104703664779663, 0.9072176218032837, 0.9081215858459473, 0.9117773771286011, 0.9081650972366333, 0.9079506993293762]
[2025-05-21 13:29:40,139]: Mean: 0.91060746
[2025-05-21 13:29:40,139]: Min: 0.90002728
[2025-05-21 13:29:40,139]: Max: 0.93484545
[2025-05-21 13:29:40,140]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-21 13:29:40,178]: Sample Values (25 elements): [-0.01065386924892664, 0.01065386924892664, 0.0, 0.0, 0.007102579344063997, -0.0035512896720319986, 0.0035512896720319986, 0.0, -0.01065386924892664, 0.0, 0.01065386924892664, -0.0035512896720319986, -0.007102579344063997, -0.0035512896720319986, -0.0035512896720319986, 0.0035512896720319986, 0.0035512896720319986, 0.01065386924892664, 0.01065386924892664, -0.01065386924892664, -0.0035512896720319986, -0.007102579344063997, 0.01065386924892664, -0.01065386924892664, 0.007102579344063997]
[2025-05-21 13:29:40,178]: Mean: 0.00033807
[2025-05-21 13:29:40,178]: Min: -0.02485903
[2025-05-21 13:29:40,178]: Max: 0.02841032
[2025-05-21 13:29:40,179]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-21 13:29:40,179]: Sample Values (25 elements): [0.9108114242553711, 0.9111517667770386, 0.9161337614059448, 0.9180681109428406, 0.9130795001983643, 0.9167600870132446, 0.909029483795166, 0.9035347104072571, 0.9084267616271973, 0.908473551273346, 0.9073852896690369, 0.9151592254638672, 0.9120445847511292, 0.9107711315155029, 0.9154933094978333, 0.9070738554000854, 0.9151159524917603, 0.9086323976516724, 0.9102364182472229, 0.9085932970046997, 0.9115344285964966, 0.9141994714736938, 0.9063803553581238, 0.9107728600502014, 0.9171263575553894]
[2025-05-21 13:29:40,179]: Mean: 0.91213334
[2025-05-21 13:29:40,179]: Min: 0.89922851
[2025-05-21 13:29:40,179]: Max: 0.92396396
[2025-05-21 13:29:40,179]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-21 13:29:40,180]: Sample Values (25 elements): [-0.009130099788308144, 0.03855961933732033, -0.008606186136603355, -0.008582360111176968, -0.011431042104959488, 0.037414561957120895, -0.0024147455114871264, -0.06395244598388672, 0.02075352519750595, 0.011812523938715458, -0.05366663262248039, -0.11748725920915604, 0.04825164005160332, -0.041924960911273956, 0.08769896626472473, -0.03610464558005333, 0.07530582696199417, 0.057666365057229996, 0.017940202727913857, -0.0808272436261177, 0.012405402958393097, -0.03124355524778366, -0.09680726379156113, -0.04934436455368996, -0.04688141494989395]
[2025-05-21 13:29:40,180]: Mean: -0.00024709
[2025-05-21 13:29:40,180]: Min: -0.16808580
[2025-05-21 13:29:40,180]: Max: 0.18773268
