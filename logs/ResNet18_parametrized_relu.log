[2025-05-14 07:02:57,026]: 
Training ResNet18 with parametrized_relu
[2025-05-14 07:04:21,281]: [ResNet18_parametrized_relu] Epoch: 001 Train Loss: 1.6324 Train Acc: 0.3974 Eval Loss: 1.3332 Eval Acc: 0.5156 (LR: 0.001000)
[2025-05-14 07:05:45,327]: [ResNet18_parametrized_relu] Epoch: 002 Train Loss: 1.2477 Train Acc: 0.5482 Eval Loss: 1.1706 Eval Acc: 0.5866 (LR: 0.001000)
[2025-05-14 07:07:09,182]: [ResNet18_parametrized_relu] Epoch: 003 Train Loss: 1.0492 Train Acc: 0.6245 Eval Loss: 0.9674 Eval Acc: 0.6591 (LR: 0.001000)
[2025-05-14 07:08:33,189]: [ResNet18_parametrized_relu] Epoch: 004 Train Loss: 0.9163 Train Acc: 0.6736 Eval Loss: 0.8698 Eval Acc: 0.6942 (LR: 0.001000)
[2025-05-14 07:09:57,312]: [ResNet18_parametrized_relu] Epoch: 005 Train Loss: 0.8176 Train Acc: 0.7089 Eval Loss: 0.7676 Eval Acc: 0.7314 (LR: 0.001000)
[2025-05-14 07:11:21,354]: [ResNet18_parametrized_relu] Epoch: 006 Train Loss: 0.7416 Train Acc: 0.7376 Eval Loss: 0.6994 Eval Acc: 0.7551 (LR: 0.001000)
[2025-05-14 07:12:45,392]: [ResNet18_parametrized_relu] Epoch: 007 Train Loss: 0.6719 Train Acc: 0.7652 Eval Loss: 0.7037 Eval Acc: 0.7627 (LR: 0.001000)
[2025-05-14 07:14:09,407]: [ResNet18_parametrized_relu] Epoch: 008 Train Loss: 0.6291 Train Acc: 0.7798 Eval Loss: 0.6805 Eval Acc: 0.7696 (LR: 0.001000)
[2025-05-14 07:15:33,268]: [ResNet18_parametrized_relu] Epoch: 009 Train Loss: 0.5826 Train Acc: 0.7956 Eval Loss: 0.5808 Eval Acc: 0.8013 (LR: 0.001000)
[2025-05-14 07:16:57,255]: [ResNet18_parametrized_relu] Epoch: 010 Train Loss: 0.5491 Train Acc: 0.8089 Eval Loss: 0.6234 Eval Acc: 0.7890 (LR: 0.001000)
[2025-05-14 07:18:21,296]: [ResNet18_parametrized_relu] Epoch: 011 Train Loss: 0.5210 Train Acc: 0.8187 Eval Loss: 0.5872 Eval Acc: 0.8041 (LR: 0.001000)
[2025-05-14 07:19:45,272]: [ResNet18_parametrized_relu] Epoch: 012 Train Loss: 0.4955 Train Acc: 0.8269 Eval Loss: 0.5488 Eval Acc: 0.8221 (LR: 0.001000)
[2025-05-14 07:21:09,306]: [ResNet18_parametrized_relu] Epoch: 013 Train Loss: 0.4657 Train Acc: 0.8378 Eval Loss: 0.5257 Eval Acc: 0.8238 (LR: 0.001000)
[2025-05-14 07:22:33,348]: [ResNet18_parametrized_relu] Epoch: 014 Train Loss: 0.4508 Train Acc: 0.8412 Eval Loss: 0.5097 Eval Acc: 0.8336 (LR: 0.001000)
[2025-05-14 07:23:57,369]: [ResNet18_parametrized_relu] Epoch: 015 Train Loss: 0.4254 Train Acc: 0.8511 Eval Loss: 0.5052 Eval Acc: 0.8311 (LR: 0.001000)
[2025-05-14 07:25:21,415]: [ResNet18_parametrized_relu] Epoch: 016 Train Loss: 0.4112 Train Acc: 0.8555 Eval Loss: 0.4716 Eval Acc: 0.8430 (LR: 0.001000)
[2025-05-14 07:26:45,618]: [ResNet18_parametrized_relu] Epoch: 017 Train Loss: 0.3950 Train Acc: 0.8599 Eval Loss: 0.4892 Eval Acc: 0.8354 (LR: 0.001000)
[2025-05-14 07:28:09,475]: [ResNet18_parametrized_relu] Epoch: 018 Train Loss: 0.3764 Train Acc: 0.8682 Eval Loss: 0.4949 Eval Acc: 0.8381 (LR: 0.001000)
[2025-05-14 07:29:33,509]: [ResNet18_parametrized_relu] Epoch: 019 Train Loss: 0.3624 Train Acc: 0.8727 Eval Loss: 0.4596 Eval Acc: 0.8504 (LR: 0.001000)
[2025-05-14 07:30:58,492]: [ResNet18_parametrized_relu] Epoch: 020 Train Loss: 0.3505 Train Acc: 0.8770 Eval Loss: 0.4572 Eval Acc: 0.8530 (LR: 0.001000)
[2025-05-14 07:32:22,389]: [ResNet18_parametrized_relu] Epoch: 021 Train Loss: 0.3355 Train Acc: 0.8820 Eval Loss: 0.4082 Eval Acc: 0.8649 (LR: 0.001000)
[2025-05-14 07:33:46,377]: [ResNet18_parametrized_relu] Epoch: 022 Train Loss: 0.3207 Train Acc: 0.8877 Eval Loss: 0.4679 Eval Acc: 0.8523 (LR: 0.001000)
[2025-05-14 07:35:10,399]: [ResNet18_parametrized_relu] Epoch: 023 Train Loss: 0.3123 Train Acc: 0.8905 Eval Loss: 0.4303 Eval Acc: 0.8598 (LR: 0.001000)
[2025-05-14 07:36:34,436]: [ResNet18_parametrized_relu] Epoch: 024 Train Loss: 0.3000 Train Acc: 0.8947 Eval Loss: 0.4401 Eval Acc: 0.8572 (LR: 0.001000)
[2025-05-14 07:37:58,464]: [ResNet18_parametrized_relu] Epoch: 025 Train Loss: 0.2895 Train Acc: 0.8995 Eval Loss: 0.3966 Eval Acc: 0.8681 (LR: 0.001000)
[2025-05-14 07:39:22,477]: [ResNet18_parametrized_relu] Epoch: 026 Train Loss: 0.2809 Train Acc: 0.9028 Eval Loss: 0.4419 Eval Acc: 0.8617 (LR: 0.001000)
[2025-05-14 07:40:46,324]: [ResNet18_parametrized_relu] Epoch: 027 Train Loss: 0.2708 Train Acc: 0.9044 Eval Loss: 0.3815 Eval Acc: 0.8781 (LR: 0.001000)
[2025-05-14 07:42:10,353]: [ResNet18_parametrized_relu] Epoch: 028 Train Loss: 0.2618 Train Acc: 0.9091 Eval Loss: 0.3950 Eval Acc: 0.8741 (LR: 0.001000)
[2025-05-14 07:43:34,295]: [ResNet18_parametrized_relu] Epoch: 029 Train Loss: 0.2570 Train Acc: 0.9098 Eval Loss: 0.4080 Eval Acc: 0.8737 (LR: 0.001000)
[2025-05-14 07:44:58,117]: [ResNet18_parametrized_relu] Epoch: 030 Train Loss: 0.2445 Train Acc: 0.9128 Eval Loss: 0.4182 Eval Acc: 0.8648 (LR: 0.001000)
[2025-05-14 07:46:22,099]: [ResNet18_parametrized_relu] Epoch: 031 Train Loss: 0.2368 Train Acc: 0.9164 Eval Loss: 0.4045 Eval Acc: 0.8751 (LR: 0.001000)
[2025-05-14 07:47:45,936]: [ResNet18_parametrized_relu] Epoch: 032 Train Loss: 0.2340 Train Acc: 0.9173 Eval Loss: 0.4028 Eval Acc: 0.8772 (LR: 0.001000)
[2025-05-14 07:49:09,981]: [ResNet18_parametrized_relu] Epoch: 033 Train Loss: 0.2250 Train Acc: 0.9201 Eval Loss: 0.3950 Eval Acc: 0.8750 (LR: 0.001000)
[2025-05-14 07:50:33,626]: [ResNet18_parametrized_relu] Epoch: 034 Train Loss: 0.2164 Train Acc: 0.9248 Eval Loss: 0.4083 Eval Acc: 0.8782 (LR: 0.001000)
[2025-05-14 07:51:57,685]: [ResNet18_parametrized_relu] Epoch: 035 Train Loss: 0.2096 Train Acc: 0.9255 Eval Loss: 0.3962 Eval Acc: 0.8808 (LR: 0.001000)
[2025-05-14 07:53:21,828]: [ResNet18_parametrized_relu] Epoch: 036 Train Loss: 0.2001 Train Acc: 0.9286 Eval Loss: 0.4054 Eval Acc: 0.8755 (LR: 0.001000)
[2025-05-14 07:54:45,838]: [ResNet18_parametrized_relu] Epoch: 037 Train Loss: 0.1976 Train Acc: 0.9304 Eval Loss: 0.3936 Eval Acc: 0.8842 (LR: 0.001000)
[2025-05-14 07:56:09,660]: [ResNet18_parametrized_relu] Epoch: 038 Train Loss: 0.1876 Train Acc: 0.9322 Eval Loss: 0.3903 Eval Acc: 0.8828 (LR: 0.001000)
[2025-05-14 07:57:33,490]: [ResNet18_parametrized_relu] Epoch: 039 Train Loss: 0.1852 Train Acc: 0.9338 Eval Loss: 0.4252 Eval Acc: 0.8759 (LR: 0.001000)
[2025-05-14 07:58:58,506]: [ResNet18_parametrized_relu] Epoch: 040 Train Loss: 0.1812 Train Acc: 0.9352 Eval Loss: 0.4070 Eval Acc: 0.8831 (LR: 0.001000)
[2025-05-14 08:00:22,489]: [ResNet18_parametrized_relu] Epoch: 041 Train Loss: 0.1719 Train Acc: 0.9389 Eval Loss: 0.4214 Eval Acc: 0.8767 (LR: 0.001000)
[2025-05-14 08:01:46,516]: [ResNet18_parametrized_relu] Epoch: 042 Train Loss: 0.1672 Train Acc: 0.9395 Eval Loss: 0.3955 Eval Acc: 0.8850 (LR: 0.001000)
[2025-05-14 08:03:10,574]: [ResNet18_parametrized_relu] Epoch: 043 Train Loss: 0.1635 Train Acc: 0.9427 Eval Loss: 0.4009 Eval Acc: 0.8841 (LR: 0.001000)
[2025-05-14 08:04:34,752]: [ResNet18_parametrized_relu] Epoch: 044 Train Loss: 0.1560 Train Acc: 0.9451 Eval Loss: 0.4091 Eval Acc: 0.8833 (LR: 0.001000)
[2025-05-14 08:05:58,780]: [ResNet18_parametrized_relu] Epoch: 045 Train Loss: 0.1564 Train Acc: 0.9445 Eval Loss: 0.4049 Eval Acc: 0.8854 (LR: 0.001000)
[2025-05-14 08:07:22,812]: [ResNet18_parametrized_relu] Epoch: 046 Train Loss: 0.1483 Train Acc: 0.9469 Eval Loss: 0.4012 Eval Acc: 0.8838 (LR: 0.001000)
[2025-05-14 08:08:46,861]: [ResNet18_parametrized_relu] Epoch: 047 Train Loss: 0.1495 Train Acc: 0.9465 Eval Loss: 0.4231 Eval Acc: 0.8857 (LR: 0.001000)
[2025-05-14 08:10:10,784]: [ResNet18_parametrized_relu] Epoch: 048 Train Loss: 0.1415 Train Acc: 0.9497 Eval Loss: 0.4212 Eval Acc: 0.8823 (LR: 0.001000)
[2025-05-14 08:11:34,626]: [ResNet18_parametrized_relu] Epoch: 049 Train Loss: 0.1416 Train Acc: 0.9502 Eval Loss: 0.4202 Eval Acc: 0.8880 (LR: 0.001000)
[2025-05-14 08:12:58,468]: [ResNet18_parametrized_relu] Epoch: 050 Train Loss: 0.1334 Train Acc: 0.9519 Eval Loss: 0.4104 Eval Acc: 0.8868 (LR: 0.001000)
[2025-05-14 08:14:22,445]: [ResNet18_parametrized_relu] Epoch: 051 Train Loss: 0.1249 Train Acc: 0.9561 Eval Loss: 0.3926 Eval Acc: 0.8917 (LR: 0.001000)
[2025-05-14 08:15:46,295]: [ResNet18_parametrized_relu] Epoch: 052 Train Loss: 0.1237 Train Acc: 0.9568 Eval Loss: 0.4468 Eval Acc: 0.8801 (LR: 0.001000)
[2025-05-14 08:17:10,123]: [ResNet18_parametrized_relu] Epoch: 053 Train Loss: 0.1188 Train Acc: 0.9577 Eval Loss: 0.3907 Eval Acc: 0.8923 (LR: 0.001000)
[2025-05-14 08:18:34,125]: [ResNet18_parametrized_relu] Epoch: 054 Train Loss: 0.1212 Train Acc: 0.9574 Eval Loss: 0.3677 Eval Acc: 0.8968 (LR: 0.001000)
[2025-05-14 08:19:58,147]: [ResNet18_parametrized_relu] Epoch: 055 Train Loss: 0.1182 Train Acc: 0.9576 Eval Loss: 0.3927 Eval Acc: 0.8926 (LR: 0.001000)
[2025-05-14 08:21:22,170]: [ResNet18_parametrized_relu] Epoch: 056 Train Loss: 0.1117 Train Acc: 0.9596 Eval Loss: 0.4059 Eval Acc: 0.8905 (LR: 0.001000)
[2025-05-14 08:22:46,207]: [ResNet18_parametrized_relu] Epoch: 057 Train Loss: 0.1070 Train Acc: 0.9621 Eval Loss: 0.4134 Eval Acc: 0.8905 (LR: 0.001000)
[2025-05-14 08:24:10,364]: [ResNet18_parametrized_relu] Epoch: 058 Train Loss: 0.1095 Train Acc: 0.9616 Eval Loss: 0.4355 Eval Acc: 0.8838 (LR: 0.001000)
[2025-05-14 08:25:34,418]: [ResNet18_parametrized_relu] Epoch: 059 Train Loss: 0.1041 Train Acc: 0.9629 Eval Loss: 0.4336 Eval Acc: 0.8882 (LR: 0.001000)
[2025-05-14 08:26:59,420]: [ResNet18_parametrized_relu] Epoch: 060 Train Loss: 0.0979 Train Acc: 0.9656 Eval Loss: 0.4125 Eval Acc: 0.8941 (LR: 0.001000)
[2025-05-14 08:28:23,616]: [ResNet18_parametrized_relu] Epoch: 061 Train Loss: 0.0948 Train Acc: 0.9663 Eval Loss: 0.4307 Eval Acc: 0.8875 (LR: 0.001000)
[2025-05-14 08:29:47,656]: [ResNet18_parametrized_relu] Epoch: 062 Train Loss: 0.0960 Train Acc: 0.9662 Eval Loss: 0.4218 Eval Acc: 0.8939 (LR: 0.001000)
[2025-05-14 08:31:11,759]: [ResNet18_parametrized_relu] Epoch: 063 Train Loss: 0.0908 Train Acc: 0.9682 Eval Loss: 0.4247 Eval Acc: 0.8915 (LR: 0.001000)
[2025-05-14 08:32:35,602]: [ResNet18_parametrized_relu] Epoch: 064 Train Loss: 0.0908 Train Acc: 0.9678 Eval Loss: 0.3863 Eval Acc: 0.8986 (LR: 0.001000)
[2025-05-14 08:33:59,620]: [ResNet18_parametrized_relu] Epoch: 065 Train Loss: 0.0892 Train Acc: 0.9685 Eval Loss: 0.3889 Eval Acc: 0.8973 (LR: 0.001000)
[2025-05-14 08:35:23,461]: [ResNet18_parametrized_relu] Epoch: 066 Train Loss: 0.0858 Train Acc: 0.9701 Eval Loss: 0.4006 Eval Acc: 0.8995 (LR: 0.001000)
[2025-05-14 08:36:47,460]: [ResNet18_parametrized_relu] Epoch: 067 Train Loss: 0.0851 Train Acc: 0.9701 Eval Loss: 0.3963 Eval Acc: 0.8985 (LR: 0.001000)
[2025-05-14 08:38:11,296]: [ResNet18_parametrized_relu] Epoch: 068 Train Loss: 0.0840 Train Acc: 0.9701 Eval Loss: 0.3900 Eval Acc: 0.9005 (LR: 0.001000)
[2025-05-14 08:39:35,283]: [ResNet18_parametrized_relu] Epoch: 069 Train Loss: 0.0797 Train Acc: 0.9720 Eval Loss: 0.3940 Eval Acc: 0.9017 (LR: 0.001000)
[2025-05-14 08:40:59,304]: [ResNet18_parametrized_relu] Epoch: 070 Train Loss: 0.0765 Train Acc: 0.9735 Eval Loss: 0.4338 Eval Acc: 0.8942 (LR: 0.000100)
[2025-05-14 08:42:23,365]: [ResNet18_parametrized_relu] Epoch: 071 Train Loss: 0.0499 Train Acc: 0.9839 Eval Loss: 0.3572 Eval Acc: 0.9104 (LR: 0.000100)
[2025-05-14 08:43:47,184]: [ResNet18_parametrized_relu] Epoch: 072 Train Loss: 0.0402 Train Acc: 0.9871 Eval Loss: 0.3517 Eval Acc: 0.9114 (LR: 0.000100)
[2025-05-14 08:45:11,133]: [ResNet18_parametrized_relu] Epoch: 073 Train Loss: 0.0362 Train Acc: 0.9888 Eval Loss: 0.3476 Eval Acc: 0.9127 (LR: 0.000100)
[2025-05-14 08:46:35,149]: [ResNet18_parametrized_relu] Epoch: 074 Train Loss: 0.0382 Train Acc: 0.9880 Eval Loss: 0.3535 Eval Acc: 0.9110 (LR: 0.000100)
[2025-05-14 08:47:58,803]: [ResNet18_parametrized_relu] Epoch: 075 Train Loss: 0.0350 Train Acc: 0.9894 Eval Loss: 0.3477 Eval Acc: 0.9102 (LR: 0.000100)
[2025-05-14 08:49:22,832]: [ResNet18_parametrized_relu] Epoch: 076 Train Loss: 0.0330 Train Acc: 0.9902 Eval Loss: 0.3500 Eval Acc: 0.9106 (LR: 0.000100)
[2025-05-14 08:50:46,618]: [ResNet18_parametrized_relu] Epoch: 077 Train Loss: 0.0312 Train Acc: 0.9909 Eval Loss: 0.3454 Eval Acc: 0.9124 (LR: 0.000100)
[2025-05-14 08:52:10,459]: [ResNet18_parametrized_relu] Epoch: 078 Train Loss: 0.0319 Train Acc: 0.9904 Eval Loss: 0.3506 Eval Acc: 0.9114 (LR: 0.000100)
[2025-05-14 08:53:34,443]: [ResNet18_parametrized_relu] Epoch: 079 Train Loss: 0.0301 Train Acc: 0.9912 Eval Loss: 0.3497 Eval Acc: 0.9109 (LR: 0.000100)
[2025-05-14 08:54:59,439]: [ResNet18_parametrized_relu] Epoch: 080 Train Loss: 0.0294 Train Acc: 0.9914 Eval Loss: 0.3515 Eval Acc: 0.9120 (LR: 0.000100)
[2025-05-14 08:56:23,316]: [ResNet18_parametrized_relu] Epoch: 081 Train Loss: 0.0299 Train Acc: 0.9907 Eval Loss: 0.3483 Eval Acc: 0.9121 (LR: 0.000100)
[2025-05-14 08:57:47,260]: [ResNet18_parametrized_relu] Epoch: 082 Train Loss: 0.0301 Train Acc: 0.9910 Eval Loss: 0.3502 Eval Acc: 0.9123 (LR: 0.000100)
[2025-05-14 08:59:11,111]: [ResNet18_parametrized_relu] Epoch: 083 Train Loss: 0.0294 Train Acc: 0.9913 Eval Loss: 0.3503 Eval Acc: 0.9128 (LR: 0.000100)
[2025-05-14 09:00:35,032]: [ResNet18_parametrized_relu] Epoch: 084 Train Loss: 0.0289 Train Acc: 0.9915 Eval Loss: 0.3531 Eval Acc: 0.9120 (LR: 0.000100)
[2025-05-14 09:01:58,685]: [ResNet18_parametrized_relu] Epoch: 085 Train Loss: 0.0275 Train Acc: 0.9923 Eval Loss: 0.3523 Eval Acc: 0.9129 (LR: 0.000100)
[2025-05-14 09:03:22,704]: [ResNet18_parametrized_relu] Epoch: 086 Train Loss: 0.0274 Train Acc: 0.9920 Eval Loss: 0.3527 Eval Acc: 0.9115 (LR: 0.000100)
[2025-05-14 09:04:46,663]: [ResNet18_parametrized_relu] Epoch: 087 Train Loss: 0.0278 Train Acc: 0.9922 Eval Loss: 0.3554 Eval Acc: 0.9119 (LR: 0.000100)
[2025-05-14 09:06:10,581]: [ResNet18_parametrized_relu] Epoch: 088 Train Loss: 0.0265 Train Acc: 0.9924 Eval Loss: 0.3559 Eval Acc: 0.9111 (LR: 0.000100)
[2025-05-14 09:07:34,893]: [ResNet18_parametrized_relu] Epoch: 089 Train Loss: 0.0276 Train Acc: 0.9918 Eval Loss: 0.3532 Eval Acc: 0.9118 (LR: 0.000100)
[2025-05-14 09:09:14,647]: [ResNet18_parametrized_relu] Epoch: 090 Train Loss: 0.0267 Train Acc: 0.9923 Eval Loss: 0.3520 Eval Acc: 0.9122 (LR: 0.000100)
[2025-05-14 09:10:57,609]: [ResNet18_parametrized_relu] Epoch: 091 Train Loss: 0.0253 Train Acc: 0.9930 Eval Loss: 0.3564 Eval Acc: 0.9126 (LR: 0.000100)
[2025-05-14 09:12:40,743]: [ResNet18_parametrized_relu] Epoch: 092 Train Loss: 0.0256 Train Acc: 0.9926 Eval Loss: 0.3542 Eval Acc: 0.9108 (LR: 0.000100)
[2025-05-14 09:14:23,838]: [ResNet18_parametrized_relu] Epoch: 093 Train Loss: 0.0255 Train Acc: 0.9924 Eval Loss: 0.3544 Eval Acc: 0.9126 (LR: 0.000100)
[2025-05-14 09:16:07,337]: [ResNet18_parametrized_relu] Epoch: 094 Train Loss: 0.0235 Train Acc: 0.9933 Eval Loss: 0.3573 Eval Acc: 0.9114 (LR: 0.000100)
[2025-05-14 09:17:49,622]: [ResNet18_parametrized_relu] Epoch: 095 Train Loss: 0.0245 Train Acc: 0.9929 Eval Loss: 0.3567 Eval Acc: 0.9115 (LR: 0.000100)
[2025-05-14 09:19:32,572]: [ResNet18_parametrized_relu] Epoch: 096 Train Loss: 0.0250 Train Acc: 0.9925 Eval Loss: 0.3617 Eval Acc: 0.9087 (LR: 0.000100)
[2025-05-14 09:21:16,108]: [ResNet18_parametrized_relu] Epoch: 097 Train Loss: 0.0248 Train Acc: 0.9924 Eval Loss: 0.3571 Eval Acc: 0.9100 (LR: 0.000100)
[2025-05-14 09:22:59,182]: [ResNet18_parametrized_relu] Epoch: 098 Train Loss: 0.0239 Train Acc: 0.9932 Eval Loss: 0.3584 Eval Acc: 0.9117 (LR: 0.000100)
[2025-05-14 09:24:44,038]: [ResNet18_parametrized_relu] Epoch: 099 Train Loss: 0.0240 Train Acc: 0.9932 Eval Loss: 0.3598 Eval Acc: 0.9106 (LR: 0.000100)
[2025-05-14 09:26:30,065]: [ResNet18_parametrized_relu] Epoch: 100 Train Loss: 0.0234 Train Acc: 0.9934 Eval Loss: 0.3594 Eval Acc: 0.9124 (LR: 0.000010)
[2025-05-14 09:28:15,023]: [ResNet18_parametrized_relu] Epoch: 101 Train Loss: 0.0232 Train Acc: 0.9936 Eval Loss: 0.3566 Eval Acc: 0.9112 (LR: 0.000010)
[2025-05-14 09:30:00,223]: [ResNet18_parametrized_relu] Epoch: 102 Train Loss: 0.0222 Train Acc: 0.9939 Eval Loss: 0.3585 Eval Acc: 0.9118 (LR: 0.000010)
[2025-05-14 09:31:45,081]: [ResNet18_parametrized_relu] Epoch: 103 Train Loss: 0.0218 Train Acc: 0.9941 Eval Loss: 0.3603 Eval Acc: 0.9116 (LR: 0.000010)
[2025-05-14 09:33:29,911]: [ResNet18_parametrized_relu] Epoch: 104 Train Loss: 0.0220 Train Acc: 0.9938 Eval Loss: 0.3591 Eval Acc: 0.9121 (LR: 0.000010)
[2025-05-14 09:35:15,184]: [ResNet18_parametrized_relu] Epoch: 105 Train Loss: 0.0210 Train Acc: 0.9945 Eval Loss: 0.3579 Eval Acc: 0.9124 (LR: 0.000010)
[2025-05-14 09:37:00,067]: [ResNet18_parametrized_relu] Epoch: 106 Train Loss: 0.0209 Train Acc: 0.9945 Eval Loss: 0.3581 Eval Acc: 0.9114 (LR: 0.000010)
[2025-05-14 09:38:44,914]: [ResNet18_parametrized_relu] Epoch: 107 Train Loss: 0.0224 Train Acc: 0.9935 Eval Loss: 0.3610 Eval Acc: 0.9117 (LR: 0.000010)
[2025-05-14 09:40:29,993]: [ResNet18_parametrized_relu] Epoch: 108 Train Loss: 0.0220 Train Acc: 0.9938 Eval Loss: 0.3585 Eval Acc: 0.9119 (LR: 0.000010)
[2025-05-14 09:42:14,801]: [ResNet18_parametrized_relu] Epoch: 109 Train Loss: 0.0200 Train Acc: 0.9950 Eval Loss: 0.3581 Eval Acc: 0.9118 (LR: 0.000010)
[2025-05-14 09:43:59,645]: [ResNet18_parametrized_relu] Epoch: 110 Train Loss: 0.0209 Train Acc: 0.9939 Eval Loss: 0.3585 Eval Acc: 0.9112 (LR: 0.000010)
[2025-05-14 09:45:44,507]: [ResNet18_parametrized_relu] Epoch: 111 Train Loss: 0.0219 Train Acc: 0.9938 Eval Loss: 0.3590 Eval Acc: 0.9112 (LR: 0.000010)
[2025-05-14 09:47:29,340]: [ResNet18_parametrized_relu] Epoch: 112 Train Loss: 0.0234 Train Acc: 0.9932 Eval Loss: 0.3598 Eval Acc: 0.9114 (LR: 0.000010)
[2025-05-14 09:49:14,206]: [ResNet18_parametrized_relu] Epoch: 113 Train Loss: 0.0212 Train Acc: 0.9942 Eval Loss: 0.3584 Eval Acc: 0.9110 (LR: 0.000010)
[2025-05-14 09:50:59,439]: [ResNet18_parametrized_relu] Epoch: 114 Train Loss: 0.0233 Train Acc: 0.9934 Eval Loss: 0.3578 Eval Acc: 0.9123 (LR: 0.000010)
[2025-05-14 09:52:44,274]: [ResNet18_parametrized_relu] Epoch: 115 Train Loss: 0.0217 Train Acc: 0.9942 Eval Loss: 0.3581 Eval Acc: 0.9121 (LR: 0.000010)
[2025-05-14 09:52:44,274]: Early stopping was triggered!
[2025-05-14 09:52:44,274]: [ResNet18_parametrized_relu] Best Eval Accuracy: 0.9129
[2025-05-14 09:52:44,361]: 
Training of full-precision model finished!
[2025-05-14 09:52:44,361]: Model Architecture:
[2025-05-14 09:52:44,363]: ResNet(
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (skip_add): FloatFunctional(
        (activation_post_process): Identity()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 09:52:44,364]: 
Model Weights:
[2025-05-14 09:52:44,364]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 09:52:44,364]: Sample Values (25 elements): [0.17854498326778412, 0.1149602010846138, -0.08944277465343475, 0.17573758959770203, -0.08125904202461243, 0.1710757166147232, 0.143569678068161, 0.05318353325128555, -0.022315850481390953, -0.011690451763570309, -0.18533770740032196, 0.13204386830329895, -0.06506317853927612, -0.08445368707180023, -0.15644071996212006, 0.11152077466249466, 0.24792926013469696, -0.10086903721094131, 0.13904231786727905, -0.025609802454710007, 0.01829681545495987, 0.057576991617679596, 0.1532820165157318, -0.1341361701488495, -0.03394787386059761]
[2025-05-14 09:52:44,364]: Mean: -0.00122935
[2025-05-14 09:52:44,364]: Min: -0.33690867
[2025-05-14 09:52:44,365]: Max: 0.36398634
[2025-05-14 09:52:44,365]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 09:52:44,365]: Sample Values (25 elements): [0.9240699410438538, 0.9304209351539612, 0.8937780857086182, 0.9880529642105103, 0.9664176106452942, 0.9918131232261658, 0.9574995636940002, 0.9342489838600159, 1.0011483430862427, 1.052838683128357, 0.9348146915435791, 0.9788317084312439, 0.9309859275817871, 0.97278892993927, 0.9351142048835754, 0.9707730412483215, 0.9635955095291138, 0.9934905171394348, 0.9310545325279236, 0.9199499487876892, 0.9748605489730835, 0.977030873298645, 0.9669978618621826, 0.9634897708892822, 0.9534458518028259]
[2025-05-14 09:52:44,365]: Mean: 0.96124768
[2025-05-14 09:52:44,365]: Min: 0.87487388
[2025-05-14 09:52:44,365]: Max: 1.16256654
[2025-05-14 09:52:44,365]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 09:52:44,366]: Sample Values (25 elements): [-0.02661960758268833, -0.020430466160178185, -0.04553218558430672, -0.03071449138224125, 0.023504657670855522, -0.041099462658166885, -0.0017154995584860444, 0.04239848628640175, 0.03466702252626419, -0.004185034893453121, -0.02593177556991577, 0.02808965928852558, 0.0060494826175272465, 0.03502928093075752, 0.036137886345386505, 0.028360288590192795, 0.010569007135927677, -0.01543626468628645, -0.002473328495398164, -0.011365418322384357, 0.012842085212469101, 0.012916957028210163, -0.011739720590412617, -0.029523205012083054, 0.033365290611982346]
[2025-05-14 09:52:44,366]: Mean: -0.00127618
[2025-05-14 09:52:44,366]: Min: -0.13690820
[2025-05-14 09:52:44,366]: Max: 0.13945967
[2025-05-14 09:52:44,366]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 09:52:44,367]: Sample Values (25 elements): [0.9287022352218628, 0.9610307216644287, 0.9922852516174316, 0.984470784664154, 0.9706209897994995, 0.9501101970672607, 0.9905276298522949, 0.9771216511726379, 0.9778727889060974, 0.9843704700469971, 0.9819344282150269, 0.9615666270256042, 0.9421377182006836, 0.9451748132705688, 0.9480106234550476, 0.9902133345603943, 1.019097924232483, 0.9710080623626709, 1.006994366645813, 0.9463413953781128, 0.9721665382385254, 1.0282135009765625, 0.9901411533355713, 0.9566245675086975, 0.9723387360572815]
[2025-05-14 09:52:44,367]: Mean: 0.97163498
[2025-05-14 09:52:44,367]: Min: 0.91425842
[2025-05-14 09:52:44,367]: Max: 1.06161165
[2025-05-14 09:52:44,367]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 09:52:44,368]: Sample Values (25 elements): [0.0007911307620815933, -0.0038248382043093443, 0.030724024400115013, 0.012888774275779724, 0.005906387697905302, -0.025038156658411026, 0.01040183287113905, 0.034830059856176376, 0.0028090474661439657, -0.034561507403850555, 0.02967957593500614, -0.02843303605914116, 0.05954929441213608, 0.03966674581170082, 0.01255130860954523, -0.030587512999773026, -0.0005423256079666317, 0.03041776642203331, 0.016662565991282463, 0.0012230087304487824, 0.055846668779850006, -0.006625685375183821, 0.016234250739216805, -0.02335008978843689, 0.010115780867636204]
[2025-05-14 09:52:44,368]: Mean: -0.00084286
[2025-05-14 09:52:44,368]: Min: -0.10350651
[2025-05-14 09:52:44,368]: Max: 0.10928202
[2025-05-14 09:52:44,368]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 09:52:44,369]: Sample Values (25 elements): [0.9642367959022522, 0.9635395407676697, 0.9630946516990662, 0.9681679606437683, 0.958830714225769, 0.9665976762771606, 0.9942876100540161, 0.9561238884925842, 0.965633749961853, 0.9600529074668884, 0.9898104667663574, 0.9749928712844849, 0.976553201675415, 0.992720365524292, 0.9398109316825867, 0.9813938736915588, 0.9682695269584656, 0.9705886244773865, 0.9540874361991882, 0.9871488213539124, 0.9855550527572632, 0.9642723798751831, 0.9746627807617188, 0.9701831936836243, 0.9805450439453125]
[2025-05-14 09:52:44,369]: Mean: 0.97383595
[2025-05-14 09:52:44,369]: Min: 0.93132639
[2025-05-14 09:52:44,369]: Max: 1.05775774
[2025-05-14 09:52:44,369]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 09:52:44,370]: Sample Values (25 elements): [0.013366647996008396, -0.041250620037317276, 0.023566333577036858, -0.011108660139143467, 0.011339377611875534, 0.03288077563047409, -0.026644384488463402, -0.0407063364982605, 0.011693710461258888, 0.020216135308146477, -0.026775868609547615, -0.05415414273738861, 0.001025948440656066, 0.01808721385896206, -0.034249287098646164, 0.0052934070117771626, 0.020831335335969925, -0.058681026101112366, -0.013970774598419666, -0.000479440699564293, -0.000820718880277127, -0.010965765453875065, 0.04467884451150894, -0.014105991460382938, 0.023854656144976616]
[2025-05-14 09:52:44,370]: Mean: -0.00074818
[2025-05-14 09:52:44,370]: Min: -0.10455961
[2025-05-14 09:52:44,370]: Max: 0.10207030
[2025-05-14 09:52:44,370]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 09:52:44,370]: Sample Values (25 elements): [0.9684470891952515, 0.9652600884437561, 0.971276044845581, 0.9607985615730286, 0.9911137223243713, 0.9553993344306946, 0.9880020022392273, 0.9732427597045898, 0.9654101133346558, 0.9673181772232056, 0.975027859210968, 0.9717174768447876, 0.9861106872558594, 0.969707727432251, 0.970879852771759, 0.9737542271614075, 0.961180567741394, 0.9709795117378235, 0.9632186889648438, 0.9595062732696533, 0.9617445468902588, 0.9646517634391785, 0.9531679153442383, 1.0195518732070923, 0.9595960974693298]
[2025-05-14 09:52:44,371]: Mean: 0.97162807
[2025-05-14 09:52:44,371]: Min: 0.95316792
[2025-05-14 09:52:44,371]: Max: 1.01955187
[2025-05-14 09:52:44,371]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 09:52:44,371]: Sample Values (25 elements): [0.006013808306306601, -0.0014337102184072137, -0.0044861468486487865, -0.01789517141878605, -0.008181434124708176, 0.03120519407093525, 0.03614337369799614, 0.0036551773082464933, 0.0005736337043344975, 0.02260407991707325, -0.041875045746564865, 0.031030762940645218, -0.0066314395517110825, 0.022950923070311546, 0.022040478885173798, -0.02660210058093071, -0.008957426995038986, 0.08150769770145416, 0.0024511751253157854, 0.03845157101750374, -0.020514976233243942, -0.014458633959293365, -0.001978643238544464, 0.023294612765312195, 0.026453571394085884]
[2025-05-14 09:52:44,372]: Mean: -0.00023772
[2025-05-14 09:52:44,372]: Min: -0.07414324
[2025-05-14 09:52:44,372]: Max: 0.09233352
[2025-05-14 09:52:44,372]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 09:52:44,372]: Sample Values (25 elements): [0.9774696826934814, 0.971367359161377, 0.9964045286178589, 0.9667615294456482, 0.9932712316513062, 0.966444194316864, 0.9712730050086975, 0.9727082848548889, 0.9775971174240112, 0.9694305062294006, 0.9784687757492065, 0.9890424013137817, 0.9856358170509338, 0.97348952293396, 0.997875452041626, 0.9871710538864136, 0.9726995229721069, 0.981691837310791, 0.9900359511375427, 0.969714343547821, 0.9743741750717163, 0.9952967166900635, 0.9700593948364258, 1.0040770769119263, 0.9883342981338501]
[2025-05-14 09:52:44,372]: Mean: 0.98264861
[2025-05-14 09:52:44,373]: Min: 0.95532912
[2025-05-14 09:52:44,373]: Max: 1.01487207
[2025-05-14 09:52:44,373]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 09:52:44,374]: Sample Values (25 elements): [-0.042264364659786224, -0.01894453726708889, -0.03925705328583717, 0.008692285977303982, 0.018856555223464966, 0.01738586090505123, 0.04613222926855087, 0.046181172132492065, 0.03244907408952713, 0.027522647753357887, 0.03552819415926933, 0.030922546982765198, 0.041259169578552246, -0.03586854785680771, -0.0277458056807518, 0.040343254804611206, -0.01295501459389925, -0.04124480113387108, -0.04283513128757477, 0.02206576243042946, -0.025456201285123825, -0.020822294056415558, -0.013483726419508457, -0.024235423654317856, 0.031805798411369324]
[2025-05-14 09:52:44,374]: Mean: -0.00076646
[2025-05-14 09:52:44,374]: Min: -0.07109388
[2025-05-14 09:52:44,374]: Max: 0.07325343
[2025-05-14 09:52:44,374]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 09:52:44,374]: Sample Values (25 elements): [0.9595518708229065, 0.9666619896888733, 0.9660932421684265, 0.9720394015312195, 0.9692468047142029, 0.9743391871452332, 0.9669537544250488, 0.9771894812583923, 0.9689323902130127, 0.9655830264091492, 0.9748956561088562, 0.9799461364746094, 0.9747526049613953, 0.9819386005401611, 0.967481255531311, 0.9703555107116699, 0.9662761688232422, 0.9744163155555725, 0.9679639935493469, 0.9671807289123535, 0.9679723381996155, 0.978574812412262, 0.9799138903617859, 0.9770695567131042, 0.9672194719314575]
[2025-05-14 09:52:44,375]: Mean: 0.97176248
[2025-05-14 09:52:44,375]: Min: 0.95669717
[2025-05-14 09:52:44,375]: Max: 0.99751186
[2025-05-14 09:52:44,375]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 09:52:44,376]: Sample Values (25 elements): [0.012551810592412949, 0.013648025691509247, 0.005185388959944248, -0.024380844086408615, 0.018646741285920143, -0.01048336923122406, 0.013563125394284725, -0.0198507159948349, -0.012481718324124813, -0.0016345306066796184, -0.008939003571867943, -0.004141323268413544, 0.009542777203023434, 0.02178172953426838, 0.014770952053368092, -0.005184628069400787, -0.012392215430736542, -0.020291268825531006, -0.016128582879900932, -0.015367775224149227, -0.013385317288339138, -0.013849095441401005, -0.008922562003135681, -0.016043562442064285, 0.0090272082015872]
[2025-05-14 09:52:44,377]: Mean: -0.00056692
[2025-05-14 09:52:44,377]: Min: -0.06098194
[2025-05-14 09:52:44,377]: Max: 0.06523149
[2025-05-14 09:52:44,377]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 09:52:44,377]: Sample Values (25 elements): [0.9828146696090698, 0.9659415483474731, 0.977009654045105, 0.9734598994255066, 0.9670111536979675, 0.9740679264068604, 0.9776553511619568, 0.9693599939346313, 0.9757074117660522, 0.9550357460975647, 0.9850170016288757, 0.9741817116737366, 0.970911979675293, 0.9827843904495239, 0.9763918519020081, 0.9657118916511536, 0.9717206358909607, 0.9675182700157166, 0.9721278548240662, 0.9917299747467041, 0.9606185555458069, 0.9709154963493347, 0.9660294651985168, 0.9663624167442322, 0.9620307683944702]
[2025-05-14 09:52:44,377]: Mean: 0.97245437
[2025-05-14 09:52:44,378]: Min: 0.95285660
[2025-05-14 09:52:44,378]: Max: 0.99915522
[2025-05-14 09:52:44,378]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 09:52:44,378]: Sample Values (25 elements): [0.11407415568828583, 0.0010229327017441392, -0.07884249091148376, 0.1034875139594078, -0.10776926577091217, -0.12454768270254135, 0.0219603031873703, -0.10822156816720963, -0.0516645684838295, -0.05366041883826256, 0.06363538652658463, 0.05339295417070389, 0.01947634667158127, 0.026175808161497116, -0.051103074103593826, -0.06592681258916855, 0.05260855332016945, -0.04446085914969444, 0.11475714296102524, -0.06300923228263855, -0.03555798530578613, 0.13121572136878967, 0.10633903741836548, -0.032537009567022324, -0.05041754990816116]
[2025-05-14 09:52:44,378]: Mean: -0.00098113
[2025-05-14 09:52:44,378]: Min: -0.15850371
[2025-05-14 09:52:44,379]: Max: 0.14197992
[2025-05-14 09:52:44,379]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 09:52:44,379]: Sample Values (25 elements): [0.9636048078536987, 0.954108715057373, 0.9585214853286743, 0.9601404070854187, 0.9551334381103516, 0.9493294358253479, 0.9516459703445435, 0.9630787968635559, 0.9493343234062195, 0.9631277918815613, 0.9639954566955566, 0.9475493431091309, 0.9543806314468384, 0.9602600336074829, 0.9619355797767639, 0.9491607546806335, 0.9495964646339417, 0.955748975276947, 0.9690579175949097, 0.9604740738868713, 0.9552993178367615, 0.9438345432281494, 0.9580748081207275, 0.9651872515678406, 0.9541040062904358]
[2025-05-14 09:52:44,379]: Mean: 0.95506024
[2025-05-14 09:52:44,379]: Min: 0.94051772
[2025-05-14 09:52:44,379]: Max: 0.96905792
[2025-05-14 09:52:44,379]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 09:52:44,381]: Sample Values (25 elements): [0.025814225897192955, 0.011319020763039589, -0.022194691002368927, 0.014430091716349125, 0.004987479653209448, -0.014359443448483944, -0.027974732220172882, 0.014103041961789131, -0.02243655174970627, -0.026705676689743996, -0.02871757186949253, 0.007672427222132683, 0.01230302918702364, 0.008112655021250248, -0.010903503745794296, 0.0001931216975208372, -0.00024067118647508323, -0.03704456612467766, 0.015171765349805355, -0.01722545363008976, -0.02657562680542469, -0.0385262593626976, -0.02635243721306324, 0.02772492542862892, -0.03560063987970352]
[2025-05-14 09:52:44,381]: Mean: -0.00050211
[2025-05-14 09:52:44,381]: Min: -0.05839116
[2025-05-14 09:52:44,381]: Max: 0.06698487
[2025-05-14 09:52:44,381]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 09:52:44,381]: Sample Values (25 elements): [0.9673294425010681, 0.9797459244728088, 0.9782149195671082, 0.9827463626861572, 0.9753642678260803, 0.9675898551940918, 0.9639999866485596, 0.967952311038971, 0.965518057346344, 0.9740368127822876, 0.9779614210128784, 0.9727644324302673, 0.9647330641746521, 0.9903053641319275, 0.9834735989570618, 0.9771628975868225, 0.9706308245658875, 0.9771731495857239, 0.9711867570877075, 0.9711730480194092, 0.9667214751243591, 0.973207950592041, 0.9702886343002319, 0.9765616059303284, 0.9754731059074402]
[2025-05-14 09:52:44,382]: Mean: 0.97173691
[2025-05-14 09:52:44,382]: Min: 0.95583433
[2025-05-14 09:52:44,382]: Max: 0.99030536
[2025-05-14 09:52:44,382]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 09:52:44,383]: Sample Values (25 elements): [-0.003889684332534671, -0.009286792017519474, 0.026989568024873734, 0.00521728815510869, -0.013556438498198986, -0.007541269529610872, -0.03214733302593231, -0.00913732685148716, -0.016075650230050087, 0.007339398842304945, -0.027425820007920265, -0.017141088843345642, -0.029474616050720215, 0.02391478419303894, 0.022201357409358025, -0.007362173404544592, 0.018507927656173706, 0.004179925657808781, 0.03327496722340584, 0.029193252325057983, 0.01579919271171093, -0.01629500649869442, -0.0022283517755568027, -0.0045320685021579266, -0.0033332807943224907]
[2025-05-14 09:52:44,384]: Mean: -0.00006061
[2025-05-14 09:52:44,384]: Min: -0.05640334
[2025-05-14 09:52:44,384]: Max: 0.06335764
[2025-05-14 09:52:44,384]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 09:52:44,384]: Sample Values (25 elements): [0.99003005027771, 0.9802781939506531, 0.9763771295547485, 0.9841673374176025, 0.9844622015953064, 0.9880002737045288, 0.9827287793159485, 0.9907488822937012, 0.9849697947502136, 0.986077606678009, 0.9873161911964417, 0.9832900762557983, 0.98304682970047, 0.9730041027069092, 0.9818205237388611, 0.9832570552825928, 0.9839290976524353, 0.9931917786598206, 0.9837889671325684, 0.992014467716217, 0.9923220276832581, 0.9822806119918823, 0.9916772246360779, 0.9693995118141174, 0.994946300983429]
[2025-05-14 09:52:44,384]: Mean: 0.98638952
[2025-05-14 09:52:44,385]: Min: 0.96913230
[2025-05-14 09:52:44,385]: Max: 1.01035547
[2025-05-14 09:52:44,385]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 09:52:44,388]: Sample Values (25 elements): [-0.009116859175264835, -0.02476467937231064, 0.026366854086518288, 0.004618067294359207, -0.017300110310316086, 0.03320295736193657, -0.030893873423337936, 0.0034027015790343285, 0.011323681101202965, 0.025361306965351105, 0.03340597823262215, -0.018492965027689934, 0.01819595880806446, -0.00041539256926625967, -0.010571375489234924, -0.014692103490233421, 0.02190306968986988, -0.021898411214351654, 0.030896179378032684, -0.009088676422834396, 0.0017408998683094978, -0.02318338118493557, 0.005076957400888205, -0.02561269700527191, -0.029210343956947327]
[2025-05-14 09:52:44,388]: Mean: -0.00014014
[2025-05-14 09:52:44,388]: Min: -0.04952309
[2025-05-14 09:52:44,388]: Max: 0.05180683
[2025-05-14 09:52:44,388]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 09:52:44,388]: Sample Values (25 elements): [0.979121208190918, 0.9781546592712402, 0.9668247103691101, 0.9714080691337585, 0.970562219619751, 0.9745948910713196, 0.9743050336837769, 0.9732685089111328, 0.9739513993263245, 0.9698511362075806, 0.9729878902435303, 0.9720317125320435, 0.9777487516403198, 0.9750190377235413, 0.9715584516525269, 0.9696337580680847, 0.9692630767822266, 0.9698879718780518, 0.9693140387535095, 0.9760152101516724, 0.971667468547821, 0.9751736521720886, 0.972484290599823, 0.9700102210044861, 0.9782030582427979]
[2025-05-14 09:52:44,389]: Mean: 0.97178662
[2025-05-14 09:52:44,389]: Min: 0.96122676
[2025-05-14 09:52:44,389]: Max: 0.98533535
[2025-05-14 09:52:44,389]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 09:52:44,395]: Sample Values (25 elements): [-0.0007000155164860189, -0.011181351728737354, 0.011091390624642372, -0.005296923220157623, 0.01909375563263893, -0.006405980326235294, 0.009971322491765022, 0.0005589078064076602, -0.01476719044148922, 0.009614965878427029, 0.0007994445040822029, -0.007633514702320099, -0.009392364881932735, -0.014163533225655556, 0.004093427676707506, -0.012581764720380306, -0.009381450712680817, -0.01788385957479477, 0.01441566739231348, 0.005852528847754002, 0.0006186730461195111, 0.0171725545078516, 0.0021521225571632385, -0.007041834760457277, -0.001289470586925745]
[2025-05-14 09:52:44,395]: Mean: -0.00037645
[2025-05-14 09:52:44,395]: Min: -0.04230614
[2025-05-14 09:52:44,395]: Max: 0.04240708
[2025-05-14 09:52:44,395]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 09:52:44,395]: Sample Values (25 elements): [0.9752485752105713, 0.9730805158615112, 0.9766809344291687, 0.9764261841773987, 0.9809067249298096, 0.9793166518211365, 0.9705075621604919, 0.9671881198883057, 0.9766147136688232, 0.9850206971168518, 0.9735369086265564, 0.9700941443443298, 0.9772534966468811, 0.9770224094390869, 0.9858845472335815, 0.967872142791748, 0.9798136949539185, 0.9746235013008118, 0.9730415344238281, 0.9783673882484436, 0.9729387760162354, 0.9775654077529907, 0.9770698547363281, 0.9739516377449036, 0.970392644405365]
[2025-05-14 09:52:44,396]: Mean: 0.97601390
[2025-05-14 09:52:44,396]: Min: 0.96254253
[2025-05-14 09:52:44,396]: Max: 1.00201368
[2025-05-14 09:52:44,396]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 09:52:44,396]: Sample Values (25 elements): [0.01799025759100914, 0.06899159401655197, 0.08781766146421432, -0.08313773572444916, 0.07512485235929489, -0.033241111785173416, -0.017358383163809776, -0.00930637214332819, -0.016825029626488686, 0.024579249322414398, -0.059178393334150314, 0.07076440006494522, 0.024442771449685097, 0.08985297381877899, -0.013150339014828205, 0.0003796759119722992, 0.06590627878904343, 0.013536756858229637, -0.010392533615231514, 0.022992707788944244, -0.025735430419445038, -0.03998871520161629, -0.02720319852232933, -0.07393162697553635, -0.08745743334293365]
[2025-05-14 09:52:44,397]: Mean: -0.00002868
[2025-05-14 09:52:44,397]: Min: -0.10407662
[2025-05-14 09:52:44,397]: Max: 0.10147195
[2025-05-14 09:52:44,397]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 09:52:44,397]: Sample Values (25 elements): [0.9690760970115662, 0.9644436240196228, 0.9568507671356201, 0.9573361277580261, 0.9628608226776123, 0.9525678157806396, 0.9525110721588135, 0.9578275084495544, 0.9577360153198242, 0.9572809338569641, 0.9557991623878479, 0.955639123916626, 0.9620522856712341, 0.9615542888641357, 0.9606570601463318, 0.9557434320449829, 0.9549914598464966, 0.9515147805213928, 0.9602978229522705, 0.9523108005523682, 0.9579455852508545, 0.9589402675628662, 0.958334743976593, 0.9566683769226074, 0.9604849815368652]
[2025-05-14 09:52:44,397]: Mean: 0.95803022
[2025-05-14 09:52:44,398]: Min: 0.94805878
[2025-05-14 09:52:44,398]: Max: 0.96907610
[2025-05-14 09:52:44,398]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 09:52:44,404]: Sample Values (25 elements): [0.01736612059175968, 0.004240098875015974, 0.0002372793824179098, -0.01625848188996315, 0.013913831673562527, 0.016339166089892387, 0.005874060560017824, -0.011594136245548725, -0.0042790742591023445, 0.017578255385160446, -0.012289058417081833, 0.004061840008944273, -0.003148492658510804, -0.008036205545067787, -0.003774144221097231, -0.008781827986240387, 0.013904296793043613, -0.016550425440073013, -0.0020424071699380875, 0.0105460649356246, -0.011392162181437016, -0.002405419247224927, 0.009526869282126427, -0.020890528336167336, -0.01410730741918087]
[2025-05-14 09:52:44,404]: Mean: -0.00040481
[2025-05-14 09:52:44,404]: Min: -0.04380014
[2025-05-14 09:52:44,404]: Max: 0.04373729
[2025-05-14 09:52:44,404]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 09:52:44,405]: Sample Values (25 elements): [0.9696321487426758, 0.9710946083068848, 0.9696849584579468, 0.9707942008972168, 0.964510977268219, 0.9710493087768555, 0.9747887849807739, 0.9745986461639404, 0.9754903316497803, 0.9721558094024658, 0.9780020117759705, 0.9754348993301392, 0.9768382906913757, 0.9727221131324768, 0.9735742807388306, 0.9691340327262878, 0.9696996212005615, 0.9746321439743042, 0.9680132865905762, 0.9731807708740234, 0.9754515290260315, 0.9742444753646851, 0.9713501334190369, 0.9771866202354431, 0.9724599123001099]
[2025-05-14 09:52:44,405]: Mean: 0.97175026
[2025-05-14 09:52:44,405]: Min: 0.95880103
[2025-05-14 09:52:44,405]: Max: 0.98353368
[2025-05-14 09:52:44,405]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 09:52:44,411]: Sample Values (25 elements): [0.0026187091134488583, -0.009027753956615925, 0.0026970210019499063, 0.012076283805072308, -0.01353957038372755, -0.010422511957585812, 0.014857922680675983, 0.006661177147179842, 0.010863996110856533, -0.0011634452966973186, 0.002120938850566745, -0.0210390854626894, 0.005691219586879015, 0.006167169194668531, -0.025245293974876404, 0.014773241244256496, 0.020295733585953712, 0.018552906811237335, 0.009406419470906258, 0.015927709639072418, -0.01792181096971035, 0.004810298793017864, -0.01929021067917347, -0.015800757333636284, 0.005980296526104212]
[2025-05-14 09:52:44,411]: Mean: -0.00010600
[2025-05-14 09:52:44,411]: Min: -0.03579534
[2025-05-14 09:52:44,411]: Max: 0.03730693
[2025-05-14 09:52:44,412]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 09:52:44,412]: Sample Values (25 elements): [0.9793477654457092, 0.9775719046592712, 0.9876289367675781, 0.981236457824707, 0.9881551265716553, 0.975033700466156, 0.9832578301429749, 0.992941677570343, 0.9835737347602844, 0.9774591326713562, 0.9764333963394165, 0.9843207597732544, 0.9834312796592712, 0.97901451587677, 0.9836781620979309, 0.9827926158905029, 0.9748175740242004, 0.9825320243835449, 0.9871310591697693, 0.9818772077560425, 0.9789926409721375, 0.985577404499054, 0.9871305823326111, 0.9808443188667297, 0.9789488911628723]
[2025-05-14 09:52:44,412]: Mean: 0.98082709
[2025-05-14 09:52:44,412]: Min: 0.97127211
[2025-05-14 09:52:44,412]: Max: 0.99563241
[2025-05-14 09:52:44,412]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 09:52:44,428]: Sample Values (25 elements): [0.0011345917591825128, -0.01975051499903202, 0.004649452865123749, -0.011473902501165867, -0.02217419445514679, 0.0072722057811915874, -0.004177419003099203, 0.006596849765628576, 0.0035651461221277714, 0.0051175435073673725, 0.018322717398405075, 0.005499334540218115, 0.008714298717677593, 0.014555085450410843, 0.013064795173704624, 0.0032089201267808676, -0.010632244870066643, -0.0074621811509132385, -0.005731845740228891, -0.0230871569365263, 0.002741584787145257, -0.016710449010133743, -0.01852412521839142, -0.001719912514090538, -0.01766114868223667]
[2025-05-14 09:52:44,428]: Mean: -0.00000998
[2025-05-14 09:52:44,428]: Min: -0.03255878
[2025-05-14 09:52:44,428]: Max: 0.03661490
[2025-05-14 09:52:44,428]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 09:52:44,429]: Sample Values (25 elements): [0.9764671921730042, 0.9726169109344482, 0.9704902768135071, 0.9710207581520081, 0.9720112085342407, 0.9703817367553711, 0.9714257121086121, 0.972810685634613, 0.9765655994415283, 0.9741491079330444, 0.970580518245697, 0.9787415862083435, 0.9706062078475952, 0.9732027649879456, 0.9709044694900513, 0.9683874249458313, 0.973217785358429, 0.9743592739105225, 0.9706133604049683, 0.9721003770828247, 0.9710848331451416, 0.9715569615364075, 0.9711735844612122, 0.9695131778717041, 0.9704219102859497]
[2025-05-14 09:52:44,429]: Mean: 0.97184551
[2025-05-14 09:52:44,429]: Min: 0.96770668
[2025-05-14 09:52:44,429]: Max: 0.97874159
[2025-05-14 09:52:44,429]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 09:52:44,470]: Sample Values (25 elements): [-0.015280078165233135, 6.825660966569558e-05, 0.0054673790000379086, 0.0010348843643441796, 0.004534516483545303, -0.01452314481139183, -0.010398576967418194, -0.014123078435659409, 0.0011941404081881046, -0.01138904970139265, 0.0065533434972167015, 0.009116988629102707, 0.0030924049206078053, -0.002331583760678768, 0.0003951449180021882, -5.038708695792593e-05, 0.006264924071729183, -0.011788707226514816, 0.003489532507956028, -0.006626078858971596, -0.0013406712096184492, 0.005055212881416082, -0.0021361480467021465, -0.004113081376999617, 0.01233998779207468]
[2025-05-14 09:52:44,471]: Mean: -0.00002687
[2025-05-14 09:52:44,471]: Min: -0.02487839
[2025-05-14 09:52:44,471]: Max: 0.02585752
[2025-05-14 09:52:44,471]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 09:52:44,471]: Sample Values (25 elements): [0.9752436876296997, 0.9731547236442566, 0.9737382531166077, 0.9732887148857117, 0.9752940535545349, 0.9751226305961609, 0.9755795001983643, 0.9772936105728149, 0.9781209230422974, 0.97463059425354, 0.9745764136314392, 0.9731507897377014, 0.9770715236663818, 0.9736729264259338, 0.9774144887924194, 0.9729562997817993, 0.9763100147247314, 0.9746381640434265, 0.9720861315727234, 0.9734609723091125, 0.979712188243866, 0.9743776321411133, 0.9722412824630737, 0.9756771922111511, 0.9745809435844421]
[2025-05-14 09:52:44,472]: Mean: 0.97583318
[2025-05-14 09:52:44,472]: Min: 0.97104937
[2025-05-14 09:52:44,472]: Max: 0.98655903
[2025-05-14 09:52:44,472]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 09:52:44,473]: Sample Values (25 elements): [0.00959003809839487, 0.03502890467643738, -0.013432643376290798, -0.042608149349689484, -0.021115854382514954, -0.03693865239620209, 0.022637078538537025, 0.0539851188659668, -0.024526013061404228, -0.015286659821867943, 0.030007977038621902, -0.0219831895083189, -0.002911405172199011, 0.04289902001619339, -0.05031644180417061, 0.03480544313788414, -0.02042643539607525, -0.05729150027036667, -0.008369049057364464, 0.03163051977753639, -0.017255274578928947, 0.03171919658780098, -0.010318673215806484, -0.03561563789844513, 0.03288168087601662]
[2025-05-14 09:52:44,473]: Mean: 0.00000640
[2025-05-14 09:52:44,473]: Min: -0.06938125
[2025-05-14 09:52:44,474]: Max: 0.07082485
[2025-05-14 09:52:44,474]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 09:52:44,474]: Sample Values (25 elements): [0.9703554511070251, 0.9696661233901978, 0.9699673056602478, 0.9701409339904785, 0.969817578792572, 0.9680909514427185, 0.9699990749359131, 0.970176637172699, 0.9707397818565369, 0.9705770611763, 0.9721558094024658, 0.9689413905143738, 0.9701961874961853, 0.9699532389640808, 0.9696136713027954, 0.9697727560997009, 0.9701281785964966, 0.9671574234962463, 0.9682299494743347, 0.9714075922966003, 0.9674021005630493, 0.9694480299949646, 0.9688374400138855, 0.9685169458389282, 0.9695684313774109]
[2025-05-14 09:52:44,474]: Mean: 0.96987158
[2025-05-14 09:52:44,474]: Min: 0.96566868
[2025-05-14 09:52:44,474]: Max: 0.97464657
[2025-05-14 09:52:44,474]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 09:52:44,511]: Sample Values (25 elements): [-0.0036268020048737526, -0.010347132571041584, -0.009252605959773064, -0.011026859283447266, -0.012878170236945152, -0.0028128253761678934, 0.0060038077645003796, -0.012951597571372986, 0.008162151090800762, 0.01468412484973669, 0.0020418940111994743, -0.008403480052947998, -0.012359550222754478, 0.013943804427981377, -0.009393470361828804, 0.011336463503539562, -0.013377764262259007, 0.014119571074843407, 0.004343355540186167, -0.012088063172996044, -0.00037852281820960343, 0.008441313169896603, 0.0024586995132267475, -0.006561580114066601, 0.012974768877029419]
[2025-05-14 09:52:44,511]: Mean: -0.00009838
[2025-05-14 09:52:44,511]: Min: -0.02412017
[2025-05-14 09:52:44,512]: Max: 0.02435883
[2025-05-14 09:52:44,512]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 09:52:44,512]: Sample Values (25 elements): [0.9715198874473572, 0.9707690477371216, 0.9744129180908203, 0.9770523309707642, 0.9733439087867737, 0.9724175333976746, 0.9697840213775635, 0.9750326871871948, 0.9739674925804138, 0.975425660610199, 0.9705072045326233, 0.976172685623169, 0.9724122285842896, 0.9710913896560669, 0.9738088846206665, 0.972352921962738, 0.9710623025894165, 0.9698362946510315, 0.9708185195922852, 0.9717389941215515, 0.9705222249031067, 0.9710856676101685, 0.9706575870513916, 0.9752184748649597, 0.9761148691177368]
[2025-05-14 09:52:44,512]: Mean: 0.97186273
[2025-05-14 09:52:44,512]: Min: 0.96785063
[2025-05-14 09:52:44,512]: Max: 0.98202592
[2025-05-14 09:52:44,512]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 09:52:44,550]: Sample Values (25 elements): [0.003394771832972765, 0.011758541688323021, -0.013262908905744553, 0.00810107309371233, -0.0011226364877074957, 0.006740421988070011, 0.005496862344443798, -0.008224424906075, 0.003949559293687344, 0.010649229399859905, 0.01474149338901043, -0.013122132048010826, 0.009743940085172653, 0.008827941492199898, 0.006898978259414434, -0.0029335415456444025, 0.01022195816040039, -0.002925921930000186, 0.007822397165000439, 0.0035377617459744215, 0.005964679643511772, -0.010610351338982582, -0.00734960101544857, 0.013042527250945568, -0.011138688772916794]
[2025-05-14 09:52:44,551]: Mean: 0.00002295
[2025-05-14 09:52:44,551]: Min: -0.02053275
[2025-05-14 09:52:44,551]: Max: 0.02118530
[2025-05-14 09:52:44,551]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 09:52:44,551]: Sample Values (25 elements): [0.9799631834030151, 0.9819091558456421, 0.9776511788368225, 0.9847596883773804, 0.9847680330276489, 0.980915367603302, 0.9825940132141113, 0.979942262172699, 0.9828106164932251, 0.9829126000404358, 0.9782959222793579, 0.9778592586517334, 0.9775184988975525, 0.9821823239326477, 0.9806503057479858, 0.9853175282478333, 0.9808369278907776, 0.9786888360977173, 0.9846160411834717, 0.9776772260665894, 0.9816514849662781, 0.9801711440086365, 0.9800589680671692, 0.9776950478553772, 0.978056013584137]
[2025-05-14 09:52:44,552]: Mean: 0.97991371
[2025-05-14 09:52:44,552]: Min: 0.97470844
[2025-05-14 09:52:44,552]: Max: 0.98634875
[2025-05-14 09:52:44,552]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 09:52:44,552]: Sample Values (25 elements): [-0.048831209540367126, 0.0005430916789919138, 0.07489822059869766, 0.09908906370401382, -0.00768737168982625, -0.07452971488237381, -0.059642575681209564, 0.10461953282356262, -0.06134564056992531, -0.05407043546438217, 0.042277559638023376, -0.08007032424211502, 0.04407663270831108, -0.0852116122841835, -0.05946950986981392, 0.058463018387556076, 0.0814136490225792, -0.06364943087100983, 0.06004637852311134, 0.07488950341939926, 0.03282026946544647, 0.03493891656398773, -0.05162319913506508, -0.05993764102458954, -0.016045885160565376]
[2025-05-14 09:52:44,553]: Mean: -0.00054735
[2025-05-14 09:52:44,553]: Min: -0.12079771
[2025-05-14 09:52:44,553]: Max: 0.14219043
[2025-05-14 09:52:44,553]: 


QAT of ResNet18 with parametrized_relu down to 4 bits...
[2025-05-14 09:52:44,785]: [ResNet18_parametrized_relu_quantized_4_bits] after configure_qat:
[2025-05-14 09:52:44,831]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 09:54:50,838]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 001 Train Loss: 0.0949 Train Acc: 0.9663 Eval Loss: 0.4979 Eval Acc: 0.8767 (LR: 0.001000)
[2025-05-14 09:56:56,921]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 002 Train Loss: 0.1029 Train Acc: 0.9629 Eval Loss: 0.4761 Eval Acc: 0.8855 (LR: 0.001000)
[2025-05-14 09:59:02,778]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 003 Train Loss: 0.1001 Train Acc: 0.9645 Eval Loss: 0.4213 Eval Acc: 0.8950 (LR: 0.001000)
[2025-05-14 10:01:08,496]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 004 Train Loss: 0.0951 Train Acc: 0.9657 Eval Loss: 0.4383 Eval Acc: 0.8925 (LR: 0.001000)
[2025-05-14 10:03:14,183]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 005 Train Loss: 0.0927 Train Acc: 0.9671 Eval Loss: 0.4253 Eval Acc: 0.8938 (LR: 0.001000)
[2025-05-14 10:05:19,862]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 006 Train Loss: 0.0892 Train Acc: 0.9689 Eval Loss: 0.4455 Eval Acc: 0.8895 (LR: 0.001000)
[2025-05-14 10:07:25,544]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 007 Train Loss: 0.0903 Train Acc: 0.9681 Eval Loss: 0.4560 Eval Acc: 0.8904 (LR: 0.001000)
[2025-05-14 10:09:31,211]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 008 Train Loss: 0.0880 Train Acc: 0.9681 Eval Loss: 0.4142 Eval Acc: 0.8982 (LR: 0.001000)
[2025-05-14 10:11:36,885]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 009 Train Loss: 0.0812 Train Acc: 0.9711 Eval Loss: 0.4163 Eval Acc: 0.8949 (LR: 0.001000)
[2025-05-14 10:13:42,399]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 010 Train Loss: 0.0804 Train Acc: 0.9716 Eval Loss: 0.4216 Eval Acc: 0.8970 (LR: 0.001000)
[2025-05-14 10:15:48,277]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 011 Train Loss: 0.0809 Train Acc: 0.9711 Eval Loss: 0.4104 Eval Acc: 0.8975 (LR: 0.001000)
[2025-05-14 10:17:53,993]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 012 Train Loss: 0.0753 Train Acc: 0.9723 Eval Loss: 0.4047 Eval Acc: 0.9001 (LR: 0.001000)
[2025-05-14 10:19:59,910]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 013 Train Loss: 0.0805 Train Acc: 0.9713 Eval Loss: 0.4474 Eval Acc: 0.8918 (LR: 0.001000)
[2025-05-14 10:22:05,728]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 014 Train Loss: 0.0730 Train Acc: 0.9737 Eval Loss: 0.4196 Eval Acc: 0.8957 (LR: 0.001000)
[2025-05-14 10:24:11,619]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 015 Train Loss: 0.0761 Train Acc: 0.9732 Eval Loss: 0.4561 Eval Acc: 0.8921 (LR: 0.001000)
[2025-05-14 10:26:17,515]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 016 Train Loss: 0.0731 Train Acc: 0.9741 Eval Loss: 0.4412 Eval Acc: 0.8972 (LR: 0.001000)
[2025-05-14 10:28:23,406]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 017 Train Loss: 0.0703 Train Acc: 0.9747 Eval Loss: 0.4865 Eval Acc: 0.8883 (LR: 0.001000)
[2025-05-14 10:30:29,340]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 018 Train Loss: 0.0653 Train Acc: 0.9773 Eval Loss: 0.4548 Eval Acc: 0.8956 (LR: 0.001000)
[2025-05-14 10:32:35,195]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 019 Train Loss: 0.0700 Train Acc: 0.9754 Eval Loss: 0.4498 Eval Acc: 0.8933 (LR: 0.001000)
[2025-05-14 10:34:41,691]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 020 Train Loss: 0.0675 Train Acc: 0.9766 Eval Loss: 0.4495 Eval Acc: 0.8964 (LR: 0.001000)
[2025-05-14 10:36:47,577]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 021 Train Loss: 0.0664 Train Acc: 0.9766 Eval Loss: 0.4131 Eval Acc: 0.9019 (LR: 0.001000)
[2025-05-14 10:38:53,677]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 022 Train Loss: 0.0648 Train Acc: 0.9772 Eval Loss: 0.4220 Eval Acc: 0.8980 (LR: 0.001000)
[2025-05-14 10:40:59,389]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 023 Train Loss: 0.0643 Train Acc: 0.9773 Eval Loss: 0.4374 Eval Acc: 0.8960 (LR: 0.001000)
[2025-05-14 10:43:05,126]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 024 Train Loss: 0.0590 Train Acc: 0.9794 Eval Loss: 0.4719 Eval Acc: 0.8933 (LR: 0.001000)
[2025-05-14 10:45:10,765]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 025 Train Loss: 0.0590 Train Acc: 0.9795 Eval Loss: 0.4272 Eval Acc: 0.9020 (LR: 0.001000)
[2025-05-14 10:47:16,272]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 026 Train Loss: 0.0594 Train Acc: 0.9792 Eval Loss: 0.4242 Eval Acc: 0.9002 (LR: 0.001000)
[2025-05-14 10:49:21,944]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 027 Train Loss: 0.0627 Train Acc: 0.9780 Eval Loss: 0.4265 Eval Acc: 0.9009 (LR: 0.001000)
[2025-05-14 10:51:27,654]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 028 Train Loss: 0.0606 Train Acc: 0.9784 Eval Loss: 0.4394 Eval Acc: 0.8987 (LR: 0.001000)
[2025-05-14 10:53:33,344]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 029 Train Loss: 0.0553 Train Acc: 0.9799 Eval Loss: 0.4185 Eval Acc: 0.9018 (LR: 0.001000)
[2025-05-14 10:55:39,229]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 030 Train Loss: 0.0560 Train Acc: 0.9804 Eval Loss: 0.4132 Eval Acc: 0.9044 (LR: 0.000250)
[2025-05-14 10:57:45,320]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 031 Train Loss: 0.0350 Train Acc: 0.9885 Eval Loss: 0.3767 Eval Acc: 0.9100 (LR: 0.000250)
[2025-05-14 10:59:51,408]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 032 Train Loss: 0.0282 Train Acc: 0.9908 Eval Loss: 0.3753 Eval Acc: 0.9122 (LR: 0.000250)
[2025-05-14 11:01:57,499]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 033 Train Loss: 0.0256 Train Acc: 0.9918 Eval Loss: 0.3821 Eval Acc: 0.9121 (LR: 0.000250)
[2025-05-14 11:04:03,391]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 034 Train Loss: 0.0241 Train Acc: 0.9927 Eval Loss: 0.3872 Eval Acc: 0.9115 (LR: 0.000250)
[2025-05-14 11:06:09,519]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 035 Train Loss: 0.0229 Train Acc: 0.9930 Eval Loss: 0.3813 Eval Acc: 0.9115 (LR: 0.000250)
[2025-05-14 11:08:15,348]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 036 Train Loss: 0.0221 Train Acc: 0.9931 Eval Loss: 0.3915 Eval Acc: 0.9125 (LR: 0.000250)
[2025-05-14 11:10:21,270]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 037 Train Loss: 0.0228 Train Acc: 0.9928 Eval Loss: 0.3947 Eval Acc: 0.9102 (LR: 0.000250)
[2025-05-14 11:12:26,955]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 038 Train Loss: 0.0214 Train Acc: 0.9934 Eval Loss: 0.3863 Eval Acc: 0.9127 (LR: 0.000250)
[2025-05-14 11:14:32,623]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 039 Train Loss: 0.0209 Train Acc: 0.9936 Eval Loss: 0.3831 Eval Acc: 0.9124 (LR: 0.000250)
[2025-05-14 11:16:39,472]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 040 Train Loss: 0.0207 Train Acc: 0.9938 Eval Loss: 0.3885 Eval Acc: 0.9109 (LR: 0.000250)
[2025-05-14 11:18:45,248]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 041 Train Loss: 0.0197 Train Acc: 0.9940 Eval Loss: 0.3799 Eval Acc: 0.9126 (LR: 0.000250)
[2025-05-14 11:20:51,145]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 042 Train Loss: 0.0192 Train Acc: 0.9943 Eval Loss: 0.3768 Eval Acc: 0.9136 (LR: 0.000250)
[2025-05-14 11:22:57,019]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 043 Train Loss: 0.0184 Train Acc: 0.9945 Eval Loss: 0.3847 Eval Acc: 0.9131 (LR: 0.000250)
[2025-05-14 11:25:02,709]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 044 Train Loss: 0.0183 Train Acc: 0.9947 Eval Loss: 0.3827 Eval Acc: 0.9109 (LR: 0.000250)
[2025-05-14 11:27:08,473]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 045 Train Loss: 0.0182 Train Acc: 0.9945 Eval Loss: 0.3876 Eval Acc: 0.9146 (LR: 0.000063)
[2025-05-14 11:29:14,319]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 046 Train Loss: 0.0168 Train Acc: 0.9953 Eval Loss: 0.3870 Eval Acc: 0.9135 (LR: 0.000063)
[2025-05-14 11:31:20,207]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 047 Train Loss: 0.0166 Train Acc: 0.9950 Eval Loss: 0.3777 Eval Acc: 0.9139 (LR: 0.000063)
[2025-05-14 11:33:26,134]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 048 Train Loss: 0.0155 Train Acc: 0.9955 Eval Loss: 0.3799 Eval Acc: 0.9147 (LR: 0.000063)
[2025-05-14 11:35:32,217]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 049 Train Loss: 0.0155 Train Acc: 0.9957 Eval Loss: 0.3850 Eval Acc: 0.9150 (LR: 0.000063)
[2025-05-14 11:37:38,511]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 050 Train Loss: 0.0148 Train Acc: 0.9961 Eval Loss: 0.3842 Eval Acc: 0.9143 (LR: 0.000063)
[2025-05-14 11:39:44,606]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 051 Train Loss: 0.0149 Train Acc: 0.9957 Eval Loss: 0.3771 Eval Acc: 0.9149 (LR: 0.000063)
[2025-05-14 11:41:50,484]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 052 Train Loss: 0.0156 Train Acc: 0.9953 Eval Loss: 0.3812 Eval Acc: 0.9142 (LR: 0.000063)
[2025-05-14 11:43:56,324]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 053 Train Loss: 0.0152 Train Acc: 0.9955 Eval Loss: 0.3883 Eval Acc: 0.9136 (LR: 0.000063)
[2025-05-14 11:46:02,235]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 054 Train Loss: 0.0146 Train Acc: 0.9956 Eval Loss: 0.3903 Eval Acc: 0.9151 (LR: 0.000063)
[2025-05-14 11:48:08,330]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 055 Train Loss: 0.0140 Train Acc: 0.9963 Eval Loss: 0.3808 Eval Acc: 0.9154 (LR: 0.000063)
[2025-05-14 11:50:14,402]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 056 Train Loss: 0.0138 Train Acc: 0.9962 Eval Loss: 0.3845 Eval Acc: 0.9147 (LR: 0.000063)
[2025-05-14 11:52:20,097]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 057 Train Loss: 0.0139 Train Acc: 0.9960 Eval Loss: 0.3937 Eval Acc: 0.9120 (LR: 0.000063)
[2025-05-14 11:54:25,584]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 058 Train Loss: 0.0141 Train Acc: 0.9959 Eval Loss: 0.3816 Eval Acc: 0.9131 (LR: 0.000063)
[2025-05-14 11:56:31,256]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 059 Train Loss: 0.0137 Train Acc: 0.9962 Eval Loss: 0.3922 Eval Acc: 0.9136 (LR: 0.000063)
[2025-05-14 11:58:38,060]: [ResNet18_parametrized_relu_quantized_4_bits] Epoch: 060 Train Loss: 0.0146 Train Acc: 0.9959 Eval Loss: 0.3901 Eval Acc: 0.9133 (LR: 0.000063)
[2025-05-14 11:58:38,061]: [ResNet18_parametrized_relu_quantized_4_bits] Best Eval Accuracy: 0.9154
[2025-05-14 11:58:38,166]: 


Quantization of model down to 4 bits finished
[2025-05-14 11:58:38,166]: Model Architecture:
[2025-05-14 11:58:38,234]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3826], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.739720344543457)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0203], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14855530858039856, max_val=0.15611782670021057)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3831], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.747225761413574)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11086289584636688, max_val=0.11191902309656143)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3807], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7106852531433105)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0153], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11548958718776703, max_val=0.11399081349372864)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3836], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.753721237182617)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07883422821760178, max_val=0.10364744067192078)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3818], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7263264656066895)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0102], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07654077559709549, max_val=0.07589264959096909)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3829], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.742959976196289)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0092], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06783552467823029, max_val=0.06960691511631012)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0206], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16369883716106415, max_val=0.14551135897636414)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3833], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.749825477600098)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0087], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05968038737773895, max_val=0.07052973657846451)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3822], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7336320877075195)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0080], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05803430825471878, max_val=0.06199624016880989)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3865], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.796791076660156)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05015493556857109, max_val=0.05659504234790802)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3830], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.744372844696045)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04505983367562294, max_val=0.047814201563596725)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0144], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10990294814109802, max_val=0.10551455616950989)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3839], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.758118629455566)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0060], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.043923091143369675, max_val=0.046110380440950394)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3735], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.603005409240723)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0055], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.041548263281583786, max_val=0.04030250385403633)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3839], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.758283615112305)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03397112712264061, max_val=0.039357200264930725)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3835], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.752743244171143)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.026395440101623535, max_val=0.02894137240946293)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0095], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07050321996212006, max_val=0.0720762237906456)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3825], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.737856388092041)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0035], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.025405170395970345, max_val=0.026548245921730995)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3834], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.751216411590576)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0028], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020820310339331627, max_val=0.021524248644709587)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3874], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.811532020568848)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 11:58:38,234]: 
Model Weights:
[2025-05-14 11:58:38,234]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 11:58:38,235]: Sample Values (25 elements): [-0.1338357925415039, 0.1389119178056717, -0.045276302844285965, -0.06723473966121674, -0.0013900959165766835, -0.17712236940860748, 0.05171826854348183, 0.0971476137638092, 0.20601947605609894, 0.06753291189670563, 0.021010078489780426, 0.17010392248630524, 0.04192567989230156, 0.01051411684602499, -0.12421264499425888, 0.020452700555324554, 0.15437538921833038, 0.2126186192035675, 0.20280635356903076, 0.21720680594444275, -0.10823765397071838, 0.17609526216983795, -0.17031008005142212, -0.13829182088375092, -0.02550646662712097]
[2025-05-14 11:58:38,235]: Mean: -0.00099519
[2025-05-14 11:58:38,235]: Min: -0.36628550
[2025-05-14 11:58:38,235]: Max: 0.37932998
[2025-05-14 11:58:38,235]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 11:58:38,235]: Sample Values (25 elements): [0.9885563254356384, 0.9703359603881836, 1.0217196941375732, 0.9493546485900879, 1.0724390745162964, 0.965848982334137, 0.9729052186012268, 0.8632780313491821, 0.9209151864051819, 0.9055608510971069, 0.8500834703445435, 0.985561192035675, 0.8943570852279663, 0.9680811166763306, 0.9154256582260132, 0.8929880857467651, 0.8586562275886536, 0.9505009651184082, 0.9895287156105042, 0.9152987003326416, 0.9648993611335754, 0.9631221890449524, 0.9313640594482422, 0.889162003993988, 0.968230664730072]
[2025-05-14 11:58:38,235]: Mean: 0.95284534
[2025-05-14 11:58:38,236]: Min: 0.84839398
[2025-05-14 11:58:38,236]: Max: 1.19444191
[2025-05-14 11:58:38,237]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 11:58:38,237]: Sample Values (25 elements): [0.04062308743596077, 0.0, -0.04062308743596077, 0.020311543717980385, 0.060934633016586304, -0.04062308743596077, 0.0, 0.060934633016586304, 0.020311543717980385, 0.0, 0.0, -0.020311543717980385, -0.020311543717980385, -0.020311543717980385, -0.020311543717980385, 0.020311543717980385, 0.04062308743596077, -0.04062308743596077, 0.020311543717980385, 0.0, -0.020311543717980385, 0.060934633016586304, -0.04062308743596077, -0.020311543717980385, 0.0]
[2025-05-14 11:58:38,238]: Mean: -0.00149207
[2025-05-14 11:58:38,238]: Min: -0.14218080
[2025-05-14 11:58:38,238]: Max: 0.16249235
[2025-05-14 11:58:38,238]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 11:58:38,238]: Sample Values (25 elements): [0.983102023601532, 0.9578776955604553, 0.9819751977920532, 0.9317395687103271, 0.9565044641494751, 0.9536009430885315, 0.9919252395629883, 0.9810783863067627, 0.9744218587875366, 0.944418728351593, 0.9278841614723206, 1.0055773258209229, 0.9663386344909668, 0.967939555644989, 0.9663425087928772, 0.9802719950675964, 0.9632300734519958, 0.9507833123207092, 0.9527909159660339, 0.9456318020820618, 1.0289214849472046, 0.9505733251571655, 0.9815027713775635, 0.9410138130187988, 0.9327905774116516]
[2025-05-14 11:58:38,238]: Mean: 0.96296477
[2025-05-14 11:58:38,239]: Min: 0.89788300
[2025-05-14 11:58:38,239]: Max: 1.08150637
[2025-05-14 11:58:38,240]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 11:58:38,240]: Sample Values (25 elements): [-0.014852127060294151, 0.0, -0.029704254120588303, 0.014852127060294151, -0.059408508241176605, -0.014852127060294151, -0.029704254120588303, 0.014852127060294151, 0.0, 0.014852127060294151, -0.029704254120588303, 0.014852127060294151, -0.029704254120588303, -0.029704254120588303, -0.014852127060294151, 0.0, 0.014852127060294151, -0.014852127060294151, 0.0, -0.029704254120588303, 0.014852127060294151, 0.029704254120588303, 0.014852127060294151, 0.014852127060294151, -0.044556379318237305]
[2025-05-14 11:58:38,240]: Mean: -0.00105074
[2025-05-14 11:58:38,241]: Min: -0.10396489
[2025-05-14 11:58:38,241]: Max: 0.11881702
[2025-05-14 11:58:38,241]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 11:58:38,241]: Sample Values (25 elements): [0.9449573755264282, 0.9314208030700684, 0.9919443130493164, 0.9585378170013428, 0.9554031491279602, 0.9675361514091492, 0.9579086303710938, 0.967420220375061, 0.9724609851837158, 0.9575340747833252, 0.9648795127868652, 0.9634798765182495, 0.9709059000015259, 0.9819760322570801, 0.944330096244812, 0.9540839791297913, 0.962032675743103, 0.9636788368225098, 0.9670525789260864, 1.0427581071853638, 0.9716695547103882, 0.9771802425384521, 0.9237141609191895, 0.9409091472625732, 0.9393108487129211]
[2025-05-14 11:58:38,241]: Mean: 0.96372688
[2025-05-14 11:58:38,241]: Min: 0.91495198
[2025-05-14 11:58:38,241]: Max: 1.07550037
[2025-05-14 11:58:38,242]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 11:58:38,243]: Sample Values (25 elements): [0.015298663638532162, 0.0, 0.0, -0.04589598998427391, -0.030597327277064323, 0.015298663638532162, 0.0, 0.030597327277064323, 0.030597327277064323, 0.030597327277064323, 0.015298663638532162, -0.030597327277064323, -0.030597327277064323, 0.015298663638532162, -0.015298663638532162, -0.015298663638532162, -0.09179197996854782, 0.0, 0.0, 0.0, 0.030597327277064323, 0.0, 0.0, -0.015298663638532162, 0.030597327277064323]
[2025-05-14 11:58:38,243]: Mean: -0.00084453
[2025-05-14 11:58:38,243]: Min: -0.12238931
[2025-05-14 11:58:38,243]: Max: 0.10709064
[2025-05-14 11:58:38,244]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 11:58:38,244]: Sample Values (25 elements): [0.9499322175979614, 0.9492620229721069, 0.9398564696311951, 0.998304009437561, 0.956821620464325, 0.9592217803001404, 0.9506114721298218, 0.9521656632423401, 0.9394305944442749, 0.9540345668792725, 0.9616691470146179, 0.9689239263534546, 0.963643491268158, 0.9691573977470398, 0.9765479564666748, 0.9591900110244751, 0.9742641448974609, 0.9663928151130676, 0.9723495244979858, 0.9807558059692383, 0.9779609441757202, 0.9490255117416382, 0.962538480758667, 0.9589361548423767, 0.9724345207214355]
[2025-05-14 11:58:38,244]: Mean: 0.96064639
[2025-05-14 11:58:38,244]: Min: 0.93943059
[2025-05-14 11:58:38,244]: Max: 1.02667582
[2025-05-14 11:58:38,245]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 11:58:38,246]: Sample Values (25 elements): [-0.01216544583439827, 0.01216544583439827, 0.02433089166879654, 0.0, -0.02433089166879654, 0.01216544583439827, -0.01216544583439827, 0.03649633750319481, 0.01216544583439827, 0.0, -0.01216544583439827, 0.0, -0.02433089166879654, 0.01216544583439827, 0.01216544583439827, 0.0, 0.01216544583439827, 0.0, 0.02433089166879654, 0.03649633750319481, -0.01216544583439827, 0.04866178333759308, -0.06082722917199135, 0.01216544583439827, -0.01216544583439827]
[2025-05-14 11:58:38,246]: Mean: -0.00022276
[2025-05-14 11:58:38,246]: Min: -0.07299268
[2025-05-14 11:58:38,246]: Max: 0.10948901
[2025-05-14 11:58:38,246]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 11:58:38,247]: Sample Values (25 elements): [0.9682809710502625, 0.983117401599884, 0.9623098969459534, 0.9841526746749878, 0.9701830148696899, 0.9705024361610413, 0.9699680805206299, 0.9894447922706604, 0.9614800810813904, 0.99159836769104, 0.9943470358848572, 0.9955301880836487, 0.9583778381347656, 0.980824887752533, 0.9939651489257812, 0.9785193800926208, 0.9851594567298889, 0.9651342630386353, 0.9618050456047058, 0.975048840045929, 0.9975860118865967, 0.9578545689582825, 0.9793661832809448, 0.959953784942627, 0.9810824990272522]
[2025-05-14 11:58:38,247]: Mean: 0.97505528
[2025-05-14 11:58:38,247]: Min: 0.94928771
[2025-05-14 11:58:38,247]: Max: 1.01238632
[2025-05-14 11:58:38,248]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 11:58:38,249]: Sample Values (25 elements): [-0.030486680567264557, -0.030486680567264557, -0.010162226855754852, -0.030486680567264557, 0.0, -0.010162226855754852, -0.020324453711509705, -0.010162226855754852, 0.010162226855754852, -0.04064890742301941, 0.020324453711509705, -0.010162226855754852, 0.030486680567264557, 0.010162226855754852, 0.030486680567264557, 0.05081113427877426, 0.030486680567264557, -0.030486680567264557, -0.030486680567264557, -0.020324453711509705, 0.0, -0.010162226855754852, 0.0, -0.04064890742301941, 0.0]
[2025-05-14 11:58:38,249]: Mean: -0.00083458
[2025-05-14 11:58:38,249]: Min: -0.08129781
[2025-05-14 11:58:38,249]: Max: 0.07113559
[2025-05-14 11:58:38,249]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 11:58:38,250]: Sample Values (25 elements): [0.9680948853492737, 0.9554111957550049, 0.9603695273399353, 0.9541013836860657, 0.9618140459060669, 0.948117196559906, 0.9551429748535156, 0.9566671252250671, 0.9614888429641724, 0.9589471220970154, 0.9592834711074829, 0.9642736911773682, 0.9507313966751099, 0.964249849319458, 0.9640475511550903, 0.953857958316803, 0.9551579356193542, 0.960365355014801, 0.9427568912506104, 0.9652800559997559, 0.9461642503738403, 0.967125654220581, 0.9601101279258728, 0.9537584781646729, 0.9574492573738098]
[2025-05-14 11:58:38,250]: Mean: 0.96006703
[2025-05-14 11:58:38,250]: Min: 0.94275689
[2025-05-14 11:58:38,250]: Max: 0.98748320
[2025-05-14 11:58:38,251]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 11:58:38,253]: Sample Values (25 elements): [0.018325667828321457, -0.018325667828321457, 0.009162833914160728, 0.018325667828321457, 0.009162833914160728, -0.018325667828321457, 0.0, -0.018325667828321457, 0.009162833914160728, 0.018325667828321457, 0.018325667828321457, -0.018325667828321457, 0.027488501742482185, -0.027488501742482185, 0.009162833914160728, -0.009162833914160728, -0.018325667828321457, -0.027488501742482185, -0.009162833914160728, 0.0, 0.04581417143344879, -0.018325667828321457, -0.018325667828321457, 0.018325667828321457, -0.018325667828321457]
[2025-05-14 11:58:38,253]: Mean: -0.00063469
[2025-05-14 11:58:38,253]: Min: -0.06413984
[2025-05-14 11:58:38,253]: Max: 0.07330267
[2025-05-14 11:58:38,253]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 11:58:38,253]: Sample Values (25 elements): [0.9558315873146057, 0.9608436822891235, 0.9672264456748962, 0.9561156034469604, 0.9707654118537903, 0.9596219062805176, 0.9593287706375122, 0.9678204655647278, 0.9553154110908508, 0.9465110301971436, 0.9770669937133789, 0.9458105564117432, 0.9657250642776489, 0.9622195363044739, 0.9602363109588623, 0.9652090668678284, 0.9602757692337036, 0.9620882272720337, 0.9587579369544983, 0.9524583220481873, 0.9564225673675537, 0.960957407951355, 0.9627036452293396, 0.9725964069366455, 0.9768860936164856]
[2025-05-14 11:58:38,254]: Mean: 0.96080804
[2025-05-14 11:58:38,254]: Min: 0.93584239
[2025-05-14 11:58:38,254]: Max: 0.99112827
[2025-05-14 11:58:38,255]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 11:58:38,255]: Sample Values (25 elements): [0.06184202805161476, -0.020614009350538254, 0.020614009350538254, -0.08245603740215302, 0.08245603740215302, 0.04122801870107651, 0.0, 0.08245603740215302, -0.10307005047798157, 0.08245603740215302, 0.04122801870107651, -0.12368405610322952, 0.020614009350538254, 0.020614009350538254, -0.08245603740215302, 0.0, 0.0, 0.04122801870107651, 0.04122801870107651, -0.020614009350538254, 0.0, 0.0, 0.10307005047798157, 0.020614009350538254, -0.06184202805161476]
[2025-05-14 11:58:38,255]: Mean: -0.00094615
[2025-05-14 11:58:38,255]: Min: -0.16491207
[2025-05-14 11:58:38,256]: Max: 0.14429806
[2025-05-14 11:58:38,256]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 11:58:38,256]: Sample Values (25 elements): [0.9242297410964966, 0.927432656288147, 0.9416320323944092, 0.9363558292388916, 0.9452733397483826, 0.942828893661499, 0.9404453039169312, 0.9271606802940369, 0.9331859946250916, 0.9309332370758057, 0.9358299970626831, 0.9287731051445007, 0.9414607882499695, 0.9385889768600464, 0.935485303401947, 0.9368146657943726, 0.9363148212432861, 0.9340106248855591, 0.9355724453926086, 0.9498342275619507, 0.9367063641548157, 0.9337654709815979, 0.9371894598007202, 0.929297924041748, 0.9380362033843994]
[2025-05-14 11:58:38,256]: Mean: 0.93656623
[2025-05-14 11:58:38,256]: Min: 0.91874433
[2025-05-14 11:58:38,256]: Max: 0.95208222
[2025-05-14 11:58:38,257]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 11:58:38,259]: Sample Values (25 elements): [0.01736135222017765, -0.008680676110088825, -0.008680676110088825, 0.008680676110088825, 0.02604202926158905, -0.02604202926158905, 0.01736135222017765, -0.008680676110088825, 0.01736135222017765, -0.0347227044403553, 0.02604202926158905, -0.008680676110088825, 0.0, 0.01736135222017765, 0.008680676110088825, 0.0, -0.0347227044403553, -0.02604202926158905, -0.01736135222017765, -0.01736135222017765, 0.02604202926158905, -0.0347227044403553, 0.0347227044403553, 0.0, -0.008680676110088825]
[2025-05-14 11:58:38,259]: Mean: -0.00055655
[2025-05-14 11:58:38,259]: Min: -0.06076473
[2025-05-14 11:58:38,259]: Max: 0.06944541
[2025-05-14 11:58:38,259]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 11:58:38,260]: Sample Values (25 elements): [0.9592671990394592, 0.9569364786148071, 0.9570289254188538, 0.9593026041984558, 0.9530266523361206, 0.9631586670875549, 0.9697460532188416, 0.946753978729248, 0.9542317986488342, 0.9538781642913818, 0.9538648128509521, 0.9637265801429749, 0.9700589179992676, 0.9599757790565491, 0.9684464335441589, 0.9617787599563599, 0.9654253125190735, 0.9567508697509766, 0.9652602076530457, 0.9654458165168762, 0.9533857107162476, 0.9500090479850769, 0.9594022035598755, 0.9701353907585144, 0.9718029499053955]
[2025-05-14 11:58:38,260]: Mean: 0.95963091
[2025-05-14 11:58:38,260]: Min: 0.93947399
[2025-05-14 11:58:38,260]: Max: 0.98107088
[2025-05-14 11:58:38,261]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 11:58:38,262]: Sample Values (25 elements): [-0.008002039045095444, 0.008002039045095444, -0.008002039045095444, 0.008002039045095444, 0.016004078090190887, -0.016004078090190887, -0.008002039045095444, -0.02400611713528633, -0.02400611713528633, 0.02400611713528633, 0.02400611713528633, -0.016004078090190887, 0.008002039045095444, 0.008002039045095444, -0.008002039045095444, 0.016004078090190887, 0.02400611713528633, 0.008002039045095444, -0.04001019522547722, 0.008002039045095444, -0.016004078090190887, -0.008002039045095444, 0.02400611713528633, -0.016004078090190887, 0.0]
[2025-05-14 11:58:38,263]: Mean: -0.00005633
[2025-05-14 11:58:38,263]: Min: -0.05601427
[2025-05-14 11:58:38,263]: Max: 0.06401631
[2025-05-14 11:58:38,263]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 11:58:38,263]: Sample Values (25 elements): [0.9839404821395874, 0.9659649133682251, 0.9749767780303955, 0.9830210208892822, 0.9746969938278198, 0.9774340987205505, 0.9887071847915649, 0.9772484302520752, 0.9800897240638733, 0.9737891554832458, 0.97346431016922, 0.9570409059524536, 0.9666272401809692, 0.9732387661933899, 0.9779457449913025, 0.9822570085525513, 0.9773465394973755, 0.972496747970581, 0.9857995510101318, 0.972888171672821, 0.9755339622497559, 0.9959419369697571, 0.9980455040931702, 0.9753777384757996, 0.9681288003921509]
[2025-05-14 11:58:38,263]: Mean: 0.97853255
[2025-05-14 11:58:38,264]: Min: 0.95704091
[2025-05-14 11:58:38,264]: Max: 1.00611901
[2025-05-14 11:58:38,265]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 11:58:38,268]: Sample Values (25 elements): [-0.014233343303203583, -0.014233343303203583, -0.014233343303203583, -0.028466686606407166, 0.007116671651601791, -0.021350014954805374, -0.028466686606407166, 0.0, 0.014233343303203583, -0.014233343303203583, 0.014233343303203583, 0.014233343303203583, -0.014233343303203583, 0.028466686606407166, -0.007116671651601791, -0.014233343303203583, -0.014233343303203583, 0.014233343303203583, -0.021350014954805374, -0.021350014954805374, 0.014233343303203583, 0.021350014954805374, 0.021350014954805374, -0.03558335825800896, -0.014233343303203583]
[2025-05-14 11:58:38,268]: Mean: -0.00016180
[2025-05-14 11:58:38,268]: Min: -0.04981670
[2025-05-14 11:58:38,268]: Max: 0.05693337
[2025-05-14 11:58:38,268]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 11:58:38,269]: Sample Values (25 elements): [0.9573634266853333, 0.964818000793457, 0.9583454132080078, 0.9609785079956055, 0.9613997340202332, 0.9602307081222534, 0.9458268880844116, 0.956416130065918, 0.9562621712684631, 0.9647508859634399, 0.9550616145133972, 0.9654252529144287, 0.9608743786811829, 0.9580893516540527, 0.9621186852455139, 0.9612722396850586, 0.9659737944602966, 0.9574981331825256, 0.9637283682823181, 0.9604132175445557, 0.9635802507400513, 0.9556130170822144, 0.9463052749633789, 0.9641885161399841, 0.962882936000824]
[2025-05-14 11:58:38,269]: Mean: 0.95946920
[2025-05-14 11:58:38,269]: Min: 0.94582689
[2025-05-14 11:58:38,269]: Max: 0.97616243
[2025-05-14 11:58:38,270]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 11:58:38,275]: Sample Values (25 elements): [0.006191594991832972, 0.006191594991832972, -0.012383189983665943, 0.006191594991832972, 0.012383189983665943, 0.006191594991832972, -0.006191594991832972, -0.012383189983665943, 0.0, -0.006191594991832972, -0.012383189983665943, 0.024766379967331886, 0.006191594991832972, 0.0, -0.012383189983665943, -0.012383189983665943, 0.018574785441160202, 0.006191594991832972, -0.018574785441160202, 0.006191594991832972, -0.006191594991832972, 0.0, 0.018574785441160202, -0.018574785441160202, -0.006191594991832972]
[2025-05-14 11:58:38,275]: Mean: -0.00039871
[2025-05-14 11:58:38,276]: Min: -0.04334116
[2025-05-14 11:58:38,276]: Max: 0.04953276
[2025-05-14 11:58:38,276]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 11:58:38,276]: Sample Values (25 elements): [0.9595822691917419, 0.969896137714386, 0.9614881873130798, 0.9533154368400574, 0.9625089764595032, 0.961516261100769, 0.96269690990448, 0.963383138179779, 0.9653730392456055, 0.9629526138305664, 0.9680038690567017, 0.9689052700996399, 0.9642597436904907, 0.9517403244972229, 0.9746028780937195, 0.9652016162872314, 0.968572199344635, 0.9585287570953369, 0.9682518243789673, 0.9722986221313477, 0.9665828347206116, 0.9622703194618225, 0.9593815207481384, 0.9703746438026428, 0.9623485803604126]
[2025-05-14 11:58:38,276]: Mean: 0.96495527
[2025-05-14 11:58:38,276]: Min: 0.94720179
[2025-05-14 11:58:38,277]: Max: 0.99587029
[2025-05-14 11:58:38,278]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 11:58:38,278]: Sample Values (25 elements): [0.05744470655918121, 0.05744470655918121, 0.04308352991938591, -0.07180587947368622, -0.07180587947368622, 0.04308352991938591, -0.028722353279590607, -0.014361176639795303, -0.05744470655918121, 0.04308352991938591, -0.028722353279590607, -0.04308352991938591, 0.07180587947368622, -0.07180587947368622, -0.05744470655918121, 0.028722353279590607, -0.028722353279590607, 0.07180587947368622, 0.05744470655918121, -0.07180587947368622, -0.028722353279590607, -0.028722353279590607, 0.028722353279590607, 0.04308352991938591, 0.0]
[2025-05-14 11:58:38,278]: Mean: -0.00004295
[2025-05-14 11:58:38,278]: Min: -0.11488941
[2025-05-14 11:58:38,279]: Max: 0.10052824
[2025-05-14 11:58:38,279]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 11:58:38,279]: Sample Values (25 elements): [0.9417480230331421, 0.9399369359016418, 0.9415574669837952, 0.9418637752532959, 0.948594331741333, 0.9455274343490601, 0.9372801780700684, 0.9500580430030823, 0.939380407333374, 0.9357265830039978, 0.9510462880134583, 0.9429498910903931, 0.9399891495704651, 0.9438157677650452, 0.9419202208518982, 0.9513751268386841, 0.9482088685035706, 0.9404091238975525, 0.9446740746498108, 0.9430190920829773, 0.9368922114372253, 0.9375714063644409, 0.9440171718597412, 0.946824312210083, 0.9434236884117126]
[2025-05-14 11:58:38,279]: Mean: 0.94065714
[2025-05-14 11:58:38,279]: Min: 0.92811114
[2025-05-14 11:58:38,279]: Max: 0.95330215
[2025-05-14 11:58:38,280]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 11:58:38,287]: Sample Values (25 elements): [0.012004473246634007, -0.03001118265092373, -0.006002236623317003, -0.006002236623317003, 0.012004473246634007, 0.006002236623317003, -0.006002236623317003, -0.018006710335612297, 0.012004473246634007, -0.012004473246634007, -0.006002236623317003, -0.018006710335612297, 0.006002236623317003, 0.018006710335612297, -0.018006710335612297, -0.012004473246634007, -0.006002236623317003, -0.012004473246634007, 0.012004473246634007, 0.012004473246634007, -0.024008946493268013, -0.006002236623317003, 0.006002236623317003, -0.012004473246634007, 0.024008946493268013]
[2025-05-14 11:58:38,287]: Mean: -0.00045197
[2025-05-14 11:58:38,287]: Min: -0.04201566
[2025-05-14 11:58:38,287]: Max: 0.04801789
[2025-05-14 11:58:38,287]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 11:58:38,287]: Sample Values (25 elements): [0.9519358277320862, 0.9606006145477295, 0.9557759165763855, 0.9608184099197388, 0.9595309495925903, 0.9519286155700684, 0.9627835750579834, 0.9596540331840515, 0.9558958411216736, 0.958655059337616, 0.9602742791175842, 0.9478897452354431, 0.9635716676712036, 0.9611566662788391, 0.9589022994041443, 0.956234872341156, 0.9569677114486694, 0.9650834202766418, 0.9554951786994934, 0.9601380825042725, 0.9649887084960938, 0.9555517435073853, 0.9649534225463867, 0.9590792655944824, 0.9592907428741455]
[2025-05-14 11:58:38,288]: Mean: 0.95937991
[2025-05-14 11:58:38,288]: Min: 0.94523615
[2025-05-14 11:58:38,288]: Max: 0.97391677
[2025-05-14 11:58:38,289]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 11:58:38,294]: Sample Values (25 elements): [-0.016370166093111038, 0.016370166093111038, -0.005456721875816584, -0.021826887503266335, -0.021826887503266335, 0.010913443751633167, -0.016370166093111038, -0.005456721875816584, -0.021826887503266335, 0.0, 0.0, 0.005456721875816584, 0.0, -0.021826887503266335, 0.016370166093111038, 0.0, 0.005456721875816584, 0.005456721875816584, -0.005456721875816584, -0.021826887503266335, -0.005456721875816584, 0.010913443751633167, 0.0, 0.016370166093111038, -0.010913443751633167]
[2025-05-14 11:58:38,294]: Mean: -0.00011603
[2025-05-14 11:58:38,295]: Min: -0.04365378
[2025-05-14 11:58:38,295]: Max: 0.03819705
[2025-05-14 11:58:38,295]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 11:58:38,295]: Sample Values (25 elements): [0.9745597839355469, 0.967680811882019, 0.9756091237068176, 0.9669376015663147, 0.9780040979385376, 0.9758662581443787, 0.9760906100273132, 0.976632297039032, 0.9755765795707703, 0.9702025055885315, 0.9726651310920715, 0.9699246287345886, 0.9724556803703308, 0.9695818424224854, 0.9717700481414795, 0.9703972339630127, 0.9757729768753052, 0.9635647535324097, 0.9704227447509766, 0.9765350222587585, 0.9772207140922546, 0.9804763197898865, 0.9653900265693665, 0.9724348783493042, 0.9652426242828369]
[2025-05-14 11:58:38,295]: Mean: 0.97100067
[2025-05-14 11:58:38,295]: Min: 0.95872825
[2025-05-14 11:58:38,296]: Max: 0.98729342
[2025-05-14 11:58:38,297]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 11:58:38,310]: Sample Values (25 elements): [-0.014665666967630386, 0.019554222002625465, 0.009777111001312733, -0.014665666967630386, 0.004888555500656366, 0.004888555500656366, -0.009777111001312733, 0.004888555500656366, -0.004888555500656366, -0.004888555500656366, 0.014665666967630386, 0.009777111001312733, 0.019554222002625465, -0.009777111001312733, 0.009777111001312733, 0.019554222002625465, 0.014665666967630386, -0.024442777037620544, 0.009777111001312733, 0.019554222002625465, -0.009777111001312733, -0.004888555500656366, 0.009777111001312733, 0.0, 0.004888555500656366]
[2025-05-14 11:58:38,311]: Mean: -0.00001396
[2025-05-14 11:58:38,311]: Min: -0.03421989
[2025-05-14 11:58:38,311]: Max: 0.03910844
[2025-05-14 11:58:38,311]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 11:58:38,311]: Sample Values (25 elements): [0.9597529768943787, 0.9576617479324341, 0.9578568935394287, 0.9604153633117676, 0.9610762596130371, 0.9573559761047363, 0.9610399007797241, 0.9608827233314514, 0.9582063555717468, 0.9642667174339294, 0.9545780420303345, 0.956133246421814, 0.9634488224983215, 0.9609909653663635, 0.9566118121147156, 0.9605305194854736, 0.9606586694717407, 0.9560969471931458, 0.9619941711425781, 0.9582714438438416, 0.9595717191696167, 0.9598061442375183, 0.9606887102127075, 0.9581054449081421, 0.9635782241821289]
[2025-05-14 11:58:38,311]: Mean: 0.95902276
[2025-05-14 11:58:38,312]: Min: 0.95420420
[2025-05-14 11:58:38,312]: Max: 0.96729028
[2025-05-14 11:58:38,313]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 11:58:38,354]: Sample Values (25 elements): [0.007378252223134041, -0.007378252223134041, -0.007378252223134041, 0.007378252223134041, 0.011067378334701061, -0.011067378334701061, 0.007378252223134041, -0.011067378334701061, -0.0036891261115670204, 0.0036891261115670204, -0.011067378334701061, -0.011067378334701061, -0.011067378334701061, -0.011067378334701061, 0.007378252223134041, 0.0036891261115670204, 0.011067378334701061, 0.007378252223134041, -0.0036891261115670204, 0.007378252223134041, 0.014756504446268082, 0.007378252223134041, -0.011067378334701061, -0.011067378334701061, -0.011067378334701061]
[2025-05-14 11:58:38,354]: Mean: -0.00003023
[2025-05-14 11:58:38,354]: Min: -0.02582388
[2025-05-14 11:58:38,355]: Max: 0.02951301
[2025-05-14 11:58:38,355]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 11:58:38,356]: Sample Values (25 elements): [0.9705966711044312, 0.9694574475288391, 0.9648390412330627, 0.9683744311332703, 0.9652649760246277, 0.9640411138534546, 0.9656499028205872, 0.9638157486915588, 0.9645766615867615, 0.9604387879371643, 0.961189866065979, 0.9614450335502625, 0.9624754190444946, 0.9635111689567566, 0.960989773273468, 0.9641117453575134, 0.9652422666549683, 0.9643352031707764, 0.9634555578231812, 0.9665743112564087, 0.9614478945732117, 0.9620517492294312, 0.9628334045410156, 0.9623265862464905, 0.9665595889091492]
[2025-05-14 11:58:38,356]: Mean: 0.96403158
[2025-05-14 11:58:38,356]: Min: 0.95783299
[2025-05-14 11:58:38,357]: Max: 0.97675294
[2025-05-14 11:58:38,358]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 11:58:38,359]: Sample Values (25 elements): [0.028515882790088654, 0.04752647131681442, 0.009505294263362885, -0.01901058852672577, 0.04752647131681442, -0.009505294263362885, -0.03802117705345154, -0.009505294263362885, 0.01901058852672577, 0.01901058852672577, 0.01901058852672577, 0.01901058852672577, 0.03802117705345154, -0.03802117705345154, -0.03802117705345154, 0.03802117705345154, -0.05703176558017731, 0.028515882790088654, -0.01901058852672577, -0.05703176558017731, 0.01901058852672577, -0.01901058852672577, -0.009505294263362885, 0.05703176558017731, -0.03802117705345154]
[2025-05-14 11:58:38,359]: Mean: 0.00002321
[2025-05-14 11:58:38,359]: Min: -0.06653706
[2025-05-14 11:58:38,359]: Max: 0.07604235
[2025-05-14 11:58:38,359]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 11:58:38,360]: Sample Values (25 elements): [0.9584694504737854, 0.9568755626678467, 0.9551874399185181, 0.9571171998977661, 0.9537588953971863, 0.9578076601028442, 0.9553428888320923, 0.9543872475624084, 0.9568487405776978, 0.9591370224952698, 0.9554281234741211, 0.9560978412628174, 0.9584699273109436, 0.9564677476882935, 0.9560913443565369, 0.9558629393577576, 0.9596520066261292, 0.9564633369445801, 0.9559522271156311, 0.9532898664474487, 0.9514408707618713, 0.9550320506095886, 0.9566645622253418, 0.9567508697509766, 0.9523423910140991]
[2025-05-14 11:58:38,360]: Mean: 0.95618993
[2025-05-14 11:58:38,360]: Min: 0.95137221
[2025-05-14 11:58:38,360]: Max: 0.96088600
[2025-05-14 11:58:38,361]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 11:58:38,403]: Sample Values (25 elements): [0.0, 0.006927123758941889, 0.010390685871243477, 0.0, 0.0, 0.010390685871243477, -0.010390685871243477, 0.010390685871243477, 0.013854247517883778, -0.006927123758941889, -0.010390685871243477, -0.006927123758941889, -0.01731780916452408, -0.010390685871243477, 0.010390685871243477, 0.010390685871243477, 0.010390685871243477, -0.006927123758941889, -0.0034635618794709444, 0.006927123758941889, 0.006927123758941889, 0.0, 0.010390685871243477, -0.010390685871243477, -0.0034635618794709444]
[2025-05-14 11:58:38,404]: Mean: -0.00010378
[2025-05-14 11:58:38,404]: Min: -0.02424493
[2025-05-14 11:58:38,404]: Max: 0.02770850
[2025-05-14 11:58:38,404]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 11:58:38,405]: Sample Values (25 elements): [0.9569946527481079, 0.9578537940979004, 0.964340090751648, 0.9573541283607483, 0.9576656818389893, 0.9582285284996033, 0.9582609534263611, 0.9573302268981934, 0.9605731964111328, 0.9580572247505188, 0.9568833112716675, 0.9576088190078735, 0.9608919620513916, 0.9605077505111694, 0.956626296043396, 0.9563325643539429, 0.9585762619972229, 0.9600903987884521, 0.9574874043464661, 0.9590254426002502, 0.9563867449760437, 0.9571629762649536, 0.9587081670761108, 0.9620217680931091, 0.9567618370056152]
[2025-05-14 11:58:38,405]: Mean: 0.95886189
[2025-05-14 11:58:38,405]: Min: 0.95459718
[2025-05-14 11:58:38,405]: Max: 0.96864122
[2025-05-14 11:58:38,406]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 11:58:38,444]: Sample Values (25 elements): [-0.008468911051750183, -0.011291882023215294, 0.011291882023215294, -0.0028229705058038235, 0.0, 0.011291882023215294, 0.005645941011607647, 0.005645941011607647, 0.0, 0.011291882023215294, -0.0028229705058038235, -0.011291882023215294, -0.008468911051750183, 0.005645941011607647, -0.005645941011607647, 0.0, -0.008468911051750183, 0.005645941011607647, 0.011291882023215294, 0.0, -0.011291882023215294, 0.005645941011607647, 0.0028229705058038235, -0.011291882023215294, 0.008468911051750183]
[2025-05-14 11:58:38,444]: Mean: 0.00002618
[2025-05-14 11:58:38,445]: Min: -0.01976079
[2025-05-14 11:58:38,445]: Max: 0.02258376
[2025-05-14 11:58:38,445]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 11:58:38,445]: Sample Values (25 elements): [0.9689902663230896, 0.9700485467910767, 0.967603325843811, 0.9679563045501709, 0.9675337076187134, 0.9659669399261475, 0.9667575359344482, 0.9722914099693298, 0.9680275917053223, 0.9702563881874084, 0.971688449382782, 0.9693471193313599, 0.9718946814537048, 0.9711769819259644, 0.9695971608161926, 0.9711126685142517, 0.9688200354576111, 0.965878427028656, 0.9655159711837769, 0.9694719910621643, 0.9723616242408752, 0.970787525177002, 0.9666270017623901, 0.9633678793907166, 0.9722523093223572]
[2025-05-14 11:58:38,445]: Mean: 0.96905184
[2025-05-14 11:58:38,446]: Min: 0.96267945
[2025-05-14 11:58:38,446]: Max: 0.97694105
[2025-05-14 11:58:38,446]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 11:58:38,446]: Sample Values (25 elements): [-0.03869884833693504, 0.05608142167329788, 0.10864637792110443, -0.013702775351703167, 0.014493986964225769, -0.05326741188764572, -0.01622234657406807, -0.04573765769600868, 0.10460799932479858, -0.055758923292160034, 0.06537500768899918, -0.0003630626015365124, 0.04159509763121605, -0.014424087479710579, -0.062136534601449966, 0.09130069613456726, -0.044314026832580566, -0.06134023144841194, -0.00921668205410242, 0.019892903044819832, 0.09986867010593414, 0.06919826567173004, 0.1351834386587143, 0.07376998662948608, -0.054261140525341034]
[2025-05-14 11:58:38,446]: Mean: -0.00054008
[2025-05-14 11:58:38,446]: Min: -0.12927994
[2025-05-14 11:58:38,447]: Max: 0.15383232
[2025-05-14 11:58:38,447]: 


QAT of ResNet18 with parametrized_relu down to 3 bits...
[2025-05-14 11:58:38,673]: [ResNet18_parametrized_relu_quantized_3_bits] after configure_qat:
[2025-05-14 11:58:38,720]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 12:00:44,689]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 001 Train Loss: 0.2240 Train Acc: 0.9213 Eval Loss: 0.4821 Eval Acc: 0.8665 (LR: 0.001000)
[2025-05-14 12:02:50,713]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 002 Train Loss: 0.1881 Train Acc: 0.9329 Eval Loss: 0.4604 Eval Acc: 0.8730 (LR: 0.001000)
[2025-05-14 12:04:56,686]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 003 Train Loss: 0.1742 Train Acc: 0.9376 Eval Loss: 0.4482 Eval Acc: 0.8760 (LR: 0.001000)
[2025-05-14 12:07:02,879]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 004 Train Loss: 0.1660 Train Acc: 0.9389 Eval Loss: 0.4408 Eval Acc: 0.8787 (LR: 0.001000)
[2025-05-14 12:09:08,814]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 005 Train Loss: 0.1650 Train Acc: 0.9409 Eval Loss: 0.4997 Eval Acc: 0.8708 (LR: 0.001000)
[2025-05-14 12:11:14,702]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 006 Train Loss: 0.1550 Train Acc: 0.9447 Eval Loss: 0.4225 Eval Acc: 0.8836 (LR: 0.001000)
[2025-05-14 12:13:20,597]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 007 Train Loss: 0.1496 Train Acc: 0.9469 Eval Loss: 0.4152 Eval Acc: 0.8907 (LR: 0.001000)
[2025-05-14 12:15:26,485]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 008 Train Loss: 0.1503 Train Acc: 0.9450 Eval Loss: 0.4497 Eval Acc: 0.8759 (LR: 0.001000)
[2025-05-14 12:17:32,390]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 009 Train Loss: 0.1434 Train Acc: 0.9486 Eval Loss: 0.4322 Eval Acc: 0.8820 (LR: 0.001000)
[2025-05-14 12:19:29,649]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 010 Train Loss: 0.1376 Train Acc: 0.9505 Eval Loss: 0.4048 Eval Acc: 0.8870 (LR: 0.001000)
[2025-05-14 12:21:47,232]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 011 Train Loss: 0.1393 Train Acc: 0.9508 Eval Loss: 0.4151 Eval Acc: 0.8852 (LR: 0.001000)
[2025-05-14 12:23:41,196]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 012 Train Loss: 0.1384 Train Acc: 0.9503 Eval Loss: 0.4506 Eval Acc: 0.8782 (LR: 0.001000)
[2025-05-14 12:25:42,196]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 013 Train Loss: 0.1342 Train Acc: 0.9524 Eval Loss: 0.4412 Eval Acc: 0.8823 (LR: 0.001000)
[2025-05-14 12:27:28,633]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 014 Train Loss: 0.1341 Train Acc: 0.9514 Eval Loss: 0.4318 Eval Acc: 0.8811 (LR: 0.001000)
[2025-05-14 12:29:17,138]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 015 Train Loss: 0.1256 Train Acc: 0.9548 Eval Loss: 0.4390 Eval Acc: 0.8876 (LR: 0.001000)
[2025-05-14 12:31:07,632]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 016 Train Loss: 0.1238 Train Acc: 0.9571 Eval Loss: 0.4042 Eval Acc: 0.8927 (LR: 0.001000)
[2025-05-14 12:32:52,265]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 017 Train Loss: 0.1226 Train Acc: 0.9557 Eval Loss: 0.4328 Eval Acc: 0.8845 (LR: 0.001000)
[2025-05-14 12:34:41,574]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 018 Train Loss: 0.1203 Train Acc: 0.9573 Eval Loss: 0.4405 Eval Acc: 0.8852 (LR: 0.001000)
[2025-05-14 12:36:32,330]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 019 Train Loss: 0.1197 Train Acc: 0.9570 Eval Loss: 0.4401 Eval Acc: 0.8838 (LR: 0.001000)
[2025-05-14 12:38:24,026]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 020 Train Loss: 0.1183 Train Acc: 0.9577 Eval Loss: 0.4224 Eval Acc: 0.8877 (LR: 0.001000)
[2025-05-14 12:40:14,868]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 021 Train Loss: 0.1117 Train Acc: 0.9585 Eval Loss: 0.4062 Eval Acc: 0.8913 (LR: 0.001000)
[2025-05-14 12:42:05,602]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 022 Train Loss: 0.1146 Train Acc: 0.9595 Eval Loss: 0.4033 Eval Acc: 0.8958 (LR: 0.001000)
[2025-05-14 12:43:56,176]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 023 Train Loss: 0.1147 Train Acc: 0.9587 Eval Loss: 0.4579 Eval Acc: 0.8830 (LR: 0.001000)
[2025-05-14 12:45:46,698]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 024 Train Loss: 0.1105 Train Acc: 0.9609 Eval Loss: 0.4132 Eval Acc: 0.8945 (LR: 0.001000)
[2025-05-14 12:47:31,128]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 025 Train Loss: 0.1053 Train Acc: 0.9631 Eval Loss: 0.4256 Eval Acc: 0.8931 (LR: 0.001000)
[2025-05-14 12:49:17,573]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 026 Train Loss: 0.1080 Train Acc: 0.9613 Eval Loss: 0.4620 Eval Acc: 0.8833 (LR: 0.001000)
[2025-05-14 12:51:13,036]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 027 Train Loss: 0.1020 Train Acc: 0.9636 Eval Loss: 0.4609 Eval Acc: 0.8875 (LR: 0.001000)
[2025-05-14 12:53:13,412]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 028 Train Loss: 0.1035 Train Acc: 0.9638 Eval Loss: 0.4201 Eval Acc: 0.8893 (LR: 0.001000)
[2025-05-14 12:55:11,469]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 029 Train Loss: 0.1012 Train Acc: 0.9647 Eval Loss: 0.4096 Eval Acc: 0.8939 (LR: 0.001000)
[2025-05-14 12:57:02,178]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 030 Train Loss: 0.0993 Train Acc: 0.9644 Eval Loss: 0.4400 Eval Acc: 0.8897 (LR: 0.000250)
[2025-05-14 12:58:52,675]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 031 Train Loss: 0.0691 Train Acc: 0.9770 Eval Loss: 0.3756 Eval Acc: 0.9052 (LR: 0.000250)
[2025-05-14 13:00:44,248]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 032 Train Loss: 0.0617 Train Acc: 0.9787 Eval Loss: 0.3720 Eval Acc: 0.9056 (LR: 0.000250)
[2025-05-14 13:02:33,210]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 033 Train Loss: 0.0547 Train Acc: 0.9814 Eval Loss: 0.3699 Eval Acc: 0.9057 (LR: 0.000250)
[2025-05-14 13:04:25,519]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 034 Train Loss: 0.0526 Train Acc: 0.9828 Eval Loss: 0.3701 Eval Acc: 0.9096 (LR: 0.000250)
[2025-05-14 13:06:17,473]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 035 Train Loss: 0.0546 Train Acc: 0.9816 Eval Loss: 0.3734 Eval Acc: 0.9055 (LR: 0.000250)
[2025-05-14 13:08:09,022]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 036 Train Loss: 0.0527 Train Acc: 0.9822 Eval Loss: 0.3781 Eval Acc: 0.9059 (LR: 0.000250)
[2025-05-14 13:09:59,598]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 037 Train Loss: 0.0514 Train Acc: 0.9832 Eval Loss: 0.3773 Eval Acc: 0.9064 (LR: 0.000250)
[2025-05-14 13:11:50,351]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 038 Train Loss: 0.0524 Train Acc: 0.9818 Eval Loss: 0.3717 Eval Acc: 0.9058 (LR: 0.000250)
[2025-05-14 13:13:41,707]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 039 Train Loss: 0.0502 Train Acc: 0.9830 Eval Loss: 0.3853 Eval Acc: 0.9046 (LR: 0.000250)
[2025-05-14 13:15:30,750]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 040 Train Loss: 0.0505 Train Acc: 0.9826 Eval Loss: 0.3723 Eval Acc: 0.9071 (LR: 0.000250)
[2025-05-14 13:17:18,947]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 041 Train Loss: 0.0492 Train Acc: 0.9836 Eval Loss: 0.3735 Eval Acc: 0.9066 (LR: 0.000250)
[2025-05-14 13:19:07,121]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 042 Train Loss: 0.0496 Train Acc: 0.9831 Eval Loss: 0.3738 Eval Acc: 0.9066 (LR: 0.000250)
[2025-05-14 13:20:55,422]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 043 Train Loss: 0.0489 Train Acc: 0.9835 Eval Loss: 0.3814 Eval Acc: 0.9082 (LR: 0.000250)
[2025-05-14 13:22:43,690]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 044 Train Loss: 0.0478 Train Acc: 0.9836 Eval Loss: 0.3855 Eval Acc: 0.9074 (LR: 0.000250)
[2025-05-14 13:24:31,836]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 045 Train Loss: 0.0467 Train Acc: 0.9849 Eval Loss: 0.3891 Eval Acc: 0.9052 (LR: 0.000063)
[2025-05-14 13:26:19,922]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 046 Train Loss: 0.0415 Train Acc: 0.9863 Eval Loss: 0.3792 Eval Acc: 0.9081 (LR: 0.000063)
[2025-05-14 13:28:08,066]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 047 Train Loss: 0.0409 Train Acc: 0.9866 Eval Loss: 0.3790 Eval Acc: 0.9103 (LR: 0.000063)
[2025-05-14 13:29:52,333]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 048 Train Loss: 0.0396 Train Acc: 0.9874 Eval Loss: 0.3666 Eval Acc: 0.9113 (LR: 0.000063)
[2025-05-14 13:31:34,628]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 049 Train Loss: 0.0379 Train Acc: 0.9875 Eval Loss: 0.3738 Eval Acc: 0.9078 (LR: 0.000063)
[2025-05-14 13:33:16,979]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 050 Train Loss: 0.0403 Train Acc: 0.9862 Eval Loss: 0.3840 Eval Acc: 0.9079 (LR: 0.000063)
[2025-05-14 13:34:59,313]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 051 Train Loss: 0.0376 Train Acc: 0.9876 Eval Loss: 0.3829 Eval Acc: 0.9080 (LR: 0.000063)
[2025-05-14 13:36:41,454]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 052 Train Loss: 0.0388 Train Acc: 0.9870 Eval Loss: 0.3780 Eval Acc: 0.9090 (LR: 0.000063)
[2025-05-14 13:38:23,619]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 053 Train Loss: 0.0369 Train Acc: 0.9882 Eval Loss: 0.3773 Eval Acc: 0.9059 (LR: 0.000063)
[2025-05-14 13:40:05,957]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 054 Train Loss: 0.0370 Train Acc: 0.9887 Eval Loss: 0.3670 Eval Acc: 0.9097 (LR: 0.000063)
[2025-05-14 13:41:48,283]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 055 Train Loss: 0.0370 Train Acc: 0.9882 Eval Loss: 0.3710 Eval Acc: 0.9110 (LR: 0.000063)
[2025-05-14 13:43:30,580]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 056 Train Loss: 0.0361 Train Acc: 0.9885 Eval Loss: 0.3684 Eval Acc: 0.9092 (LR: 0.000063)
[2025-05-14 13:45:12,918]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 057 Train Loss: 0.0378 Train Acc: 0.9879 Eval Loss: 0.3710 Eval Acc: 0.9091 (LR: 0.000063)
[2025-05-14 13:46:55,054]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 058 Train Loss: 0.0398 Train Acc: 0.9867 Eval Loss: 0.3806 Eval Acc: 0.9079 (LR: 0.000063)
[2025-05-14 13:48:37,372]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 059 Train Loss: 0.0356 Train Acc: 0.9888 Eval Loss: 0.3750 Eval Acc: 0.9098 (LR: 0.000063)
[2025-05-14 13:50:20,196]: [ResNet18_parametrized_relu_quantized_3_bits] Epoch: 060 Train Loss: 0.0351 Train Acc: 0.9886 Eval Loss: 0.3771 Eval Acc: 0.9104 (LR: 0.000063)
[2025-05-14 13:50:20,196]: [ResNet18_parametrized_relu_quantized_3_bits] Best Eval Accuracy: 0.9113
[2025-05-14 13:50:20,299]: 


Quantization of model down to 3 bits finished
[2025-05-14 13:50:20,299]: Model Architecture:
[2025-05-14 13:50:20,350]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8189], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.732113361358643)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0456], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1523381769657135, max_val=0.16701167821884155)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8209], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.746064186096191)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11061520129442215, max_val=0.10946822911500931)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8143], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.700352668762207)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0342], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11083768308162689, max_val=0.12866665422916412)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8220], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.753833770751953)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0268], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07710274308919907, max_val=0.11043619364500046)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8192], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.73453950881958)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0232], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08087872713804245, max_val=0.08165396749973297)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8217], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.751684665679932)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0192], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06719525903463364, max_val=0.06723814457654953)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0453], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1695345938205719, max_val=0.147536039352417)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8216], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.751105308532715)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0193], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.062268760055303574, max_val=0.07278932631015778)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8218], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.752404689788818)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06078295782208443, max_val=0.07058711349964142)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8318], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.822521209716797)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0157], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05298192799091339, max_val=0.057000547647476196)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8218], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.752713203430176)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0136], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04640008881688118, max_val=0.04848547279834747)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0309], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10817711800336838, max_val=0.1081729605793953)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8230], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.760951042175293)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04600051790475845, max_val=0.0502668134868145)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8185], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.72946834564209)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0116], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.04056432843208313, max_val=0.04043200612068176)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8251], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.775662899017334)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0108], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03775356709957123, max_val=0.037536535412073135)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8221], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.754465579986572)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0078], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02656526118516922, max_val=0.028186798095703125)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0205], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07200635969638824, max_val=0.07162733376026154)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8202], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.741058826446533)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0074], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02476481907069683, max_val=0.027188774198293686)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8222], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.755312442779541)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.021578827872872353, max_val=0.021207964047789574)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=7, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8304], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.813129425048828)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 13:50:20,351]: 
Model Weights:
[2025-05-14 13:50:20,351]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 13:50:20,351]: Sample Values (25 elements): [0.1050986796617508, -0.1898678094148636, 0.04116750508546829, 0.28531402349472046, -0.1263001561164856, -0.08279541879892349, -0.15267042815685272, -0.0892583355307579, -0.0811687707901001, -0.07912780344486237, 0.04923899471759796, 0.1862560510635376, 0.13210037350654602, -0.14227581024169922, 0.15214048326015472, -0.11668955534696579, 0.029191982001066208, 0.11487063020467758, 0.0031271358020603657, 0.15397533774375916, -0.21192102134227753, -0.09844785183668137, -0.03233896195888519, 0.13276301324367523, -0.07089712470769882]
[2025-05-14 13:50:20,351]: Mean: -0.00091397
[2025-05-14 13:50:20,351]: Min: -0.36641842
[2025-05-14 13:50:20,352]: Max: 0.37600067
[2025-05-14 13:50:20,352]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 13:50:20,352]: Sample Values (25 elements): [0.9695088863372803, 1.05107843875885, 0.9791708588600159, 0.977215588092804, 0.9452534317970276, 0.9655983448028564, 1.0470201969146729, 0.9769662618637085, 0.9134644865989685, 1.0061745643615723, 0.9627060294151306, 0.9373669624328613, 0.9286802411079407, 0.8668506741523743, 1.1174285411834717, 0.9421645998954773, 0.9317342638969421, 0.9735960960388184, 1.072117805480957, 1.0378261804580688, 0.9897814393043518, 0.9479495286941528, 0.8905197978019714, 1.0318117141723633, 1.0125261545181274]
[2025-05-14 13:50:20,352]: Mean: 0.97621942
[2025-05-14 13:50:20,352]: Min: 0.86084360
[2025-05-14 13:50:20,352]: Max: 1.27161348
[2025-05-14 13:50:20,353]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 13:50:20,354]: Sample Values (25 elements): [0.04562141373753548, 0.0, -0.04562141373753548, 0.0, -0.04562141373753548, -0.04562141373753548, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04562141373753548, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04562141373753548, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 13:50:20,354]: Mean: -0.00184149
[2025-05-14 13:50:20,354]: Min: -0.13686424
[2025-05-14 13:50:20,354]: Max: 0.18248565
[2025-05-14 13:50:20,354]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 13:50:20,355]: Sample Values (25 elements): [0.9648557901382446, 1.0085052251815796, 1.0012260675430298, 0.9315283894538879, 0.9406502842903137, 0.9914177656173706, 0.996823251247406, 0.9582770466804504, 1.0062856674194336, 0.9456132650375366, 0.9577552676200867, 0.973042368888855, 0.9747812747955322, 0.9333962202072144, 0.9880092144012451, 0.9930657744407654, 1.0009818077087402, 0.9356690645217896, 0.9514979124069214, 0.978468656539917, 0.9321550130844116, 1.0508692264556885, 0.9591200351715088, 0.9985138773918152, 1.0174256563186646]
[2025-05-14 13:50:20,355]: Mean: 0.97591180
[2025-05-14 13:50:20,355]: Min: 0.90988874
[2025-05-14 13:50:20,355]: Max: 1.12257564
[2025-05-14 13:50:20,356]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 13:50:20,357]: Sample Values (25 elements): [-0.031440503895282745, -0.031440503895282745, -0.031440503895282745, -0.031440503895282745, -0.031440503895282745, 0.0, -0.031440503895282745, 0.031440503895282745, -0.031440503895282745, -0.031440503895282745, 0.031440503895282745, -0.031440503895282745, 0.0, 0.031440503895282745, 0.0, -0.031440503895282745, 0.0, -0.031440503895282745, 0.031440503895282745, 0.031440503895282745, 0.0, -0.031440503895282745, 0.0, 0.031440503895282745, -0.031440503895282745]
[2025-05-14 13:50:20,357]: Mean: -0.00125714
[2025-05-14 13:50:20,357]: Min: -0.12576202
[2025-05-14 13:50:20,357]: Max: 0.09432151
[2025-05-14 13:50:20,357]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 13:50:20,357]: Sample Values (25 elements): [0.9845640063285828, 0.9651299715042114, 0.9285417795181274, 0.9845331907272339, 0.9541029930114746, 0.957504391670227, 0.9791575074195862, 0.9440605044364929, 0.9921061992645264, 0.9691619277000427, 0.9446651935577393, 1.0921357870101929, 1.0039899349212646, 0.9604302048683167, 0.9777590036392212, 0.9452999234199524, 0.9604650735855103, 0.9838553071022034, 0.9631655812263489, 0.9528058767318726, 0.9306198358535767, 0.9541203379631042, 0.964401364326477, 0.9862203598022461, 0.9424859285354614]
[2025-05-14 13:50:20,357]: Mean: 0.97332370
[2025-05-14 13:50:20,358]: Min: 0.91750890
[2025-05-14 13:50:20,358]: Max: 1.11530626
[2025-05-14 13:50:20,359]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 13:50:20,359]: Sample Values (25 elements): [-0.03421490639448166, 0.0, 0.03421490639448166, 0.0, -0.06842981278896332, -0.03421490639448166, 0.03421490639448166, 0.03421490639448166, 0.0, 0.0, 0.03421490639448166, -0.06842981278896332, 0.0, 0.0, 0.0, 0.0, 0.03421490639448166, 0.0, 0.0, -0.03421490639448166, 0.0, 0.0, 0.03421490639448166, 0.03421490639448166, -0.03421490639448166]
[2025-05-14 13:50:20,359]: Mean: -0.00102838
[2025-05-14 13:50:20,360]: Min: -0.10264472
[2025-05-14 13:50:20,360]: Max: 0.13685963
[2025-05-14 13:50:20,360]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 13:50:20,360]: Sample Values (25 elements): [0.9542243480682373, 0.9528099894523621, 0.9524390697479248, 0.9800905585289001, 0.9468971490859985, 0.956544041633606, 0.9509159922599792, 0.9605329632759094, 0.9434890747070312, 0.9649779200553894, 1.0427814722061157, 0.9552774429321289, 0.9788524508476257, 0.9710420370101929, 0.9875985980033875, 0.9748480319976807, 0.9614877700805664, 0.9667043685913086, 0.9687755703926086, 0.95267653465271, 0.9561672806739807, 0.9684804081916809, 0.9856777191162109, 0.9651141166687012, 0.9926809668540955]
[2025-05-14 13:50:20,360]: Mean: 0.96804142
[2025-05-14 13:50:20,360]: Min: 0.93951863
[2025-05-14 13:50:20,360]: Max: 1.04278147
[2025-05-14 13:50:20,361]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 13:50:20,362]: Sample Values (25 elements): [0.0, 0.026791289448738098, 0.0, 0.0, 0.026791289448738098, -0.053582578897476196, 0.026791289448738098, 0.0, 0.0, 0.026791289448738098, -0.053582578897476196, 0.026791289448738098, 0.0, 0.0, -0.053582578897476196, 0.0, 0.0, -0.026791289448738098, 0.026791289448738098, 0.0, -0.053582578897476196, 0.026791289448738098, -0.026791289448738098, -0.053582578897476196, -0.053582578897476196]
[2025-05-14 13:50:20,362]: Mean: -0.00021439
[2025-05-14 13:50:20,362]: Min: -0.08037387
[2025-05-14 13:50:20,362]: Max: 0.10716516
[2025-05-14 13:50:20,362]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 13:50:20,363]: Sample Values (25 elements): [0.9784479141235352, 0.9601483345031738, 0.9665908217430115, 0.9944697618484497, 0.9799149036407471, 1.0076508522033691, 0.9515703916549683, 0.9826657772064209, 0.9819455146789551, 0.9704500436782837, 0.9907684922218323, 1.000710368156433, 0.9660095572471619, 0.9529063105583191, 0.9701520800590515, 0.9961328506469727, 0.9768480658531189, 0.9969493746757507, 1.0062299966812134, 0.9800376892089844, 0.9732930660247803, 0.9861994385719299, 1.0115755796432495, 0.97954922914505, 0.9714273810386658]
[2025-05-14 13:50:20,363]: Mean: 0.98290831
[2025-05-14 13:50:20,363]: Min: 0.95157039
[2025-05-14 13:50:20,363]: Max: 1.02859521
[2025-05-14 13:50:20,364]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 13:50:20,365]: Sample Values (25 elements): [0.0, -0.04643799737095833, 0.0, 0.0, 0.023218998685479164, 0.0, 0.023218998685479164, 0.0, 0.023218998685479164, 0.0, -0.023218998685479164, 0.023218998685479164, 0.023218998685479164, 0.0, -0.023218998685479164, 0.0, -0.023218998685479164, 0.0, 0.023218998685479164, -0.023218998685479164, -0.023218998685479164, 0.023218998685479164, 0.023218998685479164, 0.0, 0.023218998685479164]
[2025-05-14 13:50:20,365]: Mean: -0.00088841
[2025-05-14 13:50:20,365]: Min: -0.06965700
[2025-05-14 13:50:20,365]: Max: 0.09287599
[2025-05-14 13:50:20,365]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 13:50:20,366]: Sample Values (25 elements): [0.9491548538208008, 0.9713056683540344, 0.9687395095825195, 0.9690366983413696, 0.959740400314331, 0.9581469297409058, 0.9600415229797363, 0.9609085917472839, 0.9673156142234802, 0.9578853249549866, 0.9935145974159241, 0.9599646925926208, 0.9662013649940491, 0.9554299712181091, 0.9636592864990234, 0.9603077173233032, 0.9565014243125916, 0.9696592688560486, 0.9547364711761475, 0.9742693901062012, 0.9671027064323425, 0.9603671431541443, 0.974776566028595, 0.963049054145813, 0.9501376152038574]
[2025-05-14 13:50:20,366]: Mean: 0.96506262
[2025-05-14 13:50:20,366]: Min: 0.94453228
[2025-05-14 13:50:20,366]: Max: 0.99351460
[2025-05-14 13:50:20,367]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 13:50:20,369]: Sample Values (25 elements): [-0.0384095124900341, 0.0, -0.01920475624501705, -0.01920475624501705, 0.0, 0.01920475624501705, 0.0, 0.0, -0.01920475624501705, 0.0, 0.0, 0.01920475624501705, 0.0, 0.01920475624501705, -0.01920475624501705, -0.01920475624501705, -0.01920475624501705, 0.0, -0.057614266872406006, 0.0, 0.0, -0.01920475624501705, 0.0384095124900341, -0.01920475624501705, 0.0]
[2025-05-14 13:50:20,369]: Mean: -0.00059702
[2025-05-14 13:50:20,369]: Min: -0.05761427
[2025-05-14 13:50:20,369]: Max: 0.07681902
[2025-05-14 13:50:20,369]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 13:50:20,369]: Sample Values (25 elements): [0.9829334020614624, 0.9786208868026733, 0.9456236958503723, 0.9690154790878296, 0.9501878023147583, 0.9580638408660889, 0.9652955532073975, 0.9460340142250061, 0.957838773727417, 0.9592329263687134, 0.9611605405807495, 0.9472585320472717, 0.9595342874526978, 0.9690601825714111, 0.9614338278770447, 0.94594806432724, 0.9713128209114075, 1.000691533088684, 0.9758379459381104, 0.9596518278121948, 0.9604076743125916, 0.9493190050125122, 0.9609500765800476, 0.9625864624977112, 0.9692592620849609]
[2025-05-14 13:50:20,370]: Mean: 0.96587598
[2025-05-14 13:50:20,370]: Min: 0.94562370
[2025-05-14 13:50:20,370]: Max: 1.00069153
[2025-05-14 13:50:20,371]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 13:50:20,371]: Sample Values (25 elements): [-0.090591661632061, 0.0452958308160305, -0.0452958308160305, 0.0452958308160305, 0.0, 0.0, 0.0452958308160305, -0.0452958308160305, 0.090591661632061, 0.0452958308160305, 0.090591661632061, -0.090591661632061, -0.090591661632061, 0.0, -0.0452958308160305, 0.0452958308160305, -0.090591661632061, 0.0, -0.0452958308160305, 0.0, 0.0452958308160305, -0.090591661632061, -0.1358874887228012, 0.1358874887228012, 0.0452958308160305]
[2025-05-14 13:50:20,371]: Mean: -0.00102845
[2025-05-14 13:50:20,371]: Min: -0.18118332
[2025-05-14 13:50:20,372]: Max: 0.13588749
[2025-05-14 13:50:20,372]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 13:50:20,372]: Sample Values (25 elements): [0.923742949962616, 0.9325037002563477, 0.9317192435264587, 0.9312568306922913, 0.9388930201530457, 0.9285750389099121, 0.9346745014190674, 0.9364319443702698, 0.9340526461601257, 0.9356005787849426, 0.9313504099845886, 0.924126386642456, 0.9351935386657715, 0.9397210478782654, 0.9389738440513611, 0.9418132305145264, 0.9234486222267151, 0.9218169450759888, 0.9267908930778503, 0.9490799903869629, 0.9495708346366882, 0.9283245801925659, 0.9352176785469055, 0.9321845769882202, 0.9486767649650574]
[2025-05-14 13:50:20,372]: Mean: 0.93461448
[2025-05-14 13:50:20,372]: Min: 0.91706920
[2025-05-14 13:50:20,372]: Max: 0.95274514
[2025-05-14 13:50:20,373]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 13:50:20,375]: Sample Values (25 elements): [0.0, -0.03858809173107147, -0.019294045865535736, 0.0, 0.0, 0.0, 0.019294045865535736, 0.019294045865535736, -0.019294045865535736, 0.019294045865535736, -0.019294045865535736, 0.019294045865535736, 0.0, 0.0, -0.019294045865535736, 0.03858809173107147, -0.019294045865535736, 0.019294045865535736, 0.019294045865535736, -0.019294045865535736, 0.019294045865535736, -0.019294045865535736, 0.03858809173107147, 0.0, -0.019294045865535736]
[2025-05-14 13:50:20,375]: Mean: -0.00053359
[2025-05-14 13:50:20,375]: Min: -0.05788214
[2025-05-14 13:50:20,375]: Max: 0.07717618
[2025-05-14 13:50:20,375]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 13:50:20,375]: Sample Values (25 elements): [0.9729830026626587, 0.9629273414611816, 0.9815182089805603, 0.9679810404777527, 0.9625437259674072, 0.9750908613204956, 0.9738783240318298, 0.958670437335968, 0.9801802635192871, 0.9672802686691284, 0.9635790586471558, 0.9586559534072876, 0.9661012291908264, 0.9629648923873901, 0.961298942565918, 0.9706011414527893, 0.9493768215179443, 0.9696692228317261, 0.9646729230880737, 0.9709111452102661, 0.9607910513877869, 0.9742559194564819, 0.9693566560745239, 0.9657283425331116, 0.9620785713195801]
[2025-05-14 13:50:20,376]: Mean: 0.96375299
[2025-05-14 13:50:20,376]: Min: 0.94167221
[2025-05-14 13:50:20,376]: Max: 0.98897666
[2025-05-14 13:50:20,377]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 13:50:20,378]: Sample Values (25 elements): [0.018767181783914566, 0.018767181783914566, 0.03753436356782913, -0.018767181783914566, 0.0, 0.0, 0.018767181783914566, 0.0, 0.018767181783914566, 0.0, 0.0, 0.0, 0.0, 0.018767181783914566, 0.0, -0.018767181783914566, 0.0, 0.018767181783914566, 0.0, 0.0, -0.018767181783914566, 0.018767181783914566, -0.018767181783914566, 0.0, -0.018767181783914566]
[2025-05-14 13:50:20,378]: Mean: -0.00004035
[2025-05-14 13:50:20,378]: Min: -0.05630155
[2025-05-14 13:50:20,379]: Max: 0.07506873
[2025-05-14 13:50:20,379]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 13:50:20,379]: Sample Values (25 elements): [0.9801395535469055, 0.9920624494552612, 0.985101580619812, 0.9854462742805481, 0.9820539355278015, 0.9902923703193665, 0.9803488850593567, 0.9781222343444824, 0.9849205017089844, 0.9694185853004456, 0.9805054068565369, 0.9838505387306213, 0.9826042056083679, 0.986822783946991, 0.9797609448432922, 0.9844561219215393, 0.974443256855011, 0.9854927062988281, 1.0027631521224976, 0.9808831810951233, 0.9749346971511841, 1.0063616037368774, 0.9793900847434998, 0.9857089519500732, 0.9923787117004395]
[2025-05-14 13:50:20,379]: Mean: 0.98307735
[2025-05-14 13:50:20,379]: Min: 0.95885766
[2025-05-14 13:50:20,379]: Max: 1.01405466
[2025-05-14 13:50:20,380]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 13:50:20,383]: Sample Values (25 elements): [0.0, -0.015711747109889984, 0.03142349421977997, 0.0, 0.03142349421977997, -0.015711747109889984, -0.015711747109889984, 0.015711747109889984, 0.015711747109889984, -0.03142349421977997, -0.015711747109889984, -0.03142349421977997, 0.0, 0.015711747109889984, 0.015711747109889984, 0.015711747109889984, 0.0, 0.015711747109889984, 0.0, -0.015711747109889984, 0.015711747109889984, 0.015711747109889984, -0.015711747109889984, 0.0, -0.015711747109889984]
[2025-05-14 13:50:20,383]: Mean: -0.00012802
[2025-05-14 13:50:20,383]: Min: -0.04713524
[2025-05-14 13:50:20,383]: Max: 0.06284699
[2025-05-14 13:50:20,383]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 13:50:20,384]: Sample Values (25 elements): [0.9664573669433594, 0.9711950421333313, 0.9617841839790344, 0.9542301893234253, 0.9633603096008301, 0.9582878351211548, 0.9563543200492859, 0.9638611078262329, 0.9617602825164795, 0.9540658593177795, 0.9586729407310486, 0.9542198777198792, 0.9629984498023987, 0.9647525548934937, 0.9620423316955566, 0.9632646441459656, 0.966280460357666, 0.9569929838180542, 0.9552195072174072, 0.9701249599456787, 0.9629166722297668, 0.9647680521011353, 0.9591149687767029, 0.9503628015518188, 0.9595125913619995]
[2025-05-14 13:50:20,384]: Mean: 0.96254849
[2025-05-14 13:50:20,384]: Min: 0.94760108
[2025-05-14 13:50:20,384]: Max: 0.98027098
[2025-05-14 13:50:20,385]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 13:50:20,391]: Sample Values (25 elements): [0.013555089011788368, 0.013555089011788368, 0.0, 0.0, 0.0, -0.013555089011788368, -0.013555089011788368, -0.013555089011788368, -0.013555089011788368, 0.013555089011788368, 0.0, 0.0, 0.0, 0.013555089011788368, 0.0, 0.0, -0.013555089011788368, -0.013555089011788368, 0.013555089011788368, -0.027110178023576736, 0.027110178023576736, 0.013555089011788368, 0.013555089011788368, 0.0, 0.027110178023576736]
[2025-05-14 13:50:20,392]: Mean: -0.00036548
[2025-05-14 13:50:20,392]: Min: -0.04066527
[2025-05-14 13:50:20,392]: Max: 0.05422036
[2025-05-14 13:50:20,392]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 13:50:20,392]: Sample Values (25 elements): [0.9734318256378174, 0.9705525636672974, 0.972998321056366, 0.9675524234771729, 0.9689962267875671, 0.9646729230880737, 0.9733083248138428, 0.970135509967804, 0.9569875001907349, 0.9619565606117249, 0.9776661992073059, 0.9701149463653564, 0.96926349401474, 0.9645189046859741, 0.9607325792312622, 0.9605916738510132, 0.966652512550354, 0.9669066667556763, 0.9707738161087036, 0.9682735800743103, 0.963719367980957, 0.9650563597679138, 0.9620334506034851, 0.9695911407470703, 0.9706574082374573]
[2025-05-14 13:50:20,392]: Mean: 0.96826369
[2025-05-14 13:50:20,393]: Min: 0.95191336
[2025-05-14 13:50:20,393]: Max: 1.00313187
[2025-05-14 13:50:20,394]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 13:50:20,394]: Sample Values (25 elements): [-0.0618143267929554, 0.09272149205207825, 0.0, 0.0618143267929554, -0.09272149205207825, -0.0309071633964777, 0.0309071633964777, 0.0, 0.0618143267929554, -0.0309071633964777, 0.0, 0.0618143267929554, 0.0618143267929554, 0.0618143267929554, 0.0618143267929554, 0.0, 0.0, -0.0618143267929554, 0.0618143267929554, -0.0309071633964777, -0.0309071633964777, 0.0309071633964777, 0.0618143267929554, 0.0309071633964777, 0.0618143267929554]
[2025-05-14 13:50:20,394]: Mean: -0.00010941
[2025-05-14 13:50:20,394]: Min: -0.12362865
[2025-05-14 13:50:20,395]: Max: 0.09272149
[2025-05-14 13:50:20,395]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 13:50:20,395]: Sample Values (25 elements): [0.9430728554725647, 0.9338303208351135, 0.9415618181228638, 0.9383097887039185, 0.9431375861167908, 0.9428137540817261, 0.93794184923172, 0.9381375908851624, 0.9377509355545044, 0.9456838369369507, 0.9460239410400391, 0.9494178295135498, 0.9468981623649597, 0.9368975758552551, 0.9336410164833069, 0.9405714869499207, 0.9522290229797363, 0.9325812458992004, 0.9440978169441223, 0.9399998188018799, 0.9447165131568909, 0.9308938384056091, 0.9391114711761475, 0.9352953433990479, 0.9390916228294373]
[2025-05-14 13:50:20,395]: Mean: 0.93924344
[2025-05-14 13:50:20,395]: Min: 0.92380321
[2025-05-14 13:50:20,395]: Max: 0.95294690
[2025-05-14 13:50:20,396]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 13:50:20,402]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.02750500664114952, -0.01375250332057476, 0.01375250332057476, 0.0, 0.02750500664114952, -0.01375250332057476, 0.02750500664114952, -0.01375250332057476, 0.0, -0.01375250332057476, 0.01375250332057476, 0.02750500664114952, -0.02750500664114952, 0.0, 0.01375250332057476, 0.0, 0.0, 0.0, 0.01375250332057476, 0.02750500664114952, -0.01375250332057476]
[2025-05-14 13:50:20,402]: Mean: -0.00040323
[2025-05-14 13:50:20,402]: Min: -0.04125751
[2025-05-14 13:50:20,403]: Max: 0.05501001
[2025-05-14 13:50:20,403]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 13:50:20,403]: Sample Values (25 elements): [0.9603694677352905, 0.9555306434631348, 0.9631297588348389, 0.9622047543525696, 0.9679111242294312, 0.9626112580299377, 0.9673805832862854, 0.9596045613288879, 0.9574965238571167, 0.9628812074661255, 0.9592679142951965, 0.9647105932235718, 0.9731466770172119, 0.957831621170044, 0.9570895433425903, 0.955868661403656, 0.9643542170524597, 0.9637170433998108, 0.957688570022583, 0.9678936004638672, 0.9568777084350586, 0.9636125564575195, 0.9620614647865295, 0.9637782573699951, 0.959352433681488]
[2025-05-14 13:50:20,403]: Mean: 0.96218395
[2025-05-14 13:50:20,403]: Min: 0.94626158
[2025-05-14 13:50:20,403]: Max: 0.97673881
[2025-05-14 13:50:20,404]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 13:50:20,410]: Sample Values (25 elements): [0.023141806945204735, 0.0, 0.011570903472602367, -0.023141806945204735, 0.011570903472602367, -0.011570903472602367, -0.011570903472602367, -0.011570903472602367, 0.0, -0.011570903472602367, 0.011570903472602367, 0.011570903472602367, -0.011570903472602367, -0.011570903472602367, -0.03471270948648453, -0.011570903472602367, 0.011570903472602367, 0.0, 0.011570903472602367, 0.0, -0.023141806945204735, -0.011570903472602367, 0.0, 0.011570903472602367, 0.0]
[2025-05-14 13:50:20,410]: Mean: -0.00011368
[2025-05-14 13:50:20,410]: Min: -0.04628361
[2025-05-14 13:50:20,411]: Max: 0.03471271
[2025-05-14 13:50:20,411]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 13:50:20,411]: Sample Values (25 elements): [0.9693522453308105, 0.9766215682029724, 0.9778082966804504, 0.9805311560630798, 0.9665574431419373, 0.9682237505912781, 0.9849678874015808, 0.970431923866272, 0.9756377339363098, 0.9714881181716919, 0.9818437695503235, 0.9756913781166077, 0.966195821762085, 0.9746310114860535, 0.9723915457725525, 0.9641609191894531, 0.9737324118614197, 0.9909245371818542, 0.9739673137664795, 0.9720167517662048, 0.9732916355133057, 0.9785851240158081, 0.9790964722633362, 0.9740788340568542, 0.9705426692962646]
[2025-05-14 13:50:20,411]: Mean: 0.97340864
[2025-05-14 13:50:20,411]: Min: 0.95940995
[2025-05-14 13:50:20,411]: Max: 0.99214470
[2025-05-14 13:50:20,412]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 13:50:20,426]: Sample Values (25 elements): [0.021511437371373177, -0.021511437371373177, 0.0, 0.010755718685686588, 0.010755718685686588, -0.010755718685686588, 0.0, 0.021511437371373177, 0.021511437371373177, -0.021511437371373177, -0.010755718685686588, 0.021511437371373177, -0.010755718685686588, 0.010755718685686588, 0.021511437371373177, 0.021511437371373177, 0.0, 0.0, 0.010755718685686588, 0.010755718685686588, 0.010755718685686588, 0.0, 0.010755718685686588, 0.0, 0.010755718685686588]
[2025-05-14 13:50:20,426]: Mean: -0.00000996
[2025-05-14 13:50:20,426]: Min: -0.04302287
[2025-05-14 13:50:20,427]: Max: 0.03226716
[2025-05-14 13:50:20,427]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 13:50:20,427]: Sample Values (25 elements): [0.9606849551200867, 0.9583798050880432, 0.9587603211402893, 0.9614344835281372, 0.9598853588104248, 0.9630168676376343, 0.9620518088340759, 0.9596533179283142, 0.9585253596305847, 0.9624975323677063, 0.9617125391960144, 0.9576798677444458, 0.9617889523506165, 0.9580046534538269, 0.9567877650260925, 0.9617462754249573, 0.9611849784851074, 0.9603809118270874, 0.9558832049369812, 0.9623558521270752, 0.9625120162963867, 0.9599035382270813, 0.9552818536758423, 0.9630407094955444, 0.9620049595832825]
[2025-05-14 13:50:20,427]: Mean: 0.96012402
[2025-05-14 13:50:20,427]: Min: 0.95478714
[2025-05-14 13:50:20,427]: Max: 0.96872818
[2025-05-14 13:50:20,429]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 13:50:20,469]: Sample Values (25 elements): [0.0, -0.007821737788617611, -0.007821737788617611, 0.007821737788617611, 0.0, 0.0, 0.007821737788617611, -0.007821737788617611, 0.0, 0.007821737788617611, 0.0, 0.0, 0.0, -0.007821737788617611, -0.007821737788617611, -0.007821737788617611, 0.007821737788617611, 0.0, -0.007821737788617611, 0.015643475577235222, 0.0, -0.007821737788617611, 0.007821737788617611, 0.0, -0.007821737788617611]
[2025-05-14 13:50:20,470]: Mean: -0.00003505
[2025-05-14 13:50:20,470]: Min: -0.02346521
[2025-05-14 13:50:20,470]: Max: 0.03128695
[2025-05-14 13:50:20,470]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 13:50:20,471]: Sample Values (25 elements): [0.9644219279289246, 0.9651946425437927, 0.9606865048408508, 0.9684255123138428, 0.9640113711357117, 0.9649589657783508, 0.9623993039131165, 0.9610678553581238, 0.9628554582595825, 0.9633415937423706, 0.9663479328155518, 0.9676714539527893, 0.9661316275596619, 0.9717323780059814, 0.9632959365844727, 0.9655638933181763, 0.9632980823516846, 0.9629932641983032, 0.9615814685821533, 0.9620724320411682, 0.9621688723564148, 0.9614152312278748, 0.9632461071014404, 0.9694609045982361, 0.9645304679870605]
[2025-05-14 13:50:20,472]: Mean: 0.96457249
[2025-05-14 13:50:20,472]: Min: 0.95802432
[2025-05-14 13:50:20,472]: Max: 0.97833925
[2025-05-14 13:50:20,473]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 13:50:20,474]: Sample Values (25 elements): [0.020519087091088295, -0.061557263135910034, 0.0, -0.061557263135910034, -0.04103817418217659, -0.020519087091088295, 0.020519087091088295, 0.061557263135910034, 0.0, 0.020519087091088295, -0.04103817418217659, -0.04103817418217659, -0.04103817418217659, -0.061557263135910034, -0.020519087091088295, 0.061557263135910034, -0.020519087091088295, 0.04103817418217659, 0.0, 0.04103817418217659, -0.020519087091088295, 0.0, -0.020519087091088295, -0.04103817418217659, -0.020519087091088295]
[2025-05-14 13:50:20,474]: Mean: 0.00006231
[2025-05-14 13:50:20,475]: Min: -0.08207635
[2025-05-14 13:50:20,475]: Max: 0.06155726
[2025-05-14 13:50:20,475]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 13:50:20,475]: Sample Values (25 elements): [0.9552275538444519, 0.955479621887207, 0.9545512199401855, 0.9535353183746338, 0.9545642137527466, 0.9554224014282227, 0.9608860015869141, 0.9559620022773743, 0.9554935693740845, 0.9521274566650391, 0.9574564695358276, 0.9567570090293884, 0.9568694233894348, 0.9547260403633118, 0.9528756737709045, 0.9546532034873962, 0.955375611782074, 0.9551515579223633, 0.9560313820838928, 0.9561638236045837, 0.9559210538864136, 0.9544961452484131, 0.9550551176071167, 0.9541128277778625, 0.9582459926605225]
[2025-05-14 13:50:20,475]: Mean: 0.95597255
[2025-05-14 13:50:20,475]: Min: 0.95118046
[2025-05-14 13:50:20,475]: Max: 0.96093732
[2025-05-14 13:50:20,476]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 13:50:20,518]: Sample Values (25 elements): [-0.014843915589153767, 0.007421957794576883, 0.007421957794576883, 0.0, 0.007421957794576883, 0.007421957794576883, 0.007421957794576883, -0.007421957794576883, 0.0, 0.0, 0.007421957794576883, 0.014843915589153767, -0.014843915589153767, -0.007421957794576883, 0.007421957794576883, 0.0, -0.014843915589153767, 0.0, 0.007421957794576883, -0.007421957794576883, 0.0, 0.0, 0.007421957794576883, 0.0, 0.007421957794576883]
[2025-05-14 13:50:20,518]: Mean: -0.00010193
[2025-05-14 13:50:20,518]: Min: -0.02226587
[2025-05-14 13:50:20,518]: Max: 0.02968783
[2025-05-14 13:50:20,518]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 13:50:20,519]: Sample Values (25 elements): [0.9564505815505981, 0.9598580002784729, 0.9572511315345764, 0.9595397710800171, 0.9652689099311829, 0.9578505158424377, 0.9556018710136414, 0.957800567150116, 0.9578709006309509, 0.9572884440422058, 0.9583227634429932, 0.9565960168838501, 0.9585810303688049, 0.9604626297950745, 0.9623520374298096, 0.9597748517990112, 0.9574726223945618, 0.9583277106285095, 0.9582477807998657, 0.958968997001648, 0.9614427089691162, 0.9564754366874695, 0.9566352963447571, 0.9610732197761536, 0.957587718963623]
[2025-05-14 13:50:20,519]: Mean: 0.95921445
[2025-05-14 13:50:20,519]: Min: 0.95478475
[2025-05-14 13:50:20,519]: Max: 0.96929896
[2025-05-14 13:50:20,520]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 13:50:20,558]: Sample Values (25 elements): [0.0, -0.006112403701990843, 0.0, -0.012224807403981686, 0.0, -0.006112403701990843, 0.0, 0.006112403701990843, 0.0, -0.006112403701990843, 0.006112403701990843, -0.012224807403981686, 0.012224807403981686, 0.0, -0.012224807403981686, -0.012224807403981686, 0.006112403701990843, -0.006112403701990843, 0.0, 0.0, -0.006112403701990843, 0.0, -0.012224807403981686, 0.006112403701990843, 0.006112403701990843]
[2025-05-14 13:50:20,558]: Mean: 0.00004361
[2025-05-14 13:50:20,558]: Min: -0.02444961
[2025-05-14 13:50:20,559]: Max: 0.01833721
[2025-05-14 13:50:20,559]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 13:50:20,559]: Sample Values (25 elements): [0.9663127660751343, 0.9739435315132141, 0.9687501192092896, 0.9656931757926941, 0.9675236940383911, 0.9647864103317261, 0.9713078737258911, 0.9688340425491333, 0.96662837266922, 0.970553457736969, 0.973899781703949, 0.9691771864891052, 0.9724909067153931, 0.9680652618408203, 0.9651849865913391, 0.967985212802887, 0.9674996137619019, 0.966373085975647, 0.9663042426109314, 0.966224193572998, 0.9698495864868164, 0.9656437635421753, 0.9694955945014954, 0.9687798023223877, 0.9684712886810303]
[2025-05-14 13:50:20,559]: Mean: 0.96864831
[2025-05-14 13:50:20,559]: Min: 0.96266061
[2025-05-14 13:50:20,559]: Max: 0.97588110
[2025-05-14 13:50:20,559]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 13:50:20,560]: Sample Values (25 elements): [-0.02664024569094181, -0.0805428996682167, -0.01170699018985033, -0.07633554935455322, -0.04577791690826416, 0.06893372535705566, -0.0705234706401825, 0.10421421378850937, -0.02857409231364727, 0.09104780852794647, 0.0003887243219651282, 0.0730210691690445, -0.05650043115019798, -0.03542051091790199, -0.027018029242753983, 0.04375827684998512, 0.007954672910273075, 0.08467988669872284, 0.018963927403092384, 0.013996393419802189, -0.03996143117547035, 0.041315626353025436, 0.10348344594240189, 0.03799626603722572, 0.01871209405362606]
[2025-05-14 13:50:20,560]: Mean: -0.00054006
[2025-05-14 13:50:20,560]: Min: -0.12965445
[2025-05-14 13:50:20,560]: Max: 0.15075518
[2025-05-14 13:50:20,560]: 


QAT of ResNet18 with parametrized_relu down to 2 bits...
[2025-05-14 13:50:20,788]: [ResNet18_parametrized_relu_quantized_2_bits] after configure_qat:
[2025-05-14 13:50:20,832]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 13:52:03,444]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 001 Train Loss: 0.9068 Train Acc: 0.6923 Eval Loss: 0.7455 Eval Acc: 0.7468 (LR: 0.001000)
[2025-05-14 13:53:45,960]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 002 Train Loss: 0.6365 Train Acc: 0.7767 Eval Loss: 0.6067 Eval Acc: 0.7962 (LR: 0.001000)
[2025-05-14 13:55:28,264]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 003 Train Loss: 0.5606 Train Acc: 0.8029 Eval Loss: 0.6343 Eval Acc: 0.7953 (LR: 0.001000)
[2025-05-14 13:57:10,596]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 004 Train Loss: 0.5261 Train Acc: 0.8158 Eval Loss: 0.5773 Eval Acc: 0.8095 (LR: 0.001000)
[2025-05-14 13:58:52,918]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 005 Train Loss: 0.4996 Train Acc: 0.8246 Eval Loss: 0.5537 Eval Acc: 0.8163 (LR: 0.001000)
[2025-05-14 14:00:35,055]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 006 Train Loss: 0.4778 Train Acc: 0.8312 Eval Loss: 0.5554 Eval Acc: 0.8184 (LR: 0.001000)
[2025-05-14 14:02:17,368]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 007 Train Loss: 0.4555 Train Acc: 0.8394 Eval Loss: 0.5196 Eval Acc: 0.8299 (LR: 0.001000)
[2025-05-14 14:03:59,738]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 008 Train Loss: 0.4428 Train Acc: 0.8451 Eval Loss: 0.5218 Eval Acc: 0.8289 (LR: 0.001000)
[2025-05-14 14:05:42,239]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 009 Train Loss: 0.4338 Train Acc: 0.8473 Eval Loss: 0.5258 Eval Acc: 0.8232 (LR: 0.001000)
[2025-05-14 14:07:24,763]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 010 Train Loss: 0.4162 Train Acc: 0.8531 Eval Loss: 0.5300 Eval Acc: 0.8284 (LR: 0.001000)
[2025-05-14 14:09:07,326]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 011 Train Loss: 0.4099 Train Acc: 0.8560 Eval Loss: 0.5184 Eval Acc: 0.8289 (LR: 0.001000)
[2025-05-14 14:10:49,846]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 012 Train Loss: 0.3982 Train Acc: 0.8588 Eval Loss: 0.4949 Eval Acc: 0.8385 (LR: 0.001000)
[2025-05-14 14:12:32,591]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 013 Train Loss: 0.3893 Train Acc: 0.8633 Eval Loss: 0.5210 Eval Acc: 0.8292 (LR: 0.001000)
[2025-05-14 14:14:15,116]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 014 Train Loss: 0.3777 Train Acc: 0.8672 Eval Loss: 0.4791 Eval Acc: 0.8468 (LR: 0.001000)
[2025-05-14 14:15:57,830]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 015 Train Loss: 0.3798 Train Acc: 0.8652 Eval Loss: 0.5245 Eval Acc: 0.8361 (LR: 0.001000)
[2025-05-14 14:17:40,334]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 016 Train Loss: 0.3624 Train Acc: 0.8705 Eval Loss: 0.5163 Eval Acc: 0.8354 (LR: 0.001000)
[2025-05-14 14:19:22,835]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 017 Train Loss: 0.3613 Train Acc: 0.8742 Eval Loss: 0.5214 Eval Acc: 0.8372 (LR: 0.001000)
[2025-05-14 14:21:05,190]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 018 Train Loss: 0.3551 Train Acc: 0.8747 Eval Loss: 0.4806 Eval Acc: 0.8456 (LR: 0.001000)
[2025-05-14 14:22:47,518]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 019 Train Loss: 0.3525 Train Acc: 0.8758 Eval Loss: 0.4501 Eval Acc: 0.8543 (LR: 0.001000)
[2025-05-14 14:24:30,163]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 020 Train Loss: 0.3433 Train Acc: 0.8788 Eval Loss: 0.4964 Eval Acc: 0.8407 (LR: 0.001000)
[2025-05-14 14:26:12,581]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 021 Train Loss: 0.3466 Train Acc: 0.8775 Eval Loss: 0.4682 Eval Acc: 0.8464 (LR: 0.001000)
[2025-05-14 14:27:54,888]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 022 Train Loss: 0.3391 Train Acc: 0.8793 Eval Loss: 0.4515 Eval Acc: 0.8535 (LR: 0.001000)
[2025-05-14 14:29:37,225]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 023 Train Loss: 0.3341 Train Acc: 0.8825 Eval Loss: 0.4404 Eval Acc: 0.8607 (LR: 0.001000)
[2025-05-14 14:31:19,562]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 024 Train Loss: 0.3251 Train Acc: 0.8848 Eval Loss: 0.4457 Eval Acc: 0.8562 (LR: 0.001000)
[2025-05-14 14:33:01,894]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 025 Train Loss: 0.3277 Train Acc: 0.8845 Eval Loss: 0.4897 Eval Acc: 0.8434 (LR: 0.001000)
[2025-05-14 14:34:44,217]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 026 Train Loss: 0.3214 Train Acc: 0.8866 Eval Loss: 0.4452 Eval Acc: 0.8581 (LR: 0.001000)
[2025-05-14 14:36:26,540]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 027 Train Loss: 0.3201 Train Acc: 0.8850 Eval Loss: 0.4438 Eval Acc: 0.8601 (LR: 0.001000)
[2025-05-14 14:38:08,885]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 028 Train Loss: 0.3137 Train Acc: 0.8886 Eval Loss: 0.4908 Eval Acc: 0.8440 (LR: 0.001000)
[2025-05-14 14:39:51,213]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 029 Train Loss: 0.3126 Train Acc: 0.8888 Eval Loss: 0.4574 Eval Acc: 0.8550 (LR: 0.001000)
[2025-05-14 14:41:33,300]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 030 Train Loss: 0.3085 Train Acc: 0.8898 Eval Loss: 0.4517 Eval Acc: 0.8584 (LR: 0.000250)
[2025-05-14 14:43:15,447]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 031 Train Loss: 0.2713 Train Acc: 0.9035 Eval Loss: 0.3909 Eval Acc: 0.8754 (LR: 0.000250)
[2025-05-14 14:44:57,781]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 032 Train Loss: 0.2626 Train Acc: 0.9084 Eval Loss: 0.4101 Eval Acc: 0.8686 (LR: 0.000250)
[2025-05-14 14:46:39,901]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 033 Train Loss: 0.2575 Train Acc: 0.9085 Eval Loss: 0.4190 Eval Acc: 0.8699 (LR: 0.000250)
[2025-05-14 14:48:22,030]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 034 Train Loss: 0.2590 Train Acc: 0.9078 Eval Loss: 0.4034 Eval Acc: 0.8703 (LR: 0.000250)
[2025-05-14 14:50:04,342]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 035 Train Loss: 0.2545 Train Acc: 0.9094 Eval Loss: 0.4013 Eval Acc: 0.8725 (LR: 0.000250)
[2025-05-14 14:51:46,450]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 036 Train Loss: 0.2528 Train Acc: 0.9100 Eval Loss: 0.4054 Eval Acc: 0.8700 (LR: 0.000250)
[2025-05-14 14:53:28,759]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 037 Train Loss: 0.2526 Train Acc: 0.9095 Eval Loss: 0.4110 Eval Acc: 0.8694 (LR: 0.000250)
[2025-05-14 14:55:11,112]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 038 Train Loss: 0.2491 Train Acc: 0.9106 Eval Loss: 0.4025 Eval Acc: 0.8726 (LR: 0.000250)
[2025-05-14 14:56:53,422]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 039 Train Loss: 0.2519 Train Acc: 0.9107 Eval Loss: 0.4096 Eval Acc: 0.8713 (LR: 0.000250)
[2025-05-14 14:58:36,068]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 040 Train Loss: 0.2476 Train Acc: 0.9114 Eval Loss: 0.3995 Eval Acc: 0.8717 (LR: 0.000250)
[2025-05-14 15:00:18,487]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 041 Train Loss: 0.2509 Train Acc: 0.9127 Eval Loss: 0.4189 Eval Acc: 0.8675 (LR: 0.000250)
[2025-05-14 15:02:00,802]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 042 Train Loss: 0.2478 Train Acc: 0.9126 Eval Loss: 0.4028 Eval Acc: 0.8758 (LR: 0.000250)
[2025-05-14 15:03:43,141]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 043 Train Loss: 0.2490 Train Acc: 0.9118 Eval Loss: 0.4047 Eval Acc: 0.8752 (LR: 0.000250)
[2025-05-14 15:05:25,270]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 044 Train Loss: 0.2431 Train Acc: 0.9129 Eval Loss: 0.4002 Eval Acc: 0.8731 (LR: 0.000250)
[2025-05-14 15:07:07,421]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 045 Train Loss: 0.2449 Train Acc: 0.9139 Eval Loss: 0.4077 Eval Acc: 0.8702 (LR: 0.000063)
[2025-05-14 15:08:49,741]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 046 Train Loss: 0.2300 Train Acc: 0.9189 Eval Loss: 0.3791 Eval Acc: 0.8792 (LR: 0.000063)
[2025-05-14 15:10:32,052]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 047 Train Loss: 0.2286 Train Acc: 0.9182 Eval Loss: 0.3903 Eval Acc: 0.8799 (LR: 0.000063)
[2025-05-14 15:12:14,388]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 048 Train Loss: 0.2298 Train Acc: 0.9182 Eval Loss: 0.3868 Eval Acc: 0.8781 (LR: 0.000063)
[2025-05-14 15:13:56,696]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 049 Train Loss: 0.2271 Train Acc: 0.9199 Eval Loss: 0.3975 Eval Acc: 0.8747 (LR: 0.000063)
[2025-05-14 15:15:38,861]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 050 Train Loss: 0.2259 Train Acc: 0.9187 Eval Loss: 0.3899 Eval Acc: 0.8755 (LR: 0.000063)
[2025-05-14 15:17:20,996]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 051 Train Loss: 0.2275 Train Acc: 0.9183 Eval Loss: 0.3992 Eval Acc: 0.8756 (LR: 0.000063)
[2025-05-14 15:19:03,334]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 052 Train Loss: 0.2267 Train Acc: 0.9187 Eval Loss: 0.3932 Eval Acc: 0.8769 (LR: 0.000063)
[2025-05-14 15:20:45,640]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 053 Train Loss: 0.2284 Train Acc: 0.9175 Eval Loss: 0.3899 Eval Acc: 0.8769 (LR: 0.000063)
[2025-05-14 15:22:27,960]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 054 Train Loss: 0.2268 Train Acc: 0.9168 Eval Loss: 0.3941 Eval Acc: 0.8763 (LR: 0.000063)
[2025-05-14 15:24:10,293]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 055 Train Loss: 0.2265 Train Acc: 0.9195 Eval Loss: 0.4044 Eval Acc: 0.8753 (LR: 0.000063)
[2025-05-14 15:25:52,632]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 056 Train Loss: 0.2270 Train Acc: 0.9194 Eval Loss: 0.3924 Eval Acc: 0.8793 (LR: 0.000063)
[2025-05-14 15:27:34,743]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 057 Train Loss: 0.2244 Train Acc: 0.9213 Eval Loss: 0.3807 Eval Acc: 0.8782 (LR: 0.000063)
[2025-05-14 15:29:17,079]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 058 Train Loss: 0.2220 Train Acc: 0.9209 Eval Loss: 0.3890 Eval Acc: 0.8807 (LR: 0.000063)
[2025-05-14 15:30:59,426]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 059 Train Loss: 0.2198 Train Acc: 0.9210 Eval Loss: 0.4106 Eval Acc: 0.8745 (LR: 0.000063)
[2025-05-14 15:32:42,436]: [ResNet18_parametrized_relu_quantized_2_bits] Epoch: 060 Train Loss: 0.2235 Train Acc: 0.9215 Eval Loss: 0.3954 Eval Acc: 0.8743 (LR: 0.000063)
[2025-05-14 15:32:42,437]: [ResNet18_parametrized_relu_quantized_2_bits] Best Eval Accuracy: 0.8807
[2025-05-14 15:32:42,536]: 


Quantization of model down to 2 bits finished
[2025-05-14 15:32:42,537]: Model Architecture:
[2025-05-14 15:32:42,585]: ResNet(
  (quant): QuantStub(
    (activation_post_process): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9006], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.701798915863037)
    )
  )
  (dequant): DeQuantStub()
  (initial_layer): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LoggingActivation(
      (activation): Sequential(
        (0): ParametrizedReLU()
      )
    )
  )
  (layer1): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1081], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16213804483413696, max_val=0.16219423711299896)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.744015216827393)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0802], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11783883720636368, max_val=0.12269420176744461)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9022], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.706473350524902)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0859], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1262810081243515, max_val=0.13144689798355103)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9180], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7541399002075195)
            )
          )
        )
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0630], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.078848697245121, max_val=0.11005176603794098)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9276], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.782847881317139)
            )
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0548], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.08178453147411346, max_val=0.08253544569015503)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9187], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.755972862243652)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0474], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.06950276345014572, max_val=0.07265966385602951)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1140], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.18151628971099854, max_val=0.16047091782093048)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9203], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7609758377075195)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0475], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0670633539557457, max_val=0.07536063343286514)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9184], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.755199909210205)
            )
          )
        )
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0657399371266365, max_val=0.0684596449136734)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9640], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.891894340515137)
            )
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0377], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.05447405204176903, max_val=0.05871205031871796)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9182], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.7547287940979)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0328], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.048493266105651855, max_val=0.049829814583063126)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0738], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11057456582784653, max_val=0.11096025258302689)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9276], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.782780170440674)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0323], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.045976098626852036, max_val=0.05084822326898575)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9184], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.755141258239746)
            )
          )
        )
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0280], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.041213784366846085, max_val=0.04263968765735626)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9536], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.860773086547852)
            )
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): ResidualBlock(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0245], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03404286131262779, max_val=0.03939471393823624)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9181], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.754307746887207)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194], device='cuda:0'), zero_point=tensor([2], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.02911866083741188, max_val=0.029102180153131485)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): FakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0484], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
            (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07142644375562668, max_val=0.07390477508306503)
          )
          (activation_post_process): NoopObserver()
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (activation_post_process): NoopObserver()
        )
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9191], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.757419586181641)
            )
          )
        )
      )
    )
    (1): ResidualBlock(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0176], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.026086829602718353, max_val=0.026768408715724945)
        )
        (activation_post_process): NoopObserver()
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (act_fn1): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9185], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.755504608154297)
            )
          )
        )
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): FakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0138], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)
          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020286917686462402, max_val=0.021075600758194923)
        )
        (activation_post_process): NoopObserver()
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (activation_post_process): NoopObserver()
      )
      (skip_add): FloatFunctional(
        (activation_post_process): NoopObserver()
      )
      (act_fn2): LoggingActivation(
        (activation): Sequential(
          (0): ParametrizedReLU()
          (1): QuantStub(
            (activation_post_process): FakeQuantize(
              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.9401], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.82027530670166)
            )
          )
        )
      )
    )
  )
  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
[2025-05-14 15:32:42,585]: 
Model Weights:
[2025-05-14 15:32:42,585]: 
Layer: initial_layer.0
Layer Shape: torch.Size([64, 3, 3, 3])
[2025-05-14 15:32:42,585]: Sample Values (25 elements): [0.12362726032733917, 0.16963957250118256, 0.08215596526861191, -0.07832898199558258, 0.06591964513063431, 0.02524709142744541, 0.16613996028900146, 0.10573133081197739, 0.004003406036645174, -0.24318495392799377, 0.08443305641412735, -0.11753594875335693, -0.20353277027606964, 0.09237754344940186, 0.200815811753273, -0.0610792450606823, -0.02542760781943798, -0.014144175685942173, 0.03761123865842819, -0.04005231335759163, 0.18540935218334198, -0.08192996680736542, -0.08091684430837631, 0.0005925520672462881, 0.09236471354961395]
[2025-05-14 15:32:42,585]: Mean: -0.00055659
[2025-05-14 15:32:42,586]: Min: -0.38328344
[2025-05-14 15:32:42,586]: Max: 0.40961790
[2025-05-14 15:32:42,586]: 
Layer: initial_layer.1
Layer Shape: torch.Size([64])
[2025-05-14 15:32:42,586]: Sample Values (25 elements): [1.1042015552520752, 0.9152682423591614, 1.0046744346618652, 1.1306065320968628, 0.8914349675178528, 0.9953327775001526, 1.1523621082305908, 1.0591378211975098, 1.0299111604690552, 1.007007360458374, 1.0501919984817505, 0.9936059713363647, 0.982471227645874, 1.0046199560165405, 0.957461416721344, 0.9317625761032104, 1.053109049797058, 1.1409120559692383, 1.1237218379974365, 1.2434470653533936, 0.9765558242797852, 1.0842126607894897, 1.0174777507781982, 0.961179792881012, 1.1727122068405151]
[2025-05-14 15:32:42,586]: Mean: 1.05575204
[2025-05-14 15:32:42,586]: Min: 0.85932618
[2025-05-14 15:32:42,586]: Max: 1.60961688
[2025-05-14 15:32:42,588]: 
Layer: layer1.0.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 15:32:42,588]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10811090469360352, 0.10811090469360352, 0.0, 0.0, 0.0, 0.0, 0.10811090469360352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,588]: Mean: -0.00140183
[2025-05-14 15:32:42,588]: Min: -0.10811090
[2025-05-14 15:32:42,589]: Max: 0.21622181
[2025-05-14 15:32:42,589]: 
Layer: layer1.0.bn1
Layer Shape: torch.Size([64])
[2025-05-14 15:32:42,589]: Sample Values (25 elements): [0.9786182045936584, 0.9923753142356873, 1.0641273260116577, 0.9841282963752747, 1.0166970491409302, 0.9414238929748535, 1.0283608436584473, 1.0400640964508057, 1.0036672353744507, 0.9955636262893677, 1.0086643695831299, 1.0097577571868896, 0.9734845161437988, 1.0114258527755737, 1.0152126550674438, 0.9715282320976257, 0.9523316025733948, 1.0218359231948853, 0.9608573317527771, 0.9691611528396606, 0.960043728351593, 0.9820888638496399, 1.0349069833755493, 0.9415916204452515, 0.9620640277862549]
[2025-05-14 15:32:42,589]: Mean: 1.00688672
[2025-05-14 15:32:42,589]: Min: 0.91400647
[2025-05-14 15:32:42,589]: Max: 1.19580960
[2025-05-14 15:32:42,590]: 
Layer: layer1.0.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 15:32:42,591]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08017776161432266, 0.0, 0.0, 0.0, 0.0, 0.08017776161432266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,591]: Mean: -0.00178347
[2025-05-14 15:32:42,591]: Min: -0.08017776
[2025-05-14 15:32:42,591]: Max: 0.16035552
[2025-05-14 15:32:42,591]: 
Layer: layer1.0.bn2
Layer Shape: torch.Size([64])
[2025-05-14 15:32:42,592]: Sample Values (25 elements): [0.9661008715629578, 0.9949667453765869, 1.0046883821487427, 0.9660839438438416, 0.9569737315177917, 0.9781510829925537, 0.9742774963378906, 0.9766533374786377, 1.1153638362884521, 1.007076382637024, 0.9567550420761108, 1.025483250617981, 1.0244253873825073, 0.9740234613418579, 0.9830460548400879, 0.9464349150657654, 0.9937756657600403, 0.9872626662254333, 0.9371441006660461, 1.0044629573822021, 0.9691075086593628, 0.9610667824745178, 0.9764887094497681, 0.9757585525512695, 1.0056079626083374]
[2025-05-14 15:32:42,592]: Mean: 0.98908842
[2025-05-14 15:32:42,592]: Min: 0.92939854
[2025-05-14 15:32:42,592]: Max: 1.15234315
[2025-05-14 15:32:42,593]: 
Layer: layer1.1.conv1
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 15:32:42,594]: Sample Values (25 elements): [0.0, 0.0, 0.08590936660766602, 0.0, -0.08590936660766602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08590936660766602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,594]: Mean: -0.00101374
[2025-05-14 15:32:42,594]: Min: -0.08590937
[2025-05-14 15:32:42,594]: Max: 0.17181873
[2025-05-14 15:32:42,594]: 
Layer: layer1.1.bn1
Layer Shape: torch.Size([64])
[2025-05-14 15:32:42,594]: Sample Values (25 elements): [1.0221141576766968, 1.000619888305664, 0.9766335487365723, 0.9859421253204346, 0.9646846652030945, 0.9748667478561401, 1.0053691864013672, 1.03157377243042, 0.9689276814460754, 0.9967164397239685, 0.9755917191505432, 0.9876191020011902, 0.9535244703292847, 0.9526466727256775, 0.994226336479187, 0.9969666600227356, 0.9726241827011108, 0.9739446043968201, 0.9571265578269958, 1.027685284614563, 0.9671728610992432, 0.965505838394165, 0.9681243300437927, 0.981842577457428, 1.0356260538101196]
[2025-05-14 15:32:42,594]: Mean: 0.98463893
[2025-05-14 15:32:42,595]: Min: 0.94437546
[2025-05-14 15:32:42,595]: Max: 1.08624029
[2025-05-14 15:32:42,596]: 
Layer: layer1.1.conv2
Layer Shape: torch.Size([64, 64, 3, 3])
[2025-05-14 15:32:42,596]: Sample Values (25 elements): [0.0, 0.06296682357788086, 0.0, 0.0, 0.0, 0.06296682357788086, -0.06296682357788086, 0.06296682357788086, -0.06296682357788086, -0.06296682357788086, -0.06296682357788086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06296682357788086, 0.06296682357788086, 0.0, 0.06296682357788086, 0.06296682357788086, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,596]: Mean: -0.00062345
[2025-05-14 15:32:42,597]: Min: -0.06296682
[2025-05-14 15:32:42,597]: Max: 0.12593365
[2025-05-14 15:32:42,597]: 
Layer: layer1.1.bn2
Layer Shape: torch.Size([64])
[2025-05-14 15:32:42,597]: Sample Values (25 elements): [0.9749515056610107, 0.9815163612365723, 0.9813340306282043, 0.975002110004425, 1.0167485475540161, 1.0277748107910156, 1.0021406412124634, 0.9901694059371948, 1.0075161457061768, 1.020501971244812, 0.9889928102493286, 0.9950156807899475, 1.0099931955337524, 0.9846813082695007, 0.9870697259902954, 1.017508625984192, 1.0440304279327393, 1.0292428731918335, 1.0009123086929321, 0.9822759628295898, 1.0099900960922241, 1.0525190830230713, 0.9984039664268494, 0.989635169506073, 1.0057296752929688]
[2025-05-14 15:32:42,597]: Mean: 1.00140476
[2025-05-14 15:32:42,597]: Min: 0.97160596
[2025-05-14 15:32:42,597]: Max: 1.05251908
[2025-05-14 15:32:42,598]: 
Layer: layer2.0.conv1
Layer Shape: torch.Size([128, 64, 3, 3])
[2025-05-14 15:32:42,599]: Sample Values (25 elements): [0.05477333068847656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05477333068847656, 0.05477333068847656, 0.0, 0.0, 0.0, 0.05477333068847656, 0.0, 0.05477333068847656, 0.0, 0.05477333068847656, 0.0, 0.05477333068847656, 0.0, 0.05477333068847656, -0.05477333068847656, 0.0, -0.05477333068847656, 0.0]
[2025-05-14 15:32:42,599]: Mean: -0.00097247
[2025-05-14 15:32:42,600]: Min: -0.05477333
[2025-05-14 15:32:42,600]: Max: 0.10954666
[2025-05-14 15:32:42,600]: 
Layer: layer2.0.bn1
Layer Shape: torch.Size([128])
[2025-05-14 15:32:42,600]: Sample Values (25 elements): [0.9947938919067383, 0.9893172383308411, 0.9738894104957581, 0.9924826622009277, 0.9748108983039856, 0.9897305369377136, 0.9752134680747986, 0.9582445025444031, 0.9739620089530945, 0.9673588871955872, 0.9798228144645691, 0.9683862924575806, 0.9721904993057251, 0.9874603748321533, 0.967174232006073, 0.9758394956588745, 0.9747872948646545, 0.984638512134552, 0.9751744866371155, 0.9835341572761536, 0.9848873019218445, 0.9919903874397278, 0.9858585000038147, 0.9630193114280701, 0.9709051251411438]
[2025-05-14 15:32:42,600]: Mean: 0.97909445
[2025-05-14 15:32:42,600]: Min: 0.95131749
[2025-05-14 15:32:42,600]: Max: 1.02380121
[2025-05-14 15:32:42,601]: 
Layer: layer2.0.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 15:32:42,603]: Sample Values (25 elements): [0.0, 0.0, 0.0, 0.0, 0.0, 0.047387510538101196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.047387510538101196, 0.0, 0.0, 0.0, 0.047387510538101196, -0.047387510538101196, -0.047387510538101196, 0.0, -0.047387510538101196, 0.0]
[2025-05-14 15:32:42,603]: Mean: -0.00053893
[2025-05-14 15:32:42,603]: Min: -0.04738751
[2025-05-14 15:32:42,603]: Max: 0.09477502
[2025-05-14 15:32:42,603]: 
Layer: layer2.0.bn2
Layer Shape: torch.Size([128])
[2025-05-14 15:32:42,604]: Sample Values (25 elements): [0.9780609011650085, 0.9831953644752502, 0.985603392124176, 0.9699916839599609, 0.9986672401428223, 0.9734523296356201, 0.9680030345916748, 0.9672313332557678, 0.9783617258071899, 0.987881064414978, 0.9914674162864685, 0.9822841286659241, 0.9766884446144104, 0.9881622195243835, 0.9738768339157104, 0.9725069999694824, 0.955160915851593, 0.9734624028205872, 0.9857634902000427, 0.992688775062561, 0.9711002707481384, 0.9631443023681641, 0.9683672189712524, 0.9654909372329712, 0.9685330986976624]
[2025-05-14 15:32:42,604]: Mean: 0.98054039
[2025-05-14 15:32:42,604]: Min: 0.95181167
[2025-05-14 15:32:42,604]: Max: 1.02680516
[2025-05-14 15:32:42,605]: 
Layer: layer2.0.downsample.0
Layer Shape: torch.Size([128, 64, 1, 1])
[2025-05-14 15:32:42,606]: Sample Values (25 elements): [0.11399579048156738, 0.0, 0.0, -0.11399579048156738, 0.0, 0.0, 0.0, 0.0, -0.11399579048156738, 0.11399579048156738, 0.0, 0.0, -0.11399579048156738, 0.11399579048156738, -0.11399579048156738, 0.11399579048156738, 0.11399579048156738, 0.0, -0.11399579048156738, 0.11399579048156738, 0.0, -0.11399579048156738, -0.11399579048156738, 0.11399579048156738, 0.11399579048156738]
[2025-05-14 15:32:42,606]: Mean: -0.00093234
[2025-05-14 15:32:42,606]: Min: -0.22799158
[2025-05-14 15:32:42,606]: Max: 0.11399579
[2025-05-14 15:32:42,606]: 
Layer: layer2.0.downsample.1
Layer Shape: torch.Size([128])
[2025-05-14 15:32:42,606]: Sample Values (25 elements): [0.9533707499504089, 0.953360915184021, 0.9307939410209656, 0.9378263354301453, 0.9298250675201416, 0.9398371577262878, 0.9356817603111267, 0.9261525869369507, 0.9191666841506958, 0.9317449331283569, 0.9299081563949585, 0.9326737523078918, 0.9459923505783081, 0.9102878570556641, 0.9238907098770142, 0.9370779991149902, 0.9215127825737, 0.9526005387306213, 0.9235029220581055, 0.9414024949073792, 0.9411063194274902, 0.9229704141616821, 0.9328420758247375, 0.9428545236587524, 0.9405277371406555]
[2025-05-14 15:32:42,607]: Mean: 0.93473685
[2025-05-14 15:32:42,607]: Min: 0.90654773
[2025-05-14 15:32:42,607]: Max: 0.95957530
[2025-05-14 15:32:42,608]: 
Layer: layer2.1.conv1
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 15:32:42,609]: Sample Values (25 elements): [0.0, -0.04747460409998894, 0.04747460409998894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04747460409998894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04747460409998894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04747460409998894, 0.0]
[2025-05-14 15:32:42,609]: Mean: -0.00054668
[2025-05-14 15:32:42,610]: Min: -0.04747460
[2025-05-14 15:32:42,610]: Max: 0.09494921
[2025-05-14 15:32:42,610]: 
Layer: layer2.1.bn1
Layer Shape: torch.Size([128])
[2025-05-14 15:32:42,610]: Sample Values (25 elements): [0.9598740935325623, 0.9705724716186523, 0.9609120488166809, 0.9750157594680786, 0.9800357222557068, 0.9835048913955688, 0.9735873341560364, 0.9621865153312683, 0.9821627140045166, 0.990717351436615, 0.9820916056632996, 0.9656204581260681, 0.9681369662284851, 0.9946232438087463, 0.9662600755691528, 0.9680032134056091, 0.9949647188186646, 0.9834753274917603, 0.9833493828773499, 0.9691876173019409, 0.982257604598999, 0.9792478084564209, 0.9652562141418457, 0.9500587582588196, 0.9674531817436218]
[2025-05-14 15:32:42,610]: Mean: 0.97460580
[2025-05-14 15:32:42,610]: Min: 0.94901860
[2025-05-14 15:32:42,611]: Max: 1.00912881
[2025-05-14 15:32:42,611]: 
Layer: layer2.1.conv2
Layer Shape: torch.Size([128, 128, 3, 3])
[2025-05-14 15:32:42,613]: Sample Values (25 elements): [0.04473322629928589, 0.0, 0.0, 0.0, 0.04473322629928589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.04473322629928589]
[2025-05-14 15:32:42,613]: Mean: -0.00007736
[2025-05-14 15:32:42,613]: Min: -0.04473323
[2025-05-14 15:32:42,613]: Max: 0.08946645
[2025-05-14 15:32:42,613]: 
Layer: layer2.1.bn2
Layer Shape: torch.Size([128])
[2025-05-14 15:32:42,614]: Sample Values (25 elements): [0.9941065311431885, 0.9935222268104553, 1.0031306743621826, 0.9971587061882019, 0.9846821427345276, 1.026563048362732, 1.0040509700775146, 1.0144128799438477, 1.0036773681640625, 0.9835914969444275, 1.008699893951416, 1.0025122165679932, 0.9933581352233887, 0.9965823888778687, 1.0108134746551514, 0.9987226128578186, 0.9805692434310913, 0.9976545572280884, 0.9924742579460144, 0.9849585294723511, 0.9949326515197754, 1.0042304992675781, 0.9938782453536987, 0.9869512915611267, 0.9901346564292908]
[2025-05-14 15:32:42,614]: Mean: 0.99527824
[2025-05-14 15:32:42,614]: Min: 0.96472031
[2025-05-14 15:32:42,614]: Max: 1.03969753
[2025-05-14 15:32:42,615]: 
Layer: layer3.0.conv1
Layer Shape: torch.Size([256, 128, 3, 3])
[2025-05-14 15:32:42,618]: Sample Values (25 elements): [0.0, -0.03772870823740959, 0.0, 0.0, 0.0, 0.0, -0.03772870823740959, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03772870823740959, 0.03772870823740959, -0.03772870823740959, 0.0, 0.0, -0.03772870823740959, 0.0, 0.0, 0.0, 0.0, -0.03772870823740959, 0.0, -0.03772870823740959]
[2025-05-14 15:32:42,618]: Mean: -0.00003122
[2025-05-14 15:32:42,618]: Min: -0.03772871
[2025-05-14 15:32:42,618]: Max: 0.07545742
[2025-05-14 15:32:42,618]: 
Layer: layer3.0.bn1
Layer Shape: torch.Size([256])
[2025-05-14 15:32:42,619]: Sample Values (25 elements): [0.9574694633483887, 0.958783745765686, 0.9881465435028076, 0.9732703566551208, 0.9747344255447388, 0.9664203524589539, 0.9738619327545166, 0.9826359748840332, 0.9599874019622803, 0.9639613628387451, 0.9747738838195801, 0.9660598635673523, 0.9784759879112244, 0.9706805348396301, 0.9708146452903748, 0.9659935235977173, 0.9776524305343628, 0.9769122004508972, 0.9596866965293884, 0.9698302745819092, 0.972517728805542, 0.9807514548301697, 0.9660547375679016, 0.9717080593109131, 0.9744246006011963]
[2025-05-14 15:32:42,619]: Mean: 0.97155178
[2025-05-14 15:32:42,619]: Min: 0.95480067
[2025-05-14 15:32:42,619]: Max: 0.98977524
[2025-05-14 15:32:42,620]: 
Layer: layer3.0.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 15:32:42,627]: Sample Values (25 elements): [0.03277435153722763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03277435153722763, 0.0, 0.0, 0.0, 0.0, 0.0, -0.03277435153722763, 0.03277435153722763, 0.03277435153722763, 0.0, -0.03277435153722763, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,627]: Mean: -0.00029361
[2025-05-14 15:32:42,627]: Min: -0.03277435
[2025-05-14 15:32:42,627]: Max: 0.06554870
[2025-05-14 15:32:42,627]: 
Layer: layer3.0.bn2
Layer Shape: torch.Size([256])
[2025-05-14 15:32:42,628]: Sample Values (25 elements): [0.9785870313644409, 0.9896897673606873, 0.9727998375892639, 0.9728520512580872, 0.9775649309158325, 0.9787168502807617, 0.9733795523643494, 0.9785078167915344, 0.9670048952102661, 0.9754464626312256, 0.9725248217582703, 0.9923689961433411, 0.965215265750885, 0.9777218103408813, 0.9713568687438965, 0.9796678423881531, 0.9776202440261841, 0.9731099605560303, 0.9722911715507507, 0.9763795733451843, 0.9732205867767334, 0.9732655882835388, 0.9841108918190002, 0.9819003939628601, 0.9846510291099548]
[2025-05-14 15:32:42,628]: Mean: 0.97714198
[2025-05-14 15:32:42,628]: Min: 0.95607543
[2025-05-14 15:32:42,628]: Max: 1.01773751
[2025-05-14 15:32:42,629]: 
Layer: layer3.0.downsample.0
Layer Shape: torch.Size([256, 128, 1, 1])
[2025-05-14 15:32:42,629]: Sample Values (25 elements): [-0.07384498417377472, 0.07384498417377472, 0.0, 0.07384498417377472, 0.07384498417377472, 0.07384498417377472, -0.07384498417377472, 0.0, -0.07384498417377472, 0.0, -0.07384498417377472, -0.07384498417377472, 0.07384498417377472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07384498417377472, 0.0, 0.07384498417377472, -0.07384498417377472, -0.07384498417377472, 0.0, -0.07384498417377472]
[2025-05-14 15:32:42,630]: Mean: -0.00027944
[2025-05-14 15:32:42,630]: Min: -0.07384498
[2025-05-14 15:32:42,630]: Max: 0.14768997
[2025-05-14 15:32:42,630]: 
Layer: layer3.0.downsample.1
Layer Shape: torch.Size([256])
[2025-05-14 15:32:42,630]: Sample Values (25 elements): [0.9358205199241638, 0.9378135204315186, 0.9435824155807495, 0.9449731707572937, 0.9340048432350159, 0.9333868026733398, 0.9410706162452698, 0.9263995885848999, 0.9388396143913269, 0.9398631453514099, 0.9511930346488953, 0.9428961277008057, 0.9359276294708252, 0.9536007046699524, 0.9387211203575134, 0.9348052740097046, 0.942099928855896, 0.9374507069587708, 0.9369009137153625, 0.944001317024231, 0.9378828406333923, 0.9355433583259583, 0.9359679818153381, 0.9507996439933777, 0.9386898875236511]
[2025-05-14 15:32:42,630]: Mean: 0.93922275
[2025-05-14 15:32:42,631]: Min: 0.92298955
[2025-05-14 15:32:42,631]: Max: 0.95720255
[2025-05-14 15:32:42,632]: 
Layer: layer3.1.conv1
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 15:32:42,637]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.03227468952536583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03227468952536583, 0.03227468952536583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03227468952536583, -0.03227468952536583, 0.0, -0.03227468952536583, 0.0]
[2025-05-14 15:32:42,637]: Mean: -0.00029740
[2025-05-14 15:32:42,637]: Min: -0.03227469
[2025-05-14 15:32:42,637]: Max: 0.06454938
[2025-05-14 15:32:42,637]: 
Layer: layer3.1.bn1
Layer Shape: torch.Size([256])
[2025-05-14 15:32:42,637]: Sample Values (25 elements): [0.9722022414207458, 0.9619920253753662, 0.9678884148597717, 0.9756857752799988, 0.9706350564956665, 0.9782819151878357, 0.9724859595298767, 0.9643903970718384, 0.9750839471817017, 0.9731425642967224, 0.9632546901702881, 0.963164210319519, 0.966152012348175, 0.962716281414032, 0.9643717408180237, 0.9727804064750671, 0.9664788246154785, 0.9660235047340393, 0.9741019606590271, 0.9751929640769958, 0.9697784781455994, 0.9865114092826843, 0.9678153395652771, 0.9639717936515808, 0.9723567366600037]
[2025-05-14 15:32:42,638]: Mean: 0.96928871
[2025-05-14 15:32:42,638]: Min: 0.95051402
[2025-05-14 15:32:42,638]: Max: 0.99083406
[2025-05-14 15:32:42,639]: 
Layer: layer3.1.conv2
Layer Shape: torch.Size([256, 256, 3, 3])
[2025-05-14 15:32:42,644]: Sample Values (25 elements): [0.027951177209615707, 0.027951177209615707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027951177209615707, 0.0, 0.027951177209615707, 0.0, 0.0, 0.0, 0.0, 0.0, -0.027951177209615707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027951177209615707, 0.0]
[2025-05-14 15:32:42,644]: Mean: -0.00009265
[2025-05-14 15:32:42,644]: Min: -0.02795118
[2025-05-14 15:32:42,644]: Max: 0.05590235
[2025-05-14 15:32:42,644]: 
Layer: layer3.1.bn2
Layer Shape: torch.Size([256])
[2025-05-14 15:32:42,645]: Sample Values (25 elements): [0.9798115491867065, 0.9824298620223999, 0.9939307570457458, 0.969662606716156, 0.9726949334144592, 0.9719584584236145, 0.9931063652038574, 0.9783912897109985, 0.9853935837745667, 0.9790005683898926, 0.9787628054618835, 0.9866260886192322, 0.9881083369255066, 0.9902408719062805, 0.9735960960388184, 0.9871541261672974, 0.9858253002166748, 0.9679573178291321, 0.9727197289466858, 0.9763970971107483, 0.9766424298286438, 0.9864481687545776, 0.9754928350448608, 0.98499596118927, 0.9708662033081055]
[2025-05-14 15:32:42,645]: Mean: 0.97914135
[2025-05-14 15:32:42,645]: Min: 0.96068919
[2025-05-14 15:32:42,645]: Max: 1.00228643
[2025-05-14 15:32:42,646]: 
Layer: layer4.0.conv1
Layer Shape: torch.Size([512, 256, 3, 3])
[2025-05-14 15:32:42,657]: Sample Values (25 elements): [0.024479206651449203, 0.0, 0.0, -0.024479206651449203, 0.024479206651449203, 0.0, 0.0, -0.024479206651449203, -0.024479206651449203, 0.024479206651449203, 0.0, -0.024479206651449203, 0.0, 0.0, 0.024479206651449203, 0.0, -0.024479206651449203, 0.0, 0.0, 0.0, 0.0, -0.024479206651449203, 0.0, -0.024479206651449203, 0.024479206651449203]
[2025-05-14 15:32:42,657]: Mean: 0.00005763
[2025-05-14 15:32:42,658]: Min: -0.02447921
[2025-05-14 15:32:42,658]: Max: 0.04895841
[2025-05-14 15:32:42,658]: 
Layer: layer4.0.bn1
Layer Shape: torch.Size([512])
[2025-05-14 15:32:42,658]: Sample Values (25 elements): [0.9664266705513, 0.9569217562675476, 0.9611498713493347, 0.9654284715652466, 0.9647888541221619, 0.9681940078735352, 0.9761168956756592, 0.9638084173202515, 0.9600273370742798, 0.964276909828186, 0.9617767930030823, 0.9610881209373474, 0.9716727137565613, 0.9720179438591003, 0.9675397872924805, 0.9602934718132019, 0.9617412686347961, 0.9582264423370361, 0.9701690077781677, 0.968400239944458, 0.9665578007698059, 0.9669356346130371, 0.9568431377410889, 0.9633939266204834, 0.9610785245895386]
[2025-05-14 15:32:42,658]: Mean: 0.96370417
[2025-05-14 15:32:42,658]: Min: 0.95611972
[2025-05-14 15:32:42,659]: Max: 0.97919184
[2025-05-14 15:32:42,660]: 
Layer: layer4.0.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 15:32:42,700]: Sample Values (25 elements): [-0.019406985491514206, 0.0, 0.0, 0.0, 0.0, 0.019406985491514206, -0.019406985491514206, 0.019406985491514206, -0.019406985491514206, 0.0, -0.019406985491514206, 0.0, 0.019406985491514206, 0.0, 0.0, 0.0, 0.019406985491514206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,700]: Mean: -0.00004266
[2025-05-14 15:32:42,700]: Min: -0.03881397
[2025-05-14 15:32:42,701]: Max: 0.01940699
[2025-05-14 15:32:42,701]: 
Layer: layer4.0.bn2
Layer Shape: torch.Size([512])
[2025-05-14 15:32:42,701]: Sample Values (25 elements): [0.9705403447151184, 0.9739730358123779, 0.9582985639572144, 0.9742968678474426, 0.9631403088569641, 0.96019446849823, 0.9664241671562195, 0.9626873135566711, 0.9674789905548096, 0.9683116674423218, 0.9619171619415283, 0.9637691378593445, 0.9666675925254822, 0.9631574153900146, 0.9679736495018005, 0.9651535153388977, 0.966870903968811, 0.9679511189460754, 0.9689504504203796, 0.9674856662750244, 0.9607822299003601, 0.9655226469039917, 0.9657975435256958, 0.967340886592865, 0.9645279049873352]
[2025-05-14 15:32:42,701]: Mean: 0.96623147
[2025-05-14 15:32:42,701]: Min: 0.95829856
[2025-05-14 15:32:42,701]: Max: 0.98223448
[2025-05-14 15:32:42,703]: 
Layer: layer4.0.downsample.0
Layer Shape: torch.Size([512, 256, 1, 1])
[2025-05-14 15:32:42,704]: Sample Values (25 elements): [-0.048443712294101715, 0.0, -0.048443712294101715, 0.048443712294101715, 0.0, 0.0, 0.0, -0.048443712294101715, 0.048443712294101715, -0.048443712294101715, 0.0, 0.0, -0.048443712294101715, 0.0, 0.048443712294101715, 0.048443712294101715, 0.048443712294101715, 0.0, -0.048443712294101715, -0.048443712294101715, 0.0, 0.048443712294101715, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,704]: Mean: 0.00011753
[2025-05-14 15:32:42,704]: Min: -0.04844371
[2025-05-14 15:32:42,704]: Max: 0.09688742
[2025-05-14 15:32:42,704]: 
Layer: layer4.0.downsample.1
Layer Shape: torch.Size([512])
[2025-05-14 15:32:42,705]: Sample Values (25 elements): [0.9603912234306335, 0.9549788236618042, 0.9569370150566101, 0.9559571743011475, 0.9592559337615967, 0.9566751718521118, 0.9551097750663757, 0.9538882374763489, 0.9561516642570496, 0.9551443457603455, 0.9555795192718506, 0.9545523524284363, 0.9570283889770508, 0.9549626111984253, 0.9527244567871094, 0.9538531303405762, 0.9572320580482483, 0.9556601047515869, 0.9560489654541016, 0.9553173780441284, 0.9544687867164612, 0.9566823840141296, 0.9542256593704224, 0.9536334872245789, 0.9551941752433777]
[2025-05-14 15:32:42,705]: Mean: 0.95579958
[2025-05-14 15:32:42,705]: Min: 0.94952220
[2025-05-14 15:32:42,705]: Max: 0.96190727
[2025-05-14 15:32:42,706]: 
Layer: layer4.1.conv1
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 15:32:42,736]: Sample Values (25 elements): [-0.017618408426642418, 0.017618408426642418, 0.0, 0.0, -0.017618408426642418, 0.0, 0.017618408426642418, 0.0, 0.017618408426642418, -0.017618408426642418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.017618408426642418, 0.0, 0.0, 0.017618408426642418, 0.0, 0.0, 0.0]
[2025-05-14 15:32:42,736]: Mean: -0.00012602
[2025-05-14 15:32:42,736]: Min: -0.01761841
[2025-05-14 15:32:42,736]: Max: 0.03523682
[2025-05-14 15:32:42,736]: 
Layer: layer4.1.bn1
Layer Shape: torch.Size([512])
[2025-05-14 15:32:42,738]: Sample Values (25 elements): [0.9598329663276672, 0.9570600390434265, 0.9581882953643799, 0.9588363766670227, 0.9642184972763062, 0.9575532674789429, 0.9584939479827881, 0.9592598676681519, 0.9588842988014221, 0.9639027118682861, 0.9625020623207092, 0.9580532312393188, 0.9584512114524841, 0.9587193727493286, 0.957304835319519, 0.9601449966430664, 0.9571495056152344, 0.9577878713607788, 0.956326961517334, 0.96807461977005, 0.9618997573852539, 0.9587856531143188, 0.9573467373847961, 0.964752197265625, 0.9622983336448669]
[2025-05-14 15:32:42,738]: Mean: 0.96019185
[2025-05-14 15:32:42,738]: Min: 0.95530415
[2025-05-14 15:32:42,738]: Max: 0.97103536
[2025-05-14 15:32:42,739]: 
Layer: layer4.1.conv2
Layer Shape: torch.Size([512, 512, 3, 3])
[2025-05-14 15:32:42,771]: Sample Values (25 elements): [0.0, 0.0, 0.0, -0.0137875284999609, -0.0137875284999609, 0.0, 0.0, 0.0, 0.0137875284999609, 0.0, -0.0137875284999609, 0.0, 0.0, -0.0137875284999609, -0.0137875284999609, -0.0137875284999609, 0.0, 0.0, -0.0137875284999609, 0.0, 0.0, 0.0, 0.0137875284999609, 0.0, 0.0]
[2025-05-14 15:32:42,771]: Mean: 0.00004846
[2025-05-14 15:32:42,772]: Min: -0.01378753
[2025-05-14 15:32:42,772]: Max: 0.02757506
[2025-05-14 15:32:42,772]: 
Layer: layer4.1.bn2
Layer Shape: torch.Size([512])
[2025-05-14 15:32:42,772]: Sample Values (25 elements): [0.9653303027153015, 0.9638208746910095, 0.9675447940826416, 0.9679586291313171, 0.9665629863739014, 0.9634669423103333, 0.9697989821434021, 0.9653778076171875, 0.9646610617637634, 0.9677954912185669, 0.9634419679641724, 0.966132402420044, 0.9622510671615601, 0.9640649557113647, 0.9657981395721436, 0.9685600996017456, 0.9680193066596985, 0.9668373465538025, 0.9669607281684875, 0.9630686640739441, 0.9641929864883423, 0.9660572409629822, 0.967187225818634, 0.970133364200592, 0.9685948491096497]
[2025-05-14 15:32:42,772]: Mean: 0.96636504
[2025-05-14 15:32:42,772]: Min: 0.96151704
[2025-05-14 15:32:42,773]: Max: 0.97274929
[2025-05-14 15:32:42,773]: 
Layer: fc
Layer Shape: torch.Size([10, 512])
[2025-05-14 15:32:42,773]: Sample Values (25 elements): [-0.08040560036897659, 0.01554037444293499, -0.08895503729581833, 0.09312943369150162, -0.024967892095446587, 0.050681255757808685, -0.01829773560166359, -0.08365506678819656, 0.03797917440533638, 0.011319540441036224, 0.04750610515475273, -0.056560978293418884, 0.008473848924040794, -0.04366104304790497, -0.009321118704974651, -0.06525524705648422, -0.06270544230937958, 0.01962106116116047, 0.09001421928405762, -0.05675173178315163, -0.047129105776548386, 0.06417021155357361, -0.03827406466007233, 0.027561694383621216, 0.046169545501470566]
[2025-05-14 15:32:42,773]: Mean: -0.00054002
[2025-05-14 15:32:42,773]: Min: -0.12160736
[2025-05-14 15:32:42,773]: Max: 0.13597752
